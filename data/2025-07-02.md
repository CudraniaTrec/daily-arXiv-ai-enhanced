<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

TL;DR: 论文提出了一种无须参考答案的自动识别代码生成错误的新方法incoherence，经实验证明该方法能高效识别大量错误代码，与传统评估结果高度一致，具有实际可用价值。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在根据自然语言规范生成代码时表现优异，但却存在胡乱生成信息（即“幻觉”）的问题。在没有参考实现（oracle）的情况下，人们很难评估生成代码的正确性。该论文旨在解决评估无oracle情况下自动生成代码正确性的问题。

Method: 提出了一种名为incoherence（不一致性）的错误度量方法。该方法可以无需oracle高效估算，评估LLM生成程序出错的概率，并给出错误概率的下界。通过实验验证该方法的有效性，将其与传统oracle评估法（如pass@1）进行对比。

Result: 通过实验，该方法能在无误报的前提下自动识别约三分之二的错误程序。且incoherence方法与传统oracle法在模型排序上的一致性很高，证明了新方法的可靠性。

Conclusion: incoherence度量为无oracle场景下评估自动代码生成的正确性提供了一种高效替代方案，其评估结果与传统基于oracle评估具备强一致性，可以有效辅助及取代oracle。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


### [2] [Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains](https://arxiv.org/abs/2507.00264)
*Isabella Basso do Amaral,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.PL

TL;DR: 本文通过对比实验表明：用PyO3结合Rust开发Python扩展，在性能和易用性方面优于传统的ctypes和cffi方式。


<details>
  <summary>Details</summary>
Motivation: Python以语法简洁和丰富的科学库著称，但其解释器执行速度较慢。优化Python关键代码通常很复杂，涉及到多语言二进制接口，开发人员不得不用复杂的第三方库手动实现。本文关注如何更有效地在Python中进行性能优化。

Method: 通过对比PyO3（Rust绑定工具链）、ctypes和cffi三种方法，评估它们的性能和易用性。以Rust为后端，用其工具链为Python开发扩展。

Result: 通过采用Rust工具链，能获得业界领先的性能表现，同时无需担心API兼容性问题。

Conclusion: Rust的PyO3与传统ctypes、cffi相比，在性能和易用性上均有明显优势。应用Rust工具链能极大提升Python关键代码的执行效率并简化接口开发流程。

Abstract: The Python programming language is best known for its syntax and scientific
libraries, but it is also notorious for its slow interpreter. Optimizing
critical sections in Python entails special knowledge of the binary
interactions between programming languages, and can be cumbersome to interface
manually, with implementers often resorting to convoluted third-party
libraries. This comparative study evaluates the performance and ease of use of
the PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using
Rust tooling developed for Python, we can achieve state-of-the-art performance
with no concern for API compatibility.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [VTS-Guided AI Interaction Workflow for Business Insights](https://arxiv.org/abs/2507.00347)
*Sun Ding,Ude Enebeli,Atilhan,Manay,Ryan Pua,Kamal Kotak*

Main category: cs.SE

TL;DR: VTS-AI通过融入视觉思维策略，让AI高效解析非结构化商业报告，并输出详实、可溯源的业务见解，初步验证其能提升分析效率和质量，下一步将强化财务适配和安全性。


<details>
  <summary>Details</summary>
Motivation: 现代企业在处理大量密集且非结构化的报告时，难以及时高效地提取可用见解，亟需自动化、敏捷的分析工具。

Method: VTS-AI系统将视觉思维策略（Visual Thinking Strategies，VTS）融入AI代理，实现对非结构化文本、表格和图片的商用洞察提取。系统分为micro、meso、macro三个层级，对问题进行标注、溯源、归纳整理，并以可搜索的YAML文件输出，支持分析师通过IDE进行审校与调整，保持人工决策介入。

Result: 在对18页商业报告的测试中，VTS-AI在速度上可与一轮ChatGPT提示持平，但输出内容更丰富（含页面定位、引用原文、严重程度评分、因果关系链接等），且能提示关键指标的走势与需要深入分析的领域。早期结果显示其在指标方向判断和问题预警方面有初步成效。

Conclusion: VTS-AI有效提升了面对复杂非结构化商业报告时的自动化见解生成能力，结合AI与人工判断，为业务分析提供了更敏捷和透明的辅助工具。未来将继续优化模型适配度与安全性，使其成为生产级、易审计的业务分析平台。

Abstract: Modern firms face a flood of dense, unstructured reports. Turning these
documents into usable insights takes heavy effort and is far from agile when
quick answers are needed. VTS-AI tackles this gap. It integrates Visual
Thinking Strategies, which emphasize evidence-based observation, linking, and
thinking, into AI agents, so the agents can extract business insights from
unstructured text, tables, and images at scale. The system works in three tiers
(micro, meso, macro). It tags issues, links them to source pages, and rolls
them into clear action levers stored in a searchable YAML file. In tests on an
18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt
yet produced richer findings: page locations, verbatim excerpts, severity
scores, and causal links. Analysts can accept or adjust these outputs in the
same IDE, keeping human judgment in the loop. Early results show VTS-AI spots
the direction of key metrics and flags where deeper number-crunching is needed.
Next steps include mapping narrative tags to financial ratios, adding
finance-tuned language models through a Model-Context Protocol, and building a
Risk & Safety Layer to stress-test models and secure data. These upgrades aim
to make VTS-AI a production-ready, audit-friendly tool for rapid business
analysis.

</details>


### [4] [An AST-guided LLM Approach for SVRF Code Synthesis](https://arxiv.org/abs/2507.00352)
*Abanoub E. Abdelmalak,Mohamed A. Elsayed,David Abercrombie,Ilhami Torunoglu*

Main category: cs.SE

TL;DR: 本论文提出结合AST嵌入与RAG的新方法用于SVRF代码生成，在代码准确性上提升显著，有效缓解传统SVRF开发面临的人才瓶颈和规则复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 随着工艺节点的推进，半导体应用（如DRC、LVS、OPC）需要的设计规则日益复杂，传统的SVRF开发方法难以应对，且对专家的依赖加剧。面临设计规则复杂化和人才短缺，急需更高效、精准的SVRF代码生成方法。

Method: 提出结合抽象语法树（AST）嵌入和检索增强生成（RAG）的新方法，用于提升SVRF代码合成。AST用于结构化语义校验，RAG引入领域知识增强代码生成准确性，并用SVRF专用评分体系（结合BLEU、ROUGE-L等指标）全面评估生成效果。

Result: 在包含740条DRC规则的大型基准测试中，该方法比基础文本微调方式在代码生成准确率上提升可达40%。整体优化了数据有限情况下的SVRF开发流程，提高了自动化水平和工作效率。

Conclusion: 通过将领域知识和先进代码生成策略融合，显著提升了SVRF代码自动化生成与验证精度，减轻了人工校验负担，加速设计周期，有助于行业内实现更高生产率与更少错误。

Abstract: Standard Verification Rule Format (SVRF) is essential for semiconductor
applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and
Optical Proximity Correction (OPC) and it faces challenges as advancing nodes
create complex design rules that renders traditional SVRF development
ineffective and highlight an expertise gap. This paper introduces a novel
methodology integrating Abstract Syntax Tree (AST) embedding and
Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring
semantic accuracy and error minimization through structural validation with
domain-specific insights for precise code generation.
  We evaluate different T5-based models and propose an innovative SVRF-specific
scoring framework that complements standard metrics like BLEU and ROUGE-L. In
our approach, AST provides rigorous structural validation, while RAG infuses
relevant domain knowledge, effectively enhancing the code generation workflow.
  Testing on a comprehensive benchmark of 740 DRC rule implementations, our
methodology demonstrates up to a 40\% improvement in code generation accuracy
compared to basic text-based fine-tuning process. This fusion of industry
expertise with advanced coding strategies not only optimizes SVRF development
under limited dataset constraints but also creates a more intuitive and
efficient coding environment. Consequently, users can rapidly iterate through
design cycles, reduce manual error correction, and significantly improve
overall productivity.

</details>


### [5] [iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing](https://arxiv.org/abs/2507.00378)
*Xikai Sun,Fan Dang,Kebin Liu,Xin Miao,Zihao Yang,Haimo Lu,Yawen Zheng,Yunhao Liu*

Main category: cs.SE

TL;DR: 本文提出的iPanda系统，结合LLM、关键字抽取、检索增强生成和自我修正机制，实现了协议一致性测试的高度自动化，生成测试代码的效率和准确度均有数量级提升。


<details>
  <summary>Details</summary>
Motivation: 协议一致性测试对于确保协议实现遵循规范至关重要，但传统的测试流程依赖于手动创建大量测试用例和脚本，既耗时又低效。随着大语言模型（LLMs）在代码生成与文本理解方面的进步，自动化测试流程成为可能。

Method: 本文提出了iPanda，一套端到端框架，利用LLM自动化协议一致性测试。其流程包括：1）基于关键字自动生成测试用例；2）基于代码的检索增强生成（retrieval-augmented generation）技术理解实现并生成可执行测试代码；3）引入迭代自我修正机制，交互式提高测试脚本质量；4）通过执行分析自动生成的测试来系统验证实现与规范的一致性。

Result: 在多种协议的综合实验表明，iPanda在测试代码生成的成功率（Pass@1）显著优于仅用LLM的方案，提升幅度为4.675倍至10.751倍。

Conclusion: iPanda显著提升了协议一致性测试的自动化水平和效率，在生成高质量测试代码方面远超传统LLM方法。框架有助于降低人工工作量并提高协议实现的合规性验证效果。

Abstract: Conformance testing is essential for ensuring that protocol implementations
comply with their specifications. However, traditional testing approaches
involve manually creating numerous test cases and scripts, making the process
labor-intensive and inefficient. Recently, Large Language Models (LLMs) have
demonstrated impressive text comprehension and code generation abilities,
providing promising opportunities for automation. In this paper, we propose
iPanda, the first end-to-end framework that leverages LLMs to automate protocol
conformance testing. Given a protocol specification document and its
implementation, iPanda first employs a keyword-based method to automatically
generate comprehensive test cases. Then, it utilizes a code-based
retrieval-augmented generation approach to effectively interpret the
implementation and produce executable test code. To further enhance code
quality, iPanda incorporates an iterative self-correction mechanism to refine
generated test scripts interactively. Finally, by executing and analyzing the
generated tests, iPanda systematically verifies compliance between
implementations and protocol specifications. Comprehensive experiments on
various protocols show that iPanda significantly outperforms pure LLM-based
approaches, improving the success rate (Pass@1) of test-code generation by
factors ranging from 4.675 times to 10.751 times.

</details>


### [6] [Recommending Variable Names for Extract Local Variable Refactorings](https://arxiv.org/abs/2507.00413)
*Taiming Wang,Hui Liu,Yuxia Zhang,Yanjie Jiang*

Main category: cs.SE

TL;DR: 本文提出VarNamer自动推荐提取局部变量的命名方法，相比Eclipse和IntelliJ显著提升了准确性和开发效率，并已应用于主流IDE，具备跨语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有IDE在提取局部变量（Extract Local Variable）重构中自动推荐的变量名与开发者手动命名的一致性较低（大约70%的情况下不同），导致开发者需手动重命名，降低了自动化工具的实用性。

Method: 提出了VarNamer方法，基于大规模实证研究识别出命名变量时有用的关键信息上下文，通过静态分析和数据挖掘技术，利用启发式规则有效推荐变量名。部分规则已集成进入Eclipse IDE。

Result: VarNamer相比主流IDE大幅提升了命名命中率（与Eclipse相比提升52.6%，与IntelliJ提升40.7%），在C++项目的真实重构上也能取得与Java类似效果，显示出通用性。用户实验表明，该工具可加快重构速度27.8%、减少49.3%的变量名编辑工作量。

Conclusion: VarNamer有效解决了变量自动命名的问题，提升了IDE自动化重构的实际帮助，并已被主流IDE吸收采用，具有良好应用前景与跨语言潜力。

Abstract: Extract local variable is one of the most popular refactorings, and most IDEs
and refactoring tools provide automated support for this refactoring. However,
we find approximately 70% of the names recommended by these IDEs are different
from what developers manually constructed, adding additional renaming burdens
to developers and providing limited assistance. In this paper, we introduce
VarNamer, an automated approach designed to recommend variable names for
extract local variable refactorings. Through a large-scale empirical study, we
identify key contexts that are useful for composing variable names. Leveraging
these insights, we developed a set of heuristic rules through program static
analysis techniques and employ data mining techniques to recommend variable
names effectively. Notably, some of our heuristic rules have been successfully
integrated into Eclipse, where they are now distributed with the latest
releases of the IDE. Evaluation demonstrates its superiority over
state-of-the-art IDEs. Specifically, VarNamer significantly increases the
chance of exact match by 52.6% compared to Eclipse and 40.7% compared to
IntelliJ IDEA. We also evaluated the proposed approach with real-world extract
local variable refactorings conducted in C++ projects, and the results suggest
that the approach can achieve comparable performance on programming languages
besides Java. It may suggest the generalizability of VarNamer. Finally, we
designed and conducted a user study and the results of the user study suggest
that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits
on the recommended variable names.

</details>


### [7] [Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development](https://arxiv.org/abs/2507.00421)
*Parthiv Katapara,Anand Sharma*

Main category: cs.SE

TL;DR: 本文综述了DevOps方法在嵌入式系统领域的应用，包括持续集成、部署及测试在内的实际挑战与进展，并提出了未来改进的路线图。


<details>
  <summary>Details</summary>
Motivation: 随着硬件与软件协同设计产品的复杂性增加，传统开发流程已难以满足嵌入式系统的需求，因此推动了DevOps在嵌入式系统及固件开发中的应用。

Method: 通过综述20篇学术和工业文献，综合分析了持续集成、持续交付和自动化测试等DevOps原则在嵌入式系统中的适应与实践，并对工具、测试策略、流水线自动化和安全措施进行了分类。

Result: 总结当前嵌入式DevOps在部署流程和可观测性上的局限，提出了未来的研究路线，为研究者和实践者梳理了嵌入式DevOps领域的现状和挑战。

Conclusion: 本综述对碎片化的嵌入式DevOps文献进行了结构化梳理，为未来的研究与行业实践提供了统一视角和改进方向。

Abstract: The adoption of DevOps practices in embedded systems and firmware development
is emerging as a response to the growing complexity of modern
hardware--software co-designed products. Unlike cloud-native applications,
embedded systems introduce challenges such as hardware dependency, real-time
constraints, and safety-critical requirements. This literature review
synthesizes findings from 20 academic and industrial sources to examine how
DevOps principles--particularly continuous integration, continuous delivery,
and automated testing--are adapted to embedded contexts. We categorize efforts
across tooling, testing strategies, pipeline automation, and security
practices. The review highlights current limitations in deployment workflows
and observability, proposing a roadmap for future research. This work offers
researchers and practitioners a consolidated understanding of Embedded DevOps,
bridging fragmented literature with a structured perspective.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Encoding Peano Arithmetic in a Minimal Fragment of Separation Logic](https://arxiv.org/abs/2507.00465)
*Sohei Ito,Makoto Tatsuta*

Main category: cs.LO

TL;DR: 本文证明了仅用点指谓词、常数0和后继函数的分离逻辑片段已足以表达Peano算术所有\Pi^0_1公式，有效性判定为\Pi^0_1-完全，说明哪怕极简片段也同样拥有强表达力与不可判定性。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在研究带有自然数扩展的分离逻辑最小片段的表达能力，并探讨如此简单的逻辑片段在理论与程序验证中是否足够强大。

Method: 通过构造从Peano算术（PA）的\Pi^0_1公式到该分离逻辑片段的翻译，将数论公式编码为仅包含点指谓词、常数0和后继函数的分离逻辑表达式，利用基于堆内存结构的算术操作编码。同时利用有限模型理论提供了替代的不可判定性证明。

Result: 该分离逻辑片段能够等价地表达所有PA的\Pi^0_1公式，因此，其有效性判定问题是不可判定的，并且是\Pi^0_1-完全的。作者还证实此翻译无法保留\Sigma^0_1公式的有效性。

Conclusion: 即使分离逻辑的最精简片段也具有极高的表达能力，并在有效性判定上达到了理论上的不可判定界限。此发现揭示了程序验证领域中相关逻辑的理论深度。

Abstract: This paper investigates the expressive power of a minimal fragment of
separation logic extended with natural numbers. Specifically, it demonstrates
that the fragment consisting solely of the intuitionistic points-to predicate,
the constant 0, and the successor function is sufficient to encode all
$\Pi^0_1$ formulas of Peano Arithmetic (PA). The authors construct a
translation from PA into this fragment, showing that a $\Pi^0_1$ formula is
valid in the standard model of arithmetic if and only if its translation is
valid in the standard interpretation of the separation logic fragment. This
result implies the undecidability of validity in the fragment, despite its
syntactic simplicity. The translation leverages a heap-based encoding of
arithmetic operations - addition, multiplication, and inequality - using
structured memory cells. The paper also explores the boundaries of this
encoding, showing that the translation does not preserve validity for
$\Sigma^0_1$ formulas. Additionally, an alternative undecidability proof is
presented via a reduction from finite model theory. Finally, the paper
establishes that the validity problem for this fragment is $\Pi^0_1$-complete,
highlighting its theoretical significance in the landscape of logic and program
verification.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文系统评估了文本型和多模态LLM在多领域与多模态表格理解任务中的表现，发现其对表格格式具备一定鲁棒性，但科学表格处理依然是瓶颈，并推出了TableEval基准。


<details>
  <summary>Details</summary>
Motivation: 表格是研究、商业、医学和教育等领域中常用的数据结构工具，但大语言模型（LLMs）在处理结构化表格数据上的效果尚未被系统研究。本研究旨在探讨LLMs在表格理解任务中的有效性和适用性，填补相关研究空白。

Method: 通过跨领域（科学vs.非科学）、跨模态（图片vs.文本）评测，系统比较文本型与多模态LLM对不同来源（学术文献、Wikipedia、财报）的表格理解表现。为全面评估，还进行了可解释性分析，并构建了包含五种表格格式（图片、字典、HTML、XML、LaTeX）的TableEval评测基准。

Result: 结果显示LLM在不同表格表现形式（如图片或文本）上具有一定的鲁棒性，但在处理科学领域表格时仍面临明显挑战。

Conclusion: 尽管LLM对表格格式具有良好适应性，但其在复杂科学表格方面的理解和处理能力有限，未来需要进一步改进以提升其在该场景下的表现。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [10] [Reducing Profile-Based Matching to the Maximum Weight Matching Problem](https://arxiv.org/abs/2507.00047)
*Seongbeom Park*

Main category: cs.DM

TL;DR: 本文针对profile-based匹配难以高效规约为最大权匹配问题的瓶颈，提出了混合进制权重函数方案，大幅提升了算法效率，并通过实验证明方法在多目标匹配场景中的实用性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的profile-based匹配表达了多种实际优化问题（如rank-maximal、fair和weight-maximal匹配），但将这些问题规约为最大权匹配时，通常需要极大的权重，导致效率低下。

Method: 提出并分析了一种基于混合进制权重函数的新规约方法，通过将各效用值映射到以(2U_i+1)为基的混合进制数字，实现profile-based匹配到最大权匹配的高效转换。算法复杂度被详细分析。同时，研究了rank-maximal/fair/weight-maximal匹配的权重下界及更高效的算法，并给出了验证最大权匹配的新方法。

Result: 提出的混合进制权重函数规约有效降低了权重范围，提升了最大权匹配算法处理profile-based匹配的效率，理论复杂度为O(m√n(log n + ΣlogU_i))。此外，对实际学校录取数据进行实验，验证了新算法的有效性。此外，还改进了fair和weight-maximal匹配的计算复杂度，并能将最大权匹配的验证规约到rank-maximal匹配。

Conclusion: 通过创新的混合进制权重函数，显著提升了profile-based匹配问题转换和求解的效率，并拓宽了最大权匹配在多目标优化匹配问题中的实际可用性。新方法兼具理论创新与应用实效，对相关领域有积极推动作用。

Abstract: The profile-based matching problem is the problem of finding a matching that
optimizes profile from an instance $(G, r, \langle u_1, \dots, u_r \rangle)$,
where $G$ is a bipartite graph $(A \cup B, E)$, $r$ is the number of utility
functions, and $u_i: E \to \{ 0, 1, \dots, U_i \}$ is utility functions for $1
\le i \le r$. A matching is optimal if the matching maximizes the sum of the
1st utility, subject to this, maximizes the sum of the 2nd utility, and so on.
The profile-based matching can express rank-maximal matching
\cite{irving2006rank}, fair matching \cite{huang2016fair}, and weight-maximal
matching \cite{huang2012weight}. These problems can be reduced to maximum
weight matching problems, but the reduction is known to be inefficient due to
the huge weights.
  This paper presents the condition for a weight function to find an optimal
matching by reducing profile-based matching to the maximum weight matching
problem. It is shown that a weight function which represents utilities as a
mixed-radix numeric system with base-$(2U_i+1)$ can be used, so the complexity
of the problem is $O(m\sqrt{n}(\log{n} + \sum_{i=1}^{r}\log{U_i}))$ for $n =
|V|$, $m = |E|$. In addition, it is demonstrated that the weight lower bound
for rank-maximal/fair/weight-maximal matching, better computational complexity
for fair/weight-maximal matching, and an algorithm to verify a maximum weight
matching can be reduced to rank-maximal matching. Finally, the effectiveness of
the profile-based algorithm is evaluated with real data for school choice
lottery.

</details>


### [11] [Verification of Hamiltonian Path Conjecture (BHR Conjecture) for Integers up to p=31](https://arxiv.org/abs/2507.00059)
*Ranjan N Naik*

Main category: cs.DM

TL;DR: 本文提出基于频率划分与调整及回溯的新算法，并用Python实现验证了BHR猜想对所有p<37的质数情形，比前人研究更进一步。


<details>
  <summary>Details</summary>
Motivation: BHR猜想是组合数学中的一个重要未解难题，探究指定边长序列能否在完全图上形成哈密顿路径，对于理解图的结构与实际应用具有理论意义和实际价值。本文希望通过新方法推进这一猜想的研究进展。

Method: 提出用频率划分以及局部/全局调整操作，再结合回溯搜索，开发并实现了Python程序来探索完全图上满足给定边长序列的哈密顿路径，并进行实验检验。

Result: 对质数p小于37的所有情形使用新方法和程序进行了实验探索，所有情形都找到了满足猜想的哈密顿路径，相比之前仅验证到23质数取得了更大的进展。

Conclusion: 利用频率划分和调整操作结合回溯的方法，可以有效地寻找满足BHR猜想条件的哈密顿路径，在小于37的质数情况下取得了比先前研究更好的进展。

Abstract: The BHR (Buratti-Horak-Rosa) Conjecture (2006) proposes that for every p and
a multiset L of (p-1) positive integers modulo p, there exists a Hamiltonian
path in the Complete Graph Kp with consecutive edge lengths given by the
elements of L. In this article, we outline an approach to the conjecture based
on frequency partitions and local/global adjustment operations and
backtracking. We describe the mathematical strategy, experimental evidence, and
implementation in a Python Program to explore valid Hamiltonian paths p < 37.
This is a result an improvement over by Mariusz Meszka for all primes up to 23
(included) with the aid of a computer.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [12] [Eilenberg correspondence for Stone recognition](https://arxiv.org/abs/2507.00409)
*Jorge Almeida,Ondřej Klíma*

Main category: cs.FL

TL;DR: 本文以Stone拓扑代数为基础，推广和加深了形式语言识别理论，拓展了经典的Eilenberg对换和代数刻画，能够处理正规语言之外更广泛的语言类，并提供了实际判据示例。


<details>
  <summary>Details</summary>
Motivation: 传统的Eilenberg理论主要处理正规语言与有限代数结构的对换关系，但存在对更广泛语言类刻画不足的问题。本文致力于扩展这一理论框架，使其适用于更一般的语言识别情形。

Method: 通过将语言识别表示为Stone拓扑代数下连续同态的闭开集的原像，建立了语言范畴与有序Stone拓扑代数范畴之间的Eilenberg对换，并给出了可通过伪不等式描述的Birkhoff/Reiterman型定理。还利用最小自动机的Stone完成，提出了一般性的方法，用于证明某语言不属于某个给定的语言类。

Result: 建立了一种更一般的拓扑代数视角的语言识别方法，推广了Eilenberg对应和Birkhoff/Reiterman定理，并实际应用于context-free语言交集类，演示了如何判定某语言不属于给定语言范畴。

Conclusion: 本文提出的扩展的语言识别理论不仅覆盖正规语言类，也包含更广泛的语言集合，并提供了一种新的方法来判定特定语言是否属于某个语言范畴。

Abstract: We develop and explore the idea of recognition of languages (in the general
sense of subsets of topological algebras) as preimages of clopen sets under
continuous homomorphisms into Stone topological algebras. We obtain an
Eilenberg correspondence between varieties of languages and varieties of
ordered Stone topological algebras and a Birkhoff/Reiterman-type theorem
showing that the latter may me defined by certain pseudo-inequalities. In the
case of classical formal languages, of words over a finite alphabet, we also
show how this extended framework goes beyond the class of regular languages by
working with Stone completions of minimal automata, viewed as unary algebras.
This leads to a general method for showing that a language does not belong to a
variety of languages, expressed in terms of sequences of pairs of words, which
is illustrated when the class consists of all finite intersections of
context-free languages.

</details>
