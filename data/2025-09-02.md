<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 12]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL利用统一中间表示CrossGL实现多编程语言的高效互译，不仅提升了可扩展性，还简化了新增语言的流程，实证支持其在多个领域的通用性和实际可用性，推进了语言无关的编程方法。


<details>
  <summary>Details</summary>
Motivation: 目前编程语言的跨平台翻译通常需要为每一语言对开发单独的转换器，随着支持语言数量增加，复杂度呈指数级增长。为了解决该问题，作者提出了构建一个统一的中间表示（IR），以简化多语言互译流程。

Method: 提出CrossTL系统，通过统一的中间表示CrossGL，实现包括CUDA、HIP、Metal、DirectX HLSL、OpenGL GLSL、Vulkan SPIR-V、Rust和Mojo等多种语言的双向翻译。系统架构包括：为每种语言设计的词法/语法解析器生成AST、双向CrossGL翻译模块（包含ToCrossGLConverter类用于转换导入、CodeGen类用于目标生成）、以及完整的后端管道实现。

Result: 证明了CrossTL系统在多个编程领域能够高效支持各类语言间的编译和执行。评估显示所有支持后端均能成功编译运行，且添加新语言只需实现独立的前后端组件，扩展性良好。

Conclusion: CrossTL系统通过统一IR与模块化架构，有效支撑GPU计算、图形编程、系统语言等领域的代码互译，为“一次编写，到处部署”的语言无关编程模式迈出重要一步。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [2] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: 本文分析了Lean数学库Mathlib的成长管理方式，介绍了持续演进中有效应对变更和维护负担的多种技术与策略，包括代码弃用管理、质量分析、编译优化、技术债务处理及定制贡献审核工具。


<details>
  <summary>Details</summary>
Motivation: Mathlib作为增长最快的形式数学库之一，需解决扩展过程中的变更管理和维护压力，保障项目持续健康发展。

Method: 采用弃用系统处理重大变更、利用代码质量分析工具（如linter）改善用户体验、通过有意识的库设计提速编译、处理技术债务、以及编写定制工具辅助审查与分类新贡献。

Result: 提出并实施了多项措施，包括系统性的弃用流程、用户友好反馈机制、编译性能优化以及贡献审核工具等，显著改善了项目管理及维护效率。

Conclusion: 通过一系列管理策略，成功应对了Mathlib快速发展的挑战，同时避免了维护者负担过重。

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 该论文提出利用LLM与RAG技术自动将UML模型转化为Qiskit量子代码，并显著提升了代码质量，未来可推广到更多自动化软件开发应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决量子及混合量子-经典软件系统在异构平台和开发者技能缺乏方面的成本和风险问题。通过模型驱动的方法，结合大型语言模型和RAG，加强软件系统自动化代码生成和转换能力。

Method: 利用大型语言模型（LLMs）结合检索增强生成（RAG）流程，从公共GitHub仓库检索Qiskit示例代码，自动从UML模型实例生成Python量子代码，并进行实验验证。

Result: 实验结果显示，精心设计的提示词可将CodeBLEU分数提升至原来的四倍，即生成的量子代码更准确且一致。

Conclusion: 研究验证了结合RAG流程和LLM从软件系统模型实例自动生成高质量量子代码的可行性，并提出未来可扩展至其他模型实例或代码转换场景。

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [4] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: 提出了UTRL强化学习框架，对单元测试和代码生成器进行对抗训练，获得高质量自动化测试，实验证实优于人类编写测试与现有AI大模型。


<details>
  <summary>Details</summary>
Motivation: 单元测试对于程序评估至关重要，但编写高质量、全面的单元测试很具挑战，人工智能自动生成测试也存在训练不足的问题。

Method: 提出了UTRL框架，通过强化学习和对抗训练机制，联合训练单元测试生成器与代码生成器，使二者在奖励机制下互相提升能力。

Result: 使用UTRL训练的Qwen3-4B模型生成的测试，比同模型采用有监督微调训练的测试质量更高，评估结果更接近人类真实测试集。同时，在生成高质量单元测试方面，Qwen3-4B优于GPT-4.1等主流前沿模型。

Conclusion: UTRL框架能够有效提升大语言模型生成高质量单元测试的能力，是训练相关模型的有效方法。

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [5] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: 该文提出基于指令微调和LoRA的大语言模型用于Bug分配，避免了传统方法的高成本，在实验中取得优异推荐表现，并在新数据上显著提升了准确率，被验证为实用、高效的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统 bug triage 过程在大型项目中效率低下且分配不一致，现有自动化方法又依赖高成本的工程或复杂的图结构，亟需轻量高效的新方案。

Method: 提出了基于LoRA适配器微调的大语言模型框架，引入候选约束的解码机制，确保指派结果合法；并在EclipseJDT和Mozilla数据集上进行了实验评估。

Result: 模型在Hit@10指标上最高可达0.753，虽Top-1并非极高，但在最新数据快照下准确率大幅提升，显示出在实际人工参与流程中应用的潜力。

Conclusion: 指令微调的大语言模型能有效辅助 bug triage 任务，且表现优异，成为特征工程和图方法的有力替代方案。

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [6] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 本文系统对比了LLM自动摘要与简单屏蔽过去观察的方法，发现在多个模型配置和工程任务下，简单屏蔽法在降低成本同时，效果与甚至略优于复杂摘要法，展示了极简方案在代理上下文管理上的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的智能代理通过迭代推理、探索和工具使用来解决复杂任务，这过程中会生成高昂且冗长的上下文历史。当前主流的软件工程代理（如OpenHands或Cursor）多采用LLM自动摘要来管理上下文，但尚不清楚这种复杂方法是否相较于简单去除旧观察存在实际性能提升。

Method: 在SWE-agent于SWE-bench Verified基准下，系统性地比较了LLM摘要法与简单观察屏蔽法，并在五种多样化模型配置下进行实证评估。

Result: 简单观察屏蔽策略能以一半的成本达到与原始代理相同甚至略优的解决率，有时还优于LLM摘要法。以Qwen3-Coder 480B为例，屏蔽法解决率从53.8%提升至54.8%，在更低成本下与摘要法表现持平。

Conclusion: 在SWE-agent于SWE-bench Verified场景下，最简单的上下文管理方法（如观察屏蔽）往往是最高效且有效的选择。

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [7] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 本文提出用大型语言模型辅助的指针分析方法（LMPA），通过理解和建模自定义函数，解决了错误传播和分析保守性问题，有效提升了指针分析的精度与扩展性，同时指出实现过程中需应对的新挑战。


<details>
  <summary>Details</summary>
Motivation: 指针分析领域存在已久，但现有方法对用户自定义函数往往处理过于保守，主要因为对代码语义理解不够深入，导致错误信息传播，降低了分析的精度和扩展性。

Method: 提出LMPA（LLM增强指针分析）框架，将大型语言模型（LLM）引入指针分析。通过识别和类比用户定义函数与系统API以增强建模，并利用自然语言增强的摘要分析策略，推断初始指向集并提高精准度与效率。

Result: LMPA能够减少跨调用上下文的错误信息传播，同时提出了结合自然语言的摘要策略，提升指针分析的精度和可扩展性。

Conclusion: 集成LLM的新方法为指针分析带来了更高精度和更强可扩展性，但实际部署中仍有若干挑战需要解决。

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [8] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: 本工作提出了一种从多策略角度选择强化学习测试案例的新方法，提升了测试集对不同代理的覆盖性和发现错误的能力，有助于RL系统可靠部署。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）测试大多针对单一策略，测试案例难以复用，且跨策略的测试有效性存疑，政策经过一种策略生成的测试集往往不足以暴露其他策略的缺陷。如何自动化、有效挑选能够普适于多种策略的测试案例成为实际落地部署前的关键难题。

Method: 提出Multi-Policy Test Case Selection（MPTCS）方法，从多个策略出发，根据可解性、多样性和一般难度，从测试案例候选集中自动选择有代表性的、策略不可知的测试案例。方法中引入difficulty score度量测试案例难度，并借鉴质量-多样性算法设计离散化的测试案例描述表面以增强多样性覆盖，能兼容不同的策略和框架。

Result: 实验证明，通过基于difficulty score的MPTCS方法选取出的测试案例集合，不仅能够覆盖更广泛的状态空间，还更容易触发不同策略的缺陷表现。同时研究了政策数量增多对测试效果和成本的影响，并证实该表面设计有助于揭示更广泛的策略缺陷。

Conclusion: 多策略视角的测试案例选择机制可有效提升测试集合的普适性和鲁棒性，是面向实际部署RL代理的关键一步。MPTCS方法为未来RL系统安全性和可靠性评测提供了通用工具和新思路。

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [9] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: AI生成的代码在简化结构和提高生成效率方面表现突出，但易出现特定类型的缺陷和安全隐患，需要进一步关注AI代码的质量保障。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码助手日益融入软件开发流程，理解AI生成代码与人类编写代码在质量上的差异对于保障软件的可靠性、可维护性和安全性变得非常重要。

Method: 对由人类开发者和三种先进大语言模型（ChatGPT、DeepSeek-Coder、Qwen-Coder）生成的50万段Python和Java代码样本，从代码缺陷、安全漏洞和结构复杂度等多个维度进行大规模比较。缺陷通过正交缺陷分类法（Orthogonal Defect Classification）识别，安全漏洞通过常见弱点枚举（Common Weakness Enumeration）归类。

Result: AI生成代码通常结构更简单、重复性更高，但更容易出现未使用的结构和硬编码调试信息；人类代码结构更复杂，维护性问题更为集中。值得注意的是，AI生成代码也包含更多高风险安全漏洞。

Conclusion: AI和人类编写的代码在缺陷表现上有明显不同，提示AI辅助编程下需要制定专门的质量保障措施。

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [10] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本文通过访谈IT行业敏捷与DevOps专家，分析敏捷方法在DevOps实践中的实现与融合，提出两者关系的新理解，为快速交付和高质量软件提供指导。


<details>
  <summary>Details</summary>
Motivation: IT行业对快速软件交付的需求不断增长，催生了更加灵活高效的软件开发与运维方法。本文旨在评估敏捷开发方法与DevOps实践的可行性与适用性。

Method: 通过在IT行业内不同部门的敏捷与DevOps从业者进行十一次半结构化访谈，采用主题分析法，提取出51个独特编码并综合为19个主题，覆盖DevOps生命周期各阶段。

Result: 研究结果揭示了敏捷方法在DevOps实践中的融合和实施情况，提出了敏捷方法与DevOps实践之间相互关系的新理解。

Conclusion: 敏捷方法能够有效整合进DevOps实践，推动持续集成、部署和高质量软件交付，适应快速变化的市场需求。本文为业界提供了敏捷与DevOps结合的深层洞见。

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [Interpolation for Converse PDL](https://arxiv.org/abs/2508.21485)
*Johannes Kloibhofer,Valentina Trucco Dalmas,Yde Venema*

Main category: cs.LO

TL;DR: 本论文证明了带有反向操作的Converse PDL逻辑具备局部Craig插值和Beth可定义性。作者引入了一个新颖的循环序列证明系统并改进了相关证明理论方法，为该逻辑提供了重要的理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究命题动态逻辑（PDL）与其扩展——带有反向操作的Converse PDL，在逻辑推理中的插值与可定义性质，推动理论基础的发展。

Method: 通过对Maehara的证据理论方法进行改进，提出了一个新的针对Converse PDL的循环序列系统，并设计了分析性割规则和焦点机制用于识别成功循环。

Result: 证明了Converse PDL在原子程序和命题变量上的局部Craig插值性质，并由此推导出该逻辑满足Beth可定义性性质。

Conclusion: Converse PDL具备局部插值和Beth可定义性，这些性质通过新提出的循环序列系统与证明方法得以展示，对逻辑系统结构和推理能力的理解有所深化。

Abstract: Converse PDL is the extension of propositional dynamic logic with a converse
operation on programs. Our main result states that Converse PDL enjoys the
(local) Craig Interpolation Property, with respect to both atomic programs and
propositional variables. As a corollary we establish the Beth Definability
Property for the logic.
  Our interpolation proof is based on an adaptation of Maehara's
proof-theoretic method. For this purpose we introduce a sound and complete
cyclic sequent system for this logic. This calculus features an analytic cut
rule and uses a focus mechanism for recognising successful cycles.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种新型的多偏见对抗性数据增强方法CoBA，通过三元组级别的语义操作打破虚假相关性，实验证明其能明显提升模型在下游任务中的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常常依赖训练数据中的虚假相关性（非目标特征），导致在新数据上的表现下降和泛化能力差，这成为模型应用中的重大问题。该论文旨在解决模型对多种偏见（如性别偏见、简单性偏见）的依赖及提升模型的泛化鲁棒性。

Method: 提出了一种更通用的对抗性数据增强方法，称为counterbias data augmentation，并在此基础上构建了一个统一框架CoBA。该框架通过将文本分解为主谓宾三元组，再有选择地修改这些三元组以打破虚假相关性，然后重构新文本，实现针对多重偏见的数据增强。

Result: 通过大量实验表明，CoBA方法不仅提升了下游任务的表现，还能有效减少多种偏见，并显著增强模型在非分布内数据上的鲁棒性。

Conclusion: CoBA为深度学习模型面对虚假相关性和偏见提供了高效、通用且鲁棒的解决方案。

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [13] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 首次公开了含年龄估算的大规模德语有毒评论数据集，揭示了不同年龄群体在网络毒性言论上的区别，可推动内容审核和社会语言学研究的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的有毒言论数据集缺乏人口统计背景，限制了我们对不同年龄群体网络交流方式的理解。

Method: 与德国公共服务内容网络funk合作，收集并注释了首个包含年龄估算的大规模德语毒性评论数据集。数据集包括来自Instagram、TikTok和YouTube的人工和大模型注释的评论，并结合关键词筛选和自动化与人工结合的标注流程。

Result: 数据集显示，不同年龄群体在有毒言论上的表现存在差异：年轻用户更倾向于使用表达性语言，而年长用户更常参与虚假信息和贬低性言论。

Conclusion: 该数据集为研究不同人口群体的语言变异和开发更公平、关注年龄特征的内容审核系统提供了新机会。

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [14] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: IBM推出的Granite Embedding R2嵌入模型在速度、性能与上下文支持大幅领先同时全面开源，极适用于企业大规模检索与多场景应用。


<details>
  <summary>Details</summary>
Motivation: 在企业级大规模密集检索场景下，目前的编码器嵌入模型在性能、速度、上下文长度等方面还存在不足，并且对数据的治理与开放获取的需求日益增长。作者希望提升模型的检索性能和速度，同时提供更透明的数据治理和企业级许可。

Method: 基于第一代Granite模型，提出了Granite Embedding R2系列，包括bi-encoder与cross-encoder结构，22层高效检索模型和12层轻量模型，及高质量重排序模型。所有模型完全基于适用于企业的数据进行训练，并在模型开发过程中进行全面数据治理。

Result: Granite R2模型将上下文长度提升至8192 tokens，检索速度提升19%-44%，在文本、代码、长文档检索、多轮对话和表格数据多领域达到SOTA性能。在标准基准、IBM自建评测集和真实企业用例中表现优异，且所有模型均以Apache 2.0开源提供。

Conclusion: Granite R2系列模型突破了检索嵌入模型的性能与速度瓶颈，为企业级应用提供了高效、可控和开放的数据解决方案，确立了开源嵌入模型的新基准。

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [15] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于Transformer的新型墨迹生成模型TrInk，通过创新的嵌入和注意力机制，大幅提升了生成手写体的准确率和风格一致性。实验表明，该模型优于以往方法，在关键指标上取得显著进步。


<details>
  <summary>Details</summary>
Motivation: 现有的墨迹生成方法在捕捉全局依赖和文字与生成笔画点的对齐方面存在不足，影响了生成手写体的可读性和风格一致性。为此，作者希望提出一种更有效的模型提升上述指标。

Method: 提出TrInk，一种基于Transformer结构的墨迹生成模型。为增强输入文本与生成笔画的对齐，设计了缩放位置嵌入和高斯记忆掩码，应用于交叉注意力模块。同时搭建主观与客观评测流程用于全面评估生成手写的可读性和风格一致性。

Result: 在IAM-OnDB数据集上，TrInk相较于前人方法使字符错误率（CER）降低了35.56%，单词错误率（WER）降低了29.66%。

Conclusion: TrInk在手写墨迹生成领域显著提升了生成内容的辨识度和风格匹配性，其Transformer架构及创新型对齐方法有效推动了墨迹生成技术的发展。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [16] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 本研究发现，大模型在价格谈判中会受锚定效应影响，但具备推理能力的模型更能抵御该偏差，性格特征影响不明显。这有助于未来更好地应用和设计安全的大模型系统。


<details>
  <summary>Details</summary>
Motivation: 认知偏差在人工智能中的表现会影响其在真实世界中的可靠性，因此需要研究大模型在谈判等实际任务中的认知偏差。

Method: 通过让大模型担任卖方代理，在价格谈判中有意应用锚定效应，采用客观和主观指标评估谈判过程。同时探究推理能力和性格对锚定效应的影响。

Result: 实验结果显示，大模型和人类一样会受到锚定效应影响。具备推理能力的大模型不易受锚定效应影响，长链条思维可以缓解这一效应。性格特征与锚定效应敏感性无显著相关性。

Conclusion: 该研究加深了对大模型认知偏差的理解，有助于推动大模型的安全和责任应用。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [17] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: 本文提出基础视觉感知数据集Percept-V，并实证发现主流多模态大模型在此类简单任务上随复杂度提升表现显著下降，揭示感知与认知能力短板。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）在编码、数学、科学等领域展现了强大的推理能力，但在对基本形状和结构的简单感知任务上的表现尚未有系统实验评估。该研究旨在填补这一空白。

Method: 作者构建了一个全新的数据集Percept-V，包含7200个由程序生成的基础图形图片，分为30类，每类测试一种视觉感知能力。随后，作者利用目前最先进的MLLMs（如GPT-4o、Gemini、Claude）以及大型推理模型（如OpenAI o4-mini和DeepSeek R1）进行性能测试。

Result: 随着任务复杂度提升，所有模型准确率显著下降。在不同感知类别上，模型表现出类似的趋势，并发现某些认知技能对于模型来说更难。

Conclusion: 现有多模态大模型在简单感知任务、尤其复杂度增加时表现有限，暴露其认知能力的不足。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [18] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文系统梳理了科学大型语言模型（Sci-LLMs）与其数据基础共同演化的现状与挑战，提出统一数据分类和知识层次模型，深度评析模型、数据集与评测方式的变化，讨论数据开发及自动化解决方案，规划了未来以Sci-LLMs为核心的闭环科学发现模式，为科研AI系统发展提供指南。


<details>
  <summary>Details</summary>
Motivation: 随着科学大型语言模型（Sci-LLMs）在科学研究中日益发挥重要作用，迫切需要对其数据基础和模型发展进行全面梳理，解决科学数据复杂性带来的挑战，推动更高效的知识集成与应用。

Method: 提出科学数据的统一分类法和科学知识的层次模型，系统回顾Sci-LLMs的最新进展，分析270余个预训练/后训练数据集以及190余个评测数据集，评估评测趋势，并探讨数据开发中的自动化与专家验证解决方案。

Result: 深度揭示科学语料库与一般NLP数据集的差异，强调异质性、多尺度及不确定性挑战，总结Sci-LLMs在表示和跨模态推理要求方面的独特性，并展示评测由静态考试向过程-发现导向转变。提出数据开发中的持续性问题与新兴解决办法，并展望由Sci-LLMs驱动的自动化闭环科学发现系统。

Conclusion: 本综述为构建可信、可持续进化的AI科研助手提供了数据驱动的理论框架与实施路径，对加速科学发现具有重要指导意义。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [19] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: LLMs在互评和自评时，受模型标签影响显著。Claude标签得分普遍更高，Gemini标签更低，甚至能颠覆真实结果。建议采用盲评或多模型办法以确保评测更公平公正。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常被用于文本评价，但其判断可能受偏见影响。了解LLM自评与互评时的偏见状况，对于确保模型评测的公正性具有重要意义。

Method: 通过ChatGPT、Gemini和Claude三大模型，在四种不同标签条件下（无标签、真实标签、两种错误标签）相互对白结果进行整体偏好投票和在连贯性、信息量、简洁性三个维度的质量评分（均用百分比表示），分析标签影响。

Result: 结果显示明显的不对称性："Claude"标签始终提升得分，"Gemini"标签则持续拉低得分，无关内容本身。错误标签可导致评分和排名发生大幅逆转，投票偏好最多变动超50个百分点，质量评分最高变动12个百分点。Gemini自评分在真实标签下显著下降，Claude自偏好则明显增强。

Conclusion: 模型身份标签本身对高层次判断和细致质量评分都能造成明显和微妙的扭曲，强调了盲评或多模型评测协议的重要性，以保障评测的公正性。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [20] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 作者提出基于贝叶斯实验设计的BED-LLM方法，使LLM在多轮对话与信息主动获取中表现更佳，在多项测试中取得了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多轮信息获取和外部环境交互中存在智能性与自适应性不足，需有框架提升其主动交互能力。

Method: 基于序贯贝叶斯实验设计（BED），迭代性地选择最大化期望信息增益（EIG）的查询，用LLM的概率分布来实现，并引入了专门的EIG估算器、历史回答的更优条件化方式、候选问题的合理生成策略。

Result: 在多种多轮交互测试（如20问游戏、主动用户偏好推断）上显著优于直接提示LLM及其他自适应设计方法。

Conclusion: 提出的BED-LLM方法在多轮交互（如20问游戏和主动推断用户偏好）中较现有方法有明显性能提升。

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [21] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 本文提出利用强化学习优化领域小模型，实现自动化、准确的人因航空安全分析，性能超越主流大模型，并适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统HFACS系统用于航空事故人因分析时，存在规模化和一致性不足的问题，难以实现高效、自动化的安全分析。

Method: 采用强化学习中的团体相对策略优化（GRPO）方法，对Llama-3.1 8B语言模型进行微调。引入多组件奖励系统，并结合合成数据生成以处理事故数据集中的类别不平衡问题。

Result: 所提出的GRPO微调模型在准确率方面有显著提升：准确匹配率提升了350%（从0.0400到0.1800），部分匹配准确率达到0.8800，在关键指标上优于GPT-5-mini和Gemini-2.5-fiash等先进大模型。

Conclusion: 小型、领域优化的模型在航空安全分析中不仅能实现更高效的计算，还利于部署在资源受限的边缘设备，具备实际应用价值。提出了多标签HFACS分类问题下新的基准评价方法，有助于衡量语言模型的推理能力。

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [22] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 本文提出用像素表示（将单词渲染为图片）替代传统文本嵌入，显著提升了语言模型对多语言正字法攻击的鲁棒性，并在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型对正字法攻击（如用多语言字母替换字符）非常脆弱，这是因为分词器和嵌入过程中存在词汇表外的问题。

Method: 提出用将单词渲染为独立图像的像素级表示替换传统的文本嵌入，并构建了一种像素级生成式语言模型；在多语言 LAMBADA、WMT24 和 SST-2 数据集上进行实验评估。

Result: 所提出方法在面对正字法噪声时表现出较强的鲁棒性，并且在多语言环境下依然有效。

Conclusion: 像素级生成式语言模型相比基于文本的嵌入方法，在应对正字法攻击和多语言文本方面展现了更强的鲁棒性。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [23] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本文发现自监督语音模型在语音习得并未体现类似人类的临界期效应，模型对语言暴露时间的敏感性与人类差异很大。


<details>
  <summary>Details</summary>
Motivation: 人类语言习得中的临界期效应（CP效应）被广泛讨论，但在自监督语音模型（S3M）中的体现尚不清楚。以往研究多聚焦于文本模型，缺乏对语音模型中这一现象的验证，而语音在实际语言习得中起核心作用。

Method: 研究通过训练自监督语音模型，模拟不同的第二语言（L2）训练起始时间和第一语言（L1）训练终止时间。具体使用面向儿童的语音输入，分析模型在不同时间点暴露下的训练表现，并通过音素判别任务进行评估。

Result: 自监督语音模型在语音习得的临界期效应方面并未展现出明显的证据。在模拟L2训练起始推迟时，模型在L2任务上表现更好；而推迟L1训练终止则导致L1遗忘。

Conclusion: S3M在语音习得的CP效应方面与人类表现不同，当前尚未观察到类人的CP敏感性。更晚暴露于L2有助于模型L2表现，而L1暴露终止延迟反而加剧遗忘，这与人类CP效应结论不符。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [24] [Constructive l2-Discrepancy Minimization with Additive Deviations](https://arxiv.org/abs/2508.21423)
*Kunal Dutta*

Main category: cs.DM

TL;DR: 本文提出一种新型多项式时间随机化算法，可以在高维单位范数向量集合上选取±1符号，使得前缀和的ℓ2范数最大值达到O(√d+log n)，首次在构造型意义上逼近最优理论界，并改进了Steinitz问题的相关边界。算法核心包含新的正交性约束与适合该问题的Hanson-Wright型不等式分析，对相关理论问题具有重要推进作用。


<details>
  <summary>Details</summary>
Motivation: 论文关注于ℓ2范数下的带符号序列（signed series）问题，核心动机是在高维空间给定单位范数的向量集合，探索是否总能选择±1符号使得前缀和的最大范数受控在一个较小的范围内。此前虽然存在较紧的存在性证明，但较优的构造型算法边界仍远逊色于存在性结果。该问题在凸几何、概率及理论计算机科学中均有重要意义，对Steinitz问题等也有直接推动作用。

Method: 作者提出了一种多项式时间的随机化算法，结合了Bansal与Garg提出的差异增量分析框架，并引入了新的技术：（1）在随机游走步长的协方差矩阵构造过程中，加入额外的线性及谱正交性约束，能更精确控制差异向量的线性及二次项变化；（2）提出适用于子高斯混沌变量筛选依赖和的Freedman型Hanson-Wright浓度不等式，为分析极大前缀和提供了更有力的工具。

Result: 主要结果为：该算法可以在多项式时间内给出一组±1符号，使得所有前缀和的ℓ2范数最大值为O(√d+log n)，与最优存在性界限仅有常数差距。同时，由Harvey和Samadi的构造性归约，该结果同样改进了ℓ2-Steinitz问题的构造型界限。当d≥log^2 n时，实质上解决了这两个长期公开问题的构造型猜想。

Conclusion: 该工作显著缩小了带符号序列问题及Steinitz问题ℓ2范数下存在性界与可计算界之间的差距，提出的新分析与技术为构造型不等式问题带来通用工具，并为凸几何、高维概率等领域的其他问题提供了新方案。

Abstract: The \emph{signed series} problem in the $\ell_2$ norm asks, given set of
vectors $v_1,\ldots,v_n\in \mathbf{R}^d$ having at most unit $\ell_2$ norm,
does there always exist a series $(\varepsilon_i)_{i\in [n]}$ of $\pm 1$ signs
such that for all $i\in [n]$, $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i
v_i\|_2 = O(\sqrt{d})$. A result of Banaszczyk [2012, \emph{Rand. Struct.
Alg.}] states that there exist signs $\varepsilon_i\in \{-1,1\},\; i\in [n]$
such that $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i v_i\|_2 =
O(\sqrt{d+\log n})$. The best constructive bound known so far is of
$O(\sqrt{d\log n})$, by Bansal and Garg [2017, \emph{STOC.}, 2019, \emph{SIAM
J. Comput.}]. We give a polynomial-time randomized algorithm to find signs
$x(i) \in \{-1,1\},\; i\in [n]$ such that \[ \max_{i\in [n]} \|\sum_{j=1}^i
x(i)v_i\|_2 = O(\sqrt{d + \log^2 n}) = O(\sqrt{d}+\log n).\] By the
constructive reduction of Harvey and Samadi [\emph{COLT}, 2014], this also
yields a constructive bound of $O(\sqrt{d}+\log n)$ for the Steinitz problem in
the $\ell_2$-norm. Thus, our result settles both conjectures when $d \geq
\log^2n$. Our algorithm is based on the framework on Bansal and Garg, together
with a new analysis involving $(i)$ additional linear and spectral
orthogonality constraints during the construction of the covariance matrix of
the random walk steps, which allow us to control the quadratic variation in the
linear as well as the quadratic components of the discrepancy increment vector,
alongwith $(ii)$ a ``Freedman-like" version of the Hanson-Wright concentration
inequality, for filtration-dependent sums of subgaussian chaoses.

</details>
