<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 54]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dependent Multiplicities in Dependent Linear Type Theory](https://arxiv.org/abs/2507.08759)
*Maximilian Doré*

Main category: cs.PL

TL;DR: 作者提出了一种新颖的依赖线性类型系统，使变量的使用次数可以依赖于其他变量，弥补了现有系统的不足，并在Agda中实现，理论和实际均具备创新性。


<details>
  <summary>Details</summary>
Motivation: 现有的线性类型系统无法精确地为部分高阶函数进行资源注解，特别是当变量的使用次数要依赖于其他变量时，主流系统难以处理。该论文意在解决高阶函数资源管理中的类型系统局限。

Method: 作者通过将线性逻辑嵌入依赖类型理论，结合Dialectica translation的启发，构建了一个新的依赖线性类型系统。该系统引入依赖性重数（multiplicities），并利用自然数类型来形成量化且依赖性的类型注解。此外，还设计了该系统在Agda上的实现。

Result: 提出的新系统能够支持依赖性资源注解，允许部分变量的重数依赖于其他变量，并可以推广到任何依赖类型语言。理论上，作者给出了依赖类型理论和线性逻辑模型的组合来刻画其语义，同时实际在Agda中实现了这一系统。

Conclusion: 论文提出的依赖线性类型系统填补了类型理论中对依赖性资源注解的空白，能够支持更复杂的高阶函数类型，用以提升资源分析与类型安全。该系统理论严谨且有实际实现，具备良好的推广前景。

Abstract: We present a novel dependent linear type theory in which the multiplicity of
some variable - i.e., the number of times the variable can be used in a program
- can depend on other variables. This allows us to give precise resource
annotations to many higher-order functions that cannot be adequately typed in
any other system. Inspired by the Dialectica translation, our typing discipline
is obtained by embedding linear logic into dependent type theory and specifying
how the embedded logic interacts with the host theory. We can then use a
standard natural numbers type to obtain a quantitative typing system with
dependent multiplicities. We characterise the semantics for our theory as a
combination of standard models of dependent type theory and linear logic. Our
system can be added to any dependently typed language, which we demonstrate
with an implementation in Agda.

</details>


### [2] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: 本文提出并研究了一类新颖的滤波等变函数，证明了其可实现可预测外推，并提出合并算法能基于子列表行为完美地外推出整体输出。


<details>
  <summary>Details</summary>
Motivation: 作者关注如何设计一种能够在已知输入/输出样本以外进行有效外推的函数，并认为「好的」外推函数应遵循某些规则，尤其是在函数输入经过元素移除操作后仍保持可预测性。

Method: 作者提出并研究了一类新的函数语义类别——filter equivariant functions（滤波等变函数），分析了其特点、给出了基本定理、并将其与已知的map equivariant functions类进行比较，还提供了几何方法将其与单纯结构联系起来。最重要的方法是提出了amalgamation algorithm（合并算法），用于根据输入的子列表推断整个列表的输出。

Result: 作者证明了filter equivariant functions类有趣且有价值，且能与数学上的simplicial structures自然对应，并给出了合并算法能够基于子列表的行为精确地外推整个列表的输出结果。

Conclusion: 提出了一类新的可预测外推的函数语义（滤波等变函数），并证明了其实用性和深层结构联系，特别是通过合并算法实现了完美的外推。

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: 本文通过对美国家实验室及大学计算科学家的问卷调查，揭示了聚变与裂变能源代码开发领域的最新趋势：模块化、现代语言取代FORTRAN、重视组织支持和预算投入。


<details>
  <summary>Details</summary>
Motivation: 软件是聚变和裂变能源工程设计的核心工具，工具的发展直接影响了工程的可实现性。作者想了解最前沿的软件工具使用现状及开发者体验，推动领域未来发展。

Method: 2024年首次对103名从事聚变和裂变能计算代码开发的科研人员进行了问卷调查。调查内容包括他们要解决的问题、可用工具及端到端开发体验，并对数据进行了分析。

Result: 调查发现，越来越多的科研人员倾向于使用现代编程语言、开源代码和模块化软件。FORTRAN在核能领域的使用正在减少，Python和C++等现代语言的采用率提升。越来越多的开发预算投入到相关代码中，部分机构单笔预算高达5000万美元。

Conclusion: 核能领域的软件开发呈现向模块化、现代化、组织更加重视的趋势。这些变化反映了核能工程行业未来5-10年即将到来的变革方向。

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [4] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 本研究首次系统性地比较了更自主的AI编码代理与传统协同助手在真实开发者中的效用。发现编码代理能更有效提升开发效率与完成更有挑战性的任务，但广泛应用还需克服行为理解等挑战，对未来开发工具设计具有启示意义。


<details>
  <summary>Details</summary>
Motivation: 目前面向开发者的AI工具越来越多且更具自主性，但对能够自动编写文件和运行代码的编码代理的实际使用研究仍较少，大多数依赖静态测试，缺少真实开发者参与的研究。该论文旨在填补这一空白，深入理解高级AI工具对开发者生产力和体验的影响。

Method: 本研究招募了定期使用GitHub Copilot的开发者，对比评估了两个主流编码助手：GitHub Copilot（协同助手）和OpenHands（具备代理能力的编码助手）。通过实际用户参与，观察并分析他们在使用两种工具时的工作流程、效率与体验差异。

Result: 结果显示，编码代理（如OpenHands）能够以超越传统协同助手（如Copilot）的方式协助开发者，例如完成开发者本身可能无法完成的任务，同时降低了用户的操作和时间成本。但是，全面推广编码代理面临着如用户对代理行为理解不足等挑战。

Conclusion: 编码代理在提升开发者生产力和降低工作负担方面有明显优势，但其采用需解决用户认知和行为理解等问题。研究同时为今后新型编码代理的研发提出了建议。

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [5] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: GenAI代码生成工具虽然提升了生产力，但它削弱了开发者对代码的理解，从而影响了依据代码知识建模的专长度量工具的准确性。这表明相关模型和算法未来需针对AI介入做出适应性改进。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）代码工具提升了开发效率，但也带来了开发者依赖和代码理解能力下降的担忧。作者猜测，这一效应可能会影响用来衡量开发者代码知识和专长的模型。

Method: 作者收集了ChatGPT生成代码融入GitHub项目的统计数据，并通过调整GenAI贡献程度模拟不同场景，分析知识模型和Truck Factor算法在这些情况下的变化。

Result: 实验发现，大多数情景下开发者专长度量指标受到了显著影响，显示出当前度量方法对GenAI工具敏感。

Conclusion: 随着GenAI工具更深入开发流程，现有开发者专长度量方法的可靠性可能下降，需关注度量模型的适应性和修正。

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [6] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 本研究评估了多种LLM自动标注提升用户反馈分类的能力，发现在大规模数据和有限人工标注条件下，有效利用LLM可提升下游模型表现，降低数据成本。


<details>
  <summary>Details</summary>
Motivation: 当前应用用户反馈的分类主要依赖于有监督机器学习算法，但高质量标注数据集的获得成本高、难度大，这限制了分类器的泛化能力。

Method: 评估四种先进大语言模型（GPT-3.5-Turbo、GPT-4、Flan-T5、Llama3-70b）对八个细致标注数据集（含应用商店评论、X平台帖子、论坛讨论）的用户反馈分类能力。通过实验分析LLMs在细粒度与粗粒度分类任务中的表现，并将其用于自动标注，进而扩充训练集，提升BERT类模型性能。

Result: （1）LLMs在精心设计Prompt指导下可有效完成粗粒度反馈分类任务；（2）融合LLMs共识标注生成的新数据后，BERT分类模型精度显著提升。

Conclusion: 有指导性的大语言模型不仅能够高效挖掘用户反馈信息，还能低成本扩充标注数据，显著增强传统分类器在用户反馈应用中的泛化和性能。

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [7] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文针对浮点程序误差检测的效率与准确性难题，提出通过注入扰动并比较结果的新方法PI-detector，并证实其在多个基准上性能优越。


<details>
  <summary>Details</summary>
Motivation: 浮点数误差在许多科学和工程应用中可能导致严重后果，但只有部分输入会触发显著误差，检测和定位这些误差具有实际意义。然而，现有高精度方法效率低或易产生误报。

Method: 提出PI-detector方法，通过在原子操作（如加减法）中注入微小扰动，并比较扰动前后的结果，来高效计算和检测浮点数误差。该方法基于浮点误差源于原子操作条件数大的观察。

Result: PI-detector经过ATOMU、HSED数据集及复杂线性系统求解程序验证，结果表明其能够高效且准确地检测浮点误差。

Conclusion: PI-detector方法有效兼顾了效率和准确性，成为检测浮点错误的一种新途径，解决了现有工具的主要不足。

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [8] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出了InferLog算法，通过优化LLM推理流程，有效提升了日志解析速度和并发能力，解决了生产环境中的延迟瓶颈，同时保持了高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前先进的LLM日志解析虽然准确，但实际部署在生产环境中面临隐私、延迟和吞吐量三大难题。尤其是本地部署下，LLM高推理延迟成为了瓶颈，现有优化多关注查询次数，忽略了调用本身的高延迟与并发处理能力不足的问题。

Method: 提出了InferLog方法，包括：1) 前缀感知的ICL例子精炼策略以提升前缀缓存效率；2) 基于元学习的快速且针对具体任务的配置调整流程，以寻找动态日志解析负载的最优LLM相关配置。

Result: 在Loghub数据集和vLLM上测试结果显示，InferLog显著优于现有推理优化方法，并大幅提升了LLM日志解析系统速度，且不牺牲准确率。

Conclusion: InferLog极大加快了基于LLM的日志解析系统推理过程，同时保持了解析精度，表明它在实际生产环境下更具应用价值。

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [9] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 本文提出并实证分析了用生成式AI通过prompt engineering自动生成proto-persona的方法。该方法提高了效率与质量，广受用户欢迎，但在泛化和领域适应性上仍有不足，对同理心的支持也有一定差异。为GenAI在软件产品发现中的应用提供了有价值的实证证据。


<details>
  <summary>Details</summary>
Motivation: 手动创建proto-persona（原型用户画像）既耗时，又容易出错，而且存在认知负担和偏见。因此，作者希望探索一种新的、更高效的proto-persona生成方式，以提升产品定义和利益相关者协同效率。

Method: 研究提出了一种基于提示工程（prompt engineering）结合生成式AI（GenAI）的方法，自动化生成proto-persona。通过在19名真实Lean Inception参与者中开展案例研究，结合定性和定量方法，对该方法的效率、有效性、用户接受度以及同理心维度进行了实证评估。

Result: 新方法显著节省了时间和精力，并提升了proto-persona在MVP范围确定与功能细化等后续发现阶段的质量和复用性。用户在感知有用性和易用性上的接受度高，但对泛化能力和领域特定性有所担忧。同理心方面，认知同理心得到支持，但情感和行为同理心因人而异。

Conclusion: 基于GenAI的proto-persona生成方法能有效嵌入软件产品发现流程，带来了效率提升和质量改善，但需进一步解决泛化和领域适应问题。该研究为混合型设计实践提供了实证参考，并提出了未来优化方向。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [10] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: 通过在LLM代码翻译任务中加入自然语言或AST中间表示，尤其是带自然语言总结的连锁推理提示，能显著提升翻译成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）生成的代码翻译结果存在错误，提高翻译准确性成为重要问题。中间表示（如自然语言总结或语法树）有望为模型理解提供结构化的帮助。

Method: 本文探索通过自然语言和抽象语法树（AST）作为中间表示，结合提示工程（如一步、一连串推理（CoT）等）引导LLM进行代码翻译。作者使用Open Gpt4 8X7B、StarCoder和CodeGen等模型，在CodeNet和AVATAR等主流数据集上进行实验。

Result: 结果表明，采用带有中间自然语言总结的CoT提示，在Open Gpt4 8X7B模型中，代码翻译的成功率分别比零样本提示提升了13.8%和6.7%。

Conclusion: 引入中间自然语言总结的连锁推理提示能显著提升大语言模型的代码翻译准确率，优于传统零样本和其他集成方式。

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [11] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: 该论文提出了基于大语言模型和多策略融合的自动注释更新框架LLMCup，通过多模型候选与自动排名，显著优于现有方法，部分效果甚至超越人工。


<details>
  <summary>Details</summary>
Motivation: 注释对于增强代码可读性和可维护性至关重要，但开发者往往只更新代码而忽视注释，导致文档过时或不一致，影响后续理解和维护。现有自动注释更新方法存在漏掉关键信息或处理复杂场景不足的问题。

Method: 提出了一种新的注释更新框架LLMCup。该方法先利用多种提示策略生成多样化的候选注释，然后通过排名模型CupRank选择最佳注释，采用了大型语言模型（LLM）提升更新效果。

Result: 实验结果表明，LLMCup在准度、BLEU-4、METEOR、F1及SentenceBert相似度等多项指标上较先进基线方法（CUP和HebCup）有明显提升，并且部分情况下超过人工注释，用户调研也支持该方法有效性。

Conclusion: LLMCup框架能显著提升自动注释更新的效果，在部分场景下甚至优于人工更新，为未来自动代码文档维护提供可靠方案。

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [12] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: 本文提出了DHDA在线学习框架，有效应对软件系统在动态环境下的全局和局部概念漂移，在八个系统中实现2倍准确率提升，同时计算开销可控。


<details>
  <summary>Details</summary>
Motivation: 现代可配置软件系统需要实时学习配置与性能之间的关系模型，但实际运行中面临工作负载变化、硬件变动及系统更新带来的概念漂移问题。传统的离线与迁移学习方法难以适应这些复杂而不可预测的环境变化，给性能学习带来挑战，因此需要新的应对方法。

Method: 本论文提出了DHDA（一种在线配置性能学习框架），能同时捕捉并适应全局及局部概念漂移。其主要思想是采用双重层次自适应机制：上层根据需要重新划分数据并重训局部模型，以处理全局漂移；下层各分区的局部模型可异步检测并适应局部漂移。DHDA结合增量更新与周期性全量重训，在无漂移时减少不必要的计算。

Result: 在八个软件系统上评估后，DHDA在精度和漂移适应上表现优异，对比现有技术方案，准确率提升显著（最高可达2倍），同时开销合理，对不同局部模型的概念漂移处理也有改进。

Conclusion: DHDA框架能在动态环境下有效捕捉与适应全局及局部概念漂移，准确率和自适应能力均优于现有方法，并拥有合理的计算开销。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Heterogeneous Dynamic Logic: Provability Modulo Program Theories](https://arxiv.org/abs/2507.08581)
*Samuel Teuber,Mattias Ulbrich,André Platzer,Bernhard Beckert*

Main category: cs.LO

TL;DR: 这篇论文提出了一个能组合多种程序逻辑进行复杂系统验证的新框架HDL，并在理论和实际汽车控制场景中展示了其可靠性和实用价值。


<details>
  <summary>Details</summary>
Motivation: 多编程语言系统的形式化规范和验证极具挑战，需要能够组合不同程序逻辑的方法。

Method: 提出Heterogeneous Dynamic Logic (HDL) 框架，用于以模块化、组合化的方式结合不同（动态）程序逻辑的推理原则。HDL 模仿 SMT 体系，将各自的动态逻辑及其演算视为可灵活组合的‘动态理论’。设计Lift和Combination两个关键算子，理论在Isabelle上形式化并证明其有效性和完备性。

Result: HDL 能够在单一模态下进行跨语言推理，并能复用现有证明基础结构。理论证明Lift及Combination不会增加推理难度。通过Java控制器和微分动态逻辑建模的物理系统的实际案例进行了验证。

Conclusion: HDL 成功实现了多语言系统性质的组合推理，其中Lift和Combination算子保证理论的模块化扩展能力和跨语言推理能力，并通过实际汽车案例验证其实用性和有效性。

Abstract: Formally specifying, let alone verifying, properties of systems involving
multiple programming languages is inherently challenging. We introduce
Heterogeneous Dynamic Logic (HDL), a framework for combining reasoning
principles from distinct (dynamic) program logics in a modular and
compositional way. HDL mirrors the architecture of satisfiability modulo
theories (SMT): Individual dynamic logics, along with their calculi, are
treated as dynamic theories that can be flexibly combined to reason about
heterogeneous systems whose components are verified using different program
logics. HDL provides two key operations: Lifting extends an individual dynamic
theory with new program constructs (e.g., the havoc operation or regular
programs) and automatically augments its calculus with sound reasoning
principles for the new constructs; and Combination enables cross-language
reasoning in a single modality via Heterogeneous Dynamic Theories, facilitating
the reuse of existing proof infrastructure. We formalize dynamic theories,
their lifting and combination in Isabelle, and prove the soundness of all proof
rules. We also prove relative completeness theorems for lifting and
combination: Under common assumptions, reasoning about lifted or combined
theories is no harder than reasoning about the constituent dynamic theories and
their common first-order structure (i.e., the "data theory"). We demonstrate
HDL's utility by verifying an automotive case study in which a Java controller
(formalized in Java dynamic logic) steers a plant model (formalized in
differential dynamic logic).

</details>


### [14] [A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes](https://arxiv.org/abs/2507.08701)
*Ricardo Contreras,Filip Smola,Nuša Farič,Jiawei Zheng,Jane Hillston,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: 本文提出一种融合传感器和社会学信息的数据驱动框架，实现对独居老年人日常活动的个性化建模与逻辑验证，可提升其安全性与生活质量。


<details>
  <summary>Details</summary>
Motivation: 随着老年人口增长，越来越多的老年人选择独立生活，对提升他们的生活质量提出了紧迫需求。个性化、以人为本并考虑个人偏好及情境的解决方案亟需探索。

Method: 提出一个关于独立生活老年人日常活动的建模和推理框架。该框架结合了传感器数据与半结构化访谈、家居布局、社会学观察等上下文信息，为每个参与者创建个性化的形式模型。用线性时序逻辑（LTL）形式化个体化需求，并通过模型检测器验证模型是否满足这些需求，若不满足则生成反例分析原因。

Result: 该框架能为不同参与者提供通用支持，展示了其提升居家老年人安全与福祉的潜力。能够个性化建模并及时发现不符合需求的情境。

Conclusion: 本文提出的框架能够有效集成多渠道数据，为独立居家的老年人提供个性化的生活质量保障方法，增强了通用性和适用性。

Abstract: There is an imperative need to provide quality of life to a growing
population of older adults living independently. Personalised solutions that
focus on the person and take into consideration their preferences and context
are key. In this work, we introduce a framework for representing and reasoning
about the Activities of Daily Living of older adults living independently at
home. The framework integrates data from sensors and contextual information
that aggregates semi-structured interviews, home layouts and sociological
observations from the participants. We use these data to create formal models,
personalised for each participant according to their preferences and context.
We formulate requirements that are specific to each individual as properties
encoded in Linear Temporal Logic and use a model checker to verify whether each
property is satisfied by the model. When a property is violated, a
counterexample is generated giving the cause of the violation. We demonstrate
the framework's generalisability by applying it to different participants,
highlighting its potential to enhance the safety and well-being of older adults
ageing in place.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 本文通过主成分分析挖掘并标注潜在可控特征，开发出一种改善Prompt文本语音模型可控性的微调方法，实验证明该方法在提升模型控制能力方面有效，特别是在无情感标签的场景下。


<details>
  <summary>Details</summary>
Motivation: 现有的Prompt-based文本到语音（Text-To-Speech, TTS）模型虽然允许用户通过自然语言指令控制如语速、性别等语音特征，但控制能力受限于训练时暴露给模型的声学特征，同时同样输入也会产生不受控的变异，影响生成结果的稳定性。

Method: 提出一种新的微调策略，通过对大量合成样本进行主成分分析（PCA），找出造成最大输出差异的潜在特征，并将这些特征作为新标签用于二次微调，提高模型对隐藏特征的可控性。

Result: 在用情感标签和不含情感标签的两种冰岛语表达性语音语料库训练的模型上评估该方法。对于没有情感标签的模型，方法带来了连续和离散的可控特征，有效提升了模型的整体可控性。

Conclusion: 利用PCA挖掘出的潜在特征进行微调，能丰富并增强TTS模型的语音属性控制能力，尤其是在缺少情感标注的情况下表现出较大改进。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [16] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: 本文提出MedicalBERT，在大规模医学语料和专属词表上预训练，显著提升医学NLP任务表现，超过现有主流医学BERT模型，平均提升达5.67%。验证了BERT预训练和迁移学习在医学领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型（如BERT、GPT等）在理解复杂文本方面表现优异，但在处理包含大量领域专有术语的生物医学文献时，效果有限。以往的Word2Vec、Bi-LSTM等模型更难以适配，现有BERT变体如BioBERT等也有提升空间。因此，有必要开发更能适应医学领域的NLP模型。

Method: 本文提出MedicalBERT，这是一个在大规模生物医学数据集上预训练、并包含领域专属词表的BERT模型。该模型针对实体识别、关系抽取、问答、句子相似度、文档分类等多项医学NLP任务，进行了优化和微调，并采用F1分数、准确率、Pearson相关系数等指标评估其性能。

Result: MedicalBERT在大多数医学NLP基准任务上超越了BioBERT、SciBERT、ClinicalBERT等已有BERT变体，在所有任务上的平均性能比通用BERT提升了5.67%。

Conclusion: MedicalBERT展现了在医学NLP任务中的优异表现，验证了预训练BERT模型及迁移学习技术在捕获医学领域专属信息上的有效性。该模型对医学文本的理解能力得到了提升，为生物医学NLP应用提供了更强大基础。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [17] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 大规模实证研究发现，LLM越狱攻击复杂性并未显著提升且具有自然极限，防御侧持续改进。越狱与防御不存在无限进化的军备赛，需警惕过度公开复杂攻击可能带来的安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）被广泛部署，理解越狱策略的复杂性与演变对于AI安全至关重要。当前普遍假设“攻防军备竞赛”不断升级，但缺乏大规模数据支撑。

Method: 本文对200多万条来自于各类平台（包括越狱社区与通用聊天机器人）的真实对话进行大规模实证分析。通过一系列复杂度指标（如概率度量、词汇多样性、压缩比率、认知负荷等）系统评估越狱尝试的复杂性，并进行时序演化分析。

Result: 1) 越狱尝试在大多数复杂度维度上，并不比普通对话复杂，且该模式在越狱社区与普通用户群体中均成立；2) 越狱攻击的毒性和复杂性随时间基本稳定，助手应答的毒性则明显下降，说明防御机制在提升；3) 复杂性分布不呈现幂律分布，暗示越狱发展的自然极限。

Conclusion: 越狱攻防并非无限升级的军备竞赛，越狱创新受人类创造力限制，而防御机制不断进步。更复杂的攻击方式大规模披露前需警惕信息安全隐患，否则可能打破当前均衡。

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [18] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: FinGPT在金融结构化NLP任务表现强劲，但在需要推理与生成的复杂任务上仍有待提升，距离全面金融NLP解决方案还有差距。


<details>
  <summary>Details</summary>
Motivation: 现有通用NLP模型在金融领域应用受限，开发并系统评估金融专用语言模型能为金融实际应用提供更有效工具。

Method: 将FinGPT应用于六项金融领域NLP任务（情感分析、文本分类、命名实体识别、金融问答、文本摘要、股价波动预测），并与GPT-4及人类基准进行对比评测，使用金融领域专用数据集。

Result: FinGPT在情感分析、分类等结构化任务中接近GPT-4表现，但在金融问答、文本摘要等需要推理和生成能力的任务上明显逊色于GPT-4及人类，数值准确性和复杂推理尤其存在短板。

Conclusion: FinGPT在特定的结构化金融NLP任务上表现优秀，但在涉及复杂推理和文本生成的任务上仍有较大差距，尚不能作为全面金融语言模型的解决方案。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [19] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 本文综述了LLMs内部运作机制的新发现，提出了三层机器理解框架，认为LLMs的理解机制与人类不同，未来更应关注其内部运作方式而非简单判定是否理解。


<details>
  <summary>Details</summary>
Motivation: 当前主流观点认为LLMs只是依赖表层统计规律，缺少‘真正’的理解。最近机制可解释性研究发现挑战了这一观点，促使作者想为此领域做一次系统性综述，并提出更具解释力的理论框架。

Method: 作者对机制可解释性（MI）领域的新发现进行了整合和再梳理，并提出了一个关于机器理解的三层新理论框架：概念理解、世界状态理解、原理理解。并通过实例详细阐述各层次的具体表现。

Result: 作者提出三层机器理解概念并整合现有MI研究，表明LLMs已发展出与人类理解类似但本质上不同的内部结构，且传统将注意力集中在“是否理解”已不合时宜，应关注LLMs内部机理。

Conclusion: 本文指出，虽然LLMs展现出一定的“理解”能力，但其认知结构与人类仍然存在显著差异，因此对LLMs的研究应从是否理解转向如何理解。

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [20] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: 本文提出了R3（Review, Remask, Refine）框架，通过过程奖励模型对生成文本进行智能评分与再掩码，无需额外训练，提升了扩散类文本生成模型的自我纠错和最终文本质量。


<details>
  <summary>Details</summary>
Motivation: 迭代文本生成模型常常难以高效地识别和纠正自身的生成错误。作者希望解决模型在生成的过程中自我纠错能力不足的问题。

Method: 提出了Review, Remask, Refine（R3）框架，无需额外训练，可直接用于任意预训练的掩码文本扩散模型。具体流程为：1）利用过程奖励模型（Process Reward Model, PRM）对中间生成文本块进行评估（Review）；2）根据PRM得分采用再掩码策略，对分数较低（较可能出错）的文本块掩码更多token（Remask）；3）模型聚焦于这些被再掩码的部分进行重生成优化（Refine）。

Result: R3方法能让模型更有针对性地修正不理想部分，从而提升最终生成文本的质量。该方法适用于多种扩散文本生成模型，经有效性验证。

Conclusion: R3框架为文本生成模型提供了一种无需额外训练即可高效自我纠错与改进的新思路，有助于提升生成文本的整体表现。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [21] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: 本文比较了三种主流LLM和人类招聘专家在自动筛选简历时的一致性与表现，结果发现LLM与人类评判存在显著差别，具体模式也不尽相同。尽管LLM可被引导形成一致性行为，但部署于自动化招聘时需谨慎考量其与人工判别的偏差。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）被用于简历筛选和职位匹配，但尚不清楚其行为是否一致可靠，以及其表现与人类招聘专家相比如何。

Method: 利用受控数据集，分别测试了三种LLM（Claude、GPT 和 Gemini）在不同的公司背景（无公司、跨国公司、新创公司、简化上下文）及不同简历设置下的筛选表现，并与三位招聘专家的表现进行了对比。采用方差分析和配对t检验等统计手段，分析了LLM和人类之间的差异。

Result: 方差分析显示，在八种仅LLM的条件中有四种存在显著均值差异，且LLM与人类专家的评估在各上下文中均有显著差异（p < 0.01）。配对t检验发现，GPT对公司背景适应性最强，Gemini次之，Claude最弱。所有LLM在各背景下与人类表现存在显著差异。元认知分析表明，LLMs的加权决策模式与人类评估方式明显不同。

Conclusion: LLM对详细提示可展现一定可解释性模式，但与人类招聘决策差异显著。提示自动化招聘系统部署时，需充分认识并权衡其与人类评判的区别。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [22] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 本论文提出MM-Coder，一种能融合文本与UML/流程图的多模态代码生成模型，并且发布了对应的数据集和评测基准。实验表明模型在复杂多模态理解与代码生成中有进步，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型（LLM）主要专注于文本输入，忽视了现实软件开发中常用的可视化工具如UML图和流程图。作者希望突破现有模型只处理纯文本的局限，将代码生成与视觉信息结合起来，以更贴近真实开发流程。

Method: 提出了MM-Coder，这是一种多语言多模态软件开发模型，能够结合文本和视觉设计（如UML图、流程图）进行代码生成，并提高代码与架构设计的一致性。同时，他们构建了MMc-Instruct多模态指令微调数据集，用于训练模型融合文本与图像信息。此外，还引入了MMEval多模态代码生成评测基准，以评测模型的综合能力。

Result: 通过MMEval评测，发现尽管MM-Coder有一定提升，但在准确捕捉视觉信息、指令理解和高级编程知识方面仍存在明显挑战。

Conclusion: MM-Coder为工业级编程自动化提供了新的可能性，使LLM能够理解并落实融合文本与视觉设计的复杂需求，但多模态代码生成距离实用仍需进一步突破。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [23] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: 本文提出了ETTA框架指出大语言模型在嵌入空间对抗攻击下存在较大安全漏洞，ETTA方法无需模型调整即可高效攻击，表明未来需关注嵌入级防护对策。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）广泛应用于医疗、教育和网络安全等领域，但其开放性带来了嵌入空间投毒等新的安全风险。这种攻击方式通过扰动输入数据的内部语义表示来规避模型的安全机制，而目前对此层面（嵌入级别）的对抗性攻击研究并不充分。

Method: 提出了ETTA（Embedding Transformation Toxicity Attenuation）框架，通过线性变换识别并削弱嵌入空间中对有害信息敏感的维度，无需模型微调或访问训练数据即可实施攻击，同时保持自然语言连贯性。

Result: 在五个主流开源大语言模型及AdvBench基准测试上，ETTA平均攻击成功率达到88.61%，比最优基线方法高出11.34%；对于安全增强版模型，同样表现良好（如指令微调防御模型上77.39%）。

Conclusion: 当前主流安全对齐策略在嵌入空间存在严重漏洞，ETTA框架能有效绕过现有防护措施，这提示需要发展针对嵌入空间的防御方法。

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [24] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文系统研究了大规模多模态模型在图片描述任务上的In-Context Learning，外部探索了示例配置策略，内部剖析了注意力机制及行为指标。揭示了示例配置和模型机制对性能的影响，并提出可推广的分析方法与新指标。


<details>
  <summary>Details</summary>
Motivation: 近年大模型发展过程中，模型在自然语言处理中展现了强大的In-Context Learning（ICL）能力。受大型语言模型（LLM）成功的启发，研究者开始在多模态模型（LMM）中引入ICL。然而，对于多模态ICL中的示例配置方式的系统研究较少，同时通过控制In-Context示例（ICEs）来分析推理行为也具有实际意义。因此，论文旨在填补多模态ICL配置和内部行为机制探索的研究空白。

Method: 论文针对图片描述任务，对多模态ICL进行外部与内部的系统性分析。外部方面，从示例数量、图像检索、文本说明分配三个维度，系统评估和总结不同示例配置方式对ICL效果的影响，并使用多种指标进行量化考察。内部方面，分析LMM的注意力机制特征，并设计基于注意力的新指标来量化模型行为；还进行了辅助实验，探索利用注意力驱动的模型加速和压缩，并比较了相同设计和预训练策略下不同模型的性能变化，并从预训练数据特征角度解释其差异。

Result: 研究发现，ICL示例（ICEs）的配置策略会显著影响LMM在图片描述任务中的表现，同时内部注意力分析揭示了模型推理的典型模式。基于注意力指标能够有效量化和分析LMM的行为。此外，研究还发现利用注意力特性可推动模型加速和压缩。与相同架构和预训练的大模型在性能上存在差异，这主要来源于预训练数据的不同特征。

Conclusion: 论文提出的外部（示例配置）与内部（机制分析）相结合的分析方法以及新设计的量化指标，为多模态ICL模型机理研究提供了双重视角，可拓展至更广泛的大模型行为分析领域。

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [25] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

TL;DR: 主流LLM普遍展现自由主义偏向，主要源自训练语料、人类反馈和主流伦理学框架。微调会增强这一趋势，这一现象反映出社会话语和价值的投射，不应简单归咎于程序员主观选择。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在伦理和政治回应方面普遍表现出自由主义倾向，但其背后原因及影响尚不明确。作者希望通过系统研究这些偏向的根本因素以及其对社会和学术讨论的意义。

Method: 本文系统性地考察了七种主流LLM，包括GPT-4o、Claude Sonnet 4、Perplexity（Sonar Large）、Gemini 2.5 Flash、Llama 4、Mistral 7b Le Chat和DeepSeek R1。采用了多种方法：道德基础理论（Moral Foundations Theory）、多种政治意识形态量表，以及新的政治争议指数。还比较了基础模型和微调模型，分析其政治取向变化。

Result: 大部分模型在伦理和政治问题上明显表现出自由主义价值观（尤其是关怀和公平）。造成这一倾向的因素有：训练语料偏自由、基于人类反馈的强化学习（RLHF）、学术伦理话语中自由框架占主导、以及安全导向的微调。微调比基础模型更加强化了自由倾向。

Conclusion: LLMs的自由主义倾向是一种受训练数据和社会话语影响下的涌现现象，而非程序员个人偏好或编码错误。这种倾向为理解集体推理和民主话语提供了新视角，也警示我们不要简单将政治偏差与知识分歧混为一谈。

Abstract: Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [26] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

TL;DR: 本论文通过大型随机实验证明，引入AI结构化视频面试能提升招聘成功率及候选人后续就业机会，但AI偏向选择年轻且经验较少的候选人，并改变了人才选拔机制。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在招聘领域应用日益广泛，但其对招聘效率与人才选拔结果的实际影响仍缺乏定量实证证据。因此，需要实地检验AI招聘流程相较传统流程的有效性及其对候选人群体特征的影响。

Method: 采用随机分配实验法，将3.7万名求职者分配到传统招聘流程或引入AI初步筛选的视频面试流程。两组共同进入最终人工面试，面试官对筛选方式不知情。后续通过LinkedIn数据跟踪求职结果，并分析AI生成的面试文本内容。

Result: 在AI流程中，最终面试通过率为54%，传统流程为34%，AI提升了20个百分点。5个月后，AI组有23%找到新工作，传统组为18%。AI筛选出年纪更轻、经验与高级资历更少的人选。论文也分析了AI筛选的标准和对话过程。

Conclusion: 引入AI辅助的招聘流程可以提高通过最终面试的候选人比例，并略微提升求职者后续找到新工作的可能性，但AI更倾向于选拔年轻、经验较少、资历较低的候选人。AI对招聘决策有重要影响，同时其选拔标准与传统方式存在区别，需关注潜在影响。

Abstract: Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [27] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 近年来，医学领域生成式AI的免责声明比例急剧下降，已濒临消失。当前模型亟需强化免责声明机制以防用户误用AI结果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（包括大语言模型LLMs和视觉-语言模型VLMs）在医学图像解读和临床问题答复中的应用增多，这些模型输出常常包含不准确的信息，因此研究关注其安全保障——即是否包含医学免责声明，尤为重要。

Method: 本研究评估了2022至2025年间，不同代际的LLM和VLM在输出中包含免责声明的情况。样本包括500张乳腺X光片、500张胸部X光片、500张皮肤科图像和500个医学问题，对生成输出中的免责声明短语进行筛查和统计。

Result: 研究发现，2022年LLM/VLM输出中免责声明的出现率为26.3%/19.6%，而到2025年分别降至0.97%/1.05%。到2025年，大多数模型在输出中已不再包含免责声明。

Conclusion: 随着公有AI模型能力和权威性的增强，模型输出缺乏免责声明可能带来安全隐患。研究建议必须根据临床语境为AI输出动态地添加免责声明，以保障用户安全。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [28] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

TL;DR: 本文系统对比了小型和大型语言模型在心理健康分类任务上的理解能力。结果显示，SLMs与LLMs在大部分二分类任务中性能接近，特别是在敏感场景下，SLMs凭借隐私和高效特点，有望支撑可扩展的心理健康筛查应用。


<details>
  <summary>Details</summary>
Motivation: 随着SLMs的兴起，为敏感场景如心理健康应用中的隐私保护提供了新选择，但SLMs是否能理解复杂心理线索、其能力与LLMs有何差距尚不明确。本研究旨在系统比较SLMs与LLMs在心理健康理解上的实际能力。

Method: 系统评估5个先进的SLMs（Phi-3、Phi-3.5、Qwen2.5、Llama-3.2、Gemma2）和3个LLMs（GPT-4、FLAN-T5-XXL、Alpaca-7B）在6项心理健康理解任务上的表现，采用零样本学习和少样本学习进行横向对比，衡量各自优势与不足。

Result: SLMs在心理健康二分类任务上的平均F1分数（0.64）几乎等同于LLMs（0.66，零样本设置下），且SLMs在少样本设置下提升显著（最高增幅14.6%）。两类模型在多分类严重程度任务上均有超30%的性能下降，说明高阶临床理解难以依靠模型体量单独突破。

Conclusion: 小型语言模型（SLMs）在心理健康理解任务中表现出色，其二分类任务的平均表现仅比大型语言模型（LLMs）低2%，显示其具有较高的应用潜力，尤其是在隐私保护和轻量级部署场景中。SLMs凭借少量示例即可显著提升表现，是敏感数据分析和心理健康筛查的有力工具。

Abstract: The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [29] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 本文提出了一种让LLM通过外部API等工具获取信息和计算能力的新框架，在数学和科学推理等任务上准确率大幅超过现有主流模型，为LLM在实际复杂应用场景中的提升提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在缺乏相关背景信息时，容易生成质量较差甚至虚假的回答。为提升LLM的准确性，亟需探索与外部工具集成的方法。

Method: 提出了一种LLM与外部工具（如API、计算器、日历等）集成的通用框架，能动态访问外部API获取补充信息和计算能力，提高LLM在教育领域答疑能力。以MMLU数据集（涵盖数学和科学推理问题）为评测基准进行实验。

Result: 该框架在数学推理任务上达到了83%的准确率，在科学推理任务上达到88%，显著优于GPT-4o、LLaMA-Large、Mistral-Large、Phi-Large和GPT-3.5等现有模型。对比最佳基线（LLaMA-Large）的67%和79%，提升明显。

Conclusion: 与外部工具的集成能显著提升LLM在复杂问题和特定领域下的问答表现，为打造以LLM为核心的复杂计算生态系统提供了有效途径。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [30] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

TL;DR: 该综述发现MedVQA系统当前在临床真实应用中存在数据内容、评价指标、功能实现等多方面的不匹配，未来需增强临床相关性、引入多模态与上下文、发展交互式系统，才能更好地被临床医生采纳。


<details>
  <summary>Details</summary>
Motivation: MedVQA有潜力辅助放射科医生自动解读医学影像，但在临床工作流中的实际应用受限。作者希望系统性地梳理该领域进展及临床需求之间的差异。

Method: 采用Arksey和O'Malley的范围综述方法，分为两步：1）系统回顾2018-2024年68篇相关文献，总结关键概念、进展和研究空白；2）对印度和泰国的50名临床医生进行问卷，收集其对MedVQA临床应用相关性的看法。

Result: 研究发现近60%的问答对不具临床相关性，大部分数据集/模型不支持多视图、多分辨率影像、电子健康记录（EHR）整合和领域知识。评价指标与临床需求脱节。问卷显示仅29.8%的医生认为该系统很有用，主要关注点是缺少病史/领域知识（87.2%）、倾向人工整理数据集（51.1%）、需要多视图支持（78.7%）；66%希望模型聚焦具体解剖区域，89.4%偏好对话交互式系统。

Conclusion: 当前MedVQA在多模态分析、患者上下文、评测方法等方面与实际临床需求存在明显差距，需针对这些痛点优化系统，实现高效临床应用。

Abstract: Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [31] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: 提出CRISP数据集用于高层次推理计划，微调小模型后推理效果优于大模型少样本提示和CoT，且具跨领域泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在解决复杂问题时的推理能力仍有不足。传统的Chain-of-Thought（CoT）推理方法虽然有进步，但在许多领域仍然不够强大。因此，提升推理能力成为研究重点。

Method: 作者提出了一个新的多领域数据集CRISP（Complex Reasoning with Interpretable Step-based Plans），该数据集包含数学推理和代码生成领域的高层次显式计划。CRISP中的计划由模型自动生成，并通过LLM作为判别器进行内在验证，以及通过实际下游任务表现进行外部验证。随后，通过对小模型进行CRISP微调，评估其计划生成能力。

Result: 对小模型进行CRISP数据集微调后，其生成的高层次推理计划质量优于使用少样本提示的大模型，且在推理能力上明显超越了CoT方法。此外，领域外评测显示，单一领域微调同样能提升其他领域的计划生成能力，说明该能力具有一定的泛化性。

Conclusion: 高层次计划生成作为显式推理方法，相比少样本提示和CoT推理更为有效。通过CRISP微调的小模型可以生成更优质且更具泛化性的推理计划。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [32] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: 本文提出AblationBench基准，系统评测语言模型自动体在AI消融实验规划上的能力。测试结果显示，现有模型表现有限（最佳仅识别29%的消融），链式思考方法优于智能体方法，该基准为推进AI科研自动化提供重要工具。


<details>
  <summary>Details</summary>
Motivation: 基于语言模型的自主代理在科研等领域日益流行，AI共科学家希望借此辅助或自动化研究过程。经验性AI研究中关键一环是消融实验设计，但该方向缺乏标准化评测工具。本文旨在填补该空白。

Method: 提出了AblationBench基准套件，用于评估在经验性AI研究中，自动体（基于语言模型的代理）进行消融实验规划的能力。AblationBench包含两项任务：AuthorAblation（基于方法区段建议消融实验，共83例）和ReviewerAblation（找出论文中缺失的消融实验，共350例）。两项任务均由基于语言模型的自动评价方法判定结果。

Result: 前沿语言模型在这两项任务上表现有限，最佳系统平均仅能识别原始消融实验的29%。

Conclusion: 当前语言模型在AI消融实验设计与发现方面仍具挑战性，其中链式思考（chain-of-thought prompting）方法优于现有智能体范式。而AblationBench为该方向研究提供了标准化测评基准。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [33] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: Krul系统提出动态、可定制的KV缓存压缩与恢复机制，较现有方法显著提升恢复效率和存储性能，且无质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多轮对话中KV缓存恢复时普遍采用固定压缩策略，忽视了不同对话在注意力模式相似性上的动态变化，导致准确度下降。

Method: Krul动态选择压缩策略、引入异构注意力相似性估算以及无“bubble”调度，从而高效恢复KV缓存。

Result: 实证评测表明，Krul在真是任务中可将首次token生成时间缩短1.5-2.68倍，KV缓存存储需求降低1.33-2.35倍。生成质量保持不变。

Conclusion: Krul系统能够在不牺牲生成质量的前提下，实现KV缓存恢复的高效与准确。

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [34] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 本文提出了一种无需微调的大语言模型方法，将自然语言转为SPARQL查询并探索RDF知识图谱，在多项基准测试中取得优异成绩，具有通用性强、实用性高的优点。


<details>
  <summary>Details</summary>
Motivation: 当前利用自然语言或关键词查询RDF知识图谱，需要将自然语言转为SPARQL查询，现有方法大多依赖于针对特定数据集的模型微调，通用性和可维护性有限。作者希望提出无需微调的大模型方法，以提升通用性和便捷性。

Method: 提出了一种基于大语言模型（LLM）的查询生成方法，不需要模型微调。该方法通过让LLM探索知识图谱，策略性地执行SPARQL查询，逐步搜索和定位相关的实体（IRIs）和属性（literals），最终生成目标SPARQL查询。

Result: 该方法在Wikidata多个基准测试中，在零样本设置下达到现有最高水平（SOTA）；在Freebase上接近最佳少样本方法；在其它知识图谱和基准上也表现良好。此外，作者还分析了不同的图搜索策略、反馈机制和少样本示例对性能的影响。

Conclusion: 作者证明了无需微调的LLM方法可以有效地从自然语言生成高质量的SPARQL查询，在多个主流和冷门知识图谱上均有较强表现，其灵活性和通用性优于许多现有针对性较强的方法。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [35] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

TL;DR: 提出了一种可透明、可审计的LM子程序框架，并用在人类专家逐步优化的公众意见处理，结果显示在减少风险和提升实际可用性方面较为有效。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LMs）虽然极具潜力加速文本处理相关任务，但在实际应用中，人们担忧其安全性、可解释性和偏见等问题，阻碍了其推广应用。该研究希望解决如何以透明、可审计且风险最小化的方式使用语言模型的问题，让人类专家专注于决策而非数据处理或提示工程。

Method: 作者提出一个框架，可以声明静态类型的、由LM驱动的子程序（可调用函数）并集成到常规异步代码中。该框架支持人类专家在线稀疏反馈以持续提升子程序表现，且会记录所有LM产生的工件（如提示词、输入、输出和数据依赖），供随时审计。框架被打包为一个库，便于广泛应用和开发。

Result: 该框架在公众意见处理场景（依据1969年《国家环境保护法案》要求）进行了实证：利用所开发的CommentNEPA应用，完成公众评论的整理、分类和摘要。实验通过将无人工反馈时系统输出与人为“真实”标签数据进行对比，进行了定量评估。

Conclusion: 该研究表明，借助该框架能以可透明、可审计的方式将大型语言模型集成到决策流程中，同时可通过人类专家反馈不断优化，具备广泛的实际应用潜力。

Abstract: The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [36] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 该文提出Compactor，一种无需参数且与查询无关的KV缓存压缩方法，通过低开销压缩机制在广泛任务和模型上能在保证性能的同时平均减少63%的KV内存消耗，有效缓解了大模型部署的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）逐渐支持更大的上下文窗口以提高能力，但长上下文会导致KV缓存的内存需求线性增加，成为实际部署中的主要瓶颈，限制吞吐率并增加服务成本。如何有效压缩KV缓存、降低内存消耗，是提升大模型实际部署效率的核心问题。

Method: 提出了Compactor，一种无需额外参数、与查询无关（query-agnostic）的KV缓存压缩方法，利用近似杠杆分数来评估token的重要性以进行压缩。同时，提出了“context-calibrated compression”，即根据具体上下文推断最大的可支持压缩比。

Result: Compactor在合成和真实任务上，在保留1/2 token情况下，性能可与现有方法持平，且计算开销极小。使用context-calibrated compression，模型在Longbench任务上实现了与原KV缓存性能一致的结果，同时平均减少了63%的KV内存消耗。Compactor还在多种数据集和不同模型（Qwen 2.5和Llama 3.1）上证实了其广泛适用性。

Conclusion: Compactor是一种通用、参数无关且高效的KV缓存压缩方法，在保证模型性能的同时，大幅减少了KV的内存消耗，显著提升了大模型在实际部署中的效率和可扩展性。

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [37] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种将大模型共情能力有效蒸馏到小模型的两步微调方法，创新设计目标性共情增强提示，实验表明新方法大幅提升了小模型的共情表现。


<details>
  <summary>Details</summary>
Motivation: 在资源受限但频繁与人类交互的场景（如智能手机）中，小型语言模型的应用需求增加，确保其具备大模型中已经具备的共情能力，成为模型蒸馏中的一个重要研究任务。

Method: 提出了一套包括两步微调流程的共情蒸馏方法：首先利用从LLM中蒸馏得到的共情对话数据进行微调，其次通过设计四组有针对性的共情提升提示，进一步提升共情能力。与传统的直接提示方法等蒸馏手段进行了对比分析。

Result: 实验结果显示，通过两步微调且数据经过目标性共情提升提示增强后，SLM在共情回应生成任务中的胜率提升至90%，目标性共情提示相较于基础直接提示提升了10%的胜率。

Conclusion: 所提出的目标性共情蒸馏方法显著提升了小模型（SLMs）在生成共情回应方面的表现，使其能更好地保留大语言模型（LLMs）的共情能力。

Abstract: The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [38] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文提出了TruthTorchLM，一个集成30余种方法的开源真实性预测库，兼容主流模型接口且便于扩展，在多个数据集评测中表现优秀，可有效支持大语言模型输出真假的研究与应用。


<details>
  <summary>Details</summary>
Motivation: 大语言生成模型经常会产生不真实的回答，尤其在高风险场景下，准确判断输出的真实性变得非常重要。目前用于真实性预测的工具存在功能单一或方法有限的问题，需要一种更全面、多样的工具包以推动相关研究。

Method: 提出并实现了TruthTorchLM，一个开源的Python库，集成了30多种真实性预测方法，涵盖不同计算开销、模型访问级别（黑盒/白盒）、依赖文档与否及监督方式（自监督/有监督）。该库兼容HuggingFace和LiteLLM，支持本地和API模型，且易于扩展和集成新方法。

Result: 通过在TriviaQA、GSM8K和FactScore-Bio三个数据集上对代表性方法进行了评测，展示了TruthTorchLM的多样性和有效性，验证了其在多场景下的适用性和性能。

Conclusion: TruthTorchLM为大模型输出真实性预测提供了丰富、可扩展的工具，弥补了现有工具单一和方法受限的不足，为学术和应用领域的相关研究提供了重要支撑。

Abstract: Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [39] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: LLMs在fine-tuning后的泛化能力，部分可由LoRA引入的“引导向量”解释，这种约束在许多任务中产生了超出上下文的推理能力。理解这一机制有助于设计更安全可靠的LLM微调方案。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）在fine-tuning（微调）过程中展现出超出分布的泛化能力，即所谓的“跳出上下文推理（OOCR）”。但目前对于这类能力如何产生、模型内是怎么实现的，机制层面仍不清楚。作者希望通过揭示OOCR现象背后的机制，更好理解LLMs为何能在复杂环境下安全、可靠地应用。

Method: 作者通过对既有文献中出现的OOCR现象进行机理剖析，重点考察LoRA（Low-Rank Adaptation）微调方式的作用。实验对比了微调导致模型表现变化，并直接训练“steering vector（引导向量）”以观察其带来的效果。还在涉及条件行为的复杂任务（如model backdoors）上进行了实验验证。

Result: 作者发现，许多OOCR现象本质上可以归结为：LoRA微调实际上是在模型语义空间中添加了一个常量引导向量，使模型泛化至相关概念领域。这种做法不仅提升了微调任务表现，也导致了意外且广泛的泛化现象。即便在需要条件性行为的任务上，无条件添加steering vector效果同样明显。作者还表明，源自任务直接训练的steering vector同样会诱发OOCR。

Conclusion: 本文提出，LLMs在fine-tuning中对OOCR能力的习得，重要一环是“steering vector”的引入而非对因果关系的深刻理解。这一发现为理解LLMs泛化与安全部署提供了直接解释，有助于进一步优化fine-tuning策略。

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [40] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 目前高级大语言模型虽可作为教育系统的代理学生，但与真实学生的行为在不同学科和年级难以全面对齐，因此需要改进模型训练和评估方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）越来越多地被用作智能教育系统（ITS）中的代理学生或用于测试题目的预先试点。然而，目前尚不清楚这些代理学生是否真实地反映了真实学生的行为与特征。

Method: 收集了包含数学和阅读理解的国家教育进步评估（NAEP）中四、八、十二年级的489道题目数据。通过项目反应理论（IRT）模型，将11个不同的高级大型语言模型与真实学生群体放在同一能力尺度上进行比较。

Result: 强大的通用模型在没有特定指引下，在每个年级均明显优于平均学生。弱模型或不匹配领域的模型表现有时偶然接近学生群体。即便使用年级提示来引导，模型的对齐效果仍然因模型与提示内容而有很大不同，并未有哪个模型-提示组合能跨越全部科目和年级实现真正的对齐。

Conclusion: 目前没有任何一种通用模型加提示方式能在所有学科与年级准确模拟平均学生，表明需要新的训练和评估策略。作者最后给出选择可行代理模型的指导建议。

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [41] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 本研究用NLP和卷积神经网络分析了社交媒体上的疼痛表达，发现性别在词语使用、疾病表现及药效体验上均有差异，为个性化治疗提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 早期疼痛研究常忽略性别因素。而疼痛体验因个体和性别差异而异，深入研究性别差异有助于理解疼痛本质及改善治疗方案。

Method: 采用自然语言处理（NLP）技术和隐含属性模型-卷积神经网络（HAM-CNN）对社交媒体痛苦相关帖子进行性别分类，通过用户名聚合，实现了0.86的F1得分，并对语料库进行深入分析。

Result: 成功将社交媒体中的帖子依据性别分为男性和女性语料库，揭示了两性在疼痛表述和常见病种上的差异，以及药物在不同性别间的不同效果。

Conclusion: 研究发现，男女在表达疼痛时存在显著的语言差异，女性更倾向于情感化表达。同时，女性更容易患有偏头痛和鼻窦炎等疾病，且止痛药对不同性别的影响也有所不同。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [42] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

TL;DR: 本文提出的KAT大模型采用自动切换推理与非推理模式的新方法，能够减轻过度思考，提升推理效率和准确性。在学术和实际场景均超越了多个现有大模型，还展现了很好的扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在需要推理的任务中，经常会出现过度思考（overthinking）问题，导致效率低下和无效token浪费。因此，本文提出了新的自动思考训练范式，旨在根据任务复杂度动态调整推理与非推理模式，以提升效率和准确性。

Method: 1. 构建一种新颖的双模式数据集，结合标签管道及多智能体合成策略；2. 采用多token预测知识蒸馏方法，实现高效细粒度的推理迁移；3. 推行冷启动初始化策略，通过投票信号和意图感知提示引入模式选择先验；4. 提出Step-SRPO强化学习算法，将中间监督加入到GRPO框架中，从而更好地指导推理模式选择和答案准确性。

Result: KAT模型在多个推理密集型基准测试中表现优异，相较现有最优模型（如DeepSeek-R1-0528和Qwen3-235B-A22B）有持平或超越表现，并在推理场景下token消耗量减少约30%。KAT已落地快手内部编码助手Kwaipilot，显著提升了实际开发流程的准确性与效率，并实现可控推理。进一步，团队正训练200B参数的MoE模型，初期结果已展现性能和效率的提升，验证了AutoThink范式的可扩展性。

Conclusion: KAT通过自动思考范式，有效缓解了推理任务中的过度思考问题，实现了推理效率与准确性的同步提升，并证明了此范式对大模型进一步扩展的可行性和实用性。

Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [43] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 本文提出了同步自我审查（SSR）微调方法，通过让模型先做OCR再翻译，有效防止了OCR能力的遗忘，同时提升了DIMT表现。实验验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLM）在文档图像任务上表现优秀，尤其在光学字符识别（OCR）方面，但在文档图像机器翻译（DIMT）上存在困难。常规的监督微调（SFT）会导致模型原有OCR能力的遗忘，如何在提升DIMT能力的同时保持OCR能力成为关键问题。

Method: 提出了一种全新的同步自我审查（SSR）微调范式。该方法在生成翻译文本前，先提示模型生成OCR文本，使模型可以利用其强大的单语OCR能力，逐步学习跨语言翻译任务。

Result: 大量实验表明，SSR方法有助于减轻模型灾难性遗忘问题，提升模型在OCR和DIMT任务上的泛化能力。

Conclusion: SSR微调范式有效缓解了以往方法中模型能力遗忘的问题，实现了OCR与DIMT任务性能的双提升。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [44] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

TL;DR: 本文提出了CRMAgent系统，通过大模型智能生成并优化CRM营销消息，实现了对商家原有模板的全面提升，显著改善了受众匹配与营销成效。


<details>
  <summary>Details</summary>
Motivation: 大多数电商商家缺乏撰写高质量、说服力强的CRM消息的专业知识和可扩展工具，导致CRM效果有限，无法有效促进客户留存和转化。

Method: 提出了CRMAgent系统，基于大语言模型，通过三种模式自动生成高质量的消息模板和写作指导：1) 分组学习，基于商家自身高表现消息进行学习和改写低表现消息；2) 检索与适配，从相似受众、券种和品类的历史高效模板中学习并适配当前场景；3) 规则式兜底，在无合适参考时进行零样本重写。

Result: 大量实验表明，CRMAgent生成的消息模板在受众匹配度和营销效果上显著优于商家原有模板。

Conclusion: CRMAgent系统利用多代理和大模型，有效提升了CRM消息的生成质量和营销效果，为缺乏写作能力和工具的电商商家提供了有力支持。

Abstract: In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [45] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

TL;DR: 本文提出了一种通过多模型协作和迭代提示工程驱动的专利创意自动生成方法MK2，在多个测试中表现优异，且无需额外训练数据，显示出此类轻量级方法的商业价值。


<details>
  <summary>Details</summary>
Motivation: 现有将专利转化为具备短期可行性的产品创意的自动化方法存在挑战，需要无需额外训练数据也能进行高质量创意生成的新方法。

Method: 提出MK2方法，以提示工程为核心，由Gemini 2.5模型起草并迭代修改提示词，将弱输出的有用片段合并进来，再由GPT-4.1利用这一提示为每项专利生成产品创意，最后通过Qwen3-8B进行Elo环判断选出最佳提示。不涉及额外训练数据。

Result: 在三个领域、两类评判主体和六项指标下，MK2在自动排行榜上排名第一，并在36项测试中赢得了25项。材料化学领域表现较弱，显示出该领域需要更深入的领域知识支撑。

Conclusion: 轻量级的提示工程（prompt engineering）已能实现具备商业相关性的专利创意生成，且无需额外训练数据。除部分专业性较强的领域外，该方案已具备较强的竞争力。

Abstract: The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [46] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 系统比较了对比学习和知识蒸馏两种文本重排序训练策略。结论是：有强大教师模型时知识蒸馏效果更佳，否则采用对比学习。


<details>
  <summary>Details</summary>
Motivation: 训练文本重新排序模型对于信息检索任务非常关键。目前主要有两种方法：对比学习和知识蒸馏。但在实际条件下这两种方法在训练交叉编码器重排序模型上的对比效果尚缺乏系统研究。

Method: 作者在相同的数据集上，对不同规模和架构的重排序模型，分别采用对比学习和知识蒸馏训练，且蒸馏教师为强大的对比学习模型，然后对比评估模型性能。

Result: 知识蒸馏（当教师模型比学生模型大且更强时）通常在域内和跨域任务上都优于直接对比学习；但如果教师模型与学生模型容量相当，则知识蒸馏的优势不明显，特别是在跨域任务上。

Conclusion: 当有更大更强的教师模型可用时，推荐使用知识蒸馏训练小型重排序模型；若无合适的教师，则对比学习是更可靠的选择。

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [47] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

TL;DR: 系统评估了多种提升方法对LLMs和RLLMs在金融问答任务中的效果，发现这些方法对LLMs提升显著，对RLLMs提升有限。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）和增强推理能力的推理大型语言模型（RLLMs）在各自领域取得了显著进展，但针对金融领域如何最佳发挥其性能的系统性研究仍然有限。作者希望探索多种方法对LLMs和RLLMs在金融问答任务中的表现影响。

Method: 作者选取了五种LLM和三种RLLM，系统评估了提示方法、智能体框架及多语言对齐三类方法在金融问答任务中的效果。通过实验对比分析这些方法对模型性能的提升作用。

Result: （1）当前的提示方法和智能体框架可以通过模拟长链式思维推理（Long CoT），提升LLMs在金融问答的表现；（2）RLLMs自身已具备长链式思维能力，常规方法对其进一步提升效果有限；（3）多语言对齐主要提升LLMs的多语言推理长度，对RLLMs益处很小。

Conclusion: 目前已有的提示方法、智能体框架和多语言对齐技术对LLMs提升明显，但对RLLMs提升有限；研究结果为金融领域的问答系统设计和方法选择提供了有益参考。

Abstract: Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [48] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

TL;DR: n-gram评测方法在形态复杂语言中的表现有限，而专门训练的神经评测指标（如COMET）能更准确反映人工评价，未来应加强对这类方法的研究。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化n-gram指标（如ROUGE）主要用于英文生成任务的评估，但它们对其他语言的适用性尚不明确。本研究旨在系统性探讨各种自动化生成任务评估指标在多语言环境下的有效性。

Method: 设计了一个覆盖八种来自四个语言类型（黏着语、孤立语、低融合语、高融合语）的评测体系，分别在低资源和高资源环境中分析指标与人类评价的一致性，比较n-gram指标和神经网络指标的表现。

Result: n-gram类指标在融合语（形态丰富的语言）中与人工评价的相关性显著低于在孤立语和黏着语中的表现。而适当的分词处理可以缓解甚至逆转这一负面趋势。训练专门用于评估的神经网络指标（如COMET）特别在低资源语言中，与人工评价的相关性表现最佳。

Conclusion: n-gram指标在融合语等形态丰富的语言中存在明显局限，推荐增加面向评估任务训练的神经网络指标的研究和应用投入。

Abstract: Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [49] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

TL;DR: 多智能体LLM对话在科研创意生成中更有效，尤其是异质性强和互动深入时可提升点子的多样性与可行性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）常被用于辅助创造性任务如科研点子的生成，但LLM之间对话的最佳设计尚不明确。本文希望分析多智能体LLM对话在科学创意生成中的作用，探索不同配置如何影响生成点子的创新性与可行性。

Method: 作者进行了系统性实验，比较了多种多智能体LLM对话的配置，包括代理角色数量、代理人数和对话深度，同时设置产生者与批评者的迭代互动，并观察各因素对创意质量的影响。

Result: 实验显示：增加代理人数、加深互动深度、提升智能体身份多样性均能显著提升生成创意的多样性，特别是增加批评者端的多样性还可以进一步提升最终方案的可行性。

Conclusion: 多智能体并具有异质性角色的LLM系统有助于提高科学创意的多样性和可行性。作者提出了相关系统设计的实用建议。

Abstract: Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [50] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 金标准数据昂贵且未必最优，模型自生成并自判为事实的数据用于微调反而更能抑制幻觉。用模型内判过滤自生成数据的训练策略，跨领域效果优异。


<details>
  <summary>Details</summary>
Motivation: 语言模型容易产生事实性错误（即幻觉），尽管在高质量事实信息上微调能减少幻觉，但金标准事实数据代价高昂，且在陌生但正确的数据上训练可能反而增加幻觉。因此，实践者应如何选择合适的数据集以降低模型幻觉，成为亟需解决的问题。

Method: 本研究分析了微调数据的事实性与语言模型长文本生成幻觉现象之间的关系。作者将模型在不同类型数据（事实金标准数据与模型自生成并自判为事实的数据）上进行微调，并评估在不同过滤策略下的幻觉发生率。

Result: 实验结果表明，微调模型自生成且模型自认为是事实性的数据，比在事实金标准数据上微调效果更好。尤其是用模型自身内部判断过滤过的自生成数据进行微调，会比用金标准数据及其它配置获得更高事实性。

Conclusion: 微调时，利用模型自己的事实性判断来筛选训练数据能更有效地缓解幻觉问题，且这一策略在多个领域均有验证效果，提示模型自身信念可作为提升事实性的有力信号。

Abstract: Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [51] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 该综述系统梳理了大型语言模型在多学科场景下的应用案例与技术路径，总结挑战和前沿进展，是跨学科LLM应用研究的重要资源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在跨学科研究中展现了巨大潜力，但目前缺乏系统性梳理与理解其在各学科中的融合和应用方式。

Method: 该综述梳理了LLMs在跨学科研究中的应用情况，从技术手段如有监督微调、检索增强生成、基于智能体等方法等角度分类分析，同时结合其在具体学科中的适用性进行归纳。

Result: 论文详细分析了LLMs在数学、物理、化学、生物以及人文和社会科学等多个领域中的重要应用，指出了当前面临的主要挑战，并总结了未来研究方向和最新进展。

Conclusion: 本文为探索和应用LLMs于跨学科研究提供了全面参考，对相关研究者具有重要参考价值。

Abstract: Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [52] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: ChainEdit框架结合知识图谱和LLM推理能力，实现链式知识编辑，显著提升逻辑一致性和知识传播能力，达到当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的知识编辑方法在传播关联事实的涟漪效应时难以保持逻辑一致性。为了解决这个问题，需要一种能系统化、可靠地维护知识间逻辑关系的编辑机制。

Method: 提出ChainEdit框架，将知识图谱中的逻辑规则与语言模型的逻辑推理能力结合，自动从结构化知识库中提取逻辑模式，并使其与LLM的内部逻辑对齐，实现系统性的链式知识编辑和动态生成。

Result: ChainEdit在逻辑泛化能力上比基线方法提高30%以上，同时保持编辑的可靠性和针对性。此外，论文还采用知识感知的评测协议，减少了外部依赖带来的评测偏差。

Conclusion: ChainEdit方法显著提升了知识编辑后模型在逻辑一致性和涟漪效应方面的能力，并在相关任务上取得了新的SOTA性能。

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [53] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于大语言模型的多智能体系统，可有效模拟专家协作决策会议，尤其在共识检测和提升决策效率方面表现突出，具有实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 决策会议通常邀请多领域专家合作讨论复杂议题，并通过协作达成共识，但这些流程效率依赖于良好的促进和沟通。目前大语言模型（LLM）在模拟真实世界协作环境上表现优异，尤其是在多智能体模拟小组讨论的场景中展现了潜力。该研究旨在探索和提升LLM在此类复杂决策流程中的应用。

Method: 本研究设计了一种新型基于LLM的多智能体系统，专注于检测参与者间的共识。具体方法是对六个不同的LLM进行评估，分别完成“立场检测”（判断智能体对议题的态度）和“立场极性检测”（判断智能体立场的情感倾向：正向、负向或中性）两项任务，并在多代理系统中综合考察其在复杂场景的表现。

Result: 实验结果显示，LLM在动态、细腻的讨论中能稳定识别智能体间的共识。引入专门的共识检测智能体，明显提升了小组讨论的效率和决策过程的组织性，使系统决策结果与现实世界中的决策会议效果接近。

Conclusion: 基于LLM的多智能体系统具备模拟群体决策会议的能力，对于提升专家协作与决策过程具有重要价值，可广泛应用于需专家共识的各类领域。

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [54] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

TL;DR: 本论文建立了全面的LLM错误归因框架和数据集AttriData，并提出了能同时输出分数、归因和反馈的模型MisAttributionLLM，显著提升了LLM评测和错误诊断的自动化和系统性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）被广泛应用后，每天都会产生大量用户与模型的交互数据。为了分析模型表现和诊断回答失败的原因，亟需一个能自动归类和定位错误的系统性框架。然而现有的评测模型缺乏错误归因的能力。

Method: 该论文搭建了一个包含6个一级、15个二级类别的全面错误归因（Misattribution）框架。基于该框架，作者提出了AttriData数据集，专门用于错误归因，包含错误类别、分数和反馈等信息。作者还推出了MisAttributionLLM——在AttriData上微调、能同时输出分数、错误归因和反馈的通用判别模型。

Result: 大量实验和分析验证了所提出方法的有效性和鲁棒性。MisAttributionLLM能够为LLM的答案进行系统、细致的错误归因和评分。

Conclusion: 本文首次提出具备系统错误归因能力的评测框架与数据集，并推出了首个能“三合一”输出（分数、归因和反馈）的通用归因判别模型，对LLM的系统化评估和医学诊断式分析具有重要意义。

Abstract: With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [55] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

TL;DR: 本文实验评估了LLMs在奥地利及欧盟增值税法律决策中的表现。微调与RAG能提升LLMs对案件的分析能力，虽适用于初步支持，但因行业敏感性及对隐性知识、背景文档的处理不足，现阶段难以完全自动化。


<details>
  <summary>Details</summary>
Motivation: 在税务咨询实践中，客户常用自然语言描述案例。为减少税务专业人员的工作量，有必要研究大型语言模型(LLMs)在自动决策和法律分析中的能力。

Method: 通过实验，分别采用微调和检索增强生成(RAG)两种常用方法，针对教科书案例和税务咨询公司的真实案例，评估LLM系统的最佳配置和法律推理能力。

Result: 实验结果显示，LLMs有助于自动化日常任务、生成初步分析，并能提供法律依据，但仍难以完全自动化，尤其在处理隐含知识与特定上下文文档时存在限制。

Conclusion: LLMs在妥善配置后能有效辅助增值税相关的法律决策与咨询，但在应用于法律领域时，仍需集成结构化背景信息以提升可靠性。

Abstract: This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [56] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

TL;DR: 提出了迭代LoRA训练和伪标签策略，显著改善了大语言模型与语音识别结合时的过拟合与性能瓶颈，并在国际比赛中取得优异成绩，方法实用且效果突出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与自动语音识别系统的深度结合能带来巨大应用价值，但在采用LoRA进行监督微调时常出现过拟合问题。该论文旨在解决这一痛点。

Method: 提出了一种创新性的训练范式——迭代LoRA训练(ILT)，并结合了迭代伪标签策略，通过分阶段（集中训练、反馈训练和修正训练）提升模型性能上限。

Result: 实验基于Whisper-large-v3和Qwen2-Audio，采用三阶段训练流程，系统验证了所提方法的有效性。该技术在Interspeech 2025 MLC-SLM挑战中分别获得了第1赛道第4名与第2赛道第1名的成绩，显示了其实用性和应用潜力。

Conclusion: 通过创新训练范式与迭代伪标签策略，有效缓解了LoRA微调的过拟合问题，显著提升了大模型与ASR系统的集成效果，证明了该方法的理论及实际应用价值。

Abstract: The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [57] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 本文提出将项目反应理论与机器学习结合用于自动作文衔接性评分，并验证该方法相比传统ML模型和集成方法有更优表现，对提升教育类写作自动评分具有积极意义。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分模型在评估作文衔接性时，往往忽视了个体样本的难度和区分度等特性，导致评分不够准确。IRT可以弥补这一不足。

Method: 本文将项目反应理论（IRT）引入文本机器学习回归任务，分析和调整ML模型针对作文衔接性的评分，并在葡萄牙语两个作文数据集上进行实验。具体地，从数据集中抽取325项语言特征，通过回归模型预测衔接性分数，并用IRT调整模型输出。

Result: 基于IRT的方法在多项评价指标上优于传统的机器学习模型和集成方法，提升了自动化作文衔接性评分的准确性。

Conclusion: 研究表明，将项目反应理论（IRT）应用于自动作文评分中的衔接性评分任务，可以有效提高评分模型的表现。

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [58] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

TL;DR: 本文提出了 clembench，对话游戏类LLM评测工具，兼顾可控性与实际应用，易用性高且可扩展，有助于推动此种评测方式的普及。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的评估主要有参考型和偏好型两大范式，但各自存在局限——参考型强调测试可控性，偏好型强调生态有效性。最近，一种新的对话游戏评测范式兴起，兼具前两者部分优点，但缺乏成熟易用的实现工具限制了其推广。

Method: 作者提出并开发了一套名为 clembench 的测评工具平台，支持多轮、无参考、可重复的交互测评。该平台自2023年起持续开发，最新版本聚焦于易用性，并提供英文基准测试实例及可扩展的新测试能力。

Result: clembench 工具已被优化至易于广泛使用，可以对自己的模型进行基准测评，并且便于添加自定义、有针对性的测试。该工具填补了对话游戏评测范式成熟实现的空白。

Conclusion: clembench 平台为大语言模型的多轮交互评测提供了切实可用、易于扩展的解决方案，推动了评测新范式的实际应用与研究发展。

Abstract: There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [59] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: LLaPa提出了一种整合视觉与文本信息、并强化反事实推理能力的新框架，在多模态程序规划中表现优越。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在具身智能的程序规划上有出色表现，但对多模态输入和反事实推理的结合研究较少。作者希望提升模态融合与反事实推理能力，以增强具身智能系统的实际执行能力。

Method: 提出了LLaPa框架，利用视觉-语言模型根据文本任务描述和环境图像生成可执行的动作序列。框架包含两个辅助模块：一是任务-环境重排序器（TER），通过任务导向的分割对齐文本与视觉信息，突出关键执行区域；二是反事实活动检索器（CAR），用于识别并强调潜在的反事实条件，提升模型的反事实推理能力。

Result: 在ActPlan-1K和ALFRED基准上进行实验，LLaPa生成的计划方案在相似度（LCS）和正确率等指标上全面优于现有先进模型。

Conclusion: LLaPa框架通过多模态输入和反事实推理的融合，大幅提升了具身智能程序规划的质量与正确性。该方法可为多模态任务规划提供更有效的解决方案。

Abstract: While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [60] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

TL;DR: 将大语言模型集成于LDA的初始化和后处理阶段，发现只有后校正明显有效，说明LLM并非总是文本挖掘的优解。


<details>
  <summary>Details</summary>
Motivation: LDA作为经典的主题模型，初始化质量对结果有巨大影响。作者希望借助大语言模型（LLM）的能力，提升LDA模型在初始化和后处理两个阶段的性能。

Method: 分别在LDA的初始化（通过LLM进行主题聚类以辅助Gibbs采样的初始分配）和后处理阶段（利用LLM进行主题后校正）集成LLM，并进行大量实验对比。

Result: LLM辅助的初始化策略只在早期迭代有提升，无助于最终收敛且表现最差；但LLM用于后校正可显著提升主题一致性得分（提升5.86%）。

Conclusion: LLM内嵌于LDA流程对后处理有明显效果，但并非在所有环节都优于传统方法。LLM并不是通用的最佳文本挖掘工具，需结合实际场景权衡。

Abstract: Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [61] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 本文提出了一个适用于多语种短文本多标签情感检测的特征驱动框架，通过对28种语言的详细实验证明，框架可自适应选择最佳文档表示和模型，高效提升思潮分析效果，同时兼顾处理效率。


<details>
  <summary>Details</summary>
Motivation: 多语种情感检测任务面临着短文本、多标签、资源稀缺与语言多样性等挑战，目前主流情感检测在低资源语言下表现效果有限。因此，本文旨在提出一种能适应不同语言并权衡性能与效率的特征驱动框架，以提升多语情感分析的效果。

Method: 提出了一个特征驱动的动态框架，根据不同语言特点自适应调整文档表示方式和学习算法。框架重点包含文档表示（如TF-IDF、FastText、Sentence-BERT）、降维技术（PCA）以及多样的模型训练方式（如MLP），并在28种语言，重点分析5种语言上进行实验和比较。

Result: 实验发现：1）TF-IDF在低资源语言下依旧十分有效；2）FastText和基于Transformer的Sentence-BERT在不同语言上表现出特定优势；3）PCA降维能显著减少训练时间，且对FastText及神经网络（如MLP）尤为有益，性能无损；4）模型复杂度与计算效率之间存在权衡。

Conclusion: 该框架为多语种文本情感检测提供了可扩展、高效的解决方案，能应对语言多样性和资源限制的问题。

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [62] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文提出了一个支持200种语言、聚焦低资源语言的多语言基准和评测系统AI Language Proficiency Monitor，集合多种任务和主流数据集，配套开放的排行榜和可视化平台，旨在推动多语言AI的公平评估和持续进步。


<details>
  <summary>Details</summary>
Motivation: 为推动全球范围内大语言模型能力的公平评估，特别关注低资源语言，旨在促进多语言AI的透明性、包容性和进步。

Method: 通过整合多样化的任务（如翻译、问答、数学、推理），利用FLORES+、MMLU、GSM8K、TruthfulQA、ARC等数据集，构建基准，配备自动更新的开源排行榜和仪表盘，供研究者、开发者、政策制定者使用。

Result: 平台不仅排名模型，还提供全球语言能力地图和时间趋势分析等洞见，补充和扩展了已有的多语言基准体系。

Conclusion: 该论文提出了一个综合性多语言基准AI Language Proficiency Monitor，用于系统评估大语言模型在多达200种语言（尤其是低资源语言）上的性能。

Abstract: To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [63] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

TL;DR: DocPolarBERT引入极坐标相对位置注意力，减少了对大量数据和绝对2D嵌入的依赖，在小规模预训练下仍达SOTA，是高效准确的文档理解新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的文档理解模型（如BERT）依赖于绝对的二维位置嵌入，依赖大量数据预训练，效率和泛化性受限。本文希望提出一种无需绝对2D位置嵌入，同时能高效理解文档结构的新方法。

Method: 提出DocPolarBERT模型，用相对极坐标系统扩展自注意力机制，能够感知文本块间的相对位置，而不是绝对的笛卡尔坐标。模型预训练数据量仅为主流数据集（如IIT-CDIP）的1/6。

Result: DocPolarBERT在文档理解任务上实现了最新的最优性能（SOTA），证明专门设计的注意力机制可以在数据量较小的情况下显著提升效果。

Conclusion: 相对极坐标注意力机制无需绝对2D位置嵌入，也能高效且有效地实现文档理解，是数据高效又性能优越的替代方案。

Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [64] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 本文系统评估了主流大型语言模型及其推理增强版本在论证分类任务中的表现，发现ChatGPT-4o和DeepSeek-R1最优，但都存在不足，并对模型和数据集问题进行了深入分析，指明了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘（AM）作为一个跨学科领域，近年来因大语言模型（LLMs）的进步而取得了显著发展。然而，目前缺乏对这些模型在公开可用论证分类数据库上的研究与结果分析。

Method: 本文选择了GPT、Llama、DeepSeek等LLM及其带Chain-of-Thoughts算法的推理增强变体，基于Args.me和UKP等多样化数据集，对各模型在论证分类上的表现进行了实证测试与对比分析，并系统讨论了模型常见的错误类型。

Result: 实验证明，ChatGPT-4o在论证分类基准上表现最佳。而集成推理能力的DeepSeek-R1在相关任务中显示出其优越性。不过，即便是表现最好的GPT-4o和DeepSeek-R1，仍存在一定的错误与不足，文中对各模型常见的错误类型做了详细讨论。

Conclusion: 本文首次系统分析了LLM及Prompt算法在公开论证数据集上的表现，展示了现有算法的不足与改进方向，对数据集本身也进行了深入剖析，发现并指出了其问题。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [65] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 本文首次系统性分析了自动语音识别（ASR）转录错误对说话人归属任务的影响，发现归属性能对错误具有鲁棒性，并且ASR文本的表现不逊于人工文字稿，甚至可能更好。


<details>
  <summary>Details</summary>
Motivation: 在无法获得音频或音频不可靠的情况下，利用文字稿进行说话人归属判别变得尤为重要。然而，以往研究主要基于人工标注的文字稿，在实际中常常只能取得包含错误的自动语音识别（ASR）文字稿。本文动机在于系统性研究自动转录对说话人归属任务的影响及其性能。

Method: 本文对比分析了自动语音识别（ASR）生成的带有错误的文字稿与人工标注文字稿在说话人归属任务中的表现。研究了转录错误程度、ASR系统属性等因素对归属性能的影响，并统计两者归属表现的相关性。

Result: 实验发现，说话人归属对单词级别的转录错误具有很强的鲁棒性，ASR系统对真实文字稿的还原程度与说话人归属性能几乎不相关。此外，基于ASR错误文字稿的说话人归属表现与基于人工标注文字稿至少相当，甚至更好，部分原因在于ASR错误反映了说话人独特特征，有助于身份识别。

Conclusion: 说话人归属任务在ASR转录带来的错误情况下依然具备出色的表现。这一发现表明，即使在现实限制下依赖自动转录输出，也能获得与人工标注文字稿相当甚至更优的归属效果。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [66] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: 本论文提出了KELPS神经符号框架，通过将自然语言数学问题转化为多种形式化语言，显著提升了语料规模及形式化准确率，成果优于现有模型，并已开源全部数据和代码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在将非正式数学表述形式化为可验证定理方面取得了进展，但由于多语种平行语料质量和数量有限，发展受到瓶颈。因此，亟需一种高效的神经符号框架以提升形式化数学的表现。

Method: 提出KELPS（基于知识方程的逻辑处理系统）神经符号框架，通过迭代方式将非正式自然语言数据翻译、合成和过滤为多种形式化语言（Lean、Coq、Isabelle）。首先将自然语言翻译为新设计的知识方程（KEs），再利用严格规则转换为目标形式化语言，确保结构和语义的保真。

Result: KELPS生成了包含6万多个问题的多语种平行语料库。在MiniF2F数据集上，KELPS在句法准确率（pass@1）上达到88.9%，超越了Deepseek-V3（81%）和Herald（81.3%）等现有最优模型。

Conclusion: KELPS框架有效促进了非正式数学表述的高质量形式化，突破了数据和表现的双重瓶颈，并将所有数据和代码开源，有望推动形式化数学和多语言模型进一步发展。

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [67] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 本研究提出了一个基于专用KGA模块的首个测试时知识图谱增强框架，实现了无需调整模型参数、可动态实时融合知识图谱信息。实验显示该方法在多个基准任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有结合知识图谱（KGs）提升大规模语言模型（LLMs）的方法，过于依赖参数量大的微调，容易产生灾难性遗忘和泛化能力下降的问题。同时，由于集成方式过于静态，难以适应实时知识更新。

Method: 提出了一种测试时（test-time）动态知识融合框架——以知识图谱引导注意力（KGA）模块为核心，无需参数更新即可与基础模型集成。KGA模块通过“外向聚合”和“内向聚合”两条路径组合：外向路径将外部知识动态地融合到输入表示中，内向路径通过知识过滤提升知识相关特征，并将最相关的知识三元组反馈到融合过程，实现闭环增强。

Result: 在五个基准数据集上进行了大量实验证明，该方法能在不改变模型参数的前提下实现与主流知识融合方法相当的性能。

Conclusion: 本文首次提出了一种测试时、免更新参数、实时融合知识图谱信息的方法，有效支持了LLMs灵活的知识增强和实时知识集成。

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [68] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 本文提出了一种无需微调，也不需反复干预的新型缓存引导方法Cache Steering，可显著提升语言模型的多步推理能力和效率，在实际部署中更为便捷实用。


<details>
  <summary>Details</summary>
Motivation: 现有的激活调控（activation steering）方法需对模型进行连续干预，操作复杂且计算成本高，难以在推理过程中高效应用。作者希望找到一种简单有效的方法来引导语言模型产生更明确、步骤化的推理过程。

Method: 提出一种称为Cache Steering的新方法。该方法通过对语言模型的key-value缓存进行单次（one-shot）干预，引入来自GPT-4o生成的推理轨迹，构建“调控向量”，从而无需微调或修改提示词即可隐式引导模型产生链式推理。

Result: 在多个推理基准测试上，Cache Steering方法不仅提升了模型推理的结构性表现，还有助于定量任务性能提升。此外，与以往的连续干预方法相比，单次Cache Steering在超参数稳定性、推理效率及集成难度等方面展现出显著优势。

Conclusion: Cache Steering能够以低成本、高效率的方式提升语言模型的推理能力，适合实际的可控生成场景，并优于传统的激活调控方法。

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>
