<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Semantically Reflected Programs](https://arxiv.org/abs/2509.03318)
*Eduard Kamburjan,Vidar Norstein Klungre,Yuanwei Qu,Rudolf Schlatte,Egor V. Kostylev,Martin Giese,Einar Broch Johnsen*

Main category: cs.PL

TL;DR: 本文提出将面向对象程序状态提升为知识图谱，并在语言层面实现语义反射，打通程序演化与正式知识表达。以地质建模为例验证，相关语言已开源。


<details>
  <summary>Details</summary>
Motivation: 本文关注结构性知识和行为性知识形式化之间的鸿沟，在程序与知识图谱之间建立联系。作者认为，现有知识图谱和本体论适合表达系统的正式知识，而编程语言适合描述系统的演化。因此，有必要提出新方法以统一两者。

Method: 作者提出了针对面向对象编程语言，将程序状态语义提升为知识图谱的方法，并在编程语言中提供语义反射层，允许开发者在程序中直接利用应用领域知识。具体而言，作者在一门名为SMOL的小型编程语言中正式定义了语义提升和语义反射机制，阐释了语言的操作性，并探讨了类型正确性及虚拟化以支持在运行时通过反射层进行程序查询。

Result: 作者通过地质建模案例，展示了语义提升和语义反射技术的实际应用，并讨论了该技术在不同领域的潜在应用场景。该编程语言已开源并可在线获取。

Conclusion: 论文证明语义提升和语义反射在统一程序演化描述与正式知识表达方面提供了新的技术路径，为开发者带来更强的领域知识关联能力，具有广泛应用前景。

Abstract: This paper addresses the dichotomy between the formalization of structural
and the formalization of behavioral knowledge by means of semantically lifted
programs, which explore an intuitive connection between programs and knowledge
graphs. While knowledge graphs and ontologies are eminently useful to represent
formal knowledge about a system's individuals and universals, programming
languages are designed to describe the system's evolution. To address this
dichotomy, we introduce a semantic lifting of the program states of an
executing program into a knowledge graph, for an object-oriented programming
language. The resulting graph is exposed as a semantic reflection layer within
the programming language, allowing programmers to leverage knowledge of the
application domain in their programs. In this paper, we formalize semantic
lifting and semantic reflection for a small programming language, SMOL, explain
the operational aspects of the language, and consider type correctness and
virtualisation for runtime program queries through the semantic reflection
layer. We illustrate semantic lifting and semantic reflection through a case
study of geological modelling and discuss different applications of the
technique. The language implementation is open source and available online.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems](https://arxiv.org/abs/2509.02860)
*Connor Wojtak,Darek Gajewski,Tomas Cerny*

Main category: cs.SE

TL;DR: 提出用静态方法把微服务源代码转为形式模型，再进行SMT约束与形式化验证，重点验证系统架构和安全等，提升可靠性，未来将继续扩展和评估。


<details>
  <summary>Details</summary>
Motivation: 微服务系统因其可扩展性、去中心化开发和支持持续集成/持续交付而被广泛采用。但这种由不同团队的去中心化开发和持续演进可能导致沟通不畅和实现不兼容，从而影响系统的可维护性和可靠性。

Method: 作者提出了一种新的方法学，将微服务源代码静态重构为形式化系统模型，并可从中导出可用于形式化验证的SMT约束集。这一方法学具备可扩展性，支持多种跨领域的软件验证。

Result: 方法学能够实现系统架构等跨领域关注点的形式化验证，并通过正式推理证明了其正确性与适用性。同时也考虑了安全策略实现等其他关注点。

Conclusion: 该方法学为微服务系统实现形式化验证提供了一条新路径，促进了微服务的可靠性和可维护性。未来计划扩展并进一步评估该方法学。

Abstract: Microservice systems are becoming increasingly adopted due to their
scalability, decentralized development, and support for continuous integration
and delivery (CI/CD). However, this decentralized development by separate teams
and continuous evolution can introduce miscommunication and incompatible
implementations, undermining system maintainability and reliability across
aspects from security policy to system architecture. We propose a novel
methodology that statically reconstructs microservice source code into a formal
system model. From this model, a Satisfiability Modulo Theories (SMT)
constraint set can be derived, enabling formal verification. Our methodology is
extensible, supporting software verification across multiple cross-cutting
concerns. We focus on applying the methodology to verify the system
architecture concern, presenting formal reasoning to validate the methodology's
correctness and applicability for this concern. Additional concerns such as
security policy implementation are considered. Future directions are
established to extend and evaluate the methodology.

</details>


### [3] [Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations](https://arxiv.org/abs/2509.03093)
*Fatih Pehlivan,Arçin Ülkü Ergüzen,Sahand Moslemi Yengejeh,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 提出了一套新方法，通过不同提示工程技术系统评测主流LLM跨多语言检测SOLID原则违例的能力，发现模型和提示需根据设计上下文灵活调整，GPT-4o Mini整体最优但非万能，一体化AI辅助代码设计分析尚需场景定制化。


<details>
  <summary>Details</summary>
Motivation: 现有的静态分析方法难以检测涉及面向对象设计原则（如SOLID原则）的语义性设计缺陷，且主流方案往往仅关注某一单一SOLID原则或某特定编程语言，无法覆盖多语言代码库中五项SOLID原则的全面检测。

Method: 提出了一种基于定制化提示工程（prompt engineering）的方法，通过设计不同的提示策略（零样本、少样本、Chain-of-Thought等），系统性地评估大型语言模型（LLM）检测跨多语言的SOLID原则违例能力，并构建包含240份人工验证代码的基准测试集，对四个主流LLMs进行对比测试。

Result: GPT-4o Mini在整体表现上优于其他模型，但在DIP等复杂原则上仍有挑战。不同的提示策略对检测准确率影响显著，且没有一种策略在所有场景下表现最佳——如Ensemble提示对OCP优于其他，Example提示更适合检测DIP。总体检测性能还受到代码复杂度和语言特点的影响。

Conclusion: AI驱动的设计分析要取得最佳效果，需要针对具体的设计上下文匹配最合适的模型与提示，而非依赖单一最佳模型，显示出LLM在提高代码可维护性和辅助AI代码分析中的巨大潜力。

Abstract: Traditional static analysis methods struggle to detect semantic design flaws,
such as violations of the SOLID principles, which require a strong
understanding of object-oriented design patterns and principles. Existing
solutions typically focus on individual SOLID principles or specific
programming languages, leaving a gap in the ability to detect violations across
all five principles in multi-language codebases. This paper presents a new
approach: a methodology that leverages tailored prompt engineering to assess
LLMs on their ability to detect SOLID violations across multiple languages. We
present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,
and GPT-4o Mini-on their ability to detect violations of all five SOLID
principles. For this evaluation, we construct a new benchmark dataset of 240
manually validated code examples. Using this dataset, we test four distinct
prompt strategies inspired by established zero-shot, few-shot, and
chain-of-thought techniques to systematically measure their impact on detection
accuracy. Our emerging results reveal a stark hierarchy among models, with
GPT-4o Mini decisively outperforming others, yet even struggles with
challenging principles like DIP. Crucially, we show that prompt strategy has a
dramatic impact, but no single strategy is universally best; for instance, a
deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE
prompt is superior for DIP violations. Across all experiments, detection
accuracy is heavily influenced by language characteristics and degrades sharply
with increasing code complexity. These initial findings demonstrate that
effective, AI-driven design analysis requires not a single best model, but a
tailored approach that matches the right model and prompt to the specific
design context, highlighting the potential of LLMs to support maintainability
through AI-assisted code analysis.

</details>


### [4] [AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation](https://arxiv.org/abs/2509.03270)
*Martin Skoglund,Fredrik Warg,Aria Mirzai,Anders Thorsen,Karl Lundgren,Peter Folkesson,Bastian Havers-zulka*

Main category: cs.SE

TL;DR: 该论文针对电动汽车AI组件安全，提出将ISO 26262与ISO/PAS 8800结合进行独立评估的方法。以AI电池估算为例，用故障注入测试验证了评估方法的有效性，为AI安全标准实施提供了参考。


<details>
  <summary>Details</summary>
Motivation: AI技术在电动汽车中的应用带来了全新的安全保障挑战，尤其是在需要符合汽车功能安全标准ISO 26262的前提下，传统安全评估方法不适用于AI相关功能。因此，有必要探索针对电动汽车AI组件的独立安全评估方法。

Method: 本论文结合ISO 26262与新发布的ISO/PAS 8800（专注于道路车辆AI安全），以AI驱动的电池荷电状态（SOC）估算为实例，通过系统性故障注入实验对AI组件进行鲁棒性测试。即通过人为扰乱传感器输入，考查AI组件对输入变化的适应和抗干扰能力。

Result: 识别和总结了针对AI组件独立评估的关键特性，验证了扩展的评估方法对AI组件鲁棒性的有效性。故障注入实验表明该方法能够评估AI在面对输入波动时的表现。

Conclusion: 将ISO 26262与ISO/PAS 8800结合，可实现对电动汽车AI组件（例如SOC估算）的深入独立安全评估，为AI安全标准的落地和实践提供了具体可行的路径与方法。

Abstract: Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)
introduces unique challenges for safety assurance, particularly within the
framework of ISO 26262, which governs functional safety in the automotive
domain. Traditional assessment methodologies are not geared toward evaluating
AI-based functions and require evolving standards and practices. This paper
explores how an independent assessment of an AI component in an EV can be
achieved when combining ISO 26262 with the recently released ISO/PAS 8800,
whose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)
battery estimation exemplifies the process. Key features relevant to the
independent assessment of this extended evaluation approach are identified. As
part of the evaluation, robustness testing of the AI component is conducted
using fault injection experiments, wherein perturbed sensor inputs are
systematically introduced to assess the component's resilience to input
variance.

</details>


### [5] [VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities](https://arxiv.org/abs/2509.03331)
*Weizhe Wang,Wei Ma,Qiang Hu,Yao Zhang,Jianfei Sun,Bin Wu,Yang Liu,Guangquan Xu,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 现有自动漏洞修复评测方法过于理想化，高估了LLM能力。本文提出基于真实PoC攻击的VulnRepairEval框架，发现多种主流模型真实能力远低于传统评测结果，提醒社区关注实战性评估。


<details>
  <summary>Details</summary>
Motivation: 目前利用大语言模型（LLM）实现自动化软件漏洞修复的研究，大多依赖于表面化的验证方法，缺少对真实攻击（exploit）效果的检测，可能高估了模型在实际安全场景中的表现。作者希望解决评测结果与真实安全场景之间的脱节问题。

Method: 提出VulnRepairEval评测框架：基于实际有效的漏洞利用（Proof-of-Concept, PoC）攻击进行评测。具体方法包括：基于容器化的评测管道，将修复后代码与原攻击PoC运行，要求PoC无法在修复后代码中执行，通过差分评估修复效果。框架还包括了数据集精筛与自动化测试工具。

Result: 分析12个主流LLM在这一评测框架下的表现，最佳模型也只成功修复5个/23个（约21.7%）真实Python CVE漏洞。主要失败原因是模型无法精准定位漏洞，或修复补丁存在语法/语义错误。改进提示词和多智能体方法提升有限。

Conclusion: 提出了严谨且贴近真实攻击场景的漏洞修复评测框架VulnRepairEval，并证实当前LLM在现实漏洞修复领域存在显著不足，强调了评测方式需高度贴合实际漏洞利用过程。

Abstract: The adoption of Large Language Models (LLMs) for automated software
vulnerability patching has shown promising outcomes on carefully curated
evaluation sets. Nevertheless, existing datasets predominantly rely on
superficial validation methods rather than exploit-based verification, leading
to overestimated performance in security-sensitive applications. This paper
introduces VulnRepairEval, an evaluation framework anchored in functional
Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,
containerized evaluation pipeline that enables reproducible differential
assessment, where repair success requires the original exploit to fail
execution against the modified code. The benchmark construction involved
extensive data curation: we processed over 400 CVEs and approximately 2,500
potential sources to extract a collection of authentic vulnerability instances
(23 Python CVEs) amenable to automated testing with working PoCs. Through
VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and
observe a significant performance deficit: even the top-performing model
successfully addresses merely 5/23 instances (about 21.7%), exposing critical
weaknesses in security-focused applications. Our failure analysis reveals that
most unsuccessful attempts stem from imprecise vulnerability identification and
patches containing syntactic or semantic errors. Enhanced prompting strategies
and multi-agent approaches yield minimal improvements, with overall
effectiveness remaining largely unaffected. This work contributes a stringent,
practical evaluation framework for LLM-driven vulnerability remediation and
underscores the necessity for assessment protocols that authentically reflect
real-world exploitation scenarios.

</details>


### [6] [The Impact of Critique on LLM-Based Model Generation from Natural Language: The Case of Activity Diagrams](https://arxiv.org/abs/2509.03463)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: LADEX框架通过结合算法与LLM的结构-语义校验，显著提升了从自然语言描述生成活动图的正确性和完整性，为自动化建模提供高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因具备从自然语言描述自动生成模型的潜力而受到关注。实现生成过程时，生成-评价-细化的迭代机制被广泛使用，但需要同时解决结构正确性与语义对齐的问题。该领域缺乏对结构与语义校验方式、及其组合效果系统性分析。

Method: 提出了LADEX（基于LLM的活动图提取器）方法，通过LLM主导的评价-细化流程，从自然语言过程描述中自动提取活动图。结构校验可由算法或LLM实现，语义校验仅由LLM完成。设计了五种消融变体，比较评价机制、结构校验与语义校验的角色及其协同效果。评测采用与专家标注结果的可追踪操作语义对比，自动量化正确性与完整性。

Result: 1）迭代评价-细化机制在结构有效性、正确性和完整性上优于单轮生成。2）算法结构校验比LLM校验能发现更多不一致问题，将正确性提升17.81%，完整性提升13.24%。3）结合算法结构校验与LLM语义校验（采用O4 Mini模型）获得最优表现，平均正确性达86.37%、完整性达88.56%，且平均仅需不到5次LLM调用。

Conclusion: 引入结构与语义分工、评价-细化循环的自动化流程能大幅提升活动图的自动化建模效果。算法与LLM协作在结构-语义校验上的优势显著，有效平衡性能与算力开销。

Abstract: Large Language Models (LLMs) show strong potential for automating the
generation of models from natural-language descriptions. A common approach is
an iterative generate-critique-refine loop, where candidate models are
produced, evaluated, and updated based on detected issues. This process needs
to address: (1) structural correctness - compliance with well-formedness rules
- and (2) semantic alignment - accurate reflection of the intended meaning in
the source text. We present LADEX (LLM-based Activity Diagram Extractor), a
pipeline for deriving activity diagrams from natural-language process
descriptions using an LLM-driven critique-refine process. Structural checks in
LADEX can be performed either algorithmically or by an LLM, while alignment
checks are always performed by an LLM. We design five ablated variants of LADEX
to study: (i) the impact of the critique-refine loop itself, (ii) the role of
LLM-based semantic checks, and (iii) the comparative effectiveness of
algorithmic versus LLM-based structural checks.
  To evaluate LADEX, we compare the generated activity diagrams with
expert-created ground truths using trace-based operational semantics. This
enables automated measurement of correctness and completeness. Experiments on
two datasets indicate that: (1) the critique-refine loop improves structural
validity, correctness, and completeness compared to single-pass generation; (2)
algorithmic structural checks eliminate inconsistencies that LLM-based checks
fail to detect, improving correctness by an average of 17.81% and completeness
by 13.24% over LLM-only checks; and (3) combining algorithmic structural checks
with LLM-based semantic checks, implemented using the reasoning-focused O4
Mini, achieves the best overall performance - yielding average correctness of
up to 86.37% and average completeness of up to 88.56% - while requiring fewer
than five LLM calls on average.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [7] [Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning](https://arxiv.org/abs/2509.02958)
*Kaustuv Mukherji,Jaikrishna Manojkumar Patil,Dyuman Aditya,Paulo Shakarian,Devendra Parkar,Lahari Pokala,Clark Dorman,Gerardo I. Simari*

Main category: cs.LO

TL;DR: LAT Logic通过创新的逻辑结构，实现了高效的开放世界时序推理，极大提升了速度和内存效率，并提供实际平台支持。


<details>
  <summary>Details</summary>
Motivation: 当前基于GAP的逻辑系统难以同时高效支持复杂的时序推理、非马尔可夫过程以及开放世界假设，尤其在大量或无穷常量下grounding成本巨大，亟需统一、可扩展且计算高效的逻辑推理框架。

Method: 提出了LAT Logic，将GAP扩展到结合时序推理与下格注释结构，并通过Skolemization实现高效grounding，理论上界定了计算复杂性，并实现了PyReason平台，在多智能体和知识图任务、强化学习仿真中进行实证评估。

Result: 理论上，LAT Logic支持效率更高的grounding和开放世界推理；实证上，在多任务（多智能体、知识图和强化学习环境）中实现了最高达千倍速度提升和十万倍内存优化，强化学习场景下智能体胜率提升26%。

Conclusion: LAT Logic在保持或提升任务性能的同时，大幅提升了速度与内存效率，在动态和不确定环境下展现出极强的开放世界时序推理和推断能力。

Abstract: We introduce Lattice Annotated Temporal (LAT) Logic, an extension of
Generalized Annotated Logic Programs (GAPs) that incorporates temporal
reasoning and supports open-world semantics through the use of a lower lattice
structure. This logic combines an efficient deduction process with temporal
logic programming to support non-Markovian relationships and open-world
reasoning capabilities. The open-world aspect, a by-product of the use of the
lower-lattice annotation structure, allows for efficient grounding through a
Skolemization process, even in domains with infinite or highly diverse
constants.
  We provide a suite of theoretical results that bound the computational
complexity of the grounding process, in addition to showing that many of the
results on GAPs (using an upper lattice) still hold with the lower lattice and
temporal extensions (though different proof techniques are required). Our
open-source implementation, PyReason, features modular design, machine-level
optimizations, and direct integration with reinforcement learning environments.
Empirical evaluations across multi-agent simulations and knowledge graph tasks
demonstrate up to three orders of magnitude speedup and up to five orders of
magnitude memory reduction while maintaining or improving task performance.
Additionally, we evaluate LAT Logic's value in reinforcement learning
environments as a non-Markovian simulator, achieving up to three orders of
magnitude faster simulation with improved agent performance, including a 26%
increase in win rate due to capturing richer temporal dependencies. These
results highlight LAT Logic's potential as a unified, extensible framework for
open-world temporal reasoning in dynamic and uncertain environments. Our
implementation is available at: pyreason.syracuse.edu.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Zimeng Huang,Xiaofei Sun,Jian Wang,Chengpei Tang,Keze Wang*

Main category: cs.CL

TL;DR: DrDiff通过动态调度、稀疏注意力和生成优化，提升了长文本生成的效率与质量，实验结果优于现有顶尖方法。


<details>
  <summary>Details</summary>
Motivation: 现有长文本生成模型在生成效率和质量间存在权衡，难以兼顾两者。本文旨在突破这个效率-质量瓶颈。

Method: 提出DrDiff框架，包含三项核心创新：（1）动态专家调度机制，根据文本复杂度智能分配计算资源；（2）层次化稀疏注意力（HSA）机制，根据输入长度自适应调整注意力模式，将计算复杂度从O(n^2)降至O(n)；（3）软吸收引导优化策略，与DPM-solver++结合，减少扩散步骤并提高生成速度。

Result: DrDiff在文本生成任务中展现出更高的效率和更优的文本质量，综合实验优于SOTA。

Conclusion: DrDiff在多个长文本生成基准上表现优于现有的最先进方法。

Abstract: This paper introduces DrDiff, a novel framework for long-text generation that
overcomes the efficiency-quality trade-off through three core technologies.
First, we design a dynamic expert scheduling mechanism that intelligently
allocates computational resources during the diffusion process based on text
complexity, enabling more efficient handling of text generation tasks of
varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)
mechanism that adaptively adjusts attention patterns according to a variety of
input lengths, reducing computational complexity from O($n^2$) to O($n$) while
maintaining model performance. Finally, we propose a soft absorption guidance
optimization strategy that combines with DPM-solver++ to reduce diffusion
steps, significantly improving generation speed. Comprehensive experiments on
various long-text generation benchmarks demonstrate the superiority of our
DrDiff over the existing SOTA methods.

</details>


### [9] [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.CL

TL;DR: 将多种先进参数高效微调方法引入语音领域，经系统对比后表现优异，并提出SSVD新方法进一步提升大模型领域适应能力，相关方法全部集成并开源于ESPnet。


<details>
  <summary>Details</summary>
Motivation: 随着基础大模型的广泛应用，如何高效地适配这些模型到新领域成为热点问题。PEFT（参数高效微调）作为解决大模型适配的可扩展方案受到关注，但其最新变体多应用于语言和视觉领域，对语音领域的探索和验证有限。

Method: 本文首次将多种先进PEFT方法（如LoRA、VeRA、DoRA、PiSSA和SVFT）集成到ESPnet语音处理框架中，并统一基准测试。同时提出一种结构化SVD引导的微调方法——SSVD：通过仅旋转输入关联的右奇异向量，并保持输出相关的向量不变，以更好地在参数量极少的情况下提升领域自适应和泛化能力。

Result: 在多个语音识别领域迁移任务（包括儿童语音和方言变体），涵盖从0.1B到2B参数规模的大模型基准测试中，上述方法均实现了性能提升。

Conclusion: PEFT方法及其变体在语音领域具有良好的适用性，通过结构化SVD引导的微调进一步提升了效率和迁移表现。所有实现已开源至ESPnet，促进复现与进一步研究。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for
adapting large foundation models. While low-rank adaptation (LoRA) is widely
used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,
PiSSA, and SVFT, are developed mainly for language and vision tasks, with
limited validation in speech. This work presents the first comprehensive
integration and benchmarking of these PEFT methods within ESPnet. We further
introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates
input-associated right singular vectors while keeping output-associated vectors
fixed to preserve semantic mappings. This design enables robust domain
adaptation with minimal trainable parameters and improved efficiency. We
evaluate all methods on domain-shifted speech recognition tasks, including
child speech and dialectal variation, across model scales from 0.1B to 2B. All
implementations are released in ESPnet to support reproducibility and future
work.

</details>


### [10] [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)
*Gustavo Bonil,João Gondim,Marina dos Santos,Simone Hashiguti,Helena Maia,Nadia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本研究通过定量和定性结合，揭示LLaMA模型在生成葡萄牙语故事时，不自觉地再现和强化了女性身体的历史性、殖民话语，提醒我们关注AI潜在的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（特别是LLaMA 3.2-3B）在葡萄牙语短篇故事中如何建构关于黑人和白人女性的叙事，为理解AI再现性别与种族不平等提供新的视角。

Method: 共分析2100篇文本，结合计算方法对语义相似的故事分组，再筛选代表性样本进行定性分析。采用机器学习与人工话语分析相结合的综合方法。

Result: 发现三大主要话语表达：社会跨越、祖先神话化和主观自我实现。分析表明，表面中立和语法连贯的文本实际上固化、再生产了殖民结构下的女性身体叙事，加深了历史性不平等。

Conclusion: 该研究揭示了现有大语言模型输出文本中隐含的殖民性框架，说明AI自动生成的内容可能加剧性别和种族不平等，呼吁针对AI内容的批判性分析。

Abstract: This study investigates how large language models, in particular LLaMA
3.2-3B, construct narratives about Black and white women in short stories
generated in Portuguese. From 2100 texts, we applied computational methods to
group semantically similar stories, allowing a selection for qualitative
analysis. Three main discursive representations emerge: social overcoming,
ancestral mythification and subjective self-realization. The analysis uncovers
how grammatically coherent, seemingly neutral texts materialize a crystallized,
colonially structured framing of the female body, reinforcing historical
inequalities. The study proposes an integrated approach, that combines machine
learning techniques with qualitative, manual discourse analysis.

</details>


### [11] [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)
*Hyunji Nam,Lucia Langlois,James Malamut,Mei Tan,Dorottya Demszky*

Main category: cs.CL

TL;DR: 论文提出解释性标注大模型评估新范式IDEAlgin，并实证其在衡量模型与专家一致性方面优于传统方法，为大模型教育等场景的尺度化采纳奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前尚无有效、可扩展地衡量大模型开放性解释性标注与人类专家标注一致性的标准化方法，且现有向量法等度量在实际应用中表现有限。

Method: 提出并验证了一种基于三元组任务（pick-the-odd-one-out）的专家相似性评价范式IDEAlgin，并评估了多类相似性度量方法（包括传统向量法和LLM判别方法）在两个教育领域数据集上的表现。

Result: 实验证明，大多数向量法难以捕捉人类专家认定的细致相似性，而用IDEAlgin范式提示LLM作为判官进行判别方法，可相比传统方法提升与专家一致率9-30%，验证了该方法的有效性及其扩展潜力。

Conclusion: IDEAlgin方法能够显著提升大模型与专家在解释性标注任务中的一致性评估表现，为开放性任务下大模型的可信实际应用提供了工具基础。

Abstract: Large language models (LLMs) are increasingly applied to open-ended,
interpretive annotation tasks, such as thematic analysis by researchers or
generating feedback on student work by teachers. These tasks involve free-text
annotations requiring expert-level judgments grounded in specific objectives
(e.g., research questions or instructional goals). Evaluating whether
LLM-generated annotations align with those generated by expert humans is
challenging to do at scale, and currently, no validated, scalable measure of
similarity in ideas exists. In this paper, we (i) introduce the scalable
evaluation of interpretive annotation by LLMs as a critical and understudied
task, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing
expert similarity ratings via a "pick-the-odd-one-out" triplet judgment task,
and (iii) evaluate various similarity metrics, including vector-based ones
(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human
benchmarks. Applying this approach to two real-world educational datasets
(interpretive analysis and feedback generation), we find that vector-based
metrics largely fail to capture the nuanced dimensions of similarity meaningful
to experts. Prompting LLMs via IDEAlgin significantly improves alignment with
expert judgments (9-30% increase) compared to traditional lexical and
vector-based metrics. These results establish IDEAlgin as a promising paradigm
for evaluating LLMs against open-ended expert annotations at scale, informing
responsible deployment of LLMs in education and beyond.

</details>


### [12] [A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation](https://arxiv.org/abs/2509.02864)
*Kesen Wang,Daulet Toibazar,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 该文提出了一个完全自动化且自我优化的阿拉伯语长文本问答系统，结合多模型闭环自学习机制，能自动生成高难度、上下文相关的问题并优化答案质量。建立了大规模基准数据AraLongBench，显著提升阿拉伯语视觉语言模型在长文本上的理解与回答能力。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语长文本理解和问答缺乏高质量流程与基准数据，不同领域和长篇内容难以实现自动化精准生成与评估，传统静态流程难以应对复杂与动态的场景需求。

Method: 通过组织多个专用的视觉语言模型（包括问题生成器、答案生成器集群和评估器），形成一个自动化的闭环自学习系统，从原始阿拉伯语文档出发，自动生成与评估专业问题并逐步优化答案质量。引入可调节的质量度量作为超参数，以实现可控难度的问题生成。

Result: 开发了AraLongBench这一大规模阿拉伯语长文本问答基准，覆盖单页与多页任务。实验证明该自进化流程比静态流程效果显著提升，为阿拉伯语长文本的视觉语言理解设定了新的业界标杆，同时也实现了自动化的文档收集流程。

Conclusion: 本论文提出的自我进化问答生成流程能显著提升阿拉伯语长文本的理解与问答性能，超越传统静态流程，同时生成了规模巨大的阿拉伯语长文本问答基准数据集。

Abstract: We present an end-to-end, self-evolving adversarial workflow for long-context
Question-Answer (QA) Generation in Arabic. By orchestrating multiple
specialized LVLMs: a question generator, an evaluator, and a swarm of answer
generators, our system iteratively refines its own performance without any
human intervention. Starting from raw, multi-page Arabic documents across
diverse domains, the question generator produces fine-grained, context-aware
queries to be tackled by the answer generator swarm, and the evaluator assesses
and feeds back quality metrics. This closed-loop cycle enables continuous
learning: low-confidence outputs trigger automated re-generation and model
updates, progressively enhancing question difficulty and relevance. Moreover,
we set the quality metrics as a tunable hyperparameter, enabling question
generation at controllable and customizable difficulty levels. We release
AraLongBench, a large-scale Arabic benchmark of single- and multi-page
challenges spanning hundreds of pages, and demonstrate that our self-evolving
workflow substantially outperform static pipelines, markedly boosting the
long-context comprehension capabilities of leading Arabic Large Vision Language
Models (LVLMs). Lastly, we also meticulously architect a fully automated
agentic workflow for long-context Arabic document collection.

</details>


### [13] [Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets](https://arxiv.org/abs/2509.02908)
*Santosh Chapagain,Cory J Cascalheira,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi,Jillian R. Scheer*

Main category: cs.CL

TL;DR: 本研究首次系统评估了多种 transformer 及图增强模型对性与性别少数群体压力的在线识别效果。实验证明，图结构能有效提升检测能力，尤其是细致捕捉身份隐瞒、内化污名和求助信号，对公共健康和数字干预具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 性少数群体在健康和心理障碍方面面临更高的风险，这主要与少数群体压力相关，急需有效的在线识别与干预手段。

Method: 综合评估多种基于transformer的模型（包括ELECTRA、BERT、RoBERTa、BART），与传统机器学习和图增强模型对比，并测试零样本及少样本学习，对大规模Reddit语料库进行多次实验。

Result: 加入图结构显著提升了基于transformer的模型在检测少数群体压力上的表现，监督微调结合关系上下文优于零/少样本方法。

Conclusion: 图增强的transformer模型在识别少数群体压力相关语言标志上表现最优，为数字健康干预和公共健康政策等实际应用提供了可靠基础。

Abstract: Individuals from sexual and gender minority groups experience
disproportionately high rates of poor health outcomes and mental disorders
compared to their heterosexual and cisgender counterparts, largely as a
consequence of minority stress as described by Meyer's (2003) model. This study
presents the first comprehensive evaluation of transformer-based architectures
for detecting minority stress in online discourse. We benchmark multiple
transformer models including ELECTRA, BERT, RoBERTa, and BART against
traditional machine learning baselines and graph-augmented variants. We further
assess zero-shot and few-shot learning paradigms to assess their applicability
on underrepresented datasets. Experiments are conducted on the two largest
publicly available Reddit corpora for minority stress detection, comprising
12,645 and 5,789 posts, and are repeated over five random seeds to ensure
robustness. Our results demonstrate that integrating graph structure
consistently improves detection performance across transformer-only models and
that supervised fine-tuning with relational context outperforms zero and
few-shot approaches. Theoretical analysis reveals that modeling social
connectivity and conversational context via graph augmentation sharpens the
models' ability to identify key linguistic markers such as identity
concealment, internalized stigma, and calls for support, suggesting that
graph-enhanced transformers offer the most reliable foundation for digital
health interventions and public health policy.

</details>


### [14] [English Pronunciation Evaluation without Complex Joint Training: LoRA Fine-tuned Speech Multimodal LLM](https://arxiv.org/abs/2509.02915)
*Taekyung Ahn,Hosung Nam*

Main category: cs.CL

TL;DR: 用LoRA技术微调的多模态大模型，无须结构改动即可高效同时完成发音自动评估和错误检测，且表现优异，简化了训练流程，推动了综合发音训练系统的发展。


<details>
  <summary>Details</summary>
Motivation: 自动语音评估和错误检测一直是语音学习领域的重要但复杂任务，常需分别训练不同模型且过程繁琐。作者希望通过简化模型和训练流程，实现这两项任务的集成，提高效率和应用可行性。

Method: 采用微软Phi-4-multimodal-instruct多模态大模型，在无需结构调整的情况下，使用LoRA（低秩适应）对模型进行微调（只微调LoRA层）。模型在Speechocean762数据集上进行微调和评估。

Result: 模型预测得分与人工评分的皮尔逊相关系数大于0.7，并且词错误率和音素错误率均小于0.15。仅微调LoRA层即可达到与全部音频层微调相当的性能。

Conclusion: LoRA微调大模型，无需复杂结构改变即可实现发音自动评估和错误检测、诊断任务集成，训练过程更简单高效。为英语第二语言学习者的发音训练技术发展提供了更易用的解决方案。

Abstract: This study demonstrates that a Multimodal Large Language Model (MLLM) adapted
via Low-Rank Adaptation (LoRA) can perform both Automatic Pronunciation
Assessment (APA) and Mispronunciation Detection and Diagnosis (MDD)
simultaneously. Leveraging Microsoft's Phi-4-multimodal-instruct, our
fine-tuning method eliminates the need for complex architectural changes or
separate training procedures conventionally required for these distinct tasks.
Fine-tuned on the Speechocean762 dataset, the pronunciation evaluation scores
predicted by the model exhibited a strong Pearson Correlation Coefficient (PCC
> 0.7) with human-assigned scores, while achieving low Word Error Rate (WER)
and Phoneme Error Rate (PER) (both < 0.15). Notably, fine-tuning only the LoRA
layers was sufficient to achieve performance levels comparable to those
achieved by fine-tuning all audio layers. This research highlights that an
integrated pronunciation assessment system can be established by adapting large
multimodal models without full fine-tuning, utilizing a significantly simpler
training methodology compared to previous joint models designed for
simultaneous APA and MDD. This efficient LoRA-based approach paves the way for
more accessible, integrated, and effective Computer-Assisted Pronunciation
Training (CAPT) technologies for English L2 learners.

</details>


### [15] [Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities](https://arxiv.org/abs/2509.02926)
*Youngwoo Kim,Himanshu Beniwal,Steven L. Johnson,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 该研究通过分析历史审核数据，自动提取并建模了各社区内容审核中的隐性标准，发现社区间对规范的实际执行差异显著，并为审核系统的透明化提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 在线社区（如subreddit）内容审核标准经常不明确，存在多样、隐性的标准。缺乏显式标准会带来审核的不一致性，这促使研究者希望能系统地识别和提取这些隐性标准。

Method: 提出一种可解释性架构，从历史审核数据中自动识别与内容移除相关的词汇表达，并以得分表的形式表示，将这些得分表作为各社区的审核标准。通过实验评估这些结构与神经网络审核模型的性能比较。

Result: 提取出的词汇模式不仅能够有效模拟神经网络审核模型的性能，还能带来更透明的决策解释。结果矩阵揭示了各社区对看似一致的规范有显著不同的实际执行方式，包括语言容忍度、特定话题限制特征及有害言论的细分类标准。

Conclusion: 本文方法为不同社区审核标准的系统揭示和比较提供了工具，既解释了已有的历史审核决策，也发现了许多未被记录的、社区特有的审核模式与规范。

Abstract: Effective content moderation systems require explicit classification
criteria, yet online communities like subreddits often operate with diverse,
implicit standards. This work introduces a novel approach to identify and
extract these implicit criteria from historical moderation data using an
interpretable architecture. We represent moderation criteria as score tables of
lexical expressions associated with content removal, enabling systematic
comparison across different communities. Our experiments demonstrate that these
extracted lexical patterns effectively replicate the performance of neural
moderation models while providing transparent insights into decision-making
processes. The resulting criteria matrix reveals significant variations in how
seemingly shared norms are actually enforced, uncovering previously
undocumented moderation patterns including community-specific tolerances for
language, features for topical restrictions, and underlying subcategories of
the toxic speech classification.

</details>


### [16] [ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly](https://arxiv.org/abs/2509.02949)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Susan Holm,Yuran Wang,Vincent Zhou,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: 提出了专注装配场景的多模态QA数据集ProMQA-Assembly，通过半自动流程构建并进行模型评测，发现现有模型仍有改进空间，数据集有助于推动流程型助手智能体发展。


<details>
  <summary>Details</summary>
Motivation: 当前缺少支持实用场景（特别是装配领域）应用导向系统评估的测试基准，影响了助理智能体在装配任务中的发展。

Method: 提出了ProMQA-Assembly多模态QA数据集，涵盖装配活动所需的人体活动录像及说明书理解，采用大模型生成候选问答对并由人工验证的半自动化标注流程，结合细粒度动作标签以丰富问题类型。此外，建立装配玩具车辆的指令任务图用于标注和基准测试。

Result: 创建了391对多模态QA对及配套任务图，并对现有多模态模型进行了基准测试，发现它们在该数据集上的表现有较大提升空间。

Conclusion: 所提出的数据集能有效促进面向流程活动的智能助手系统发展，为后续研究提供更具实用价值的测试数据。

Abstract: Assistants on assembly tasks have a large potential to benefit humans from
everyday tasks to industrial settings. However, no testbeds support
application-oriented system evaluation in a practical setting, especially in
assembly. To foster the development, we propose a new multimodal QA dataset on
assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs
that require the multimodal understanding of human-activity recordings and
their instruction manuals in an online-style manner. In the development, we
adopt a semi-automated QA annotation approach, where LLMs generate candidates
and humans verify them, as a cost-effective method, and further improve it by
integrating fine-grained action labels to diversify question types.
Furthermore, we create instruction task graphs for the target tasks of
assembling toy vehicles. These newly created task graphs are used in our
benchmarking experiment, as well as to facilitate the human verification
process in the QA annotation. Utilizing our dataset, we benchmark models,
including competitive proprietary multimodal models. Our results suggest great
room for improvement for the current models. We believe our new evaluation
dataset can contribute to the further development of procedural-activity
assistants.

</details>


### [17] [DiaCBT: A Long-Periodic Dialogue Corpus Guided by Cognitive Conceptualization Diagram for CBT-based Psychological Counseling](https://arxiv.org/abs/2509.02999)
*Yougen Zhou,Ningning Zhou,Qin Chen,Jie Zhou,Aimin Zhou,Liang He*

Main category: cs.CL

TL;DR: 本文构建并公开了一个高质量CBT心理咨询对话数据集（DiaCBT），并用其训练和评测了心理咨询对话模型，效果显著，有潜力在实际心理健康服务中应用。


<details>
  <summary>Details</summary>
Motivation: 传统心理治疗难以普及，主要由于社会污名和治疗师资源有限。本文希望通过大型语言模型（LLMs）辅助心理健康服务，扩大心理治疗的可及性，但受限于缺乏高质量心理对话数据集。

Method: 构建了一套基于认知行为疗法（CBT）的长周期心理咨询对话语料库，涵盖多轮咨询内容，并引入了认知概念化图谱（CCD）以模拟多元情景。使用该语料库训练心理咨询模型，并设计详尽的评估框架，将模型表现与CBT标准进行对比评估。

Result: 结果显示，DiaCBT能有效提升LLMs模拟专业CBT心理咨询师的能力。该模型在CBT相关的评测标准上表现优异。

Conclusion: DiaCBT数据集和模型为训练更专业、更有效的心理咨询对话代理提供了重要基础，有望扩展心理健康服务的覆盖面。

Abstract: Psychotherapy reaches only a small fraction of individuals suffering from
mental disorders due to social stigma and the limited availability of
therapists. Large language models (LLMs), when equipped with professional
psychotherapeutic skills, offer a promising solution to expand access to mental
health services. However, the lack of psychological conversation datasets
presents significant challenges in developing effective psychotherapy-guided
conversational agents. In this paper, we construct a long-periodic dialogue
corpus for counseling based on cognitive behavioral therapy (CBT). Our curated
dataset includes multiple sessions for each counseling and incorporates
cognitive conceptualization diagrams (CCDs) to guide client simulation across
diverse scenarios. To evaluate the utility of our dataset, we train an in-depth
counseling model and present a comprehensive evaluation framework to benchmark
it against established psychological criteria for CBT-based counseling. Results
demonstrate that DiaCBT effectively enhances LLMs' ability to emulate
psychologists with CBT expertise, underscoring its potential for training more
professional counseling agents.

</details>


### [18] [Mitigating Data Imbalance in Automated Speaking Assessment](https://arxiv.org/abs/2509.03010)
*Fong-Chun Tsai,Kuan-Tang Huang,Bi-Cheng Yan,Tien-Hong Lo,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了BLV loss来解决ASA模型的类别不平衡问题，并且在BERT上取得了更高的准确率和公平性，改进了自动口语评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动口语评估系统在面对类别不平衡（即学习者水平分布不均）时，预测结果常常会出现偏差，导致评估不公平。为此，作者希望通过优化模型目标来缓解这一问题，提高评估的准确性和多样性学生的公平性。

Method: 提出了一种新的损失函数BLV loss，通过扰动模型预测结果来提升对少数类别的特征表示能力，无需修改原始数据集，并将其集成到经典的BERT文本模型中进行实验。

Result: 在ICNALE基准数据集上实验结果表明，集成BLV loss后，BERT模型的分类准确率和公平性均有明显提升，使自动化口语评估系统对不同学习者更加健壮。

Conclusion: 引入Balancing Logit Variation（BLV）损失能够有效提升自动口语评估模型在少数类别上的表现，从而改善模型的公平性和准确性。

Abstract: Automated Speaking Assessment (ASA) plays a crucial role in evaluating
second-language (L2) learners proficiency. However, ASA models often suffer
from class imbalance, leading to biased predictions. To address this, we
introduce a novel objective for training ASA models, dubbed the Balancing Logit
Variation (BLV) loss, which perturbs model predictions to improve feature
representation for minority classes without modifying the dataset. Evaluations
on the ICNALE benchmark dataset show that integrating the BLV loss into a
celebrated text-based (BERT) model significantly enhances classification
accuracy and fairness, making automated speech evaluation more robust for
diverse learners.

</details>


### [19] [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://arxiv.org/abs/2509.03020)
*Chang Su,Dengliang Shi,Siyuan Huang,Jintao Du,Changhua Meng,Yu Cheng,Weiqiang Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 作者提出了一种新训练策略，通过补充性生成重构任务（EBQ2D和EBD2Q）来丰富LLM最后标记嵌入的语义，在多个基线和规模下刷新了文本嵌入领域的性能极限。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLM）的文本嵌入方法通常利用最后一个特殊标记（如[EOS]）的嵌入，但这些标记没有被有意识地训练用于捕获整体语义，这限制了其作为文本嵌入的能力，尤其是在检索与重排序任务上。

Method: 作者提出在对比学习之前增加新的训练阶段，通过双向生成式重构任务（EBQ2D与EBD2Q），强化[EOS]标记的语义表达力。这些任务用于互补地锚定[EOS]嵌入并重构查询-文档对的任一侧。

Result: 实验结果显示，该额外训练阶段显著提升了LLM在MTEB上的表现，并且在不同模型与规模下达到了新的SOTA（最先进）成绩。

Conclusion: 引入新的训练阶段和生成重构任务能够有效提升LLM文本嵌入的语义能力，对下游任务如检索和重排序有显著帮助。

Abstract: Large language models (LLMs) have increasingly been explored as powerful text
embedders. Existing LLM-based text embedding approaches often leverage the
embedding of the final token, typically a reserved special token such as [EOS].
However, these tokens have not been intentionally trained to capture the
semantics of the whole context, limiting their capacity as text embeddings,
especially for retrieval and re-ranking tasks. We propose to add a new training
stage before contrastive learning to enrich the semantics of the final token
embedding. This stage employs bidirectional generative reconstruction tasks,
namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based
Document-to-Query), which interleave to anchor the [EOS] embedding and
reconstruct either side of Query-Document pairs. Experimental results
demonstrate that our additional training stage significantly improves LLM
performance on the Massive Text Embedding Benchmark (MTEB), achieving new
state-of-the-art results across different LLM base models and scales.

</details>


### [20] [Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models](https://arxiv.org/abs/2509.03057)
*Ming Gong,Yingnan Deng,Nia Qi,Yujun Zou,Zhihao Xue,Yun Zi*

Main category: cs.CL

TL;DR: 本论文提出了一种结构可学习的adapter微调方法，自动优化模型结构，应对多任务环境。实验显示该方法在准确率、压缩和鲁棒性上均超过主流高效微调技术，效果突出。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型微调过程中存在参数冗余、结构僵化和任务适应性有限等问题。该文旨在改进微调方法，以提高模型在多任务环境下的灵活性和效率。

Method: 提出了一种基于adapter的结构可学习微调方法。引入可微分门控函数和结构稀疏性控制变量，能够自动优化adapter的插入位置、激活路径和模块组合。主干参数保持冻结，通过结构搜索机制，在训练过程中动态构建适应特定任务的高效子结构。还设计了灵敏度分析实验，系统评估稀疏权重、噪声注入比例、数据扰动对模型性能的影响。

Result: 实验表明，该方法在多个多任务自然语言理解任务上优于主流高效参数微调技术。在准确率、压缩率、抗噪和抗扰动性之间实现了更佳平衡。

Conclusion: 结构可学习的adapter微调方法通过高效结构搜索和自动适配机制，提升了参数利用率和表达能力，并具备稳定性与鲁棒性。对此类任务有更强适应性和潜力。

Abstract: This paper addresses the issues of parameter redundancy, rigid structure, and
limited task adaptability in the fine-tuning of large language models. It
proposes an adapter-based fine-tuning method built on a structure-learnable
mechanism. By introducing differentiable gating functions and structural
sparsity control variables, the method enables automatic optimization of
adapter insertion points, activation paths, and module combinations. This
allows the model to adjust its structure flexibly in multi-task settings to
match different task characteristics. With the backbone parameters kept frozen,
the method uses a structure search mechanism to guide the dynamic construction
of task-specific efficient substructures during training. This significantly
improves parameter utilization and representational capacity. In addition, the
paper designs a set of sensitivity analysis experiments to systematically
evaluate the effects of sparsity weight, noise injection ratio, and data
perturbation on model performance. These experiments verify the stability and
robustness of the proposed method across various multi-task natural language
understanding tasks. The experimental results show that the proposed method
outperforms mainstream parameter-efficient tuning techniques on multiple tasks.
It achieves a better balance among accuracy, compression rate, and robustness
to noise and perturbation.

</details>


### [21] [A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network](https://arxiv.org/abs/2509.03060)
*Md. Jahidul Islam Razin,Md. Abdul Karim,M. F. Mridha,S M Rafiuddin,Tahira Alam*

Main category: cs.CL

TL;DR: 本文提出一种基于LSTM的商业情感分析方法，解决了常规RNN的梯度消失问题，在产品评论数据集上表现优异，准确率达91.33%，可助力企业优化营销策略。


<details>
  <summary>Details</summary>
Motivation: 现有的情感分析方法在商业领域应用广泛，但常规RNN模型存在梯度消失问题，影响准确性。本文旨在通过改进模型提升商业情感分析的效果。

Method: 采用长短时记忆网络（LSTM）作为核心方法，对产品评论数据集进行情感分析。数据分为70%训练和30%测试进行实验，并与传统RNN模型进行对比。

Result: 改进型LSTM模型取得了约91.33％的准确率，超过了常规RNN模型，实现了更高效的业务反馈识别。

Conclusion: 提出的LSTM（改进型RNN）模型在商业情感分析方面表现优异，准确率达到约91.33%，优于其他常规RNN模型。

Abstract: Business sentiment analysis (BSA) is one of the significant and popular
topics of natural language processing. It is one kind of sentiment analysis
techniques for business purposes. Different categories of sentiment analysis
techniques like lexicon-based techniques and different types of machine
learning algorithms are applied for sentiment analysis on different languages
like English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM)
is applied for business sentiment analysis, where a recurrent neural network is
used. An LSTM model is used in a modified approach to prevent the vanishing
gradient problem rather than applying the conventional recurrent neural network
(RNN). To apply the modified RNN model, product review dataset is used. In this
experiment, 70\% of the data is trained for the LSTM and the rest 30\% of the
data is used for testing. The result of this modified RNN model is compared
with other conventional RNN models, and a comparison is made among the results.
It is noted that the proposed model performs better than the other conventional
RNN models. Here, the proposed model, i.e., the modified RNN model approach has
achieved around 91.33\% of accuracy. By applying this model, any business
company or e-commerce business site can identify the feedback from their
customers about different types of products that customers like or dislike.
Based on the customer reviews, a business company or e-commerce platform can
evaluate its marketing strategy.

</details>


### [22] [Measuring Scalar Constructs in Social Science with LLMs](https://arxiv.org/abs/2509.03116)
*Hauke Licht,Rupak Sarkar,Patrick Y. Wu,Pranav Goel,Niklas Stoehr,Elliott Ash,Alexander Miserlis Hoyle*

Main category: cs.CL

TL;DR: 此文系统评估了LLM用于社会科学连续构量测量的四种方法，发现微调及概率加权能显著优于简单打分，给出了实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究语言特征（如复杂性、情感性）通常具备连续性的语义结构，而现有大型语言模型（LLMs）输出在数值处理上有独特方式，如何更有效利用其测量连续变量成为问题。

Method: 综合评估了四种基于LLM的社会科学连续结构测量方法：直接点值评分、成对比较聚合、基于token概率加权的点值评分，以及微调模型。

Result: 通过多政治科学数据集实证，直接点值评分会导致分布不连续，且出现分数集中现象。成对比较能提升测量质量，而采用token概率加权的点值评分提升更显著。最后，微调小模型仅需约1000组训练数据即可超越或匹配Prompt式LLM性能。

Conclusion: 建议研究者在实际社会科学连续变量测量中，优先考虑成对比较与token概率加权，并采用微调以提升模型表现，避免直接点值评分带来的不良分布影响。

Abstract: Many constructs that characterize language, like its complexity or
emotionality, have a naturally continuous semantic structure; a public speech
is not just "simple" or "complex," but exists on a continuum between extremes.
Although large language models (LLMs) are an attractive tool for measuring
scalar constructs, their idiosyncratic treatment of numerical outputs raises
questions of how to best apply them. We address these questions with a
comprehensive evaluation of LLM-based approaches to scalar construct
measurement in social science. Using multiple datasets sourced from the
political science literature, we evaluate four approaches: unweighted direct
pointwise scoring, aggregation of pairwise comparisons,
token-probability-weighted pointwise scoring, and finetuning. Our study yields
actionable findings for applied researchers. First, LLMs prompted to generate
pointwise scores directly from texts produce discontinuous distributions with
bunching at arbitrary numbers. The quality of the measurements improves with
pairwise comparisons made by LLMs, but it improves even more by taking
pointwise scores and weighting them by token probability. Finally, finetuning
smaller models with as few as 1,000 training pairs can match or exceed the
performance of prompted LLMs.

</details>


### [23] [From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: 论文提出用知识编辑替代传统指令微调，将指纹注入大语言模型以保护知识产权，并创新性提出FSFT方法，显著提升指纹稳定性和持久性，但区分指纹与相似文本仍是难点。


<details>
  <summary>Details</summary>
Motivation: 现有通过指令微调注入指纹的方案存在明显弊端，包括模型性能下降、计算开销大和指纹容易失效。因此，论文探索更轻量且更持久的指纹注入方法，以增强模型知识产权保护。

Method: 首次将知识编辑方法应用于大语言模型的指纹注入，并提出了Fingerprint Subspace-aware Fine-Tuning（FSFT）方法，通过限制特定子空间的参数更新，减少了指纹的退化。与此同时，分析了注入指纹模型在面临大规模微调时的表现和区分能力。

Result: 知识编辑相比传统微调在指纹注入上更加高效且持久。提出的FSFT方法可在极端情况下，指纹准确率领先普通微调10%。但指纹模型对指纹和相似内容的区分仍有困难，凸显出改进需求。

Conclusion: 通过知识编辑方法注入指纹，为大语言模型（LLMs）知识产权保护提供了一种高效、持久的解决方案。提出的FSFT方法能显著降低指纹退化，表现优于传统微调方法。当前指纹模型在区分指纹和相似文本上存在局限，亟需更精细的指纹注入技术。

Abstract: The intellectual property (IP) protection of Large Language Models (LLMs) is
increasingly critical. Injecting specialized fingerprints into LLMs through
instruction tuning is a common IP protection technique. However, this may
significantly degrade model performance, requires substantial computational
resources, and exhibits poor persistence under model modifications. We argue
that knowledge editing offers a lightweight alternative that is more suitable
for fingerprint injection. Accordingly, we apply knowledge editing to
fingerprint injection for the first time and demonstrate its strong capability.
Despite using scrambled text as fingerprints to prevent them from being
overwritten during fine-tuning, degradation still occurs under large-scale
fine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning
(FSFT), which reduces fingerprint degradation by constraining the update of the
fingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even
in the worst-case scenario. Additionally, we observe that the
fingerprint-injected models struggle to distinguish between fingerprints and
similar texts due to the high similarity of their features. This finding
underscores the urgent need for more robust and fine-grained fingerprinting
injection methods for LLMs.

</details>


### [24] [An experimental and computational study of an Estonian single-person word naming](https://arxiv.org/abs/2509.03143)
*Kaidi Lõo,Arvi Tavast,Maria Heitmeier,Harald Baayen*

Main category: cs.CL

TL;DR: 本文通过结合词汇命名和眼动追踪，发现基于判别词典模型的语义指标对词汇加工有较强预测力，但不一定优于传统词汇变量，也揭示了词义在词汇命名任务中的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨爱沙尼亚语词汇加工机制，特别关注心理词典的计算模型（判别词典模型，DLM）对词汇加工的预测作用，并与传统变量如词频、邻近词数量、屈折范式规模等进行比较。

Method: 通过大规模单被试实验，结合词汇命名任务与眼动追踪技术。分析了五个反应变量（首次注视时长、总注视时长、注视次数、词汇命名潜伏期、口语词时长），使用广义可加模型（GAM）建模。比较了DLM计算模型在采用线性与深度映射时的预测能力，并与传统变量进行比对。

Result: DLM模型的预测指标对词汇加工有强解释力。采用深度学习的DLM指标未必比线性映射更优。传统预测变量整体拟合效果略优（除总注视时长，两者相当）。词汇变量不能很好预测首次注视时长与注视次数。DLM指标对总注视时长、命名潜伏期、口语词时长的预测性，表明词义参与了命名任务的加工过程。

Conclusion: 判别词典模型的指标是预测词汇加工的重要变量，且深度学习映射并不一定优于线性模型。传统变量一般预测更准，但DLM指标可揭示意义加工的重要作用。爱沙尼亚语词汇命名任务中，词汇变量对部分眼动指标的预测作用有限，而意义参与了反应时等加工。

Abstract: This study investigates lexical processing in Estonian. A large-scale
single-subject experiment is reported that combines the word naming task with
eye-tracking. Five response variables (first fixation duration, total fixation
duration, number of fixations, word naming latency, and spoken word duration)
are analyzed with the generalized additive model. Of central interest is the
question of whether measures for lexical processing generated by a
computational model of the mental lexicon (the Discriminative Lexicon Model,
DLM) are predictive for these response variables, and how they compare to
classical predictors such as word frequency, neighborhood size, and
inflectional paradigm size. Computational models were implemented both with
linear and deep mappings. Central findings are, first, that DLM-based measures
are powerful predictors for lexical processing, second, that DLM-measures using
deep learning are not necessarily more precise predictors of lexical processing
than DLM-measures using linear mappings, third, that classical predictors tend
to provide somewhat more precise fits compared to DLM-based predictors (except
for total fixation duration, where the two provide equivalent goodness of fit),
and fourth, that in the naming task lexical variables are not predictive for
first fixation duration and the total number of fixations. As the DLM works
with mappings from form to meaning, the predictivity of DLM-based measures for
total fixation duration, naming latencies, and spoken word duration indicates
that meaning is heavily involved in the present word naming task.

</details>


### [25] [Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader](https://arxiv.org/abs/2509.03148)
*Jannis Vamvas,Ignacio Pérez Prat,Not Battesta Soliva,Sandra Baltermia-Guetg,Andrina Beeli,Simona Beeli,Madlaina Capeder,Laura Decurtins,Gian Peder Gregori,Flavia Hobi,Gabriela Holderegger,Arina Lazzarini,Viviana Lazzarini,Walter Rosselli,Bettina Vital,Anna Rutkiewicz,Rico Sennrich*

Main category: cs.CL

TL;DR: 本论文构建了Romansh六种变体的机器翻译评测基准，支持与多语言对齐并收集了高质量人工译文。实验表明，Romansh译出德语效果良好，但译入Romansh仍有不少难题。


<details>
  <summary>Details</summary>
Motivation: Romansh语是一种在瑞士使用但机器翻译资源有限的小语种。为推进其机器翻译评测，亟需构建适用于该语种的评价基准。

Method: 本研究为Romansh语的六个变体（Rumantsch Grischun及五个地区变体：Sursilvan、Sutsilvan、Surmiran、Puter、Vallader）建立了基准数据。参考译文由人工翻译，并基于WMT24++ benchmark，以确保与其他55种语言的译文对齐。随后对现有机器翻译系统和大型语言模型进行了自动评价。

Result: 自动评价显示，所有Romansh变体译出至德语的表现较好，而德语译入Romansh仍具较大挑战性。

Conclusion: 现有MT系统在Romansh译出方面已较成熟，但Romansh译入方面需进一步提升，所建设的多变体基准为未来Romansh相关机器翻译研究奠定了评价基础。

Abstract: The Romansh language, spoken in Switzerland, has limited resources for
machine translation evaluation. In this paper, we present a benchmark for six
varieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five
regional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our
reference translations were created by human translators based on the WMT24++
benchmark, which ensures parallelism with more than 55 other languages. An
automatic evaluation of existing MT systems and LLMs shows that translation out
of Romansh into German is handled relatively well for all the varieties, but
translation into Romansh is still challenging.

</details>


### [26] [Domain Adaptation of LLMs for Process Data](https://arxiv.org/abs/2509.03161)
*Rafael Seidi Oyamada,Jari Peeperkorn,Jochen De Weerdt,Johannes De Smedt*

Main category: cs.CL

TL;DR: 本文提出直接对LLM进行高效微调以处理流程挖掘中的非自然语言数据，无需叙述式转化。结果显示，这种方法在预测能力和训练效率上优于传统方法，特别适用于多任务预测。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在流程挖掘（PM）中已经得到应用，但现有方法大多依赖prompt engineering或将事件日志转为叙述式数据集。本文动机在于探索LLMs直接处理非自然语言数据的可行性，基于LLMs生成token序列的能力与PM任务目标的相似性。

Method: 本文采用参数高效的微调技术，通过不将流程数据转化为自然语言，直接对预训练LLM进行适应。实验聚焦于预测性流程监控（PPM），针对单任务和多任务两种预测场景进行对比评估。

Result: 实验结果显示，本文方法在预测性能上优于RNN等主流模型和基于叙述式数据的方法，特别是在多任务场景下。此外，微调后的模型收敛更快，对超参数的需求明显减少。

Conclusion: 大型语言模型可以无需自然语言重构，直接应用于流程数据，并在参数高效微调后，在预测性流程监控任务上表现优异，尤其适合多任务环境，并具有更高训练效率。

Abstract: In recent years, Large Language Models (LLMs) have emerged as a prominent
area of interest across various research domains, including Process Mining
(PM). Current applications in PM have predominantly centered on prompt
engineering strategies or the transformation of event logs into narrative-style
datasets, thereby exploiting the semantic capabilities of LLMs to address
diverse tasks. In contrast, this study investigates the direct adaptation of
pretrained LLMs to process data without natural language reformulation,
motivated by the fact that these models excel in generating sequences of
tokens, similar to the objective in PM. More specifically, we focus on
parameter-efficient fine-tuning techniques to mitigate the computational
overhead typically associated with such models. Our experimental setup focuses
on Predictive Process Monitoring (PPM), and considers both single- and
multi-task predictions. The results demonstrate a potential improvement in
predictive performance over state-of-the-art recurrent neural network (RNN)
approaches and recent narrative-style-based solutions, particularly in the
multi-task setting. Additionally, our fine-tuned models exhibit faster
convergence and require significantly less hyperparameter optimization.

</details>


### [27] [SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala](https://arxiv.org/abs/2509.03162)
*Ashmari Pramodya,Nirasha Nelki,Heshan Shalinda,Chamila Liyanage,Yusuke Sakai,Randil Pushpananda,Ruvan Weerasinghe,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 首次提出针对僧伽罗语的大型多项选择题数据集SinhalaMMLU，评测26款LLM发现整体表现有限，尤其在人文学科等文化相关题目上，强调了LLM在低资源和文化特定场景下的明显改进需求。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的评测主要集中在全球性或英语为中心的主题，忽视了低资源语言和文化特定内容。现有多语言基准多依赖自动翻译，造成错误并曲解文化语境。

Method: 提出SinhalaMMLU，这是首个专为僧伽罗语（低资源语言）设计的多项选择题问答基准，涵盖7000多道符合斯里兰卡国家课程的问题，覆盖6大领域和30个学科。

Result: 对26个LLM进行测试，Claude 3.5 sonnet和GPT-4o平均准确率分别为67%和62%，但整体表现有限。模型在文化相关领域（如人文学科）表现较差。

Conclusion: 当前LLM在低资源和文化特定背景下的适应性显著不足，尤其在人文学科等文化丰富领域，改进空间巨大。

Abstract: Large Language Models (LLMs) demonstrate impressive general knowledge and
reasoning abilities, yet their evaluation has predominantly focused on global
or anglocentric subjects, often neglecting low-resource languages and
culturally specific content. While recent multilingual benchmarks attempt to
bridge this gap, many rely on automatic translation, which can introduce errors
and misrepresent the original cultural context. To address this, we introduce
SinhalaMMLU, the first multiple-choice question answering benchmark designed
specifically for Sinhala, a low-resource language. The dataset includes over
7,000 questions spanning secondary to collegiate education levels, aligned with
the Sri Lankan national curriculum, and covers six domains and 30 subjects,
encompassing both general academic topics and culturally grounded knowledge. We
evaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and
GPT-4o achieve the highest average accuracies at 67% and 62% respectively,
overall model performance remains limited. In particular, models struggle in
culturally rich domains such as the Humanities, revealing substantial room for
improvement in adapting LLMs to low-resource and culturally specific contexts.

</details>


### [28] [Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge](https://arxiv.org/abs/2509.03256)
*Aleksei Žavoronkov,Tanel Alumäe*

Main category: cs.CL

TL;DR: 本文为挪威语二语儿童发音测评挑战提出三种新模型，在标准任务上GOP-CTC新模型效果最好，超越了基线和竞赛排行榜。


<details>
  <summary>Details</summary>
Motivation: 针对儿童挪威语二语学习者的自动单词级发音测评任务，开发高效模型以提升发音质量评估的准确性和实用性。

Method: 提出了三种端到端模型：基于编码器-解码器的Siamese结构（E2E-R），利用wav2vec2.0表示并进行前缀微调的直接分类模型，以及融合无对齐GOP特征（用CTC方法计算）的新型模型。同时引入加权有序交叉熵损失以优化评价指标。

Result: GOP-CTC模型获得了最佳性能，大幅领先挑战基线，在排行榜上名列前茅。

Conclusion: 采用GOP-CTC特征的模型在NOCASA 2025挑战中表现最佳，显著超过基线并进入排行榜前列。

Abstract: This paper presents an analysis of three end-to-end models developed for the
NOCASA 2025 Challenge, aimed at automatic word-level pronunciation assessment
for children learning Norwegian as a second language. Our models include an
encoder-decoder Siamese architecture (E2E-R), a prefix-tuned direct
classification model leveraging pretrained wav2vec2.0 representations, and a
novel model integrating alignment-free goodness-of-pronunciation (GOP) features
computed via CTC. We introduce a weighted ordinal cross-entropy loss tailored
for optimizing metrics such as unweighted average recall and mean absolute
error. Among the explored methods, our GOP-CTC-based model achieved the highest
performance, substantially surpassing challenge baselines and attaining top
leaderboard scores.

</details>


### [29] [LatPhon: Lightweight Multilingual G2P for Romance Languages and English](https://arxiv.org/abs/2509.03300)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: 提出了一种高效小巧的多语种G2P Transformer模型LatPhon，在六种拉丁语系语言的公开数据集上取得低音素错误率，模型容量小，适合实际落地应用。


<details>
  <summary>Details</summary>
Motivation: G2P（字母到音素转换）在多种语音系统的前端中都非常重要，尤其是在多语种环境下，但现有方法在精度、模型体积及多语种统一性之间存在权衡，亟需小巧且高效的多语种G2P方法。

Method: 提出了LatPhon，这是一种仅有7.5M参数的Transformer模型，通过联合训练六种拉丁语系语言（英语、西班牙语、法语、意大利语、葡萄牙语和罗马尼亚语）。在IPA-dict公开语料库上进行了性能测试，并与ByT5和WFSTs方法进行对比。

Result: LatPhon模型在IPA-dict上的平均音素错误率达到3.5%，优于ByT5基线模型（5.4%），接近语言特定的WFSTs模型（3.2%），且模型体积仅30MB，利于部署到本地设备。

Conclusion: LatPhon展示了一个小巧、高效且多语种通用的G2P方案，能作为拉丁语种语音系统的统一前端，兼顾精度与部署便捷性。

Abstract: Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech
(TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST)
and alignment systems, especially across multiple Latin-script languages.We
present LatPhon, a 7.5 M - parameter Transformer jointly trained on six such
languages--English, Spanish, French, Italian, Portuguese, and Romanian. On the
public ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%,
outperforming the byte-level ByT5 baseline (5.4%) and approaching
language-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes
on-device deployment feasible when needed. These results indicate that compact
multilingual G2P can serve as a universal front-end for Latin-language speech
pipelines.

</details>


### [30] [AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?](https://arxiv.org/abs/2509.03312)
*Guibin Zhang,Junhao Wang,Junjie Chen,Wangchunshu Zhou,Kun Wang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文针对多LLM智能体系统故障定位准确率低的问题，提出了AgenTracer框架与AgenTracer-8B模型，通过新型数据集和方法实现了业界领先的故障归因能力，并有效提升多智能体系统的整体性能和自我改进能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的多Agent系统比单体智能体表现更好，但其复杂性也导致更容易系统性失败，而现有最先进的推理LLM在定位系统失败责任上的准确率极低（低于10%）。

Method: 提出了AgenTracer框架，通过反事实重放和程序化故障注入自动标注多Agent失败轨迹，生成专用数据集TracerTraj；并以此开发轻量级故障追踪模型AgenTracer-8B，结合多粒度强化学习进行训练。

Result: AgenTracer-8B在Who&When基准上超越了大型专有LLM（如Gemini-2.5-Pro和Claude-4-Sonnet），准确率提高最多18.18%。在实际多Agent系统（如MetaGPT和MaAS）中的性能提升为4.8-14.2%。

Conclusion: AgenTracer-8B为LLM智能体系统故障归因设立了新标准，并可为现有多Agent系统带来性能提升，推动其自纠与自演化能力。

Abstract: Large Language Model (LLM)-based agentic systems, often comprising multiple
models, complex tool invocations, and orchestration protocols, substantially
outperform monolithic agents. Yet this very sophistication amplifies their
fragility, making them more prone to system failure. Pinpointing the specific
agent or step responsible for an error within long execution traces defines the
task of agentic system failure attribution. Current state-of-the-art reasoning
LLMs, however, remain strikingly inadequate for this challenge, with accuracy
generally below 10%. To address this gap, we propose AgenTracer, the first
automated framework for annotating failed multi-agent trajectories via
counterfactual replay and programmed fault injection, producing the curated
dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a
lightweight failure tracer trained with multi-granular reinforcement learning,
capable of efficiently diagnosing errors in verbose multi-agent interactions.
On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs
like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard
in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers
actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS
with 4.8-14.2% performance gains, empowering self-correcting and self-evolving
agentic AI.

</details>


### [31] [LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations](https://arxiv.org/abs/2509.03405)
*Daniela Gottesman,Alon Gilae-Dotan,Ido Cohen,Yoav Gur-Arieh,Marius Mosbach,Ori Yoran,Mor Geva*

Main category: cs.CL

TL;DR: 本文介绍了LMEnt工具套件，通过实体注释和检索方法，以及多个预训练模型，创造了分析语言模型知识获取的理想环境，推动相关表征、编辑与学习机制的实证研究。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的知识获取过程及其内部机制尚不清晰，理解这些过程能有助于开发更一致、稳健和完整的知识表征模型。

Method: 提出了一个基于实体注释的预训练语料库，以及实体检索方法，并推出了12个不同规模预训练模型和中间检查点。

Result: LMEnt包含知识丰富的语料库、在实体检索方面显著优于原有方法（提升高达80.4%）、以及具备可比性能的多规模预训练模型资源，验证了实体频率对知识学习的重要性。

Conclusion: LMEnt套件为分析语言模型在预训练过程中的知识获取提供了有力工具，支持更深入地研究模型知识表示的形成与变化。

Abstract: Language models (LMs) increasingly drive real-world applications that require
world knowledge. However, the internal processes through which models turn data
into representations of knowledge and beliefs about the world, are poorly
understood. Insights into these processes could pave the way for developing LMs
with knowledge representations that are more consistent, robust, and complete.
To facilitate studying these questions, we present LMEnt, a suite for analyzing
knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a
knowledge-rich pretraining corpus, fully annotated with entity mentions, based
on Wikipedia, (2) an entity-based retrieval method over pretraining data that
outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained
models with up to 1B parameters and 4K intermediate checkpoints, with
comparable performance to popular open-sourced models on knowledge benchmarks.
Together, these resources provide a controlled environment for analyzing
connections between entity mentions in pretraining and downstream performance,
and the effects of causal interventions in pretraining data. We show the
utility of LMEnt by studying knowledge acquisition across checkpoints, finding
that fact frequency is key, but does not fully explain learning trends. We
release LMEnt to support studies of knowledge in LMs, including knowledge
representations, plasticity, editing, attribution, and learning dynamics.

</details>


### [32] [Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning](https://arxiv.org/abs/2509.03407)
*Yarden Tzach,Ronit D. Gross,Ella Koresh,Shalom Rosner,Or Shpringer,Tal Halevi,Ido Kanter*

Main category: cs.CL

TL;DR: 本文系统研究了NLP预训练机制如何提升下游分类任务效果，发现token聚类和高层表征随Transformer层加深而增强，并呈现跨领域应用潜力。研究结合BERT-6在Wiki预训练与FewRel/DBpedia微调，解释了预训练机制、token准确率以及输出信心之间的关系。


<details>
  <summary>Details</summary>
Motivation: 了解NLP预训练成功机制，分析预训练准确率与下游分类微调之间的关联及机制，探索是否有普适性于其它领域（如图像任务）。

Method: BERT-6架构在大规模Wikipedia语料进行预训练，并在FewRel与DBpedia分类任务上进行微调分析；通过token混淆矩阵衡量token聚类和对称性破缺，剖析各Transformer层的性能变化。

Result: 1. token准确率与其出现频率正相关。2. token平均准确率可量化预训练表现，并随Transformer层加深而提高。3. 预训练导致token聚类和强匹配token出现，显著提升输出层性能。4. 高阶语言结构在单token目标下自然涌现。5. 微调性能随Transformer层提升。6. 输出预测置信度与输入token平均准确率无关。7. NLP预训练机制与图像分类微调类似，具备一定普适性。

Conclusion: 预训练不仅能提升NLP模型的泛化能力，还能够在Transformer块内显著提升token聚类、表征和分类性能，其机制可能具有跨领域的普适性。

Abstract: Natural language processing (NLP) enables the understanding and generation of
meaningful human language, typically using a pre-trained complex architecture
on a large dataset to learn the language and next fine-tune its weights to
implement a specific task. Twofold goals are examined; to understand the
mechanism underlying successful pre-training and to determine the interplay
between the pre-training accuracy and the fine-tuning of classification tasks.
The following main results were obtained; the accuracy per token (APT)
increased with its appearance frequency in the dataset, and its average over
all tokens served as an order parameter to quantify pre-training success, which
increased along the transformer blocks. Pre-training broke the symmetry among
tokens and grouped them into finite, small, strong match token clusters, as
inferred from the presented token confusion matrix. This feature was sharpened
along the transformer blocks toward the output layer, enhancing its performance
considerably compared with that of the embedding layer. Consequently,
higher-order language structures were generated by pre-training, even though
the learning cost function was directed solely at identifying a single token.
These pre-training findings were reflected by the improved fine-tuning accuracy
along the transformer blocks. Additionally, the output label prediction
confidence was found to be independent of the average input APT, as the input
meaning was preserved since the tokens are replaced primarily by strong match
tokens. Finally, although pre-training is commonly absent in image
classification tasks, its underlying mechanism is similar to that used in
fine-tuning NLP classification tasks, hinting at its universality. The results
were based on the BERT-6 architecture pre-trained on the Wikipedia dataset and
fine-tuned on the FewRel and DBpedia classification tasks.

</details>


### [33] [Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges](https://arxiv.org/abs/2509.03419)
*Weiyuan Li,Xintao Wang,Siyu Yuan,Rui Xu,Jiangjie Chen,Qingqing Dong,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TL;DR: 提出ComplexEval基准系统，揭示并量化大语言模型评测中辅助信息带来的多种偏差，发现所有模型都易受这些偏差且复杂任务尤甚，对提升评测方法有重要启示。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）能力提升，其评测任务变得更加多样和复杂，现有评测方法难以应对。尤其是在复杂任务中，涉及多维评价标准、非结构化答案和细致标准，评测的可靠性问题亟需研究。

Method: 本文构建了ComplexEval这一基准，系统设计以暴露并量化由辅助信息引入的偏差。研究涵盖了6种之前未探讨的偏差类型，横跨12种基础场景和3种高级场景，来验证和分析模型的表现。

Result: 结果显示，所有评测的大模型都明显易受这些偏差影响，且偏差随任务复杂度增加而加强。尤其是大型推理模型（LRM）在一些场合表现出出人意料的易受偏差影响现象。

Conclusion: 本研究通过对辅助信息诱发偏差的系统分析，为提升评测信号的准确性和可验证性提供了关键见解，为更通用、健壮的评测模型奠定基础。

Abstract: As large language models (LLMs) grow more capable, they face increasingly
diverse and complex tasks, making reliable evaluation challenging. The paradigm
of LLMs as judges has emerged as a scalable solution, yet prior work primarily
focuses on simple settings. Their reliability in complex tasks--where
multi-faceted rubrics, unstructured reference answers, and nuanced criteria are
critical--remains understudied. In this paper, we constructed ComplexEval, a
challenge benchmark designed to systematically expose and quantify Auxiliary
Information Induced Biases. We systematically investigated and validated 6
previously unexplored biases across 12 basic and 3 advanced scenarios. Key
findings reveal: (1) all evaluated models exhibit significant susceptibility to
these biases, with bias magnitude scaling with task complexity; (2) notably,
Large Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth
analysis offers crucial insights for improving the accuracy and verifiability
of evaluation signals, paving the way for more general and robust evaluation
models.

</details>


### [34] [Continuous Saudi Sign Language Recognition: A Vision Transformer Approach](https://arxiv.org/abs/2509.03467)
*Soukeina Elhassen,Lama Al Khuzayem,Areej Alhothali,Ohoud Alzamzami,Nahed Alowaidi*

Main category: cs.CL

TL;DR: 文中提出了首个沙特手语连续语料库，并用ResNet-18+Transformer+BiLSTM模型取得了高识别率，对提升阿拉伯地区手语研究与实际应用意义重大。


<details>
  <summary>Details</summary>
Motivation: 尽管手语对于听障和聋人社群极为重要，但沙特及阿拉伯世界针对本地手语（如沙特手语，SSL）的翻译技术和资源极其稀缺，现有研究多集中于非阿语手语，且数据集多以孤立词汇为主，缺少语句级连续数据。该论文正是为了解决缺乏SSL连续语料和高精度翻译技术的问题。

Method: 构建了首个针对沙特手语的连续语句级数据集（KAU-CSSL），并提出基于Transformer的识别模型：利用预训练ResNet-18提取空间特征，结合Transformer Encoder+双向LSTM建模时序依赖，实现SSL的识别与翻译。

Result: 该系统在说话者依赖模式下识别准确率达99.02%，在说话者独立模式下也达到77.71%的准确率，显著推动了沙特手语识别技术在连续语境下的发展。

Conclusion: 论文首次为沙特手语研究者提供了连续语句级数据集和有效的深度学习识别体系，为SSL及阿拉伯手语方向的研究奠定了基础，也为改善听障群体交流条件、促进社会融合做出了实际贡献。

Abstract: Sign language (SL) is an essential communication form for hearing-impaired
and deaf people, enabling engagement within the broader society. Despite its
significance, limited public awareness of SL often leads to inequitable access
to educational and professional opportunities, thereby contributing to social
exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend
on Saudi Sign Language (SSL) as their primary form of communication. Although
certain technological approaches have helped to improve communication for
individuals with hearing impairments, there continues to be an urgent
requirement for more precise and dependable translation techniques, especially
for Arabic sign language variants like SSL. Most state-of-the-art solutions
have primarily focused on non-Arabic sign languages, resulting in a
considerable absence of resources dedicated to Arabic sign language,
specifically SSL. The complexity of the Arabic language and the prevalence of
isolated sign language datasets that concentrate on individual words instead of
continuous speech contribute to this issue. To address this gap, our research
represents an important step in developing SSL resources. To address this, we
introduce the first continuous Saudi Sign Language dataset called KAU-CSSL,
focusing on complete sentences to facilitate further research and enable
sophisticated recognition systems for SSL recognition and translation.
Additionally, we propose a transformer-based model, utilizing a pretrained
ResNet-18 for spatial feature extraction and a Transformer Encoder with
Bidirectional LSTM for temporal dependencies, achieving 99.02\% accuracy at
signer dependent mode and 77.71\% accuracy at signer independent mode. This
development leads the way to not only improving communication tools for the SSL
community but also making a substantial contribution to the wider field of sign
language.

</details>


### [35] [Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games](https://arxiv.org/abs/2509.03479)
*Haonan Wang,Mingjia Zhao,Junfeng Sun,Wei Liu*

Main category: cs.CL

TL;DR: 本文提出结合深度学习和策略梯度强化学习的新型智能体设计，在文本游戏测试中取得了显著性能提升，为强化学习智能体在广泛领域的应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术和文本游戏研究的进步，传统强化学习智能体在文本游戏中的表现仍有限，因此需要提出新方法以提升其理解和决策能力。

Method: 首先通过深度学习处理游戏文本，构建世界模型；随后利用基于策略梯度的深度强化学习方法，使智能体将状态价值转换为最优策略。

Result: 实验表明，所提出的增强型智能体在多个文本游戏中表现优异，游戏完成率和获胜率均明显高于现有方法。

Conclusion: 本文提出的方法显著提升了文本游戏中的完成率和胜率，超越了以往的智能体，并为强化学习在文本游戏应用提供了新的理论和实验依据。

Abstract: As AI technology advances, research in playing text-based games with agents
has becomeprogressively popular. In this paper, a novel approach to agent
design and agent learning ispresented with the context of reinforcement
learning. A model of deep learning is first applied toprocess game text and
build a world model. Next, the agent is learned through a policy gradient-based
deep reinforcement learning method to facilitate conversion from state value to
optimal policy.The enhanced agent works better in several text-based game
experiments and significantlysurpasses previous agents on game completion ratio
and win rate. Our study introduces novelunderstanding and empirical ground for
using reinforcement learning for text games and sets thestage for developing
and optimizing reinforcement learning agents for more general domains
andproblems.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [36] [Store Languages of Turing Machines and Counter Machines](https://arxiv.org/abs/2509.02828)
*Noah Friesen,Oscar H. Ibarra,Jozef Jirásek,Ian McQuillan*

Main category: cs.FL

TL;DR: 本文证明有限访问一向非确定性Turing机的存储语言为正则，拓展到带反转有界计数器时仍具良好性质，对理论与实际应用（如验证、容错）有价值。


<details>
  <summary>Details</summary>
Motivation: 研究自动机的存储语言有助于理解各种自动机的计算能力、存储约束对语言性质的影响，从而推动验证、容错等应用的发展。本工作关注一种有限访问Turing机（fvNTM）的存储语言，探讨其正则性及与反转有界计数器模型的关系。

Method: 作者定义并分析了一种一向有限访问非确定性Turing机（fvNTM）。首先，证明这类Turing机的所有存储语言都是正则语言。接着，进一步展示通过反转有界计数器增强的fvNTM，它们的存储语言也能被仅带反转有界计数器的机器，无需工作带，所接受。同时，作者还对一向和两向机器模型的存储语言的递归性进行讨论，并探讨了具体应用。

Result: 得出fvNTM的所有存储语言都是正则语言。此外，fvNTM拓展到带反转有界计数器的情况下，其存储语言同样可以由只带反转有界计数器、无工作带的机器接受。同时，对存储语言递归性给出若干条件，并应用于验证、容错及右商研究。

Conclusion: 本文完善了对有限访问Turing机存储语言性质的认识，证明其具有正则性，并拓展到带反转有界计数器的情景。相关结果在自动机理论、验证与容错等领域具有理论及实际意义。

Abstract: The store language of an automaton is the set of store configurations (state
and store contents, but not the input) that can appear as an intermediate step
in an accepting computation. A one-way nondeterministic finite-visit Turing
machine (fvNTM) is a Turing machine with a one-way read-only input tape, and a
single worktape, where there is some number $k$ such that in every accepting
computation, each worktape cell is visited at most $k$ times. We show that the
store language of every fvNTM is a regular language. Furthermore, we show that
the store language of every fvNTM augmented by reversal-bounded counters can be
accepted by a machine with only reversal-bounded counters and no worktape.
Several applications are given to problems in the areas of verification and
fault tolerance, and to the study of right quotients. We also continue the
investigation of the store languages of one-way and two-way machine models
where we present some conditions under which their store languages are
recursive or non-recursive.

</details>
