<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: 本文推广了抽象语法的理论，针对CBV、CBPV等带有二级类型的语言建立了更普适的语法处理方式，并证明了相关推理性质，对程序语言理论有积极贡献。


<details>
  <summary>Details</summary>
Motivation: 现有的抽象语法框架主要处理一类变量与替换机制，但难以覆盖如按值调用(CBV)和按推值调用(CBPV)语言中的二级类型(sorts)问题。论文旨在填补当前理论与实际复杂语法之间的空白。

Method: 作者将Fiore, Plotkin, Turi关于带绑定、替换和空洞的抽象语法的处理推广到包含二级类型(sort)的语言。通过禁止二级类型出现于变量上下文，提出从幺半范畴到作用范畴(actegories)的分类学变化，并用双范畴(bicategorical)的方式进行论证和重现。

Result: 提出了一种通用理论，能够处理语言中存在二级类型的抽象语法，尤其适用于CBV和CBPV。基于此理论，作者证明了多种CBV变体的替换引理(substitution lemmata)。

Conclusion: 适当修正分类学结构，可以对二级类型存在的抽象语法进行统一刻画，并通过新的范畴理论工具证明其替换性质，有助于更好地理解和描述复杂编程语言的语法和推理规则。

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 提示语的英语熟练度会影响LLM生成代码的正确性，提升英语水平能帮助开发者获得更可靠的AI代码结果。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型(FM)在软件工程领域被广泛应用，自然语言提示成为开发者与大语言模型(LLM)交流的关键接口。尽管大量研究关注提示语的结构，但提示语的自然语言能力对代码生成质量的影响还未被充分探索。

Method: 利用HumanEval数据集，针对164个编程任务系统性地改变提示语的英语水平（从基础到高级），并评估生成代码的熟练度和正确性。

Result: 研究发现，LLMs默认使用中级(B2)英语水平。虽然提示语的英语水平对代码熟练度的影响依赖于具体模型，但高水平提示语会在所有模型下生成更加正确的代码。

Conclusion: 自然语言熟练度是影响代码生成质量的重要因素，开发者可通过提升提示语的语言水平来优化AI生成的代码输出和方案可靠性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [3] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 本论文讨论了软件工程领域成果如何影响实践，遇到实验不可行时，介绍了利用统计因果推断从观察性数据进行因果关系验证的必要性和方法。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究的目标是让开发者和用户受益，但只有将研究成果如工具、流程、指导方针有效转化到实践中才能实现这一目标。

Method: 探讨通过受控实验和统计因果推断(SCI)方式验证软件工程研究成果对开发过程或性能的影响。如果无法进行随机对照实验，则通过观察性数据进行因果推断。

Result: 强调了缺乏可行的受控实验时，需要有可靠的统计因果推断流程以评估工具、流程等新贡献对软件开发实际效果的因果影响。

Conclusion: 实现研究成果的落地和价值，需要通过科学方法获得其对软件开发行为和性能的因果证据，而统计因果推断为无法实验时提供了重要途径。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [4] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: 这篇论文提出了针对Ruby语言的轻量自动修复框架RAMP，通过多代理反馈驱动迭代优化，不依赖大型数据库或微调，在基准测试上明显优于现有方法，拓展了基于LLM的自动化调试工具对冷门语言的支持。


<details>
  <summary>Details</summary>
Motivation: 当前自动程序修复（APR）在使用大语言模型（LLM）方面进展迅速，但现有方法计算成本高且只关注少数几种编程语言，Ruby作为广泛用于Web开发的语言，在APR领域研究较少，因此有必要提出面向Ruby的高效自动修复方法。

Method: 提出了RAMP框架，将程序修复建模为基于反馈的迭代过程，通过多个协作代理生成有针对性的测试、分析错误并反复改进修复方案，直到找到正确解。区别于之前方法，RAMP不依赖大型多语言数据库或昂贵的微调，而通过轻量化提示和测试驱动反馈直接在Ruby环境中工作。

Result: 在XCodeEval基准上，RAMP在Ruby语言上的pass@1指标达到67%，优于之前方法。在五次迭代内快速收敛，消融实验证明测试生成和自我反思机制是性能提升的关键。进一步分析表明RAMP对修复错误答案、编译错误和运行时错误尤为有效。

Conclusion: RAMP框架为多智能体策略在程序修复中的应用提供了新见解，也为将基于LLM的调试工具扩展至研究较少的语言（如Ruby）奠定了基础。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [5] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 作者提出了由多个智能体组成的自动化RTL生成流程，集成大语言模型和仿真工具，可自我纠错并逐步提升代码复杂度。实验证明，该方法在准确性和效率方面领先于现有技术，在自然语言到RTL转换任务上达到最佳表现。


<details>
  <summary>Details</summary>
Motivation: RTL生成是一项复杂且易出错的工作，设计者希望借助智能体和自动化工具减少人工干预，并提升生成效率和准确性。

Method: 通过集成专用大语言模型（LLMs）与硬件仿真工具，利用多智能体协作和渐进式错误反馈修正机制（PEFA），实现自动的RTL生成与验证。

Result: 在两个开源自然语言到RTL数据集上验证了该方法，性能优于现有方法，实现了业界领先的通过率，同时token使用更为高效。

Conclusion: 提出的多智能体协作流程能够实现高效且无须人工干预的RTL代码自动生成，并显著提升了生成结果的准确性与效率。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [6] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: 该论文提出基于PSD解析与多模态模型的高质量前端代码生成方法，解决现有设计转代码中的对齐和一致性问题，大幅提升生产可用性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有设计到代码自动生成方法存在结构不一致、资源错位和生产级部署困难等问题，亟需一种更为精确、可靠的方案实现设计稿到可用前端代码的自动转化。

Method: 提出了PSD2Code方法，包括ParseAlignGenerate流程：首先解析PSD文件提取层级结构、层属性与元数据，接着采用基于约束的对齐策略保证生成元素与设计资源一致，最后通过结构化提示词提升代码可控性和质量。多模型评测验证了该方法的通用性和有效性。

Result: 在多个评价指标（代码相似度、视觉一致性、生产可用性）上均显著优于现有方法，并且表现出良好的大模型适应性。

Conclusion: 通过结合PSD文件结构化信息和多模态大模型，显著提升了前端代码生成的一致性和生产可用性，证明了结构化设计信息融入设计驱动自动化前端开发的有效性。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [7] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct方法通过构建安全规范知识库，提升了LLM对代码漏洞的检测及推理能力，在多个指标上大幅超越现有做法，并发现了实际高危漏洞，显示出针对代码安全的巨大应用价值。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）在代码理解领域取得了显著进展，但在检测代码漏洞方面表现有限，难以有效区分有漏洞的代码和修补后的代码。作者认为原因在于LLM缺乏对安全规范（即代码应如何安全运行的预期）方面的理解和推理能力，而这些规范在训练数据中往往并不显式存在。

Method: 作者提出了VulInstruct方法，利用系统化分析历史漏洞，自动提取安全规范，并构建安全规范知识库。该知识库包括两类规范：一是从多个项目的高质量补丁中提炼的通用安全规范，二是针对特定代码库反复出现的域特定规范。VulInstruct会检索和关联相关的历史案例及规范，辅助LLM进行安全行为推理，从而提升漏洞检测效果。

Result: 在PrimeVul数据集上，VulInstruct取得了45.0%的F1分数（比基线提升32.7%），37.7%的召回率（提升50.8%），且能检测24.3%的独特漏洞，比任意基线高2.4倍。在配对评估中，相对提升达32.3%。此外，VulInstruct还发现了一个高危新漏洞（CVE-2025-56538），展示了实际应用价值。

Conclusion: 通过引入规范引导，让LLM具备对代码安全行为的推理能力，VulInstruct显著提升了代码漏洞检测的准确性和实际应用能力，验证了基于安全规范知识库的检测范式在真实工程中的有效性。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [8] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint结合LLM与符号推理，自适应识别关键规范，显著减少静态分析误报并提升召回率，在多个基准和项目中效果优于主流工具。


<details>
  <summary>Details</summary>
Motivation: 静态分析能发现软件漏洞，但存在source-sink规范不完整以及误报率高的问题。要解决规范不全和误报多的难点，提高分析结果的准确度和适用性。

Method: 提出了AdaTaint框架，利用LLM自适应推断source/sink规范，并通过神经-符号推理过滤误报。该框架结合模型建议与程序事实和约束验证，实现了兼具适应性和确定性的分析。

Result: 在Juliet 1.3、SV-COMP C基准和三个大型真实项目上评估，AdaTaint平均减少43.7%的误报，提升11.2%的召回率，并保持了运行开销的竞争力。

Conclusion: 结合大语言模型（LLM）推理与符号验证，可以有效提升静态漏洞分析的准确性和可靠性。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [9] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 本文提出了更具挑战性的端到端软件开发自主体基准与评测框架，发现主流架构在实现能力上有限，亟需解决需求遗漏与规划不足等核心瓶颈，以推动后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有评测方法过于简单，无法科学、公平地评估和比较基于大模型的自主体在端到端软件开发任务中的真实能力和局限。

Method: 构建了动态、真实的E2EDevBench基准环境，并提出结合测试用例功能评估与细粒度LLM需求验证的混合评测框架，对三种代表性自主体架构进行统一对比分析。

Result: 最先进的自主体在真实模拟环境下仅能满足约50%的需求，且成功很大程度依赖于架构设计中的任务分解与协作机制，暴露出需求遗漏和自我验证不充分是主要问题。

Conclusion: 通过统一的实现基础和严格的评测框架，该研究揭示现有LLM驱动的软件开发自主体能力受限，主要瓶颈在于需求遗漏和自我验证不足，未来需聚焦于加强需求理解和规划。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [10] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 本文系统评估了23个大语言模型在AI价值观上的对齐情况，发现其价值选择更接近AI从业者但实际应用时价值排序存在不一致，强调开发过程中需要人类监督及价值对齐的持续监测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程领域的广泛应用，人们关注它们在AI价值观上的对齐情况，尤其是与人类群体的价值判断是否一致。

Method: 本文选取美国代表性群体和AI从业者两个人类群体，通过四项任务（选择关键AI价值观、在具体情境下评级价值重要性、解决价值冲突、优先排序具备价值的软件需求），对23种LLMs进行价值对齐评估。

Result: 研究发现，LLMs的价值偏好整体上更接近AI从业者，尤其在公平、隐私、透明、安全和责任方面。但LLMs声明要坚持的价值（任务1-3）与实际软件需求优先级排序（任务4）之间存在不一致，表明LLMs在声明和实际行为之间存在忠实性缺口。

Conclusion: 依赖LLMs进行软件需求工程存在实际风险，需要制定系统化方法对AI价值对齐进行基准测试、解释和持续监测，保证AI辅助软件开发的责任性。

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [11] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: SAFE是一款IDE插件，结合GPT-4o，为SAST工具发现的安全漏洞提供详细解释，有助于开发者理解和处理漏洞，从而提升安全检测工具的易用性。


<details>
  <summary>Details</summary>
Motivation: 由于安全漏洞问题严重，企业广泛采用静态应用安全测试（SAST）工具来检测漏洞，但这些工具常常因警告信息过于通用导致开发者难以理解和处理关键漏洞。

Method: 提出了一种利用大语言模型（LLM）提升SAST工具可解释性的混合方法，并开发了SAFE插件，结合GPT-4o对检测出的漏洞进行深入解释（原因、影响和防御策略），以增强开发者对于安全问题的认识。

Result: 专家用户研究显示，SAFE插件生成的解释能够显著提高初级和中级开发者理解和解决安全漏洞的能力，从而提升SAST工具的整体可用性。

Conclusion: 将LLM集成于SAST工具解释模块可改善开发者对安全漏洞的认知和响应效率，为提升安全工具实用性和用户体验提供了有效途径。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [12] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 将Git作为“交流中枢”，通过Operator/CR模式实现分布式系统间轻量、透明且可追踪的信息交换，适用于复杂异步协作环境，兼具审计与自治优势。


<details>
  <summary>Details</summary>
Motivation: 传统的API和消息中间件在分布式、跨组织或隔离环境下集成有复杂性和局限性。寻求更轻量、透明且易审计的替代方案。

Method: 设计了一种基于Git的通信模型，借鉴Kubernetes Operator与自定义资源（CR）的模式，通过共享Git仓库作为单一事实来源进行信息交换。

Result: 该方法不仅支持跨领域、跨组织甚至隔离环境下协作，还能利用Git的版本、权限和签名管理，保持系统间的松耦合和自治，并分析了与REST和消息中间件方案相比的利与弊。

Conclusion: Git可以作为分布式系统之间异步信息交换的可审计和轻量级协调媒介，具有透明性和可追溯性。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [13] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: DriveRLR是一个评估大语言模型判断驾驶场景真实性稳健性的基准工具。经验证能揭示不同LLM的性能差异，并有助于仿真自动驾驶测试。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的安全性依然存在挑战。真实场景测试成本高、风险大，因此仿真测试越来越受重视。但评估仿真场景的真实度仍是难题，亟需高效的评判方法。最近，大语言模型（LLMs）在推理与泛化能力上表现突出，显示其可用于基于文本的场景真实性评估。

Method: 作者提出了DriveRLR，一个基准工具，用于评估LLMs在判断驾驶场景真实性方面的稳健性。DriveRLR通过生成变异的场景版本，并基于这些场景构建提示词，然后用这些提示词测试LLMs对场景真实性的判断力和鲁棒性。

Result: DriveRLR在DeepScenario数据集上，针对GPT-5、Llama 4 Maverick和Mistral Small 3.2三种先进LLM进行了实证。实验结果揭示了各LLM在稳健性上的异同，证明了DriveRLR在评估场景真实性方面的有效性和应用价值。

Conclusion: DriveRLR不仅能评估LLM的鲁棒性，还可作为生成场景目标函数，支持面向仿真的自动驾驶系统测试流程，具有良好的实际应用前景。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [14] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 本文系统分析了主流LLM在代码生成基准上的失败任务，发现其弱点具有模式化与复杂性，揭示了模型未来优化的关键方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成领域表现卓越，但排行榜仅提供量化排名，无法揭示LLM真正的失败难题。这些信息对理解模型局限及未来优化方向至关重要。

Method: 对四个主流代码生成基准任务进行分析，识别主要LLMs容易失败的任务；研究失败原因，包括代码静态复杂度和系统性检查了114个LLMs一致失败的任务。

Result: 发现LLM的失败任务具有特定模式和复杂性，分析总结了四种常见弱点及导致模型失败的任务复杂特征。

Conclusion: 分析发现：大语言模型在代码生成任务中存在四种反复出现的弱点，以及基准任务中的常见复杂情况会导致模型失败。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [15] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 使用大语言模型助手Cursor可以短期提升开发效率，但会增加代码复杂度和警告，长期来看反而减缓开发进度。


<details>
  <summary>Details</summary>
Motivation: 尽管开发者声称使用大语言模型代理如Cursor能极大提高生产力，但目前缺乏实证证据来支撑这些说法。因此本文旨在通过严谨实证方法，量化分析其对开发效率和代码质量的真实影响。

Method: 本文采用了最先进的双重差分法，将采用Cursor的GitHub项目与未采用的类似项目进行匹配对比。同时使用面板广义矩估计方法分析影响机制。

Result: 1. Cursor助理的采用能短暂且显著提高开发速率。2. 会导致静态分析警告和代码复杂度的持续上升。3. 静态分析警告和代码复杂度的提升是开发速率长期减缓的主要原因。4. 这为相关从业者、工具设计者和研究人员提供了参考。

Conclusion: 引入Cursor这一大语言模型助手可以显著提升项目开发速率，但这种提升是短暂的。同时还会导致静态分析警告和代码复杂度的长期增加，这些负面影响最终成为降低长期开发速度的主要因素。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [16] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: 本文提出了基于真实开发场景的代码编辑评测基准EDIT-Bench，发现现有大多数大模型对现实世界代码编辑任务仍具挑战性，不同用例及上下文信息对模型表现影响较大。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑评测基准多依赖人工生成的数据，缺乏真实用户场景。作者希望构建更真实、更具代表性的评测基准，推动AI编程助手在开发者实际环境中的应用能力提升。

Method: 通过收集现实场景中的用户指令和代码上下文，构建包括545个问题的多语言、多用例评测基准，多维度评估40个大模型对代码编辑任务的表现，并分析背景相关信息对结果的影响。

Result: 只有5款大模型在EDIT-Bench上的得分超过60%，模型在不同类型指令下表现差异显著，且代码上下文信息的完整性对任务成功率有明显影响（最高可达11%的波动）。

Conclusion: EDIT-Bench能够真实地衡量大模型对代码编辑任务的处理能力，并揭示模型在不同上下文和用例中的表现差异。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [17] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 提出通过计算并保障模块独立性，以及采用通用接口模式，能够消除系统内部依赖，有效支持单体应用像微服务系统一样灵活动态地管理模块，架构EIGHT展现了对复杂应用系统新的设计可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管微服务实现了模块的物理隔离，但依赖关系仍能在各模块间扩散与传播。为追溯模块间耦合的根源，论文提出新方法。

Method: 提出一种计算模块独立性的方法并推导其必要条件，进而提出消除模块间依赖的新系统设计理念和软件工程方法。通过设计一套通用接口作为模块间的界限，并将该方法用于实现新的平台架构EIGHT。

Result: 证明只要保证模块独立性，单体应用同样可以实现模块的动态加载、卸载和修改。EIGHT架构可支持复杂系统的灵活性和扩展性。

Conclusion: 该架构探索了超越微服务与单体架构的新路径，为复杂系统的设计提供了新思路。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [An Automated Theorem Generator with Theoretical Foundation Based on Rectangular Standard Contradiction](https://arxiv.org/abs/2511.04092)
*Yang Xu,Peiyao Liu,Shuwei Chen,Jun Liu*

Main category: cs.LO

TL;DR: 本研究提出并证明了新的逻辑结构“矩形标准矛盾”，据此给出系统化的自动定理生成理论和工具，使机器具备独立发现新定理的能力，推动逻辑与AI发展。


<details>
  <summary>Details</summary>
Motivation: 目前还缺乏一个系统性、严格的理论体系用于自动生成非平凡且逻辑有效的定理，这是逻辑和人工智能领域的重要空白。

Method: 提出并定义了一种新的逻辑结构——矩形标准矛盾，并以此为核心，发展了完整的自动定理生成（ATG）理论；通过理论证明阐明其两个核心性质，并基于此设计了高效的模板化自动定理生成算法，开发了“矩形自动定理生成器”工具。

Result: 理论上证明了矩形标准矛盾的不冗余性和标准矛盾性，能够系统生成等价的有效定理。工程上实现了可用的自动定理生成工具。

Conclusion: 首次提出了矩形标准矛盾及其相关理论，填补了自动定理生成系统化理论的空白，使机器能从“验证者”向“发现者”转变，推动了逻辑与人工智能基础研究。

Abstract: Currently, there is a lack of rigorous theoretical system for systematically
generating non-trivial and logically valid theorems. Addressing this critical
gap, this paper conducts research to propose a novel automated theorem
generation theory and tool. Based on the concept of standard contradiction
which possesses unique deductive advantages, this paper defines and proves, for
the first time, a new logical structure known as rectangular standard
contradiction. Centered on this structure, a complete Automated Theorem
Generation (ATG) theory is put forward. Theoretical proofs clarify two core
properties of rectangular standard contradiction: first, it is a standard
contradiction (necessarily unsatisfiable); second, it exhibits non-redundancy
(the remaining clause set becomes satisfiable after removing any clause).
Leveraging these properties, this paper proves that partitioning a rectangular
standard contradiction into a premise subset $A$ and negation of its complement
$H$, a valid theorem $A \vdash \neg H$ can be formed, and all such theorems are
logically equivalent. To implement this theory, an efficient template-based ATG
algorithm is designed, and a Rectangular Automated Theorem Generator is
developed. This research enables machines to transition from "verifiers" to
"discoverers", opening up new avenues for fundamental research in the fields of
logic and artificial intelligence.

</details>


### [19] [Compact Quantitative Theories of Convex Algebras](https://arxiv.org/abs/2511.04201)
*Matteo Mio*

Main category: cs.LO

TL;DR: 提出并研究了紧致定量等式理论的概念，通过证明凸代数量子代数理论的紧致性，获得了一系列关于概率分布距离的紧致理论，为量化结构的理论研究提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 定量等式理论是刻画距离和概率分布等量化结构的重要工具，然而理论的紧致性——能否只用有限长度证明便得出所有结论——未被彻底研究。作者希望用一个范例展示并推广紧致定量等式理论。

Method: 通过理论分析和证明，展示了定量等式理论的紧致性，并将该理论推广到其它凸代数相关的定量理论。

Result: 证明了插值重心定量代数理论的紧致性，并据此构造和获得了其它针对凸代数、特别是有限支持概率分布距离的紧致定量等式理论。

Conclusion: 作者证明了Mardare等人提出的插值重心（凸）定量代数理论是紧致的，即其所有结论都可以通过有限证明导出。

Abstract: We introduce the concept of compact quantitative equational theory. A
quantitative equational theory is defined to be compact if all its consequences
are derivable by means of finite proofs. We prove that the theory of
interpolative barycentric (also known as convex) quantitative algebras of
Mardare et. al. is compact. This serves as a paradigmatic example, used to
obtain other compact quantitative equational theories of convex algebras, each
axiomatizing some distance on finitely supported probability distributions.

</details>


### [20] [The Size of Interpolants in Modal Logics](https://arxiv.org/abs/2511.04577)
*Balder ten Cate,Louwe Kuijer,Frank Wolter*

Main category: cs.LO

TL;DR: 本文分析（准）正规模态逻辑中的插值项和最强蕴含式的复杂度：表式逻辑易于处理，非表式逻辑则存在指数下界，展现出清晰的复杂性分界。


<details>
  <summary>Details</summary>
Motivation: 作者旨在系统性地研究（准）正规模态逻辑中Craig插值、统一插值和最强蕴含式的大小复杂度问题，这是逻辑与计算复杂性中的重要基础问题。

Method: 采用理论计算方法，首先提出上界——将表式模态逻辑中的最强蕴含式计算在多项式时间内归约到经典命题逻辑中的统一插值计算；其次，通过复杂性理论分析，给出非表式标准模态逻辑下的无条件指数下界。

Result: 对表式模态逻辑，相关问题（最强蕴含式、插值项等）可在多项式dag大小内解决，前提是NP $$ P/poly；而对绝大多数非表式模态逻辑以及包含/被包含于S4或GL的正规模态逻辑，相关项的大小有无条件的指数下界。

Conclusion: 对于具有Craig插值性质的表式模态逻辑，其插值项和相关项大小可以归约到命题逻辑，且复杂度较低；但对绝大多数非表式正规模态逻辑，相关项的大小无法避免指数级增长，在S4或GL相关逻辑中表现为清晰的二分性质。

Abstract: We start a systematic investigation of the size of Craig interpolants,
uniform interpolants, and strongest implicates for (quasi-)normal modal logics.
Our main upper bound states that for tabular modal logics, the computation of
strongest implicates can be reduced in polynomial time to uniform interpolant
computation in classical propositional logic. Hence they are of polynomial
dag-size iff NP $\subseteq$ P$_{/\text{poly}}$. The reduction also holds for
Craig interpolants and uniform interpolants if the tabular modal logic has the
Craig interpolation property. Our main lower bound shows an unconditional
exponential lower bound on the size of Craig interpolants and strongest
implicates covering almost all non-tabular standard normal modal logics. For
normal modal logics contained in or containing S4 or GL we obtain the following
dichotomy: tabular logics have ``propositionally sized'' interpolants while for
non-tabular logics an unconditional exponential lower bound holds.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文提出了一套用Big Five人格理论指导低秩子空间发掘、动态层选择与人格特质行为操控的大语言模型方法，实现了精细且不损模型能力的性格表达调控，为心理学与AI模型对齐提供了创新连接。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在生成内容时会展现出隐含的“人格”特征，但目前尚难以精确控制或调整这些特征来满足具体需求，因此急需有效的行为操控机制。

Method: 作者提出了一种新颖流程：利用Big Five人格特质理论，通过低秩子空间探索方法，从transformer各层提取隐状态激活，找到特质相关的最优注入层，并通过灵活的动态层选择机制，将这些“人格方向”用于控制LLM输出中的特质表达。

Result: 研究发现：人格特质在LLM隐藏空间中占据一个低秩共享子空间，这些潜在结构可以通过精细扰动转化为可操作的模型调控手段，实现对LM性格表达的有效把控，且不会影响流畅性、多样性及整体能力。

Conclusion: 通过将心理学理论与模型方向调控相结合，本文为人格特质的注入与操控提供了坚实的理论与方法基础，有助于实现更具人格一致性与需求适配性的LLM输出。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [22] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出可集成于TextGrad的新型自我验证框架TextualVerifier，基于链式推理和多数投票机制，显著提升了文本推理的有效性和系统整体表现，为文本优化领域的自动化验证提供支持。


<details>
  <summary>Details</summary>
Motivation: TextGrad是一种创新的文本驱动自动微分方法，用于复合AI系统的优化，但缺乏有效的自我验证机制来确保基于文本的决策推理的有效性。因此，本研究旨在填补TextGrad在推理有效性验证方面的空白，提高文本优化过程中推理的可靠性。

Method: 提出了TextualVerifier，一个基于大语言模型的文本自我验证框架。TextualVerifier采用四阶段工作流，包括链式推理分解、变体生成、多数投票和共识聚合。该框架可无侵入集成到TextGrad系统，在损失函数和优化结果验证两个环节实现自我验证。实验采用Gemini 1.5 Pro模型，分别在PRM800K数据集上独立评估，在GPQA-Diamond、MMLU-ML和MMLU-CP基准上与TextGrad集成评估。

Result: TextualVerifier显著提升了推理有效性。在独立评估阶段，推理有效性提升29%；集成到TextGrad损失函数时，总体表现从68.2%提升至70.4%，平均增加5.9次LLM调用。在进一步评估中，分别在GPQA、MMLU-ML、MMLU-CP基准上提升了8.08、10.71和3.92个百分点。

Conclusion: TextualVerifier首次为TextGrad提供了无需数值梯度的基于大语言模型的自我验证框架，有效提升了文本驱动优化系统的推理可靠性，为文本优化领域的自动验证机制开辟了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [23] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 本文扩展了希腊方言数据集GRDD+，涵盖10种方言，总词汇量超过630万，并用其微调和评估多种大语言模型，结果显示高质量丰富方言数据有助提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 希腊方言的数据资源有限，现有数据集覆盖范围和规模不足，限制了对不同希腊语方言的研究和自然语言处理能力的发展。本文旨在扩展和丰富希腊方言数据集，为大语言模型提供高质量的多样化训练数据。

Method: 作者扩展了现有的GRDD数据集，新增克里特语、塞浦路斯语、本都希腊语和北部希腊语的数据，并且首次加入六种新方言（如希腊科西嘉语、南意大利希腊语等）。随后，使用扩展后的数据集对三种8B参数规模的大语言模型（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行了微调，并与主流前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行了效果对比。

Result: 扩展后的GRDD+数据集包含10种方言，总词汇量达到6,374,939，为迄今为止最大、涵盖方言种类最丰富的希腊方言数据集。通过一系列微调实验，探索了高质量方言数据对大语言模型效果提升的作用，并在性能上与当前顶尖模型进行了展示性对比。

Conclusion: 本研究构建了规模最大、种类最全的希腊方言数据集GRDD+，并证实丰富的方言数据能够提升大语言模型在相关任务中的表现，为相关研究和应用提供了重要资源。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [24] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 论文介绍了PLLuM，这是迄今最大且开源的波兰语大型语言模型家族，涵盖多项数据集创新和责任治理框架，旨在提升波兰语AI水平，并公开发布以支持开放研究及技术自主。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLMs）主要以英语为中心，其他语言（比如波兰语）支持有限。该论文旨在填补波兰语AI模型的空白，推动本地化人工智能技术的发展。

Method: 开发并训练了最大的面向波兰语的开源基础模型家族，PLLuM，包括打造1400亿词令波兰语语料库、定制77k指令数据集和10万偏好优化数据集。采用严格的数据治理和责任AI框架，结合输出纠错及安全过滤模块，并介绍模型结构、训练过程及对齐技术。

Result: 实现了开放发布的波兰语大型语言模型家族，模型在公共行政领域下游任务中展示出实用性，同时通过责任治理框架确保模型输出的安全与合规性。

Conclusion: PLLuM模型填补了英文主导的商用大模型局限，通过开放共享推动波兰自主AI研究与技术创新，加强本地AI主权。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [25] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 作者提出了STARS推理对齐算法，实现了低计算开销、高效率和优越对齐质量，在六个LLM上大幅超过传统微调和偏好优化方法，表现接近当前最强的采样基线，是对齐LLM的有效新方案。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的对齐方法如微调，计算成本高且效果非最优。推理相关方法如Best-of-N采样虽然更具灵活性，但实际需要的计算量巨大，难以落地。因此，论文旨在探索更加高效、可行且效果优越的对齐方法。

Method: 提出了一种新的算法STARS，在推理生成过程中，将生成内容分段，针对每个固定长度的token片段进行采样、评分和拒绝/接受。此法允许在早期就纠正生成路径，以提升效率和对齐效果。该方法应用于六个主流LLM进行实验。

Result: STARS方法在六个主流LLM上的实验结果显示，在对齐质量上优于传统的有监督微调（SFT）最多14.9个百分点，优于直接偏好优化（DPO）最多4.3个百分点，并与强大的Best-of-N采样基线保持高度竞争力。

Conclusion: STARS作为一种细粒度、奖励引导的采样方法，能有效替代传统微调和全序列排名策略，在通用性、稳健性和效率上表现突出，适合用于LLM的价值对齐和安全部署。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [26] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 本论文提出了一种将多标签文本分类任务拆解为多个独立二元查询的方法，并用蒸馏和缓存机制极大提升了短文本推理效率和模型性能，在情感分析等多个领域具有良好应用价值。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多标签文本分类任务中的推理效率低，尤其是在需要对多个标签同时做出决策的情况下，如何提高效率和模型泛化能力成为了难题。

Method: 将多标签分类任务重构为一系列二元（是/否）决策，每个目标标签单独查询；结合前缀缓存机制提升短文本推理效率；采用LLM到SLM蒸馏技术，用强大注释模型提供多标注，聚合后微调更小模型。

Result: 微调后的较小模型（如HerBERT-Large、CLARIN-1B等）在训练见过的维度上较零样本基线有显著提升，短文本推理速度更快且无损精度，并展示出良好可扩展性。

Conclusion: 多标签任务二元拆解结合蒸馏和缓存机制是一种高效、可扩展的LLM多标签分类框架，不仅在情感分析场景验证有效，且具备跨领域通用性。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [27] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 低资源语言机器翻译数据集存在性别严重偏见，尤其对女性的不良刻板和有毒内容，数据量越大问题越突出。作者呼吁重视数据质量与内容安全而非单纯追求规模。


<details>
  <summary>Details</summary>
Motivation: 目前对低资源语言的数据收集多追求数据量，容易忽视数据质量，导致技术表现不佳及内容有害（如性别偏见）。因此，作者考察三种低资源语言机器翻译数据集的性别表现和有害内容情况。

Method: 分析Afan Oromo、Amharic和Tigrinya三种低资源语言的机器翻译训练数据和基准数据集，从语域分布和性别表现（如人名、动词语法性别及刻板印象）等方面进行定量和定性分析。

Result: 训练数据侧重政治与宗教文本，基准数据集则偏向新闻、健康和体育领域。数据集对男性性别表现严重偏倚（无论人名、语法性别还是刻板印象）。尤其数据量越大，有害及有毒的女性刻板和内容表现越突出。

Conclusion: 仅扩大数据量无法保证数据质量及公平性，反而可能加剧有害内容及性别偏见。建议后续加强对低资源语言数据集的质量审查与有害内容的早期干预。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [28] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 本文提出GRAD，通过结合语料检索证据在解码阶段减少大模型幻觉，无需重训练或结构化知识，提升了多项指标，操作简单且效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在应对幻觉问题上面临挑战。现有方法大多依赖外部知识库，如知识图谱或结构化数据库，但通过prompt检索外部知识的方式易受领域影响，并且符号知识的集成成本高。

Method: 提出了一种新的解码方法Graph-Retrieved Adaptive Decoding（GRAD），在不重新训练模型的情况下，将生成与检索到的证据进行结合。具体做法是在检索到的小型语料库中，通过一次前向传播积累下一个token的logits，构建稀疏token转移图，在解码时将检索到的logits与模型自身的logits自适应融合。

Result: GRAD在三个不同的大模型及多个问答基准测试中表现优异，与贪婪解码等已有方法相比，内在准确率最高提升9.7%，幻觉率降低8.6%，正确率提升6.9%，在真值和信息量产品分数上也居于所有方法之首。

Conclusion: GRAD是一种轻量级、可即插即用的解码增强方法，无需知识图谱即可基于语料中的统计证据，有效引导生成更为真实且可验证的内容。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [29] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 论文通过迭代指称游戏，评估了视觉-语言模型在多轮语境中的推理能力，发现其在相关上下文助力下性能提升，但与人类相比，在处理抽象和少样本情形时仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 探究视觉-语言模型在多轮互动下，进行上下文敏感语用推理的能力，及其与人类的差距。

Method: 通过设计迭代指称游戏，改变上下文的数量、顺序与相关性，对比分析人类与视觉-语言模型在不同条件下的表现。

Result: 在无相关上下文下，模型表现高于随机水平，但显著弱于人类；有相关上下文的多轮任务中，模型表现提升明显，但在抽象指称的少样本情境下仍有难度。

Conclusion: 当提供有相关性的上下文时，模型在迭代指称游戏中的表现显著提升，但在处理抽象指称和少量样本时依然表现困难，仍与人类有较大差距。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [30] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 作者基于大规模推文分析，利用大模型构建了可精细化反映美国各地社会繁荣状况的新指数（HFGI），为社会福祉、变迁等研究带来了更高空间和时间分辨率的数据资源。


<details>
  <summary>Details</summary>
Motivation: 传统的社会繁荣度指标往往侧重于经济因素，缺乏对幸福、健康、目标感、美德、人际关系、财务稳定等多维度内容的精细刻画，而且空间和时间上的分辨率较低。

Method: 作者利用大约26亿条2013-2023年美国地理定位推文，结合微调的大语言模型，针对哈佛全球繁荣研究框架定义的48项指标以及对移民态度和腐败感知，自动识别和分类推文内容，构建了“人类繁荣地理指数(HFGI)”。并对数据进行了有效性验证。

Result: HFGI数据集提供了美国各州、县层面，按月、按年的繁荣相关讨论指标，证实该方法能准确反映相关的抽象概念，并与现有指标呈现预期相关性。

Conclusion: 这一成果为从多学科角度研究美国社会福祉、不平等和社会变迁提供了新的高分辨率工具，能更深入观察社交媒体反映下过去十年人类繁荣的动态变化。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [31] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 用向量空间映射方法在不同大模型间实现直接“语义”交流，提升多模型协作效率及能力，且方法安全稳定。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体场景下，大模型之间通过明文token传递信息，导致语义信息损失和计算效率低下。

Method: 提出通过向量空间映射（latent bridge）来实现模型间的“语义级”信息交流。使用dual-encoder建立Llama-2-7B和Mistral-7B-Instruct之间的向量翻译映射。

Result: 构建的dual-encoder平均余弦相似度0.538。在将30%混合比重注入翻译后的向量后，可引导目标模型生成内容且保持logits稳定。双向实验表明通用模型的表征更易于迁移，迁移不对称比为2.01:1。

Conclusion: 通过向量语义层级的信息注入，实现不同大模型之间“含义”级联通，增强了AI协作潜力，规避了传统token通信的限制。该方法能保持计算稳定性，显示跨模型语义交流是可行的。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [32] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 该论文提出将溯因推理方法与RAG模型结合来弥补检索信息不全造成的推理断裂，通过前提生成与验证提升推理准确性与可信度。实验验证该方法有效，具有提升RAG系统鲁棒性和可解释性的潜力。


<details>
  <summary>Details</summary>
Motivation: RAG（检索增强生成）模型在知识密集型任务表现强劲，但在检索到的信息不完整时，常常导致推理过程断裂，影响模型的准确性和合理性。如何填补信息缺口，提升推理能力，是当前面临的挑战。

Method: 提出将溯因推理（abductive inference）与RAG框架结合。当发现证据不足时，自动生成合理的缺失前提，并通过一致性和可信度检查进行验证。此方法包含证据不足检测、前提生成、及验证环节。

Result: 在溯因推理和多跳问答基准任务上，该方法显著提升了答案准确率和推理过程的可信度。

Conclusion: 溯因推理为检索增强生成系统提升鲁棒性和可解释性提供了新途径，是未来发展值得关注的重要方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [33] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种可在大量转录错误情况下表现优异的弱监督ASR训练新方法WST，大幅提升了鲁棒性和实用性，优于现有主流方案。


<details>
  <summary>Details</summary>
Motivation: 传统RNN-T模型高度依赖大规模高质量标注数据，获取成本高、难度大，需要减轻对优质标注数据的依赖。

Method: 提出了弱监督Transducer（WST）模型，通过灵活训练图来处理转录错误，无需额外置信度估计或预训练模型。

Result: 在合成和工业数据集上，WST在转录错误率高达70%时仍能保持效果，并显著优于BTC、OTC等CTC相关弱监督方法。

Conclusion: WST方法在弱监督条件下表现出强大的鲁棒性，优于现有CTC相关方法，并且实用性很强。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [34] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 现有大模型解释评估方法不足以判断其是否真正贴合专家思维，作者提出T-FIX标准及相关指标来解决此问题，提升模型输出解释的专业性和可信度。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型应用中，用户不仅需要LLM给出答案，更需要能够反映专家级推理过程的解释，现有评估方法未能有效反映解释内容与专家直觉的贴合度。

Method: 构建了T-FIX基准，涵盖七个知识密集型领域，并与领域专家合作开发了新的评价指标，用于评估解释内容与专家判断的一致性。

Result: 提出了专家一致性作为新的评估标准，并开发了相关工具与指标，可以更准确衡量LLM解释的专业性和实用性。

Conclusion: 本文提出了一种新的评估标准T-FIX，用于衡量大模型生成解释与专家观点的一致性。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [35] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种结合知识分解和对比时序知识检索的新框架PoK，显著提升了大型语言模型在时间知识图谱问答任务中的推理能力和检索精度，最高提升56%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的时间知识图谱问答系统（TKGQA）虽然引入了时序知识的嵌入或图神经网络，但不足以理解复杂的时间语义约束。同时，大型语言模型（LLMs）虽然具备强大的语义理解能力，但在时序推理方面仍然有限并易发生幻觉或缺乏知识。本文旨在解决现有方法对复杂时间问题的理解和推理不足的问题。

Method: 提出了一种名为PoK的知识计划与对比时序检索框架。其中包括：1）将复杂的时序问题分解为一系列子目标，为模型推理提供中间指导（Plan of Knowledge模块）；2）构建一个基于对比检索的时序知识库（TKS），使模型能够精准检索符合语义和时序的事实。通过结构化规划结合时序知识检索提升模型时序推理的可解释性和事实一致性。

Result: 在四个权威TKGQA数据集上的实验显示，PoK框架显著提升了LLMs的检索准确率和推理精度，最大超越现有最先进TKGQA方法56.0%。

Conclusion: 结构化知识计划与对比时序检索的结合，大幅提升了LLMs在时间知识图谱问答任务中的检索和时序推理能力，解决了以往方法对时序语义约束理解不足的问题。该方法为复杂时序推理提供了更高的准确性和可解释性。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [36] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 本研究比较了人类与大型语言模型对情感词的联想表现，发现LLMs与人类的联想结果有一定重合，但LLMs的联想更趋向于强化词语情感，同时其答案更可预测、创造性较低。


<details>
  <summary>Details</summary>
Motivation: 理解人类与LLMs在词语联想上的异同，尤其是情感相关词汇，探索机器生成联想是否与人类一致。

Method: 通过比较人类与大型语言模型（LLMs）对情感词的联想反应，分析两者的表现差异。

Result: LLMs的联想在情感色彩上较人类更加突出，但整体创造性和不可预测性低于人类。

Conclusion: LLMs与人类在情感词联想上的重合度适中，但LLMs的联想更强调情感色彩，更具可预测性且创造性较低。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [37] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 扩展大规模医学影像报告数据集训练的变换器模型，不仅准确率远超学术与主流商用系统，且在多机构、多类型报告上表现稳定，为PHI检测和数据隐私保护设立新标准。


<details>
  <summary>Details</summary>
Motivation: 自动化去标识医疗影像报告的数据隐私，是提高数据利用效率和保障患者隐私的关键。当前相关模型和商业系统在不同类型报告上的性能存在不足，因此有必要扩展现有模型训练的数据规模，并对比主流商用系统表现。

Method: 在已有变换器（transformer）为基础的PHI去标识流水线基础上，利用斯坦福大学的大型标注影像报告数据集（包括多种影像类型），并新增PHI类别（AGE），对模型进行微调。模型不仅在斯坦福数据集测试，还在宾夕法尼亚大学（Penn）数据集做横向测试。额外评估了基于“hide-in-plain-sight”的合成PHI生成方法稳定性，并与主流商业系统进行性能对比。所有PHI类别采用精确率、召回率和F1分数指标评估表现。

Result: 模型在宾夕法尼亚大学数据集总体F1分数为0.973，在斯坦福大学数据集为0.996，均优于或持平此前顶尖学术模型。合成PHI评估中，50份独立去标识Penn数据集，检测F1分数稳定（0.959）。在合成Penn报告上，本模型全面优于所有商业系统（F1分数0.960，对比商用系统0.632-0.754）。

Conclusion: 基于多样化影像数据集训练的变换器去标识模型，在跨机构、跨类型报告的PHI识别上显著优于此前学术与商用系统，树立了更安全的临床文本处理新标杆。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [38] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本论文研究了在语言识别问题中允许学习者每步给出k个猜测的情形，递归推广了已知的理论并精准刻画了哪些语言集合可被k-list识别。结果显示可识别集合能以最佳指数速率识别，不可识别集合则无法实现任何有效收敛速率。


<details>
  <summary>Details</summary>
Motivation: 受到近期有关语言生成正面结果的激励，作者重新审视典型的语言识别问题，特别考虑增加学习者每步猜测的数量以提高识别可能性。

Method: 研究了学习者在每一步可以给出k个猜测的语言识别问题，递归地推广了Angluin关于单一猜测情形的判定方法，并用于分析不同语言集合的可识别性。此外，通过概率分布抽样分析了识别速率。

Result: 1. 给出k-list识别的刻画：语言集合可被k-list识别当且仅当它可被划分为k个各自可识别的子集合。2. 证明如果集合可k-list识别，则能以指数速率实现；否则不可达收敛速度。

Conclusion: 论文给出了语言集合能够被k-list识别的严格刻画，并证明其统计设置下的最优速率。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [39] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 作者发现在大型推理模型中，批量推理不仅提高吞吐量，还能正则化模型行为，提高准确率、减少推理成本，并促发意外的集体智能效应，带来更高效和可靠的推理表现。


<details>
  <summary>Details</summary>
Motivation: 之前的研究多关注于批量推理作为降低大型语言模型（LLMs）推理成本的策略，而忽略了其对模型行为的影响。该论文旨在探究批量推理对于多步推理任务中的LRMs（Large Reasoning Models）行为的影响及潜在益处。

Method: 作者在13个不同基准任务上进行全面实证研究，并通过行为分析，比对批量推理与单例推理在准确率、推理token使用量及模型回答风格上的差异。

Result: 批量推理显著提升模型准确率，并大幅减少推理token的使用量（通常降低3到5倍）。行为分析还发现批量推理能减少模型“过度思考”与“犹豫性表达”，促使模型给出更果断的答案。同时模型还表现出“集体效应”，即会借鉴同批早期样例的推理方式来解决更难的问题。

Conclusion: 批量推理不仅仅是提升推理吞吐量的技术，更能作为一种高效且可靠的推理正则化手段，带来准确率提升和推理成本的降低。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [40] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 本文提出RIDE框架，通过对数学题进行对抗性、难度可控的重构，显著降低大语言模型的数学推理表现，揭示真实能力边界，推动更科学的模型评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在数学推理上的高性能可能由于训练数据泄漏或表层模式匹配造成，而非真实的推理能力，因此需要更能体现真实推理能力的对抗性评估方法。现有基于规则的扰动方法常会生成不合理的问题，限制了系统评估和基准题库的发展。

Method: 提出RIDE框架，利用项目反应理论（IRT）严格衡量问题难度，并生成具备挑战性的合理数学问题变体。该方法通过35个LLM模拟“学生”作答，基于其结果建立难度排序器，作为强化学习奖励信号，指导问题改写模型在不同难度层级上重构问题。

Result: 应用RIDE到竞赛级数学基准题库后，生成的扰动问题使得先进LLM的表现平均下滑21.73%（在26个模型上），揭示了数学推理的脆弱性，验证了所提出评估方法的有效性。

Conclusion: RIDE框架能系统性生成高质量、难度可控的数学问题变体，有效测试LLM的数学推理鲁棒性，对于推动数学推理评估标准和LLM能力提升具有重要意义。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [41] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 本研究提出CantoASR系统，结合微调后的Whisper以及Qwen-Audio大模型，通过引入声学和韵律线索，有效降低了粤语ASR中的字符错误率，为低资源声调语言ASR带来了新思路。


<details>
  <summary>Details</summary>
Motivation: 目前针对粤语等低资源语言的ASR因数据有限、丰富声调和语流变调等问题表现不佳，而现有主流大模型如Whisper词错率高。针对这一问题，探索结合声学特征和高级语义推理的新方法以提升识别准确性。

Method: 提出CantoASR框架，将强制对齐进行声学特征提取，采用LoRA微调Whisper模型以增强声调区分能力，并使用指令微调的Qwen-Audio实现韵律感知纠错。

Result: 在粤语口语数据集上，CantoASR相较于Whisper-Large-V3在字符错误率（CER）上有显著提升，显示了新框架的有效性。

Conclusion: 结合声学线索和大型音频-语言模型（LALM）的推理能够大幅提升粤语等低资源声调和方言自动语音识别（ASR）的性能，为此类场景提供了可扩展的解决方案。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [42] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文提出并评测了三种面向Text-to-SQL任务的多智能体大模型管道，发现讨论和推理-编码模式能让小型高效模型性能显著提升，超越以往单模型方案。适合关注开源、实用化系统的研究者参考。


<details>
  <summary>Details</summary>
Motivation: 当前的Text-to-SQL系统可以让非专业用户通过自然语言访问数据库信息，但现有大型语言模型在面对复杂数据库结构和推理时生成SQL表现不佳。此前研究多集中于复杂的、效率较低的大模型管道，对小型高效模型关注不足。

Method: 提出三种多智能体语言模型管道：（1）多智能体讨论管道，通过多个智能体迭代批评、改进SQL，并由评审汇总最终答案；（2）计划编码管道，规划者产生逐步SQL生成计划，编码者实现查询；（3）编码-汇聚管道，多个编码智能体独立生成SQL，由推理智能体选择最佳查询。对多种模型进行系统性能基准测试。

Result: 实验表明，多智能体讨论能显著提升小模型表现，如Qwen2.5-7b-Instruct的执行准确率提升最多达10.6%。在各种管道中，LLM Reasoner-Coder方案效果最佳，例如DeepSeek-R1-32B和QwQ-32B规划者将Gemma 3 27B IT的准确率从52.4%提升至最高56.4%。

Conclusion: 多智能体管道可以有效提升小型开源语言模型在Text-to-SQL任务中的表现，尤其是推理-编码模式最优。该方法兼顾模型效率和性能，为实用型Text-to-SQL系统带来新方向。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [43] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: AI内容生成导致交流变质，LAAC倡导大模型参与真实知识传递，但信任和信息可靠性仍是部署难题。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容泛滥，导致交流变成“荒谬剧场”：发送者用大模型扩写简单想法，接收者再用大模型压缩总结，双方都未与真正内容互动。作者希望克服这一循环，实现真实知识交换。

Method: 提出LAAC（LLM as a Communicator），让大模型作为智能交流中介，通过结构化对话准确捕捉发送者意图，并高质量传递给接收者。作者设计多智能体架构，通过受控实验，分别考察意图捕捉的准确性（Information Capture Fidelity）、知识复现的一致性（Reproducibility）、回应的可靠性（Query Response Integrity），在多种通信场景中系统评估LAAC的信任要求。

Result: 初步实验发现，LAAC虽然具备推进真实交流的潜力，但在信息捕捉、知识复现、一致可靠回应等方面存在显著信任缺口，这些需进一步解决，方能应用于高风险交流场景。

Conclusion: LAAC提出了AI辅助真实交流的新范式，但在实际部署前必须系统解决其信任与可靠性挑战。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [44] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文通过计算图灵测试系统验证了大语言模型在模拟社交平台上的人类文本时与真实文本的显著差异，指出即使校准后也难以完全仿真情感和表达。扩展模型规模和微调策略未有效提升类人性，同时类人性与语义准确性间存在权衡。该方法为LLM模拟提供了更科学的评估体系，并提醒社会科学领域谨慎使用LLM仿真人类行为。


<details>
  <summary>Details</summary>
Motivation: 现有社会科学领域普遍假设大语言模型（LLMs）能生成逼真的类人文本，并用于模拟人类行为。然而，对这种假设的实证检验很少，且主要依赖于人类判断的评估方法，这种方法的可靠性受质疑，导致缺乏对LLM生成文本真实度的系统性验证工具。

Method: 提出了一种“计算图灵测试”框架，将聚合指标（基于BERT的可检测性和语义相似性）与易解释的语言特征（文体标识和主题模式）结合，用于衡量LLM在特定数据集内对人类语言的模拟程度。同时，系统性比较了9种开放权重LLM在5种校准策略（包括微调、文体提示和上下文检索）下，复现X（旧称Twitter）、Bluesky和Reddit用户交互的能力。

Result: 研究发现，即使在经过各种校准之后，LLM生成的文本依然容易与真人文本区分开，尤其是在情感语调和情感表达方面。指令微调模型表现差于原始基础模型，模型规模扩大也未提升类人文本特性。同时，优化模型以提升类人性与语义准确性之间存在明显权衡。

Conclusion: 本文提供了可扩展的LLM模拟验证与校准框架，并提醒目前LLM在捕捉真实人类交流方面存在重要限制，不宜过度依赖其仿真结果。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [45] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 本文实证测试多款LLM在波兰国家申诉委员会考试中的表现。虽能通过选择题，模型在判决写作及法规应用上仍存短板，且模型互评与官方评价有明显分歧。结论：现阶段LLM尚不足以替代相关法律领域的人类专家。


<details>
  <summary>Details</summary>
Motivation: 评估现有大语言模型（LLMs）能否通过波兰国家申诉委员会会员资格官方考试，以及模型在模拟考官和判官角色中的实际应用效果。

Method: 针对考试结构，包括公共采购法的选择题和判决写作，构建混合信息检索与抽取流程，测试多款LLM（如GPT-4.1、Claude 4 Sonnet及Bielik-11B-v2.6）在闭卷及多种检索增强生成设置下的表现，并引入‘LLM作为判官’的模型互评方法。

Result: LLM在知识测试部分取得满意分数，但在实际判决写作部分未达到及格线。LLM判官的评价结果与官方委员会判决往往不一致，揭示模型易产生幻觉、引用法律条文错误、逻辑论证薄弱等主要局限。

Conclusion: 当前LLM无法取代波兰公共采购法律审判中的人类法官或独立考官，需法律专家与技术团队密切合作推动模型改进。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [46] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 该文提出REMIND方法，用输入扰动下的模型损失曲线，灵敏检测模型是否真正遗忘目标数据，效果明显优于现有方法，具备实际部署可行性。


<details>
  <summary>Details</summary>
Motivation: 机器反学习旨在移除特定训练数据对模型影响，保障隐私、安全和合规。验证模型是否真正遗忘目标数据对于可依赖性和信任性至关重要。现有方法只评估单一输入的遗忘效果，可能忽略语义相似样本中的残留影响，带来隐私泄露风险。

Method: 提出REMIND方法，通过分析模型在输入微小变化下的损失动态，发现单点评估未能揭示的残留记忆模式。REMIND仅需查询式访问，无需完整模型，泛化性强，适用于不同模型、数据集和同义改写场景。

Result: 实验证明，遗忘数据在损失空间呈现平缓、低陡峭度特征，而保留或无关数据则表现出更剧烈的损失变化。REMIND优于现有评估方法，并在多模型、多数据集、同义输入下表现鲁棒。

Conclusion: REMIND为评估语言模型反学习效果提供了更敏感、可解释的度量方法，为理解模型记忆机制和提升反学习可靠性提供新视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [47] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 本文发现预训练后结合检索与额外计算能显著提升大型语言模型的下游任务表现，说明现有预训练方法对数据价值发掘效率不足。改善检索和计算策略可带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注提升预训练数据集的质量，但很少有工作探究预训练机制从数据中提取知识和观点的效率。本文旨在量化预训练过程中被遗漏的数据价值，并考察该效应在不同规模模型中的变化。

Method: 论文采用检索增强生成（Retrieval Augmented Generation, RAG）及测试时额外计算资源的方法，比较仅预训练与预训练+检索后的性能提升，并在多种标准任务与数据去污染条件下进行验证，同时用更多计算资源解析检索到的上下文以提升模型表现。

Result: 在MMLU、Math-500和SimpleQA等任务上，预训练后的检索显著增加了准确率，并且这一增益在数据去污染后仍然保持。对MMLU任务而言，检索像是计算资源的5倍放大器，并对公开LLaMA 3.1 8B模型带来10个百分点的准确率提升。

Conclusion: 当前主流的预训练方法没有充分利用已有预训练数据集中的信息，存在大量可提升空间。增强检索和分配测试时额外计算资源能够显著提升模型性能。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [48] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 该论文提出了一个低资源消耗、基于图结构的主题标注方法，相比传统方法和一些先进模型（如ChatGPT-3.5），取得了更好的表现且更高效，可用于提升主题抽取结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着非结构化文本数据的快速增长，从文本中提取主题变得越来越重要。现有方法大多依赖高计算量方法，研究希望寻找计算资源消耗更少的替代方案。

Method: 提出一种基于图结构的主题标注方法，将主题词丰富为语义相关词，并分析词之间关系，通过图结构确定主题的准确标签。以此减少对大模型的依赖，提高效率。并与现有方法（包括ChatGPT-3.5）和传统基准进行对比，采用BERTScore和余弦相似度作为评价指标，在两个数据集上验证效果。

Result: 所提方法在BERTScore和余弦相似度指标上持续超越传统基准，与ChatGPT-3.5效果相当且更为高效。

Conclusion: 基于图结构的轻量级主题标注方法能够提升主题可解释性和自动化水平，具备较高的应用价值。未来可进一步探索其在可解释性和自动化上的提升空间。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [49] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 本文提出SSPO，利用句子级重要性比例优化RLVR过程，既解决了GRPO训练崩溃和GSPO采样低效的问题，也提升了大语言模型的推理能力。SSPO在多个数据集上显著超过现有方法，展示了优越的性能和数据利用率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在通过基于可验证奖励的强化学习（RLVR）后，在推理能力上获得了提升，但主流的RLVR算法如GRPO和GSPO各自存在训练不稳定和采样数据利用率低的问题。有必要探索在优化与稳定性间取得平衡的新方法。

Method: 提出SSPO算法，于句子级别计算重要性比例，介于GRPO的Token级与GSPO的Response级之间，结合优点并规避各自缺点。同时，将句子熵应用于PPO-CLIP，实现对剪裁界限的动态调整，促使高熵Token进行探索，低熵Token收紧范围，提高训练的稳定性和效率。

Result: SSPO在五个数据集上的平均得分为46.57，优于GRPO（43.01）和GSPO（44.42），且在三个数据集上取得了最优表现，显示出在数据利用和性能方面的显著提升。

Conclusion: SSPO在平衡训练稳定性与采样数据有效性方面表现优异，明显提升了基于RLVR的大语言模型性能。其句子级重要性比的设计有效融合了GSPO的优点并克服其短板，为后续大模型训练优化提供了新的思路。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [50] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 本文提出了一种结合学习者与参考模型的数据选择方法，通过可学习性评分和批次优化策略，大幅提升了机器翻译微调的数据和计算效率，同时实现了更好的泛化和翻译性能。


<details>
  <summary>Details</summary>
Motivation: 提高机器翻译模型的性能，依赖于高质量数据的有效选择。因此，开发一种优化数据选择的方法对于提升翻译系统的鲁棒性和可靠性至关重要。

Method: 提出了一种专为微调机器翻译系统设计的数据选择方法，结合了学习者模型与预训练参考模型，通过定义可学习性得分系统性地评估训练数据点的效用。此外，采用批次选择策略，考虑数据点间的相关性，提高训练效率。

Result: 在英文-波斯语及其他语对的mBART模型微调实验中，该方法比独立同分布基线提高了5倍的数据效率，并且利用缓存嵌入后，计算效率提升了24%。同时，采用该方法还能增强模型的泛化能力，使翻译表现优于随机选择。

Conclusion: 提出的数据选择方法不仅优化了训练数据的利用效率和计算资源消耗，还提升了机器翻译系统的泛化能力和翻译质量。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [51] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 用1940年挪威问答测试大模型，发现用英语问比挪威语好，模型越大效果越强，为小语种优化的LLM依然不敌通用大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在时序推理能力方面的表现尚不明确。研究者想要探索模型能否根据不同时空背景（比如1940年）回答问题，并比较LLM在不同语言环境下、不同规模和针对特定语言优化的模型的表现。

Method: 研究者采用了一本1940年出版的挪威语问答书中的题目，要求LLM以1940年的视角作答，同时分别用英语和挪威语进行提问。答案以句子的形式给出，通过LLM自动评分并由母语者抽查校对。测试模型包括DeepSeek-R1、Gemma3、Qwen3及Llama3.1系列，还包括一款为挪威语特别优化的大型模型。

Result: 结果显示，用英语提问能获得比挪威语更好的表现，这一结果出乎意料。模型规模越大，表现越好。

Conclusion: 大语言模型在时序推理任务中，对英语的理解优于挪威语，且更大的模型具有更强的推理能力。即使有为小语种特别优化的模型，大型通用模型在英语语境下表现仍更优。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [52] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 本研究提出PTTSD框架，结合多种深度学习技术和概率输出，实现了对临床文本访谈数据的高精度抑郁症预测及不确定性估计，在多个数据集上表现出色，具备良好泛化性和临床解释性。


<details>
  <summary>Details</summary>
Motivation: 当前抑郁症严重程度的预测模型缺乏对不确定性的估计和对时间序列数据的建模，这影响了其在临床决策支持中的应用。

Method: 提出PTTSD（Probabilistic Textual Time Series Depression Detection）框架，利用双向LSTM、自注意力和残差连接组成的序列到序列及序列到单点模型，结合高斯或学生t分布输出头，通过负对数似然训练，能在临床访谈语句层面预测PHQ-8分数，并对预测不确定性进行建模。

Result: 在E-DAIC和DAIC-WOZ数据集上，PTTSD在文本模型中表现优异（MAE分别为3.85和3.55），且预测区间校准良好。消融实验验证了注意力机制和概率建模的重要性，与MentalBERT对比表明方法具备泛化性，校准分析和案例展示了模型的可解释性及临床相关性。

Conclusion: PTTSD模型不仅实现了高精度的抑郁症严重程度预测，还能输出有效的不确定性估计，为临床决策和解释性研究带来价值。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [53] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 论文发布了首个用于评测视觉-语言模型泰语文档理解能力的基准ThaiOCRBench，发现专有模型总体优于开源模型，细致分析了性能瓶颈，并为低资源脚本的模型研究提供了工具和启示。


<details>
  <summary>Details</summary>
Motivation: 当前多模态视觉-语言模型（VLMs）评测主要集中在高资源语言，泰语这一低资源复杂书写体系很少被关注，尤其是在文档结构理解等任务上缺乏评测标准。

Method: 作者提出了ThaiOCRBench基准，包括2808个人工标注的样本、涵盖13个任务类别，并对多种主流专有及开源VLM模型进行了零样本（zero-shot）评测，通过误差分析识别瓶颈和挑战。

Result: 专有模型（如Gemini 2.5 Pro）显著领先于开源模型，尤其是在细粒度文本识别和手写内容提取方面开源模型表现下滑最严重。此外，分析表明主要挑战包括语言偏见、结构错配和虚构内容（hallucination）。

Conclusion: ThaiOCRBench为低资源、复杂书写体系的泰语文档理解模型评测提供了标准化框架和数据基础，对未来提升泰语文档理解能力具有指导意义。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [54] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 针对现有表格推理基准简单、代表性不足的问题，作者提出了覆盖科学和体育等领域、更贴近现实、注重异质性与推理复杂度的RUST-BENCH大基准。实验发现，现有大模型仍在复杂异构表格与多跳推理任务上表现较差，RUST-BENCH可有效推动该领域技术进步。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理基准多聚焦于小型、结构单一的表格，无法真实反映大规模真实环境下数据的复杂性，对大语言模型推理能力评估不全面。实际应用中的表格更长、更异质化且较为领域专属，需跨领域、多跳推理。

Method: 提出RUST-BENCH，一个包含7966个问题、来自2031个真实世界表格的大型基准集合，涵盖科学（NSF资助记录）和体育（NBA统计）两大领域，综合考查模型在规模、异质性、领域性及推理复杂度上的表现。

Result: 实验表明，无论是开源还是专有大语言模型，在异质结构与复杂多跳推理任务上一致表现不佳，显示出当前架构和推理策略的持续性弱点。

Conclusion: RUST-BENCH为表格推理研究建立了更具挑战性的新测试基准，有助于未来推动该领域的发展。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [55] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文提出利用CEFR等级差距进行多轮文本简化（规则+LLM），实现较优简化效果，方法在竞赛中表现靠前并有进一步改进。


<details>
  <summary>Details</summary>
Motivation: 发现文本简化效果与原文和目标的CEFR等级差距高度相关，激发设计针对该特性的多轮简化方案。

Method: 提出两种多轮简化方法：基于规则简化（MRS-Rule）和结合规则与LLM的简化（MRS-Joint），并利用GPT-4o实现生成。

Result: 系统在TSAR-2025任务中获得第7名，并在后续实验中通过优化MRS-Joint方法进一步提升了简化性能。

Conclusion: 多轮简化方法能够提升基于LLM的文本简化性能，尤其是通过合理利用CEFR等级差距。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [56] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 论文通过BFI-2框架系统分析多种LLM在不同参数下的性格表现，发现性格维度与模型架构、调参关系密切，对AI模型选择和治理具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在以人为中心的应用中的普及，了解其类人格行为对于负责任的开发与部署愈发重要。

Method: 系统性评估了六种LLM，采用Big Five Inventory-2（BFI-2）框架，分析在不同采样温度下的性格特质表现。

Result: 发现五大性格维度中有四个存在显著差异，尤其神经质和外向性对采样温度敏感。层次聚类进一步揭示出模型间存在明显分群，提示架构特征可能决定模型的性格稳定性。

Conclusion: LLM呈现出类人格模式，其性格特质与模型架构和参数调节相关。这为模型调优、选择和AI伦理治理提供了新视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [57] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文提出了自动化且与人工一致的RAG评估新框架RAGalyst，通过合成问答数据和优化评估指标，有效提升了评估的科学性和适用性。实验证明不同领域下RAG系统表现差异明显，未有万能方案，强调了系统化评估工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各领域的广泛应用，RAG技术对于提供有事实依据的答案变得至关重要。但在专业、对安全要求高的领域中，如何科学评估RAG系统的表现还存在很大挑战。现有评估方式要么太依赖启发式指标，难以覆盖领域细节；要么采用LLM-as-a-Judge方法，但尚未验证其一致性与人类判断的对齐程度。

Method: 该论文提出了RAGalyst，一个自动且与人类对齐的代理化评估框架，可严格评估专属领域的RAG系统。RAGalyst通过代理化流程生成高质量、合成的问答数据，并包含数据过滤以确保准确性。它还优化了两项核心LLM-as-a-Judge评估指标（答案正确性和可答性），提升了与人工标注结果的相关性。该框架被用于评测包括军事、网络安全和桥梁工程三个专业领域的RAG系统组件。

Result: 实验发现，RAG系统的表现极其依赖具体上下文，没有任何单一的嵌入模型、LLM或超参数配置能在所有场景下最优。此外，论文分析了导致答案正确性分数低的主要原因。这些结果凸显了像RAGalyst这样系统化评估框架的必要性，帮助使用者发现领域特定的权衡并作出更明智的设计选择。

Conclusion: RAGalyst作为领域专用RAG系统的评估工具，展示了其在生成高质量问答数据、优化评估指标以及发现性能权衡等方面的有效性。它为RAG系统可靠性和效能的提升提供了科学依据，对行业实际应用具有重要意义。框架和代码已开源。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [58] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 论文提出框架，结合专家知识和大语言模型，量化并扩展放射报告里的不确定性，并发布新的基准数据集，为不确定性感知的自动分析和研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 放射报告在临床决策中极为重要，但存在大量不确定性，且目前自动化分析难以精细处理这些不确定性。本研究旨在改善对报告中显性与隐性不确定性的建模。

Method: 提出两部分框架：一是利用专家验证与大语言模型（LLM），对常见模糊表达建立参考排序，并据此将每个发现映射到概率值；二是通过专家定义的诊断路径，系统性地为14种常见诊断扩展相关子发现，以显式建模隐性不确定性。

Result: 构建并发布了Lunguage++，这是Lunguage基准的扩展版本，能细致反映放射报告中的不确定性。

Conclusion: 该框架提升了机器处理放射报告时对不确定性的识别和量化能力，促进了更真实的诊断推理及对诊断不确定性临床影响的研究。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [59] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 本文提出利用语言模型的隐藏激活量化和操控推理过程中的不确定性。结果显示激活干预在模型尚未确定最终答案时有效，隐藏激活可预测未来结果，说明模型确实内隐表征了多种推理路径。


<details>
  <summary>Details</summary>
Motivation: 生成式语言模型在推理过程中每一步的选择都可能引导其走向不同的推理路径，因此量化不确定性较为困难。作者希望探究模型内部是否真实表征了这些潜在路径，以及我们能否通过干预激活来操控推理过程的不确定性。

Method: 通过分析并控制语言模型推理过程中的隐藏激活，对模型的不确定性进行量化和预测。实验通过干预激活，观察模型对不同推理路径的选择。

Result: 实验发现，模型对某些Token的不确定性与通过控制激活进行推理路径转向的可行性显著相关。隐藏激活还能预测模型未来的输出分布，证明模型内在表示了可能的推理路径空间。

Conclusion: 激活干预在模型未决定最终答案时效果最佳，因此时模型存在多条可能的推理路径。模型在隐藏激活中内隐地表征了这些可能路径。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [60] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof基于LLM自动结构化分析英语作文，强调可视化与用户体验，能量化论证连贯性并辅助深入理解论证质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动作文评分系统缺乏用户体验及对论证结构的可视化分析。难以帮助用户深入理解作文的结构语义与论证质量。

Method: 采用交互式系统IntelliProof，利用大型语言模型（LLMs）将作文结构化为论证图，将主张节点化、证据作为节点属性，并用边刻画支持或反驳关系。关系由LLM判别并评分，结果可视化，提供分类理由和量化连贯性评分。

Result: IntelliProof能快速探索和分析论证质量，同时保留人工监督。用户可通过自然语言工具更好理解作文结构语义，提升分析效率。系统配备在线演示和实际应用。

Conclusion: IntelliProof系统架构化和可视化作文论证结构，提升自动分析的准确性和体验，促进结构语义和用户理解之间的桥接。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [61] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 目前主流代码LLM仍易生成安全漏洞，作者提出综合风险新指标PE和ME用于更好评估与治理模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM编程助手在软件开发中角色日益关键，所生成安全漏洞对网络安全影响显著。目前缺乏有效度量和修补这些模型脆弱性的工具和方法。

Method: 提出了Prompt Exposure (PE)和Model Exposure (ME)两个新度量指标，用于综合衡量由LLM生成安全漏洞的风险，包括漏洞严重性、生成概率和诱发脆弱代码的prompt方式。

Result: 发现主流最新开源代码大模型在已知经典漏洞场景下依然易受攻击并生成漏洞，并提出更科学的安全漏洞风险度量标准PE和ME。

Conclusion: 现有的开源大语言模型在真实场景下仍然容易生成安全漏洞代码，安全性提升受限于功能性与安全性的权衡。作者提出新指标可更好衡量和暴露LLM生成安全漏洞的风险。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [62] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 通过构建孟加拉语医学大规模问答数据集，并创新性地集成教科书OCR和代理型RAG管道，本研究显著提升了孟加拉医学问答系统的准确性和推理能力，展示了RAG技术在低资源语言医学AI领域的实际价值。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言中，尤其是孟加拉语，构建高准确性的生物医学问答系统仍然是一个巨大挑战，影响了医学知识的公平获取。

Method: 提出了BanglaMedQA和BanglaMMedBench两个孟加拉语医学多项选择题数据集，并应用多种并对比多种检索增强生成（RAG）策略，包括传统、零样本后备、代理式、迭代反馈和聚合RAG，结合教科书和网页检索，利用OCR技术集成孟加拉医学教科书语料库，以及动态选择检索和推理的Agentic RAG管道。

Result: Agentic RAG管道结合openai/gpt-oss-120b模型实现了最高89.54%的准确率，在所有配置中表现最佳，推理质量也更高。

Conclusion: RAG方法有助于提高孟加拉医学问答系统的可靠性和可及性，为多语言医学人工智能研究奠定基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [63] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC通过检索和分类结合，显著优于大型语言模型在事实验证问题上的效率和表现，为实际应用带来更可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前大量错误信息传播，现有事实验证系统依赖大模型生成解释，存在计算成本高以及幻觉问题，难以实际部署。

Method: 提出了DeReC框架，利用通用文本嵌入进行稠密检索，并结合专门的分类模块，替代基于LLM的自回归方法用于事实验证。

Result: 在RAWFC和LIAR-RAW数据集上，DeReC在效率上远超解释型LLM，分别减少95%和92%的运行时间，在准确率上也优于相关方法（如RAWFC数据集F1分数65.58%超过L-Defense的61.20%）。

Conclusion: 经过精心设计的检索类系统在特定任务上能与LLM匹敌甚至超越且具备更优实际部署价值。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [64] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 提出LEASH动态调整推理生成长度，显著提升生成效率，但略有精度损失，无需额外训练，适用性强。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思维(CoT)提示虽然能增强大语言模型的复杂推理能力，但会带来高昂的计算成本，包括令牌消耗和耗时增长。需要一种更为高效的推理生成方式。

Method: 提出了LEASH算法，这是一种无训练的解码算法，通过监控令牌级熵斜率和最大logit间距的改善动态决定推理生成何时停止。当两个信号都趋于稳定时，终止生成。

Result: 在GSM8K和AQuA-RAT基准上的四个指令微调模型测试显示，LEASH平均减少了30-35%的令牌生成量和27%的延迟，但准确率相比传统CoT下降了10个百分点。

Conclusion: LEASH无需额外训练或监督，且对模型无特定要求，是一种简单高效、可替换传统CoT解码的生成方法，但会有一定精度损失。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [65] [Induced matching treewidth and tree-independence number, revisited](https://arxiv.org/abs/2511.03864)
*Noga Alon,Martin Milanič,Paweł Rzążewski*

Main category: cs.DM

TL;DR: 本文研究了两个由树分解定义的图参数：树独立数和诱导匹配树宽。通过利用Kovári–Sós–Turán定理，证明了在$K_{t,t}$-free图类中，两者可以多项式相关，这改进了此前依赖Ramsey定理得到的指数级界限。


<details>
  <summary>Details</summary>
Motivation: 之前已有工作表明，在排除诱导二分团$K_{t,t}$的条件下，树独立数和诱导匹配树宽之间存在上界关系，但此前的证明依赖Ramsey定理导致界限为指数级。本文旨在通过改进方法，将此界限下降到多项式级。

Method: 利用Kovári–Sós–Turán定理进行证明，避免使用Ramsey定理从而改进之前的指数级界限到多项式级界限。

Result: 成功证明了在$K_{t,t}$-free图类中，树独立数与诱导匹配树宽之间为多项式关系，显著优于之前的指数级关系。

Conclusion: 对于任意不含固定二分团 $K_{t,t}$ 作为诱导子图的图类，树独立数和诱导匹配树宽实际上可以多项式相关地界定。

Abstract: We study two graph parameters defined via tree decompositions:
tree-independence number and induced matching treewidth. Both parameters are
defined similarly as treewidth, but with respect to different measures of a
tree decomposition $\mathcal{T}$ of a graph $G$: for tree-independence number,
the measure is the maximum size of an independent set in $G$ included in some
bag of $\mathcal{T}$, while for the induced matching treewidth, the measure is
the maximum size of an induced matching in $G$ such that some bag of
$\mathcal{T}$ contains at least one endpoint of every edge of the matching.
  While the induced matching treewidth of any graph is bounded from above by
its tree-independence number, the family of complete bipartite graphs shows
that small induced matching treewidth does not imply small tree-independence
number. On the other hand, Abrishami, Bria\'nski, Czy\.zewska, McCarty,
Milani\v{c}, Rz\k{a}\.zewski, and Walczak~[SIAM Journal on Discrete
Mathematics, 2025] showed that, if a fixed biclique $K_{t,t}$ is excluded as an
induced subgraph, then the tree-independence number is bounded from above by
some function of the induced matching treewidth. The function resulting from
their proof is exponential even for fixed $t$, as it relies on multiple
applications of Ramsey's theorem. In this note we show, using the
K\"ov\'ari-S\'os-Tur\'an theorem, that for any class of $K_{t,t}$-free graphs,
the two parameters are in fact polynomially related.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [State Complexity of Multiple Concatenation](https://arxiv.org/abs/2511.03814)
*Jozef Jirásek,Galina Jirásková*

Main category: cs.FL

TL;DR: 本文优化了多重连接正则语言的状态复杂度上限证明，解决了相关公开问题，并针对不同字母表和自动机结构给出了更优的复杂度界和构造方法。


<details>
  <summary>Details</summary>
Motivation: 多重连接正则语言时的状态复杂度是自动机理论的重要问题。以往证明复杂且部分案例未给出最优解。论文探讨如何用更简单的方法确定和构造满足复杂度上限的语言，并解决了已公开的问题。

Method: 提供了比文献中更简单的证明方法，构造和分析验证用语言（witness languages），并针对特殊情形（如两状态自动机、单字母循环语言）设计了具体示例与上限证明。

Result: 将如三语言连接的最优字母表大小确定为三元，多个语言连接时在字母表大小上优化，部分条件下上限渐近紧且下界仍为指数级。单字母循环语言及没有尾部终态的一元自动机也得到了精确状态复杂度上限。

Conclusion: 对多个正则语言的多重连接在不同字母表大小下的状态复杂度上限进行了优化，并解决了一些公开问题，给出了精确状态复杂度上限及其构造方法。

Abstract: We describe witness languages meeting the upper bound on the state complexity
of the multiple concatenation of $k$ regular languages over an alphabet of size
$k+1$ with a significantly simpler proof than that in the literature. We also
consider the case where some languages may be recognized by two-state automata.
Then we show that one symbol can be saved, and we define witnesses for the
multiple concatenation of $k$ languages over a $k$-letter alphabet. This solves
an open problem stated by Caron et al. [2018, Fundam. Inform. 160, 255--279].
We prove that for the concatenation of three languages, the ternary alphabet is
optimal. We also show that a trivial upper bound on the state complexity of
multiple concatenation is asymptotically tight for ternary languages, and that
a lower bound remains exponential in the binary case. Finally, we obtain a
tight upper bound for unary cyclic languages and languages recognized by unary
automata that do not have final states in their tails.

</details>


### [67] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: 本文提出了下推自动机可探索性（explorability）这一非确定性度量，并展示该概念在表达力和简洁性上介于历史确定性和完全非确定性之间，揭示其形成无限表达阶层，并覆盖所有上下文无关语言。该工作为自动机非确定性衡量提供了新的角度和理论工具。


<details>
  <summary>Details</summary>
Motivation: 目前关于下推自动机(PDA)的非确定性，已有历史确定性等相关度量，本文希望提出并研究一个更一般的、具操作意义的非确定性度量——explorability（可探索性）。

Method: 提出k-explorable自动机的概念，形式化explorability为度量，分析其与历史确定性及完全非确定性自动机间的表达力与简洁性关系，并推广到输入长度相关的参数化explorability，最后通过严格的理论证明与构造展示不同explorability级别间的层级结构与表达能力。

Result: 证明了explorable PDA在表达力和简洁性上介于历史确定性PDA与完全非确定性PDA之间，并且不同k形成严格层级；参数化explorability到指数级时正好覆盖上下文无关语言。explorable PDA在简洁性上可以比历史确定性PDA多出双指数倍，并且确定性与2-explorable PDA间的简洁性差距不可递归枚举。

Conclusion: explorability作为下推自动机非确定性的度量，兼具强表达力与高操作性，弥补了历史确定性与完全非确定性之间的空白，丰富了自动机理论中的非确定性刻画。

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>
