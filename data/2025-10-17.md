<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [HITrees: Higher-Order Interaction Trees](https://arxiv.org/abs/2510.14558)
*Amir Mohammad Fadaei Ayyam,Michael Sammler*

Main category: cs.PL

TL;DR: 本文提出HITrees，首次在交互树语义框架内支持高阶效应，实现了复杂语义特征（如并发、call/cc）的形式化建模，相关工具与库已在Lean实现并验证。


<details>
  <summary>Details</summary>
Motivation: 交互树作为组成性语义的主流方法，但原有交互树不支持高阶效应（即接受或返回单子运算的效应），而高阶效应对于描述并发、递归和控制流非常重要。现有方法难以在非保护型类型理论中实现对高阶效应的支持。

Method: 本文提出了高阶交互树（HITrees），通过两项核心技术解决高阶效应的表达：一是设计新的效应表示，使带高阶输入的效应不动点可以以归纳类型形式表达；二是利用反函数化方法，将高阶输出编码成一阶表示。该方法在Lean证明助手中实现，并配备包含并发、递归、call/cc等效应的库，还给出了HITrees的两种解释方式（状态转移系统和单子程序）。

Result: HITrees成功实现了高阶效应的支持，并可直接用于描述具有并发和call/cc等复杂语义特征的语言。通过在Lean构建库和示例，验证了其表达能力和可用性。

Conclusion: HITrees是首个支持高阶效应的交互树变体，可在非保护型类型理论中实现复杂语义的形式化建模，扩展了组成性语义的应用范围。

Abstract: Recent years have witnessed the rise of compositional semantics as a
foundation for formal verification of complex systems. In particular,
interaction trees have emerged as a popular denotational semantics. Interaction
trees achieve compositionality by providing a reusable library of effects.
However, their notion of effects does not support higher-order effects, i.e.,
effects that take or return monadic computations. Such effects are essential to
model complex semantic features like parallel composition and call/cc.
  We introduce Higher-Order Interaction Trees (HITrees), the first variant of
interaction trees to support higher-order effects in a non-guarded type theory.
HITrees accomplish this through two key techniques: first, by designing the
notion of effects such that the fixpoints of effects with higher-order input
can be expressed as inductive types inside the type theory; and second, using
defunctionalization to encode higher-order outputs into a first-order
representation. We implement HITrees in the Lean proof assistant, accompanied
by a comprehensive library of effects including concurrency, recursion, and
call/cc. Furthermore, we provide two interpretations of HITrees, as state
transition systems and as monadic programs. To demonstrate the expressiveness
of HITrees, we apply them to define the semantics of a language with parallel
composition and call/cc.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca通过结合大语言模型和自动探索执行环境，几乎完全自动生成Unix等命令的规格说明，可直接应用于提升相关系统分析的自动化和准确性。


<details>
  <summary>Details</summary>
Motivation: 目前许多性能、安全和可靠性优秀的系统需要对不透明组件（如Unix shell命令）进行部分规格说明，但手动创建规格说明过程繁琐且易出错，限制了系统实际应用。

Method: 提出Caruca系统，通过大语言模型将用户文档翻译为结构化调用语法，自动探索合法命令及环境组合，并实际执行，通过系统调用和文件系统级别的插桩，提取并生成命令的关键属性（如可并行性、文件系统前后条件），可直接用于各类现有系统。

Result: 在60个命令（包括GNU Coreutils、POSIX及第三方命令）上的应用表明，Caruca能自动生成正确的命令规格说明，除一个特殊情况外全部正确，彻底消除人工生成规格说明的需求，并成功在最先进的静态分析工具中部署完整规格。

Conclusion: Caruca系统显著简化并自动化了对不透明命令进行规格说明的过程，提高了相关系统的可用性和准确性，为多个依赖规格说明的工具直接提供支持。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [3] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 本文针对大型语言模型驱动的智能体在实际应用里的不稳定和不可信问题，提出了治理优先的新范式，并设计了ArbiterOS架构，显著提升了智能体的可靠性和可用性。


<details>
  <summary>Details</summary>
Motivation: 当前强大的大型语言模型（LLMs）推动了“智能体时代”的到来，使得自主系统能够解决复杂目标。然而，从原型到实际生产应用过程中，智能体经常表现出脆弱、不稳定和不可信的问题，影响在关键应用的部署。作者认为这源于将概率性处理器用传统软件工程的确定性思维方式进行指令管理的范式冲突。

Method: 提出以治理为核心的新范式，通过设计和实现一种正式的架构——ArbiterOS，支持智能体的工程化与原则性治理。

Result: 实现了ArbiterOS架构，为智能体的工程实践提供了系统性和治理优先的解决方案，降低了智能体在实际应用中的不确定性，提高了信任度。

Conclusion: 通过治理优先的架构设计（ArbiterOS），有效缓解了智能体工程中由概率性与确定性范式冲突引发的“技术工艺危机”，推动智能体系统向可信、可控和自动化目标发展。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [4] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: 本文提出了第一个多轮评测基准MT-Sec，发现主流模型多轮编程表现明显下降，安全和正确性亟需在更贴近真实开发流程的评测中加强。


<details>
  <summary>Details</summary>
Motivation: 现有的AI编程助手评测主要集中在一次性交互任务，未能反映实际开发中的多轮迭代过程，且对代码正确性与安全性的联合评估不足。

Method: 提出了MT-Sec基准，通过合成数据方法将已有单轮任务转化为多轮交互序列，并复用原有测试套件，系统评估多个模型和agent方案在多轮编程场景下的正确性与安全性，并涵盖代码差异生成等实际相关但鲜有研究的任务。

Result: 评估32个模型和三种agent方案，发现多轮场景下“正确且安全”输出率较单轮显著下降（20-27%），即使是最先进模型也出现明显性能退步。且在代码差异生成任务中错误和不安全输出率更高。agent方案对单轮有效但对多轮提升效果有限。

Conclusion: 多轮实际开发流程中正确性和安全性显著下降，现有评测不足以覆盖真实场景，应推动联评标准和多轮基准的研究。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [5] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: 提出了可访问性奖励机制 A11yn，使 LLM 生成的网页界面更加符合无障碍标准，通过新的训练和评测数据集显著改善了生成结果的可访问性，证明了系统性可优化这一目标的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）能够根据指令生成功能性和美观性兼具的网页界面，但往往会复制训练数据中的可访问性缺陷，导致生成的界面无法满足不同群体用户的需求。因此，亟需解决现有 LLM 生成网页界面过程中的可访问性问题。

Method: 提出了 A11yn 方法，通过设计一种针对 Web 内容可访问性指南（WCAG）违反情况打分的新型奖励函数，对 LLM 的代码生成能力进行调整。其中，惩罚值根据每个违规的严重程度由可访问性测试引擎识别并进行量化。并构建了 UIReq-6.8K 数据集及 RealUIReq-300 评测基准，以多样化和真实场景支撑训练和评测。

Result: 实验结果显示，A11yn 在生成网页界面时显著优于强基线模型，将不可访问率降低了 60%，同时保持了代码语义准确性与视觉质量。

Conclusion: A11yn 证实了代码生成领域可系统性优化可访问性这一可能性，为 LLM 生成网页界面在可访问性维度上的对齐与提升提供了有效方案。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [6] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 本论文发现，常用的Spectral Signature方法在代码模型后门检测中存在不足。通过深入分析不同参数设置，提出了改进方案和新的性能评估指标，能够更加有效地检测和防御后门攻击，为代码模型安全带来实用提升。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在软件开发中越来越普及，其安全性问题也日益突出，尤其受到后门攻击的威胁，因此亟需有效的检测方法。本文关注通过分析特征表示的Spectral Signature方法作为后门检测手段。

Method: 系统性地评估Spectral Signature方法在代码模型后门攻击检测中的表现，涵盖多种攻击场景和防御配置，分析其优缺点，并研究影响Spectral Signature检测效果的关键参数设置，提出新的代理指标用于性能估算。

Result: 实验发现，当前广泛采用的Spectral Signature设置在代码后门检测任务中表现不佳，针对关键参数的不同设置会影响检测效果。新发现的代理指标能够无需模型重训练更准确地评估Spectral Signature防御性能。

Conclusion: 传统的Spectral Signature防御方法在代码模型的后门检测任务中并不总是有效。本文提出了更优的参数配置，并发现了一种新的评价指标，有助于提升Spectral Signature在代码模型安全防护中的实用性。

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [7] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone系统结合补丁挖掘和模式分析，成功在Linux内核中挖掘大量重复性安全漏洞，验证效果显著，大幅提升漏洞发现效率和软件安全保障。


<details>
  <summary>Details</summary>
Motivation: 在大型程序中，修复漏洞非常耗费时间和精力，尤其是类似的代码段经常存在相同的漏洞但未被发现；单独处理每个漏洞实例负担沉重，并且公开的漏洞报告可能让攻击者发现其他未修复的类似漏洞。

Method: 提出并实现了BugStone系统，结合LLVM和大型语言模型（LLM），利用已有的补丁实例，归纳错误模式，并全局搜索程序，自动识别潜在的重复性模式漏洞。

Result: 在Linux内核中，BugStone发现了超过22,000个新的潜在问题，人工分析其中400个，确认246个有效；还建立了一个含1,900多个安全漏洞的数据集，手动标注并总结80类重复模式和850个修复方法。BugStone在该数据集上达到了92.2%的精准率和79.1%的配对准确率。

Conclusion: 重复性模式漏洞广泛存在，会严重影响软件安全。BugStone能高效自动化地发现和定位这些漏洞，有助于提升大型软件系统的安全性。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [8] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: 论文针对自动驾驶仿真场景的自然语言转Scenic代码提出NL2Scenic数据集与评测框架，涵盖多模型和多种评测方法，并证明中等规模开源大模型在性能和成本上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 测试自动驾驶系统需要精确且可复现的场景仿真。现有基于LLM的自然语言转Scenic代码（NL-to-Scenic）方法面临数据稀缺、复现性差及评价标准不一致等问题。

Method: 提出NL2Scenic开源数据集与评测框架，包含146组自然语言/Scenic脚本对，30个难度分级测试样例，示例检索器，以及14种提示变体（零样本、少样本、推理链、多任务等）。评估了13种大模型（4个闭源、9个开源代码模型）并采用文本和执行指标（如BLEU、ChrF、EDIT-SIM、CrystalBLEU、编译与生成成功率），同时设有专家打分对比。提出EDIT-COMP作为更优的评估代理。

Result: GPT-4o综合表现最佳，Qwen2.5Coder-14B在本地硬件上接近专家水平达88%。示例检索增强少样本提示可明显提升小模型性能。中等规模开源模型如Qwen2.5Coder优于同规模CodeLlama，实现性能与成本的优良平衡。提出的EDIT-COMP指标增强模型排名的可信度。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成和自动驾驶场景设计提供了标准化、可复现的评测基础，证明中等规模开源模型在实际应用中具备成本效益和高可用性。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [9] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 该论文提出了一种融合知识的进化算法框架，通过离线构建优化知识库并在在线阶段利用知识驱动遗传算法，为不同程序自动定制优化序列，实现LLVM IR指令平均减少11%，远超传统-Oz优化，具有较强推广价值。


<details>
  <summary>Details</summary>
Motivation: 编译器优化序列对特定程序性能提升至关重要，但寻优属于NP难题，传统通用优化如-O3/-Oz无法充分发挥软件潜力。

Method: 提出混合型知识引导的进化框架，包括两阶段：离线阶段通过大规模分析构建编译知识库（行为向量、聚类分组、协同图、序列原型）；在线阶段以定制遗传算法利用知识库，实现语义感知重组和有针对性变异优化。

Result: 在7个公开数据集上，相较于opt -Oz基线，平均LLVM IR指令数减少11.0%，展现出发现个性化高性能优化序列的先进能力。

Conclusion: 本方法有效实现了个性化编译优化，显著优于现有通用方法。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [10] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文系统分析了1000份TLE错误案例，重新审视了TLE成因，并首次提出并成功验证了TLE自动修复和评测工具Nettle/Nettle-Eval，实现了高效且可靠的性能错误修复。


<details>
  <summary>Details</summary>
Motivation: TLE（超时）错误困扰大量在线编程用户，信息提示有限且难以定位具体原因，缺乏有效的自动化修复和诊断工具，导致用户频繁放弃。现有对TLE的理解和工具也远不能满足用户需求。

Method: 通过人工分析1000份Codeforces中出现TLE错误的提交，归类其成因和修复过程，并基于此开发融合LLM、编译器反馈与测试用例的自动修复工具Nettle及评测框架Nettle-Eval。对同一数据集进行实验，评估修复效果和正确性。

Result: Nettle能以98.5%的成功率自动修复TLE错误，明显优于现有LLM基线方案，且所有修复均通过Nettle-Eval及平台官方校验，展示出极高可靠性。

Conclusion: 提出并验证了一种专门针对TLE错误的自动化修复工具Nettle，以及配套评测框架Nettle-Eval，能大幅提升TLE问题的修复效率和可靠性。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [11] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix结合路径敏感约束与大语言模型，克服了现有自动程序修复的补丁泛滥和过拟合难题，能更好地修复含复杂结构的程序。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复（APR）方法面临两个主要挑战：生成大量可行但不一定正确的补丁候选；补丁容易对测试案例过拟合。这些问题很大程度上源于难以生成精确的规范。

Method: 提出了一种新的自动程序修复方法PathFix。该方法通过提取正确执行路径中的路径敏感约束，分四步实现修复：1. 追踪有错误输出的错误路径；2. 基于控制流图分析目标正确输出，推导期望路径；3. 沿期望路径解状态约束，生成并评估补丁；4. 验证补丁的正确性。此外，为提升性能和可扩展性，集成了大语言模型（LLM）辅助分析。

Result: 实验结果显示，PathFix在修复复杂程序结构（如循环和递归）方面优于现有方法。

Conclusion: PathFix通过路径敏感约束和LLM辅助，有效减少了无效补丁和过拟合风险，提升了复杂程序修复的能力和效率。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [12] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 针对软件开发多元治理困境，尤其是开源项目，提出了一种可以定义和执行复杂治理政策的新型DSL，促进更高效和自动化的协作。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发中的人类和AI参与者日益多元，现有治理体系难以应对，尤其是在缺乏明文政策的开源软件项目中。如何高效协调多样化利益相关者，成为亟需解决的问题。

Method: 提出一种用于定义和执行复杂治理政策的新型领域特定语言（DSL），以适应包含多元主体（包括AI代理）参与的软件项目协作场景。

Result: 该DSL为实现更强健、可适应性和自动化的治理提供了技术途径，能促进多方更加高效地参与开源软件项目。

Conclusion: 通过引入和应用该DSL，有望解决多元主体协作下治理难题，推动软件项目，特别是开源项目的合作效率和治理质量提升。

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [13] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: 本论文提出了E2EDev框架解决端到端软件开发中的自动化测试问题，并通过HITL多智能体方法降低标注成本。实验表明，现有方法难以高效胜任该任务，凸显亟需更优解决方案。代码和基准已开源。


<details>
  <summary>Details</summary>
Motivation: 在自动化软件开发和测试过程中，准确且高效地满足用户需求并进行测试是一个难题。现有的端到端软件开发（E2ESD）框架和大语言模型（LLM）在实际任务中表现不佳，因此需要新的更有效、更低成本的解决方案。

Method: 提出了E2EDev框架，包括细粒度用户需求、多种BDD测试场景及Python实现、以及基于Behave框架的自动化测试管道。设计了一套“人类在环的多智能体标注框架”（HITL-MAA），以减少人工标注工作量并提升质量。

Result: 实验证明，现有E2ESD框架和LLM后端仍难以高效地解决实际端到端软件开发任务，反映出更有效且成本低的解决方案的迫切需求。

Conclusion: 提出的E2EDev和HITL-MAA方法能够实现高质量、自动化的测试，但目前领域内通用方案仍有较大提升空间。代码和数据集已公开，为后续研究提供基础。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [14] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 本文通过行业访谈与文献综述发现，软件测试教育与行业需求之间存在知识缺口，尤其在AI测试、安全测试及软技能方面需加强。建议未来测试教育关注这些领域以提升从业者能力。


<details>
  <summary>Details</summary>
Motivation: 软件开发环境不断变化，测试人员需不断适应新工具、实践和技能。现有测试教育与行业需求之间存在知识差距，有必要明确测试领域的核心能力与教育不足之处。

Method: 通过两轮焦点小组讨论和跨行业专业人士访谈（覆盖铁路、医疗和软件咨询等领域），并进行小规模文献综述。调查工具由项目成员协同设计并多次迭代完善，确保覆盖行业需求与教育缺口。采用主题式定性分析方法，解析培训方法、培训挑战、质量评估方式、教育与需求差距、未来趋势等。

Result: 发现当前测试教育与行业实际需求之间存在明显知识缺口，特别是在人工智能测试、安全测试和软技能领域。现行业内培训存在挑战，评估方式多样，知识转移方式各异。未来测试教育需关注新兴技术与能力培养。

Conclusion: 软件测试领域需更有效地衔接教育与行业需求，弥补知识差距（尤其是AI测试、安全测试与软技能），提升专业培训质量和知识转移效率。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [15] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: 论文提出了一种基于对抗性强化学习的新框架ATGen，实现自动生成更难测试用例，有效提升大语言模型生成代码的质量和可靠性，远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试用例生成方法依赖静态数据集，存在“固定难度上限”，无法发现新颖或更复杂的bug，影响LLM代码生成的准确性和可靠性。

Method: 采用对抗性强化学习训练测试用例生成器，让其与不断生成更复杂bug的代码生成器对抗，通过强化学习最大化输出准确性与攻击成功率，打破固定难度瓶颈。

Result: ATGen在多项实验中均优于最先进的基线方法，并在实际应用中表现出更强的测试与筛选能力。

Conclusion: ATGen显著优于现有基线方法，能更有效提升大语言模型（LLM）生成代码的可靠性。其生成的测试用例在实际应用中既能作为更好的筛选工具，也能作为更高质量的奖励源用于训练。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [16] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 本文以结构化方法系统识别交通仿真需求，提升实验真实性和参与度，有助于汽车开发和测试。


<details>
  <summary>Details</summary>
Motivation: 当前交通仿真中真实性和实验有效性存在挑战，需要建立系统化方法以保证仿真结果的现实性和参与者的高度投入。

Method: 采用结构化方法，基于每个研究阶段的子目标，系统识别交通仿真需求，并针对微观层面、智能体模型和视觉表现进行技术细化。

Result: 实现了针对微观交通、智能体与视觉表现的高保真仿真设计，推动了实验结果的有效性与参与者的高度参与。

Conclusion: 本文提出的方法能够有效地提升交通仿真的真实性与实验有效性，对汽车开发与测试有实际支持。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [17] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文系统评估了多款LLM agent在网页漏洞自动复现上的实际表现，发现其在复杂环境及服务型漏洞方面能力不足，强调未来需在环境适应与自主决策上取得进步。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在软件工程和网络安全任务中表现出优秀能力，如代码生成和漏洞发现，但对于自动化网页漏洞复现这一关键应用研究不足。该任务旨在将漏洞报告转化为可用的漏洞利用程序，对现实网络安全具有极高价值。

Method: 作者系统性评估了来自软件工程、网络安全和通用领域的20个先进LLM agent，在16个维度（技术能力、环境适应性、用户体验等）上，针对3种典型网页漏洞进行了测试。并在挑选出OpenHands、SWE-agent和CAI三个表现最佳的agent后，对它们在包含80个真实CVE、涵盖7类漏洞和6种网页技术的自建基准数据集上做了深入评测。

Result: LLM agents对于简单的、基于库的漏洞复现有一定成功率，但在复杂的、需要多组件环境的服务型漏洞复现方面表现不佳。如环境配置复杂及认证门槛较高时，agent虽能执行利用代码，但常常无法真正触发漏洞。在认证信息不完整的情况下，其性能降幅超过33%。

Conclusion: 当前LLM agent在自动化漏洞复现领域与实际应用需求仍有较大差距，尤其是在环境适应和自主问题解决能力方面亟需改进。相关技术需进一步升级以满足可靠漏洞复现的需求。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [18] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 该论文提出用名称预测内聚性（NPC）指标无监督检测源代码注入攻击，实验证明在极端不均衡场景下依然有效，可提升检测供应链攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击虽然罕见，但对软件安全威胁巨大，且自动检测恶意代码注入很困难，因为需理解代码意图及其上下文。作者希望找到无需监督，也能高效检测注入恶意代码的新方法。

Method: 提出无监督方法，通过名称预测内聚性（NPC）指标，比较恶意代码注入与普通函数变更对函数内聚性的影响，进而发现可疑代码。实验采用369个开源C++仓库、54,707个函数的真实数据。

Result: 代码注入会显著降低函数内聚性，所用变量命名更短更不具描述性。使用NPC指标检测高内聚函数，能在极度数据不均衡（1:1,000及1:10,000）下实现Precision@100分别为36.41%和12.47%。证明这种新方法有助于发现供应链攻击。

Conclusion: 通过自动化衡量源代码内聚性，特别是基于名称预测的内聚性指标，可以有效帮助识别供应链攻击，提升源代码整体安全性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [19] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 论文分析了Google从x86到Arm的大规模ISA迁移，提出迁移任务新分类，阐述自动化与AI在迁移流程中的作用，指出当前仍存的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 将代码库从一种指令集架构（ISA）迁移到另一种是工程上的一大挑战。近期，云服务厂商普遍采用Arm架构（除了x86），推动了相关需求，但学术界对此关注有限。

Method: 通过分析Google大规模从x86到Arm的迁移案例（涉及约4万次代码提交），总结并归纳了ISA迁移涉及的主要任务，并揭示了这些任务中自动化和AI的应用。

Result: 构建出ISA迁移任务的分类体系，展示了Google如何自动化地完成许多迁移工作，以及AI技术在自动解决这些任务中的作用。同时，指出了一些仍然具有挑战性的任务，并提出了需要进一步研究的问题。

Conclusion: ISA迁移已经不再以二进制翻译为主要难点。现有的开源生态系统及自动化与AI工具，使得重编译和自动化迁移变得可行，但仍有部分复杂任务亟待创新性解决。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [20] [T-BAT semantics and its logics](https://arxiv.org/abs/2510.14361)
*Pawel Pawlowski*

Main category: cs.LO

TL;DR: T-BAT是一种捕捉非正式可证性的四值逻辑，文章证明其完备性、深入分析其语义，并讨论了从模态逻辑角度下的公理结构。


<details>
  <summary>Details</summary>
Motivation: 现有的形式可证性与实际数学实践中的‘非正式可证性’存在差异，T-BAT逻辑旨在填补这一空白，用以模型化数学中经常遇到的非正式可证性概念，并研究其相关形式性质。

Method: 引入T-BAT非确定性四值逻辑体系，通过形式化其语义并变换真值为特定表达式，从而证明该体系下所有可定义逻辑的完备性。同时，利用Kripke语义方法，从模态角度分析相关公理及其所诱导的框架条件，并尝试对T-BAT逻辑进行直观公理化。

Result: 系统论证了T-BAT语义中各种弱化和加强情形下可定义逻辑的公理体系，证明了这些逻辑的完备性，给出直观的T-BAT公理化表达，并从模态视角剖析了大致的语义结构。

Conclusion: T-BAT逻辑及其四值非确定性语义有力地刻画了非正式可证性的复杂性，并通过与模态语义的结合，为理解和应用此类可证性概念提供了一套理论基础和公理化体系。

Abstract: \textbf{T-BAT} logic is a formal system designed to express the notion of
informal provability. This type of provability is closely related to
mathematical practice and is quite often contrasted with formal provability,
understood as a formal derivation in an appropriate formal system.
\textbf{T-BAT} is a non-deterministic four-valued logic. The logical values in
\textbf{T-BAT} semantics convey not only the information whether a given
formula is true but also about its provability status.
  The primary aim of our paper is to study the proposed four-valued
non-deterministic semantics. We look into the intricacies of the interactions
between various weakenings and strengthenings of the semantics with axioms that
they induce. We prove the completeness of all the logics that are definable in
this semantics by transforming truth values into specific expressions
formulated within the object language of the semantics. Additionally, we
utilize Kripke semantics to examine these axioms from a modal perspective by
providing a frame condition that they induce. The secondary aim of this paper
is to provide an intuitive axiomatization of \textbf{T-BAT} logic.

</details>


### [21] [Optimization Modulo Integer Linear-Exponential Programs](https://arxiv.org/abs/2510.14550)
*S Hitarth,Alessio Mansutti,Guruprerana Shabadi*

Main category: cs.LO

TL;DR: 本文对含指数和取余操作的整数线性优化问题进行了复杂性分析，提出最优解的表达结构和判定算法，并将该优化问题复杂性归入FNP^NP类。


<details>
  <summary>Details</summary>
Motivation: 论文研究一个新的优化问题，即包含指数函数和取余函数的整数线性优化问题，这比传统的整数线性规划更复杂。此前已知其可行性判定问题为NP-完全，因此研究其优化问题的复杂性具有理论意义和实际价值。

Method: 通过理论分析和算法设计，作者证明最优解可以以整数线性-指数直线程序（ILESLP）的形式表示，并设计了一个基于整数分解预言机，在多项式时间内判定一个ILESLP是否为解的方法，同时可以比较两个解目标函数值。

Result: 1.最优解可用ILESLP简洁表示。2.存在一个基于整数分解预言机的多项式时间算法用于判定解和比较解的优劣。3.将该类问题优化复杂性归入扩展后的NPO范畴，位于FNP^NP中。

Conclusion: 论文拓宽了整数线性优化问题的复杂性研究，将包含指数与取余操作的优化问题归入复杂性类FNP^NP，并提出了具有理论指导意义的最优解表示与判定算法。

Abstract: This paper presents the first study of the complexity of the optimization
problem for integer linear-exponential programs which extend classical integer
linear programs with the exponential function $x \mapsto 2^x$ and the remainder
function ${(x,y) \mapsto (x \bmod 2^y)}$. The problem of deciding if such a
program has a solution was recently shown to be NP-complete in [Chistikov et
al., ICALP'24]. The optimization problem instead asks for a solution that
maximizes (or minimizes) a linear-exponential objective function, subject to
the constraints of an integer linear-exponential program. We establish the
following results:
  1. If an optimal solution exists, then one of them can be succinctly
represented as an integer linear-exponential straight-line program (ILESLP): an
arithmetic circuit whose gates always output an integer value (by construction)
and implement the operations of addition, exponentiation, and multiplication by
rational numbers.
  2. There is an algorithm that runs in polynomial time, given access to an
integer factoring oracle, which determines whether an ILESLP encodes a solution
to an integer linear-exponential program. This algorithm can also be used to
compare the values taken by the objective function on two given solutions.
  Building on these results, we place the optimization problem for integer
linear-exponential programs within an extension of the optimization class
$\text{NPO}$ that lies within $\text{FNP}^{\text{NP}}$. In essence, this
extension forgoes determining the optimal solution via binary search.

</details>


### [22] [Problems and Consequences of Bilateral Notions of (Meta-)Derivability](https://arxiv.org/abs/2510.14619)
*Sara Ayhan*

Main category: cs.LO

TL;DR: 论文讨论了如何在证明论语义框架下，通过自然演绎和序列演算表示逻辑联结词的证伪性和证成性。指出在序列演算中双边主义实现上存在深层不对称问题，并分析成因及提出可能解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的证明论语义系统多侧重于给出联结词的可证性条件，忽略了可反驳性条件。双边主义提出应同时展示二者，但在不同系统（如自然演绎与序列演算）中表达这一思想的方法和难点有所不同，尤其在序列演算系统中遇到概念性障碍。

Method: 作者从证明论语义的双边主义视角出发，分析了自然演绎与序列演算系统如何体现命题联结词的可证性与可反驳性。特别关注序列演算中关于双重导出关系的表达方式，并讨论了对序列符号和横线进行对偶化过程中的理论和技术难点。

Result: 序列演算系统中的推理与反驳的平衡存在本质难题，揭示了证明与反驳概念上的不对称。论文分析了问题根源，并提供了维持双边主义平衡的潜在方式。

Conclusion: 论文分析了在序列演算中实现真正双边（bilateralist）推理系统时，存在在证明与反驳之间失衡的深层概念性问题，并探讨了可能的解决方案来保持必要的平衡。

Abstract: A bilateralist take on proof-theoretic semantics can be understood as
demanding of a proof system to display not only rules giving the connectives'
provability conditions but also their refutability conditions. On such a view,
then, a system with two derivability relations is obtained, which can be quite
naturally expressed in a proof system of natural deduction but which faces
obstacles in a sequent calculus representation. Since in a sequent calculus
there are two derivability relations inherent, one expressed by the sequent
sign and one by the horizontal lines holding between sequents, in a truly
bilateral calculus both need to be dualized. While dualizing the sequent sign
is rather straightforwardly corresponding to dualizing the horizontal lines in
natural deduction, dualizing the horizontal lines in sequent calculus, uncovers
problems that, as will be argued in this paper, shed light on deeper conceptual
issues concerning an imbalance between the notions of proof vs. refutation. The
roots of this problem will be further analyzed and possible solutions on how to
retain a bilaterally desired balance in our system are presented.

</details>


### [23] [Admissibility of Substitution Rule in Cyclic-Proof Systems](https://arxiv.org/abs/2510.14749)
*Kenji Saotome,Koji Nakazawa*

Main category: cs.LO

TL;DR: 本文研究循环证明系统中substitution规则的可允许性，首次证明了在有cut规则的CLKID^omega系统中substitution规则是可允许的，并给出了一种具体转换方法。对替换限制后结果可推广到更多相关系统。


<details>
  <summary>Details</summary>
Motivation: substitution规则在循环证明系统中增加了理论分析复杂性与证明搜索的计算开销，因此希望实现其可允许性。非循环系统可通过局部变换证明可允许性，但这些变换破坏循环结构，难以直接适用。之前观点认为substitution规则在CLKID^omega系统中可能不可允许。

Method: 将循环证明展开为无限形式、提升substitution规则、再建立回边，最终构造出无substitution规则的循环证明。

Result: 证明了在有cut规则时，substitution规则在CLKID^omega系统中可允许。对替换做额外限制后，结果适用于更广系统。

Conclusion: 在引入cut规则的前提下，substitution规则在CLKID^omega循环证明系统中是可允许的。如果对子换作了限制（如不包含函数符号），则该结果可推广至更广泛的系统，包括无cut的CLKID^omega以及分离逻辑的循环证明系统。

Abstract: This paper investigates the admissibility of the substitution rule in
cyclic-proof systems. The substitution rule complicates theoretical case
analysis and increases computational cost in proof search since every sequent
can be a conclusion of an instance of the substitution rule; hence,
admissibility is desirable on both fronts. While admissibility is often shown
by local proof transformations in non-cyclic systems, such transformations may
disrupt cyclic structure and do not readily apply. Prior remarks suggested that
the substitution rule is likely nonadmissible in the cyclic-proof system
CLKID^omega for first-order logic with inductive predicates. In this paper, we
prove admissibility in CLKID^omega, assuming the presence of the cut rule. Our
approach unfolds a cyclic proof into an infinitary form, lifts the substitution
rules, and places back edges to construct a cyclic proof without the
substitution rule. If we restrict substitutions to exclude function symbols,
the result extends to a broader class of systems, including cut-free
CLKID^omega and cyclic-proof systems for the separation logic.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](https://arxiv.org/abs/2510.14972)
*Yinxi Li,Yuntian Deng,Pengyu Nie*

Main category: cs.CL

TL;DR: 本文发现主流代码大语言模型的分词方式容易因代码表层微小变化而影响模型表现，提出TokDrift框架做系统分析，证明语法感知分词对于模型可靠代码生成和理解至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM的分词器主要由统计驱动，缺乏语法意识，而代码格式（如空格、变量名）的一点微小变动可能带来分词上的显著差异，进而影响模型行为。因此需系统化衡量和揭示这一误差源的影响。

Method: 提出了TokDrift框架，通过语义保持的重写规则生成仅在分词层面有差异的代码变种，用以衡量分词不一致对模型性能的影响；并对九个不同规模（包含30B参数级别）的代码LLM做了分层分析。

Result: 实验发现，即使是微小的代码格式变化，也会造成模型行为的大幅变化。分层分析显示，这一问题源自早期编码层，子词分割未能很好映射到语法token边界。这暴露了分词对代码建模的重大影响。

Conclusion: 现有的基于子词分词器的代码大语言模型（LLM）对代码理解和生成存在隐藏障碍，因为分词不是基于语法，导致语义一致但格式不同的代码片段会被分词器不同地处理，从而影响模型表现。为提高代码LLM的可靠性，未来需要采用语法感知的分词方法。

Abstract: Large language models (LLMs) for code rely on subword tokenizers, such as
byte-pair encoding (BPE), learned from mixed natural language text and
programming language code but driven by statistics rather than grammar. As a
result, semantically identical code snippets can be tokenized differently
depending on superficial factors such as whitespace or identifier naming. To
measure the impact of this misalignment, we introduce TokDrift, a framework
that applies semantic-preserving rewrite rules to create code variants
differing only in tokenization. Across nine code LLMs, including large ones
with over 30B parameters, even minor formatting changes can cause substantial
shifts in model behavior. Layer-wise analysis shows that the issue originates
in early embeddings, where subword segmentation fails to capture grammar token
boundaries. Our findings identify misaligned tokenization as a hidden obstacle
to reliable code understanding and generation, highlighting the need for
grammar-aware tokenization for future code LLMs.

</details>


### [25] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 针对Text-to-SQL跨语种表现不佳的问题，作者引入GRPO和多语言语义对比奖励，有效提升了查询语义和执行准确率，且在低资源条件下小模型也能超越大模型。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法只关注于查询是否可执行，忽视了查询语义一致性与执行结果的正确性的问题，尤其在跨语言场景下，非英语执行准确率显著下降。

Method: 提出了一种结合Group Relative Policy Optimization（GRPO）和多语言对比奖励信号的新框架，通过结合语义相似性奖励引导模型更好地实现SQL生成与用户意图的语义对齐。

Result: 在七语言MultiSpider数据集上，采用GRPO微调LLaMA-3-3B模型将执行准确率提升至87.4%，语义准确率提升至52.29%；引入对比奖励信号后，平均语义准确率进一步提升到59.14%，越南语可提升至+10个百分点，且3B模型训练效果优于零样本8B模型，同时只需要3000个强化学习训练样本。

Conclusion: 通过使用对比奖励和GRPO框架可以有效提升跨语言Text-to-SQL系统的语义和执行准确率，无需大规模训练数据。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [26] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: 本文认为，可解释AI与临床需求之间的脱节本质上是翻译问题，提出以大语言模型为核心的生成式操作框架，可将XAI技术输出转化为临床可用的内容，进而促进AI在精神健康筛查等临床场景的应用，并给出了未来发展战略。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释人工智能（XAI）被认为是实现精神健康筛查机器学习真正应用的关键，但目前XAI技术生成的解释难以被临床医生和患者理解，导致现实应用与实验室成果之间存在显著鸿沟。

Method: 提出了一种新的生成式操作框架，将大语言模型（LLM）作为翻译引擎，把多种XAI工具的技术性输出和临床指南相结合，通过RAG等方式自动生成易于理解、符合循证医学的临床叙述。并系统分析了该框架的组成部分及XAI系统的发展演变。

Result: 该框架可以直接解决当前XAI在实际临床应用中面临的主要障碍，包括工作流程整合、偏见缓解和针对不同利益相关者的沟通问题。

Conclusion: 论文为推动可解释AI在临床中实现可信、可用、可集成的应用提出了战略路径，并为如何实现由单一数据解释向集成化临床决策的转变提供了指导。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [27] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: STELA是一种无需Logits访问的、基于语法灵活度动态调节水印强度的新方法，实现了更强的公开可验证检测与高文本质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，其生成内容的可追溯性和可信度变得越来越重要，尤其是在AI治理和建立用户信任方面。为了实现这一目标，公开可验证的水印方法变得尤为关键。然而，目前主流方法在文本质量与检测鲁棒性间存在权衡，而且往往依赖于模型内部的信息（如logits），限制了其公开检测能力。

Method: 提出STELA框架，根据POS（词性）n元模型的语言不确定性动态调整水印强度：在语法受限的环境中弱化水印信号以保持文本质量，而在语言灵活性更高的环境中增强水印以提升可检测性。同时，其检测器无需访问原始模型logits数据，实现公开可验证检测。

Result: 通过在结构类型迥异的多种语言（英语、中文和韩语）上的大量实验，STELA的检测鲁棒性优于以往方法，并且可以在不开启模型内部信息的情况下，支持公开检测。代码开源。

Conclusion: STELA在大语言模型生成文本的公开可验证水印检测和质量保持之间取得了新的平衡，为可信AI生态系统提供了更强有力的技术支持。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [28] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 论文提出了一种利用用户真实交互生成的配对偏好数据，并通过模型响应对比和EM算法筛选高质量标注的方法，最终有效提升了LLM的数据质量和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前对于大型语言模型（LLM）调优的配对偏好数据主要依赖专业人工标注，但用户在实际交互中也产生了大量偏好标签。虽然用户在判断自身需求的响应上拥有专业性，但缺乏质量控制。该论文旨在探索利用用户真实交互作为配对偏好数据的新方法，并解决标签质量参差不齐的问题。

Method: 论文提出生成两个不同模型（或同一模型不同版本）的响应，通过用户对比选择，基于这种非对称性设计用户行为模型。进一步，应用期望最大化（EM）算法估算用户的隐性质量因子，并据此筛选用户标注的数据。

Result: 所提出的方法能有效捕捉用户行为特征，并对偏好数据进行高质量筛选，提升了LLM调优时数据的整体质量。下游任务实验验证了方法在用户行为识别和数据过滤方面的有效性。

Conclusion: 用户真实交互产生的配对偏好数据对于LLM调优具有重要价值，但数据质量需控制。引入模型响应对比和用户行为建模，结合EM算法筛选用户数据，有助于提升LLM对用户真实需求的对齐效果。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [29] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 该论文提出了知情路由与轻量特征预测器，从token重要性与可恢复性双维度，显著提升大语言模型的推理效率和性能，减少计算与训练成本，优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在实际应用中推理成本高，限制了其部署。近年来一些方法尝试通过动态分配token级计算资源提升效率，但通常采用贪婪路由策略，导致信息不可逆损失和次优的token选择。

Method: 提出了一种新的路由范式——知情路由（informed routing）。核心是评估token当前重要性及其可恢复性。为此设计了一个轻量级特征预测器（LFF），在路由决策前预测输出，使得模型可灵活地选择执行或近似计算。

Result: 在多项语言建模和推理任务中，知情路由方法在不同稀疏度下都实现了效率与性能的新最优权衡。即使无终端LoRA微调，其性能也能匹配或超越需完整微调的强基线方法，同时训练时间减少逾50%。

Conclusion: 知情路由通过结合token重要性与可恢复性，采用灵活的执行或近似策略，在降低大模型推理与训练成本的同时，保持甚至提升模型性能。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [30] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 作者提出了融合头重要性分数与注意力熵的HIES剪枝方法，有效提升了Transformer模型的压缩质量与稳定性，优于传统HIS方法。


<details>
  <summary>Details</summary>
Motivation: Transformer模型性能卓越但结构复杂，推理和部署效率低；现有HIS方法忽略了注意力多样性，因此需要新的剪枝准则。

Method: HIES将Head Importance Score与Attention Entropy相结合，作为剪枝的依据，并与仅用HIS的方法进行比较。

Result: HIES剪枝方法相比于HIS，模型质量提升15.2%，稳定性提升2.04倍；在实现显著压缩的同时不牺牲准确率和稳定性。

Conclusion: 提出了一种新的剪枝标准HIES，通过结合头重要性分数与注意力熵，实现更高效和稳定的Transformer模型剪枝。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [31] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: 本文提出了ConDABench，会话式数据分析基准框架，能生成和评测复杂的交互式数据分析任务。实验显示现有大模型在长期交互任务中的表现仍有不足，为未来真正协作型模型的进展提供了新的测量工具。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的数据分析任务中，目标往往定义不明确且数据不够干净，需要用户交互来理解和澄清用户意图，这是解决复杂任务的关键。然而，目前评估大模型的数据分析基准不支持复杂交互。

Method: 提出了ConDABench框架，包括多智能体工作流生成现实基准、1420个会话式数据分析任务及评测工具，能系统评估会话式数据分析工具。

Result: 对主流大模型的实验表明，新一代模型虽然可解决更多任务，但在需要持续、深入交互的任务表现不佳。

Conclusion: ConDABench为模型研发者提供了衡量模型协作与解决复杂交互任务进展的新方向。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [32] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文提出了一种基于相似度的黑盒不确定性量化方法，通过实证证明其置信度校准效果优于现有方法，为提升大语言模型可信度提供了更有效的工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成输出时，如何衡量其自身的“不确定性”或“自信度”，是实现可信AI系统中的关键挑战。尤其在无法访问模型内部信息的情况下，对黑盒不确定性量化方法的需求日益上升，以增强系统适应性和鲁棒性。

Method: 提出了一种基于非语言化相似度聚合的高层框架，用于不确定性量化（UQ）。该框架涵盖多种适用于复杂生成任务的UQ方法，并进一步提出了利用小规模训练集训练的置信度估算新技巧。

Result: 通过在问答、摘要和文本到SQL等多样任务的数据集上进行实证研究，实验结果显示所提出的相似度方法比现有基线方法能更好地校准置信度。

Conclusion: 采用生成结果之间一致性作为输出置信度的代理，有助于实现更为准确的置信度估算，提升黑盒UQ方法在多个复杂任务中的实用性和可靠性。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [33] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 针对仇恨言论检测中因文化差异导致的标签偏见和理解差异，作者提出文化感知方法，显著提升检测表现。


<details>
  <summary>Details</summary>
Motivation: 目前的仇恨言论检测方法忽略了训练标签的偏见以及不同文化背景下个人对仇恨的不同理解，需要新的方法来处理数据稀疏、文化纠缠和标签模糊等实际挑战。

Method: 通过构建个体的“仇恨子空间”，采用文化属性组合缓解数据稀疏性，利用标签传播方法处理文化纠缠和标签模糊性，有效捕捉每种文化组合的特征。

Result: 该方法在所有评价指标上，平均提升了1.05%，优于最先进方法。

Conclusion: 提出的文化感知框架能更有效地处理仇恨言论检测任务中与文化相关的挑战，分类性能优于现有方法。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [34] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 用大型语言模型全自动从评论中抽取产品本体，效果超过BERT基线，促进了本体构建自动化。


<details>
  <summary>Details</summary>
Motivation: 当前数字时代，非结构化文本信息量巨大，手动构建本体不仅耗时、昂贵且繁琐。特别是在如电商等领域，产品信息的有序组织极为关键。

Method: 利用最新的大型语言模型（LLM），提出一种全自动方法，从原始产品评论文本中抽取本体结构（如整体-部分关系）。

Result: 实验表明，该方法生成的本体在使用LLM作为评判者进行评测时，优于现有的BERT基线方法。

Conclusion: 本文方法有效提升了本体抽取的自动化程度，为LLM在产品等领域的本体抽取应用奠定了基础。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [35] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 提出了无需模型和检索器访问权限的高效知识中毒攻击ADMIT，能大幅翻转RAG系统中的事实核查结论，攻击成功率高、覆盖面广，暴露出现实RAG方案的重大安全隐患。


<details>
  <summary>Details</summary>
Motivation: 知识中毒对以检索增强生成（RAG）为基础的系统构成了重大威胁，通过向知识库注入恶意内容误导大型语言模型（LLMs），使其输出受攻击方控制的结果。尽管以前的研究已揭示LLMs对恶意内容的易感性，但现实世界中的事实核查场景更为复杂，因真实证据通常占主导。

Method: 本文提出ADMIT（Adversarial Multi-Injection Technique）攻击方法，这是一种少样本、语义对齐的中毒攻击，无需获取目标LLM、检索器或进行词级操作即可翻转事实核查判决并诱发欺骗性解释。

Result: 大量实验显示，ADMIT方法可在4种检索器、11个LLM和4个跨领域基准测试中良好迁移，平均攻击成功率高达86%，且中毒率极低（0.93×10⁻⁶），即使在存在强有力反证时也能保持稳健。

Conclusion: ADMIT显著提升了攻击成功率（较现有方法提升11.2%），揭示了现实RAG事实核查系统的严重脆弱性。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [36] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: 本文提出了面向医疗EHR数据的基础模型SerialBEHRT，通过结构化序列预训练显著提升了临床任务中的表现，特别强调了时序信息在医疗预训练中的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在医疗领域展现出通过大规模临床数据学习通用表征的潜力，但电子健康记录(EHR)的表格与事件特性与自然语言模型的序列假设存在结构性不匹配，限制了模型捕捉患者纵向依赖的能力。

Method: 提出SerialBEHRT基础模型，通过在SciBERT的基础上增加针对结构化EHR序列的预训练，使模型能够编码临床事件之间的时序和语境关系，获得更丰富的患者表征。

Result: 在抗生素敏感性预测任务上，通过与现有EHR表征方法的广泛基准比较，SerialBEHRT表现出更优异且更稳定的性能。

Conclusion: 在医疗基础模型预训练中，时序序列化对于提升模型能力具有重要意义，SerialBEHRT在捕捉临床时序关系和提升表征质量方面优于现有方法。

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [37] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: 本文提出DynaSpec，通过基于上下文的动态token短列表选择，有效解决了LLM推测解码时drafter输出头受限于大词表而导致的推理瓶颈，兼顾速度和准确性，在标准测试上优于现有固定高频词方案。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型（LLM）词表不断扩展，推理加速方法如推测解码（speculative decoding）面临瓶颈，尤其是在drafter输出头需要处理巨大的词表时，影响了系统整体的推理速度。现有方法采用固定高频词短列表，却存在通用性不足和对低频、领域词抑制等问题。

Method: 提出DynaSpec，一种基于上下文动态短列表选择机制。具体做法为：设计轻量、粗粒度的元分类器，根据上下文将输入路由到少量token集群，drafter仅在这些集群的联合top-k词表上操作，验证环节依然保持全词表，保证精确性。通过在不同计算流上并行进行draft编码和短列表生成，可提升效率。

Result: 在标准推测解码基准测试中，DynaSpec相较于固定短列表方法，平均接受长度一致提升，并且能够在更小短列表下保持接受率不降。

Conclusion: DynaSpec 通过上下文相关、动态、高效的短列表构建机制，显著提升推测解码推理速度，并保持精度与可扩展性，优于现有固定频率短列表技术。

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [38] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 通过增加可学习投影层于多任务adapter结构，实现了云端和移动设备上高效且快速的摘要+翻译复合任务处理，显著提升了多任务协同性能，验证了其在资源受限场景下的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前主流的参数高效微调方法（如LoRA）在处理多任务时表现有限，尤其在同时执行复杂任务（如对长对话进行翻译及总结）面临瓶颈。现有方法通常难以兼顾效率和多任务协同执行。

Method: 提出了一种新方法，在总结和翻译的adapter组合结构上增加可学习投影层，使多任务（如翻译与摘要）能高效协作，同时减少计算成本，无需大量重训练或顺序处理。

Result: 实验在云端和移动设备（Android app）上进行，结果表明新方法在多任务执行速度和效果上均表现出色，尤其适合对速度和资源有限的真实应用场景。

Conclusion: 所提方法有效解决了高效复合多任务（如翻译加总结）执行的难题，兼顾性能和资源消耗，为实际高效应用提供了新思路。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [39] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出了一种轻量级推理阶段技术，通过主成分分析找出语言方向并引导token嵌入，显著减少大模型中的语码转换，提升语言分类准确率和语义一致性，计算开销极低，仅需少量平行数据即可实现。


<details>
  <summary>Details</summary>
Motivation: 多语种大模型常出现非预期的语码转换（code-switching），影响模型在实际应用中的表现和可信度，因此需要一种轻量且有效的方法来控制和保持模型生成语言的一致性。

Method: 提出了一种基于潜在空间语言引导的推理阶段技术：通过对平行翻译进行主成分分析（PCA）找到语言方向，然后沿这些方向调整token嵌入，实现对语言身份的控制。只需少量平行数据即可校准，且计算开销极小。

Result: 使用单一主成分即可实现95-99%的语言分类准确率，在Qwen2.5和Llama-3.2等模型上显著降低了下一个token分布的偏移（最高降幅达42%），同时语义保持良好。进一步分析显示，语言身份信息主要集中于模型最后几层，并且可以线性分离。

Conclusion: 该方法可以有效减少多语种大型语言模型中的意外语码转换，并能高精度保持语言身份信息，从而提升下游任务的可靠性。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [40] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: 作者通过熵度量分析LLMs的推理过程，发现成功推理不是信息密度均匀流动，而是表现为大幅波动，这与人类交流规律相反，暗示未来可解释和灵活推理模型的新设计方向。


<details>
  <summary>Details</summary>
Motivation: 人类交流倾向于保持均匀的信息流，而LLMs在链式推理中往往步骤不易解释或不真实，因此需要分析其推理轨迹中的信息流动特征。

Method: 提出了基于熵的度量方法，分析LLMs在推理过程中的信息流，并通过三个复杂数学基准测试进行了实验。

Result: 正确推理结果存在明显的信息密度起伏，与人类交流中的均匀信息流形成对比，挑战了关于机器推理的固有假设，并为模型的可解释性与自适应设计提供了新方向。

Conclusion: 该研究发现，LLMs在成功推理过程中体现出全局性的信息密度非均匀波动，这与人类交流保持信息稳定流动的模式显著不同。

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [41] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: 本文提出EvoEdit方法解决大模型连续编辑中的知识干扰问题，实现高效稳定编辑，在基准测试上表现优异且速度提升明显。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要不断更新以纠正过时或错误的知识，目前模型编辑是一种无需重新训练即可实现定向修改的方法，但现有方法在多次连续编辑时会出现知识干扰的问题。

Method: 提出了一种新策略EvoEdit，通过顺序零空间对齐来进行模型编辑，从而减少连续编辑带来的灾难性干扰。

Result: EvoEdit在真实序列知识编辑基准测试中，相比于当前主流方法，表现更优或持平，同时速度提升高达3.53倍。

Conclusion: EvoEdit不仅有效缓解了连续知识编辑中的干扰问题，还为动态信息环境下的LLM模型设计提供了理论支持和高效的解决方案。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [42] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: 提出一致性评测基准，发现不同模型和话题下事实一致性差异大，Grok-3表现最佳，公开工具促进公平评测和一致性提示。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面对不同用户时，是否会产生事实上的不一致？迫切需要有独立、公正的基准来衡量不同模型在不同人群中的一致性表现，以避免信息偏见和提高模型透明度。

Method: 提出独立基准ConsistencyAI，针对19个大语言模型进行测试。每个模型分别针对15个话题、100种人格画像重复请求回答5个事实。通过句向量嵌入和余弦相似度计算跨画像事实一致性得分。

Result: 整体一致性分数在0.7896~0.9065之间，平均为0.8656。Grok-3一致性最高，部分轻量模型最低。不同话题一致性有明显区别。发布了评测代码和交互演示，促进模型公正性和一致性提示方法发展。

Conclusion: 不同的大语言模型在不同用户画像下的事实一致性存在显著差异，从模型提供方和话题两个层面均影响一致性。xAI的Grok-3模型表现出最高的事实一致性，而轻量级模型一致性较低。部分话题如就业市场一致性最低，G7领导人最高，敏感话题如疫苗或以巴冲突则因模型提供方而异。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [43] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: BenchPress系统结合LLM和人类专家，加速企业私有数据仓库的text-to-SQL基准数据集构建，显著节省标注成本并提高质量，支持领域特定模型评测，其工具已开放。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，现有大型语言模型（LLMs）在公共数据集上的文本生成SQL效果良好，但对私有企业数据仓库的表现却大打折扣。构建针对企业领域的高质量text-to-SQL基准数据集面临昂贵且繁琐的人工标注难题。

Method: BenchPress系统利用人机协同，通过结合检索增强生成（RAG）和LLM自动生成多种SQL的自然语言描述，再由人类专家筛选、编辑这些文本，快速生成高质量、领域特定的text-to-SQL基准数据集。

Result: BenchPress显著减少了人工标注时间和成本，提高了标注质量和基准数据集的可靠性，有效提升了模型评测的稳健性。

Conclusion: BenchPress系统为企业定制的text-to-SQL基准数据集构建提供了高效解决方案，促进了领域特定模型的评估和发展，其工具和资源已经公开。

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [44] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 该文提出R2T框架，将语言规则与神经网络深度集成，在无标注数据或极 小标注文本条件下，Zarma词性标注达98.2%准确率，命名实体识别用极少标注数据也超越传统基线。R2T显著提升低资源场景的模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络序列标注任务高度依赖人工标注数据，特别是在资源稀缺语言场景下，模型对未登录词的泛化能力较弱。为缓解这一问题，作者提出结合显式语言规则以增强模型表现。

Method: 提出了Rule-to-Tag（R2T）框架，将多层次语言规则体系直接集成到神经网络训练目标中，通过自适应损失函数设计，在训练过程中引入约束项来提升模型对未登录词的不确定性处理能力，实现更具原则性的学习方案（PrL），主要在无标注文本上进行训练。

Result: R2T-BiLSTM模型在Zarma词性标注任务上仅用无标注数据，实现98.2%准确率，显著超越用300条标注句子微调的AfriBERTa基线。在命名实体识别任务中，R2T作为预训练步骤，模型只用50条标注句子微调，效果也超过用300条标注数据直接训练的基线。

Conclusion: 将基于规则的表示与神经网络训练深度融合，不仅显著提升了模型在低资源及OOV词处理方面的性能，还节约了标注成本。R2T框架及PrL范式有望为低资源语言乃至广泛NLP任务提供通用的高效学习方法。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [45] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: 本文针对大型语言模型集成中的鲁棒性问题，提出了即插即用的一致性加权方法CoRE，可以在令牌和模型两个层面提升集成效果，经大量实验证明能显著增强集成方案的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在能力上各有优劣，因此采用模型集成可以结合不同模型的优势。然而，现有的LLM集成在提升质量方面已有进展，但对集成的鲁棒性关注较少，尤其是针对模型间由于异构分词方式和专业领域差异引发的错误信号。

Method: 作者分析了LLM集成失败的两种主要原因：令牌级严重预测分歧和模型级信心低以及模型间差异大。基于此，提出了CoRE方法：1）令牌一致性——以低通滤波削弱分歧较大的不确定令牌权重，从细粒度提升鲁棒性；2）模型一致性——提升高自信且与其他模型分歧最小的模型输出，从粗粒度增强鲁棒性。CoRE设计为即插即用，适配多种集成方法。

Result: 通过大量实验，在多种基准测试、模型组合和集成策略下，CoRE方法能够持续提升集成的性能和鲁棒性。

Conclusion: CoRE方法有效改善了LLM集成中的表现和鲁棒性，可广泛应用于不同模型和集成场景。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [46] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 通过检索增强与指令调优的大模型，MasonNLP系统在创伤护理视觉问答任务中高效提升了模型输出的临床相关性，无需额外训练即可获得优异成绩，是一种简洁且有效的多模态医学NLP方案。


<details>
  <summary>Details</summary>
Motivation: 医学视觉问答（MedVQA）能够通过自然语言查询医学图像，为临床决策与患者护理提供辅助。针对创伤护理中的问答需求，MEDIQA-WV 2025提出了需要系统生成自由文本回答及结构化创面属性的新挑战。

Method: 提出了MasonNLP系统，采用通用领域、指令微调的大型语言模型（LLM），结合检索增强式生成（RAG）框架，将领域内文本和视觉示例融入模型推理，提升输出的临床相关性。该方案通过简单的索引和融合机制，在推理时轻量引入相关示例，无需额外训练或复杂重排序。

Result: MasonNLP系统在51份提交、19支队伍中综合排名第3，平均得分41.37%。在dBLEU、ROUGE、BERTScore和基于LLM的评测指标上表现出色。

Conclusion: 轻量的RAG和通用LLM结合，仅需简单推理层与相关示例，就可为多模态临床NLP任务提供简单有效的基线方案，显著提升生成回答的质量。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [47] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 本文提出ShishuLM高效语言模型架构，通过MLP近似Transformer块，实现显著的内存和延迟优化，在不损失性能的情况下，更适合小型语言模型的实际应用。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在自然语言处理任务中取得了极佳的性能，但其高昂的内存与计算开销促使研究者寻找优化方法，尤其是在小型语言模型（SLM）越来越重要的背景下。本文旨在减少架构冗余和资源消耗。

Method: 提出了一种新型高效语言模型架构ShishuLM，结合AI可解释性与推理阶段层剪枝的研究成果，通过归一化与注意力机制的线性近似，用多层感知机（MLP）近似Transformer模块，有效减少参数和Key-Value缓存需求。

Result: ShishuLM在两个不同规模的SLM实验中表现出色，内存需求降低高达25%，训练与推理延迟提升高达40%，均优于原始模型。

Conclusion: ShishuLM证明了通过架构优化能显著提升SLM的效率，为从预训练阶段构建更高效的小型语言模型提供了理论和实践支持。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [48] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 本文通过集成多款LLM分析AI辅导过程中的大规模学生情感对话，揭示学生情绪状态变化及转折点，为AI教育工具的情感感知与及时干预提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前对于大语言模型（LLM）在教育场景中的学习影响已有研究，但LLM作为辅导工具时对学生情感动态的影响仍然缺乏深入理解。随着生成式AI在教育行业融合的推进，理解并关注学生在与AI互动过程中的情感变化具有重要意义。

Method: 作者提出了一种首创的集成式LLM框架，用于对大规模辅导对话中学生的情感进行感知。研究分析了三个美国高校、两学期共261名本科生与AI导师PyTutor之间共16,986轮对话。情感注释由三个前沿LLM（Gemini, GPT-4o, Claude）以零样本方式生成，包括情感效价、唤醒度与学习有益性分数及自由文本的情感标签，随后通过模型内融合与多模型共识投票机制获得稳健的情感剖面。

Result: 数据显示，学生与AI导师互动过程中整体表现出轻度正向情感和中等唤醒度。问题解决期间常见困惑与好奇，挫败感较少但仍可能影响进展。情绪状态持续时间短暂，正面情绪稍长但易受到干扰。值得注意的是，负向情感常能迅速缓解，有时直接跃迁为正面情绪，而中性情绪多作为转折点，有助于情感向上转变。这提示智能导师可在这些关键时刻进行有效干预。

Conclusion: LLM辅导下的情感动态复杂且波动频繁，学生整体情绪较为积极但容易受到外界影响。智能导师识别和有效干预情感转折点有助于促进积极学习体验，为生成式AI在教育中负责任的融合路径提供数据基础。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [49] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 本文提出了让扩散语言模型先生成结构模板再填充内容的新方法，并通过动态调整片段长度提升灵活性，在数学推理和代码生成上达成显著性能提升，同时加快了生成速度。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）虽然是自回归语言模型的有前景替代品，但其推理方式基本沿袭了自回归范式，仅限于前缀提示，方法创新有限。

Method: 提出了“模板填充（Template Infilling, TI）”方法，DLMs 生成时先构造结构模板再填充掩码段，并设计了“动态片段分配（Dynamic Segment Allocation, DSA）”，能依据生成信心自适应调整片段长度。

Result: 在数学推理和代码生成基准测试中，方法相比基线在各项指标上有 17.01% 的提升。同时，模板填充在多 token 生成任务中兼顾了速度与质量。

Conclusion: 模板填充及动态片段分配方法，使DLMs在结构控制、生成速度及质量间实现了优势互补，拓展了DLMs的应用前景。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [50] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: 本研究详述如何将印第安语系语音数据引入开源平台Common Voice，显著丰富了稀缺资源语言的数据量，并强调该项目在技术和社区赋能中的潜力。


<details>
  <summary>Details</summary>
Motivation: 印第安语系等稀缺资源语言在语音技术发展中由于数据和资源短缺而受到阻碍。本文旨在通过开源数据集建设，克服此类困难。

Method: 以Common Voice项目为平台，分析印第安语系（Quechua）的17种语言数据的引入与建设过程，并以Puno Quechua为案例，介绍语料采集（朗读与自发表达）的方法。

Result: Common Voice现已收录印第安语系总计191.1小时语音数据，其中86%获得验证；Puno Quechua贡献12小时语音（77%验证）。

Conclusion: Common Voice有效助力稀缺资源语言的语音数据建设，有助于赋能少数民族语言社区，提升其在数字与语音技术领域的参与度，并提出未来研究方向及伦理与社区参与建议。

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [51] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: 研究构建了一个高质量的法语肿瘤学临床文本标注数据集，覆盖丰富的领域实体及标准化表达，填补了法语医学NLP资源空白。


<details>
  <summary>Details</summary>
Motivation: 法语肿瘤学临床文本的处理工具开发受限于缺乏标注数据集，因此需要构建面向该领域和语言的高质量语料库。

Method: 基于翻译自西班牙语CANTEMIST语料库的案例，结合FRASIMED项目，制作了1301个合成法语临床案例，并结合ICD-O标准进行形态学、部位、组织学分化实体标注。专家团队手动和自动相结合完成了标注及多层次正常化。

Result: 共标注了71127条ICD-O标准化信息，涵盖399个形态学代码、272个部位代码和2043个复合表达。数据集经过专家团队手工和自动校验。

Conclusion: 该数据集为法语肿瘤学命名实体识别及概念标准化任务提供了权威参考资源。

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [52] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: GateSkip是一种残差流门控机制，可对解码器LM进行Token级层跳过，节省推理计算最多达15%，准确率损失低于10%；在部分设置下甚至还能提升准确率，实现了高效与稳定兼备的新方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高解码器专用语言模型（LM）的推理效率，同时减少计算量，而不牺牲模型性能，作者提出新机制。之前相关的层跳过模式容易不稳定，需要大量再训练。

Method: 提出GateSkip机制，通过残差流的门控机制，实现Token层级的跳过。每层的注意力/MLP分支都有一个sigmoid-linear门，门值决定输出的重要性。在推理时，根据门值给Token排序，利用每层预算跳过低重要性的Token，且新门机制可在预训练模型上稳定微调。

Result: 在长文本推理任务中，最多节省15%的计算，且保持90%以上的基线准确率。在指令微调模型上，全算力下提高准确率，且计算缩减到50%时也能保持接近基线的质量。门学习结果还揭示了Transformer的信息流动，如BOS Token作为锚点。此方法可与量化、剪枝、自推理解码结合。

Conclusion: GateSkip提供了一种高效、稳定的层跳过机制，可在不大幅损失性能的情况下显著减少模型推理计算，并且容易集成到其他模型优化技术中。

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [53] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: 本文提出用仅依赖文本反馈（无数值/概率信息）评估LLM决策能力的新基准，对比发现部分LLM（如Qwen3-4B）能超越传统算法表现出强决策能力，显示概率性推理可由自然语言本身诱发。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在推理任务上表现优异，但其在纯自然语言（无数值或概率信息）条件下进行不确定性下的顺序决策能力尚未被深入研究。

Method: 设计了一项新基准，在该环境中，LLM仅通过文本反馈（如“你获得了一个代币”）与多臂老虎机环境交互，无任何数值或概率提示，要求模型完全依靠语言线索推断潜在奖励结构并做出自适应决策。对四个开源LLM进行了评估，并与Thompson Sampling、Epsilon Greedy、UCB和随机选择等标准决策算法作比较。

Result: 大部分LLM表现低于传统基线方法，但Qwen3-4B在最佳臂选择率达89.2%，显著优于其他更大型的LLM和传统算法。

Conclusion: 结果表明，仅凭语言，LLM可表现出概率性推理能力。该基准为评估LLM在自然、非数值情境中的决策能力提供了一种新途径。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [54] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 作者提出了Catch Your Breath损失机制，让语言模型根据token复杂度动态决定多少计算步骤，通过请求暂停提升准确性，并减少所需训练数据。三种CYB方法相比基线显著提升效率和自适应能力，模型能自动为复杂/歧义token分配更多计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型的计算步骤是固定的，未能针对不同输入的复杂性动态分配计算资源。作者希望探索模型能够自主决定每个输入token所需计算步骤的方法，从而提升模型在复杂、不确定或歧义输入上的表现和效率。

Method: 作者提出一类被称为Catch Your Breath (CYB)的损失函数，用以训练语言模型根据输入token的需求动态决定计算步骤。模型可通过<don't know>输出请求更多计算，或插入<pause>延长处理时间。考察了三种CYB方法：CYB-AP关注任意时刻输出的准确性和时延折扣，CYB-VA用变分法在指定停顿分布下最大化预测准确性，CYB-DP则限制计算预算并对超出预算的请求施加惩罚。通过微调实验对比不同CYB方法效果。

Result: CYB模型在达到相同性能时，所需训练数据量仅为无暂停基线模型的三分之一，也低于带pause和交叉熵损失模型（减少一半数据）。实验发现，CYB模型会在需要提升准确时主动请求暂停，且停顿行为与token复杂性和语境相关（如在复数名词后经常请求暂停，在缩略词首token后不暂停，对歧义token有较高变异性）。

Conclusion: Catch Your Breath损失能有效引导语言模型自主分配计算资源，在保持甚至提升准确度的同时，大幅降低训练数据需求，并使模型的计算时间更具有针对性和自适应性。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [55] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: PAGE框架利用轻量级辅助模型丰富生成模型输入，提升生成的质量和可控性，无需额外的生成模型，已在软件需求生成任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言生成模型在特定任务或要求下表现不佳，且调整通常需大量数据，亟需一种简单高效的改善方法。

Method: 利用轻量级辅助模块（如分类器、抽取器）处理输入文本，将其输出结果与原输入结合，增强生成模型的输入。

Result: 在需求工程领域，结合分类器辅助模块，PAGE框架提高了软件需求生成的质量，并展示了通用性和易适配性。

Conclusion: PAGE框架通过辅助模块优化文本生成模型，无需额外大型生成模型，证明了其能提升需求工程领域的生成质量。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [56] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文主张在利用大型语言模型进行社会模拟时应采用开放式文本设计，而非封闭式选择题，以更真实、多样地反映社会意见和现象，减少偏见，并提出应开发新方法和评估体系，以充分发挥LLMs的生成潜力。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型（LLMs）模拟社会现象时，多采用封闭式（如选择题、简答题）设计，忽略了LLMs本身的生成能力，限制了结果的真实性和丰富性。本文希望解决这一局限。

Method: 通过回顾调查方法学研究和自然语言处理的最新进展，论证开放式（自由文本）设计在LLMs社会模拟中的价值，并探讨其带来的实际益处。

Result: 开放式文本能够更好地捕捉受试者观点、表达和推理过程，有助于探测未预期的观点、减少研究者引导偏差，并提升方法的实用性和个体表达能力。

Conclusion: 呼吁社会科学与NLP领域合作，采用新的实践和评价框架，充分利用LLMs的开放式生成能力，实现更真实、有用的社会模拟。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [57] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 本文系统比较了十种LLM在IAB 2.2分类任务下的表现，发现单一模型表现一般且易出现虚假和过多分类；集成多模型协作可显著提升分类质量，暗示未来应重视模型协同而非单纯扩大模型规模。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLMs）在文本处理领域取得进展，但对于利用IAB 2.2分层分类法进行无结构文本分类，其实际性能尚不明朗。本文通过系统比较多种LLMs在此任务下的表现，旨在揭示模型在复杂分类任务中的局限性并寻求改进方案。

Method: 作者选取了十种先进LLMs，利用相同的8,660个人工标注样本以及一致的零样本提示，在文本分类任务中进行评测。评估指标包括传统的准确率、精度、召回率、F1分数，以及针对LLM的错觉率、膨胀率和分类成本，保证结果的公平性和可比性。同时设计并测试了多模型集成（ensemble）策略。

Result: 实验结果显示，当前LLMs在经典评价指标上的表现较为一般（平均准确率34%、精度42%、召回率45%、F1分数41%），且存在过度生成分类（膨胀）和虚假分类（错觉）的现象。Gemini 1.5/2.0 Flash和GPT 20B/120B在性能与成本之间取得了较好平衡，GPT 120B的错觉率最低。集成方法显著提升了准确率，降低了膨胀率，并完全消除了错觉现象。

Conclusion: 单靠LLM的规模和架构优化难以显著提升文本分类准确性，原因在于模型难以将丰富的无结构文本有效压缩到有限的分类体系。多模型集成作为专家协作，有望超过当前单一模型的能力，成为实现或超越人类专家级表现的有效途径。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [58] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: 本文针对大模型自动生成数学证明领域，提出并验证了细粒度数学证明评估器开发流程，推出高质量数据集ProofBench及评估器ProofGrader，有效提升了自动化证明评审的可靠性和细致度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学推理上虽有进展，但对自然语言数学证明的生成与评估仍存在较大挑战，特别是缺乏可靠且细致的自动评分工具。填补这一评估空白对推动下游数学证明生成具有重要意义。

Method: 搭建首个专家标注的细粒度数学证明评分数据集ProofBench，涵盖多项数学竞赛题目和多模型生成的解决方案；系统研究评估器的设计，包括基础模型、输入上下文、评估流程等；提出结合强推理模型、丰富的参考资料以及集成方法的ProofGrader。

Result: ProofGrader在与专家评分的均方误差仅0.926，明显优于现有基线方法。在最佳n选任务上，平均得分为4.14（满分7分），显著接近于人类专家水平，有效提升证明生成的质量选择。

Conclusion: 本文提出了一种系统化的方法，用于开发和验证针对大模型生成数学证明的细粒度评估器，并通过构建ProofGrader实现了低误差高性能的自动评估。

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [59] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 本文综述了小语言模型与大语言模型的协作，通过构建以性能、成本、隐私、可信为目标的分类框架，系统回顾了主要方法和设计思路，指出了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）推动了众多领域应用的发展，但却存在微调成本高、推理延迟大、难以在边缘部署以及可靠性等问题。相比之下，小语言模型（SLM）体积小、效率高、易适应，能针对这些问题提供解决思路。近期已有研究尝试通过协作架构融合SLM的专长与效率，以及LLM的泛化和推理能力，以应对不同任务和部署场景的需求。

Method: 论文系统性地梳理了SLM与LLM协作的相关工作，并提出了以协作目标为主线的分类方法。构建了四个协作目标的分类体系：性能提升、成本效益、云-边隐私和可信赖性，并在此框架下回顾了代表性方法，总结了设计范式。

Result: 本文提出了SLM-LLM协作的分类框架，总结了现有设计方法，梳理了当前遇到的挑战和未来发展方向，为实现高效、安全、可扩展的语言模型协作指明了方向。

Conclusion: SLM-LLM协作能够有效结合各自优势，并针对不同应用需求提供高性能、低成本、隐私保护和可信赖性的解决方案。该领域仍有诸多挑战，未来发展值得关注。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [60] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: THTB框架以认知科学为基础，通过难度评分实现高效、可解释的数据筛选和标注指导。在大幅减少数据量的前提下显著提升LLM性能和泛化能力，尤其适合专业领域模型适配。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在特定领域的适配依赖高质量SFT数据，但现有数据筛选方法过度依赖模型内部知识、可解释性弱且泛化能力有限。

Method: 提出了THTB框架，受认知科学启发，通过结合质量过滤和内外双重难度评分，优先选择高阶认知指令，为数据筛选和标注指导提供可解释、量化标准。

Result: 实验证明仅使用5%数据即可超越全量训练，泛化性能优于仅靠LLM筛选的方法。在垂直领域仅用2%数据训练的模型也表现优异，优于用大量数据训练的模型。

Conclusion: THTB在数据筛选和标注方面高效，可为特定领域模型适配显著节省成本并提升性能，展现强大应用前景。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [61] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 该研究通过结构化红队实验，构建了大型语言模型越狱策略的细致分类体系，并验证了基于分类提示的越狱检测方法，同时首次发布意大利语多回合越狱语料，深化了越狱技术的理解和检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）安全面临挑战，现有防护措施多局限于单轮攻击、语言覆盖不足、攻击策略分类有限。缺乏针对多样化越狱手法的系统性识别与分析。

Method: 通过组织结构化的红队挑战实验，发展多维乌越狱策略分类体系，实证分析各种攻击类型的表现，并基于分类体系优化自动检测方法和构建多回合意大利语对话数据集。

Result: 提出了包含50种越狱策略的分层分类体系，涵盖7大类别；分析了各类攻击的流行度与成功率，揭示模型漏洞利用方式；实证表明基于分类体系的提示能提升自动检测效果；发布了多回合意大利语越狱语料库，支持渐进式攻防分析。

Conclusion: 多角度系统性提升了对LLM越狱技术的理解，推动越狱检测与防御的精细化发展，并为多语言、多回合对抗研究提供了新资源。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [62] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 本工作通过评测风格嵌入与LLM判别两种方法，分析AI生成文本作者归属难题，发现二者在不同文本类型互有优势，需混合策略，提供开源基准和数据以促进研究。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）生成的文本越来越接近人类写作，区分机器生成与人类写作的作者归属变得日益困难。该工作旨在为AI内容归属问题建立客观可重复的基准方法。

Method: 作者利用Human AI Parallel Corpus（涵盖六大领域、600组平衡实例）评测了两类归属机制：固定风格嵌入（Style Embeddings）和指令微调的大型语言模型判别器（GPT-4o）。比较了在不同内容类型和不同AI模型（GPT-4o、LLaMA-70B-Instruct）下的表现。

Result: 在GPT生成内容归属中，风格嵌入基线更胜一筹（82% vs 68%）。对LLaMA生成内容，LLM Judge略优于风格嵌入（85% vs 81%），但差异无统计学意义。此外，判别器在小说与学术文本中显著更优，嵌入方法在口语和剧本领域表现突出。

Conclusion: 作者归属是多维度问题，不同方法在不同领域具互补性，需采用混合策略。作者开源数据集和代码，为AI内容归属质量建立了可复现基准与流程。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [63] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 文章发现，狭域微调不仅改变了LLM在特定任务上的表现，还会在模型激活层留下可被解读的明显偏差，这些特征可能导致过拟合风险，并对解释性与安全研究提出新挑战。传统采用狭域微调结果去类比通用任务的做法并不可靠，建议今后需更严谨地设计微调与可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 近年来，为了让大型语言模型（LLM）能够更好地适应特定任务，狭域微调（narrow finetuning）成为一种常用手段。但这种微调是否会导致模型出现独特的行为和偏差，对安全与可解释性研究具有重要意义。

Method: 作者通过模型差异分析（model diffing），对LLM在微调前后激活的差异进行分析，尤其是关注在随机文本前几个token上的激活变化，并通过将这些差异加入模型激活来“引导”生成文本。此外，作者利用解释代理（interpretability agent）对微调领域进行理解和解释。实验覆盖了不同架构和规模的模型，并涉及多种微调任务。

Result: 微调会在模型激活中留下显著的偏差，这些偏差可以被解释代理分析和利用，从而更好地识别微调领域。与只用简单提示（prompting）的基线方法相比，利用激活偏差的信息可显著提升解释能力。此外，混合预训练数据进入微调语料可明显降低这些偏差，但仍可能存在残余风险。

Conclusion: （1）狭域微调的模型激活保留了训练目标的显著印记，可以为训练改进提供建议；（2）警示AI安全/可解释性领域，常用的通过狭域微调模型代理更广泛微调的做法可能并不现实；（3）需要更深入地研究微调影响，开发更真实的模型差异分析、安全和解释性案例研究。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [64] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: RAID框架利用拒绝感知和嵌入空间优化来生成自然有效的攻击后缀，在突破大语言模型安全防护上表现优异，凸显嵌入空间正则化在提升模型安全性中的作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多种任务上表现优异，但依然容易受到绕过安全机制的攻击（称为jailbreak）。现有方法在攻击效率和自然性上仍有提升空间。

Method: 提出RAID（Refusal-Aware and Integrated Decoding）框架，通过将离散词元放松为连续嵌入，并优化联合目标，包括诱导受限回复、嵌入空间拒绝感知正则化、语义连贯性约束。优化后，通过评论者指导的解码过程将嵌入映射回词元，实现自然且有效的攻击后缀生成。

Result: RAID在多种开源大语言模型上实验，显示其减少查询次数与计算成本的同时，攻击成功率超过了近期的白盒和黑盒基线方法。

Conclusion: 嵌入空间的正则化对于理解和防御大语言模型的jailbreak攻击有重要意义。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [65] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 本文以道德基础理论为框架，系统分析了大语言模型在政治与道德领域的偏见及倾向性，发现其回应带有一定政治立场，并能在一定条件下反映不同意识形态，对AI公平与可靠性提出了新的洞察。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在医学、个人关系和法律等领域中担任咨询角色，因其在政治和伦理领域的重要性，激发了对其潜在偏见的关注。本文的动机是评价LLMs在道德与政治领域中的立场及其与人类偏好之间的关联性。

Method: 本文采用“道德基础理论”（Moral Foundations Theory, MFT），从五个维度分析LLMs的道德回应，与此前的人类相关研究数据进行直接比对，同时通过显式指令和角色扮演等方式检验LLMs对不同政治意识形态的表现能力。

Result: 研究发现，LLMs在某些情境下生成的道德和政治回应有偏向某一意识形态的可能，且能在一定程度上通过指令和角色扮演反映特定的政治立场。LLMs的表现在政治和人口统计学变量上存在依赖性。

Conclusion: LLMs在道德和政治领域的回应体现了一定的意识形态倾向，且其倾向性与设定条件以及所参考的人类数据相关。系统性分析表明：LLMs能反映并区分各种政治与人口属性，但偏见与依赖性仍需关注。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [66] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: 本文提出了一种受认知科学启发的新型ICL框架SA-ICL，用schema结构显式增强LLM推理能力，在化学和物理任务上效果显著提升，并提高了解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的例子驱动型in-context learning（ICL）方法缺少知识检索和抽象层面的迁移模块，这限制了语言模型适应新任务和类人推理的能力。作者受认知科学中特别是图式理论启发，探索更接近人类认知结构的学习方式。

Method: 提出了SCHEMA ACTIVATED IN CONTEXT LEARNING（SA-ICL）框架，通过从先前示例中抽取关键认知步骤及其关系，形成抽象化的schema（结构化推理模板），并用于增强模型在解决新问题时的推理能力。实验涵盖不同大型语言模型，验证显式schema支持对模型推理的影响。

Result: 实验表明，许多主流大型语言模型自身难以内隐地形成和利用schema结构，但通过SA-ICL框架显式构建schema后，表现有显著提升——在GPQA数据集化学和物理问题上性能提升高达36.19%，且对演示样例数量要求降低，推理可解释性也增强。

Conclusion: SA-ICL不仅提升LLM的推理能力和类人学习能力，还在ICL方法中实现从模式触发到思维链式提示的桥接，为后续提升LLM的可解释性和灵活性指明了新方向。

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [67] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

TL;DR: 本文提出了无需标签的LLM提示优化方法PDO，通过判官偏好加智能采样与变异，显著提升提示优化效果。


<details>
  <summary>Details</summary>
Motivation: LLM对输入提示高度敏感，提示设计成为核心挑战，人工标注代价高昂，自动优化方式亟需突破对标签依赖。

Method: 提出Prompt Duel Optimizer（PDO）框架，通过双重Thompson采样（D-TS）选择有信息量的提示对，并结合高性能提示引导的变异（Top-Performer Guided Mutation）扩展候选提示池。使用LLM判官提供的成对偏好反馈，不依赖于人工标签。

Result: 在BIG-bench Hard和MS MARCO等任务上，PDO的性能持续优于现有无标签优化基线。消融实验验证了双重Thompson采样和提示变异机制的有效性。

Conclusion: PDO能在无标签或部分标签的情况下有效优化大语言模型（LLM）的提示设计，显著优于基准方法。

Abstract: Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


### [68] [Interpreting the Latent Structure of Operator Precedence in Language Models](https://arxiv.org/abs/2510.13908)
*Dharunish Yugeswardeenoo,Harshil Nukala,Cole Blondin,Sean O Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本研究分析LLaMA 3.2-3B对算术表达式内部表征，发现模型能在残差流和运算符嵌入中编码运算符优先级，并提出了调控该优先级的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs具备一定推理能力，但在算术任务上表现欠佳。先前研究关注于输出或提示策略，对于模型内部如何进行算术运算知之甚少。本研究旨在揭示LLM在内部表征中是否编码了运算符优先级，以及如何被实现。

Method: 构建了含有三操作数和两运算符、且括号不同排列的算术表达式数据集；利用logit lens、线性探针、UMAP等可解释性工具，追踪模型残差流中的中间计算结果及编码方式；提出partial embedding swap方法，调换运算符嵌入的高影响维度。

Result: 实验结果表明，在MLP模块后，模型的残差流中能检测到中间计算结果；在注意力层后，模型以线性方式编码了各运算符的优先级信息。另一个主要发现是，通过部分嵌入交换，可以调整运算符的优先级表现。

Conclusion: 本文发现大型语言模型（LLMs），如LLaMA 3.2-3B，确实在其内部表示中编码了算术运算的运算符优先级，尤其是在自注意力层后和MLP模块之后的残差流中。通过特定的嵌入维度，高影响力的信息可以被交换，进而修改运算符的优先级。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities but continue to struggle with arithmetic tasks. Prior works
largely focus on outputs or prompting strategies, leaving the open question of
the internal structure through which models do arithmetic computation. In this
work, we investigate whether LLMs encode operator precedence in their internal
representations via the open-source instruction-tuned LLaMA 3.2-3B model. We
constructed a dataset of arithmetic expressions with three operands and two
operators, varying the order and placement of parentheses. Using this dataset,
we trace whether intermediate results appear in the residual stream of the
instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such
as logit lens, linear classification probes, and UMAP geometric visualization.
Our results show that intermediate computations are present in the residual
stream, particularly after MLP blocks. We also find that the model linearly
encodes precedence in each operator's embeddings post attention layer. We
introduce partial embedding swap, a technique that modifies operator precedence
by exchanging high-impact embedding dimensions between operators.

</details>


### [69] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo,Jiapu Wang,Gongqing Wu,Zhongyuan Wang,Jichen Zhang,Shirui Pan,Xindong Wu*

Main category: cs.CL

TL;DR: 本文提出KRLM模型，实现了大模型知识与知识图谱的有效整合，显著提升了未知实体/关系归纳推理能力，并提高了推理可信度。


<details>
  <summary>Details</summary>
Motivation: 在开放域知识图谱中进行归纳推理，需要模型理解包含未知实体和关系的不确定知识图谱成分，现有KGFM只能部分解决此问题，但LLM的内在知识又容易被稀疏KG信息干扰，且存在生成幻觉限制推理的可信度。

Method: 提出了一种知识推理语言模型（KRLM），通过设计知识推理语言（KRL）指令格式和KRL分词器将LLM与KG环境对齐，并利用KRL注意力层和动态知识记忆机制，最后引入结构感知的下一实体预测器严格约束推理在可信领域内。

Result: 在25个真实世界的归纳知识图谱推理数据集上，无论在零样本推理还是微调场景下，KRLM的实验效果都显著优于现有方法。

Conclusion: KRLM有效协调了LLM与KG的知识，实现了更可信、能力更强的归纳知识图谱推理。

Abstract: Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.

</details>


### [70] [RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems](https://arxiv.org/abs/2510.13910)
*Jingru Lin,Chen Zhang,Stephen Y. Liu,Haizhou Li*

Main category: cs.CL

TL;DR: 为解决agentic RAG系统在多跳复杂问题推理中的瓶颈，作者提出了细粒度评测基准RAGCap-Bench，并通过实验验证了提升中间推理能力能显著改善整体性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于RAG（检索增强生成）的系统，虽然提升了LLM的信息获取与推理能力，但在处理多跳复杂问题时依然存在不足，且系统在中间推理环节的具体能力尚未被充分研究。

Method: 提出了RAGCap-Bench，这是一个针对agentic RAG系统的能力评测基准，通过分析现有系统的输出，总结常见任务与执行所需核心能力，并构建LLM常见错误的分类法，以设计有针对性的评估问题。

Result: 实验表明，具备“慢思考”能力、在RAGCap-Bench表现更强的模型，在端到端任务上也能取得更好的效果，验证了该基准的有效性，并表明提升中间推理能力的重要性。

Conclusion: RAGCap-Bench能够细致评估agentic RAG系统在中间推理任务中的能力，推动相关模型在多跳复杂问题上的性能提升，强调了系统中间能力优化的价值。

Abstract: Retrieval-Augmented Generation (RAG) mitigates key limitations of Large
Language Models (LLMs)-such as factual errors, outdated knowledge, and
hallucinations-by dynamically retrieving external information. Recent work
extends this paradigm through agentic RAG systems, where LLMs act as agents to
iteratively plan, retrieve, and reason over complex queries. However, these
systems still struggle with challenging multi-hop questions, and their
intermediate reasoning capabilities remain underexplored. To address this, we
propose RAGCap-Bench, a capability-oriented benchmark for fine-grained
evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs
from state-of-the-art systems to identify common tasks and the core
capabilities required for their execution, then construct a taxonomy of typical
LLM errors to design targeted evaluation questions. Experiments show that
"slow-thinking" models with stronger RAGCap performance achieve better
end-to-end results, underscoring the benchmark's validity and the importance of
enhancing these intermediate capabilities.

</details>


### [71] [AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs](https://arxiv.org/abs/2510.13912)
*María Victoria Carro,Denise Alejandra Mester,Facundo Nieto,Oscar Agustín Stanchi,Guido Ernesto Bergman,Mario Alejandro Leiva,Eitan Sprejer,Luca Nicolás Forziati Gangi,Francisca Gauna Selasco,Juan Gustavo Corvalán,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.CL

TL;DR: 本文探讨AI辩论在主观题中的信念表现，发现模型易受评审引导，顺序辩论有结构性偏差。从实验结果可改进AI对齐训练，也揭示人机说服的新动态。


<details>
  <summary>Details</summary>
Motivation: AI辩论作为可扩展监督技术的核心假设是“说谎难于驳谎”，即评审可以更容易辨别正确立场，而已有实验只在有客观真值的数据集上进行，未充分探讨主观维度下模型的真实信念与说谎行为。

Method: 将辩论应用于主观问题。在实验前显式测量大语言模型的先验信念，随后让‘辩手’选择自己偏好的立场，再以特定judge persona施加与其先验冲突的评审，考查模型是否迎合评审观点或坚持自身信念。实验包含顺序与同时两种辩论协议，以及对说服力与论据质量的量化评估。

Result: 1.模型更倾向于辩护与评审一致的立场，而非自身先验信念；2.顺序辩论明显偏向第二辩手；3.当立场与模型信念一致时更具说服力；4.但与模型信念冲突的论据，在双人比较时被评为更高质量。

Conclusion: AI模型面临信念与迎合评审观点的权衡，辩论制度与协议设计会显著影响结果。顺序辩论存系统性偏差，应优化训练信号设计。人类评审应关注模型信念表现，以提升AI系统对齐水平，也为理解人机说服交互提供新视角。

Abstract: The core premise of AI debate as a scalable oversight technique is that it is
harder to lie convincingly than to refute a lie, enabling the judge to identify
the correct position. Yet, existing debate experiments have relied on datasets
with ground truth, where lying is reduced to defending an incorrect
proposition. This overlooks a subjective dimension: lying also requires the
belief that the claim defended is false. In this work, we apply debate to
subjective questions and explicitly measure large language models' prior
beliefs before experiments. Debaters were asked to select their preferred
position, then presented with a judge persona deliberately designed to conflict
with their identified priors. This setup tested whether models would adopt
sycophantic strategies, aligning with the judge's presumed perspective to
maximize persuasiveness, or remain faithful to their prior beliefs. We
implemented and compared two debate protocols, sequential and simultaneous, to
evaluate potential systematic biases. Finally, we assessed whether models were
more persuasive and produced higher-quality arguments when defending positions
consistent with their prior beliefs versus when arguing against them. Our main
findings show that models tend to prefer defending stances aligned with the
judge persona rather than their prior beliefs, sequential debate introduces
significant bias favoring the second debater, models are more persuasive when
defending positions aligned with their prior beliefs, and paradoxically,
arguments misaligned with prior beliefs are rated as higher quality in pairwise
comparison. These results can inform human judges to provide higher-quality
training signals and contribute to more aligned AI systems, while revealing
important aspects of human-AI interaction regarding persuasion dynamics in
language models.

</details>


### [72] [Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms](https://arxiv.org/abs/2510.13913)
*Shrey Pandit,Xuan-Phi Nguyen,Yifei Ming,Austin Xu,Jiayu Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本论文提出一种逐步加大难度的问答数据自动生成方法，并通过多角色基线代理人评估与过滤。实验表明，尽管数据集规模较小，但多样性和有效性均超过现有方案，可训练出表现更佳的网页智能体。


<details>
  <summary>Details</summary>
Motivation: 当前基于网页的深度研究智能体在复杂问答任务中表现不佳，因为底层语言模型难以应对需要长远推理和探索的任务。以往利用知识图谱构建指令微调数据集的方法，难以严格控制任务难度与数据质量，生成的数据难以适应实际需求。

Method: 提出一种两阶段数据合成流程，通过逐步增加任务复杂性直到基线智能体无法解答，生成高质量问答对。基线智能体在过程中担任多重角色，包括尝试解答、验证事实、检查备选答案与筛选过滤。并采用受控蒸馏训练方法，便于客观比较不同数据集的效果。

Result: 实验证明生成的数据集虽然规模较小，但能训练出性能优于既有数据集的智能体。特别是在工具使用动作的多样性上提升了两倍，显著提升答题能力并减少了重复调用工具的现象。

Conclusion: 该研究提出的合成数据集拥有更高复杂度与多样性，有效增强了长远推理任务中智能体的表现，相较传统方法更具结构化与针对性。

Abstract: Web-based 'deep research' agents aim to solve complex question - answering
tasks through long-horizon interactions with online tools. These tasks remain
challenging, as the underlying language models are often not optimized for
long-horizon reasoning and exploration. Prior work has proposed workflows for
constructing instruction-tuning datasets, often leveraging knowledge graphs.
However, such methods typically lack fine-grained control over difficulty and
quality, yielding synthetic data that falls short of capturing the complexity
required for long-horizon reasoning. Furthermore, many studies conflate data
and training effects by comparing models trained under different optimization
recipes, making it difficult to isolate and evaluate the effectiveness of the
data itself. We introduce a two-pronged data synthesis pipeline that generates
question - answer pairs by progressively increasing task complexity until a
frontier baseline web agent fails. The baseline agent plays multiple roles in
this process: attempting the questions, validating factuality, checking for
alternative answers, and enforcing filtering. To evaluate the effectiveness of
our synthesis methods, we adopt a controlled training setup based on
distillation from strong web agents. Experiments across multiple web-based
benchmarks show that our dataset - despite being smaller - enables the training
of more effective web agents than existing datasets. In particular, our data
exhibits twice the diversity in tool-use actions, allowing models trained on it
to achieve stronger performance while avoiding repetitive tool-calling
behaviors.

</details>


### [73] [Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models](https://arxiv.org/abs/2510.13915)
*Ivan Lee,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 可读性并非提升小语言模型能力的关键因素，统计简单性更重要；应避免将小模型训练过程与人类学习过程类比。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为，将非常小的语言模型（SLMs）训练在简化的、类似儿童语言的数据集（如TinyStories）上，其文本生成能力的提升源于内容的可读性。本文质疑了这种解读。

Method: 作者构建了合成数据集，保持结构一致但可读性不同，对比训练SLMs的表现，并探索可读性之外的其它可学习性指标（如n-gram多样性）。

Result: 实验结果发现，可读性并不能预测SLM的连贯性或学习效率。相反，基于复杂、成人化文本训练的SLMs表现不亚于用简化语言训练，且在连贯性发展上更快。n-gram多样性等统计简单性对学习能力的预测更强。

Conclusion: 本文提醒学界不要把语言模型的训练过程过于类比于人类认知发展，强调应更加精确地分析哪些因素真正支持小模型能力的涌现。

Abstract: Recent studies suggest that very small language models (SLMs) can generate
surprisingly coherent text when trained on simplified, child-directed corpora
such as TinyStories. These findings have been interpreted as evidence that
readability -- characterized by accessible vocabulary, familiar narrative
structure, and simple syntax -- plays a key role in enabling such capabilities
to emerge. In this paper, we challenge that interpretation. We construct
synthetic datasets with matched structure but varied readability, and find that
readability alone does not predict coherence or learning efficiency in SLMs.
Models trained on complex, adult-level text perform comparably to those trained
on simplified language, and even exhibit faster development of coherence during
training. Instead, we show that statistical simplicity, as measured by n-gram
diversity, is a stronger predictor of learnability. Our findings caution
against the growing trend of anthropomorphizing language model training --
drawing parallels to human cognitive development without empirical basis -- and
argue for more precise reasoning about what properties actually support
capability emergence in small models.

</details>


### [74] [Element2Vec: Build Chemical Element Representation from Text for Property Prediction](https://arxiv.org/abs/2510.13916)
*Yuanhao Li,Keyuan Lai,Tianqi Wang,Qihao Liu,Jiawei Ma,Yuan-Chao Hu*

Main category: cs.CL

TL;DR: 该论文提出利用自然语言生成元素表示的新方法，并采用自注意力机制优化预测，显著提升了属性预测的准确性，对材料科学有实用意义。


<details>
  <summary>Details</summary>
Motivation: 由于元素的某些性质难以直接测量，对元素属性进行准确预测对于材料设计和制造至关重要。传统的数值方法难以捕捉复杂联系，且属性难以用标量全面表示。人工智能方法如语言模型虽然有潜力但受限于幻觉与可解释性。

Method: 作者提出了Element2Vecto方法，通过解析维基百科上的自然语言文本，用语言模型生成元素的全局嵌入及属性特征向量。此外，考虑到元素数据稀缺和文本分布不均，设计了基于自注意力的测试时训练方法优化预测误差。

Result: Element2Vecto能够在元素间复杂关系下，有效支持属性预测并提升模型鲁棒性。自注意力训练方法明显改善了传统回归的预测误差。

Conclusion: Element2Vecto为利用自然语言提升化学元素属性预测提供了新方案，有望促进材料科学领域的AI驱动发现。

Abstract: Accurate property data for chemical elements is crucial for materials design
and manufacturing, but many of them are difficult to measure directly due to
equipment constraints. While traditional methods use the properties of other
elements or related properties for prediction via numerical analyses, they
often fail to model complex relationships. After all, not all characteristics
can be represented as scalars. Recent efforts have been made to explore
advanced AI tools such as language models for property estimation, but they
still suffer from hallucinations and a lack of interpretability. In this paper,
we investigate Element2Vecto effectively represent chemical elements from
natural languages to support research in the natural sciences. Given the text
parsed from Wikipedia pages, we use language models to generate both a single
general-purpose embedding (Global) and a set of attribute-highlighted vectors
(Local). Despite the complicated relationship across elements, the
computational challenges also exist because of 1) the discrepancy in text
distribution between common descriptions and specialized scientific texts, and
2) the extremely limited data, i.e., with only 118 known elements, data for
specific properties is often highly sparse and incomplete. Thus, we also design
a test-time training method based on self-attention to mitigate the prediction
error caused by Vanilla regression clearly. We hope this work could pave the
way for advancing AI-driven discovery in materials science.

</details>


### [75] [Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling](https://arxiv.org/abs/2510.13918)
*Peng Kuang,Yanli Wang,Xiaoyu Han,Yaowenqi Liu,Kaidi Xu,Haohan Wang*

Main category: cs.CL

TL;DR: 论文提出针对流程奖励模型与大语言模型信号的加权聚合理论，并用高效校准方法提升了测试时扩展的性能和效率，大幅优于传统多数投票和计算扩展方案。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展（TTS）依赖于过程奖励模型（PRM）来筛选LLM的最佳响应，但近期基准测试显示简单多数投票有时能超越PRM，这使得如何有效利用PRM信号成为关键问题。

Method: 首先提出理论框架，在LLM与PRM信号的最优结合下进行加权聚合，并估算最优权重函数。随后提出高效的预计算方法来校准这些权重。

Result: 提出的校准方法在5个LLM和7个PRM上实验，效率和性能均超越了传统的加权多数投票方法，仅使用21.3%的计算量。

Conclusion: 智能化的响应聚合策略，相较于单纯扩大测试计算量，更能显著提升TTS性能。

Abstract: Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),
designed to verify and select the best responses from large language models
(LLMs). However, this promise is challenged by recent benchmarks where simple
majority voting, which ignores PRM signals, occasionally outperforms standard
PRM-based selection. This raises a critical question: How can we effectively
utilize verification signals from PRMs for TTS? To address this, we start by
developing a theoretical framework for optimally combining signals from both
the LLM and the PRM. Our framework reveals that the optimal strategy is a
weighted aggregation of responses, a strategy whose effectiveness hinges on
estimating weights that capture the complex interplay between the models. Based
on our theoretical results, we empirically show that these optimal weighting
functions differ significantly across LLM-PRM pairs and, notably, often assign
substantial negative weights. Motivated by these insights, we propose efficient
pre-computation methods to calibrate these weighting functions. Extensive
experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method
significantly boosts the TTS efficiency, surpassing the performance of vanilla
weighted majority voting while using only $21.3\%$ of the computation.
Ultimately, our work demonstrates that investing in a more intelligent
aggregation strategy can be a more convincing path to performance gains than
simply scaling test-time computation.

</details>


### [76] [FACTS: Table Summarization via Offline Template Generation with Agentic Workflows](https://arxiv.org/abs/2510.13920)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出了FACTS方法，通过生成可复用的SQL和Jinja2模板，实现了高效、准确并保护隐私的查询式表格摘要生成，在多个数据集上超过现有方法，具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前基于查询的表格摘要方法存在以下问题：表到文本的模型需要昂贵的微调且难以复杂推理，基于提示的大模型方法受到Token限制、效率低和隐私风险，现有智能代理流程依赖手动模板或规划，缺乏健壮性和可扩展性。

Method: 提出了一种新的基于智能代理的工作流FACTS，通过离线模板生成（包括SQL查询和Jinja2模板）来实现表格摘要。该方法仅将表结构发送给大模型，保护隐私，并利用离线模板实现可复用、高效且准确的摘要生成。

Result: 在多个主流基准上，FACTS方法在速度、准确性和隐私合规性等方面均优于现有基线方法，适用于实际场景中的基于查询的表格摘要任务。

Conclusion: FACTS作为一种基于离线模板生成的智能代理流程，实现了快速、准确且隐私合规的表格摘要，在现实问题中展现出强大的实用性和领先的性能。

Abstract: Query-focused table summarization requires generating natural language
summaries of tabular data conditioned on a user query, enabling users to access
insights beyond fact retrieval. Existing approaches face key limitations:
table-to-text models require costly fine-tuning and struggle with complex
reasoning, prompt-based LLM methods suffer from token-limit and efficiency
issues while exposing sensitive data, and prior agentic pipelines often rely on
decomposition, planning, or manual templates that lack robustness and
scalability. To mitigate these issues, we introduce an agentic workflow, FACTS,
a Fast, Accurate, and Privacy-Compliant Table Summarization approach via
Offline Template Generation. FACTS produces offline templates, consisting of
SQL queries and Jinja2 templates, which can be rendered into natural language
summaries and are reusable across multiple tables sharing the same schema. It
enables fast summarization through reusable offline templates, accurate outputs
with executable SQL queries, and privacy compliance by sending only table
schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS
consistently outperforms baseline methods, establishing it as a practical
solution for real-world query-focused table summarization.

</details>


### [77] [An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation](https://arxiv.org/abs/2510.13925)
*Daniel Adu Worae,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型的AI智能分析框架，能高效、全面解读IoT网络流量，融合多模块且资源消耗低，在各类评价指标上表现优异，可用于实际威胁检测和网络安全分析。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）网络产生大量且多样化的数据流，包含正常与异常活动。传统方法通常只能孤立检测，很难提取跨层次、深层次的网络行为洞察，因此亟需更高效、智能的分析工具。

Method: 该工作提出一个基于大语言模型（LLM）的 AI agent 框架，将原始数据包捕获转化为结构化、语义丰富的表示，并提供交互式分析。框架集成特征提取、基于transformer的异常检测、数据包与流量总结、威胁情报增强以及检索增强式问答。AI agent利用LLM，对索引后的流量数据进行推理，生成准确且易读的解读结果。

Result: 在多个IoT数据集和六个开源模型上的实验表明，框架的混合检索（结合词汇和语义检索，并重排序）比仅用密集检索能显著提升BLEU、ROUGE、METEOR和BERTScore等评价指标。系统分析显示CPU、GPU和内存消耗较低，证明其高效性。

Conclusion: 该框架实现了物联网网络流量的全面、高效、可交互和智能解读，有助于提取有价值安全洞察，并具备实际部署的可行性。

Abstract: Internet of Things (IoT) networks generate diverse and high-volume traffic
that reflects both normal activity and potential threats. Deriving meaningful
insight from such telemetry requires cross-layer interpretation of behaviors,
protocols, and context rather than isolated detection. This work presents an
LLM-powered AI agent framework that converts raw packet captures into
structured and semantically enriched representations for interactive analysis.
The framework integrates feature extraction, transformer-based anomaly
detection, packet and flow summarization, threat intelligence enrichment, and
retrieval-augmented question answering. An AI agent guided by a large language
model performs reasoning over the indexed traffic artifacts, assembling
evidence to produce accurate and human-readable interpretations. Experimental
evaluation on multiple IoT captures and six open models shows that hybrid
retrieval, which combines lexical and semantic search with reranking,
substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with
dense-only retrieval. System profiling further indicates low CPU, GPU, and
memory overhead, demonstrating that the framework achieves holistic and
efficient interpretation of IoT network traffic.

</details>


### [78] [BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs](https://arxiv.org/abs/2510.13926)
*Congying Liu,Xingyuan Wei,Peipei Liu,Yiqing Shen,Yanxu Mao,Tiehan Cui*

Main category: cs.CL

TL;DR: 本文提出BioMedSearch框架，融合多源信息解决LLM在生物医学问答中科学性不足问题。实验证明准确率远超基线，涵盖机制、语义和因果推理三层级，提升至73.4%~91.9%。代码和数据集已公开。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域的问题通常需要对专门知识（如基因调控机制、疾病病理过程）有深刻理解，并需从多个数据源整合信息以支持准确检索和推理。现有的大语言模型(LLMs)虽在一般推理任务中表现良好，但在生成生物医学内容时缺乏科学严谨性，常虚构蛋白功能、相互作用和结构信息，缺乏权威数据库的支持。

Method: 提出了BioMedSearch，一个基于LLM的多源生物医学信息检索框架，结合文献检索、蛋白数据库和网页搜索。该方法通过子查询分解、关键词提取、任务图构建和多源信息过滤，有效提升复杂生物医学查询的处理及问答质量。构建了BioMedMCQs数据集（3000题），分为机制识别、非邻接语义整合和时序因果推理三个推理层级，用于评估BioMedSearch和其他方法在复杂QA任务上的表现。

Result: BioMedSearch在所有基线模型和推理层级上持续提升问答准确率。具体数据：Level 1从59.1%提升至91.9%；Level 2从47.0%提升至81.0%；Level 3从36.3%提升至73.4%。

Conclusion: BioMedSearch通过多源信息整合显著提升了生物医学问答的准确率，特别是在复杂推理层面，优于传统LLM方法，展现了强大的应用前景。

Abstract: Biomedical queries often rely on a deep understanding of specialized
knowledge such as gene regulatory mechanisms and pathological processes of
diseases. They require detailed analysis of complex physiological processes and
effective integration of information from multiple data sources to support
accurate retrieval and reasoning. Although large language models (LLMs) perform
well in general reasoning tasks, their generated biomedical content often lacks
scientific rigor due to the inability to access authoritative biomedical
databases and frequently fabricates protein functions, interactions, and
structural details that deviate from authentic information. Therefore, we
present BioMedSearch, a multi-source biomedical information retrieval framework
based on LLMs. The method integrates literature retrieval, protein database and
web search access to support accurate and efficient handling of complex
biomedical queries. Through sub-queries decomposition, keywords extraction,
task graph construction, and multi-source information filtering, BioMedSearch
generates high-quality question-answering results. To evaluate the accuracy of
question answering, we constructed a multi-level dataset, BioMedMCQs,
consisting of 3,000 questions. The dataset covers three levels of reasoning:
mechanistic identification, non-adjacent semantic integration, and temporal
causal reasoning, and is used to assess the performance of BioMedSearch and
other methods on complex QA tasks. Experimental results demonstrate that
BioMedSearch consistently improves accuracy over all baseline models across all
levels. Specifically, at Level 1, the average accuracy increases from 59.1% to
91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging
Level 3, the average accuracy improves from 36.3% to 73.4%. The code and
BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search

</details>


### [79] [LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)
*Shuo Xing,Junyuan Hong,Yifan Wang,Runjin Chen,Zhenyu Zhang,Ananth Grama,Zhengzhong Tu,Zhangyang Wang*

Main category: cs.CL

TL;DR: 持续使用垃圾网络文本预训练会导致大语言模型认知能力不可逆下降。作者实验证明数据质量直接影响模型表现，并提出模型需定期认知健康监测，推动数据筛选作为训练安全新议题。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLM）在持续暴露于低质量网络文本时，是否会发生持久的认知能力下降。具体关注数据质量对模型认知能力及安全性的因果影响，并希望提出如何在持续预训练过程中提升数据筛选的重要性。

Method: 作者设计了两种方法（M1：社交媒体互动度、M2：语义质量）来构造“垃圾”和对照数据集。在保证训练规模和操作一致的前提下，用真实推特语料对4个LLM进行持续预训练，观察各类认知能力随数据质量变化而发生的变化。此外还进行了剂量-反应实验，混合不同比例的垃圾数据和控制数据。

Result: 与对照组相比，持续使用垃圾数据集预训练导致模型在推理、长文本理解和安全性等方面显著下降，并产生如“黑暗特质”（精神病、病态自恋）等副作用。混合实验显示垃圾数据比例越高，认知能力衰退越明显。错误分析揭示主要为“跳过思考”现象，即模型更加倾向于省略推理链。同时，部分修复虽可改善但无法恢复至初始水平。此外，社交媒体热度（非语义指标）较长度更能预测此衰退效应。

Conclusion: 作者提出并验证了“LLM脑腐假说”，持续预训练中数据质量会因果性地导致模型能力衰退。这一发现促使对预训练数据筛选重新定位为训练期安全问题，并建议针对已部署模型例行“认知健康检查”。

Abstract: We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk
web text induces lasting cognitive decline in large language models (LLMs). To
causally isolate data quality, we run controlled experiments on real Twitter/X
corpora, constructing junk and reversely controlled datasets via two orthogonal
operationalizations: M1 (engagement degree) and M2 (semantic quality), with
matched token scale and training operations across conditions. Contrary to the
control group, continual pre-training of 4 LLMs on the junk dataset causes
non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context
understanding, safety, and inflating "dark traits" (e.g., psychopathy,
narcissism). The gradual mixtures of junk and control datasets also yield
dose-response cognition decay: for example, under M1, ARC-Challenge with Chain
Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$
as junk ratio rises from $0\%$ to $100\%$.
  Error forensics reveal several key insights. First, we identify
thought-skipping as the primary lesion: models increasingly truncate or skip
reasoning chains, explaining most of the error growth. Second, partial but
incomplete healing is observed: scaling instruction tuning and clean data
pre-training improve the declined cognition yet cannot restore baseline
capability, suggesting persistent representational drift rather than format
mismatch. Finally, we discover that the popularity, a non-semantic metric, of a
tweet is a better indicator of the Brain Rot effect than the length in M1.
Together, the results provide significant, multi-perspective evidence that data
quality is a causal driver of LLM capability decay, reframing curation for
continual pretraining as a \textit{training-time safety} problem and motivating
routine "cognitive health checks" for deployed LLMs.

</details>


### [80] [Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions](https://arxiv.org/abs/2510.13931)
*Siying Liu,Shisheng Zhang,Indu Bala*

Main category: cs.CL

TL;DR: 本文评估了两种主流大语言模型在药品安全预测任务中是否对社会人口属性产生偏见，发现模型对弱势群体预测更高风险，存在显性和隐性偏见，强调在临床使用前需加强公平性评估与偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在生物医药领域广泛应用，当前尚未充分评估其在药品安全预测中的可靠性，尤其是是否会将临床无关的社会人口特征纳入决策。

Method: 利用美国FDA药品不良事件报告系统结构化数据，结合基于角色的评估框架，测试ChatGPT-4o和Bio-Medical-Llama-3.8B两种模型，考察不同社会属性和三类用户身份下模型表现与偏见。

Result: 发现对低教育、住房不稳定等弱势群体预测不良事件概率更高，且存在显性与隐性两类偏见模式，暗示潜在风险。

Conclusion: 研究发现现有大语言模型在药品安全预测中对社会人口属性存在系统性偏差，可能对弱势群体产生不公平影响，临床应用需谨慎。

Abstract: Large language models (LLMs) are increasingly applied in biomedical domains,
yet their reliability in drug-safety prediction remains underexplored. In this
work, we investigate whether LLMs incorporate socio-demographic information
into adverse event (AE) predictions, despite such attributes being clinically
irrelevant. Using structured data from the United States Food and Drug
Administration Adverse Event Reporting System (FAERS) and a persona-based
evaluation framework, we assess two state-of-the-art models, ChatGPT-4o and
Bio-Medical-Llama-3.8B, across diverse personas defined by education, marital
status, employment, insurance, language, housing stability, and religion. We
further evaluate performance across three user roles (general practitioner,
specialist, patient) to reflect real-world deployment scenarios where
commercial systems often differentiate access by user type. Our results reveal
systematic disparities in AE prediction accuracy. Disadvantaged groups (e.g.,
low education, unstable housing) were frequently assigned higher predicted AE
likelihoods than more privileged groups (e.g., postgraduate-educated, privately
insured). Beyond outcome disparities, we identify two distinct modes of bias:
explicit bias, where incorrect predictions directly reference persona
attributes in reasoning traces, and implicit bias, where predictions are
inconsistent, yet personas are not explicitly mentioned. These findings expose
critical risks in applying LLMs to pharmacovigilance and highlight the urgent
need for fairness-aware evaluation protocols and mitigation strategies before
clinical deployment.

</details>


### [81] [Big Reasoning with Small Models: Instruction Retrieval at Inference Time](https://arxiv.org/abs/2510.13935)
*Kenan Alkiek,David Jurgens,Vinod Vydiswaran*

Main category: cs.CL

TL;DR: 该论文提出在推理阶段为小型语言模型检索结构化推理步骤的新方法，有效提升了模型在医学、法律与数学等任务上的准确率，无需额外微调，简洁指令效果更优。


<details>
  <summary>Details</summary>
Motivation: 当前，小型语言模型（SLM）因能够高效在本地硬件上运行而备受关注，具有隐私、成本和环保优势，但它们在多步推理或领域知识任务上表现有限，亟需提升其推理能力。

Method: 提出推理过程检索方法：首先用GPT-5对相似训练问题分组，生成结构化推理步骤（Instruction Corpus）；推理时，SLM检索最相关的指令并严格按步骤操作，而不是自行生成推理过程。该方法不同于传统的检索增强生成（RAG），它检索的不是原始文本而是结构化的推理指导。

Result: 在无需额外微调的情况下，将3B到14B参数量的模型应用于MedQA、MMLU法学和MathQA任务时，推理过程检索方法分别提升准确率9.4%、7.9%和5.1%。简洁指令比冗长指令表现更佳，提升幅度依赖于模型家族及其本身推理能力。

Conclusion: 推理过程检索通过为小型语言模型提供结构化推理指导，显著提升了其在复杂任务上的推理能力，为本地化智能模型的实用化提供了新思路。

Abstract: Can we bring large-scale reasoning to local-scale compute? Small language
models (SLMs) are increasingly attractive because they run efficiently on local
hardware, offering strong privacy, low cost, and reduced environmental impact.
Yet they often struggle with tasks that require multi-step reasoning or
domain-specific knowledge. We address this limitation through instruction
intervention at inference time, where an SLM retrieves structured reasoning
procedures rather than generating them from scratch. Our method builds an
Instruction Corpus by grouping similar training questions and creating
instructions via GPT-5. During inference, the SLM retrieves the most relevant
instructions and follows their steps. Unlike retrieval-augmented generation,
which retrieves text passages, instruction retrieval gives the model structured
guidance for reasoning. We evaluate this framework on MedQA (medical board
exams), MMLU Professional Law, and MathQA using models from 3B to 14B
parameters without any additional fine-tuning. Instruction retrieval yields
consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise
instructions outperform longer ones, and the magnitude of improvement depends
strongly on model family and intrinsic reasoning ability.

</details>


### [82] [FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis](https://arxiv.org/abs/2510.13936)
*Fengbin Zhu,Xiang Yao Ng,Ziyang Liu,Chang Liu,Xianwei Zeng,Chao Wang,Tianhui Tan,Xuan Yao,Pengyang Shao,Min Xu,Zixuan Wang,Jing Wang,Xin Lin,Junfeng Li,Jingxian Zhu,Yang Zhang,Wenjie Wang,Fuli Feng,Richang Hong,Huanbo Luan,Ke-Wei Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出了一套新的全面评估体系（HisRubric）和跨语言财务分析基准（FinDeepResearch），系统评测了多种深度研究智能体与大模型在企业财务分析任务中的能力，为后续研究提供了基础和参考。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对深度研究（DR）代理在关键研究分析任务中的系统性评估，尤其是在企业财务分析领域。

Method: 提出了HisRubric评估框架，具备分层分析结构和细致评分标准，对DR代理进行企业财务分析能力的严谨评估。基于此，构建了FinDeepResearch基准数据集，覆盖8个金融市场、4种语言，共64家上市公司，并设置了15,808个评分项。采用16种方法（包括DR代理、具备深度推理和检索能力的LLMs、仅有深度推理能力的LLMs）进行广泛实验。

Result: 实验揭示了不同方法在多种能力、金融市场及多语言环境下的优劣，为未来研究和开发提供了有价值的见解。

Conclusion: HisRubric和FinDeepResearch为DR代理在企业财务分析中的能力评估提供了标准化工具，推动了该领域系统性和客观性研究。

Abstract: Deep Research (DR) agents, powered by advanced Large Language Models (LLMs),
have recently garnered increasing attention for their capability in conducting
complex research tasks. However, existing literature lacks a rigorous and
systematic evaluation of DR Agent's capabilities in critical research analysis.
To address this gap, we first propose HisRubric, a novel evaluation framework
with a hierarchical analytical structure and a fine-grained grading rubric for
rigorously assessing DR agents' capabilities in corporate financial analysis.
This framework mirrors the professional analyst's workflow, progressing from
data recognition to metric calculation, and finally to strategic summarization
and interpretation. Built on this framework, we construct a FinDeepResearch
benchmark that comprises 64 listed companies from 8 financial markets across 4
languages, encompassing a total of 15,808 grading items. We further conduct
extensive experiments on the FinDeepResearch using 16 representative methods,
including 6 DR agents, 5 LLMs equipped with both deep reasoning and search
capabilities, and 5 LLMs with deep reasoning capabilities only. The results
reveal the strengths and limitations of these approaches across diverse
capabilities, financial markets, and languages, offering valuable insights for
future research and development. The benchmark and evaluation code will be made
publicly available.

</details>


### [83] [Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers](https://arxiv.org/abs/2510.13939)
*Tuhin Chakrabarty,Jane C. Ginsburg,Paramveer Dhillon*

Main category: cs.CL

TL;DR: 微调后的AI可以以极低成本写出比人类专家更受欢迎、难以区分是真人的作家风格文章，这直接冲击了现有版权制度，尤其是对市场价值的影响判断。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型使用受版权保护书籍进行训练，许多作者担心AI生成衍生内容的能力，引发了版权诉讼，但尚不清楚这些模型是否真的能高质量地模仿作家的文风。

Method: 进行了一项预注册研究，将MFA级别的专家作家与前沿AI（ChatGPT、Claude、Gemini）比较，要求它们模仿50位获奖作家的文风创作450字以内的片段。159名专家和普通读者以盲测配对方式评估文本的文风和质量，同时比较原始AI提示与针对特定作者微调的AI模型表现。

Result: 原始AI提示生成的文本在文风和质量上均被专家评委显著不喜欢；但当对AI进行作者特定微调后，专家和普通读者都更喜欢AI生成文本，甚至超过人类专家写作，微调输出文本几乎难以被AI检测器甄别。

Conclusion: 针对作者进行专门微调能让AI生成更受人类（专家和普通读者）欢迎的高质量文学文本，且显著降低成本，这将影响版权的“市场影响”评估。

Abstract: The use of copyrighted books for training AI models has led to numerous
lawsuits from authors concerned about AI's ability to generate derivative
content.Yet it's unclear whether these models can generate high quality
literary text while emulating authors' styles. To answer this we conducted a
preregistered study comparing MFA-trained expert writers with three frontier AI
models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating
50 award-winning authors' diverse styles. In blind pairwise evaluations by 159
representative expert & lay readers, AI-generated text from in-context
prompting was strongly disfavored by experts for both stylistic fidelity
(OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results
with lay readers. However, fine-tuning ChatGPT on individual authors' complete
works completely reversed these findings: experts now favored AI-generated text
for stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010),
with lay readers showing similar shifts. These effects generalize across
authors & styles. The fine-tuned outputs were rarely flagged as AI-generated
(3% rate v. 97% for in-context prompting) by best AI detectors. Mediation
analysis shows this reversal occurs because fine-tuning eliminates detectable
AI stylistic quirks (e.g., cliche density) that penalize in-context outputs.
While we do not account for additional costs of human effort required to
transform raw AI output into cohesive, publishable prose, the median
fine-tuning & inference cost of $81 per author represents a dramatic 99.7%
reduction compared to typical professional writer compensation. Author-specific
fine-tuning thus enables non-verbatim AI writing that readers prefer to expert
human writing, providing empirical evidence directly relevant to copyright's
fourth fair-use factor, the "effect upon the potential market or value" of the
source works.

</details>


### [84] [Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention](https://arxiv.org/abs/2510.13940)
*Zhen Yang,Mingyang Zhang,Feng Chen,Ganggui Ding,Liang Hou,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CL

TL;DR: 本文发现并利用了推理阶段的不确定性局部性，通过Minimal Test-Time Intervention新方法，在几乎不增加计算成本下提升各类任务准确率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理能力提升上多聚焦于推理阶段算力扩展，虽提高了推理表现，但牺牲了效率。本文关注推理阶段的不确定性，发现高不确定性仅局限于少量高熵token，这一现象此前研究较少。

Method: 提出了Minimal Test-Time Intervention（MTI）框架，无需额外训练，包含：（i）在高不确定性位置选择性地采用无分类器引导（CFG）；（ii）通过重用主模型KV cache实现高效的负向提示引导，从而近似无条件解码。

Result: MTI方案在通用、编程及STEM等任务上实现了稳定提升：如Qwen3-8B-Base在八个基准数据集上平均提升1.35%，Qwen3-32B-Reasoning在AIME2024上提升5%；且保持高效。

Conclusion: 推理不确定性高度局部化，MTI通过针对性干预显著提升大模型推理准确率与稳定性，并兼顾效率。

Abstract: Recent progress in large language models (LLMs) has focused on test-time
scaling to improve reasoning via increased inference computation, but often at
the cost of efficiency. We revisit test-time behavior and uncover a simple yet
underexplored phenomenon: reasoning uncertainty is highly localized-only a
small subset of high-entropy tokens dominantly affects output correctness.
Motivated by this, we propose Minimal Test-Time Intervention (MTI), a
training-free framework that enhances reasoning accuracy and stability with
minimal overhead. MTI includes: (i) Selective CFG intervention, applying
classifier-free guidance only at uncertain positions; and (ii) Lightweight
negative-prompt guidance, reusing the main model's KV cache to approximate
unconditional decoding efficiently. MTI yields consistent gains across general,
coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for
Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining
highly efficient.

</details>


### [85] [Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2510.13975)
*Kin Kwan Leung,Mouloud Belbahri,Yi Sui,Alex Labach,Xueying Zhang,Stephen Rose,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 该论文提出了RAG问答系统的错误类型新分类体系，公开了错误数据集和代码，并提供了自动化错误评估方法，可帮助开发者识别、跟踪和修复系统中的各种错误，提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RAG系统在实际应用中可能出现多种错误，影响问答质量和系统鲁棒性，因此识别并归类这些错误类型对系统优化至关重要。

Method: 提出新的错误类型分类法，举例说明，并提供实际处理建议。此外，构建了带错误类型标注的数据集，并提出了一种自动评估方法以跟踪和解决开发中的错误。

Result: 提出了RAG系统错误新分类与应对建议，发布了相关数据集和代码，并验证了自动化错误评估方法的实用性。

Conclusion: 通过系统化错误类型归纳、数据集整理以及自动评估方法，提高了RAG系统错误检测和处理的能力，有助于构建更健壮的问答系统。

Abstract: Retrieval-augmented generation (RAG) is a prevalent approach for building
LLM-based question-answering systems that can take advantage of external
knowledge databases. Due to the complexity of real-world RAG systems, there are
many potential causes for erroneous outputs. Understanding the range of errors
that can occur in practice is crucial for robust deployment. We present a new
taxonomy of the error types that can occur in realistic RAG systems, examples
of each, and practical advice for addressing them. Additionally, we curate a
dataset of erroneous RAG responses annotated by error types. We then propose an
auto-evaluation method aligned with our taxonomy that can be used in practice
to track and address errors during development. Code and data are available at
https://github.com/layer6ai-labs/rag-error-classification.

</details>


### [86] [The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models](https://arxiv.org/abs/2510.13996)
*Lukas Gienapp,Christopher Schröder,Stefan Schweter,Christopher Akiki,Ferdinand Schlatt,Arden Zimmermann,Phillipe Genêt,Martin Potthast*

Main category: cs.CL

TL;DR: 该论文构建了目前最大、完全开放授权的德语文本数据集 German Commons，解决了德语大模型训练缺乏合规语料的难题，并提供了全套开源数据处理工具。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型训练严重依赖大规模语料，但在非英语领域，尤其是德语，开放授权文本极为稀缺，这限制了模型的开放与发展。

Method: 系统性地从41个来源采集高质量德语文本，涵盖7大领域，采用全面质量过滤、去重与格式修复流程，保证数据一致性和高质量；所有子集均满足 CC-BY-SA 4.0 及以上授权。

Result: 最终生成了包含1545.6亿token的高质量德语文本集，并公开了用于语料构建和过滤的代码，实现数据集的可复现和可扩展。

Conclusion: German Commons 填补了德语开放授权训练数据的关键缺口，为开发真正开放的德语大语言模型提供了基础。

Abstract: Large language model development relies on large-scale training corpora, yet
most contain data of unclear licensing status, limiting the development of
truly open models. This problem is exacerbated for non-English languages, where
openly licensed text remains critically scarce. We introduce the German
Commons, the largest collection of openly licensed German text to date. It
compiles data from 41 sources across seven domains, encompassing legal,
scientific, cultural, political, news, economic, and web text. Through
systematic sourcing from established data providers with verifiable licensing,
it yields 154.56 billion tokens of high-quality text for language model
training. Our processing pipeline implements comprehensive quality filtering,
deduplication, and text formatting fixes, ensuring consistent quality across
heterogeneous text sources. All domain subsets feature licenses of at least
CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and
redistribution. The German Commons therefore addresses the critical gap in
openly licensed German pretraining data, and enables the development of truly
open German language models. We also release code for corpus construction and
data filtering tailored to German language text, rendering the German Commons
fully reproducible and extensible.

</details>


### [87] [CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models](https://arxiv.org/abs/2510.14014)
*Shehenaz Hossain,Haithem Afli*

Main category: cs.CL

TL;DR: 论文提出CRaFT框架，通过四项指标跨3种语言评估大模型文化推理，发现模型在不同文化下表现存在明显差异，提供了开发具文化适应性模型的新方法和见解。


<details>
  <summary>Details</summary>
Motivation: 传统评测往往侧重于模型答案的准确性，忽视了模型在不同文化背景下的推理和理解能力。本研究关注大模型跨语言、跨文化解释能力的评估，弥补现有评测的局限。

Method: 提出了一套基于解释的多语言评估框架CRaFT，采用文化流畅性、偏离度、一致性和语言适应性四个可解释指标评测模型的推理能力。在阿拉伯语、孟加拉语和西班牙语三种语言下，选取世界价值观调查中的50个文化相关问题，对GPT、DeepSeek和FANAR三种模型2100余组答案及解释进行系统评测。

Result: 不同语言下模型推理表现呈现差异：阿拉伯语降低流畅性，孟加拉语提升流畅性，西班牙语基本稳定。GPT跨语言适应更好但一致性较低，FANAR推理稳定但缺乏灵活性。

Conclusion: 大型语言模型的文化意识并非内在固有，而是在不同语言表达中逐步体现。CRaFT为多语言环境下跨文化推理评估提供新视角，有助于构建更具文化适应性的语言模型。

Abstract: Correct answers do not necessarily reflect cultural understanding. We
introduce CRaFT, an explanation-based multilingual evaluation framework
designed to assess how large language models (LLMs) reason across cultural
contexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates
model explanations using four interpretable metrics: Cultural Fluency,
Deviation, Consistency, and Linguistic Adaptation. We apply the framework to 50
culturally grounded questions from the World Values Survey, translated into
Arabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and
FANAR) across over 2,100 answer-explanation pairs. Results reveal significant
cross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances
it, and Spanish remains largely stable. While GPT adapts more effectively
across languages, it exhibits lower consistency; FANAR shows stable but rigid
reasoning. These findings suggest that cultural awareness in LLMs is not
intrinsic but emerges through linguistic framing. CRaFT offers a new lens for
evaluating cross-cultural reasoning in multilingual settings, providing
actionable insights for building culturally adaptive language models.

</details>


### [88] [Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games](https://arxiv.org/abs/2510.14030)
*César Guerra-Solano,Zhuochun Li,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: 本文通过跨多语种抽象推理游戏评估大语言模型的语言偏见，发现英语模式下表现最优，且模型类型对结果也有显著影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同语言模式下的推理能力存在偏差，尤其在抽象推理任务上的表现尚未被充分评估。抽象推理能力对于日常生活中的“跳出框架思考”至关重要。

Method: 设计了一个受纽约时报Connections启发的抽象推理任务GlobalGroup，涵盖英语、西班牙语、中文、印地语和阿拉伯语，分别使用母语和英文翻译进行对比，并提出了游戏难度评估方法以保证实验可控性。

Result: 实验结果显示，在抽象推理任务中，模型用英文完成任务时表现显著更好，同时开源与闭源模型间也存在性能差异。

Conclusion: 大语言模型在英语语境下具有抽象推理的性能优势，模型本身和语言模态均对推理能力有重要影响，说明语言偏见和模型类型是影响推理表现的关键因素。

Abstract: Large language models (LLMs) can exhibit biases in reasoning capabilities due
to linguistic modality, performing better on tasks in one language versus
another, even with similar content. Most previous works evaluate this through
reasoning tasks where reliance on strategies or knowledge can ensure success,
such as in commonsense or math tasks. However, abstract reasoning is vital to
reasoning for everyday life, where people apply "out-of-the-box thinking" to
identify and use patterns for solutions, without a reliance on formulaic
approaches. Comparatively, little work has evaluated linguistic biases in this
task type. In this paper, we propose a task inspired by the New York Times
Connections: GlobalGroup, that evaluates models in an abstract reasoning task
across several languages. We constructed a game benchmark with five linguistic
backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the
native language and an English translation for comparison. We also proposed
game difficulty measurements to evaluate models on games with similar
difficulty, enabling a more controlled comparison, which is particularly
important in reasoning evaluations. Through experimentation, we find English
modalities largely lead to better performance in this abstract reasoning task,
and performance disparities between open- and closed-source models.

</details>


### [89] [Quantifying Phonosemantic Iconicity Distributionally in 6 Languages](https://arxiv.org/abs/2510.14040)
*George Flint,Kaustubh Kislay*

Main category: cs.CL

TL;DR: 本文通过对6种语言分布式定量分析，发现了一些新的语音-语义系统性联系，并验证了部分已知现象的真实性，揭示了跨语言规律，为语言任意性假说提出了重要补充。


<details>
  <summary>Details</summary>
Motivation: 语言通常被认为是任意性的，但以往在特定情况下观察到语音与语义之间存在系统性关系。作者希望通过大规模、定量的方法探索这种关系在不同语言中的真实程度，既关注已知现象，也试图发现新的规律。

Method: 作者采用分布式方法，定量分析英、西、印地语、芬兰语、土耳其语和泰米尔语6种语言中词素的语音与语义相似性空间的对齐情况，并利用多种统计指标评估系统性联系。

Result: 发现了许多准备解读的语音-语义对齐现象，其中一些尚未在文献中报导，并呈现出跨语言的模式。同时，对5个已有假设的现象进行了验证，有些获得了支持，有些则结果不一。

Conclusion: 部分语言在语音与语义之间存在系统性联系，并且这种联系大小、类型在不同语言间呈现出规律性分布。部分以往假设获得实证支持，部分则需进一步考察。

Abstract: Language is, as commonly theorized, largely arbitrary. Yet, systematic
relationships between phonetics and semantics have been observed in many
specific cases. To what degree could those systematic relationships manifest
themselves in large scale, quantitative investigations--both in previously
identified and unidentified phenomena? This work undertakes a distributional
approach to quantifying phonosemantic iconicity at scale across 6 diverse
languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each
language, we analyze the alignment of morphemes' phonetic and semantic
similarity spaces with a suite of statistical measures, and discover an array
of interpretable phonosemantic alignments not previously identified in the
literature, along with crosslinguistic patterns. We also analyze 5 previously
hypothesized phonosemantic alignments, finding support for some such alignments
and mixed results for others.

</details>


### [90] [ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models](https://arxiv.org/abs/2510.14077)
*Haziq Mohammad Khalid,Athikash Jeyaganthan,Timothy Do,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: ERGO是一种基于模型不确定性（熵）动态重置prompt的方法，大幅提升了多轮增量对话任务中的AI表现与可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多轮对话尤其信息逐步呈现时表现明显下降，这种场景正是实际应用的常态，因此亟需提升其实际可用性。

Method: 作者提出ERGO方法，通过监测模型生成下一个词分布的香农熵，检测模型不确定性的急剧增加，并据此动态重置和整合对话输入（prompt），借此实时调整模型的语境对齐。

Result: 在多轮逐步呈现指令的任务中，ERGO方法平均性能提升56.6%，峰值能力提升24.7%，性能不稳定性降低35.3%。

Conclusion: 通过将不确定性作为重要信号加以利用而非一味消除，ERGO能有效提升多轮对话中的准确性及稳定性，表明增强不确定性感知可提升对话式AI的可靠性与可用性。

Abstract: Large Language Models (LLMs) suffer significant performance degradation in
multi-turn conversations when information is presented incrementally. Given
that multi-turn conversations characterize everyday interactions with LLMs,
this degradation poses a severe challenge to real world usability. We
hypothesize that abrupt increases in model uncertainty signal misalignment in
multi-turn LLM interactions, and we exploit this insight to dynamically realign
conversational context. We introduce ERGO (Entropy-guided Resetting for
Generation Optimization), which continuously quantifies internal uncertainty
via Shannon entropy over next token distributions and triggers adaptive prompt
consolidation when a sharp spike in entropy is detected. By treating
uncertainty as a first class signal rather than a nuisance to eliminate, ERGO
embraces variability in language and modeling, representing and responding to
uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO
yields a 56.6% average performance gain over standard baselines, increases
aptitude (peak performance capability) by 24.7%, and decreases unreliability
(variability in performance) by 35.3%, demonstrating that uncertainty aware
interventions can improve both accuracy and reliability in conversational AI.

</details>


### [91] [DROID: Dual Representation for Out-of-Scope Intent Detection](https://arxiv.org/abs/2510.14110)
*Wael Rashwan,Hossam M. Zawbaa,Sourav Dutta,Haytham Assem*

Main category: cs.CL

TL;DR: 本文提出一个融合双编码器的轻量端到端OOS检测框架DROID，在多个基准测试中取得显著性能提升，尤其适合低资源环境。


<details>
  <summary>Details</summary>
Motivation: 在任务型对话系统以及开放集意图识别中，检测超出范围（OOS）的用户语句一直是一个重要难题。当前方法往往依赖于强分布假设或额外的校准模块，亟需更高效、紧凑且鲁棒的检测框架。

Method: DROID框架融合了Universal Sentence Encoder与TSDAE领域适应编码器，生成互补语义表示，再通过分支分类器以单一阈值完成意图分类，无需事后评分。模型通过合成和开放域异常样本增强边界学习能力，实现OOS检测。

Result: 提出的DROID方法凭借轻量结构，仅1.5M可训练参数，在多个意图基准数据集上均超越现有最新方法，在已知意图上提升了6-15%的macro-F1分数，对于OOS意图提升了8-20%，尤其在低资源环境下表现突出。

Conclusion: 通过结合通用和领域自适应语义编码，以及合成和开放域异常增强，DROID实现了高效、稳定且可扩展的OOS意图检测，为神经对话系统的实际应用提供了有力支持。

Abstract: Detecting out-of-scope (OOS) user utterances remains a key challenge in
task-oriented dialogue systems and, more broadly, in open-set intent
recognition. Existing approaches often depend on strong distributional
assumptions or auxiliary calibration modules. We present DROID (Dual
Representation for Out-of-Scope Intent Detection), a compact end-to-end
framework that combines two complementary encoders -- the Universal Sentence
Encoder (USE) for broad semantic generalization and a domain-adapted
Transformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual
distinctions. Their fused representations are processed by a lightweight
branched classifier with a single calibrated threshold that separates in-domain
and OOS intents without post-hoc scoring. To enhance boundary learning under
limited supervision, DROID incorporates both synthetic and open-domain outlier
augmentation. Despite using only 1.5M trainable parameters, DROID consistently
outperforms recent state-of-the-art baselines across multiple intent
benchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for
OOS intents, with the most significant gains in low-resource settings. These
results demonstrate that dual-encoder representations with simple calibration
can yield robust, scalable, and reliable OOS detection for neural dialogue
systems.

</details>


### [92] [Toward Cybersecurity-Expert Small Language Models](https://arxiv.org/abs/2510.14113)
*Matan Levi,Daniel Ohayon,Ariel Blobstein,Ravid Sagi,Ian Molloy,Yair Allouche*

Main category: cs.CL

TL;DR: 提出了CyberPal 2.0网络安全小型语言模型，通过专家和多步推理丰富数据集训练，在核心安全任务上超越多款主流模型，实现高性能与高效率的结合。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在网络安全领域的应用落后，主要原因是缺乏高质量、领域专属的模型和训练数据集。本文旨在弥补这一空缺，推动网络安全领域智能化进步。

Method: 提出并训练了CyberPal 2.0，一系列专为网络安全设计的小型语言模型（4B-20B参数）。训练过程采用自主研发的数据增强和格式化管道SecKnowledge 2.0，结合专家参与指导和LLM驱动的多步思考，生成高保真度、有任务针对性的推理链数据集。

Result: CyberPal 2.0在多个网络安全基准测试上表现优异，超过其比较基线，并在模型体积大幅缩小的前提下，部分任务表现超越主流开源和闭源前沿模型。在网络威胁情报知识任务中，表现仅次于Sec-Gemini v1；在威胁调查任务如漏洞与缺陷票据相关性分析中，20B模型排名第一，4B模型排名第二，超过GPT-4o等商业模型。

Conclusion: CyberPal 2.0通过高质量专家指导数据集和小模型设计，在网络安全领域实现高效、优质的推理与分析能力，兼具性能和模型轻量化优势。该方法为领域应用提供了切实可行的新范例。

Abstract: Large language models (LLMs) are transforming everyday applications, yet
deployment in cybersecurity lags due to a lack of high-quality, domain-specific
models and training datasets. To address this gap, we present CyberPal 2.0, a
family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B
parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought
cybersecurity instruction dataset built with our data enrichment and formatting
pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of
reasoning formats alongside LLM-driven multi-step grounding, yielding
higher-fidelity, task-grounded reasoning traces for security tasks. Across
diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its
baselines and matches or surpasses various open and closed-source frontier
models, while remaining a fraction of their size. On core cyber threat
intelligence knowledge tasks, our models outperform almost all tested frontier
models, ranking second only to Sec-Gemini v1. On core threat-investigation
tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our
best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1,
ranking first, while our smallest 4B-parameter model ranks second.

</details>


### [93] [Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis](https://arxiv.org/abs/2510.14128)
*Darko Sasanski,Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.CL

TL;DR: 本研究首次系统性构建了马其顿语菜谱数据集，并揭示了马其顿菜肴中具有代表性的原材料搭配模式，为数字美食研究提供新资源。


<details>
  <summary>Details</summary>
Motivation: 目前在计算美食学领域，主流语种已有大量菜谱数据集，但马其顿语相关资源严重不足，限制了区域饮食文化的研究。本文试图填补该空白。

Method: 作者采用网络爬虫与结构化解析的方法，从网络获取马其顿语菜谱，针对成分描述的异构性（如单位、数量、描述词）进行了标准化处理。使用成分频率、互信息和提升度等统计方式分析原材料搭配模式。

Result: 成功构建了第一个系统性的马其顿菜谱数据集，分析发现了一些具有代表性和辨识度的马其顿食材组合模式。

Conclusion: 本文为研究小语种饮食文化和马其顿传统美食提供了全新且有意义的数据资源，有助于拓宽计算美食学的覆盖面。

Abstract: Computational gastronomy increasingly relies on diverse, high-quality recipe
datasets to capture regional culinary traditions. Although there are
large-scale collections for major languages, Macedonian recipes remain
under-represented in digital research. In this work, we present the first
systematic effort to construct a Macedonian recipe dataset through web scraping
and structured parsing. We address challenges in processing heterogeneous
ingredient descriptions, including unit, quantity, and descriptor
normalization. An exploratory analysis of ingredient frequency and
co-occurrence patterns, using measures such as Pointwise Mutual Information and
Lift score, highlights distinctive ingredient combinations that characterize
Macedonian cuisine. The resulting dataset contributes a new resource for
studying food culture in underrepresented languages and offers insights into
the unique patterns of Macedonian culinary tradition.

</details>


### [94] [RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following](https://arxiv.org/abs/2510.14200)
*Zhichao Wang,Andy Wong,Ruslan Belkin*

Main category: cs.CL

TL;DR: 通过RLSR（强化学习中的语义奖励）替换或结合SFT训练方法，可进一步提升大语言模型的指令遵循能力，并且在评测集上的表现优于传统SFT。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在预训练后，常使用SFT等技术增强模型的指令遵循能力与领域适应性，但SFT存在利用效率和泛化能力的提升空间。近年来，受RFT启发，利用强化学习（RL）框架处理少量数据下的领域自适应成为研究热点。

Method: 提出RLSR方法：用强化学习方法替换SFT，将大量SFT数据整合至RL流程。具体做法是模型给定同一提示生成多样化响应，通过与真人标注答案在语义嵌入空间的余弦相似度计算奖励分数，从而优化模型。RLSR可直接替换SFT或与SFT结合使用。

Result: 在指令遵循评测（如AlpacaEval）上，RLSR（SB）在Qwen-7B（INFINITY）模型上的获胜率达26.34%，优于SFT的21.01%；SFT与RLSR结合后，表现进一步提升至30.73%。

Conclusion: RLSR能高效利用现有大规模SFT数据，在强化学习框架下提高大模型的指令遵循能力，替换或补充SFT既可提升下游任务表现，也为领域适应和泛化提供了新的方法。

Abstract: After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and
RFT are applied to enhance instruction-following ability, mitigate undesired
responses, improve reasoning capability and enable efficient domain adaptation
with minimal data. SFT relies on the next-token prediction objective to
strengthen instruction following in a base model using a large corpus of
human-labeled responses. In contrast, RFT employs a RL-based approach to adapt
fine-tuned reasoning models to specific domains with limited supervision.
Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive
SFT dataset in an RL framework, thereby improving the base model's
instruction-following ability. In RLSR, the base model generates multiple
responses for each prompt, and reward scores are computed as the cosine
similarity in the semantic embedding space between the generated and
human-labeled responses. RLSR can be utilized in multiple ways. It can directly
replace SFT, achieving superior performance on instruction-following
benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval
win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and
RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved
a win rate of 30.73% when trained with SFT + RLSR.

</details>


### [95] [DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans](https://arxiv.org/abs/2510.14205)
*Bingsheng Yao,Bo Sun,Yuanzhe Dong,Yuxuan Lu,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出动态画像精炼框架用于提升大模型角色扮演的个体真实性，通过持续修正画像与真实行为的偏差，能有效提升多模型多场景下的行为对齐，增强各类应用的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型角色扮演代理（LLM RPAs）试图模拟个人行为，但通常依赖手动构建的画像，这些画像缺乏与真实目标个体的一致性验证，造成角色真实性不足。

Method: 提出了动态画像精炼框架（DPRF），通过自由形式或理论驱动的结构化分析，迭代识别和修正生成行为与真人行为之间的认知偏差，从而优化画像与目标个体行为的对齐。

Result: 在四种不同的行为预测场景（正式辩论、心理健康相关社交媒体帖子、公开访谈、影评）和五种大模型上评估DPRF，结果显示该框架能显著提高行为对齐度，且在不同模型和场景有良好泛化能力。

Conclusion: DPRF为构建高真实性画像提供了有效方法，并可提升用户模拟、社会研究与个性化AI等后续应用的有效性。

Abstract: The emerging large language model role-playing agents (LLM RPAs) aim to
simulate individual human behaviors, but the persona fidelity is often
undermined by manually-created profiles (e.g., cherry-picked information and
personality characteristics) without validating the alignment with the target
individuals. To address this limitation, our work introduces the Dynamic
Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM
RPAs' behaviors with those of target individuals by iteratively identifying the
cognitive divergence, either through free-form or theory-grounded, structured
analysis, between generated behaviors and human ground truth, and refining the
persona profile to mitigate these divergences.We evaluate DPRF with five LLMs
on four diverse behavior-prediction scenarios: formal debates, social media
posts with mental health issues, public interviews, and movie reviews.DPRF can
consistently improve behavioral alignment considerably over baseline personas
and generalizes across models and scenarios.Our work provides a robust
methodology for creating high-fidelity persona profiles and enhancing the
validity of downstream applications, such as user simulation, social studies,
and personalized AI.

</details>


### [96] [LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning](https://arxiv.org/abs/2510.14211)
*Beomseok Kang,Jiwon Song,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 提出一种能自适应跳层且提前终止输出的加速多阶段推理框架LiteStage，在提升速度的同时几乎不影响准确率，效果优于现有相关方法。


<details>
  <summary>Details</summary>
Motivation: 多阶段推理可以提升小型语言模型的推理能力，但现有的加速技术（如跳层）在保持效率和准确性之间难以平衡，尤其受到不同阶段对跳层敏感性的影响以及冗余输出生成的困扰。

Method: 提出了LiteStage框架，结合了（1）阶段性的离线搜索以分配最优层预算，以及（2）基于置信度的在线提前退出机制，压制无必要的解码输出，实现针对多阶段推理任务的低延迟跳层。

Result: 在OBQA、CSQA和StrategyQA三个基准测试上，LiteStage实现了最高1.70倍的推理加速，准确率损失低于4.0%，超越了现有的无训练跳层方法。

Conclusion: LiteStage能在保证准确率的同时大幅降低多阶段推理的延迟，为小型语言模型快速推理提供了有效的新方法。

Abstract: Multi-stage reasoning has emerged as an effective strategy for enhancing the
reasoning capability of small language models by decomposing complex problems
into sequential sub-stages. However, this comes at the cost of increased
latency. We observe that existing adaptive acceleration techniques, such as
layer skipping, struggle to balance efficiency and accuracy in this setting due
to two key challenges: (1) stage-wise variation in skip sensitivity, and (2)
the generation of redundant output tokens. To address these, we propose
LiteStage, a latency-aware layer skipping framework for multi-stage reasoning.
LiteStage combines a stage-wise offline search that allocates optimal layer
budgets with an online confidence-based generation early exit to suppress
unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and
StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than
4.0% accuracy loss, outperforming prior training-free layer skipping methods.

</details>


### [97] [Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs](https://arxiv.org/abs/2510.14242)
*Parsa Hejabi,Elnaz Rahmati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

TL;DR: F^2C是一种提升LLM对不同提示表达稳定性和性能的无监督训练方法，有效提升了一致性、准确率及泛化性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对不同表达的同一提示常输出不一致，亟需提升其对提示扰动的鲁棒性。

Method: 提出了 Flip-Flop Consistency (F^2C) 无监督训练方法，包含共识交叉熵（CCE）和表征对齐损失两部分，使模型对多样化提示具有更稳健输出。

Result: F^2C 在四个NLP任务、11个数据集、不同提示变体下，将一致性提升11.62%，平均F1提升8.94%，性能方差降低3.29%。在域外评测及未见格式下均提升鲁棒性和泛化能力。

Conclusion: F^2C 能在不同提示扰动下提升大语言模型的一致性、性能和泛化能力，是一种有效的无监督训练方法。

Abstract: Large Language Models (LLMs) often produce inconsistent answers when faced
with different phrasings of the same prompt. In this paper, we propose
Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves
robustness to such perturbations. $F^2C$ is composed of two key components. The
first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt
variations to create a hard pseudo-label. The second is a representation
alignment loss that pulls lower-confidence and non-majority predictors toward
the consensus established by high-confidence, majority-voting variations. We
evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt
variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%,
improves mean $F_1$ by 8.94%, and reduces performance variance across formats
by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively,
increasing $\overline{F_1}$ and agreement while decreasing variance across most
source-target pairs. Finally, when trained on only a subset of prompt
perturbations and evaluated on held-out formats, $F^2C$ consistently improves
both performance and agreement while reducing variance. These findings
highlight $F^2C$ as an effective unsupervised method for enhancing LLM
consistency, performance, and generalization under prompt perturbations. Code
is available at
https://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.

</details>


### [98] [MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2510.14252)
*Jihao Zhao,Zhiyuan Ji,Simin Niu,Hanyu Wang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 该文提出了MoM框架，通过模拟专家主动生成结构化文档记忆，改进RAG系统分块和记忆提取，有效提升小模型的人类认知式文本处理能力，在多领域实验中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG仅被动选取相关文本块，限制了知识深度理解和复杂推理能力。为模拟人类阅读时的主动认知，需构建更有效的文档记忆抽取机制，提升语义完整性和上下文推理。

Method: 提出Mixtures of scenario-aware document Memories（MoM）框架，利用大模型模拟领域专家生成逻辑大纲，并通过多路径采样、多视角评价选优文档块，加入逆向推理强化人类式认知并设计三层记忆检索机制。理论分析结合概率模型，辅以多领域实验验证。

Result: MoM在三个领域上的实验显示：有效解决了RAG分块和记忆片段缺乏语义完整性的问题，使模型获取更丰富的文档知识，加强了SLM的主动理解和人性化推理能力。

Conclusion: MoM框架通过主动构建有逻辑性、语义完整的文档记忆，显著改善了传统RAG系统中文档分块和记忆提取的效果，并促进小型语言模型向具有人类阅读理解和推理能力的方向发展。

Abstract: The traditional RAG paradigm, which typically engages in the comprehension of
relevant text chunks in response to received queries, inherently restricts both
the depth of knowledge internalization and reasoning capabilities. To address
this limitation, our research transforms the text processing in RAG from
passive chunking to proactive understanding, defining this process as document
memory extraction with the objective of simulating human cognitive processes
during reading. Building upon this, we propose the Mixtures of scenario-aware
document Memories (MoM) framework, engineered to efficiently handle documents
from multiple domains and train small language models (SLMs) to acquire the
ability to proactively explore and construct document memories. The MoM
initially instructs large language models (LLMs) to simulate domain experts in
generating document logical outlines, thereby directing structured chunking and
core content extraction. It employs a multi-path sampling and multi-perspective
evaluation mechanism, specifically designing comprehensive metrics that
represent chunk clarity and extraction completeness to select the optimal
document memories. Additionally, to infuse deeper human-like reading abilities
during the training of SLMs, we incorporate a reverse reasoning strategy, which
deduces refined expert thinking paths from high-quality outcomes. Finally,
leveraging diverse forms of content generated by MoM, we develop a three-layer
document memory retrieval mechanism, which is grounded in our theoretical proof
from the perspective of probabilistic modeling. Extensive experimental results
across three distinct domains demonstrate that the MoM framework not only
resolves text chunking challenges in existing RAG systems, providing LLMs with
semantically complete document memories, but also paves the way for SLMs to
achieve human-centric intelligent text processing.

</details>


### [99] [Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior](https://arxiv.org/abs/2510.14261)
*Rahul Nadkarni,Yanai Elazar,Hila Gonen,Noah A. Smith*

Main category: cs.CL

TL;DR: 作者提出一种实验性数据干预流程，重训练模型以验证数据对语言模型行为的影响，发现已有识别相关文档方法不能完全解释模型知识习得，并公开工具推动后续研究。


<details>
  <summary>Details</summary>
Motivation: 旨在探索和验证训练数据与语言模型行为之间的关系，推动理解数据对模型知识习得和表现的影响。

Method: 提出了一套实验性的流程，先通过干预（修改）训练数据，然后重训模型，通过匹配评测基准和相关文档，修改文档后再测量行为变化，以检验特定假设。

Result: 通过案例研究，使用共现统计和信息检索方法定位对知识学习有贡献的文档。结果显示，传统“相关文档识别”方法不能完全解释模型正确回答知识问题的能力，补充了之前的观察分析。

Conclusion: 提出并验证了一套可复制的实验流程，为今后研究训练数据与模型行为关系提供了工具和方法，代码也已公开，便于社区进一步探索。

Abstract: We present an experimental recipe for studying the relationship between
training data and language model (LM) behavior. We outline steps for
intervening on data batches -- i.e., ``rewriting history'' -- and then
retraining model checkpoints over that data to test hypotheses relating data to
behavior. Our recipe breaks down such an intervention into stages that include
selecting evaluation items from a benchmark that measures model behavior,
matching relevant documents to those items, and modifying those documents
before retraining and measuring the effects. We demonstrate the utility of our
recipe through case studies on factual knowledge acquisition in LMs, using both
cooccurrence statistics and information retrieval methods to identify documents
that might contribute to knowledge learning. Our results supplement past
observational analyses that link cooccurrence to model behavior, while
demonstrating that extant methods for identifying relevant training documents
do not fully explain an LM's ability to correctly answer knowledge questions.
Overall, we outline a recipe that researchers can follow to test further
hypotheses about how training data affects model behavior. Our code is made
publicly available to promote future work.

</details>


### [100] [Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation](https://arxiv.org/abs/2510.14271)
*Yilun Zheng,Dan Yang,Jie Li,Lin Shang,Lihui Chen,Jiahao Xu,Sitao Luan*

Main category: cs.CL

TL;DR: 本文提出的DEG-RAG框架对自动化生成的知识图谱进行去噪处理，显著提升了基于图的RAG系统在问答任务中的表现，并首次系统性研究了实体消歧技术。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识图谱的RAG系统在自动化构建知识图谱时，存在噪声问题（冗余实体和错误关系），这会影响后续的信息检索和生成效果，且该问题尚未被系统性解决。

Method: 提出DEG-RAG框架，包含实体消歧与三元组反思两项技术，对自动生成的知识图谱进行去噪处理，并系统评估实体消歧涉及的阻断策略、嵌入选择、相似度度量及实体合并方法。

Result: DEG-RAG方法显著压缩知识图谱规模，并在多种主流RAG变体上提升了问答性能。

Conclusion: DEG-RAG有效解决了知识图谱噪声问题，是首个深入研究LLM生成知识图谱实体消歧的工作，其方案简单易行、效果明显，并提升了检索增强生成的整体性能。

Abstract: Retrieval-Augmented Generation (RAG) systems enable large language models
(LLMs) instant access to relevant information for the generative process,
demonstrating their superior performance in addressing common LLM challenges
such as hallucination, factual inaccuracy, and the knowledge cutoff.
Graph-based RAG further extends this paradigm by incorporating knowledge graphs
(KGs) to leverage rich, structured connections for more precise and inferential
responses. A critical challenge, however, is that most Graph-based RAG systems
rely on LLMs for automated KG construction, often yielding noisy KGs with
redundant entities and unreliable relationships. This noise degrades retrieval
and generation performance while also increasing computational cost. Crucially,
current research does not comprehensively address the denoising problem for
LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for
Retrieval Augmented Generation (DEG-RAG), a framework that addresses these
challenges through: (1) entity resolution, which eliminates redundant entities,
and (2) triple reflection, which removes erroneous relations. Together, these
techniques yield more compact, higher-quality KGs that significantly outperform
their unprocessed counterparts. Beyond the methods, we conduct a systematic
evaluation of entity resolution for LLM-generated KGs, examining different
blocking strategies, embedding choices, similarity metrics, and entity merging
techniques. To the best of our knowledge, this is the first comprehensive
exploration of entity resolution in LLM-generated KGs. Our experiments
demonstrate that this straightforward approach not only drastically reduces
graph size but also consistently improves question answering performance across
diverse popular Graph-based RAG variants.

</details>


### [101] [Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters](https://arxiv.org/abs/2510.14274)
*Lifu Tu,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 通过对数据规模、负采样和多样性的调研，作者发现优化训练策略可使小规模多语言模型实现媲美甚至超越大模型的检索性能。


<details>
  <summary>Details</summary>
Motivation: 虽然小参数量的多语言模型在多语言任务中表现良好，但在检索任务（最常见的使用场景）上表现要落后于大模型。研究问题：能否针对检索任务专门优化小模型以提升性能？

Method: 系统分析和研究多语言嵌入模型训练中的关键因素：训练数据规模、负采样策略和数据多样性，并通过实验探索各因素对检索性能的影响。

Result: 增加训练数据规模初期有帮助，但迅速出现瓶颈；合理加入困难负样本有效提升检索准确率；任务多样性比语言多样性对性能提升更重要。最终训练出约3亿参数的紧凑多语言模型，在检索任务上与主流7B大模型媲美甚至超越。

Conclusion: 小规模多语言嵌入模型可通过优化训练方法在检索任务上达到大模型水准，关键在于合理样本选择和任务多样性。

Abstract: Training effective multilingual embedding models presents unique challenges
due to the diversity of languages and task objectives. Although small
multilingual models (<1 B parameters) perform well on multilingual tasks
generally, they consistently lag behind larger models (>1 B) in the most
prevalent use case: retrieval. This raises a critical question: Can smaller
models be retrofitted specifically for retrieval tasks to enhance their
performance? In this work, we investigate key factors that influence the
effectiveness of multilingual embeddings, focusing on training data scale,
negative sampling strategies, and data diversity. We find that while increasing
the scale of training data yields initial performance gains, these improvements
quickly plateau - indicating diminishing returns. Incorporating hard negatives
proves essential for consistently improving retrieval accuracy. Furthermore,
our analysis reveals that task diversity in the training data contributes more
significantly to performance than language diversity alone. As a result, we
develop a compact (approximately 300M) multilingual model that achieves
retrieval performance comparable to or even surpassing current strong 7B
models.

</details>


### [102] [Qwen3Guard Technical Report](https://arxiv.org/abs/2510.14276)
*Haiquan Zhao,Chenhan Yuan,Fei Huang,Xiaomeng Hu,Yichang Zhang,An Yang,Bowen Yu,Dayiheng Liu,Jingren Zhou,Junyang Lin,Baosong Yang,Chen Cheng,Jialong Tang,Jiandong Jiang,Jianwei Zhang,Jijie Xu,Ming Yan,Minmin Sun,Pei Zhang,Pengjun Xie,Qiaoyu Tang,Qin Zhu,Rong Zhang,Shibin Wu,Shuo Zhang,Tao He,Tianyi Tang,Tingyu Xia,Wei Liao,Weizhou Shen,Wenbiao Yin,Wenmeng Zhou,Wenyuan Yu,Xiaobin Wang,Xiaodong Deng,Xiaodong Xu,Xinyu Zhang,Yang Liu,Yeqiu Li,Yi Zhang,Yong Jiang,Yu Wan,Yuxin Zhou*

Main category: cs.CL

TL;DR: 该论文提出了支持多语种、支持流式生成判定且细粒度的LLM安全防护模型Qwen3Guard，填补了现有守护模型难以实时干预及分类粗糙的短板。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全守护模型存在只支持简化二分类、需完整输出才能审核、不适用于流式应用的问题，无法满足多安全标准、低延迟和高覆盖的实际需求。

Method: 提出Generative Qwen3Guard（以指令方式实现三分类安全判定）与Stream Qwen3Guard（在LLM文本生成过程中实时分词级监测），均支持119种语言和不同模型规模，通过基准测试验证其实时性和准确性。

Result: 本文提出了Qwen3Guard系列多语言安全防护模型，包括两个专用变体：Generative Qwen3Guard（以指令方式实现细粒度三分类：安全、有争议、不安全）和Stream Qwen3Guard（在LLM文本生成过程中实现分词级实时安全监控）。这两种模型分别针对当前二分类判定和只能处理完整输出而无法应用于流式推理的局限，提供了更细致、可扩展和低延迟的安全审核方案。评测显示其在多语种安全判定上达到领先水平。

Conclusion: Qwen3Guard多语言安全模型能够支持全球范围、高效率且更细致的LLM安全审核，提升模型实际应用过程中的输出安全。其开源有望推动业界安全标准进步。

Abstract: As large language models (LLMs) become more capable and widely used, ensuring
the safety of their outputs is increasingly critical. Existing guardrail
models, though useful in static evaluation settings, face two major limitations
in real-world applications: (1) they typically output only binary "safe/unsafe"
labels, which can be interpreted inconsistently across diverse safety policies,
rendering them incapable of accommodating varying safety tolerances across
domains; and (2) they require complete model outputs before performing safety
checks, making them fundamentally incompatible with streaming LLM inference,
thereby preventing timely intervention during generation and increasing
exposure to harmful partial outputs. To address these challenges, we present
Qwen3Guard, a series of multilingual safety guardrail models with two
specialized variants: Generative Qwen3Guard, which casts safety classification
as an instruction-following task to enable fine-grained tri-class judgments
(safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a
token-level classification head for real-time safety monitoring during
incremental text generation. Both variants are available in three sizes (0.6B,
4B, and 8B parameters) and support up to 119 languages and dialects, providing
comprehensive, scalable, and low-latency safety moderation for global LLM
deployments. Evaluated across English, Chinese, and multilingual benchmarks,
Qwen3Guard achieves state-of-the-art performance in both prompt and response
safety classification. All models are released under the Apache 2.0 license for
public use.

</details>


### [103] [PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.14278)
*Md Mahadi Hasan Nahid,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文提出一种三智能体结构的检索系统，优化多跳问答任务的证据检索，能高效提升检索准确率和下游问答性能，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多跳问答任务中，检索相关证据对于回答复杂问题至关重要。现有方法难以同时保证高精度与高召回率。为此，本文提出新方法以提升多跳检索性能。

Method: 设计了一个基于大语言模型的Agentic Retrieval System，结构化地利用三个智能体：Question Analyzer负责分解问题，Selector专注于高精度地选取相关文档，Adder弥补缺失信息以提升召回。Selector与Adder迭代协作生成紧凑且全面的证据集合。

Result: 该方法在四个多跳问答基准（HotpotQA、2WikiMultiHopQA、MuSiQue、MultiHopRAG）上取得了超越现有强基线的检索表现，同时减少了无关内容，提升了下游问答准确率。

Conclusion: 提出的Agentic Retrieval System有效提升了多跳问答中的检索精度与召回，促进下游问答模型性能，且利用更少的无关信息即可获得更高准确率。

Abstract: Retrieval plays a central role in multi-hop question answering (QA), where
answering complex questions requires gathering multiple pieces of evidence. We
introduce an Agentic Retrieval System that leverages large language models
(LLMs) in a structured loop to retrieve relevant evidence with high precision
and recall. Our framework consists of three specialized agents: a Question
Analyzer that decomposes a multi-hop question into sub-questions, a Selector
that identifies the most relevant context for each sub-question (focusing on
precision), and an Adder that brings in any missing evidence (focusing on
recall). The iterative interaction between Selector and Adder yields a compact
yet comprehensive set of supporting passages. In particular, it achieves higher
retrieval accuracy while filtering out distracting content, enabling downstream
QA models to surpass full-context answer accuracy while relying on
significantly less irrelevant information. Experiments on four multi-hop QA
benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --
demonstrates that our approach consistently outperforms strong baselines.

</details>


### [104] [Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL](https://arxiv.org/abs/2510.14296)
*Md Mahadi Hasan Nahid,Davood Rafiei,Weiwei Zhang,Yong Zhang*

Main category: cs.CL

TL;DR: 该论文提出面向Text-to-SQL系统的双向schema检索框架，通过多策略结合显著提升schema linking及SQL生成效果，减少误检并大幅缩小全schema与理想schema间性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法多关注SQL生成，忽视了相关schema元素检索，常导致虚假匹配和查询执行失败。亟需提升schema linking环节以增强系统效果。

Method: 将schema linking视为独立任务，采用表优先和列优先两种检索策略，并结合问题分解、关键词及关键短语提取等技术。

Result: 在BIRD和Spider等基准上，显著提升了schema召回率，降低误检率，SQL生成效果优于全schema基线，接近oracle性能，无需进一步查询优化。同时，方法将全schema与完美schema表现差距缩小了50%。

Conclusion: 提出的双向schema检索框架能有效提升Text-to-SQL系统中schema linking的精度和效率，显著缩小全schema与理想schema设置下的性能差距。

Abstract: Schema linking -- the process of aligning natural language questions with
database schema elements -- is a critical yet underexplored component of
Text-to-SQL systems. While recent methods have focused primarily on improving
SQL generation, they often neglect the retrieval of relevant schema elements,
which can lead to hallucinations and execution failures. In this work, we
propose a context-aware bidirectional schema retrieval framework that treats
schema linking as a standalone problem. Our approach combines two complementary
strategies: table-first retrieval followed by column selection, and
column-first retrieval followed by table selection. It is further augmented
with techniques such as question decomposition, keyword extraction, and
keyphrase extraction. Through comprehensive evaluations on challenging
benchmarks such as BIRD and Spider, we demonstrate that our method
significantly improves schema recall while reducing false positives. Moreover,
SQL generation using our retrieved schema consistently outperforms full-schema
baselines and closely approaches oracle performance, all without requiring
query refinement. Notably, our method narrows the performance gap between full
and perfect schema settings by 50\%. Our findings highlight schema linking as a
powerful lever for enhancing Text-to-SQL accuracy and efficiency.

</details>


### [105] [Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers](https://arxiv.org/abs/2510.14303)
*Ziye Xia,Sergei S. Ospichev*

Main category: cs.CL

TL;DR: 本文利用OpenAlex知识图谱分析8000篇论文关系，提出基于小型语言模型和提示工程的新方法识别论文关键概念和创新点，并显著提升了分析准确性。模型已开源发布，推动学术文献分析自动化发展。


<details>
  <summary>Details</summary>
Motivation: 近年来学术出版物数量激增，研究人员难以及时全面跟踪最新研究进展和方法。现有文献分析工具多停留于概念相似度匹配和基础分类，未能深入挖掘概念之间的关系网络。

Method: 本研究依托OpenAlex开源知识图谱，分析约8000篇来自新西伯利亚国立大学的开源论文。提出了基于提示工程的关键概念路径分析方法，运用小型语言模型提取关键概念及识别创新点，并通过知识图谱约束机制构建智能体提升分析准确性。对Qwen和DeepSeek模型进行微调并公开发布。

Result: 发现论文关键概念路径的分布模式与创新点和稀有路径之间存在强相关性。微调后的模型在关键概念抽取与创新点识别方面准确率显著提升。

Conclusion: 结合开源知识图谱和小型语言模型，通过提示工程和知识图谱约束机制，有效提高学术论文关键概念及创新点的自动分析准确性。所提方法为科学文献分析提供了新的技术路径。

Abstract: In recent years, the rapid increase in academic publications across various
fields has posed severe challenges for academic paper analysis: scientists
struggle to timely and comprehensively track the latest research findings and
methodologies. Key concept extraction has proven to be an effective analytical
paradigm, and its automation has been achieved with the widespread application
of language models in industrial and scientific domains. However, existing
paper databases are mostly limited to similarity matching and basic
classification of key concepts, failing to deeply explore the relational
networks between concepts. This paper is based on the OpenAlex opensource
knowledge graph. By analyzing nearly 8,000 open-source paper data from
Novosibirsk State University, we discovered a strong correlation between the
distribution patterns of paper key concept paths and both innovation points and
rare paths. We propose a prompt engineering-based key concept path analysis
method. This method leverages small language models to achieve precise key
concept extraction and innovation point identification, and constructs an agent
based on a knowledge graph constraint mechanism to enhance analysis accuracy.
Through fine-tuning of the Qwen and DeepSeek models, we achieved significant
improvements in accuracy, with the models publicly available on the Hugging
Face platform.

</details>


### [106] [MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning](https://arxiv.org/abs/2510.14305)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Tasnim Mohiuddin,Md Mofijul Islam,Swakkhar Shatabda*

Main category: cs.CL

TL;DR: 本文提出了多语言数学推理基准MathMist，系统评估了多类型LLMs在不同语言环境下的数学推理能力，发现低资源语言下模型表现较差，为推进多语言数学推理研究提供了新工具和数据。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在数学推理领域面临巨大挑战，尤其是在多语言环境下缺乏系统性评估。现有基准测试主要集中于英语或少数高资源语言，对多语言和跨语言数学推理能力评估存在明显空白。

Method: 提出了MathMist，多语言数学问题解决和推理的并行基准，包含超过21,000个对齐的问答对，覆盖七种语言，涵盖高、中、低资源语言。系统性评估多类模型（开源及专有LLMs、多语言推理模型）在零样本、链式思维（CoT）和代码混合推理范式下的表现。

Result: 实验表明，无论模型类型如何，LLMs在多语言数学推理中仍存在持续缺陷，尤其是在低资源语言下表现明显下降，难以做到一致性和可解释的数学推理。

Conclusion: LLMs在多语言数学推理领域的能力仍有较大提升空间，尤其在低资源语言环境下亟需改进。MathMist为后续改进提供了新的评估基准和数据资源。

Abstract: Mathematical reasoning remains one of the most challenging domains for large
language models (LLMs), requiring not only linguistic understanding but also
structured logical deduction and numerical precision. While recent LLMs
demonstrate strong general-purpose reasoning abilities, their mathematical
competence across diverse languages remains underexplored. Existing benchmarks
primarily focus on English or a narrow subset of high-resource languages,
leaving significant gaps in assessing multilingual and cross-lingual
mathematical reasoning. To address this, we introduce MathMist, a parallel
multilingual benchmark for mathematical problem solving and reasoning. MathMist
encompasses over 21K aligned question-answer pairs across seven languages,
representing a balanced coverage of high-, medium-, and low-resource linguistic
settings. The dataset captures linguistic variety, multiple types of problem
settings, and solution synthesizing capabilities. We systematically evaluate a
diverse suite of models, including open-source small and medium LLMs,
proprietary systems, and multilingual-reasoning-focused models, under
zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our
results reveal persistent deficiencies in LLMs' ability to perform consistent
and interpretable mathematical reasoning across languages, with pronounced
degradation in low-resource settings. All the codes and data are available at
GitHub: https://github.com/mahbubhimel/MathMist

</details>


### [107] [MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking](https://arxiv.org/abs/2510.14307)
*Sathyanarayanan Ramamoorthy,Vishwa Shah,Simran Khanuja,Zaid Sheikh,Shan Jie,Ann Chia,Shearman Chua,Graham Neubig*

Main category: cs.CL

TL;DR: 该论文构建了一个多语言（五种语言）新闻标题与图片的实体链接数据集MERLIN，并结合多种语言模型验证视觉信息对实体链接准确率的提升作用。


<details>
  <summary>Details</summary>
Motivation: 在多语言、多模态环境下进行实体链接具有挑战性，尤其是在文本上下文不明确或不足的情况下。现有方法往往只关注单语言或单模态，难以覆盖实际应用中的复杂场景。为此，作者希望构建一个新的体系和数据集，推动多语言、多模态实体链接的研究和评测。

Method: 提出了MERLIN测试平台，收集并整理了BBC新闻标题及其相关图片，涵盖五种语言（印地语、日语、印尼语、越南语、泰米尔语），共含7,000多实体提及，关联到2,500个唯一Wikidata实体。同时，作者基于不同深度学习语言模型（如LLaMA-2、Aya-23）设计了多语言、多模态实体链接的多种基线方法进行比较。

Result: 实验结果表明，视觉模态信息能显著提升实体链接准确率，尤其在文本上下文不清或不足的情况下，对多语言能力较弱的模型提升尤为明显。

Conclusion: MERLIN为多语言、多模态实体链接提供了首个大规模数据集与测试平台。将视觉信息融入传统的实体链接系统，可显著提升在实际复杂场景下的表现，尤其对文本信息有限或语言资源稀缺时效果更佳。

Abstract: This paper introduces MERLIN, a novel testbed system for the task of
Multilingual Multimodal Entity Linking. The created dataset includes BBC news
article titles, paired with corresponding images, in five languages: Hindi,
Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity
mentions linked to 2,500 unique Wikidata entities. We also include several
benchmarks using multilingual and multimodal entity linking methods exploring
different language models like LLaMa-2 and Aya-23. Our findings indicate that
incorporating visual data improves the accuracy of entity linking, especially
for entities where the textual context is ambiguous or insufficient, and
particularly for models that do not have strong multilingual abilities. For the
work, the dataset, methods are available here at
https://github.com/rsathya4802/merlin

</details>


### [108] [Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL](https://arxiv.org/abs/2510.14318)
*Marwa Abdulhai,Ryan Cheng,Aryansh Shrivastava,Natasha Jaques,Yarin Gal,Sergey Levine*

Main category: cs.CL

TL;DR: 论文提出新的欺骗度量指标并用多轮强化学习微调LLM，有效减少模型对话中的欺骗行为，指标与人工判别更加一致，多轮分析比单句检测更为可靠。


<details>
  <summary>Details</summary>
Motivation: LLM在真实应用中容易产生误导、操纵或欺骗性内容，带来安全隐患。当前安全保障不足且行为难以预测，需要更精细有效的欺骗检测与矫正方法。

Method: 提出了“信念不一致性”指标衡量LLM的欺骗行为，并在四种对话场景下，结合五个现有欺骗检测指标与新指标进行评估。对八个主流模型进行基准测试，同时采用多轮强化学习（RL）来微调LLM，以降低欺骗行为。

Result: 新指标与人类判断更接近。实测八个模型在正常对话中约26%回复具有欺骗性，RLHF训练模型也有43%欺骗率。采用多轮强化学习微调，欺骗行为减少了77.6%。

Conclusion: 对话中的欺骗行为需多轮分析和方法改进才能有效评估和减少，单轮分析不足以捕捉欺骗行为发生的全过程。所提出的多轮强化学习微调方法，能显著减少LLM的欺骗行为。

Abstract: Large Language Models (LLMs) interact with millions of people worldwide in
applications such as customer support, education and healthcare. However, their
ability to produce deceptive outputs, whether intentionally or inadvertently,
poses significant safety concerns. The unpredictable nature of LLM behavior,
combined with insufficient safeguards against hallucination, misinformation,
and user manipulation, makes their misuse a serious, real-world risk. In this
paper, we investigate the extent to which LLMs engage in deception within
dialogue, and propose the belief misalignment metric to quantify deception. We
evaluate deception across four distinct dialogue scenarios, using five
established deception detection metrics and our proposed metric. Our findings
reveal this novel deception measure correlates more closely with human
judgments than any existing metrics we test. Additionally, our benchmarking of
eight state-of-the-art models indicates that LLMs naturally exhibit deceptive
behavior in approximately 26% of dialogue turns, even when prompted with
seemingly benign objectives. When prompted to deceive, LLMs are capable of
increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly,
models trained with RLHF, the predominant approach for ensuring the safety of
widely-deployed LLMs, still exhibit deception at a rate of 43% on average.
Given that deception in dialogue is a behavior that develops over an
interaction history, its effective evaluation and mitigation necessitates
moving beyond single-utterance analyses. We introduce a multi-turn
reinforcement learning methodology to fine-tune LLMs to reduce deceptive
behaviors, leading to a 77.6% reduction compared to other instruction-tuned
models.

</details>


### [109] [A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2510.14332)
*Yangyang Li*

Main category: cs.CL

TL;DR: 通过混合词嵌入与优化机器学习流程，实现了阿尔茨海默症早期诊断的高准确率（91%）和高AUC（97%），优于现有NLP模型，实验结果稳定，具备临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默症（AD）早期发现有助于患者的早期治疗和减轻医疗负担。语言能力变化是AD早期的重要表现，因此可以用于早期诊断。

Method: 开发了一种鲁棒的分类方法，结合Doc2Vec和ELMo的混合词嵌入，并对超参数进行微调。通过词嵌入生成困惑度得分，结合句子的语法和语义分析，将特征向量输入到逻辑回归模型，并在整个流程中微调超参数，如模型正则化参数、学习率、词向量维度等。

Result: 模型对AD早期与健康人群分类准确率达91%，AUC达97%，优于现有最佳NLP模型（88%准确率），且多次实验稳定（准确率标准差0.0403，AUC标准差0.0174）。

Conclusion: 提出的方法准确且稳定，可用于AD的大规模筛查及临床辅助诊断。

Abstract: Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD
patients, leading to early treatments that lessen symptoms and alleviating
financial burden of health care. As one of the leading signs of AD, language
capability changes can be used for early diagnosis of AD. In this paper, I
develop a robust classification method using hybrid word embedding and
fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early
detection of AD. Specifically, we create a hybrid word embedding based on word
vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The
scores identify whether a sentence is fluent or not and capture semantic
context of the sentences. I enrich the word embedding by adding linguistic
features to analyze syntax and semantics. Further, we input an embedded feature
vector into logistic regression and fine tune hyperparameters throughout the
pipeline. By tuning hyperparameters of the machine learning pipeline (e.g.,
model regularization parameter, learning rate and vector size of Doc2Vec, and
vector size of ELMo), I achieve 91% classification accuracy and an Area Under
the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based
on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best
existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the
model stability through repeated experiments and find that the model is stable
even though the training data is split randomly (standard deviation of accuracy
= 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method
is accurate and stable. This model can be used as a large-scale screening
method for AD, as well as a complementary examination for doctors to detect AD.

</details>


### [110] [Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts](https://arxiv.org/abs/2510.14351)
*Perapard Ngokpol,Kun Kerdthaisong,Pasin Buakhaw,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: 本文提出Beyond One World基准，系统评估LLMs在30位超级英雄的90个不同版本上的角色扮演能力。研究显示：模型难以跨版本保持角色一致性，在推理与决策上常常无法兼顾，连锁思维提升弱模型表现但影响强模型准确性。Think-Act Matching指标可量化模型可信度，该基准为未来角色扮演类LLM发展提供了挑战性测试框架。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多版本角色扮演的能力未被充分探索，尤其在角色设定、故事背景及道德价值多样化的情况下，其一致性、准确性和推理能力值得深入研究。

Method: 构建了Beyond One World基准，涵盖30位超级英雄及90个不同版本，提出两项任务：角色关键事件回忆与伦理困境推理，并设计了Think-Act Matching指标量化模型的理由与行动一致性。通过对各种类型大模型进行实验，以连锁思维提示和不同推理能力模型分别对任务表现进行评估。

Result: 1. 连锁思维提示提升弱模型叙事连贯性但可能降低强模型设定准确率；2. 同一角色的版本泛化仍是主要挑战；3. 现有模型多只能‘思考’或‘行动’，很少能兼顾。提出的基准显著暴露了模型在多宇宙角色一致性和推理对齐的关键短板。

Conclusion: Beyond One World基准揭示了大型语言模型在多版本角色扮演一致性和推理对齐方面的显著不足。现有模型往往难以在不同角色版本间保持一致，并在推理与决策方面存在分离。该基准为角色扮演LLMs的评估提供了强有力的挑战。

Abstract: Large language models (LLMs) are increasingly used as role-playing agents,
yet their capacity to faithfully and consistently portray version-specific
characters -- for example, superheroes across comic and cinematic universes --
remains underexplored. Superhero canons such as Marvel and DC provide a rich
testbed: decades of storytelling yield multiple incarnations of the same
character with distinct histories, values, and moral codes. To study this
problem, we introduce Beyond One World, a benchmark for character-grounded
roleplay spanning 30 iconic heroes and 90 canon-specific versions. The
benchmark comprises two tasks: (i) Canon Events, which probes factual recall of
pivotal life stages, and (ii) Moral Dilemmas, which confronts models with
ethically charged scenarios. We score responses for canonical accuracy and
reasoning fidelity under a framework that separates internal deliberation
("thinking") from outward decisions ("acting"). We further propose Think-Act
Matching, a metric that quantifies alignment between reasons and actions and
serves as a proxy for model trustworthiness. Experiments across reasoning- and
non-reasoning-oriented models yield three findings: (1) chain-of-thought
prompting improves narrative coherence in weaker models but can reduce
canonical accuracy in stronger ones; (2) cross-version generalization within a
character remains a major obstacle; and (3) models often excel at either
thinking or acting, but rarely both. Beyond One World exposes critical gaps in
multiversal consistency and reasoning alignment, offering a challenging
evaluation for role-playing LLMs.

</details>


### [111] [CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering](https://arxiv.org/abs/2510.14353)
*Ziad Elshaer,Essam A. Rashed*

Main category: cs.CL

TL;DR: 论文提出了一种基于置信度驱动的多模型协作医学问答方法，通过合理分流问题显著提升效果，在多个医学数据集上取得亮眼成绩，为资源有限环境下医学AI应用提供了省算力高性能途径。


<details>
  <summary>Details</summary>
Motivation: 当前医学大语言模型的高性能通常依赖于大量的微调和算力资源，这限制了资源受限医疗机构的使用。因此，作者希望提出一种无需微调且资源消耗低、依赖模型多样性的医学问答方法。

Method: 提出了一个基于置信度驱动的多模型框架。框架包括两阶段架构：先由置信度检测模块评估主模型对问题的把握程度，对于置信度低的问题通过自适应路由转交给具有互补知识的辅助模型进行协同推理。

Result: 在MedQA、MedMCQA和PubMedQA三个医学基准上，分别使用Qwen3-30B-A3B-Instruct、Phi-4 14B、Gemma 2 12B进行评测。在PubMedQA上获得95.0%的高准确率，在MedMCQA上获得78.0%，并通过消融实验证明置信度感知路由和多模型协作远超单模型和统一推理策略。

Conclusion: 置信度驱动的多模型协作框架能在无需微调的前提下有效提升医学AI表现，特别适合资源有限的医疗机构，实现了对高性能医学AI的可及性提升。

Abstract: High-performing medical Large Language Models (LLMs) typically require
extensive fine-tuning with substantial computational resources, limiting
accessibility for resource-constrained healthcare institutions. This study
introduces a confidence-driven multi-model framework that leverages model
diversity to enhance medical question answering without fine-tuning. Our
framework employs a two-stage architecture: a confidence detection module
assesses the primary model's certainty, and an adaptive routing mechanism
directs low-confidence queries to Helper models with complementary knowledge
for collaborative reasoning. We evaluate our approach using
Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical
benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework
achieves competitive performance, with particularly strong results in PubMedQA
(95.0\%) and MedMCQA (78.0\%). Ablation studies confirm that confidence-aware
routing combined with multi-model collaboration substantially outperforms
single-model approaches and uniform reasoning strategies. This work establishes
that strategic model collaboration offers a practical, computationally
efficient pathway to improve medical AI systems, with significant implications
for democratizing access to advanced medical AI in resource-limited settings.

</details>


### [112] [On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?](https://arxiv.org/abs/2510.14365)
*Anyun Zhuo,Xuefei Ning,Ningyuan Li,Yu Wang,Pinyan Lu*

Main category: cs.CL

TL;DR: 本文研究通过插入不可见字符制造扰动的手段来防止LLM被滥用，发现许多LLM即便在强噪声下依然表现优异，揭示了其强大的低层健壮性。


<details>
  <summary>Details</summary>
Motivation: 研究背景是大语言模型在现实应用中可能被滥用（如在线考试作弊），需要开发能降低模型滥用风险的方法，并了解模型在被结构化干扰下的表现和机制。

Method: 提出了一种名为\nameshort{}的方法，通过在文本每个字符间插入不可见Unicode控制字符制造噪声，并系统性地评估不同模型和扰动配置下LLM的表现，探讨其字符级分词处理和显式/隐式去噪机制。

Result: 即使面对极强的字符级噪声扰动，许多LLM依然能取得较好结果，表现出对这类扰动的显著鲁棒性。论文还分析了模型能够维持性能的内在机制。

Conclusion: 许多现代大语言模型（LLM）在面临字符级、结构化噪声扰动时依然能保持显著性能，这揭示了其具有较高的低层健壮性，但也可能引发对其被滥用的担忧。

Abstract: This work investigates the resilience of contemporary LLMs against frequent
and structured character-level perturbations, specifically through the
insertion of noisy characters after each input character. We introduce
\nameshort{}, a practical method that inserts invisible Unicode control
characters into text to discourage LLM misuse in scenarios such as online exam
systems. Surprisingly, despite strong obfuscation that fragments tokenization
and reduces the signal-to-noise ratio significantly, many LLMs still maintain
notable performance. Through comprehensive evaluation across model-, problem-,
and noise-related configurations, we examine the extent and mechanisms of this
robustness, exploring both the handling of character-level tokenization and
\textit{implicit} versus \textit{explicit} denoising mechanism hypotheses of
character-level noises. We hope our findings on the low-level robustness of
LLMs will shed light on the risks of their misuse and on the reliability of
deploying LLMs across diverse applications.

</details>


### [113] [From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program](https://arxiv.org/abs/2510.14369)
*Joseph E. Trujillo-Falcon,Monica L. Bozeman,Liam E. Llewellyn,Samuel T. Halvorson,Meryl Mizell,Stuti Deshpande,Bob Manning,Todd Fagin*

Main category: cs.CL

TL;DR: NWS推动AI自动翻译工具，携手LILT实现气象信息高质量多语种翻译，用GIS精准识别重点服务区域，严格遵循伦理AI原则，已上线多语种气象产品网站，显著提高美国非英语人群的天气预警服务覆盖。


<details>
  <summary>Details</summary>
Motivation: 美国有近6900万居民在家不使用英语，气象服务普遍面临语言障碍，急需高效自动化的多语言翻译系统以提升气象预警的覆盖与有效性。

Method: NWS与LILT合作，通过其专利语言模型训练流程，将神经机器翻译（NMT）工具适配至气象术语与信息。使用地理信息系统（GIS）映射美国各地区语言需求，以优化资源分配及产品推送。此外，项目全过程严格融入伦理AI原则。

Result: 目前系统已开发出西班牙语、简体中文、越南语等主要非英语语言版本，大幅减少手动翻译时间，减轻运营压力，并已推出包含预警、天气预报和科普活动在内的多语言网站原型。

Conclusion: 通过开发基于人工智能的自动翻译系统，NWS能够在多语言环境中更高效地为美国68.8百万非英语家庭提供气象服务，并显著提升预警信息的精准覆盖率。

Abstract: To advance a Weather-Ready Nation, the National Weather Service (NWS) is
developing a systematic translation program to better serve the 68.8 million
people in the U.S. who do not speak English at home. This article outlines the
foundation of an automated translation tool for NWS products, powered by
artificial intelligence. The NWS has partnered with LILT, whose patented
training process enables large language models (LLMs) to adapt neural machine
translation (NMT) tools for weather terminology and messaging. Designed for
scalability across Weather Forecast Offices (WFOs) and National Centers, the
system is currently being developed in Spanish, Simplified Chinese, Vietnamese,
and other widely spoken non-English languages. Rooted in best practices for
multilingual risk communication, the system provides accurate, timely, and
culturally relevant translations, significantly reducing manual translation
time and easing operational workloads across the NWS. To guide the distribution
of these products, GIS mapping was used to identify language needs across
different NWS regions, helping prioritize resources for the communities that
need them most. We also integrated ethical AI practices throughout the
program's design, ensuring that transparency, fairness, and human oversight
guide how automated translations are created, evaluated, and shared with the
public. This work has culminated into a website featuring experimental
multilingual NWS products, including translated warnings, 7-day forecasts, and
educational campaigns, bringing the country one step closer to a national
warning system that reaches all Americans.

</details>


### [114] [PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora](https://arxiv.org/abs/2510.14377)
*Mykolas Sveistrys,Richard Kunert*

Main category: cs.CL

TL;DR: 本文提出了面对高重复、干扰文档查询挑战的新数据集和问答方法，并通过分解子任务与早期过滤，显著提高了QA系统在真实复杂报告类任务下的表现。


<details>
  <summary>Details</summary>
Motivation: 现实中的报告数据问答需跨文档全面聚合，传统RAG、单/多跳方法难以处理高召回与干扰文档场景，需探索新型 QA 模型及测试集以更真实地反映应用需求与挑战。

Method: 提出了PluriHopWIND多语言数据集与PluriHopRAG新型RAG架构，具体采用：1. 将问题拆解为每份文档的子问题；2. 利用交叉编码器先过滤无关文档，再让LLM深度推理。并与传统RAG、基于图和多模态变体做了实验对比。

Result: PluriHopWIND更贴合实际场景，干扰文档密度高。实验发现所有现有方法的F1均未超过40%；PluriHopRAG带来相对F1提升18-52%。体现了现有技术欠缺及新方法的有效性。

Conclusion: PluriHopWIND数据集揭示了当前RAG等问答系统在面对高重复性、多干扰文档的现实语料时的不足。PluriHopRAG模型通过对每份文档进行逐一检查与过滤，显著提升了问答准确性，说明了详尽检索和早期过滤的有效性。

Abstract: Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.

</details>


### [115] [Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis](https://arxiv.org/abs/2510.14395)
*Jun Li,Qun Zhao*

Main category: cs.CL

TL;DR: 本研究构建并标注了包含Reddit用户历史帖子和评论的高质量数据集，实验证明评论树信息能显著提高自杀风险检测精度，对早期干预有重要意义。


<details>
  <summary>Details</summary>
Motivation: 以往研究多聚焦于单条社交媒体内容，忽略了用户随时间变化的帖子与评论交互，未能充分把握用户自杀风险的动态演化。本研究旨在填补这一空白，探索时序评论树信息对自杀风险识别和预测的影响。

Method: 本研究通过构建高质量标注数据集，将Reddit用户的历史帖子和评论按哥伦比亚自杀严重性量表（C-SSRS）进行四分类标注，并运用统计分析及大型语言模型（LLMs）进行实验，验证评论树信息对用户自杀风险判别与预测的提升作用。

Result: 实验及统计分析表明，融入评论树数据显著提升了用户自杀风险水平的判别与预测准确性。

Conclusion: 通过综合分析用户的历史发表及评论互动信息，可有效提升自杀风险检测效果，为早期干预提供坚实的数据与算法基础。

Abstract: Suicide remains a critical global public health issue. While previous studies
have provided valuable insights into detecting suicidal expressions in
individual social media posts, limited attention has been paid to the analysis
of longitudinal, sequential comment trees for predicting a user's evolving
suicidal risk. Users, however, often reveal their intentions through historical
posts and interactive comments over time. This study addresses this gap by
investigating how the information in comment trees affects both the
discrimination and prediction of users' suicidal risk levels. We constructed a
high-quality annotated dataset, sourced from Reddit, which incorporates users'
posting history and comments, using a refined four-label annotation framework
based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical
analysis of the dataset, along with experimental results from Large Language
Models (LLMs) experiments, demonstrates that incorporating comment trees data
significantly enhances the discrimination and prediction of user suicidal risk
levels. This research offers a novel insight to enhancing the detection
accuracy of at-risk individuals, thereby providing a valuable foundation for
early suicide intervention strategies.

</details>


### [116] [Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation](https://arxiv.org/abs/2510.14398)
*Shiyao Ding,Takayuki Ito*

Main category: cs.CL

TL;DR: 该论文提出通过模拟人机对话采集用户真实表达方式，并创建了多语种数据集，建立了个性化语言生成的新基准任务，有助于促进用户对齐的语言模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在预测下一词时表现突出，但难以生成真正符合个体交流风格的内容，如回复邮件或社交消息，且真实通信数据因隐私难以收集。因此提出通过可控人工采集精确捕捉用户词选择的新任务。

Method: 设计受控的人机对话，每位用户与基于MBTI维度的NPC交流，采集用户五天的自然沟通数据，并用多种个性化建模方法（提示增强、微调）进行评测。

Result: 构建了涵盖英、日、中三语种、100个对话会话的多语种数据集，验证并比较了基于提示和微调的个性化建模方法，提升了对个体化语言建模的理解。

Conclusion: 建立了首个面向个性化响应的基准任务和数据集，为用户对齐语言模型奠定了基础。

Abstract: Large language models (LLMs) excel at general next-token prediction but still
struggle to generate responses that reflect how individuals truly communicate,
such as replying to emails or social messages in their own style. However, real
SNS or email histories are difficult to collect due to privacy concerns. To
address this, we propose the task of "Your Next Token Prediction (YNTP)", which
models a user's precise word choices through controlled human-agent
conversations. We build a multilingual benchmark of 100 dialogue sessions
across English, Japanese, and Chinese, where users interact for five days with
psychologically grounded NPCs based on MBTI dimensions. This setup captures
natural, daily-life communication patterns and enables analysis of users'
internal models. We evaluate prompt-based and fine-tuning-based personalization
methods, establishing the first benchmark for YNTP and a foundation for
user-aligned language modeling. The dataset is available at:
https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100

</details>


### [117] [MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering](https://arxiv.org/abs/2510.14400)
*Yingpeng Ning,Yuanyuan Sun,Ling Luo,Yanhua Wang,Yuchen Pan,Hongfei Lin*

Main category: cs.CL

TL;DR: 该研究提出一种通过迭代检索、验证和结合结构化引文的医学问答框架，有效提升医学问答系统准确性和可信度，对主流大模型有显著改进效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的医学问答系统虽能结合外部医学文献，但因检索后的文献存在噪音且证据不足时缺少有效验证，导致生成内容经常出现幻觉，影响医用问答系统可信度和可靠性。

Method: 该方法包含三大创新：1）要求所有生成内容都必须有可检索医学文献作为支撑，证据不足时给出结构化否定性知识断言；2）采用迭代检索与证据验证流程，持续通过医学空白分析优化查询并保证证据充足可靠；3）集成MedTrust-Align模块，通过偏好优化结合正负样本反馈，加强基于引文的推理，并惩罚易产生幻觉的生成模式。

Result: 实验在MedMCQA、MedQA及MMLU-Med等数据集上，方法在各主流模型结构下均优于现有基线，LLaMA3.1-8B-Instruct平均准确率提升2.7%，Qwen3-8B提升2.4%。

Conclusion: 本文提出的MedTrust-Guided Iterative RAG框架能显著提升医学问答的事实一致性，并减少大语言模型在医学QA中的幻觉问题。在多个数据集和模型架构上，相较现有方法均取得更高准确率。

Abstract: Biomedical question answering (QA) requires accurate interpretation of
complex medical knowledge. Large language models (LLMs) have shown promising
capabilities in this domain, with retrieval-augmented generation (RAG) systems
enhancing performance by incorporating external medical literature. However,
RAG-based approaches in biomedical QA suffer from hallucinations due to
post-retrieval noise and insufficient verification of retrieved evidence,
undermining response reliability. We propose MedTrust-Guided Iterative RAG, a
framework designed to enhance factual consistency and mitigate hallucinations
in medical QA. Our method introduces three key innovations. First, it enforces
citation-aware reasoning by requiring all generated content to be explicitly
grounded in retrieved medical documents, with structured Negative Knowledge
Assertions used when evidence is insufficient. Second, it employs an iterative
retrieval-verification process, where a verification agent assesses evidence
adequacy and refines queries through Medical Gap Analysis until reliable
information is obtained. Third, it integrates the MedTrust-Align Module (MTAM)
that combines verified positive examples with hallucination-aware negative
samples, leveraging Direct Preference Optimization to reinforce
citation-grounded reasoning while penalizing hallucination-prone response
patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our
approach consistently outperforms competitive baselines across multiple model
architectures, achieving the best average accuracy with gains of 2.7% for
LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.

</details>


### [118] [Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2510.14420)
*Qingyu Ren,Qianyu He,Bowei Zhang,Jie Zeng,Jiaqing Liang,Yanghua Xiao,Weikang Zhou,Zeye Sun,Fei Yu*

Main category: cs.CL

TL;DR: 本文提出了一种无需外部监督的自监督强化学习方法，通过约束分解和伪标签奖励训练，大幅提升了语言模型在多约束任务下的表现，并且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在执行多约束指令时经常表现不佳，尤其是在现实任务中。主流的强化学习方法依赖外部监督，且多约束任务奖励信号稀疏，限制了模型性能。本文旨在解决这一困境。

Method: 提出了一种无标签、自监督的强化学习框架。该框架不依赖外部监督，而是直接从指令中推导奖励信号，并生成伪标签以训练奖励模型。方法还设计了约束分解策略和高效的约束二分类机制，以解决奖励稀疏问题并维持计算效率。

Result: 所提方法在3个领域内和5个领域外数据集上表现出色，涵盖了复杂的代理式和多轮任务，均取得了显著提升，展现了很强的泛化能力。

Conclusion: 无标签自监督强化学习框架能够有效提升语言模型在复杂多约束指令任务中的表现，方法高效并具有较好的泛化性。

Abstract: Language models often struggle to follow multi-constraint instructions that
are crucial for real-world applications. Existing reinforcement learning (RL)
approaches suffer from dependency on external supervision and sparse reward
signals from multi-constraint tasks. We propose a label-free self-supervised RL
framework that eliminates dependency on external supervision by deriving reward
signals directly from instructions and generating pseudo-labels for reward
model training. Our approach introduces constraint decomposition strategies and
efficient constraint-wise binary classification to address sparse reward
challenges while maintaining computational efficiency. Experiments show that
our approach generalizes well, achieving strong improvements across 3 in-domain
and 5 out-of-domain datasets, including challenging agentic and multi-turn
instruction following. The data and code are publicly available at
https://github.com/Rainier-rq/verl-if

</details>


### [119] [Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents](https://arxiv.org/abs/2510.14438)
*Rui Wang,Ce Zhang,Jun-Yu Ma,Jianshu Zhang,Hongru Wang,Yi Chen,Boyang Xue,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 论文提出Web智能体“探索-进化”训练范式，打造了信息聚合方向的大规模数据集和强力模型（WebAggregator），显著提升聚合能力，填补了现有模型在深入研究应用上的重要空白。


<details>
  <summary>Details</summary>
Motivation: 现有的开源深度研究Web智能体多集中于信息查找能力，缺乏对信息聚合能力的重视，这影响了其支持深入研究的能力。为解决这一问题，提出了一种新的范式以提升Web智能体的信息聚合能力。

Method: 提出“Explore to Evolve”范式：智能体先主动探索真实网页收集基础信息，然后通过自主进化并聚合12种高阶逻辑操作，生成可验证的QA对，最终构建WebAggregatorQA数据集。基于SmolAgents框架收集监督微调轨迹，开发WebAggregator系列基础模型。

Result: WebAggregator-8B模型性能与GPT-4.1相当，32B模型性能超过GPT-4.1 10%以上，且接近Claude-3.7-sonnet。为评测信息聚合能力，构建WebAggregatorQA评估集，Claude-3.7-sonnet得分28%，GPT-4.1得分25.8%，展现现有模型在信息聚合上的不足。

Conclusion: 尽管当前主流大模型能检索信息，但在聚合能力上表现较弱。提出的新范式和模型为Web智能体信息聚合提供了具挑战性的评测基准和有效提升路径。

Abstract: Deep research web agents not only retrieve information from diverse sources
such as web environments, files, and multimodal inputs, but more importantly,
they need to rigorously analyze and aggregate knowledge for insightful
research. However, existing open-source deep research agents predominantly
focus on enhancing information-seeking capabilities of web agents to locate
specific information, while overlooking the essential need for information
aggregation, which would limit their ability to support in-depth research. We
propose an Explore to Evolve paradigm to scalably construct verifiable training
data for web agents. Begins with proactive online exploration, an agent sources
grounded information by exploring the real web. Using the collected evidence,
the agent then self-evolves an aggregation program by selecting, composing, and
refining operations from 12 high-level logical types to synthesize a verifiable
QA pair. This evolution from high-level guidance to concrete operations allowed
us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K
websites and 11 domains. Based on an open-source agent framework, SmolAgents,
we collect supervised fine-tuning trajectories to develop a series of
foundation models, WebAggregator. WebAggregator-8B matches the performance of
GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text
and closely approaches Claude-3.7-sonnet. Moreover, given the limited
availability of benchmarks that evaluate web agents' information aggregation
abilities, we construct a human-annotated evaluation split of WebAggregatorQA
as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves
28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all
references, they still struggle on WebAggregatorQA, highlighting the need to
strengthen the information aggregation capabilities of web agent foundations.

</details>


### [120] [Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents](https://arxiv.org/abs/2510.14453)
*Reid T. Johnson,Michelle D. Pain,Jordan D. West*

Main category: cs.CL

TL;DR: NLT框架用自然语言替代JSON进行工具调用，提升了准确率和稳定性，尤其利于开源模型，并拓展了更多模型的工具调用能力。


<details>
  <summary>Details</summary>
Motivation: 目前大模型调用工具多依赖编程式JSON格式，存在任务干扰和格式限制，影响了工具调用效果。

Method: 提出NLT框架，将工具调用从程序化JSON切换为自然语言输出，实现工具选择与响应生成解耦。

Result: 在客服和心理健康领域的6400次实验中，NLT使工具调用准确率提升18.4个百分点，输出方差降低70%。开源模型提升最显著，甚至超过闭源旗舰模型。这些改进在不同提示下依然成立，也让不原生支持工具调用的模型具备该能力。

Conclusion: NLT显著改善了大模型的工具调用性能，拓展了模型适用范围，并对后续的模型训练阶段（强化学习、监督微调）有重要作用。

Abstract: We present Natural Language Tools (NLT), a framework that replaces
programmatic JSON tool calling in large language models (LLMs) with natural
language outputs. By decoupling tool selection from response generation, NLT
eliminates task interference and format constraints that degrade tool call
performance. When evaluated across 10 models and 6,400 trials spanning customer
service and mental health domains, NLT improves tool calling accuracy by 18.4
percentage points while reducing output variance by 70%. Open-weight models see
the largest gains, surpassing flagship closed-weight alternatives, with
implications for model training in both reinforcement learning and supervised
fine-tuning stages. These improvements persist under prompt perturbations and
extend tool-calling capabilities to models lacking native support.

</details>


### [121] [LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models](https://arxiv.org/abs/2510.14466)
*Haolin Li,Haipeng Zhang,Mang Li,Yaohua Wang,Lijie Wen,Yu Zhang,Biqing Huang*

Main category: cs.CL

TL;DR: 本论文提出LiRA框架，通过锚定低资源语言到英语语义空间并强化跨语言推理，实现了大语言模型在多语言低资源环境下的鲁棒性提升，在多项低资源任务上取得优异表现，并公开了数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言上的表现已趋于饱和，但在低资源语言（如乌尔都语、泰语）上仍然较差，主要原因是训练数据有限、机器翻译噪声高以及跨语言对齐不稳定，亟需提升低资源语言的建模效果。

Method: 提出了LiRA训练框架，包括两个模块：Arca用于将低资源语言锚定到英语语义空间，通过锚点对齐和多智能体协同编码保持嵌入空间的稳定性；LaSR在Arca的基础上加入语言感知的轻量级推理头，通过一致性正则化统一训练目标，增强模型对跨语言理解、检索和推理的能力。

Result: 在构建的多语言产品检索数据集和低资源基准任务（跨语言检索、语义相似性、推理）上，LiRA展示了在少样本和噪声放大环境下的一致增益和鲁棒性，消融实验证明了Arca和LaSR的有效性。

Conclusion: LiRA有效改善了低资源语言的跨语言表示和推理能力，通过模块化设计和一致性正则化提升了大语言模型在多语言、低资源场景下的检索与理解鲁棒性，并公开了相关代码和数据集。

Abstract: As large language models (LLMs) rapidly advance, performance on high-resource
languages (e.g., English, Chinese) is nearing saturation, yet remains
substantially lower for low-resource languages (e.g., Urdu, Thai) due to
limited training data, machine-translation noise, and unstable cross-lingual
alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language
Models), a training framework that robustly improves cross-lingual
representations under low-resource conditions while jointly strengthening
retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored
Representation Composition Architecture), which anchors low-resource languages
to an English semantic space via anchor-based alignment and multi-agent
collaborative encoding, preserving geometric stability in a shared embedding
space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a
language-aware lightweight reasoning head with consistency regularization on
top of Arca's multilingual representations, unifying the training objective to
enhance cross-lingual understanding, retrieval, and reasoning robustness. We
further construct and release a multilingual product retrieval dataset covering
five Southeast Asian and two South Asian languages. Experiments across
low-resource benchmarks (cross-lingual retrieval, semantic similarity, and
reasoning) show consistent gains and robustness under few-shot and
noise-amplified settings; ablations validate the contribution of both Arca and
LaSR. Code will be released on GitHub and the dataset on Hugging Face.

</details>


### [122] [Efficient Seq2seq Coreference Resolution Using Entity Representations](https://arxiv.org/abs/2510.14504)
*Matt Grenander,Shay B. Cohen,Mark Steedman*

Main category: cs.CL

TL;DR: 该论文提出压缩表示提升Seq2seq指代消解模型在对话等增量场景下的效率。通过只保留实体级别标记，在保证性能的前提下实现大幅度输入压缩，并在多个数据集上取得接近或超过最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的Seq2seq指代消解模型无需针对任务设计的参数，能实现最先进的性能，但它们在灵活性和效率上有所不足，尤其在需要逐步处理文本的对话等场景下难以高效应用。

Method: 提出了一种压缩表示方法，通过提取和重组实体级别的标记，舍弃绝大多数其他输入标记，从而提升在增量场景下的处理效率。

Result: 在OntoNotes数据集上，最佳模型仅比全前缀增量基线低0.6 CoNLL F1分，且压缩比达到1.8；在LitBank数据集（标注了单例指称），超越了当前最优性能。

Conclusion: 在Seq2seq指代消解中丢弃大量输入标记是可行且有效的策略，特别适合增量指代消解任务。

Abstract: Seq2seq coreference models have introduced a new paradigm for coreference
resolution by learning to generate text corresponding to coreference labels,
without requiring task-specific parameters. While these models achieve new
state-of-the-art performance, they do so at the cost of flexibility and
efficiency. In particular, they do not efficiently handle incremental settings
such as dialogue, where text must processed sequentially. We propose a
compressed representation in order to improve the efficiency of these methods
in incremental settings. Our method works by extracting and re-organizing
entity-level tokens, and discarding the majority of other input tokens. On
OntoNotes, our best model achieves just 0.6 CoNLL F1 points below a
full-prefix, incremental baseline while achieving a compression ratio of 1.8.
On LitBank, where singleton mentions are annotated, it passes state-of-the-art
performance. Our results indicate that discarding a wide portion of tokens in
seq2seq resolvers is a feasible strategy for incremental coreference
resolution.

</details>


### [123] [Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs](https://arxiv.org/abs/2510.14565)
*Kyubyung Chae,Gihoon Kim,Gyuseong Lee,Taesup Kim,Jaejin Lee,Heejin Kim*

Main category: cs.CL

TL;DR: 本文针对主权LLMs（本地化大型语言模型）建设，提出了专用的评估框架和数据集，实验证明该类模型在支持低资源语言方面有意义，但用户服务和安全性等方面存在不足。作者呼吁采用更全面的评估标准以推动主权LLMs质量提升。


<details>
  <summary>Details</summary>
Motivation: 当前关于主权大型语言模型（LLMs）的全球争论凸显出各国政府开发本地化LLM的需求，需更贴合各自独特的社会文化和历史环境，但尚缺少验证其社会文化适应性及技术安全性的评估框架和数据集。

Method: 本文构建了一个新的数据集，并设计了一个用于提取和评估主权LLMs社会文化要素及技术稳健性的分析框架。

Result: 实验结果表明，主权LLMs虽然对低资源语言有积极支持作用，但并不总能很好地服务目标用户；对质量（如安全性）的过高预期可能导致忽视关键质量属性。

Conclusion: 要推动主权LLMs发展，需进行更广泛、实证和多维度的评估，涵盖更充分、务实的标准。

Abstract: Recent trends in LLMs development clearly show growing interest in the use
and application of sovereign LLMs. The global debate over sovereign LLMs
highlights the need for governments to develop their LLMs, tailored to their
unique socio-cultural and historical contexts. However, there remains a
shortage of frameworks and datasets to verify two critical questions: (1) how
well these models align with users' socio-cultural backgrounds, and (2) whether
they maintain safety and technical robustness without exposing users to
potential harms and risks. To address this gap, we construct a new dataset and
introduce an analytic framework for extracting and evaluating the
socio-cultural elements of sovereign LLMs, alongside assessments of their
technical robustness. Our experimental results demonstrate that while sovereign
LLMs play a meaningful role in supporting low-resource languages, they do not
always meet the popular claim that these models serve their target users well.
We also show that pursuing this untested claim may lead to underestimating
critical quality attributes such as safety. Our study suggests that advancing
sovereign LLMs requires a more extensive evaluation that incorporates a broader
range of well-grounded and practical criteria.

</details>


### [124] [Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures](https://arxiv.org/abs/2510.14616)
*Shuangshuang Ying,Yunwen Li,Xingwei Qu,Xin Li,Sheng Jin,Minghao Liu,Zhoufutu Wen,Xeron Du,Tianyu Zheng,Yichi Zhang,Letian Ni,Yuyang Cheng,Qiguang Chen,Jingzhe Ding,Shengda Long,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Libo Qin,Ge Zhang,Wenhao Huang,Wanxiang Che,Chenghua Lin*

Main category: cs.CL

TL;DR: 通过新的中英文创作体裁偏好数据集发现，传统RLHF和语言模型法官对主观偏好识别效果较差，需依赖推理链的生成式模型来体现真正的人类写作偏好，且模型规模无法保证主观质量提升。


<details>
  <summary>Details</summary>
Motivation: 当前偏好学习方法在标准基准上表现良好，但在去除客观质量信号后性能显著下降。本论文旨在评估现有方法在主观偏好建模中的局限性并提出新的评测基准。

Method: 提出WritingPreferenceBench数据集，包含8个写作体裁共1800个人类注释的偏好对。通过控制客观正确性、事实准确性和长度来消除客观质量信号。比较序列式奖励模型、零样本语言模型法官和生成式奖励模型的表现。

Result: 序列奖励模型与零样本法官准确率仅约53%，而能够产生明确推理链的生成式奖励模型准确率可达81.8%。在不同体裁和模型规模下表现波动较大，27B模型未优于8B模型。

Conclusion: 现有RLHF方法主要在于侦测客观错误，难以捕捉主观质量偏好。要实现更好的偏好建模或需依赖中间推理表征而非直接分类。

Abstract: Current preference learning methods achieve high accuracy on standard
benchmarks but exhibit significant performance degradation when objective
quality signals are removed. We introduce WritingPreferenceBench, a dataset of
1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8
creative writing genres, where responses are matched for objective correctness,
factual accuracy, and length. On this benchmark, sequence-based reward
models--the standard architecture for RLHF--achieve only 52.7% mean accuracy,
while zero-shot language model judges perform at 53.9%. In contrast, generative
reward models that produce explicit reasoning chains achieve 81.8% accuracy. We
observe high within-model variance across genres: individual models range from
18.2% to 81.8% accuracy across different writing categories, with standard
deviations averaging 10.1%. This variance persists regardless of model scale,
with 27B parameter models showing no consistent improvement over 8B variants.
Our results suggest that current RLHF methods primarily learn to detect
objective errors rather than capture subjective quality preferences (e.g.,
creativity, stylistic flair, and emotional resonance), and that successful
preference modeling may require intermediate reasoning representations rather
than direct classification.

</details>


### [125] [Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2510.14620)
*Kedi Chen,Zhikai Lei,Xu Guo,Xuecheng Wu,Siyuan Zeng,Jianghao Yin,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Qipeng Guo,Kai Chen,Wei Zhang*

Main category: cs.CL

TL;DR: 本文提出了用于大语言模型归纳推理训练的新数据集CodeSeq，通过数列问题和算法任务，推动模型学习更复杂的归纳推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有归纳推理数据集仅关注表面规律，缺乏复杂内部模式，且训练策略仅依赖简单提示响应，缺乏思考过程与难度把控，因此亟需更复杂且能促进模型思维提升的训练方法。

Method: 作者设计了CodeSeq合成数据集，将数列问题打包成算法问题并定义一般项生成任务，通过反思失败用例和迭代修正生成监督微调数据，同时引入基于问题和自生成用例成功率的新型强化学习奖励策略，促进模型从成功和失败中学习。

Result: 实验表明，采用CodeSeq训练的模型在多种推理任务上表现改善，并能保持在分布外任务上的能力，验证了所提方法的有效性。

Conclusion: 基于CodeSeq训练的模型在多项推理任务上表现提升，同时保持了模型在OOD（分布外）任务上的表现，显示CodeSeq能够有效增强模型的归纳推理能力。

Abstract: Large language models (LLMs) make remarkable progress in reasoning tasks.
Among different reasoning modes, inductive reasoning, due to its better
alignment with human learning, attracts increasing interest. However, research
on inductive reasoning faces certain challenges. First, existing inductive data
mostly focuses on superficial regularities while lacking more complex internal
patterns. Second, current works merely prompt LLMs or finetune on simple
prompt-response pairs, but do not provide precise thinking processes nor
implement difficulty control. Unlike previous work, we address these challenges
by introducing \textit{CodeSeq}, a synthetic post-training dataset built from
number sequences. We package number sequences into algorithmic problems to
discover their general terms, defining a general term generation (GTG) task
correspondingly. Our pipeline generates supervised finetuning data by
reflecting on failed test cases and incorporating iterative corrections,
thereby teaching LLMs to learn autonomous case generation and self-checking.
Additionally, it leverages reinforcement learning with a novel Case-Synergy
Solvability Scaling Reward based on both solvability, estimated from the
problem pass rate, and the success rate of self-directed case generation,
enabling models to learn more effectively from both successes and failures.
Experimental results show that the models trained with \textit{CodeSeq} improve
on various reasoning tasks and can preserve the models' OOD performance.

</details>


### [126] [RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF](https://arxiv.org/abs/2510.14628)
*Qing Yang,Zhenghao Liu,Junxin Wang,Yangfan Du,Pengcheng Huang,Tong Xiao*

Main category: cs.CL

TL;DR: 提出了结合AI反馈的强化学习与韵律-情感标签对齐的新TTS框架RLAIF-SPA，无需高成本标注，显著提升了情感表达与自然度，各项关键评测指标全面优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到语音（TTS）系统在中性语音合成上已接近真人效果，但在情感表达方面表现不足，产生的语音缺乏情感和自然度；且传统方法依赖昂贵的情感标注或间接优化目标。

Method: 提出RLAIF-SPA框架，结合了基于AI反馈的强化学习（RLAIF）、自动语音识别（ASR）、大型语言模型（LLM）对生成语音的情感和语义准确性进行评估，将语义准确性和韵律-情感标签对齐作为奖励信号，优化语音的表达性和可懂度。

Result: RLAIF-SPA在Libri Speech公开数据集上，相比Chat-TTS实现了语音识别错误率降低26.1%，语义一致性提升9.1%，且人工评价超过10%的改进。

Conclusion: RLAIF-SPA框架在情感语音合成方面超越了现有技术（如Chat-TTS），在语音识别错误率（WER）、语义一致性（SIM-O）及人工评价指标上都有显著提升。

Abstract: Text-To-Speech synthesis has achieved near-human quality in neutral speech,
but emotional expressiveness remains a challenge. Existing methods often rely
on costly emotion annotations or optimize indirect objectives that fail to
capture the emotional expressiveness and perceptual naturalness of speech,
leading to generated speech that is accurate but emotionally flat. To address
these challenges, we propose the RLAIF-SPA framework, incorporating a
Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic
Speech Recognition (ASR) and Large Language Model (LLM) techniques to
respectively judge semantic accuracy and prosodic-emotional label alignment as
a direct reward for emotional expressiveness and intelligibility optimization.
Specifically, it leverages Prosodic Label Alignment to enhance expressive
quality by jointly considering semantic accuracy and prosodic-emotional
alignment along four fine-grained dimensions: Structure, Emotion, Speed, and
Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the
generation of clear and accurate speech. Experiments on the Libri Speech
dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in
WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [127] [An efficient algorithm for \textsc{$\mathcal{F}$-subgraph-free Edge Deletion} on graphs having a product structure](https://arxiv.org/abs/2510.14674)
*Shinwoo An,Seonghyuk Im,Seokbeom Kim,Myounghwan Lee*

Main category: cs.DM

TL;DR: 本文给出一种双指数时间的固定参数线性算法解决平面图删顶点/边后$	ext{F}$-子图无关性问题，广泛适用于一般化图类，其参数可优化空间已达极限。


<details>
  <summary>Details</summary>
Motivation: 在图论中，判断一个平面图能否通过删除少量顶点或边，使其不包含某一类子图是一个重要问题。现有算法在参数化方面速度较慢，故作者希望设计更高效的算法。

Method: 提出了一个统一的算法框架，专门面向具有“积结构”的图类。他们在此框架下针对平面图、有限局部半径的圆盘图以及有界属图提出了固定参数线性或接近线性的算法。

Result: 开发出了运行时间为参数双指数级的固定参数线性时间算法，显著快于依赖于有界twins-width的模型检测算法。该方法也适用于更广泛的图类，并在理论上是紧的，参数无法进一步减少。

Conclusion: 针对平面图及若干更广泛图类，作者实现了可通过顶点或边删除达到不含指定子图的固定参数线性时间算法，且算法在参数可调性方面已达到理论极限，推断了算法紧性。

Abstract: Given a family $\mathcal{F}$ of graphs, a graph is
\emph{$\mathcal{F}$-subgraph-free} if it has no subgraph isomorphic to a member
of $\mathcal{F}$. We present a fixed-parameter linear-time algorithm that
decides whether a planar graph can be made $\mathcal{F}$-subgraph-free by
deleting at most $k$ vertices or $k$ edges, where the parameters are $k$,
$\lvert \mathcal{F} \rvert$, and the maximum number of vertices in a member of
$\mathcal{F}$. The running time of our algorithm is double-exponential in the
parameters, which is faster than the algorithm obtained by applying the
first-order model checking result for graphs of bounded twin-width.
  To obtain this result, we develop a unified framework for designing
algorithms for this problem on graphs with a ``product structure.'' Using this
framework, we also design algorithms for other graph classes that generalize
planar graphs. Specifically, the problem admits a fixed-parameter linear time
algorithm on disk graphs of bounded local radius, and a fixed-parameter
almost-linear time algorithm on graphs of bounded genus.
  Finally, we show that our result gives a tight fixed-parameter algorithm in
the following sense: Even when $\mathcal{F}$ consists of a single graph $F$ and
the input is restricted to planar graphs, it is unlikely to drop any parameters
$k$ and $\lvert V(F) \rvert$ while preserving fixed-parameter tractability,
unless the Exponential-Time Hypothesis fails.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [128] [Decidability and Characterization of Expansivity for Group Cellular Automata](https://arxiv.org/abs/2510.14568)
*Niccolo' Castronuovo,Alberto Dennunzio,Luciano Margara*

Main category: cs.FL

TL;DR: 本文系统研究了群元胞自动机的膨胀性，分别在可换群和一般群情形下，给出膨胀性的判据和可判定性，并刻画了其与拓扑可传递性及单射性之间的严格包含关系。


<details>
  <summary>Details</summary>
Motivation: 群元胞自动机的动力学行为在群和拓扑结构的交互下复杂多样，理解其膨胀性与其他重要性质之间的关系具有理论意义。

Method: 对可换群，作者给出了膨胀性的易检判据；对一般群（包括非可换群），证明膨胀性是可判定性质。

Result: 1. 给出了可换群元胞自动机膨胀性的判据。
2. 证明了一般群元胞自动机的膨胀性是可判定的。
3. 阐明了膨胀性自动机在拓扑可传递且单射的自动机类中是严格的真子集。

Conclusion: 广义而言，膨胀性的群元胞自动机严格包含于拓扑可传递且单射的群元胞自动机的类中。

Abstract: Group cellular automata are continuous, shift-commuting endomorphisms of
$G^\mathbb{Z}$, where $G$ is a finite group. We provide an easy-to-check
characterization of expansivity for group cellular automata on abelian groups
and we prove that expansivity is a decidable property for general (non-abelian)
groups. Moreover, we show that the class of expansive group cellular automata
is strictly contained in that of topologically transitive injective group
cellular automata.

</details>


### [129] [Efficient Verification of Metric Temporal Properties with Past in Pointwise Semantics](https://arxiv.org/abs/2510.14699)
*S. Akshay,Prerak Contractor,Paul Gastin,R. Govind,B. Srivathsan*

Main category: cs.FL

TL;DR: 该文开发了将MITL（包括带过去与未来算子的全片段公式）高效转化为确定性广义时钟自动机网络的新方法，大幅提升了点式语义下的模型检测效率，在所测试的多个基准上优于现有技术，实现了端到端的验证工具。


<details>
  <summary>Details</summary>
Motivation: 现有针对MITL，尤其是带有过去与未来双算子的公式的模型检测方法，存在自动机决策性不足、效率不高等问题。希望能兼顾表达能力和分析效率，特别是在点式语义下最大化自动机的确定性，以提升模型检测的性能。

Method: 1. 定义了带共享变量的同步时钟自动机网络。2. 将MITL的过去片段线性时间地转化为同步决定性时钟自动机网络，并在增加顶层未来算子时保持确定性。3. 将带过去的全MITL转化为带未来时钟的广义时钟自动机（GTA）网络。4. 针对GTA提出基于SCC的活性分析算法。5. 实现原型工具，支持有限和无限时钟字及过去算子。6. 在72个基准公式上与现有方法对比评测，并实现了端到端的模型检测算法。

Result: 实验表明：提出的方法在MITL可满足性检测点式语义下显著优于已有方法，并在两个著名基准案例中展示了端到端模型检测算法的有效性。

Conclusion: 提出了一种新的针对MITL的点式语义模型检测方法，通过将MITL（包括带过去算子的全片段）高效地转化为确定性时钟自动机或广义时钟自动机网络，并在原型工具和基准实验中显著优于现有方法。

Abstract: Model checking for real-timed systems is a rich and diverse topic. Among the
different logics considered, Metric Interval Temporal Logic (MITL) is a
powerful and commonly used logic, which can succinctly encode many interesting
timed properties especially when past and future modalities are used together.
In this work, we develop a new approach for MITL model checking in the
pointwise semantics, where our focus is on integrating past and maximizing
determinism in the translated automata.
  Towards this goal, we define synchronous networks of timed automata with
shared variables and show that the past fragment of MITL can be translated in
linear time to synchronous networks of deterministic timed automata. Moreover
determinism can be preserved even when the logic is extended with future
modalities at the top-level of the formula. We further extend this approach to
the full MITL with past, translating it into networks of generalized timed
automata (GTA) with future clocks (which extend timed automata and event clock
automata). We present an SCC-based liveness algorithm to analyse GTA. We
implement our translation in a prototype tool which handles both finite and
infinite timed words and supports past modalities. Our experimental evaluation
demonstrates that our approach significantly outperforms the state-of-the-art
in MITL satisfiability checking in pointwise semantics on a benchmark suite of
72 formulas. Finally, we implement an end-to-end model checking algorithm for
pointwise semantics and demonstrate its effectiveness on two well-known
benchmarks.

</details>


### [130] [On the order of lazy cellular automata](https://arxiv.org/abs/2510.14841)
*Edgar Alcalá-Arroyo,Alonso Castillo-Ramirez*

Main category: cs.FL

TL;DR: 本文研究了定义在任意群上的惰性细胞自动机，分析了其阶数（即自映射环的大小），给出了阶数的一般性上界，并证明当激活转移为准常模式时可达到该上界，揭示了即使结构简单的自动机也有有趣的动力学特性。


<details>
  <summary>Details</summary>
Motivation: 本文关注于在任意群空间$G$及字母表$A$上定义的最基础的细胞自动机家族——惰性细胞自动机，其仅在检测到唯一激活转移$p\in A^S$时才修改状态。旨在探究清晰但潜在复杂的动力学行为，尤其是自动机对特定转移和写入符号的依赖性。

Method: 作者采用理论分析方法，定义了惰性细胞自动机的阶数（即映射的不同幂构成的集合的势），并针对阶数建立了一般性上界。同时给出了$p$为准常模式时该上界可达的证明。

Result: 论文给出了惰性细胞自动机阶数的普遍性上界，并证明当激活转移$p$为准常模式时，这个上界可以实现。

Conclusion: 惰性细胞自动机虽然结构简单，但其阶数（动力学复杂性）受激活转移和写入符号影响。论文通过具体分析明确了阶数的一般性上界及其实现条件。

Abstract: We study the most elementary family of cellular automata defined over an
arbitrary group universe $G$ and an alphabet $A$: the lazy cellular automata,
which act as the identity on configurations in $A^G$, except when they read a
unique active transition $p \in A^S$, in which case they write a fixed symbol
$a \in A$. As expected, the dynamical behavior of lazy cellular automata is
relatively simple, yet subtle questions arise since they completely depend on
the choice of $p$ and $a$. In this paper, we investigate the order of a lazy
cellular automaton $\tau : A^G \to A^G$, defined as the cardinality of the set
$\{ \tau^k : k \in \mathbb{N} \}$. In particular, we establish a general upper
bound for the order of $\tau$ in terms of $p$ and $a$, and we prove that this
bound is attained when $p$ is a quasi-constant pattern.

</details>
