<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 109]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
*Anjiang Wei,Tarun Suresh,Tianran Sun,Haoze Wu,Ke Wang,Alex Aiken*

Main category: cs.PL

TL;DR: 本文系统性地评估了主流大语言模型在程序验证中循环不变式自动合成的能力，结果显示微调和采样能改善模型表现，但整体效果仍难超越传统验证工具，未来仍有大量挑战亟待解决。


<details>
  <summary>Details</summary>
Motivation: 程序验证高度依赖于循环不变式，但自动化发现强不变式一直是个难题，因此本论文旨在系统评估大语言模型（LLM）在不变式合成上的能力。

Method: 提出了一个基于验证器的决策流程，并给出了形式化的正确性保证。不仅评估了不变式合成的正确性，同时考察了不变式对验证速度的提升。实验比较了7个先进LLM和现有的基于LLM的验证方法与传统验证器UAutomizer，并分析了模型能力和调优技术（如监督微调与Best-of-N采样）对性能的影响。

Result: LLM-based验证器具有潜力，但目前对比UAutomizer尚无显著优势。不同模型在加速验证上差异明显。针对Qwen3-Coder-480B，通过微调，验证加速案例比例从8%提升至29.2%；Claude-sonnet-4通过Best-of-N采样，性能由8.8%提高到22.1%。

Conclusion: 当前LLM在自动不变式合成方面尚未超越传统工具，模型能力和采样及微调策略可提升表现，但相关基准仍是开放性挑战。

Abstract: Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.

</details>


### [2] [Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics](https://arxiv.org/abs/2509.21793)
*Jianhong Zhao,Everett Hildenbrandt,Juan Conejero,Yongwang Zhao*

Main category: cs.PL

TL;DR: 作者提出了将验证证明转化为编译优化规则的新方法，不仅保证程序正确性，还显著提升了执行性能，在K框架上获得了实验证明。


<details>
  <summary>Details</summary>
Motivation: 验证证明蕴含了完整的程序行为信息，但传统上在验证后会被丢弃。作者希望充分利用这些验证证明，将其用于程序优化，以提升程序性能并保证其正确性。

Method: 通过符号执行构建全路径可达性证明，并将证明的图结构编译为合并了多个语义重写的单一规则，进而实现优化。该方法实现为K框架中的一种语言无关扩展。

Result: 该方法在不同粒度（如opcode级和全程序级）都带来了显著的性能提升，在全程序编译时性能提升达到数量级水平。

Conclusion: 该论文提出了一种“通过验证证明编译”的新范式，能够在保证正确性的同时，将验证证明转化为优化后的执行规则，实现编译优化效果。实验证明该方法在不同粒度下均带来明显性能提升。

Abstract: Verification proofs encode complete program behavior, yet we discard them
after checking correctness. We present compiling by proving, a paradigm that
transforms these proofs into optimized execution rules. By constructing
All-Path Reachability Proofs through symbolic execution and compiling their
graph structure, we consolidate many semantic rewrites into single rules while
preserving correctness by construction. We implement this as a
language-agnostic extension to the K framework. Evaluation demonstrates
performance improvements across different compilation scopes: opcode-level
optimizations show consistent speedups, while whole-program compilation
achieves orders of magnitude greater performance gains.

</details>


### [3] [Committing to the bit: Relational programming with semiring arrays and SAT solving](https://arxiv.org/abs/2509.22614)
*Dmitri Volkov,Yafei Yang,Chung-chieh Shan*

Main category: cs.PL

TL;DR: 提出了基于semiring的关系编程语言semiringKanren，通过类型系统和SAT求解器，提升了求解效率，实验显示优于miniKanren。


<details>
  <summary>Details</summary>
Motivation: 论文希望提升relational programming语言在求解复杂问题时的效率，现有如miniKanren等工具在特定应用场景下效率仍有提升空间。作者尝试引入semiring结构和类型系统优化编程语言性能。

Method: 作者提出了semiringKanren语言，将每个关系表达式表示为semiring数组，并用类型系统约束其为有限数组。该语言的语义被参数化为可选的semiring结构。类型进一步被编译为位字符串表示，对于Boolean semiring可以结合SAT求解器高效执行。

Result: 作者通过数独求解对semiringKanren与miniKanren进行了性能对比，并证明在实验证明semiringKanren在某些情况能更高效地运行。

Conclusion: semiringKanren结合类型系统和semiring结构，可通过SAT求解器高效执行，展现出比miniKanren更优的性能潜力。

Abstract: We propose semiringKanren, a relational programming language where each
relation expression denotes a semiring array. We formalize a type system that
restricts the arrays to finite size. We then define a semantics that is
parameterized by the semiring that the arrays draw their elements from. We
compile semiringKanren types to bitstring representations. For the Boolean
semiring, this compilation enables us to use an SAT solver to run
semiringKanren programs efficiently. We compare the performance of
semiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment
shows that semiringKanren can be a more efficient variant of miniKanren.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens通过关注点抽象，有效提升LLM处理大规模库中Issue定位的准确性，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于大型语言模型（LLM）和LLM代理的方法提升了错误定位的准确率，但在大型代码仓库中，相关逻辑混杂或分散，导致这些方法效果有限。为解决这一困境，研究人员试图更好地抽象和利用代码库中的概念知识，以便更精确地定位问题。

Method: 提出了一种新方法RepoLens，通过抽象和重组代码仓库中的概念知识来引导LLM进行错误定位。分为离线阶段（构建知识库）和在线阶段（根据具体issue聚类、排序相关知识，并增强LLM提示），最终将高层次的“关注点”无侵入地集成进定位流程。

Result: 在SWE-Lancer-Loc基准集上测试，RepoLens在三种SOTA工具（AgentLess、OpenHands、mini-SWE-agent）上均显著提升了文件和函数级的定位表现，平均Hit@k提升超22%，Recall@k提升46%以上。对多种模型（如GPT-4o、GPT-4.1）具有良好泛化性，Hit@1和Recall@10最大提升分别达504%和376%。消融实验和人工评估均验证了所构建关注点的有效性和可靠性。

Conclusion: RepoLens通过关注点抽象和集成，将概念知识高效引入LLM定位流程，大幅提升了大规模代码仓库中的错误定位性能，并在多模型、多工具上展现出良好的泛化能力。

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [5] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 该论文关注女性软件工程师因职业中断（如怀孕、移民、缺乏弹性工作）重返学术界时的挑战，通过多国多校的调研，揭示现有政策不足，提出改善招聘和支持体系的建议。


<details>
  <summary>Details</summary>
Motivation: IT行业为女性重返职场提供了多种支持通道，但学术界相应机会有限，导致女性重返学术或研究岗位时面临挑战，尤其因怀孕、移民或缺乏弹性工作等造成职业中断。

Method: 提出一个多元文化的研究项目，将在多所大学、多个国家进行田野调查，对女性重返学术/研究岗位的挑战进行深入比较分析，关注现行政策与机会的国家间差异。

Result: 该研究将收集并分析不同国家和学术机构政策，为解决性别多样性不足、透明招聘等问题，提出针对性的改进建议。

Conclusion: 学术界在促进女性重返工作岗位方面存在不足，通过跨国多校的调研，本研究将揭示具体障碍，推动学术机构制定更具包容性与透明性的招聘和支持措施。

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [6] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: 本文提出首个完全自动化、基于自然语言任务描述生成Excel教程的框架，显著提升生成质量和效率，具备推广价值。


<details>
  <summary>Details</summary>
Motivation: 现有的Excel教程多由专家手工编写，更新频繁且劳动成本高昂，且现有自动化方法还需人工设定操作序列或示例，难以实现完全自动化。

Method: 提出首个可从自然语言任务描述中全自动生成Excel教程的框架。该框架利用“执行代理(Execution Agent)”组件，在Excel中规划并执行任务，收集中间产物，并将其转化为结构化文档和视频演示。任务数据来自1559条真实场景，评估采用大模型和人工双重评审体系。

Result: 框架在执行成功率上较现有最优方法提升8.5%，生成的教程在可读性和教学效果上接近或超过专家编写，自动化流程将人力成本降至专家的二十分之一。

Conclusion: 首次实现从自然语言自动高效地产生高质量Excel教程，为大规模推广与更新提供了可行方案。

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [7] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 本文提出并验证了一个面向特定领域的软件分析框架，实现了对异构软件库的高效查询和集成。


<details>
  <summary>Details</summary>
Motivation: 异构软件库的数据分析具有挑战性，需要高效的工具来进行整合和分析。

Method: 框架采用多层抽象机制，由领域专用操作组成，并通过案例研究展示其应用。

Result: 验证了该框架的有效性，展示了其在实际案例中的潜力。

Conclusion: 提出了一种领域专用的软件分析框架，可以对异构的软件库进行查询、建模和集成。

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [8] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: 该论文提出AgentPack语料库，收集了大量人机协作生成的高质量代码变更，证明用于模型微调后能显著提升代码编辑任务效果，显示软件工程代理数据对于未来开发极具价值。


<details>
  <summary>Details</summary>
Motivation: 以往用于代码编辑的大语言模型微调数据主要来自于挖掘提交和拉取请求，但这些数据存在噪声，如提交信息简略、编辑混杂、包含机器人提交等问题。随着软件工程代理（Agent）的普及，代码更专注、信息更详实，也更易被过滤，提升数据质量。

Method: 构建AgentPack语料库，涵盖130万条由Claude Code、OpenAI Codex和Cursor Agent等人机共同编辑的代码提交。描述数据筛选和整理流程，统计代理采用趋势，分析编辑的结构特性。并将用AgentPack微调的模型在性能上与仅用人类提交数据训练的模型对比。

Result: 基于AgentPack微调的代码编辑模型性能优于仅基于人类提交语料训练的模型，表明软件工程代理公开数据作为训练资源具有很大潜力。

Conclusion: 高质量、聚焦性更强且由人机共同创作的编辑语料，可以有效提高代码编辑类大模型的表现，公开的软件工程代理数据是未来模型训练的重要方向。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [9] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: 本研究挑战了配置优化中只重视模型准确性的传统观念，系统分析了替代评估方法，并开发了实用预测工具Model4Tune，在大规模验证中表现优异，为后续研究和实际应用提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有配置调优通常只追求模型准确性，但准确性高未必等价于调优效果好。原理和作用尚不清楚，亟需系统性研究与新的实用评估方法。

Method: 通过适应度景观分析提出用理论方法替代单一准确性指标评估模型的有效性，结合大规模实证研究（达2.7万案例）验证理论，并开发自动化预测工具Model4Tune。

Result: 提出适应度景观分析新角度，开发并验证Model4Tune工具。实验证明其准确率在79%-82%之间，显著优于随机选择。

Conclusion: Model4Tune可以有效预测不同系统下最优的模型和调优器组合，并显著优于随机选择，能够辅助实用配置调优。

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [10] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: 本文提出了用于评估大语言模型代码代理安全编程能力的新基准SecureAgentBench，涵盖真实漏洞场景、多文件编辑和多重测试。实验证明现有代理安全表现差，单靠增加安全指令提升有限，显示该领域需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）赋能的代码代理在软件工程领域自动化测试、调试和修复等任务的应用日益广泛，其生成代码的安全性问题也日益突出。现有基准虽然有助于评估，但未能充分反映真实漏洞背景或采用的评估方法过于狭窄，难以全面衡量功能正确性及新引入漏洞。

Method: 本文提出SecureAgentBench，一个包含105个编程任务的基准，专为全面评估代码代理的安全代码生成能力而设计。每个任务具有：（i）要求在大型代码库中进行多文件编辑的真实情境，（ii）基于真实开源漏洞且精确定位漏洞引入点的场景，（iii）结合功能测试、漏洞利用验证与静态分析检测新漏洞的完整评估体系。

Result: 评估三种代表性代码代理（SWE-agent、OpenHands、Aider）与三种主流LLM（Claude 3.7 Sonnet、GPT-4.1、DeepSeek-V3.1），结果发现：（1）当前代码代理难以生成安全代码，即便表现最好的SWE-agent+DeepSeek-V3.1正确且安全的解决率仅为15.2%；（2）部分代理虽可生成功能正确代码，却仍引入新漏洞；（3）增加明确安全指令对安全编码能力提升有限。

Conclusion: SecureAgentBench成为安全代码生成领域的严格基准，有助于推动LLM安全生成能力的深入研究，并为未来可靠软件开发奠定基础。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [11] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: 提出SK2Decompile，两阶段基于强化学习模型的反编译方法，实现结构和标识符的高质量还原，显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLM）的二进制反编译技术在还原源码原始结构和标识符方面存在局限性，影响程序的可读性和可维护性。

Method: 提出了SK2Decompile，两阶段反编译方法。第一阶段使用结构恢复模型将二进制代码翻译为中间表示（IR），作为程序的骨架，并通过强化学习优化结构恢复质量。第二阶段用标识符命名模型将通用占位符替换为有语义的标识符，同样用强化学习目标优化。两阶段过程独立提升反编译的正确性和可读性。

Result: SK2Decompile显著优于当前最先进方法。实验表明，在HumanEval数据集上平均可重执行率提升21.6%，在GitHub2025基准上的R2I指标比Idioms方法提升了29.4%。

Conclusion: SK2Decompile通过骨架-皮肤两步生成及强化学习，提升了二进制反编译的结构恢复与标识符命名能力，有效提高了反编译程序的可重执行性和可读性。

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [12] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: 本文提出了基于大语言模型的MMORPG自动化测试系统TITAN，在真实商用游戏中大幅提升测试效率与Bug发现能力，并已在多款游戏实际落地应用，推动了智能化游戏测试的发展。


<details>
  <summary>Details</summary>
Motivation: MMORPG拥有高度复杂和频繁更新的游戏环境，使得传统手动或自动化测试效率低下且覆盖度不足。现有LLM游戏自动化方法对复杂环境和长程任务的处理能力有限，因此亟需更高智能化、高效并可实际落地的MMORPG自动化测试方案。

Method: 采用包含四大核心模块的LLM驱动agent框架：状态感知与抽象、动作优化与优先级排序、长程推理与自我纠错、LLM判定Oracles进行功能与逻辑Bug检测，并通过原型系统在2款大型商用MMORPG（PC及移动平台）上开展实验测试和消融分析。

Result: TITAN系统在实验中达到95%任务完成率和显著高于现有自动化测试的Bug检测性能，并发现了4个此前未被识别的游戏缺陷。消融实验验证各组件对框架整体性能的贡献。同时，TITAN已成功部署到8个真实游戏QA流程中，显示了其实际影响力。

Conclusion: TITAN系统作为一种基于大语言模型（LLM）的智能化MMORPG测试框架，在实际测试中展现出显著优于传统自动化测试方法的性能，并且已在多个真实游戏QA流程中成功应用，表明其在游戏自动化测试领域具有实际应用价值和推动作用。

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [13] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 本文系统性揭示了LLMs生成代码时对用户级自然提示变化的脆弱性，库幻觉问题严重，提示工程部分有效但不稳定，亟需开发防护机制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs生成代码时存在库幻觉且风险日益增加，但实际提示变化如何影响幻觉率尚不明确，有必要针对真实用户行为展开系统性研究。

Method: 首次系统性研究用户级提示变化对LLM生成代码库幻觉的影响，评估六种不同的LLM在库名称和成员幻觉上的表现，使用开发者论坛真实语言与人为拼写错误、虚假名称等多种用户错误进行实验。

Result: 一字符拼写错误导致最高26%的任务出现库幻觉，虚假库名被接受率高达99%，时间相关提示导致最高84%的任务出现幻觉。提示工程能部分缓解问题，但效果不一且依赖特定模型。

Conclusion: LLMs对自然提示变化十分脆弱，存在严重的库相关幻觉问题，亟需有效防护措施。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [14] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 通过调整提示词的可读性（简化语言），可在不影响模型性能的同时有效降低能耗，推动绿色提示工程和可持续AI实践。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型在软件工程中应用广泛，但其推理过程带来的环境影响日益受到关注。过去的研究多聚焦于硬件配置和提示词长度，对语言复杂性作为可持续性因素关注不足。

Method: 提出了绿色提示工程理念，将语言复杂性作为影响能耗和性能的设计维度。在需求分类任务上，采用开源的小型语言模型，调节提示词的可读性进行实证研究。

Result: 实验结果显示，提示词的可读性会影响环境可持续性和性能，并揭示了两者之间的权衡。简单的提示语在几乎无F1分数损失的情况下可以显著降低能耗。

Conclusion: 对于实际开发者，优化提示语可降低能耗又保持性能；对于研究者，该工作为可持续提示设计相关的研究和指导提供了新方向，推动绿色人工智能的发展。

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [15] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: 该论文提出了结合GPU加速和灵活更新策略的LBP算法，在大规模程序分析上性能大幅超越现有方法，并保持高准确率。


<details>
  <summary>Details</summary>
Motivation: Loopy Belief Propagation (LBP)算法在概率图模型中的应用广泛，但在大规模程序分析中计算压力巨大。虽然GPU并行计算有潜力解决这一问题，但现有方法灵活性有限、未融合逻辑约束与GPU加速，导致性能不理想。

Method: 提出一种支持用户自定义更新策略的统一表示方法及其依赖分析算法，实现灵活、高效的消息更新，并结合Horn子句的结构，分组消息以减少GPU的warp分歧，提高资源利用率。

Result: 在八个真实Java程序的数据竞争分析实验中，所提方法比最先进的串行方法平均加速2.14倍，较现有GPU方案平均加速5.56倍，并保持高准确率。

Conclusion: 结合逻辑约束的GPU加速LBP算法显著提升了程序分析的速度与准确性，实现了灵活策略支持与高效资源利用，为大规模程序分析提供了实用工具。

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [16] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: 该研究系统比较了四种ADS测试模式对现实差距的影响，发现混合现实方法在提升真实感知方面具有优势，并揭示了影响测试结果转移性的关键因素，为更可靠的自动驾驶验证方法提供参考。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）在发展过程中，需要安全且可扩展的方法来评估其在多样化驾驶场景下的性能，因此基于仿真的测试成为关键。但仿真与现实世界的表现存在差距（即“reality gap”），这影响了测试结果在实际系统中的可转移性。本文探讨了不同测试模式在弥补现实差距上的有效性。

Method: 通过构建小型实体车辆（配备真实传感器）及其数字孪生体，分别实现并比较了四种测试模式：软件在环（SiL）、车辆在环（ViL）、混合现实（MR）、以及真实场景测试。在包含真实障碍、道路拓扑、室内环境的多样室内场景中，测试两种ADS架构（模块化与端到端），并从驱动、感知和行为共性三维度系统性评估现实差距的影响。

Result: 结果显示，SiL和ViL测试在真实动态和感知方面存在简化，混合现实（MR）测试能有效提升感知真实度且不影响安全与控制。此外，作者明确了哪些情况下测试失效不会在不同模式间转移，并定位了导致表现不同的具体现实差距维度。

Conclusion: 本文提供了各类测试模式在ADS验证中的优势与局限，提出了提升验证鲁棒性和转移性的建议，为未来自动驾驶系统的测试路径指明方向。

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [17] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: 结合具体问题细节的情境化指导能显著提高新手错误定位的正确率、速度及满意度，远胜于抽象指导或通用步骤，1-2次训练即可见显著成效，持续练习可进一步稳定性能。


<details>
  <summary>Details</summary>
Motivation: 初学者在错误定位方面缺乏系统方法，传统的抽象指导或一般性具体步骤效果有限，尚不清楚针对具体情境的指导是否能改善学习效果。

Method: 设计了四种教学条件：无指导、抽象指导、具体步骤指导、结合具体Bug定位步骤与问题细节的情境化指导（context-specific instruction）。进行为期八周的纵向对比实验，44名本科生参与并完成五场调试任务，系统记录正确率、完成时间及主观感受。

Result: 情境化指导组（G4）在正确率和完成时间上显著优于其他组，首次训练即达到80%正确率，三周后仍保持80%，完成时间快速稳定在13-15分钟。主观反馈显示G4压力更低、满意度更高，参与者能通过具体情境举例更好地掌握策略。

Conclusion: 情境化指导能显著提升新手错误定位技能的习得速度和保留效果，结合具体例子与抽象原理有助于理论和实践的转化，为新手提供更公平的学习路径。

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [18] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind结合大语言模型与蒙特卡洛树搜索，有效解决了Android应用复杂UI下bug报告自动复现的难题，结果显著优于现有方法，开创了语义推理与战略规划协作的新方向。


<details>
  <summary>Details</summary>
Motivation: 根据文本bug报告自动重现Android应用崩溃非常具有挑战性，尤其是在报告信息不完整及现代UI交互呈现高度组合复杂性的情况下。现有的方法（如强化学习或大语言模型）在推断未观测到的用户步骤、重建实际操作序列，以及有效探索庞大UI空间方面均存在局限。

Method: 提出了TreeMind方法，将大语言模型与定制化的蒙特卡洛树搜索（MCTS）算法结合，实现策略性的UI探索。具体方法包括：将问题建模为目标驱动的搜索任务，核心借助MCTS进行计划。引入两个不同职能的LLM代理——Expander负责基于当前UI状态生成前k个最优动作，Simulator用于预测各动作导致崩溃复现的概率。方法还结合多模态UI输入与提示工程，反馈驱动地逐步重建用户操作路径。

Result: 在三个主流基准的93个真实Android bug报告数据集上进行评估。结果显示，TreeMind在bug复现成功率方面显著优于四种最先进的对比方法。案例研究进一步表明LLM语义推理与MCTS规划联合应用在自动化bug复现领域具有强大潜力。

Conclusion: TreeMind首次将大语言模型的语义推理与外部决策算法（MCTS）有机结合，有效提升了复杂UI环境下自动化崩溃复现的效果。该方法为面向实际应用的移动端自动化bug重现提供了新思路，并验证了在目标导向任务中融合智能推理与战略规划的可行性。

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [19] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 本文提出AFD方法，结合值流分析和LLM自动识别自定义分配函数，极大增强指针分析的细致度和效率，在真实项目中显著改善别名分析和漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 指针分析对于静态分析任务至关重要，但C/C++程序中用户自定义分配函数普遍存在且难以精确建模，导致别名分析粗糙、分析效果受限。现有方法多忽略自定义分配器，从而降低分析的精度。

Method: 提出了AFD技术，通过自动识别和建模自定义分配函数来增强指针分析。采用混合方法：对简单包装器用值流分析，对复杂分配模式结合大型语言模型进行推理，从而精确刻画每次分配的堆对象，类似上下文敏感但无显著额外开销。

Result: 在15个真实C项目中，发现超过600个自定义分配函数。引入AFD后，被建模堆对象数量提升26倍，别名集合规模减少39%，仅提升1.4倍运行时开销，同时改进了间接调用解析，发现了17个之前未检测到的内存Bug。

Conclusion: 对自定义分配函数进行精确建模，能有效提升大规模软件系统的指针分析精度和可扩展性。

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [20] [Can Large Language Models Autoformalize Kinematics?](https://arxiv.org/abs/2509.21840)
*Aditi Kabra,Jonathan Laurent,Sagar Bharadwaj,Ruben Martins,Stefan Mitsch,André Platzer*

Main category: cs.LO

TL;DR: LLM有潜力自动将自然语言物理描述转为可供形式化分析的物理模型，目前在基准测试中可达到约70%的准确率，但仍需要改进建模能力。


<details>
  <summary>Details</summary>
Motivation: 自主网络-物理系统如机器人和自动驾驶汽车，需要形式化方法以确保其控制决策的可靠性。但目前模型的形式化过程高度依赖人工，成为应用落地的瓶颈。

Method: 该研究设立了由20个本科物理运动学问题组成的基准测试套件。每个问题要求LLM根据自然语言描述，生成差分博弈逻辑（dGL）物理模型，通过语法检查和语义执行来评估模型。

Result: 在实验中，LLM获得了70%的问题成功建模（采样最优结果）。同时，作者分析了失败案例，为今后提升自动形式化能力提出了改进方向。

Conclusion: 本文首次定量展示了LLM可自动将自然语言运动学问题转化为含连续动力学的混合博弈逻辑模型的可行性和现有性能，为未来智能自动形式化奠定了基线与研究框架。

Abstract: Autonomous cyber-physical systems like robots and self-driving cars could
greatly benefit from using formal methods to reason reliably about their
control decisions. However, before a problem can be solved it needs to be
stated. This requires writing a formal physics model of the cyber-physical
system, which is a complex task that traditionally requires human expertise and
becomes a bottleneck.
  This paper experimentally studies whether Large Language Models (LLMs) can
automate the formalization process. A 20 problem benchmark suite is designed
drawing from undergraduate level physics kinematics problems. In each problem,
the LLM is provided with a natural language description of the objects' motion
and must produce a model in differential game logic (dGL). The model is (1)
syntax checked and iteratively refined based on parser feedback, and (2)
semantically evaluated by checking whether symbolically executing the dGL
formula recovers the solution to the original physics problem. A success rate
of 70% (best over 5 samples) is achieved. We analyze failing cases, identifying
directions for future improvement. This provides a first quantitative baseline
for LLM-based autoformalization from natural language to a hybrid games logic
with continuous dynamics.

</details>


### [21] [A Correct by Construction Fault Tolerant Voter for Input Selection of a Control System](https://arxiv.org/abs/2509.22236)
*Arif Ali AP,Jasine Babu,Deepa Sara John*

Main category: cs.LO

TL;DR: 本文通过形式化方法和Rocq定理证明器，实现了用于航空电子控制系统的高可靠性投票单元的建模、验证和合成。


<details>
  <summary>Details</summary>
Motivation: 安全关键系统需要高可靠性和容错性，因此引入冗余输入单元，并通过投票逻辑来筛选可靠输入，提高系统稳定性和安全性。

Method: 本研究采用基于形式化需求建模的方法，使用Rocq定理证明器，按照“正确性即构造”的准则设计、验证和自动合成通用投票单元。

Result: 成功提出并验证了一种适用于N重测量系统的通用投票单元，能够满足航空电子控制系统的需求，并保证其安全与可靠性。

Conclusion: 该研究为航空电子系统中冗余测量控制的安全投票单元提供了形式化、可验证和自动合成的解决方案，提升了系统的安全性和可靠性。

Abstract: Safety-critical systems use redundant input units to improve their
reliability and fault tolerance. A voting logic is then used to select a
reliable input from the redundant sources. A fault detection and isolation
rules help in selecting input units that can participate in voting. This work
deals with the formal requirement formulation, design, verification and
synthesis of a generic voting unit for an $N$-modular redundant measurement
system used for control applications in avionics systems. The work follows a
correct-by-construction approach, using the Rocq theorem prover.

</details>


### [22] [Specifying an Obligation Taxonomy in the Non-Markovian Situation Calculus](https://arxiv.org/abs/2509.22533)
*Kalonji Kalala,Iluju Kiringa,Tet Yeap*

Main category: cs.LO

TL;DR: 本文利用Situation Calculus并引入非Markovian控制机制，实现了对于“义务”问题的多种文献定义的规范。与传统Event Calculus不同，新方法保证了理论上的直观性和正确性，扩展了Situation Calculus的应用领域。


<details>
  <summary>Details</summary>
Motivation: 过去三十年，Situation Calculus已被广泛用于描述动态领域及代理行为。先前对于利用过去情形进行非Markovian控制的理论关注度不高，而且现有的关于“义务”的规范多使用Event Calculus，Situation Calculus应用较少。

Method: 研究通过Situation Calculus及其非Markovian控制机制对“义务”的各种定义进行规范，基于事件与状态描述义务条件。与现有的Event Calculus方法进行对比分析，确保新规范具备直观正确性。

Result: 成功在Situation Calculus框架下实现了多种文献中的“义务”定义，并证明这些规范具有良好的直观属性，保障了描述的正确性。推进了Situation Calculus的应用边界。

Conclusion: Situation Calculus能够有效地通过非Markovian控制来规范不同类型的义务，为该领域引入新的理论工具补充了Event Calculus的不足。

Abstract: Over more than three decades, the Situation Calculus has established itself
as an elegant, powerful, and concise formalism for specifying dynamical domains
as well as for reasoning about the effects of actions of those domains both in
the world and in the mental state of the modelled agents. Moreover, it has also
been established that the preconditions of a given action and its effects may
be determined entirely by the current situation alone, or they may be
determined by past situations as well. When past situations are involved in
determining action preconditions and effects, resulting theories are
non-Markovian. Assuming a specification of actions that produce obligations, we
consider using non-Markovian control in the Situation Calculus to specify
different notions of obligations found in the literature. These notions have
been specified using Event Calculus; but, as far as we know, they have never
been specified using the Situation Calculus. The specifications in this paper
yield intuitive properties that ensure the correctness of the whole endeavour.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 本论文首次系统性证明次正规语言类在判定谓词下具有线性可分性，这不仅保证了有限可观测和线性模型可学习，同时通过实验验证了理论成果在自然语言结构建模中的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索次正规语言类是否具备线性可分性，从而实现可观测性和用简单线性模型的可学习性，并验证这些性质在自然语言结构建模中的作用。

Method: 通过理论证明以及合成实验和真实数据实验，分析语言类的可分性和可学习性。

Result: 在无噪声条件下合成实验证实线性可分性，在英语形态学真实数据实验中，学习到的特征符合已知的语言学约束。

Conclusion: 文章证明了所有标准的次正规语言类在其判定谓词表示下是线性可分的，这为自然语言建模提供了一个严谨且可解释的基础。

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [24] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 本文提出新颖架构，有效定位语言模型的幻觉信号，发现信号高度集中，可用极少维度高效检测，有望推动更节能的幻觉检测应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉（输出与事实不符）是一个严重问题，既影响实际应用，也阻碍模型部署。尽管已有研究发现隐藏层存在幻觉与真实内容的差异，然而幻觉信号在具体层级的分布和定位仍不清楚，这限制了有效检测方法的发展。

Method: 本文提出了一种双模型架构，集成了“投影融合（PF）”模块用于自适应层间特征权重，以及“差异特征学习（DFL）”机制，通过对比两个学习互补表示的编码器的结果，识别区分幻觉与真实内容的特征。

Result: 系统性实验证明，幻觉信号集中于极为稀疏的特征子集，在HaluEval问答、对话和摘要数据集上显著提高了检测准确率。分析发现存在一种“漏斗型”层级结构，浅层特征多样，深层特征高度集中，仅用1%的特征维度也能保持检测性能，模型推断成本大幅降低。

Conclusion: 该方法揭示幻觉信号比预期更集中，为高效、低成本的幻觉检测系统提供了新思路，可在保证准确率的同时显著减少推理资源消耗。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [25] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的上下文质量评估指标CI value，将RAG选取上下文过程数据价值化，并通过分层代理模型实现高效推理。实验广泛验证，其方法在过滤无关或噪声信息并保留重要信息方面效果显著，超过现有主流方法。


<details>
  <summary>Details</summary>
Motivation: RAG通过结合外部知识减少LLM幻觉，但效果受限于检索到的上下文质量，传统方法单独评估上下文质量效果提升有限。作者认为这是由于未能全面利用请求、上下文列表及生成器信息。为更好评估上下文质量，提出新的方法。

Method: 把上下文质量评估转化为推理时的数据价值度量问题，提出Contextual Influence Value（CI value）指标，通过移除某个上下文衡量性能下降来定量其价值，综合考虑查询相关性、列表唯一性及生成器一致性，并用分层架构的代理模型实现高效估算，训练采用oracle监督和生成器端到端反馈。

Result: 在8个NLP任务和多种LLM上实验，作者提出的方法在上下文选择方面有效优于现有方法，能显著过滤低质量上下文并保留关键信息。

Conclusion: CI value结合多源信息，显著提升RAG选取高质量上下文的能力，有效缓解低质量信息带来的性能瓶颈，同时降低参数调优复杂度，在实际任务中表现优异。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [26] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 论文揭示大语言模型最大有效上下文窗口（MECW）与厂商宣称的最大上下文窗口（MCW）存在巨大差距，并随问题类型而变化。多数模型在不到宣称值1%的上下文窗口就显著失效，用户在实际应用中需关注MECW并据此选择和优化模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）厂商通常宣传其最大上下文窗口（MCW）很大，然而这些窗口在实际应用中的有效性备受质疑。鉴于此，本文希望澄清“有效上下文窗口”与“最大上下文窗口”之间的区别，帮助用户和研究者正确评估模型实际性能。

Method: 作者提出了最大有效上下文窗口（MECW）的概念，并设计了一套测试方法，用于在不同窗口大小和问题类型下评估模型的有效性。此外，构建了标准化比较流程，以便发现模型在递增上下文窗口时的失效点。研究采集了不同模型在数十万数据点上的表现。

Result: 研究发现，不同模型报告的最大上下文窗口与实际有效窗口存在巨大差异。部分主流模型在仅有100个token作为上下文时就会失效；多数模型在1000 token后准确率急剧下降。所有模型实际可用上下文窗口远低于宣称值，偏差高达99%。最大有效上下文窗口也会随任务类型而变化，影响模型准确率和幻觉率。

Conclusion: 厂商所宣传的大型上下文窗口并不能完全代表模型的实际有效能力，用户在实际任务中需要关注最大有效上下文窗口而非理论值，根据不同任务类型进行模型选择和优化。该研究为提升模型准确率和减少幻觉现象提供了实用参考。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [27] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 该文系统分析了代码数据中影响LLM推理能力的关键因素，发现结构性特征远比语义更重要。使用抽象表示方式可有效提升性能，而不同编程语言风格对特定任务有独特优势，对未来LLM训练数据采集与设计有重要启示。


<details>
  <summary>Details</summary>
Motivation: 尽管代码数据已被证明能增强大语言模型（LLM）的推理能力，但究竟代码的哪些方面在其中起到关键作用尚不清楚。本文旨在系统研究代码各属性对LLM推理性能的影响。

Method: 采用数据驱动和系统的方法：构建十种编程语言的平行指令数据集，通过有控制的扰动来破坏代码的结构或语义特性；在五个模型系列和八种规模的LLM上，对各数据变体进行微调，再在自然语言、数学和代码任务上评估其表现。

Result: 总计3331个实验显示，模型对结构扰动比语义扰动更脆弱，尤其是在数学和代码任务。合适的抽象表现（例如伪代码和流程图）和代码一样有效，且可以用更少的token表达相同信息，甚至有时性能更佳。即使代码被损坏，但表层的规律性存在时，性能仍有竞争力。不同语言风格对于任务也有影响，如Python更有利于自然语言推理，而Java、Rust等底层语言更有利于数学任务表现。

Conclusion: 不同的代码属性对LLM推理能力有显著影响。结构性的扰动负面作用最大，且合理的代码抽象能替代原始代码信息。代码风格和语言类型会影响模型在不同任务上的表现，这些洞见可用于优化训练数据设计，提升LLM推理能力。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [28] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: 仅靠模型规模扩展无法推动AI实现突破，需人类符号系统引导其直觉，从而促进AI的创新与发现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在参数规模不断增长，但仍缺乏自主发现和深度理解的能力。作者希望通过引入人类符号系统，弥补模型的认知短板。

Method: 本文通过观点阐述和理论讨论，强调加入人类设计的符号系统对大模型认知能力提升的重要性。

Result: 提出了为大语言模型引入人类符号作为指导的设想，认为这将激发AI的真正创新能力。

Conclusion: AI的未来发展不仅仅依赖于模型的规模扩展。为了实现真正的发现，需要人为设计的符号来指引大语言模型的强大却盲目的直觉。

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [29] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 本文系统研究了transformer对传递关系推断（连通性任务）的能力，发现模型在低维、结构规则的图上表现良好，模型规模越大泛化越强，但在非规则、复杂连通性任务上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注的是变换器（transformer）能否基于输入提示中的上下文例子推断传递关系，但尚未探索模型通过训练样本学习推断传递关系的能力，以及模型规模扩展对这一能力的影响。推断传递关系能力对于许多如因果推断等场景至关重要，因此有必要进一步研究。

Method: 作者通过生成不同规模的有向图，训练不同大小的transformer模型，并系统性地评估它们在各种不同规模有向图上的推断传递关系（即连通性）能力。重点关注'网格型'有向图（即每个节点可嵌入低维空间，连通性可由节点嵌入直接推断）和非网格型多连通分量的图。

Result: transformer能较好地学习'网格型'有向图中的连通性任务，且随着底层图的维度提升，学习难度增加。另外，模型规模越大，对连通性任务的泛化能力越好。但在非网格、分量众多的图上，transformer很难学习连通性，尤其分量数越多越困难。

Conclusion: transformer擅长处理低维嵌入、结构较规则（如网格型）的有向图上的连通性推断，但在结构复杂或分离组件较多的图上表现不佳，且模型规模扩展能有效提升泛化能力。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [30] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 该文发现现有大语言模型在多语言多文化环境下道德推理表现不一致，容易产生文化不适应，呼吁开发更具文化敏感性的AI模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）越来越多地应用于多语言和多文化环境中，然而其在道德推理方面主要依赖英语训练数据，难以适应不同文化和语言背景下的道德判断，亟需系统研究其跨文化泛化能力。

Method: 将两个已经建立的道德推理基准翻译为五种具有文化和类型多样性的语言，并开展多语言零样本评测。结合设计的问题，分析LLM在不同语言下的道德决策差异及其原因，并通过案例研究探讨LLM预训练数据对其道德判断的影响。

Result: 分析结果显示，LLM在不同语言下的道德判断存在显著不一致性，往往反映出文化错位。研究揭示了驱动这些差异的因素，包括模型间分歧和采用的推理策略。

Conclusion: LLM的道德推理能力在多语言多文化环境下面临挑战，存在文化适应性不足的问题。需要更加关注和改进AI的文化敏感性，并给出了模型道德错误的结构化类型学。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [31] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 本研究采用模拟病例评估GPT-5在五种糖尿病相关场景下的表现，包括症状识别、实验室解读、妊娠筛查等，结果显示GPT-5决策与ADA标准高度一致，证明其可用于医生及患者辅助，同时强调了规范评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病在全球范围内成为重大健康挑战，患者人数剧增。虽然美国糖尿病协会（ADA）设定了明确的诊断标准，但早期识别依然困难，原因包括症状不明显、实验室结果临界、孕期诊断复杂，以及长期监测要求高。传统诊断方式和患者管理面临严峻挑战。

Method: 本研究利用完全模拟的合成病例（模拟框架），依据ADA 2025护理标准并参考公开数据集（NHANES、Pima Indians、EyePACS、MIMIC-IV）评估最新的生成式预训练模型GPT-5。研究设置五种典型场景：症状识别、实验室结果解读、妊娠期糖尿病筛查、远程监测、多模态并发症检测，并对每个场景进行分类、临床推理生成功能、患者解释及生成结构化JSON总结输出。

Result: GPT-5在各场景下产生的决策与ADA标准高度一致，能够辅助病例分类、临床推理和患者解释，同时实现结构化输出，并具备双重服务能力（临床及患者）。

Conclusion: GPT-5有望成为医生和患者双重辅助工具，能生成结构化、可解释且易理解的输出，提升糖尿病诊断及患者管理水平。同时，本研究强调建立可重复的评估框架对于“在医疗领域负责任地使用大模型”至关重要。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [32] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 探索消除语言模型性别刻板印象的公平目标背后的遗忘机制，发现现有方法难以兼顾公平与下游性能，遗忘受刻板印象和整体内容高度耦合。


<details>
  <summary>Details</summary>
Motivation: 道德对齐成为调控预训练语言模型行为的主流方法，尤其用于减缓性别刻板印象，但通常会导致下游任务性能下降。作者关注如何在消除刻板印象与保持模型有用性之间取得平衡。

Method: 分析遗忘机制以及公平目标对减缓性别刻板印象过程中任务性能的影响。通过量化遗忘的整体和选择性程度，探究他们与任务性能之间的关系。

Result: （1）下游任务性能主要受整体遗忘程度驱动；（2）针对性遗忘刻板印象会增加整体遗忘；（3）现有通用的缓解遗忘方法效果有限，无法提升任务性能。

Conclusion: 当前针对公平目标的遗忘机制存在局限：无法真正平衡刻板印象的消除与模型功能的保持。提升公平性通常会以较大代价削弱模型原本学习内容，对下游任务不利。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [33] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 文中提出了一套用于企业定制推理模型的RLVR训练方法，并在BIRD数据科学基准任务上首次即达SOTA，且不依赖额外数据或专有模型，方法简单通用，适用于广泛企业场景。


<details>
  <summary>Details</summary>
Motivation: 企业客户常面临需求特定推理模型以整合自身知识，因此需要能应用企业特定奖励函数的强化学习方法。

Method: 提出并应用了RLVR（可验证奖励的强化学习）框架，结合仔细的prompt和模型选择、TAO离线预热阶段和在线RLVR训练。

Result: 在BIRD（将自然语言数据库查询转为SQL）的测试集首次提交即达73.56%准确率（无自洽性），75.68%（有自洽性），并比第二优方法生成次数更少。未用额外训练数据或专有模型。

Conclusion: 提出的简单且通用RLVR框架不仅在BIRD任务上表现优异，也可广泛应用于企业数据科学等领域。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [34] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 本文针对RL可验证奖励推理训练中信息利用受限问题，提出MoT-G方法，在10项任务中取得5-35%提升，并显著提升训练效率。MoT-G方法增加隐状态熵，促进token空间探索，是提升大模型推理的新路径。


<details>
  <summary>Details</summary>
Motivation: 目前的强化学习可验证奖励（RLVR）方法主要采用离散的token采样，忽略了模型关于候选token概率分布的信息。这种做法可能不利于模型的推理能力。作者认为应充分利用分布式信息，提升推理效果和训练效率。

Method: 提出MoT-G（mixture-of-token generation）方法，将token的概率分布信息引入RLVR流程。统一现有MoT-G方法框架，并扩展RLVR在连续混合空间直接进行chain-of-thought生成。对比两种MoT-G变体并在Reasoning-Gym推理任务集上进行实证评测。

Result: MoT-G方法在7个任务上相较于标准解码实现了5-35%的性能提升，且用一半轨迹数量达到类似准确率，训练效率显著提升。进一步分析发现该方法在推理过程中能够保持更高的隐状态熵并促进token空间探索。

Conclusion: MoT-G可以显著提升RLVR针对大模型推理能力和训练效率，为后续基于分布信息的语言模型推理训练带来新的启示。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [35] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: 提出了一种新型双头蒸馏方法DHRD，可兼得推理准确率和高推理效率，在多个任务上效果显著，推理速度远超传统CoT。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought（CoT）提示能提升模型的分类准确率，但生成推理过程会严重降低推理吞吐量。如何在保持高准确率的同时，避免生成冗长推理过程，提升推理速度，是当前亟需解决的矛盾。

Method: 提出了Dual-Head Reasoning Distillation（DHRD），即在解码器语言模型上新增两个头：（1）训练和推理时用的聚合分类头；（2）仅在训练时用、由教师推理标注监督的推理头。训练时损失为分类交叉熵和输入+推理序列的LM token级损失加权和。推理时关闭推理头，仅用分类头，提升推理效率。

Result: 在SuperGLUE七项任务上，DHRD较聚合分类头（baseline）实现了0.65-5.47%的相对提升，在蕴含/因果推理任务上提升更明显。推理阶段吞吐量与分类器相同，比CoT方法高96-142倍QPS。

Conclusion: DHRD有效解决了CoT提升准确率但牺牲吞吐量的问题，实现了高准确率且高推理速度的分类模型。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [36] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 提出并实现了基于Kisan Call Center数据集的农业聊天机器人，通过模型优化（同义词消除+实体抽取）将准确率提升至86%，显著提高了农民获取农业信息的效率和质量，有助于改善农业产出和呼叫中心资源分配。


<details>
  <summary>Details</summary>
Motivation: 印度是农业为主的经济体，农民获得正确的农业信息对提高产量和质量极为重要，当前信息渠道不足，亟需智能化解决方案。

Method: 构建基于Kisan Call Center数据集的农业聊天机器人，采用句子嵌入模型，并进一步通过消除同义词和实体抽取提升模型性能。

Result: 最初模型准确率为56%，经优化后准确率提升至86%。系统可全天候运行，易于使用，可解答天气、市场价格、植保与政府政策等农业相关问题。

Conclusion: 该智能聊天机器人能够有效提升农民对农业实践信息的获取效率，优化呼叫中心工作分配，为农业增产和劳动力优化带来积极影响。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [37] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 本文分析了非洲口音英语的说话人分离，在临床与通用对话语料上均有较大领域误差，主要由于语音重叠与短轮次。微调分割模块有一定缓解作用，但领域差距依旧明显，未来需改进分割方法并丰富资源。


<details>
  <summary>Details</summary>
Motivation: 本研究关注在说话人分离任务中，非母语（非洲口音）英语语音的领域效应，尤其是在临床语音和普通语音环境下误差差异明显。由于非洲口音英语资源匮乏，现有模型在该领域表现有限。

Method: 对多个主流和开源说话人分离系统，在普通对话和临床对话语料下进行评估，采用严格的DER（Diarization Error Rate）及重叠计分标准。同时，实施轻量化领域自适应：通过在口音匹配样本上微调分割模块，并对误差类型进行细致分析。

Result: 临床对话领域的说话人分离DER误差显著高于普通对话，且该“领域罚分”在不同模型下均存在。误差主要源于重叠语音、频繁短轮次导致的误检和漏检。经过微调的分割模块能降低误差，但无法消除领域间的差距。

Conclusion: 领域差异对非洲口音英语语音分离造成显著影响。目前仅采用分割模块微调难以解决所有误差。未来应研发对重叠更敏感的分割方法，并增加平衡、丰富的临床语音资源。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [38] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 本文对LLM文献引用的两大范式做了系统对比，发现检索和后置补充更适合高风险场景，而生成时精准引用适合要求高精度的场景，并提供了权衡建议。


<details>
  <summary>Details</summary>
Motivation: 在医疗、法律、学术和金融等高风险领域，LLM必须引用人类可验证的来源以增强可信度，避免小错误造成严重后果。然而引用方式存在两种主要思路：生成时附带引用（G-Cite）与事后补充引用（P-Cite），尚未有系统对比分析。本文旨在明确这两者的优劣权衡。

Method: 作者定义并探讨了Generation-Time Citation (G-Cite)和Post-hoc Citation (P-Cite)两种范式，并对现有主流归因数据集进行了全方位实验分析，包括零样本和高级检索加强方法，综合评价覆盖率、准确性和延迟等指标。

Result: 所有实验均发现，两种范式之间在覆盖率和正确性上存在稳定权衡，检索是决定归因质量的主要因素。P-Cite方法往往覆盖更广，准确性有竞争力，延迟中等。G-Cite则准确率更高，但覆盖范围较窄、速度较慢。

Conclusion: 高风险领域推荐以检索为中心、优先采用P-Cite的方案；对于需要极高精准度的场景，则可选用G-Cite。作者公开了实验代码和人工评估结果。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [39] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 提出了一种通过用户偏好对比实现个性化多文档摘要的框架ComPSum，并构建了数据集和无参考评估方法，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 个性化多文档摘要（MDS）需要满足用户在写作风格和内容重点上的个人偏好。为了实现有效个性化，需要细粒度地比较和分析不同用户之间的偏好差异，从而更好地理解目标用户的个性化需求。

Method: 提出了ComPSum框架，通过比较目标用户与其他用户的偏好，生成结构化分析，并利用该分析指导个性化摘要生成。同时，为了评估ComPSum的个性化效果，提出了AuthorMap评估框架，根据不同用户摘要间的归属作者属性进行性能评估。此外，构建了PerMSum数据集，涵盖评论和新闻领域，用于个性化MDS的实验。

Result: 实验表明，ComPSum在PerMSum数据集上通过AuthorMap评估框架取得了优于现有强基线方法的结果，显示出更好的个性化摘要生成能力。

Conclusion: 细粒度用户偏好对比能够有效提升个性化多文档摘要系统的表现，ComPSum框架和AuthorMap评估工具共同为个性化MDS研究提供了新方法和实践基础。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [40] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 将VLM用于形式化规划并用PDDL求解，优于端到端计划生成，瓶颈在于视觉理解，相关领域仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽然能解决简单的多模态规划任务，但在需要长序列动作的长时序任务上表现不佳。纯文本场景下，通过将LLM定位为“形式化工具”并利用形式规划语言与求解器取得了显著进展，而在多模态代入场景，类似思路尚未充分探索。

Method: 本文提出了五种基于VLM作为形式化工具（VLM-as-formalizer）的流程，用于一轮、一词汇开放以及多模态PDDL形式化。同时，还引入两项新的基准，首次考虑使用真实、低质量且多视角的图像进行规划。模型将规划域与问题映射到PDDL，并借助形式求解器求解。

Result: 实验显示，VLM作为形式化工具明显优于端到端生成计划的方式。结果还发现任务的瓶颈主要在视觉环节：VLM难以全面获取对象关系。生成如图像字幕或场景图等中间文本表示虽有一定补偿，但效果不稳定。

Conclusion: VLM-as-formalizer在多模态、开放词汇规划场景下表现突出，优于端到端方案，主要短板是视知觉能力，后续研究可重点解决视觉理解相关问题。

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [41] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 论文提出了用于评估大型语言模型在高层次自然语言编码场景下特征实现能力的新基准FeatBench，发现该任务难度较大（最高成功率29.94%），并公开了相关工具与数据以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评测基准无法有效评估大型语言模型（LLMs）在“vibe coding”这一新兴编程范式中的能力，尤其是缺乏对自然语言特征实现能力的测评。

Method: 提出并构建了FeatBench基准，专注于通过纯自然语言描述进行特征实现评测。FeatBench采用多层过滤、自动化演化数据收集流程，包含高质量测试用例，并涵盖多元领域代码库。

Result: 使用FeatBench评测了两个主流智能体框架与四个顶尖LLM，结果显示在vibe coding范式下完成特征实现的最高成功率仅为29.94%。分析还发现，“激进实现”策略既可能引发关键错误，也可能带来更优的软件设计。

Conclusion: FeatBench有效揭示了vibe coding下特征实现的挑战，并为社区进一步研究与性能提升提供了工具和数据基础。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [42] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本研究发现，主流多语言LLM在翻译习语和双关语等有文化内涵的表达时，存在显著不足，往往需要人工优化。数据量与翻译质量并非完全正相关，文化适应性评估应成为机器翻译新标准。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）翻译研究主要关注语法准确性和词法正确性，较少涉及文化适应性和本地化质量，而这些因素在营销、电商等实际应用中至关重要。因此，该研究旨在探究LLM在翻译中处理文化内涵丰富的语言（如习语和双关语）时的表现。

Method: 本研究选取了87个电商营销邮件样本，涵盖20种语言的24个区域方言；由目标语言的母语人士对LLM生成的译文进行定量评分和定性反馈，重点评价对原文语气、含义和受众的忠实度。

Result: 研究发现，主流LLM虽然能够生成语法正确的翻译，但在文化差异和含有深层含义的表达（如习语、双关语）上表现不足，往往需要大量人工润色。即使在数据资源丰富的主流语言中，LLM仍常常误译带文化内涵的表达。

Conclusion: 当前多语种AI系统在实际本地化应用中存在明显局限性，数据量并非决定机器翻译质量的唯一因素，文化适应性至关重要。研究验证了现有行业和学术基准在文化适应性评估上的不足，有必要在大规模范围内进一步系统研究以改进多语言翻译工作流。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [43] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本文系统分析了多目标强化学习(MORL)在大语言模型(LLM)优化中的应用，提出分类方法、构建基准框架，并展望通过元策略方法提升效率和灵活性的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习(MORL)领域，尤其针对大语言模型(LLM)的优化，存在着多目标优化带来的复杂性和挑战。因此，作者希望探索高效、灵活的方法来同时满足多目标需求，并支持个性化功能。

Method: 作者提出了MORL的方法分类，并系统评估了不同MORL方法用于LLM优化时的优缺点。此外，提出构建一个MORL基准框架，以评估不同方法下目标间的关系影响，并展望了基于元策略的MORL研究方向。

Result: 梳理并分析了当前MORL方法在LLM优化上的适用性及不足之处，提出需要更高效且灵活的MORL方法，尤其关注元策略对象的双层学习范式在提升效率和灵活性上的潜力。

Conclusion: 作者明确了MORL对LLM多目标优化的挑战，强调了构建基准框架和发展元策略MORL的重要性，并对未来的提升策略提出关键科学问题。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [44] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: OjaKV通过混合存储与在线低秩压缩，有效提升大模型长文本处理的内存效率和推理准确性，且易于部署，无需微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文能力受KV缓存内存瓶颈限制，现有KV压缩方法不能很好应对数据分布变化的问题。

Method: 结合了混合存储策略和在线主成分分析（Oja算法），通过保留关键token并对中间token进行低秩压缩，且能在推理过程中动态更新压缩子空间。

Result: OjaKV在高压缩率下保持或提升了零样本准确率，尤其在需要复杂推理的长上下文任务上效果显著，且与现代注意力模块兼容。

Conclusion: 提出OjaKV框架，实现了内存高效的长上下文推理，无需模型微调，并能动态适应上下文变化。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [45] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文综述了语言模型领域的可解释AI方法，按三大Transformer架构系统组织并评估了各技术，指出现有方法的优势、劣势及研究难题，为推动语言模型的透明化和可靠性提供理论支撑和未来指引。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）虽极大推动了自然语言处理的发展，但其“黑箱”特性使模型内部机制和决策过程难以解释，尤其在高风险领域，这种透明度缺失影响了模型的信任和问责。因此，需要明确讨论现有可解释AI方法在语言模型上的适用性和局限。

Method: 本文对当前针对语言模型的可解释AI（XAI）技术进行综述，并根据Transformer架构（仅编码器、仅解码器、编码器-解码器）分类，深入分析各类方法的适配过程及其优缺点。此外，通过“合理性”和“可信度”两大视角评价这些技术，最后总结研究难点并未来发展方向。

Result: 组织和系统性地归纳了XAI在三大Transformer语言模型架构上的应用现状，比较了不同技术的适应性及效果，并从合理性与可信度出发进行了评估，指出了当前XAI在语言模型上的主要挑战及研究机遇。

Conclusion: 本文为推动透明和可信赖语言模型的发展提供了全面框架，对各类可解释方法的优劣进行分析，明确了研究痛点和前景，有助于后续XAI领域在语言模型上的突破。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [46] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 该研究通过定义和自动检测错误评审点，并利用大模型提升同行评审质量筛查的自动化水平，验证了自动化引擎的有效性和未来发展前景。


<details>
  <summary>Details</summary>
Motivation: 由于AI会议投稿数量激增，现有同行评审的质量在持续下降，亟须开发有效机制来检测低质量评审。

Method: 作者定义了错误导向的评审点（错误的“弱点”或可由文章回答的问题），并通过自动化引擎重构“弱点”的显性与隐性前提。构建了ReviewScore数据集，并测试了八个先进大模型自动评估ReviewScore的能力及其与人的一致性。还分析了分歧来源。

Result: 统计发现15.2%的评审“弱点”和26.4%的问题属于错误导向。大模型在评审点自动评估与人类专家间达成了中等一致，且前提级别的评审一致性显著高于整体弱点的评估分析。

Conclusion: 通过仔细分析分歧，研究表明开发完全自动化ReviewScore评估引擎具有很大潜力，能有效提升评审质量检验的自动化程度。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [47] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: GRAB基准首次为金融风险披露的无监督主题模型提供大规模、自动化、标准化的数据、标签和评测流程，促进可复现和公平比较。


<details>
  <summary>Details</summary>
Motivation: 10-K风险披露的风险分类对于监管和投资具有重要意义，但目前缺乏针对无监督主题模型在该任务上的公开基准，导致模型评估不统一、难以复现。

Method: 提出了GRAB金融领域专用基准，包含来自8,247份文件、161万句子的无人工标注句级标签。标签生成综合了FinBERT词元关注机制、YAKE关键词信号和结合风险分类体系的搭配匹配，实现高效、自动化标注。该体系将193个风险术语映射到21个细类型，归属5个宏观类别，以此进行弱监督和宏观层面的评估。

Result: GRAB基准统一了金融领域风险话题建模的评估流程和指标，包括准确率、Macro-F1、Topic BERTScore和基于熵的有效主题数。基准提供固定数据集划分和标签，促进各类主题模型（经典、嵌入、神经、混合）在财报风险识别中的可复现性和可比性。

Conclusion: GRAB为金融风险披露主题模型标准化评估提供了首个大规模公开数据和统一框架，有助于推动领域进展和模型公平比较。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [48] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: 论文提出Think-on-Graph 3.0，多智能体协作实现动态图索引进化，有效解决传统RAG构图瓶颈，在多项推理任务上取得明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG和图结构RAG在利用外部知识增强大模型能力上有重要价值，但面临图结构质量、构建成本与模型能力之间的权衡。手工构建知识图谱难以扩展，自动抽取又受限于模型性能，导致实际应用效果有限。

Method: 提出ToG-3框架，引入MACER机制，即多智能体上下文进化与检索系统。其核心是动态构建与优化Chunk-Triplets-Community异构图索引，实现查询和子图的双重进化。系统由四个智能体协作完成证据检索、答案生成、反思与查询/图谱演化，实现针对查询的自适应图索引构建。

Result: ToG-3能克服静态一次性图索引的局限，允许在推理过程中自适应完善图结构，提升轻量化LLM在精准深度推理上的能力。在深推理与广推理基准上的实验证明ToG-3优于现有方法，消融实验也证实MACER框架各部分有效。

Conclusion: ToG-3提出了面向问题的动态图索引与多智能体协作机制，显著提升了RAG方法在知识增强、深度推理等场景下的表现，特别适合轻量化本地模型场景。

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [49] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了ProPerSim仿真框架和ProPerAssistant助手，实现了AI助手主动性与个性化的联合优化，并通过实验证明了其可有效提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型虽然在反应性、主动性和个性化方面分别取得了进展，但主动性与个性化的结合尚未被充分探索。实践中，用户对更加主动且个性化的AI助手有更高需求。

Method: 提出了ProPerSim任务与仿真框架，用于开发能在现实家庭场景中做出主动、个性化推荐的AI助手。在该仿真环境中，用户代理具备详细的人设，并为助手的建议进行评分，助手需根据评分不断学习与调整策略。基于ProPerSim，作者又提出了ProPerAssistant，一种检索增强、偏好对齐、可持续自我学习的助手。

Result: 在包含32种不同人设的实验中，ProPerAssistant能够自适应调整策略，持续提升用户满意度，展现了主动性与个性化结合的潜力。

Conclusion: 联合主动性和个性化的AI助手能更好适应用户需求，ProPerSim和ProPerAssistant为该方向提供了有效工具与初步实证。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [50] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 本文通过针对长语境下多问答任务，系统评估了专有与公开LLM的性能。结果显示，微调后的公开大模型在准确性上有望超越业界最强专有模型，有助于实际应用的成本优化和透明度提升。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，基于冗长上下文进行多轮QA任务时，易受高计算成本和延迟影响；研究如何提升多问题同语境下LLM的实际表现。

Method: 对多种专有及公开LLM进行广泛实验和基准测试，比较其在相同语境下回答多个问题的能力。

Result: GPT-4o等专有模型表现最佳，但经微调的大参数公开模型精度可超越GPT-4o，且部署更透明、更节约成本。

Conclusion: 强大的专有LLM（如GPT-4o）总体表现最佳，但经过微调的公共LLM（最大8B参数）在准确性上可超越GPT-4o，具备透明且高性价比的实际应用潜力。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [51] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 本文针对大型语言模型流式应用推理慢和输出不稳定问题，提出一种新解码策略——自我推断偏置解码。该方法通过利用已有输出作为草稿并在验证阶段进行偏置处理，显著加快推理速度，减少输出闪烁，对模型结构无要求，可即插即用。实验证明，在实时翻译等场景有显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在流式应用（如实时翻译）中推理速度慢，且逐步扩展输入时，需要不断重新生成全部输出，导致计算成本高、延时大，还会让用户看到频繁变化的输出片段。

Method: 采用自我推断偏置解码——每次仅将最近一次输出当作当前输入扩展后的草稿，在“验证”阶段，输出偏向草稿token以提高接受率，减少输出变化。同时引入mask-k技术进一步减少显示变化。

Result: 实验表明，在同时文本翻译重传任务上，该方法相较传统自回归方式提速1.7倍，且引入mask-k技术后可减少80%输出闪烁，且不会损失输出质量。

Conclusion: 本文提出的Self-Speculative Biased Decoding方法无需重复生成草稿，从而加速流式应用的推理过程。在保证翻译质量的前提下，显著提升了生成速度并减少了输出“闪烁”现象。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [52] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了可让音频-语言大模型实时进行音频分析的TwS框架，并通过MELD-Hard1k基准测试验证其大幅提升了模型在复杂环境下的鲁棒性，最高提升达36.61%。该方法无需重新训练，极具实用价值，为音频理解领域指明了新路径。


<details>
  <summary>Details</summary>
Motivation: 虽然当前的大型音频-语言模型（LALMs）在语音翻译和音频问答等任务上表现优异，但在复杂声学环境下的音频推理任务中仍存在显著局限。这类场景中，利用噪声抑制、声源分离和精确时间对齐等声学工具能大大提升表现，但现有模型无法访问这些工具。

Method: 提出了Thinking-with-Sound（TwS）框架，让LALMs结合语言推理能力与即时音频分析（Audio CoT），可动态处理和分析音频信号，实现多模态的数值分析和数字处理。这一框架打破了以往仅将音频作为静态输入的模式。为验证方法效果，构建了MELD-Hard1k基准，加入多种音频扰动测试模型鲁棒性。

Result: 现有先进LALMs在MELD-Hard1k上准确率下降超过50%。TwS显著提升了鲁棒性：小模型准确率提升24.73%，大模型提升达36.61%。

Conclusion: TwS框架有效加强了LALMs的音频鲁棒性，无需重新训练便能实现大幅提升，显示出Audio CoT的积极作用，为更鲁棒的音频理解系统提供了新方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [53] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 本文提出SynerGen生成式推荐模型，用于同时优化个性化搜索与推荐，通过新颖的多任务训练与时间感知编码，显著优于现有强基线，并展示了统一信息访问的新方向。


<details>
  <summary>Details</summary>
Motivation: 当前主流的推荐系统使用“召回-排序”两阶段结构，由于架构分离和优化目标的不同，导致性能不匹配和工程复杂度增加。近年来生成式序列模型在统一召回和排序方面表现出潜力，但大多只能处理个性化搜索或无查询推荐中的其中一种，难以兼顾两者。

Method: 提出了SynerGen模型，通过一个解码器式Transformer，以行为序列为训练数据，联合优化检索（用InfoNCE损失）和排序（混合点对损失）。同时引入了创新的时间感知旋转位置编码，将时间信息融入注意力机制。该模型实现了搜索与推荐任务的统一优化和语义互补。

Result: 在推荐和搜索的标准数据集上，SynerGen显著优于当前强大的生成式推荐模型及联合搜索推荐模型基线。

Conclusion: SynerGen证明了基于单一生成式基础模型实现工业级大规模统一信息访问的可行性。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [54] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 论文采用因果推断方法精细分析结构化输出对大语言模型生成质量的影响，发现用传统指标容易得出误导性结论，而真正的因果影响非常有限，主要与具体指令因素相关。


<details>
  <summary>Details</summary>
Motivation: 目前工业应用广泛采用大语言模型生成结构化输出，以提升信息处理效率，但关于结构化输出对生成质量的影响，学界观点不一，缺乏精确分析。

Method: 作者运用因果推断方法，对结构化输出影响进行精细分析。通过假定与保证的约束，推导出五种结构化输出影响生成的可能因果结构，并在七个公开任务及一个自研任务上开展实证研究。

Result: 粗略评价指标显示结构化输出对GPT-4o生成效果有正面、负面或中性影响，但用因果推断分析后，发现48个情景中有43个不存在因果影响，剩余5个情景中有3个与具体指令相关的复杂因果结构。

Conclusion: 结构化输出对大语言模型生成质量的影响，用粗略指标可能误导，因果推断表明大部分情境下无实质性因果作用，只有极少数情况与指令有关。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [55] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 该论文提出了涵盖多种文化的奖励模型评测基准CARB，并创新性地利用RLVR强化学习提升奖励模型对文化细节的理解和评判能力，有效缓解了对表层特征的依赖，推动了大语言模型的全球文化对齐发展。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型评估缺乏针对文化意识的评价数据集，难以有效评估和提升大语言模型在不同文化背景下的适应能力。提升模型的全球文化对齐水平亟需相关研究。

Method: 提出了Cultural Awareness Reward modeling Benchmark（CARB），涵盖10种不同文化和4个文化领域，系统评估主流奖励模型的文化意识表现；分析奖励模型评分存在的表层特征相关性问题，并提出'Think-as-Locals'方法，结合可验证奖励的强化学习（RLVR），用精心设计的奖励提高模型的偏好判断和结构化评估标准。

Result: 实验证明，CARB能够揭示主流奖励模型在文化意识上的不足，表现与下游多语种文化对齐任务正相关；'Think-as-Locals'方法有效缓解了奖励模型对表层特征的依赖，提升了其文化认知和奖励评判的准确性。

Conclusion: 本工作填补了文化意识奖励模型评估的数据集空白，提出的方法切实增强了大语言模型的文化对齐能力，为模型的全球适应性和公平性提供了新思路。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [56] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文针对同步机器翻译实时性与译质的难题，提出四类自适应动作并在大语言模型中实现，显著提升了语义质量与延迟表现，为机器口译带来新突破。


<details>
  <summary>Details</summary>
Motivation: 传统SiMT方法仅使用READ/WRITE动作，难以在保持高译质同时满足严格实时性。为提升机器口译系统性能，亟需更灵活的译文生成策略。

Method: 将SiMT的动作空间扩展为四种自适应动作：SENTENCE_CUT（句子切分）、DROP（信息删减）、PARTIAL_SUMMARIZATION（部分摘要）和PRONOMINALIZATION（指代化），并在仅用Decoder的大型语言模型框架下实现。通过动作感知提示构建训练参考，用结合延迟感知的文本转语音管线评估译文质量与时延。

Result: 在ACL60/60英中、英德基准上，新方法带来更高的语义指标（如COMET-KIWI）和更低的延迟，相较参考译文和基准方法均有提升。尤其是DROP与SENTENCE_CUT结合时，在流畅度与时延间达最佳平衡。

Conclusion: 扩充LLM-SiMT动作空间能显著提升译文质量与同步性，为弥合机器与人类口译差距提供了新方向。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [57] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 针对多模态语言理解中的泛化问题，论文提出了因果信息瓶颈方法CaMIB，通过去噪、因果/捷径分离及背门调整，显著提升了模型在分布外任务上的表现和因果解释能力。


<details>
  <summary>Details</summary>
Motivation: 当前人类多模态语言理解（MLU）方法容易受到数据集偏差影响，导致模型容易将统计捷径误认为真正因果特征，致使模型在分布外（OOD）泛化能力较差。因此亟需能提升泛化性的方法。

Method: 提出了因果多模态信息瓶颈（CaMIB）模型。该方法首先通过信息瓶颈过滤各模态输入，去除与任务无关的噪声。然后，借助参数化掩码生成器，将融合后的多模态表征拆分为因果和捷径子表征。为保证因果特征的全局一致性，模型引入工具变量约束，同时采用背门调整，通过随机重组因果和捷径特征来稳定因果估计。

Result: 在多模态情感分析、幽默检测和讽刺检测等任务，以及OOD测试集上，CaMIB模型取得了显著的效果提升。理论与实证分析也展示了其可解释性与合理性。

Conclusion: CaMIB基于因果原理，有效提升了多模态语言理解任务中的分布外泛化性能，具备良好的可解释性。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [58] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在解决和生成语言谜题上的应用，证明其优于人类，且能够自动生成有助于语言学普及的谜题，尤其对宣传冷僻语言具有积极意义。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在扩展语言奥林匹克竞赛中的语言谜题解决与生成任务，突破现有人类及机器在语言谜题处理上的局限，推动语言学普及，尤其是对冷僻语言的传播。

Method: 首先扩展了现有语言谜题解决基准，随后使用多个大型语言模型（LLM，包括OpenAI的最新模型o1）对各类语言谜题进行求解，并对其在不同语言主题上的表现进行系统性分析。通过谜题求解实验，进一步探索自动化生成谜题的可行性和潜力。

Result: 结果显示，大型语言模型在绝大多数语言谜题类型上均优于人类，仅在书写系统相关及冷僻语言谜题上略显不足。同时，自动化生成谜题具有拓展语言学兴趣和普及的巨大潜力，尤其可以助力稀有语言的知识传播。

Conclusion: LLMs可以高效解决并生成语言谜题，有望成为语言学教育和传播的新工具，尤其在推广冷僻语言领域更具研究和应用价值。未来可进一步完善书写系统和冷门语言相关能力。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [59] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 本文提出了一种针对工具使用任务、基于熵感知token重加权的新型策略优化方法ResT，有效解决了工具型任务中训练不稳定和效率低的问题，并在多个任务上超越了主流方法和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）不仅能够被动生成文本，还能通过调用外部工具实现目标导向的行为。当前使用强化学习优化工具使用策略时，普遍依赖稀疏回报，忽略工具任务的特殊性，导致策略梯度方差增大，从而训练效率低下。为此，作者希望理解并解决这些挑战。

Method: 论文首先建立了策略熵与工具使用任务训练稳定性的理论联系，发现结构化、低熵的token是奖励的主要决定因素。在此基础上，提出Reshaped Token-level policy gradients（ResT），通过基于熵的信息调整token权重，随着训练进程逐步提升推理相关token的权重要，实现结构正确性到语义推理的平滑过渡，并提升多轮任务训练收敛性。

Result: 在BFCL和API-Bank任务上的评测显示，ResT取得了最高水平结果，最高优于以往方法8.76%。在4B参数的基础LLM微调下，单轮任务超过GPT-4o 4.11%，多轮基础任务超过1.50%。

Conclusion: 通过熵感知的token重加权策略（ResT），可以显著提高LLM工具使用任务的训练效率和性能，实现更稳定收敛，更好激发推理能力。

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [60] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 本文提出利用不同模型输出的语义一致性作为级联调用判断信号，实现显著降本提效且不依赖额外训练和模型细节，在多规模模型实验中效果优异，适合现实LLM应用场景。


<details>
  <summary>Details</summary>
Motivation: 级联系统在大模型部署中能有效权衡成本与质量，但在开放性文本生成时，难以判断不同模型输出的可靠性，尤其是答案多样且质量连续时。

Method: 提出了“语义一致性”的指标，通过模型输出间的语义共识，作为无需额外训练的可靠性判断信号，实现更合理的模型级联。此方法不依赖模型内部细节，无需微调，适用于黑盒API及模型迭代。

Result: 语义级联方法在500M到70B参数规模的模型实验中表现突出，能以40%成本达到或超过目标模型输出质量，延时降低高达60%。

Conclusion: 语义一致性信号为LLM部署中的级联系统提供了高效、实用、鲁棒的可靠性判据，有望成为实际部署的基线方法。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [61] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: 针对同理响应生成任务，作者提出了分析与生成结合的新框架TRACE，通过结构化分解显著提升了对话代理的同理性和表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 同理响应生成对于构建更类人且具备支持性的对话智能体至关重要，但现有方法在专业模型的分析深度与大语言模型（LLMs）的流畅生成之间存在权衡。

Method: 提出TRACE框架，将同理心建模为结构化认知过程，通过将任务分解为分析和生成的流水线，先进行全面理解再生成回应，实现分析与表达的结合。

Result: TRACE在自动评测及基于LLM的评测中均显著优于强基线，验证了结构化分解这一新范式在同理智能体构建方面的潜力。

Conclusion: 结构化的任务分解方法可提升同理对话代理的能力和可解释性。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [62] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了首个针对知识密集型多轮长文本问答（MT-LFQA）的系统评测基准KnowMT-Bench，证实多轮场景下大语言模型回答的事实性及信息效率均下降，检索增强生成可显著改善该问题。该基准有助于评估和优化模型在实际应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在知识密集型领域的多轮长文本问答（MT-LFQA）应用较为重要，但现有基准仅支持单轮对话或评测非知识相关能力，缺乏系统性评估多轮知识密集型问答的方法。该研究旨在填补这一评测空白。

Method: 作者提出了KnowMT-Bench，这是首个专门用于系统性评估LLMs在医学、金融和法律等知识密集型领域多轮长文本问答能力的基准。基准采用动态评估：模型需自生成逻辑递进的多轮问答历史，最后一轮回答通过人工验证的自动化流程，评估其事实性和信息传递效率。

Result: 实验结果显示，随着多轮对话进展，模型的事实性能力降低（受自生成历史的噪声干扰），信息效率也下降（回答变得冗长）。通过检索增强生成（RAG）方法，可以有效缓解甚至逆转事实性能力的下降。

Conclusion: KnowMT-Bench基准对于评估和提升大语言模型在真实知识密集型多轮问答中的事实性非常重要，检索增强生成（RAG）是提升模型多轮对话性能的关键策略。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [63] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: 本文提出LoRAN，通过非线性扩展LoRA提升大模型微调表达能力，并引入Sinter正弦激活函数，在多任务上均优于现有方法，激活函数设计对低秩微调至关重要。


<details>
  <summary>Details</summary>
Motivation: LoRA作为一种高效参数微调方法，普遍用于大语言模型，但其线性特性限制了表达能力。作者希望解决LoRA表达能力不足的问题。

Method: 提出了LoRAN，一种对LoRA进行非线性拓展的新方法，通过轻量级转换增强低秩更新表达能力。同时，提出了一种新激活函数Sinter（基于正弦），用于在不增加参数量的前提下加入结构化扰动。

Result: 在摘要与分类任务实验中，LoRAN在表现上优于QLoRA。消融实验显示，Sinter激活函数比Sigmoid、ReLU及Tanh等标准激活函数性能更优，说明激活函数设计对低秩微调非常重要。

Conclusion: LoRAN和Sinter显著提升了低秩微调方法的效果，尤其在参数高效微调场景下具有潜力，突出非线性和激活函数选择的重要性。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [64] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: LUMINA通过双信号量化与统计验证，能高效、稳健地检测RAG系统中的幻觉，且性能优于现有方法，在实际应用中更具通用性与便利性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG方法通过检索外部文档加强LLM回答，降低幻觉，但即使给出准确且充分的上下文，RAG系统仍会产生幻觉。模型对外部上下文与内部知识的利用存在失衡，现有幻觉检测方法普遍依赖大量超参数调整，泛化性有限。

Method: 提出LUMINA框架，通过量化外部上下文和内部知识信号来检测RAG系统中的幻觉。外部上下文利用采用分布距离衡量，内部知识利用追踪预测token在Transformer层间的演变。还引入了统计验证框架保障测量可靠性。

Result: 在主流RAG幻觉检测基准和4种开源LLM上测试，LUMINA在AUROC和AUPRC得分上均优于先前方法。在HalluRAG数据集上AUROC提升最高达13%。LUMINA对检索质量和模型匹配的假设更宽松下也表现稳健，兼具效果与实用性。

Conclusion: LUMINA为RAG系统幻觉检测提供了更准确、稳健且通用的方法，显著改善了以往方法的局限。

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [65] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出RL-ZVP算法，突破性地从以前被忽视的零方差提示中提取有效学习信号，显著提升了LLM在数学推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前用于提升大型语言模型推理能力的RLVR方法忽略了零方差提示（即模型各种回答得分一致的提示），认为它们对优化无用。作者认为这种提示依然能够提供有意义的反馈。

Method: 提出了一种新算法RL-ZVP，能够从零方差提示中提取学习信号。该算法即便在没有对比回答的情况下，也能对正确和错误直接进行奖励和惩罚，并利用token级特征调节反馈，以保留细致、信息丰富的信号。

Result: 在六个数学推理基准测试中，RL-ZVP比GRPO和其他忽略零方差提示的基线方法表现更好，在准确率和通过率上分别提升最高8.61和7.77个百分点。

Conclusion: 从零方差提示中学习对于提升RLVR框架下模型性能具有巨大潜力，RL-ZVP证明了这些被忽略的数据包含可用的学习信号。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [66] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出QoNext框架，将QoE理念应用于基础模型评估，综合考虑用户互动体验，通过实验数据和预测模型实现更全面、实用的模型优化和评估指导。


<details>
  <summary>Details</summary>
Motivation: 现有对基础模型的评估（包括以人为中心的方法）无法真实反映用户的互动体验，只关注输出正确性，而忽视了用户满意度是回复质量和互动过程共同作用的结果。

Method: 提出QoNext框架，引入网络与多媒体的体验质量（QoE）理念，通过控制性实验收集多个配置下的用户评分，建立数据库，并训练预测模型根据可测参数预测用户体验。

Result: QoNext框架能实现更主动和细粒度的基础模型评估，并为产品服务优化基础模型提供可行性的指导。

Conclusion: QoNext有效填补了评估基础模型时对用户体验机制认知的空白，显著提升评估准确性和可操作性。

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [67] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文发现MoE模型推理时盲目增加激活专家数会导致性能下降，提出EMoE训练框架增强专家协作性，显著扩展推理阶段专家数调节的有效范围，并提高模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型在推理时激活更多专家（增加k'）未必带来更好性能，实际上很快出现性能下降，原因是缺乏专家间的协作。需要一种新方法支持推理时自由扩展激活专家数且保持高性能。

Method: 提出了一种新的Elastic Mixture-of-Experts (EMoE)训练框架，通过同时训练专家在多样组合下的协作能力，以及改进分配器选择质量，提升模型在不同计算预算下推理时的表现。

Result: EMoE在多种MoE设定下进行实验，结果显示其能将有效扩展范围提升2-3倍，并将模型性能峰值进一步提升。

Conclusion: EMoE显著拓展了推理时激活专家数量的有效扩展范围，并提升了模型的性能峰值。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [68] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本文针对土耳其语引文意图识别提出了公开数据集和高效分类方法。通过自动优化提示和集成学习，分类准确率达91.3%，为相关研究提供了坚实基础。


<details>
  <summary>Details</summary>
Motivation: 在学术研究评价中，理解引文的意图至关重要，但对于像土耳其语这样的黏着语，识别引文意图具有挑战性。本文旨在解决土耳其语引文意图识别的问题。

Method: 本文提出了系统化的方法论，包括开发注释工具以构建公开的土耳其语引文意图数据集。采用大语言模型的In-Context Learning（ICL）进行评估，并通过DSPy框架的可编程分类流程优化提示，最终利用堆叠泛化集成方法（Stacked Generalization Ensemble）和XGBoost元模型进行最终分类。

Result: 提出的集成分类方法在分类土耳其语引文意图任务上取得了91.3%的最高准确率，数据集和工具已公开。

Conclusion: 本文为土耳其语NLP和学术界提供了首个基础数据集与稳健的分类框架，有助于未来的定性引文研究。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [69] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: 该论文提出了结构化多智能体评分新框架AutoSCORE，有效解决了大模型自动评分准确率和解释性等难题，在多个数据集和模型上的实验均获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有利用大模型进行自动评分存在准确率低、对提示敏感、解释性不足和与评分标准不一致的问题，影响了其在教育评估中的实际应用。

Method: 提出AutoSCORE，一个多智能体语言模型框架，包括两个代理：一个负责从学生答案中提取评分标准相关成分并编码为结构化表示，另一个利用这些结构化信息进行最终打分。该方法模拟人工评分的流程，以提升解释性和鲁棒性。

Result: 在ASAP的四个数据集上进行实验，采用GPT-4o和LLaMA-3.1系列模型，AutoSCORE在准确率、人机一致性和误差指标上均优于单智能体方法，尤其在复杂评分标准和小模型上提升显著。

Conclusion: 通过结构化成分识别和多智能体设计，AutoSCORE实现了高效、可靠、可解释的自动评分，对教育评估具有实用价值。

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [70] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: 该论文提出SimulSense框架，通过模仿人译员决策显著提升同声传译系统速度和效率，且无需特殊训练数据或昂贵推理，在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的同声传译系统在做读写决策时，依赖复杂的多回合对话形式，需要专门的交错训练数据，并且用大语言模型进行推理，计算成本高。研究者希望提出更高效、更贴近人类译员的自动决策方法。

Method: 提出SimulSense框架，通过持续读取输入语音，并在感知到新的语义单元时做翻译决策，模拟人类译员的决策过程，而不是依赖昂贵的大模型或特殊数据。

Result: 实验结果表明，SimulSense在质量与延迟的权衡上优于两种现有基线，并且实时效率显著提升，决策速度比基线快最多9.6倍。

Conclusion: SimulSense有效提升了同声传译系统的决策速度和效率，同时保证了翻译质量和延迟的平衡。

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [71] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 论文系统考察了链式思维(CoT)在95个LLM对87个临床任务上的作用，发现大多数模型采用CoT时效果反而变差。CoT虽然提升了解释性，却可能降低了可靠性，提示临床应用需更审慎设计LLM推理策略。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维(CoT)在多任务上提升了大型语言模型的表现与可解释性，但其在临床领域，尤其是对电子健康记录(EHR)的应用，效果尚未被系统性研究。由于EHR文档通常长篇且碎片化，探究CoT在真实临床语境中的有效性十分关键。

Method: 作者设计了首个针对临床文本理解的CoT大规模系统性研究，评估了95个先进LLM在87个真实临床文本任务上的表现，任务涵盖9种语言、8类任务类型。采用LLM自身评判与临床专家评估，进行推理长度、医学概念对齐及错误分析。

Result: 86.3%的模型在使用CoT策略时，性能出现持续下滑，强模型较为稳健，弱模型则显著下降。细致分析表明，CoT在提升解释性的同时，可能削弱在临床文本任务中的可靠性。

Conclusion: CoT在临床文本理解中的效用存在悖论，其能增强解释性但可能降低结果可靠性。研究为LLM临床推理策略提供了首次大规模实证依据，强调需发展既透明又可信的解决方案。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [72] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 该论文针对泰国政治立场检测提出了ThaiFACTUAL无须微调的偏见校准框架，通过反事实增强与推理监督，有效减轻了大模型的情感泄露及偏向实体的问题，并首次发布相关高质量数据集，实验验证效果显著，提升了模型在低资源和复杂文化背景下的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 泰国语言中的政治立场检测因间接语言、极化人物和情感与立场纠缠而复杂，现有大模型易出现系统性偏见，影响公平性和可靠性。

Method: 该方法通过反事实数据增强与基于推理的监督，无需微调，解耦情感与立场，并减少政治偏见。

Result: 提出了ThaiFACTUAL校准框架并发布高质量泰国政治立场数据集，实验证明该框架能有效增强多种大模型的公平性和泛化能力。

Conclusion: ThaiFACTUAL显著减少了大语言模型在泰国政治立场检测中的偏见，提高了零样本泛化能力和公平性。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [73] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出MotivGraph-SoIQ框架，通过知识图和苏格拉底双代理系统，显著提升LLM创新想法的动机根基与质量，在多项评价中超过现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在学术创新方面有潜力，但在理念落地和消除确认偏差方面存在挑战。需要一种系统方法来为LLM产生的想法提供动机基础，并促进更高质量的创新。

Method: 提出MotivGraph-SoIQ框架，将动机知识图（MotivGraph）与苏格拉底式问答（Q-Driven Socratic Ideator）相结合。知识图存储问题、挑战、解决方案三类节点，为想法提供动机根基；双代理系统通过苏格拉底式提问，实现确认偏差的减少和思想的多维度完善。

Result: 在ICLR25论文主题数据集上，MotivGraph-SoIQ在LLM评分、ELO排名、人类评价等多项指标上，比现有先进方法表现更优。

Conclusion: MotivGraph-SoIQ显著提升了LLM在学术创新中的落地性和想法质量，解决了落地和确认偏差问题，为学术理念生成和完善提出了更有效的新框架。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [74] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出一种基于不确定性表达的黑盒幻觉检测方法，无需依赖外部或模型内部信息，在检测LLMs非事实性回答时准确性高于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）如GPT3尽管取得了长足进步，但仍会生成“幻想”型非事实性回答，现有检测与缓解方法普遍依赖外部资源或模型内部状态，实用性受限。因此，迫切需要一种不依赖这些资源的黑盒方法。

Method: 本文通过分析LLMs在表达不确定性时的行为，提出了一种简单的黑盒幻觉检测指标，无需访问模型内部状态或依赖外部知识，仅通过模型输出表现进行评估。

Result: 实验表明，该黑盒指标在预测模型回答的事实性方面优于那些依赖LLMs内部知识的基线方法。

Conclusion: 提出的黑盒指标能够高效且有效地检测LLMs的“幻觉”问题，有助于实际应用对模型输出的可靠性把控。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [75] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: GraphSearch通过模块化的多轮推理框架和双通道检索策略，解决了GraphRAG的检索不全和数据利用低效问题，在多个多跳基准上显著提升了生成准确率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG（图结构增强检索生成）方法存在两个主要问题：一是检索过程较为浅层，无法获取全部关键证据；二是对预先构建的结构化图数据利用效率低，导致复杂查询推理效果不佳。本文旨在解决上述问题。

Method: 提出了GraphSearch，这是一种具有代理特性的深度搜索流程，采用双通道检索策略。其检索过程被组织成六个模块，支持多轮交互与迭代推理。双通道检索策略分别对文本数据和结构化图数据进行语义和关系查询，实现两种数据优势的互补。

Result: 在六个多跳RAG基准数据集上的实验结果表明，GraphSearch在答案准确率和生成质量上均优于传统方法。

Conclusion: GraphSearch作为一种新型深度图结构检索生成方法，能够充分发挥图结构和文本数据的互补优势，有效提升复杂检索生成的推理能力。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [76] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 离群点非噪声，能预示新兴主题，累积聚类和语义嵌入追踪表明其最终演变为热点话题。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模常将离群点视为噪声，但这些离群点可能预示着新主题的出现，故研究其动态演化价值。

Method: 利用先进语言模型的向量嵌入与累积聚类方法，追踪离群点在不同语料与时间段中的变化。

Result: 法语和英语新闻数据实验显示，离群点随着时间推移持续演化，最终转变为具有一致性的主题模式，结果在模型和语言间均具一致性。

Conclusion: 离群点在动态新闻语料中能从微弱信号逐步演变成重要的新兴主题。

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [77] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一个细粒度的21类安全分类体系TACOS，用于提升临床对话机器人的安全性，实验证明该方法对安全过滤和工具选择均有改进效果，适用于广泛的医疗和非医疗场景。


<details>
  <summary>Details</summary>
Motivation: 现有的临床对话机器人安全机制如安全防线和工具调用，对于临床领域的复杂需求支持不足，存在不准确甚至有害回答的风险，因此亟需更精细、专门化的安全保障方法。

Method: 本文提出了一种21类细粒度的安全性分类体系TACOS，将安全过滤与工具选择整合为单一的用户意图分类步骤。通过构建TACOS标注数据集，并进行大量实验进行验证。

Result: 实验结果表明，TACOS体系提升了分类和决策的准确性，有助于改善模型对训练数据分布和预训练知识的利用，特别针对临床智能体安全需求展现了突出效果。

Conclusion: TACOS能有效提升临床对话机器人在安全性和工具调用方面的表现，对不同类型的临床与非临床查询具有适用性。该方法为临床智能体领域提供了细致高效的用户意图识别方案。

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [78] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: 该文提出FRC框架，将概率推理与模糊推理结合，有效应对含糊文本，在情感分析上提升了模型可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理在面对含糊、歧义或不确定性文本时仍存在挑战。

Method: 提出Fuzzy Reasoning Chain (FRC) 框架，将大模型的语义先验与连续模糊隶属度结合，实现概率推理与模糊推理交互。

Result: 在情感分析任务上，通过理论分析和实证实验验证了该方法，结果表现出稳定推理，并支持跨模型规模的知识迁移。

Conclusion: FRC为处理细微和模糊表达提供了通用且具有可解释性和鲁棒性的机制。

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [79] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 论文发布了首个包含5年数据的社交媒体AIGT研究数据集，并提出了基于心理语言学的可解释检测框架，有效提升了AI文本检测和对用户互动理解。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上由LLM催生的AI文本日益增多，内容随用户互动不断变化，但现有数据集只支持静态检测，缺乏对动态和长期演化的分析手段。

Method: 提出并应用了一种可解释的心理语言AIGT检测框架（PLAD），利用心理语言学特征分析区分AI和人类生成内容，同时结合社交媒体用户互动数据进行纵向研究。

Result: PLAD框架在检测AIGT上优于现有方法，并能解释各种心理语言特征与用户互动之间的复杂关系。提出的RedNote-Vibe数据集可用于长期追踪AI文本在社交平台的演变。

Conclusion: 该论文提出的PLAD框架在社交媒体AIGT检测上表现优异，并揭示了语言特征与用户互动的复杂关系。研究还表明，某些心理语言特征可有效区分人类与AI生成内容。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [80] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: NLP评估缺乏统一标准导致结果不可比，本文提出QCET分类体系，系统标准化质量评估，解决了跨实验对比难题，并有助于评估设计和合规。


<details>
  <summary>Details</summary>
Motivation: 由于NLP领域在同一质量标准名字下的不同评估实验其实未必评价了同一质量维度，导致跨实验结果不可比，这阻碍了领域的科学进步。需要建立统一标准以解决这一问题。

Method: 采用严格描述性方法，通过三项调查总结出标准化的质量标准名称及定义，并提出一个“质量标准评估分类体系”（QCET），将标准按层级结构组织，每一父节点涵盖子节点的共性。

Result: 提出了QCET分类法和相关资源，展示了其三大用途：1）建立不同评估之间的可比性；2）指导新评估的设计；3）用于监管合规性评估。

Conclusion: 通过QCET标准化体系，有助于消除NLP评估实验间的可比性困惑，推动领域科学发展。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [81] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 过去认为微调不适合模型编辑，但事实上，采用合适的优化流程和参数位置后，微调能显著提升编辑效果。新方法LocFT-BF实现了大规模、高性能的模型编辑，改写了领域认知。


<details>
  <summary>Details</summary>
Motivation: 微调一直被认为在模型编辑方面效果不佳，本文质疑这种观点，认为过去的失败更多源于任务适应方式而非微调本身的局限性。

Method: 通过控制实验，将微调恢复为标准的广度优先（即基于epoch的）流水线，并采用mini-batch优化。同时，系统性分析微调参数位置，提出LocFT-BF方法，实现局部化编辑。

Result: LocFT-BF方法在多种大型语言模型和数据集上均显著优于现有最先进方法。首次实现支持10万条编辑和72B参数的模型，性能远超之前，且不影响模型的通用能力。

Conclusion: 本文纠正了长期以来对微调的不当认识，并提出了高效的局部微调方案，使微调成为模型编辑领域的领先方法，奠定了坚实的研究基础。

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [82] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: CoSpaDi是一种基于稀疏字典学习的模型压缩新方法，能在不损失模型准确性的情况下显著减少参数量，优于传统低秩方法，并支持高效部署和进一步量化。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）的压缩主要依赖于低秩分解方法，但这种方法的结构约束较为刚性，容易导致模型准确率显著下降。为实现更高效的压缩且能提高模型表现，需要探索更灵活的结构逼近方案。

Method: 提出了一种新的无训练压缩框架 CoSpaDi（Compression via Sparse Dictionary Learning，稀疏字典学习压缩）。该方法通过稠密字典和列稀疏系数矩阵，实现对权重矩阵的结构化稀疏分解，使不同列可以用不同的子空间来逼近。同时利用小规模校准数据集优化分解过程，使压缩后层的输出激活与原始层尽量一致，从而减少功能重建误差，而不仅仅是权重逼近误差。此外，该方法的结构化稀疏性支持高效的稀疏-稠密矩阵乘法，并可与后训练量化兼容，以进一步提升内存与延迟性能。

Result: 在多个 Llama 和 Qwen 模型、不同分层和分组设置下（压缩率为 20-50%），CoSpaDi 在准确率和困惑度上均优于当前最先进的数据感知低秩方法。表现出良好的模型保真度和压缩效果，并支持更高效的部署。

Conclusion: 结构化稀疏字典学习成为传统低秩压缩之外，执行高效 LLM 部署的强大替代方法。CoSpaDi 实现了无需微调的优越压缩性能，不仅提高了模型准确率，还兼顾了执行效率和内存占用。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [83] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出了DAS结构化框架，用以生成多语言、本地化且文化适应性强的对话，效果优于传统翻译方式。


<details>
  <summary>Details</summary>
Motivation: 现有的非英语对话数据集稀缺，研究通常将英文对话翻译成其他语言进行训练和评估，这种方法会引入降低自然性与文化适配性的瑕疵。

Method: 提出了一种称为Dialogue Act Script（DAS）的结构化框架，用以从抽象意图表示出发，编码、本地化及生成多语言对话。该方法通过对话行为的结构化表示，灵活支持多语言本地化，避免直接翻译带来的生硬和文化不适。

Result: 在人类评估中，DAS生成的意大利语、德语和中文对话在文化相关性、连贯性与情境适应性方面，持续优于机器及人工翻译生成的对话。

Conclusion: DAS框架能够以更自然和适应目标文化需求的方式生成多语言对话，显著提升对话系统在多语言环境下的表现。

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [84] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: 论文发现现有奖励模型在“能解决问题却评判错误”的现象普遍存在（solve-to-judge gap）。提出S2J方法，把解决问题能力和判断能力显性结合优化，显著降低gap、增强评判性能，实现小数据集上的SOTA，同时不用依赖外部更强模型。


<details>
  <summary>Details</summary>
Motivation: 以往奖励模型主要通过偏好数据训练，但发现即使模型能解决问题，在判断上仍有较大比例出现错误，影响评估和泛化。文章旨在解决模型明明会做却评判不准的实际困境。

Method: 提出Solve-to-Judge（S2J）方法，通过同时利用模型的解决问题与评判能力，将两者显性连接起来进行监督优化，从而缩小solve-to-judge gap。该方法无需外部更强模型，仅靠自进化实现提升。

Result: S2J方法使solve-to-judge gap降低16.2%，模型评判性能提升5.8%；在同样基础模型和更少数据下，S2J获得了SOTA效果。并且方法自洽，无需外部蒸馏。

Conclusion: S2J方法显著缩小了GRM在能够解决问题但判断错误（solve-to-judge gap）的比例，提升了模型的评判能力，并实现了更优的性能。

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [85] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 本论文提出针对复杂数值型事实核查任务，利用多路径推理和训练的验证模型选优，并通过自适应机制提升测试效率，从而显著改善大语言模型的推理准确性和算力利用率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂的数值型事实核查任务上表现欠佳，特别是在需要组合推理和数值推理时易发生推理漂移，难以正确解读数值细节。作者旨在提升LLM在复杂事实核查中的推理准确性和效率。

Method: 系统性地扩展测试时算力（TTS），产生多条推理路径，训练一个验证器模型在推理路径空间中选择可达到正确结论的路径。同时提出了一种自适应机制，根据声明的复杂性选择性地进行TTS，以提升计算效率。

Result: 自适应TTS机制在保持高准确性的同时，提升了1.8倍计算效率，并在准确率上比单次验证方法提升了18.8%。

Conclusion: 通过在推理过程中采用多路径推理，并用训练的验证模型（VERIFIERFC）选择最优路径，能够有效提升大型语言模型对于复杂数值型事实核查任务的表现，尤其能缓解推理漂移问题。

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [86] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 面向法律条文预测，提出了融合有监督模型和大语言模型的Uni-LAP框架，能够更准确且普适地预测相关法律条文，实验证明其优于既有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的法律条文预测方法难以兼顾复杂案情理解和法律制度差异，传统模型和大模型各有局限，缺乏通用性与高精度方案。

Method: 将增强型有监督分类模型（SCM，带Top-K损失）与大型语言模型（LLM，采用三段论推理）结合，实现候选条文筛选与最终推理的协作。

Result: 在多个法域数据集上，Uni-LAP表现优于主流基线方法，证明其优越的效果与广泛适用性。

Conclusion: 提出的Uni-LAP框架能够有效提升法律条文预测的准确率和通用性，优于现有方法。

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [87] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文综述了多语言视觉-语言模型发展现状，梳理了语言中立性和文化意识的权衡，指出训练和评测手段间存在的差距，并提出改善方向。


<details>
  <summary>Details</summary>
Motivation: 调研多语言视觉-语言模型在处理跨语言文本和图像时面临的主要挑战，尤其关注语言中立性与文化意识之间的权衡。

Method: 回顾了31种模型和21个基准测试，包括仅编码器模型和生成式架构，综合分析当前主流训练方法（如对比学习）及其在文化适应性和语言中立性表现上的差异。

Result: 现有训练方法更倾向于通过对比学习实现语言中立性，而达到文化意识则依赖于多样化数据。同时，约三分之二的评测基准采用基于翻译的方法，优先考虑语义一致性，但最新工作开始纳入文化相关内容。分析揭示了跨语言能力上的不一致，以及训练目标和评估目标之间存在的差距。

Conclusion: 多语言视觉-语言模型在实现语言中立性和文化意识间存在关键矛盾。训练方法、评测基准和目标之间需进一步协调，以提升模型跨语言与跨文化的泛化能力。

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [88] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: 作者提出并开源了FoodSEM——一个针对食品命名实体链接任务的先进微调大语言模型，在多个本体和数据集上表现优异，显著超过现有模型，并公开了相关语料和模型资源。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大型语言模型和定制领域模型在食品相关命名实体链接（NEL）任务上的表现不佳，缺乏精确且适用于食品本体的自动化解决方案。

Method: 提出FoodSEM模型，通过微调大型语言模型，并采用指令-响应(IR)情境，实现食品实体到多个本体（如FoodOn、SNOMED-CT、Hansard taxonomy）的自动链接。同时发布食物标注语料用于微调和评估。

Result: FoodSEM模型在相关本体和数据集上获得了最高的F1分数，部分指标高达98%。对比实验表明，FoodSEM在zero-shot、one-shot及few-shot设定下显著优于未微调版本和其他基线模型。

Conclusion: FoodSEM显著提升了食品领域文本的语义理解能力，并为后续食品NEL任务提供了强基线和公开资源。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [89] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 作者提出了R-Capsule推理框架，有效压缩和表示推理过程，提升大模型在复杂推理任务中的效率与可解释性，同时保证或提升准确率。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT) 提示提升了大语言模型 (LLMs) 的复杂推理能力，但其冗长性带来了延迟和内存占用，并且容易因早期错误导致后续推理连锁失误。因此需要一种方法兼顾高效与可解释性。

Method: 提出了 Reasoning Capsule (R-Capsule) 框架，通过将高层次推理计划压缩为少量学习到的潜变量（Reasoning Capsule），结合轻量化执行步骤，借鉴了信息瓶颈（IB）原理，设立最小化和充足性双重目标，并采用重建损失稳定潜空间。

Result: 该方法在减少推理token数量的同时，在复杂基准上维持或提升了推理准确性。

Conclusion: R-Capsule 平衡了推理的效率、准确性和可解释性，是对现有CoT提示法的一种有效改进。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [90] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: 论文针对speculative decoding中的训练与实际推理策略不一致问题，提出了Group Tree Optimization（GTO）方法，在多任务和多模型上均获得了推理效率提升，为LLM的高效推理提供了新的通用优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有的speculative decoding方法用于加速LLM推理，但训练目标仅优化单一路径，而实际解码时间采用树策略，多分支重排列验证，导致训练与推理策略不一致，限制了加速效果。

Method: 提出了Group Tree Optimization (GTO)方法，包括两个核心组件：（1）Draft Tree Reward，采样无关、直接衡量草稿树下目标模型的期望接受长度；（2）Group-based Draft Policy Training，借鉴PPO优化，对当前及冻结参考草稿模型生成的树进行对比，规范化优势，并沿最长接受序列更新策略。理论上证明Draft Tree Reward提升可增加接受长度和推理速度。

Result: 在对话（MT-Bench）、代码（HumanEval）、数学（GSM8K）等任务，以及多种主流LLM模型上，GTO方法将接受长度提高了7.4%，在现有EAGLE-3方法基础上实现了额外7.7%的速度提升。

Conclusion: GTO方法有效弥合了草稿策略与推理过程的不一致性，为高效LLM推理提供了实用且通用的解决方案。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [91] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文综述了NFDI4DS联盟主办的12项学术文档处理共享任务，强调其推动FAIR原则和研究透明性，并为社区贡献了大量可复用的开放数据和工具。


<details>
  <summary>Details</summary>
Motivation: 推动学术研究的进步和加速社区标准化评估流程，强调数据和研究的FAIR（可查找、可访问、可互操作、可复用）原则，以及促进透明和可重复的研究实践。

Method: 概述并介绍德国国家研究数据基础设施（NFDI4DS）联盟下开发和主办的12项共享任务，这些任务涵盖学术文档处理领域的多样化挑战。通过在顶级会议举办任务，提供开放获取的数据集、模型和工具，并将其集成到研究数据基础设施中。

Result: 在学术文档处理领域推动了方法创新，为研究社区贡献了丰富的开放数据资源、模型和工具，并强化了研究数据共享和再利用能力。

Conclusion: NFDI4DS举办的共享任务能有效促进FAIR和可重复性的研究实践，丰富了社区的工具与数据资源，对学术文档处理有重大推动作用。

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [92] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: MACC方法通过多轮自适应压缩，有效提升了链式推理的准确率并大幅减少推理耗时，验证压缩有效且表现可预测，无需重复微调，多模型适用。


<details>
  <summary>Details</summary>
Motivation: CoT推理能提升复杂任务表现，但造成显著的推理延迟。为兼顾精度与效率，亟需找到压缩CoT表达且不影响推理质量的有效方法。

Method: 提出MACC（多轮自适应链式推理压缩）框架，利用token弹性现象，通过多轮迭代逐渐优化、压缩CoT，结合可解释特征如困惑度与压缩率预测模型在测试时的表现，无需反复微调即可实现高效模型选择。

Result: MACC在多个模型和任务评测中相较最新基线平均提升5.6%准确率，并平均减少47 tokens，显著降低延迟；测试时只需通过简单特征（如困惑度、压缩率）即可可靠预测模型表现，无需重复微调。

Conclusion: 原链式思维(CoT)推理虽有助于复杂任务，但推理过程冗长、延迟高。MACC方法能有效压缩CoT表达，在不损失精度的同时显著缩短推理时间，实现了高效且可预测的推理优化。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [93] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 本论文针对人类与机器生成文本的区分问题，提出了多检测场景方法和BMAS English数据集，助力更加精准和多角度的机器文本检测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）发展，机器创造力有可能超越人类，引发了关于人类原创性和创造力保护的问题。如何检测并区分机器生成的文本、维护人类工作真实性，成为亟需研究的重要方向。

Method: 论文设计了多种机器生成文本检测任务，包括文档级二分类和多分类（识别机器文本及其生成器）、句子级分割（判别人机协作文本片段的边界），以及对抗性攻击分析（降低机器文本可检测性）。并提出了BMAS English数据集，支持各类检测任务。

Result: BMAS English为英文学术界提供了机器与人类文本二分类、多分类（包含生成器识别）、对抗检测和句子级分割等多种检测场景的数据支持，有利于推动现有的机器生成文本检测研究朝更精细和多元化方向发展。

Conclusion: 论文系统性提出了上述检测任务和数据集，有望提升对机器生成文本的识别精度，保护人类创造力，对相关领域具有重要意义。

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [94] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文提出Compositional Adapter Generation（CompAs），用可组合的adapter参数高效整合多段上下文，提升大模型多信息处理能力，兼顾效率、安全性与扩展性，在多输入任务中优于传统解法。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL和SFT存在效率低下、灵活性不足等局限，且以往基于adapter生成的方法无法整合多块信息，亟需一种能高效扩展LLM并支持多信息组合的新方法。

Method: 提出了一种元学习框架CompAs，将多段上下文组成信息转化为adapter参数，并可代数式合并这些参数，支持信息的可逆编码和恢复，提升了安全性和适应性。

Result: CompAs显著降低推理成本，提升长上下文鲁棒性，并能应对超长输入。实验表明，在多项选择和抽取式问答任务上，随着输入规模增大，CompAs持续优于ICL与已有生成式adapter方法。

Conclusion: Compositional adapter generation（CompAs）为大语言模型提供了一种高效、可组合的新扩展方式，相比ICL和传统生成式方法在多输入场景下表现更佳，为LLM部署提供了实际、有效的替代方案。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [95] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 本文系统分析了LLM推理能力与指令微调的表现，发现推理模型在大规模与复杂任务上能超越IFT，尤其在开放式和高推理需求下具备显著优势；同时，应该关注两者的训练与计算成本权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在许多任务上表现优异，其推理能力、适用任务和模型规模，以及训练和推理成本等关键问题仍未被充分探索。

Method: 采用合成数据蒸馏框架，进行大规模监督学习研究。对比不同规模的指令微调（IFT）模型与推理模型，在涵盖数学和通用任务的多选题与开放式题目上进行评估。

Result: 推理能力显著提升模型表现，经常能与更大规模的IFT系统匹敌或超越。IFT在训练和推理成本方面依然具备帕累托最优性，但推理模型在模型规模增加时更加有价值，在高推理需求和开放式任务上超越IFT性能。

Conclusion: 推理能力对大语言模型性能提升至关重要，在推理密集任务和模型规模扩大时，推理模型能打破IFT的性能瓶颈。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [96] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 作者通过哲学论证指出，大型语言模型因缺乏意图，输出内容本质上无意义，尽管在实际中它依然能被有效利用。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型在各领域表现出极高的能力，但其输出究竟是否具有“意义”这一哲学问题引发广泛讨论，作者希望探讨LLM输出是否真的有“字面意义”。

Method: 作者提出基于“意图”条件的哲学论证，通过界定意义必须依赖于适当的意图，并断言语言模型无法拥有这种意图；此外，他们批驳了外在主义与内在主义的常见反驳，从语义理论和指称关系上展开争论。

Result: 作者认为由于LLM缺乏正确类型的意图，因此其输出本质上是无意义的，同时反驳了语义外在主义和内在主义的主流观点。

Conclusion: 虽然LLM输出在哲学上可以被归为无意义，但其表现出的“好像有意义”的特征，仍能在实际应用中帮助人类获取真实信念甚至知识。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [97] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: 本文提出RTP框架，利用大语言模型，将数据语义递归划分为二叉树，每个节点皆为易懂问题，显著提升主题聚类的解释性，可用于分类任务和提示生成模型；相比传统方法，RTP更易用于数据洞察与知识驱动分析。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督文本集分析方法在数据稀缺领域表现不佳，特别是传统主题模型仅能用关键词描述聚类，解释性差且需要大量人工解读。

Method: 提出了一种新的分析框架——递归主题划分（RTP），利用大语言模型（LLM）以互动方式构建二叉树。树节点由自然语言问题组成，以语义方式对数据进行划分，形成可完全解释的分类体系，每个聚类的逻辑透明。

Result: 实验表明，RTP的以问题为驱动的层级结构在可解释性上优于强基线模型BERTopic 的关键词主题。同时，RTP生成的聚类可作为特征用于下游分类任务，在数据主题与任务标签相关时表现突出。此外，RTP树的主题路径能作为生成式模型结构化、可控的提示，实现对源语料特征的稳定模拟。

Conclusion: RTP 提出了一种新的数据探索范式，从统计模式发掘转向以知识为驱动的主题分析，同时将分析框架升级为强大的合成工具，为大语言模型的应用带来更高的解释性与控制力。

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [98] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: 现有语音分词器在噪音下稳定性差，作者提出了多分支共识机制的StableToken方法，显著提高了分词和下游语音大模型的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的语义语音分词器虽然能捕捉语言内容，但在面对与语义无关的声音扰动时十分脆弱，即使在高信噪比（SNR）条件下，分词结果也会发生巨大变化，导致下游的大型语言模型学习难度增加。作者发现造成这种不稳定的原因在于单一通路量化架构和训练信号无视分词稳定性的设计。

Method: 提出了一种名为 StableToken 的分词器，采用多分支架构并行处理音频，通过强大的逐位投票机制合并表示，生成单一且稳定的分词序列，从而提升结果稳定性。

Result: StableToken 在分词稳定性上达到了新的最先进水平，在不同类型的噪声条件下，分词序列的编辑距离显著降低。

Conclusion: Tokenizer的基础稳定性直接带来下游语音大模型鲁棒性的明显提升，在多个任务上取得了更好的表现。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [99] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 本文提出了复合推理框架，让LLMs结合多种推理方式，实验证明该方法在科学和医学问答任务中优于传统推理方式，并自适应领域选择推理风格，提升了模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）主要依赖单一的主导性推理范式，导致在需要多种认知策略的复杂问题上表现有限。该论文旨在解决LLMs在复杂推理任务上的不足。

Method: 提出了复合推理（Composite Reasoning, CR）方法，使LLMs能够动态探索并结合多种推理方式，如演绎、归纳和溯因推理，提升模型解决问题的细致性和能力。

Result: 在科学和医学问答基准测试中，CR方法优于现有的推理方法，如链式思维（CoT）和DeepSeek-R1式推理，并展示了更高的样本效率和适度的算力消耗。CR能根据领域自适应选择推理方式，比如在医学问答中突出溯因和演绎推理，在科学问答中强调因果、演绎和归纳推理。

Conclusion: 增强LLM内部推理风格的多样性，可以大幅提升其鲁棒性、适应性和解决复杂问题的效率。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [100] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 将大模型推理能力迁移到小模型易因分布不对齐导致学习障碍，逆向推测解码能针对学生模型概率分布，过滤低概率token，显著提升迁移效果，但需对每个小模型单独校准。


<details>
  <summary>Details</summary>
Motivation: 当前将大型语言模型的推理能力通过有监督微调迁移到小模型时，尽管有高质量的教师示范，性能反而下降，背后原因未明。

Method: 提出了逆向推测解码（RSD）机制，教师模型生成候选token，小模型根据自身概率分布决定接受与否，过滤掉低概率token，从而生成更适合学生模型学习的推理轨迹。

Result: 直接通过传统知识蒸馏，大型模型推理轨迹会导致Qwen3-0.6B等学生模型在推理基准测试集上平均表现下降20.5%；而采用RSD机制生成的推理轨迹则让性能提升4.9%。此外，分析表明低概率token是迁移失败的关键瓶颈，并且RSD轨迹具有模型特异性，不可通用。

Conclusion: 高质量推理能力迁移受限于分布不对齐和低概率token，逆向推测解码能有效缓解该问题，但需要针对不同学生模型进行定制化分布校准。

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [101] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: FLEXI是首个针对紧急语音中断的人机全双工大模型基准，揭示了现有模型的主要不足，并提出了优化语音交互的建模建议。


<details>
  <summary>Details</summary>
Motivation: 当前全双工语音大模型是实现自然实时人机交互的基础，但对其进行评测与建模仍困难重重，尤其缺乏针对紧急打断场景的系统评价方法。

Method: 提出并设计了FLEXI基准，用于系统化评估六类实际人机全双工语音交互场景中的模型性能，包括质量、延迟与对话有效性，特别关注紧急场景下的中断能力。

Result: 通过FLEXI基准测试，发现开源和商业模型在对话应急反应、正确结束话轮和实时响应速度等方面存在明显性能差异；并提出基于“下一个token对”预测的建模方向，可助力实现更流畅、更拟人的全双工语音交互。

Conclusion: FLEXI基准全面揭示了开放源代码与商业语音大模型在应急意识、话轮终止和交互延迟等方面存在显著差距。

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [102] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文结合法律法规（如欧盟AI法案、GDPR），提出LLM安全合规新概念，开发了能用于安全测试的基准，并以群体策略优化对Qwen3-8B实现符合标准的安全推理器，通过实验验证有显著安全性提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在安全性方面逐渐受到关注，但现有的安全方法缺乏系统和严谨的框架，不能很好地覆盖现代LLM复杂行为的安全保障。本文旨在从法律合规的角度，提出更为科学有效的LLM安全性标准和方法。

Method: 将欧盟AI法案和GDPR等已确立的法律框架作为LLM安全合规的标准，提出 '安全合规' 概念。开发了基于具体法律条款生成的真实LLM安全场景新基准，并采用群体策略优化（GRPO）对Qwen3-8B进行安全对齐，构建了合规推理器 Compliance Reasoner，让LLM能够依据法律标准进行安全推理和风险规避。

Result: 新提出的Compliance Reasoner在所构建的安全合规基准上表现优异，较原有方法在欧盟AI法案基准上提升10.45%，在GDPR基准上提升11.85%。

Conclusion: 以法律合规为核心建立LLM安全标准和测评体系，有效提升了模型的安全性，对未来LLM安全技术和法规遵从具有重要参考价值。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [103] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: 本文提出SSKG-LLM新架构，专注于结构和语义信息的深度整合，显著增强了大语言模型的事实推理能力，为解决幻觉问题提供了新的技术路线。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型解决“幻觉”问题主要通过知识图谱，但通常只利用其语义信息，忽略结构特性；KG嵌入与LLM文本嵌入的空间差异也阻碍了有效整合。为此探索如何高效融合KG语义和结构信息。

Method: 提出SSKG-LLM模型，包括：知识图谱检索(KGR)模块、知识图谱编码(KGE)模块用于保留语义和结构信息，知识图谱适配(KGA)模块用于使LLM理解KG嵌入。并进行了大量实验分析。

Result: 实验结果表明融入知识图谱结构信息能显著提升大语言模型的事实推理能力。

Conclusion: 通过SSKG-LLM架构，有效融合知识图谱的结构与语义信息到大模型的推理过程，提升了模型的事实推理能力。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [104] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本研究系统分析了可解释性技术在仇恨言论检测中的公平性作用。结果表明，输入解释有助于发现和缓解偏见，但在公平模型筛选上效果有限。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理（NLP）模型往往会从训练数据中复制或放大社会偏见，影响公平性。同时，由于其黑箱特性，用户难以识别偏见预测，开发者也难以有效缓解偏见。当前关于公平NLP的可解释性研究大多为定性分析，缺乏大规模定量研究。

Method: 系统性分析可解释性与公平性在仇恨言论检测中的关系，涵盖编码器和解码器模型，关注识别偏见预测、选择公平模型、训练过程中的偏见缓解三维度。

Result: 输入驱动的解释方法能够有效检测偏见预测，并在训练期间作为减少偏见的有用监督，但在从多个候选中选择公平模型时不可靠。

Conclusion: 输入驱动解释在检测偏见预测和缓解训练偏见方面有效，但在选择公平模型方面存在局限。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [105] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 论文系统评估微调LLMs用于自然语言到一阶逻辑翻译，提出有效技术显著提升性能。T5系模型优于更大的仅解码模型，结构逻辑翻译能力突出，但谓词提取仍是主要难点。


<details>
  <summary>Details</summary>
Motivation: 将自然语言自动翻译为一阶逻辑（FOL）对于知识表示和形式方法至关重要，但这一任务依然具有挑战性。该论文动机是系统性评估微调的大型语言模型（LLMs）在此任务上的表现，并比较不同架构和训练策略以提升翻译效果。

Method: 使用MALLS和Willow数据集，论文对编码-解码器与仅解码器架构进行比较，探索词汇扩展、谓词条件与多语言训练等技术。论文引入了准确匹配、逻辑等价、谓词对齐等评测指标，并针对FOL翻译任务对LLMs进行微调。

Result: 微调的Flan-T5-XXL模型在带谓词列表任务上达到70%的准确率，明显优于GPT-4o及DeepSeek-R1-0528（含链式推理）以及符号系统（如ccg2lambda）。有趣发现包括：（1）谓词可用性可提升15-20%性能；（2）T5模型优于更大的仅解码模型；（3）模型可在FOLIO数据集上泛化至未见逻辑论断。

Conclusion: 结构化逻辑翻译性能强劲，但谓词提取成为主要瓶颈。模型提供了新的自动化自然语言到一阶逻辑翻译方法，丰富了知识表示领域工具箱。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [106] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 本研究专注于都铎时期宗教抨击言论的自动检测，构建了高质量语料库，并证明微调的BERT模型优于通用大模型，为历史文本自动分析提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 利用自然语言处理技术来辅助历史宗教抨击言论的研究，特别聚焦于都铎时期英格兰的宗教改革语境下的相关文本分析。

Method: 设计了从原始数据到预处理、数据选择，再到多轮人工标注的完整流程，构建了InviTE语料库，并用它对BERT微调模型和零样本大模型进行性能评估与对比。

Result: 建立了包含近2000条早期现代英语句子的InviTE语料库，并通过专家标注丰富语料。实验结果表明，在历史数据上预训练并微调以检测抨击性语言的模型表现最佳。

Conclusion: 领域专属微调与历史语料预训练对于提升宗教抨击言论检测任务的NLP模型效果至关重要，InviTE语料库为相关研究提供了新的资源。

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [107] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文展示贝叶斯概率理论和相关计算工具可以有效建模相关理论语用学中的隐含意义交流，为语用学提供更科学的分析框架。


<details>
  <summary>Details</summary>
Motivation: 最新的贝叶斯理论进展和概率计算工具促进了语用学和语义学领域的“概率转向”。作者希望探索贝叶斯方法能否在相关理论语用学中有效应用，处理隐含意义的交流问题。

Method: 采用贝叶斯概率理论和计算工具，对语用学现象进行概率建模。

Result: 本文证明了贝叶斯方法可以用来分析和建模隐含意义（如会话含义）的交流过程，扩展了以往的Gricean语用学模型。

Conclusion: 本文提出可以用贝叶斯方法进行相关理论语用学的建模，特别是处理会话含义的传递。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [108] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: 作者提出了覆盖250年英语书籍的时间结构化语料库CHRONOBERG，发现主流LLM在历时意义转变方面表现有限，强调了时间敏感训练的重要性。数据与工具已公开，将促进语言演化研究和更智能的模型发展。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）多依赖社交媒体和网络数据，这些语料库虽然多样化，但往往缺乏长期的时间结构，限制了模型对语言语义和规范演变的理解能力。为此，需要开发具有时间结构的语料库，支持对语言历时变化的研究和模型训练。

Method: 作者提出并构建了CHRONOBERG，这是一个涵盖250年英语图书文本的时间结构化语料库。语料来源于Project Gutenberg，加入了多种时间注释。作者利用这些数据和编辑书籍的特点，采用Valence-Arousal-Dominance（VAD）分析方法量化词义随时间的变化，构建历史校准的情感词典用于时间性相关语言解释。通过历时训练和分析，测试LLM对意义随时间变化的适应性。

Result: 研究发现，即使在依次用CHRONOBERG训练的LLM，也难以有效编码语言意义的历时变化，显示当前主流模型和训练方法无法很好解释长期演变的语言特征，需要在训练与评估流程上引入时间敏感性。CHRONOBERG可作为研究语言变化和模型时间泛化能力的重要资源。

Conclusion: CHRONOBERG语料库弥补了现有语料的时间性结构不足，有助于推动大型语言模型在语言历时变化捕捉、歧视性语言识别与情感分析等领域的发展。作者公开了该数据集与代码，为相关研究提供了新的基础工具。

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [109] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 风力涡轮机维护日志蕴藏丰富信息但难以分析。本文创新性地提出用大型语言模型进行深层语义推理，远超现有分类方法，实现故障和因果链等多流程分析。实验证明，LLMs可作为可靠性分析的高级工具，有效挖掘文本数据价值，对风能行业意义重大。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机维护日志中的大量操作智能信息被锁定在非结构化的自由文本中，难以被传统的定量可靠性分析利用。现有机器学习方法多仅止步于文本分类，无法进行更深层次的语义推理。

Method: 提出并使用了一种基于大型语言模型（LLMs）的探索性分析框架，框架可实现故障模式识别、因果链推断、场地对比分析和数据质量审核等多种分析流程。应用这一框架对大规模工业数据集进行了实证。

Result: LLMs不仅可以实现文本分类，更能深度合成和推理文本信息，生成可操作的专家假设，显示出强大的“可靠性副驾驶”功能。

Conclusion: 本研究为风能行业提供了一种利用LLMs解锁非结构化数据中隐藏洞见的新方法，为提升运维智能和可靠性分析能力开辟了新路径。

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [110] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 本文针对OLMO2模型的数据集，首次系统分析了训练数据的政治偏向，不仅发现左倾内容占主导，还确认了其与模型判断高度相关，强调了数据筛选透明化和政治内容分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）常被指出会生成带有政治偏见的文本，但这些偏见如何产生尚不清楚。其中很重要的一环是对训练数据的政治内容分析，这部分在现有LLM研究中被严重忽视。因此，论文旨在填补训练数据政治分析的研究空白。

Method: 选取OLMO2这一最大且完全开源，并附带完整数据集的模型，对其训练前和训练后的语料库进行分析。采用大规模随机抽样，并自动标注文档的政治倾向，分析其来源领域和内容。同时评估训练数据中政治内容与模型在具体政策议题上的立场之间的相关性。

Result: 分析发现，左倾（左派）文档在多个数据集里占主导地位，训练前的语料数据中有明显更多的政治相关内容。左右倾文档虽然讨论类似主题，但在价值观及合法性来源上有明显不同。此外，训练数据中的主导政治立场和模型在政策议题上的政治偏见高度相关。

Conclusion: 研究表明，训练数据的政治偏向深刻影响了大语言模型的输出偏见。未来的数据筛选管道需要集成政治内容分析，同时对过滤策略进行详细记录以提升透明度。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [111] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出了Chimera图表理解测试集，发现当前VLM模型主要依赖捷径而非真正理解图表，揭示了VLM在复杂视觉输入理解上的严重不足，强调需要更严谨的评测方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLM模型在图表相关基准测试中表现良好，但这些成绩可能来源于知识、推理或模态捷径，而非真实的图表理解。缺乏能有效区分捷径与真正理解的评测手段，亟需专项测试集以揭示模型短板。

Method: 作者提出Chimera评测集，收集了7500个高质量的维基百科图表，每个图表配有语义三元组标注和多层级问题设计，针对实体识别、关系理解、知识关联和视觉推理等四个维度进行评估。作者分析并检测了三种捷径行为，并对15个开源VLM模型进行了系统性评测。

Result: 实证显示：视觉记忆捷径影响有限，知识回忆捷径影响中等，而Clever-Hans捷径（表层语言模式）影响最显著。模型的高分主要是因为捷径行为。暴露了VLM在图表理解上的显著缺陷，现有测试无法有效测量真正的理解能力。

Conclusion: 当前的视觉-语言模型在处理图表理解任务时，往往依赖于捷径（如记忆、知识回忆和表层语言模式）而非真正的图表理解能力，这暴露了模型在复杂视觉输入理解上的重大局限。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [112] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 提出了一种通过分析模型激活空间方向检测不可回答问题的新方法，实验证实优于现有方法，具有良好泛化性并能调控模型拒答行为，适用于抽取式QA及更广泛的不可回答场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在缺乏必要信息时仍常自信作答，导致虚构（hallucinated）内容，急需提升模型判断不可回答性能力，尤其在抽取式问答任务下识别是否有充分信息回答问题。

Method: 通过在推理过程中施加激活加和，测量其对模型拒答行为的影响，从而选择出能反映不可回答性的方向，并将隐藏激活投影到该方向上以获得分类分数。同时，通过因果干预（加/除该方向）验证对拒答行为的控制效能。

Result: 实验证明，该方法在两种开源LLM和四个抽取式QA基准上的不可回答问题检测表现优于现有基于提示和分类器的方法，并能更好地跨数据集泛化。此外，该方法延展于因科学共识缺失、主观性等因素引发的不可回答性。最后，通过因果干预实现对模型拒答行为的调控。

Conclusion: 找到了一种在模型激活空间中刻画不可回答性的方向，并证实其能够有效用于（不可）回答性的分类和模型行为控制。

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [113] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 论文系统评估了主流LLM在多语言法律任务中的表现和鲁棒性，发现法律任务准确率仍偏低（多低于50%），且对提示和攻击易受影响，英文虽稳定但并非最优，句法相似性影响性能，Gemini整体优于LLaMA。对LLM在法务等关键领域的实际部署仍需谨慎。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）广泛应用于法律等高风险领域背景下，了解其多语言、多法域及对抗性任务中的能力和局限，对确保法律工作流程智能化的可靠性至关重要。

Method: 评估LLaMA和Gemini在多语言法律与非法律基准上的表现，以及通过字符级和单词级扰动测试其在法律任务中的对抗鲁棒性。采用LLM-as-a-Judge方法进行偏向人类的评分，并提出了开放源码、模块化、多任务基准测试管道，支持多语言和多法律任务。

Result: 发现LLMs在法律推理任务上的准确率通常低于50%（如LEXam），而在泛用任务上可达70%以上（如XNLI）；英文表现较稳定但不一定最高；各语种对提示和对抗扰动均较为敏感。LLM性能与语种的英语句法相似性相关。LLaMA整体较Gemini弱，Gemini在同类任务平均领先24个百分点。

Conclusion: 尽管新一代LLM有所进步，但在关键、多语种法务场景中部署仍存显著挑战，准确性和可靠性需进一步提升。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [114] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ürker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: 作者提出了NeLLCom-Lex神经代理框架，模拟词汇语义变化，并用颜色命名任务验证其在人类语义变化机制再现上的有效性，为理解语义变化因果机制提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 传统语义变化研究主要依赖观察性和实验性方法，但观察性方法无法揭示因果机制，而实验方法又因语义变化的历时性过程难以应用。

Method: 提出了NeLLCom-Lex神经元代理框架，让神经代理首先在真实词汇系统（如英语）中进行语义基础，然后系统性地操纵他们的交流需求，使用颜色命名任务模拟词汇系统的演化，通过监督学习和强化学习，探索其命名行为和词汇变化。

Result: 神经代理经过训练后能够在颜色命名任务中产生类似人类的命名模式，并且依据交流需求调整行为和词汇结构，表现出与人类类似的语义变化现象。

Conclusion: NeLLCom-Lex能够有效再现和解释词汇语义变化的关键机制，未来可用于深入揭示语义变化的因果过程。

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [115] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 作者发现：LLM在同一问题上的解法差异（分歧）越大，其问题解决能力越强。基于此，提出“解法分歧”作为新的模型训练和评估指标，并在多任务测试中证实其能显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 目前LLM训练多靠监督微调或强化学习，作者想探究不同解法的“分歧度”是否能帮助提升LLM的问题解决能力。

Method: 提出并评估“解法分歧”作为度量标准，在三大问题领域测试其对LLM成功率提升的作用。

Result: “解法分歧”与模型问题解决能力呈正相关，并且在三类任务领域中提升了LLM的成功率。

Conclusion: “解法分歧”可作为提升LLM训练与评估的新型有效工具。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [116] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 此文提出一种面向资源稀缺斯拉夫语言的LLM微调方案，覆盖机器翻译和问答任务，结合多数据源及增强技术，效果超越传统基线。


<details>
  <summary>Details</summary>
Motivation: 探索如何在资源有限的斯拉夫语言（乌克兰语、上索布语、下索布语）上，通过LLM实现高质量的机器翻译和问答，解决当前相关语言数据不足的问题。

Method: 基于Qwen2.5-3B-Instruct模型，采用参数高效微调，同时针对机器翻译与多项选择问答任务联合训练；乌克兰语问答还结合了检索增强生成，部分任务采用模型集成。

Result: 实验结果表明，所提出的方法在所有任务上均优于基线。

Conclusion: JGU Mainz团队的方法在乌克兰语、上索布语及下索布语的机器翻译和问答任务上，均超越了基线模型。

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [117] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 该工作提出了一种高效、无需额外训练的LLM表示方法，用以实现模型选择和性能预测，具备极高可扩展性、实时性和可解释性，在实际任务上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 在预训练大语言模型（LLMs）越来越多，公开模型库日益扩大的情况下，如何高效选择任务表现最优的LLM成为挑战。以往的方法面临可扩展性差和高昂的重训练成本等问题，因此需要新的解决方案。

Method: 提出了一种高效、无需训练的方法，将LLM表示为在提示的语义任务空间内的线性算子。这一方法通过闭式几何计算实现，能够高度可解释地评估模型，并具备优异的扩展性与实时适应动态扩展模型库的能力。

Result: 方法在成功预测和模型选择任务上展现出具有竞争力甚至达到SOTA的效果，尤其在未见样本场景下表现突出。

Conclusion: 文中方法有效解决了LLM表示和选择的可扩展性、解释性和泛化性问题，为实际应用中动态挑选最佳LLM提供了有力支持。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [118] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了一种新颖的两阶段适应性多分支引导（AMBS）架构，用于提升大语言模型多目标（HHH）对齐。通过共享表征和策略机制，方法在多项数据集上显著提升了有用性、安全性和诚实性表现，有效解决了遗忘和一致性难题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）的多目标对齐（如有用性、无害性、诚实性，简称HHH）对于安全和可靠部署至关重要。以往方法通过向隐藏状态注入控制信号（steering vector）以引导输出，面临在优化某一目标时遗忘其他目标的问题，或多分支设计导致目标间推理不一致。因此，亟需一种既能统一又高效的多目标对齐方法。

Method: 提出了适应性多分支引导（AMBS）框架。在第一阶段，对Transformer层的注意力后隐藏状态进行一次计算，形成共享表征。在第二阶段，将该表征复制到多个并行分支，通过策略—参考机制进行目标特定的引导，从而实现目标特异控制并维持跨目标一致性。

Result: 在Alpaca、BeaverTails和TruthfulQA数据集以及多款7B参数的LLM上测试，AMBS方法在HHH目标一致性和安全性方面均有显著提升。例如，在DeepSeek-7B模型上，AMBS对齐分数提升32.4%，不安全输出减少11.0%，与最新方法具有竞争力。

Conclusion: AMBS框架可以有效提升大语言模型在多目标（有用性、无害性、诚实性）对齐任务中的性能，既解决了遗忘和一致性问题，又具备高效性和普适性。

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [119] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出全面的开源FP8训练流程，结合预训练与微调，通过混合量化实现性能无损及效率大幅提升，表现媲美BF16，显著降低资源消耗，将推动LLM训练更普及。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLM）需要极高的计算成本，成为创新的主要障碍。尽管FP8训练理论上可以显著提升效率，但由于缺乏完整的开源训练流程，其广泛应用受限。本文旨在解决这一瓶颈。

Method: 提出了一个端到端的FP8训练方案，结合了持续式预训练与有监督微调，并采用细粒度、混合粒度的量化策略以保证数值稳定性和计算效率。

Result: 通过大规模实验（包括对1600亿token语料的持续预训练），该方案在多个推理基准上的表现与BF16基线相当，同时实现了高效：训练时长减少最多22%，峰值内存消耗降低14%，吞吐量提升19%。

Conclusion: 证明FP8训练不仅高效且稳定，是BF16的实用替代方案，并承诺开源代码以推动大规模模型训练的民主化。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [120] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 论文提出认知推理新范式与CogFlow框架，通过认知流程建模和强化学习训练，大幅增强了大模型和人类的社会决策与认知能力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLMs）在逻辑推理上表现优秀，擅长一步步推导出可验证答案，但对于社会情境中的不明确线索与多解问题适应较差。需要让大模型能像人一样进行有解释性的社会认知推理。

Method: 提出认知推理（Cognitive Reasoning）这一新范式，把人类社会认知建模为一系列互相关联的认知单元（如观察、归因）结合成认知流程。实现上，设计并提出了CogFlow框架：首先通过树状规划模拟人类联想性和渐进性思维，生成认知流程数据集；再通过监督微调进行基础训练，最后引入强化学习，通过多目标奖励分别优化认知流程与回答质量，自主试错提升模型能力。

Result: 实验结果显示，CogFlow显著提升了大模型在社会认知方面的能力，并且在人类用户中也能辅助实现更有效的社会决策。

Conclusion: CogFlow框架能够有效提升大模型的社会认知推理能力，使其在应对复杂社会情境时更加高效、智能。

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [121] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 本研究针对EHR患者异步消息回复提出了RAEC检索增强评价机制，结合错误本体、历史信息检索与DSPy分层检测，显著提高了LLM回复草案的错误识别率和一致性，为AI辅助临床沟通提供了高效安全的评估工具。


<details>
  <summary>Details</summary>
Motivation: 随着EHR门户系统中异步患者与临床医生的信息沟通日益增加，临床医生的工作量持续上升，因此有兴趣利用大语言模型（LLM）协助草拟回复。然而，LLM生成的内容可能存在临床不准确、信息遗漏或语气不当等问题，亟需建立稳健的评估机制。

Method: （1）提出一个临床为本的错误本体系统，涵盖5大领域和59个细分错误代码，通过归纳编码和专家判读制定；（2）开发了检索增强评价流程（RAEC），通过语义相似的历史消息-回复对来提升判别质量；（3）提供了一种基于DSPy的两阶段提示架构，实现可扩展、可解释且分层的错误检测。

Result: 在独立评估和基于历史检索参考的评估方式下，DSPy两阶段流程在1500多条患者消息上进行了对比。引用检索背景信息提升了对临床完整性和工作流程适宜性等领域的错误识别。针对100条消息进行人工验证时，带有参考背景标注的结果在一致性（50% vs. 33%）和性能F1指标（0.500 vs. 0.256）方面均显著优于基础方法。

Conclusion: 检索增强的评价流程RAEC在患者消息回复场景中可以作为AI安全护栏，显著提高临床错误的识别率和评估一致性，对推动LLM可靠落地具有重要意义。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [122] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 本文提出针对LLM幻觉定位的新基准与评价方法，发现现有LLM在该任务上效果一般，并分析了幻觉定位的主要挑战与优化方向。


<details>
  <summary>Details</summary>
Motivation: 现有关于定位上下文相关幻觉（即模型输出中包含无法从源文本验证的信息）的评价方法复杂且缺乏专门的基准，因此需要一种更实用的评价方式，针对大语言模型（LLM）进行有效的幻觉定位。

Method: 构建面向LLM的幻觉定位评测基准，包含1000余例人类注释样本，并提出自由文本形式的错误描述；基于LLM设定评估协议并通过人工验证其质量，同时评估四种主流LLM在该任务上的表现。

Result: 所提出的评测基准具有较高难度，目前最佳LLM在该基准上的F1分数仅为0.67。此外，通过分析，发现LLM在定位幻觉时存在两个主要挑战：一是容易误将遗漏细节判定为不一致，二是难以处理输出中虽事实正确但在源文档中不可验证的信息。

Conclusion: LLM能够参与幻觉定位任务，但准确性有限，基准评测显示现有模型表现仍有较大提升空间。研究还揭示了针对该任务的优化提示策略及影响LLM定位幻觉的核心难点。

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [123] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: ArabJobs是一个包含8500多条阿拉伯招聘广告的大型公开语料库，分析了性别、职业和方言差异，展示了在劳动市场、性别偏见和职业分类等任务的应用前景，促进了阿拉伯语的公平NLP研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、多样化的阿拉伯语招聘广告数据，阻碍了公平NLP研究和对阿拉伯地区劳动市场的深入理解。

Method: 收集并分析来自埃及、约旦、沙特阿拉伯和阿联酋的8500多个招聘广告，研究性别代表、职业结构和方言差异，并利用大语言模型进行薪资估算和职位类别标准化，同时基准性别偏见检测和职业分类任务。

Result: 展示了该语料库在薪资估算、职位类别归一化、性别偏见检测、职业分类等任务中的应用价值，揭示了阿拉伯劳动市场中的语言、地区和社会经济多样性。

Conclusion: ArabJobs语料库对于促进公平的阿拉伯语自然语言处理和劳动市场研究非常有用。

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [124] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: 本文通过比较两种先进NLP模型，验证其在模拟和分析多人讨论及观点变化上的有效性，优于以往技术，对实际社会分析和决策具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术的进步，可以对群体决策中的深入讨论（deliberation）进行建模，分析观点变化，并预测不同情况下的可能结果。作者希望检验NLP技术在解读群体讨论和观点转变中的能力。

Method: 收集来自不同背景个体的观点，构建多元化自有数据集。通过富含吸引性事实的产品展示模拟讨论过程，诱发听众观点的变化。对比分析“基于词频的讨论调制模型”和“量子-讨论框架”这两种模型，并与最新方法进行性能比较。

Result: 所提出的两个模型在解释和建模群体讨论中展现出优于现有技术的表现。能有效捕捉讨论语境和观点变化，为后续分析和预测提供更有力支持。

Conclusion: 基于NLP的建模能够更好地理解和预测群体讨论中的观点转变。相关成果在公共政策制定、辩论评估、决策支持、以及社交媒体舆情分析等场景具备重要应用价值。

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [125] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne Sälevä,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: 提出了一种综合考虑模型和数据变异性的重采样方法，提升了多语言/多任务NLP评测中各类指标不确定性的量化精度，并验证了在问答、翻译、实体识别任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多语言或多任务NLP评测往往忽视了性能分数的变异性来源，导致评估结果不够精准和可靠。需要更合理的度量方法来反映模型评估中的不确定性。

Method: 提出了一套基于重采样(resampling)的方法，能同时考虑模型和数据带来的评估分数变异性，用于度量多语言/多任务NLP基准上的评估指标的统计不确定性和精度。

Result: 利用多语言问答、机器翻译、命名实体识别任务，方法能够有效计算各种排行榜常用量（如均值、中位数、模型间差异、排名等）的抽样分布，更准确评估与比较模型性能。

Conclusion: 在NLP评测中，若未同时考虑模型和数据造成的变异性，容易低估性能分数的不确定性。提出的重采样方法能够更科学地量化排行榜上的评估指标不确定性，提升模型比较的可靠性。

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


### [126] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 针对RNN处理长序列能力弱的问题，本文提出StateX，可低成本扩展状态并提升回忆能力，实验验证效果显著，无需增加大量参数或训练成本。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然在语言建模性能上表现突出，但其高复杂度导致处理长文本时成本较高。而RNN（如线性注意力和状态空间模型）由于每个token的计算复杂度恒定，受到关注。但RNN在需要准确回忆长文本上下文信息的任务上表现不佳，因为所有上下文信息都被压缩进一个固定大小的状态。之前的研究表明，回忆能力与状态大小呈正相关，但直接训练大状态RNN的成本很高。

Method: 提出StateX训练流程，通过后训练(post-training)高效扩展已有RNN的状态大小。针对线性注意力和状态空间模型，设计了结构修改，能显著扩大状态大小且模型参数增加很少或不增加。

Result: 在最大1.3B参数规模的模型上实验，StateX能高效提升RNN的回忆及上下文学习能力，同时不会带来高后训练成本，也不损失其他能力。

Conclusion: StateX是一种高效扩展RNN状态、提升性能的新方法，能够在无需大量额外开销的前提下，显著增强RNN处理长序列信息的能力。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [127] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出用变分推理统一推理轨迹优化，为语言模型推理能力提升构建稳定且理论坚实的目标，兼容RL微调方法，并在多个模型和任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型在提升推理能力时，训练目标往往缺乏稳定性，并且不同优化方法之间的联系不清晰。作者希望通过概率推断的思路，统一变分推断与强化学习类方法，为增强语言模型推理能力提供更稳定、理论基础更强的目标函数。

Method: 提出了一种变分推理框架，把“思考轨迹”作为潜变量，通过变分推断进行优化。从ELBO出发，推广到多轨迹目标以获得更紧的界，提出用于稳定训练的前向KL目标，并将现有的拒绝采样微调和二值奖励RL（如GRPO）解释为局部前向KL目标，揭示其对简单问题的隐式偏置。方法在Qwen 2.5和Qwen 3模型族的推理任务上进行了实证验证。

Result: 新提出的变分推理目标训练出的模型推理能力更强，目标函数更稳定。实验证明方法在Qwen 2.5、Qwen 3家族的大量推理任务上有效。理论上还揭示了强化学习方法中的偏置本质。

Conclusion: 该方法为语言模型推理能力提升提供了一个统一的概率推断视角和更稳定有效的优化目标，理论上融合了变分推断与强化学习方法，实践中显著提升模型的推理表现。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [128] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出用反馈作条件信号而非奖励压缩，创新性地实现了语言反馈驱动的条件生成学习（FCP），提升了LLM对反馈的直接理解与利用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多通过人类或AI反馈的强化学习（RLHF）进行训练，但通常将丰富的反馈信息压缩为单一标量奖励，导致信息损失和尺度失衡。作者希望解决反馈表现力不够和训练方式受限的问题。

Method: 提出将语言反馈作为条件信号（conditioning signal），借鉴了跨模态生成任务中的语言先验理念。核心方法是设计了feedback-conditional policy（FCP），该策略通过最大似然训练直接学习响应-反馈对的条件分布，且引入在线自举阶段，在正面条件下生成并收集最新反馈不断优化。

Result: FCP能够让LLM以反馈为条件直接生成输出，相较于传统奖励优化方式，能更充分利用语言反馈的信息表达能力，提升了模型的表达能力和迭代效率。作者还开源了代码。

Conclusion: 将反馈作为生成条件而非转化为标量奖励，能够更直接、细致地指导大模型行为，并增强其从复杂语言反馈中学习的能力。FCP为反馈驱动学习提供了一种更具表现力的新范式。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [129] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: n-gram新颖度不能充分衡量创造性。人类专家更注重合理性和实用性，大模型在创造性和实用性识别上仍有很大进步空间。


<details>
  <summary>Details</summary>
Motivation: 传统n-gram新颖度无法全面衡量文本创造性，缺乏对“合理性和实用性”的考察。研究旨在从理论与实证角度检验n-gram新颖度与真正创造性之间的关联，并评估大模型在创造性检测上的表现。

Method: 通过对7542份人类和AI生成文本的专家批注，分析文本的新颖度、实用性和合理性。比较不同大模型（开源与闭源）及其训练设置（零/小样本、微调）在识别创意表达及非实用表达上的能力。

Result: n-gram新颖度与专家判断创意性有正相关，但n-gram新颖度高的表达中近91%并未被认定为有创意。开源大模型中，新颖度升高会降低实用性；而闭源前沿模型创造力仍不如人类。前沿大模型能在“创意识别”上超越随机但不善于识别非实用表达。最佳模型的判官型新颖评分预测性较强。

Conclusion: 前沿大模型在识别创造性和非实用性表达方面尚有改进空间，且单纯依赖n-gram新颖度不足以衡量创造性。专家人工评价与模型输出之间存在明显差异。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [130] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出WebGen-Agent，结合视觉模型多层反馈和新型奖励机制，极大提升了网站代码生成的准确性与视觉表现，整体性能显著优于前沿方案。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型驱动的智能体在代码生成领域表现出色，但在网站生成任务中，由于该类任务极度依赖视觉效果和用户交互反馈，现有代码智能体只利用简单的代码执行进行反馈与验证，无法有效评估代码质量。因此，需要更加全面和细致的反馈方式来提升网站代码智能体的生成能力。

Method: 本文提出了WebGen-Agent，这是一种新颖的网站生成智能体，结合了多层次的视觉反馈进行网站代码的迭代生成与优化。系统利用视觉语言模型针对网站截图和GUI-agent测试生成详细的文本描述、建议及质量分数。积分机制采用积分回溯与优选策略，强化生成效果。此外，还提出了Step-GRPO训练方法，将每步的截图和GUI-agent分数作为奖励信号，密集监督WebGen-Agent的推理引擎。

Result: WebGen-Agent显著提升了现有模型在WebGen-Bench数据集上的表现，将Claude-3.5-Sonnet的准确率从26.4%提升至51.9%，外观得分从3.0提升到3.9；采用Step-GRPO训练后，Qwen2.5-Coder-7B-Instruct的准确率由38.9%升至45.4%，外观得分由3.4升至3.7，整体超越了现有最优智能体系统。

Conclusion: 提出的WebGen-Agent通过综合视觉反馈和多层次量化评分，显著提升了网站生成质量，且通过Step-GRPO监督进一步提升了推理引擎表现，为未来复杂、视觉交互密集的代码生成任务提供了新的解决思路。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [131] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出全面评估语音助手能力的新基准，并展示多模态语音助手当前的优缺点。商业模型未必优于开源小模型，但多模态和语音模仿仍存挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多模态系统能力的提升，对以语音为中心的 AI 助手需求日益增加，但现有基准无法全面评估这些系统的能力。

Method: 提出 VoiceAssistant-Eval 基准，覆盖听、说、看三大维度，包含 10,497 个例子和 13 类任务，并对 21 个开源模型及 GPT-4o-Audio 进行内容、语音质量及一致性测试。

Result: 发现 (1) 商业模型并不总是优于开源模型；(2) 多数模型在说话任务表现优异但音频理解较差；(3) 精心设计的小模型可与更大模型媲美，但在多模态和语音模仿任务有难度、稳健性和安全性仍存在明显差距。

Conclusion: VoiceAssistant-Eval 为新一代语音助手的研究与开发提供严格的评测框架，揭示现有模型的关键挑战与未来发展方向。

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [132] [Semi-Random Graphs, Robust Asymmetry, and Reconstruction](https://arxiv.org/abs/2509.21516)
*Julian Asilis,Xi Chen,Dutch Hansen,Shang-Hua Teng*

Main category: cs.DM

TL;DR: 本文研究了图重构猜想中的两个核心属性（非对称性与唯一性）在半随机和强扰动环境下的鲁棒性。证明这类性质在更广泛分布模型下仍然成立，并进一步刻画了嵌入结构和有限度破坏情况下的图非对称性，改进了不可重构图的概率质量界。结果说明图重构一些关键结论并不完全依赖于理想随机分布，拓展了该领域分析的适用范围。


<details>
  <summary>Details</summary>
Motivation: 图重构猜想长期以来都是图论中的重要难题，研究该问题有助于理解图结构的本质。目前针对该猜想的部分成果，分别属于依赖结构特性具体分类、或依赖随机图平均分布的两种分析方法，前者只解决某些特殊类型图，后者虽然覆盖面广但对分布假设依赖性强。论文希望通过研究这两者之间的桥梁，从半随机与光滑分析的角度，探索相关性质的鲁棒性。

Method: 作者结合半随机分析与光滑分析方法，关注于随机图的两个重要属性：子图的非对称性与唯一性。论文借助Bollobás在1990年关于Erdős-Rényi随机图的经典结论，分析这些属性在更加广泛的半随机分布下是否仍然成立，并研究图结构被扰动（允许对每一对顶点独立进行大概率加边/删边）时这些性质的稳定性。

Result: 作者证明了：即使在强扰动下，部分随机图的非对称性与唯一性仍然可以保持，表现出与Erdős-Rényi分布随机图类似的渐近性质。此外，还得到了在随机图中嵌入结构与有限度对抗性破坏下，图非对称性的渐近刻画，并改进了不可重构图概率质量的界。

Conclusion: 本文展示了随机图部分核心性质具有强鲁棒性，对理解图重构猜想在半随机和对抗性环境下的表现有重要推进。相关分析论证了某些重构性结论不仅仅依赖于理想化的分布假设，对未来研究更一般情形的图重构具有启发意义。

Abstract: The Graph Reconstruction Conjecture famously posits that any undirected graph
on at least three vertices is determined up to isomorphism by its family of
(unlabeled) induced subgraphs. At present, the conjecture admits partial
resolutions of two types: 1) casework-based demonstrations of
reconstructibility for families of graphs satisfying certain structural
properties, and 2) probabilistic arguments establishing reconstructibility of
random graphs by leveraging average-case phenomena. While results in the first
category capture the worst-case nature of the conjecture, they play a limited
role in understanding the general case. Results in the second category address
much larger graph families, but it remains unclear how heavily the necessary
arguments rely on optimistic distributional properties. Drawing on the
perspectives of smoothed and semi-random analysis, we study the robustness of
what are arguably the two most fundamental properties in this latter line of
work: asymmetry and uniqueness of subgraphs. Notably, we find that various
semi-random graph distributions exhibit these properties asymptotically, much
like their Erd\H{o}s-R\'enyi counterparts.
  In particular, Bollob\'as (1990) demonstrated that almost all
Erd\H{o}s-R\'enyi random graphs $G = (V, E) \sim \mathscr{G}(n, p)$ enjoy the
property that their induced subgraphs on $n - \Theta(1)$ vertices are
asymmetric and mutually non-isomorphic, for $1 - p, p = \Omega(\log(n) / n)$.
We show that this property is robust against perturbation -- even when an
adversary is permitted to add/remove each vertex pair in $V^{(2)}$ with
(independent) arbitrarily large constant probability. Exploiting this result,
we derive asymptotic characterizations of asymmetry in random graphs with
planted structure and bounded adversarial corruptions, along with improved
bounds on the probability mass of nonreconstructible graphs in $\mathscr{G}(n,
p)$.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [133] [Permutation closure for multiple context-free languages](https://arxiv.org/abs/2509.22239)
*Andrew Duncan,Murray Elder,Lisa Frenkel,Mengfan Lyu*

Main category: cs.FL

TL;DR: 本文证明了多重上下文无关语言对排列闭包运算闭合，采用了树堆自动机的方法，补充和扩展了此前工作的研究范围。


<details>
  <summary>Details</summary>
Motivation: 过去关于排列闭包性质（如循环移位闭包）主要针对正则、上下文相关等语言类进行过，作者希望扩展到多重上下文无关语言。

Method: 证明使用了Denkinger提出的受限树堆自动机，而早期相关工作多使用文法方法。

Result: 成功证明了多重上下文无关语言在排列闭包下仍属同类，也补充了已有针对其他语言类（如EDT0L、ET0L）的结果。

Conclusion: 多重上下文无关语言的排列闭包仍然是多重上下文无关语言。

Abstract: We prove that the \emph{permutation closure} of a multiple context-free
language is multiple context-free, which extends work of Okhotin and Sorokin
[LATA 2020] who showed closure under \emph{cyclic shift}, and complements work
of Brandst\"adt [1981, RAIRO Inform. Th\'{e}or.] (resp. Brough \emph{et al.}
[2016, Discrete Math. Theor. Comput. Sci.]) who showed the same result for
regular, context-sensitive, recursively enumerable (resp. EDT0L and ET0L)
languages. In contrast to Okhotin and Sorokin who work with grammars, our proof
uses restricted tree stack automata due to Denkinger [DLT 2016].

</details>


### [134] [Passive Learning of Lattice Automata from Recurrent Neural Networks](https://arxiv.org/abs/2509.22489)
*Jaouhar Slimi,Tristan Le Gall,Augustin Lemesle*

Main category: cs.FL

TL;DR: 本文提出了一种结合抽象解释与自动机学习的新算法，可实现对大型及无限字母表的循环网络自动机提取，并提出了新的无限字母表基准，实验显示效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有自动机提取算法难以处理字母表极大或无限的情形，限制了自动机从复杂神经网络模型中的应用与理论分析。本文致力于攻克这一难题，提升自动机学习在神经网络理解中的效用。

Method: 该方法结合了抽象解释（Abstract Interpretation）领域的过近似技术与文法推断（Grammatical Inference）领域的被动自动机学习。首先在Tomita语法训练的RNN上与现有算法进行比较，然后扩展到具有无限字母表的正规语言，并提出了新的基准测试。

Result: 实验结果表明，所提算法在有限与无限字母表的RNN自动机提取任务中均有良好表现，同时提出了针对无限字母表的正则语言新基准数据集。

Conclusion: 本文提出的方法能够从具有非常大甚至无限字母表的循环神经网络中有效地提取自动机，并通过与现有最先进方法的比较以及对无限字母表情形的实验验证了其实用性。

Abstract: We present a passive automata learning algorithm that can extract automata
from recurrent networks with very large or even infinite alphabets. Our method
combines overapproximations from the field of Abstract Interpretation and
passive automata learning from the field of Grammatical Inference. We evaluate
our algorithm by first comparing it with the state-of-the-art automata
extraction algorithm from Recurrent Neural Networks trained on Tomita grammars.
Then, we extend these experiments to regular languages with infinite alphabets,
which we propose as a novel benchmark.

</details>
