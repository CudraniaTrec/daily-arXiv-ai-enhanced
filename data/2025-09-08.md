<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 93]
- [cs.DM](#cs.DM) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: 该论文首次对GitHub上静态类型语言中的浮点数使用展开大规模实证分析，发现浮点数广泛应用，并指出现有基准测试与真实代码存在差异，研究成果有助于推动更贴近实际的分析和工具开发。


<details>
  <summary>Details</summary>
Motivation: 浮点数运算推理极为复杂，现有静态和动态分析技术进展明显，但与真实世界代码的关联度仍需提升，首要挑战是理解实际代码中浮点数的真实使用情况。

Method: 该论文通过对GitHub公开仓库中静态类型语言的大规模浮点数运算使用行为进行实证研究。研究采用了随机抽样、基于本质属性的筛选等主流数据挖掘方法，结合关键词检索和代码解析以识别浮点数运用及相关编程结构。

Result: 结果确认了业界普遍认为的浮点数运算被广泛使用的观点，并发现学术评价自动推理技术的基准程序在某些方面反映了真实代码，但在其他方面尚有不足。

Conclusion: 本研究的发现有助于未来浮点数相关技术的设计和评测更好地契合实际用户需求，并提供了有价值的数据集支持后续研究。

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [2] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: 论文提出一种结合领域特定建模和实时可视化的AI辅助编程方法，用户可用多种输入方式创建并实时检查形式模型。基于Lingua Franca开发的原型表明，此方法显著提升了代码生成和验证的透明度和效率。


<details>
  <summary>Details</summary>
Motivation: AI辅助编程大幅提升了软件开发效率，但现有方法在代码透明性和可验证性方面仍有不足。作者希望通过引入领域特定建模技术，并以可视化手段增强AI生成代码的透明性，促进代码的可视化检查和形式化验证。

Method: 提出结合领域特定建模技术，实现即时、图形化的AI生成代码语义可视化。用户可通过编程、自然语言提示、语音命令等方式构建形式模型，并在每步转化后获得实时反馈。这一方法可根据具体领域或目的进行定制。作者还开发了原型系统，在Visual Studio Code环境下为Lingua Franca语言实现了上述功能。

Result: 原型系统展示了新型领域特定建模方法的潜力。通过集成可视化和即时反馈，提升了模型的创建、验证与检查过程。为代码生成和后期验证带来了更高的效率和准确性。

Conclusion: 集成透明性和可视化验证的AI辅助编程方法，可提升代码生成质量和验证效率。基于Lingua Franca的原型验证了该方法的有效性，展示了未来领域特定建模新方向。

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [3] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: Pulse Infinite 工具能在超大规模实际代码库中有效检测程序发散性，发现多项新问题，显著推动了该领域的实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有针对程序发散性（non-termination）检测方法仅能处理小型程序，难以扩展到百万级代码量的大型实际项目，但大型工业软件项目中发散性问题具有实际意义。

Method: 提出了工具 Pulse Infinite，采用了组合式（compositionally）和下近似（under-approximation）证明技术：组合式有利于处理大规模代码基，下近似保证了证明发散性时的正确性。

Result: 将 Pulse Infinite 应用于一亿多行的 C、C++、Hack 语言的开源和私有软件，发现30多个此前未知的发散性问题。

Conclusion: Pulse Infinite 工具首次实现了对百万级代码量实际代码库中的发散性分析与检测，显著扩展了此前方法的适用范围，开创了检测实际大型代码库程序发散性的先河。

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 本研究评测多种LLM自动生成单元测试框架的表现，发现DeepSeek最适合结构和维护，GPT-4则更完整但规范性差。提示设计和上下文输入对生成结果影响大。


<details>
  <summary>Details</summary>
Motivation: 在测试驱动开发（TDD）中，测试框架的编写十分重要，但传统上手动编写既耗时又容易出错，尤其是面对教育场景或大型项目时。自动化生成测试框架可以极大提升效率和质量，因此作者探索LLM自动生成单元测试框架（skeleton）的可行性与效果。

Method: 作者比较了四种大型语言模型（GPT-4、DeepSeek-Chat、Llama4-Maverick、Gemma2-9B）自动生成实际Rub​​y课程类的RSpec测试框架的能力；具体方法包括静态分析和专家盲审，评估结构正确性、可维护性、清晰性及测试规范符合度。

Result: DeepSeek能够生成最易维护且结构良好的测试框架，而GPT-4生成的框架虽然更完整但在测试规范上一致性不足。各模型对代码结构和测试规范理解存在显著差异。

Conclusion: LLM 在自动生成测试框架方面各有所长，DeepSeek在可维护性和结构性上表现最佳，GPT-4更注重完整性但规范一致性相对较弱。提示工程和上下文输入对输出质量关键。

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [5] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: 该论文推出了TinyML评测新框架，实测分析主流嵌入式平台间的性能优劣，为TinyML在实际部署中的平台选择和优化提出建议。


<details>
  <summary>Details</summary>
Motivation: TinyML模型部署在资源受限的嵌入式系统时，缺乏可靠、通用的性能评测工具，且实际应用需求与理论研究之间存在差距。因此开发一个平台无关且可扩展的评测框架，帮助开发者理解不同平台及模型的权衡。

Method: 提出了一个模块化且与平台无关的基准框架PICO-TINYML-BENCHMARK，通过在两种主流嵌入式平台（BeagleBone AI64与Raspberry Pi 4）上，使用真实数据集对三类代表性TinyML模型（手势识别、关键词检测、MobileNet V2）进行多项性能指标评测，包含推理延迟、CPU利用率、内存效率及预测稳定性。

Result: 框架评测显示各平台在不同任务下的优缺点，为TinyML模型和平台选择及优化提供了参考方案，并有助于推动TinyML技术实践落地。

Conclusion: BeagleBone AI64在AI特定任务上具有更稳定的推理延迟，而Raspberry Pi 4在资源效率与成本方面更具优势。这些结果为TinyML在嵌入式系统实际部署时的优化提供了指导。

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [6] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ是一种基于多样性引导的量子程序测试方法，能够生成更丰富的测试输入并发现更多缺陷，验证效果优于现有测试基线。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子程序的可靠性变得日益重要，但目前缺乏有效的测试手段来提升量子程序的测试覆盖率与缺陷检测能力。

Method: 提出了一种名为NovaQ的多样性引导测试框架。该框架利用基于分布的测试用例生成器来随机扰动电路参数，产生多样化的量子状态输入，再通过新颖性驱动的评估模块，依据电路中的内在状态指标（如幅值、相位和纠缠度）量化行为新颖性，从而优先选取映射到测试盲区的输入。

Result: 在不同规模和复杂度的量子程序上进行实验，NovaQ相比现有基线方法，能够持续获得更高的测试用例多样性，并检测到更多缺陷。

Conclusion: NovaQ框架有效提升了量子程序的测试多样性和缺陷检测能力，是比传统方法更优的量子程序自动化测试方案。

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [7] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: 用大模型生成低资源语言代码变更样本，提升自动代码审查系统在新兴语言环境下的表现，效果接近真实标注数据训练。


<details>
  <summary>Details</summary>
Motivation: 新出现的编程语言和框架下，缺乏足够的标注数据来训练自动化代码审查系统，影响了软件质量保障和开发流程自动化。

Method: 利用大型语言模型（LLMs）将丰富标注数据的主流语言代码变更翻译为新兴或低资源语言的等效变更，生成合成训练数据，并用其训练监督分类器，再与真实标注数据训练的模型效果进行系统性对比。

Result: 实验表明，基于LLM生成的合成数据能显著提升低资源语言下的代码审查推荐系统性能，有效缩小与真实标注数据模型之间的性能差距。

Conclusion: 该方法为在缺乏标注数据的新兴技术栈上推广自动代码审查提供了可扩展的解决方案，提升了系统的适应力和自动化能力。

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [8] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: 本文通过GitHub项目仓库挖掘，实证验证了LLMs在软件工程教育中激励与阻碍因素的分类，为制定负责任整合LLMs的教育框架奠定基础。


<details>
  <summary>Details</summary>
Motivation: LLMs广泛应用于软件工程教育，但其带来的机遇与挑战并存，因此需要系统性研究以指导其在课程中的负责任整合。

Method: 通过对400个GitHub项目的README文件和讨论问题进行仓库挖掘分析，以识别此前文献回顾中总结的激励与阻碍因素。

Result: 发现主要激励因素为提升学习动力与参与度、软件工程流程理解和编程/调试支持，主要阻碍为剽窃与知识产权问题、安全与隐私、对AI过度依赖；而课程重设计和学习成果评估等问题未被提及。

Conclusion: 本研究对大型语言模型（LLMs）在软件工程教育中的激励因素与阻碍因素进行了首次实证验证，为未来负责任地将LLMs纳入软件工程课程提供了重要的框架基础。

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [9] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC通过集成数据流分析和符号执行，显著提升了二进制模糊测试的覆盖率和漏洞检测能力，能发现传统方法遗漏的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有的grey-box fuzzing主要依赖控制流覆盖，难以充分暴露通过控制流难以发现的漏洞。整合数据流分析有望揭示更多潜在bug。

Method: 提出了FuzzRDUCC框架，结合符号执行和二进制程序的def-use链重构，通过新颖的启发式算法选择相关数据流链以指导fuzzing。

Result: 在binutils基准测试中，FuzzRDUCC发现了其他fuzzer未能检测到的新型漏洞。

Conclusion: FuzzRDUCC可以识别到现有最先进fuzzer未发现的独特crash，能够作为下一代漏洞检测与发现机制的可行方案。

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [10] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: 本研究结合生成式AI、大语言模型及视觉-语言模型，实现自然语言需求与系统图自动转化为汽车测试用例，有效减少人工投入并提升测试效率，但尚需人工干预以克服现有平台和技术局限。


<details>
  <summary>Details</summary>
Motivation: 提升汽车软件测试自动化水平，减少人工编写测试用例的工作量，实现跨汽车子系统的兼容与第三方工具的无缝集成，并为软件定义汽车功能的快速验证提供支持。

Method: 提出了一种基于生成式AI（GenAI）的自动测试用例生成方法，结合大语言模型和视觉-语言模型，将自然语言需求和系统图转化为结构化Gherkin测试用例，并通过车辆信号规范建模实现信号定义标准化，最终在digital.auto平台上执行测试。

Result: 在儿童在场检测系统的用例验证中，提出的方法显著减少了人工测试规范制定的工作量，并实现了测试用例的快速执行。

Conclusion: 尽管实现了显著的自动化，测试用例和测试脚本的生成仍然需要人工干预，原因是GenAI流程和digital.auto平台存在当前技术限制。

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [11] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber利用AI agent自动模拟用户对网站的操作，有效发现了以往传统工具难以检测的可用性问题，展示了基于AI agent网页测试方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化网页测试方法主要关注代码覆盖和负载测试，无法有效捕捉复杂的用户行为，导致许多可用性问题未被发现。随着大语言模型（LLM）和AI agent技术的发展，为网页测试带来模拟真实用户交互的新可能性。

Method: 作者提出了WebProber，一个基于AI agent的网页测试框架。WebProber能自主浏览指定网址，模拟真实用户交互，自动发现bug和可用性问题，并生成人类可读的测试报告。作者通过案例研究，测试了120个学者个人网站。

Result: WebProber在120个学术个人网站上发现了29个可用性问题，其中许多是传统工具无法发现的。

Conclusion: 基于agent的自动化网页测试是一个有前景的方向，有助于发现以往被忽略的用户体验和可用性问题。未来该方法可进一步发展成以用户为中心的下一代网页测试框架。

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [Forall-Exists Relational Verification by Filtering to Forall-Forall](https://arxiv.org/abs/2509.04777)
*Ramana Nagasamudram,Anindya Banerjee,David A. Naumann*

Main category: cs.LO

TL;DR: 本文针对现有关系验证工具对$orall\exists$性质支持不足的问题，提出了一种基于filter-adequacy转换的方法，将$orall\exists$性质转化为$orall\forall$性质，并用现有验证工具完成自动化验证，拓展了关系验证的应用范围。


<details>
  <summary>Details</summary>
Motivation: 动机在于目前关于关系验证的大多数工具和逻辑主要集中在$orallorall$（即2-安全性）以及一般的k-安全性等性质，但对于$orallorall$性质在处理非确定性系统时不足，而$orallorall$性质是许多实际安全与隐私、抽象等问题（如secure compilation、张量程序函数性规格等）所需要的。现有对$orallorall$性质（所有输入都存在输出）的逻辑和工具极少，导致实际需求无法覆盖。

Method: 本文提出了一种新颖的filter-adequacy变换方法，通过在乘积程序(product program)中添加断言(assertion)，将待验证程序的$orallorall$性质转化为变换后乘积程序的$orallorall$性质，从而利用现有$orallorall$工具间接验证原始的$orallorall$性质。进一步提出了用于包含断言失败的$orallorall$逻辑，并引入了bicoms（一种可直接转化$orallorall$到一元正确性的乘积程序），并用该逻辑证明了只要bicoms的$orallorall$验证成功，则原一元命令的$orallorall$规范成立。还开发了原型工具。

Result: 方法论实现了可以用普通断言和假设，并可用标准断言语言，使得现有包括自动关系验证器在内的工具可直接用于$orallorall$性质的自动化验证。并且已用该原型工具成功验证了文中所有示例。

Conclusion: 本文提出的新方法拓展并简化了$orallorall$性质的自动化关系验证，使得用户可以复用现有的自动化工具来解决更复杂的关系性验证任务，有效填补了现有工具对$orallorall$性质的支持空白。

Abstract: Relational verification encompasses research directions such as reasoning
about data abstraction, reasoning about security and privacy, secure
compilation, and functional specificaton of tensor programs, among others.
Several relational Hoare logics exist, with accompanying tool support for
compositional reasoning of $\forall\forall$ (2-safety) properties and,
generally, k-safety properties of product programs. In contrast, few logics and
tools exist for reasoning about $\forall\exists$ properties which are critical
in the context of nondeterminism.
  This paper's primary contribution is a methodology for verifying a
$\forall\exists$ judgment by way of a novel filter-adequacy transformation.
This transformation adds assertions to a product program in such a way that the
desired $\forall\exists$ property (of a pair of underlying unary programs) is
implied by a $\forall\forall$ property of the transformed product. The paper
develops a program logic for the basic $\forall\exists$ judgement extended with
assertion failures; develops bicoms, a form of product programs that represents
pairs of executions and that caters for direct translation of $\forall\forall$
properties to unary correctness; proves (using the logic) a soundness theorem
that says successful $\forall\forall$ verification of a transformed bicom
implies the $\forall\exists$ spec for its underlying unary commands; and
implements a proof of principle prototype for auto-active relational
verification which has been used to verify all examples in the paper. The
methodology thereby enables a user to work with ordinary assertions and
assumptions, and a standard assertion language, so that existing tools
including auto-active verifiers can be used.

</details>


### [13] [Higher order differential calculus in mathlib](https://arxiv.org/abs/2509.04922)
*Sébastien Gouëzel*

Main category: cs.LO

TL;DR: 本文介绍了对Lean数学库中高阶微分计算的推广实现，支持一般域和函数定义域，统一了分析和光滑函数，解决了推广导致的相关难题。


<details>
  <summary>Details</summary>
Motivation: 为支持更广泛的应用需求，需要将高阶微分计算推广到更一般的标量域及函数定义域，同时，希望能统一和扩展解析函数与光滑函数的整体框架。

Method: 1. 允许标量是任意域；2. 支持函数定义在子域（不是全空间）上；3. 将解析函数纳入更广泛的光滑函数体系；4. 数学和形式化层面对推广难点进行解决和处理。

Result: 构建了一个具有高度一般性和灵活性的高阶微分计算库，能支持多种领域需求，并在mathlib中实现了这些推广和形式化。

Conclusion: 本文扩展了Lean数学库mathlib，将高阶微分计算工具库集成其中，并对传统定义进行了推广和改进。提出的方法不仅更加通用，同时解决了推广带来的数学和形式化困难。

Abstract: We report on the higher-order differential calculus library developed inside
the Lean mathematical library mathlib. To support a broad range of
applications, we depart in several ways from standard textbook definitions: we
allow arbitrary fields of scalars, we work with functions defined on domains
rather than full spaces, and we integrate analytic functions in the broader
scale of smooth functions. These generalizations introduce significant
challenges, which we address from both the mathematical and the formalization
perspectives.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance](https://arxiv.org/abs/2509.04455)
*Shisong Chen,Qian Zhu,Wenyan Yang,Chengyi Yang,Zhong Wang,Ping Wang,Xuan Lin,Bo Xu,Daqian Li,Chao Yuan,Licai Qi,Wanqing Xu,sun zhenxing,Xin Lu,Shiqiang Xiong,Chao Chen,Haixiang Hu,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了针对中国保险领域的大模型能力评测基准INSEva，数据全面、评测精细，实验发现现有大模型虽有基础表现，但在复杂保险任务上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准通常未能反映保险领域的独特特性和需求，保险行业对AI应用的准确性和可靠性要求极高，因此亟需专门的评测基准。

Method: 提出了一个面向中国保险领域的全面AI能力评测基准INSEva，采用多维度评测体系，涵盖业务、任务、难度和认知-知识维度，共包含38,704个高质量样本，并采用针对性评测方法衡量自由回答的信实性和完整性。

Result: 用INSEva对8个主流大模型评测，发现不同维度表现有明显差异。通用大模型在保险领域有基本能力，平均得分超过80，但在处理复杂、真实世界的保险场景时仍有较大差距。

Conclusion: INSEva能有效揭示大模型在保险领域的能力短板，为领域AI提升和基准设计提供有力工具，基准即将公开发布。

Abstract: Insurance, as a critical component of the global financial system, demands
high standards of accuracy and reliability in AI applications. While existing
benchmarks evaluate AI capabilities across various domains, they often fail to
capture the unique characteristics and requirements of the insurance domain. To
address this gap, we present INSEva, a comprehensive Chinese benchmark
specifically designed for evaluating AI systems' knowledge and capabilities in
insurance. INSEva features a multi-dimensional evaluation taxonomy covering
business areas, task formats, difficulty levels, and cognitive-knowledge
dimension, comprising 38,704 high-quality evaluation examples sourced from
authoritative materials. Our benchmark implements tailored evaluation methods
for assessing both faithfulness and completeness in open-ended responses.
Through extensive evaluation of 8 state-of-the-art Large Language Models
(LLMs), we identify significant performance variations across different
dimensions. While general LLMs demonstrate basic insurance domain competency
with average scores above 80, substantial gaps remain in handling complex,
real-world insurance scenarios. The benchmark will be public soon.

</details>


### [15] [Mentalic Net: Development of RAG-based Conversational AI and Evaluation Framework for Mental Health Support](https://arxiv.org/abs/2509.04456)
*Anandi Dutta,Shivani Mruthyunjaya,Jessica Saddington,Kazi Sifatul Islam*

Main category: cs.CL

TL;DR: 本文开发了一款基于LLM和RAG框架的心理健康支持聊天机器人，重视安全和责任。系统评估表明其准确、可信，建议以人类参与及长期视角推动技术发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的出现既带来了无限可能，也带来了很多挑战，尤其是在心理健康支持方面。研究者希望通过发展的方式，将AI更安全、有效地应用于专业医疗领域。

Method: 采用检索增强生成（RAG）框架，结合提示工程技术，并对预训练模型在新数据集上进行微调。同时，对chatbot进行了包括准确性、共情能力、可信度、隐私性和偏见等方面的严格评估。

Result: 研发的Mentalic Net会话AI在多种指标表现良好，其中BERT得分达到0.898，其它评估指标也在满意范围内。

Conclusion: 建议采用人类参与的开发模式（human-in-the-loop），并倡导长期且负责任的技术发展策略。在充分认识AI带来的积极影响和潜在风险的前提下，推动变革性技术在心理健康领域安全应用。

Abstract: The emergence of large language models (LLMs) has unlocked boundless
possibilities, along with significant challenges. In response, we developed a
mental health support chatbot designed to augment professional healthcare, with
a strong emphasis on safe and meaningful application. Our approach involved
rigorous evaluation, covering accuracy, empathy, trustworthiness, privacy, and
bias. We employed a retrieval-augmented generation (RAG) framework, integrated
prompt engineering, and fine-tuned a pre-trained model on novel datasets. The
resulting system, Mentalic Net Conversational AI, achieved a BERT Score of
0.898, with other evaluation metrics falling within satisfactory ranges. We
advocate for a human-in-the-loop approach and a long-term, responsible strategy
in developing such transformative technologies, recognizing both their
potential to change lives and the risks they may pose if not carefully managed.

</details>


### [16] [Do MLLMs Really Understand the Charts?](https://arxiv.org/abs/2509.04457)
*Xiao Zhang,Dongyuan Li,Liuyu Xiang,Yao Zhang,Cheng Zhong,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文发现MLLMs在无注释图表上理解力不足，提出了图表推理基准CRBench和模仿人类推理行为的新方法ChartReasoner，显著提升了模型对图表的理性理解和推理能力，优于同类SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在图表理解方面表现出色，但它们在处理无注释图表时经常出现幻觉和性能大幅下降的问题。因此，论文提出了一个问题：MLLMs是否真的“理解”了图表？人类可以通过视觉推理理解图表并估计数值，所以有必要建立一个严谨的评测机制。

Method: 论文首先构建了一个名为CRBench的全面图表推理基准，用于严格评估MLLMs在无注释图表上的视觉推理能力。同时提出ChartReasoner模型，通过模仿人类基于图表理解而进行的估值行为，引导模型进行合理的图表理解。

Result: 在CRBench上的大量实验结果显示，ChartReasoner-3B/7B在图表推理任务上的表现优于GPT-4o和Gemini-2.5-Flash。同时，该模型在公开数据集上的一般图表理解能力也有显著提升，使MLLMs可以更理性地理解图表。

Conclusion: ChartReasoner有效提升了MLLM在无注释图表推理任务中的表现，克服了依赖识别而非推理的固有缺陷，为实现更合理、更接近人类的图表理解能力迈进了重要一步。

Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated
increasingly impressive performance in chart understanding, most of them
exhibit alarming hallucinations and significant performance degradation when
handling non-annotated charts. Therefore, a question arises: Do MLLMs really
understand the charts? Since a human is capable of understanding charts and
estimating the values by visual reasoning, we first carefully establish a
comprehensive Chart Reasoning Benchmark CRBench to rigorously evaluate the
visual reasoning abilities of MLLMs on non-annotated charts. We argue that
MLLMs are primarily relying on recognition rather than reasoning to interpret
the charts. To steer MLLMs to reasonable chart understanding, we propose
ChartReasoner that mimics human behavior by grounding their estimation in chart
understanding. Extensive results on the proposed CRBench show that
ChartReasnoner-3B/7B achieves superior performance in chart reasoning, even
compared to GPT-4o and Gemini-2.5-Flash. More importantly, ChartReasnoner also
demonstrates the visual reasoning abilities in general chart comprehension on
public benchmarks, leading to significant performance gains and enabling MLLMs
to rationally understand the charts. The code and dataset will be publicly
available upon publication.

</details>


### [17] [Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies](https://arxiv.org/abs/2509.04458)
*Daniel B. Hier,Steven Keith Platt,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 研究发现，提升生物医学本体标识符的训练曝光度能显著增强大模型链接本体术语的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在生物医学NLP任务表现突出，但在术语到本体标识符链接上经常失败，亟需探明失败原因以提升模型应用效果。

Method: 对GPT-4o和LLaMA 3 405B在两个本体（人类表型本体和基因本体）上的链接表现进行分析，并结合九项特征（如术语熟悉度、标识符使用频率、形态特征、本体结构）进行单变量和多变量统计分析。

Result: 统计分析表明，模型训练时接触本体标识符次数（曝光度）显著影响链接准确率，胜过其他特征因素。

Conclusion: 接触本体标识符的频率是大模型链接本体术语与正确标识符成功与否的最强预测因素。

Abstract: Large language models often perform well on biomedical NLP tasks but may fail
to link ontology terms to their correct identifiers. We investigate why these
failures occur by analyzing predictions across two major ontologies, Human
Phenotype Ontology and Gene Ontology, and two high-performing models, GPT-4o
and LLaMa 3.1 405B. We evaluate nine candidate features related to term
familiarity, identifier usage, morphology, and ontology structure. Univariate
and multivariate analyses show that exposure to ontology identifiers is the
strongest predictor of linking success.

</details>


### [18] [Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis](https://arxiv.org/abs/2509.04459)
*Shiqin Han,Manning Gao,Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 提出一种可根据样本难度动态分配算力的方法，将高效模型和多模态大模型协作，显著降低推理消耗同时保持高准确率，在多模态情感分析任务上效果优异。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）在多模态机器学习领域表现卓越，但高昂的计算消耗限制了其实用。相较之下，小型高效模型则更节能，却牺牲了性能。该论文旨在解决性能与效率的折中问题。

Method: 提出一种不确定性感知协同系统（U-ACS），结合强大的MLLM和高效的轻量级基线模型用于多模态情感分析。系统利用不确定性驱动级联机制，小模型先快速筛选输入，对高不确定性的样本再交由大模型处理，并引入预测权重平均及基于提示的交叉验证来处理预测冲突。

Result: 在基准数据集上，该方法大幅减少了推理成本，并且在仅用部分算力情况下，实现了与单独MLLM媲美甚至超越的准确率，取得了最新最优表现。

Conclusion: 该协同系统有效平衡了性能与计算效率，为多模态模型实际部署提供了切实可行的新思路。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has significantly
advanced the state-of-the-art in multimodal machine learning, yet their
substantial computational demands present a critical barrier to real-world
deployment. Conversely, smaller, specialized models offer high efficiency but
often at the cost of performance. To reconcile this performance-efficiency
trade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)
that synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a
lightweight baseline model for multimodal sentiment analysis. The core of our
system is an uncertainty-driven cascade mechanism, where the efficient small
model first acts as a rapid filter for all input samples. Only those samples
yielding high predictive uncertainty, thereby indicating greater difficulty,
are selectively escalated to the MLLM for more sophisticated analysis.
Furthermore, our system introduces advanced strategies to handle ambiguous or
conflicting predictions, including weighted averaging for predictions of
similar polarity and a prompt-based cross-verification to resolve conflicting
predictions when both models exhibit high uncertainty. This
sample-difficulty-aware approach allows for a dynamic allocation of
computational resources, drastically reducing inference costs while retaining
the high accuracy of MLLM. Extensive experiments on benchmark datasets
demonstrate that our proposed method achieves state-of-the-art performance,
while requiring only a fraction of the computational resources compared to
using a standalone MLLM.

</details>


### [19] [CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection](https://arxiv.org/abs/2509.04460)
*Yihan Chen,Jiawei Chen,Guozhao Mo,Xuanang Chen,Ben He,Xianpei Han,Le Sun*

Main category: cs.CL

TL;DR: 论文提出面向内容而非风格的AI评审生成内容检测方法，构建了新基准数据集与多任务检测框架，有效提升了检测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在学术同行评议流程中的广泛应用，其带来的公平性和可靠性风险日益突出。现有AI生成文本检测器在区分“语言润色”与“实质内容生成”方面表现不佳，易受到改写攻击，导致误判并影响评议质量，亟需更精确的检测方法以保障评议公正。

Method: 本文提出了一种从风格识别转向内容识别的新检测范式。具体包括：1）构建了CoCoNUTS内容导向型基准，基于细粒度AI生成评审数据集，涵盖六种人机协作模式；2）开发了CoCoDet多任务学习框架，实现对AI参与评审内容的更准确和鲁棒检测。

Result: 实验表明，基于内容的检测方法（CoCoDet）在识别AI参与评审生成内容的准确性和鲁棒性方面显著优于传统风格驱动方法。所提出的数据集和算法为学术评审场景中的AI应用评估及检测带来实用基础。

Conclusion: 本文通过改进检测范式和工具，为提升同行评审流程中AI文本生成识别的公平性和可靠性提供了实际解决方案，对学术界应对AI技术带来的透明性和公正性挑战具有积极意义。

Abstract: The growing integration of large language models (LLMs) into the peer review
process presents potential risks to the fairness and reliability of scholarly
evaluation. While LLMs offer valuable assistance for reviewers with language
refinement, there is growing concern over their use to generate substantive
review content. Existing general AI-generated text detectors are vulnerable to
paraphrasing attacks and struggle to distinguish between surface language
refinement and substantial content generation, suggesting that they primarily
rely on stylistic cues. When applied to peer review, this limitation can result
in unfairly suspecting reviews with permissible AI-assisted language
enhancement, while failing to catch deceptively humanized AI-generated reviews.
To address this, we propose a paradigm shift from style-based to content-based
detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark
built upon a fine-grained dataset of AI-generated peer reviews, covering six
distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an
AI review detector via a multi-task learning framework, designed to achieve
more accurate and robust detection of AI involvement in review content. Our
work offers a practical foundation for evaluating the use of LLMs in peer
review, and contributes to the development of more precise, equitable, and
reliable detection methods for real-world scholarly applications. Our code and
data will be publicly available at https://github.com/Y1hanChen/COCONUTS.

</details>


### [20] [From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media](https://arxiv.org/abs/2509.04461)
*Tian Ma,Kaiyu Feng,Yu Rong,Kangfei Zhao*

Main category: cs.CL

TL;DR: 文章提出PtoP框架，针对LLM在性格预测中的幻觉和类别不均衡两大难题，通过检索增强生成、合成过采样和模型微调显著提升了社交媒体MBTI预测效果。


<details>
  <summary>Details</summary>
Motivation: 近年来，社交媒体上的内容为性格预测提供了丰富的数据来源，而MBTI作为流行的人格测试指标，在心理学及社会学等众多应用场景中具有重要价值。但利用大型语言模型（LLMs）直接进行MBTI预测面临虚假生成（幻觉）及类别数据不均衡两大难题。

Method: 本文提出一种新颖的LLM框架PostToPersonality（PtoP），用于从社交媒体帖文预测MBTI。该框架结合了基于检索的生成增强与上下文学习以减轻LLMs的幻觉问题，并采用合成少数类过采样技术对预训练大模型进行微调，从而缓解类别不均衡，实现更准确的MBTI理解。

Result: 基于真实社交媒体数据集的实验显示，PtoP框架在与10种主流ML与DL基线方法的对比中，取得了最新的最优性能。

Conclusion: PtoP框架利用大语言模型和数据增强方法，有效解决了LLMs用于MBTI预测中的幻觉及类别不均衡问题，显著提升了预测准确性，在实际社交媒体性格预测任务中具备应用前景。

Abstract: Personality prediction from social media posts is a critical task that
implies diverse applications in psychology and sociology. The Myers Briggs Type
Indicator (MBTI), a popular personality inventory, has been traditionally
predicted by machine learning (ML) and deep learning (DL) techniques. Recently,
the success of Large Language Models (LLMs) has revealed their huge potential
in understanding and inferring personality traits from social media content.
However, directly exploiting LLMs for MBTI prediction faces two key challenges:
the hallucination problem inherent in LLMs and the naturally imbalanced
distribution of MBTI types in the population. In this paper, we propose
PostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from
social media posts of individuals. Specifically, PtoP leverages Retrieval
Augmented Generation with in context learning to mitigate hallucination in
LLMs. Furthermore, we fine tune a pretrained LLM to improve model specification
in MBTI understanding with synthetic minority oversampling, which balances the
class imbalance by generating synthetic samples. Experiments conducted on a
real world social media dataset demonstrate that PtoP achieves state of the art
performance compared with 10 ML and DL baselines.

</details>


### [21] [Benchmarking GPT-5 for biomedical natural language processing](https://arxiv.org/abs/2509.04462)
*Yu Hou,Zaifu Zhan,Rui Zhang*

Main category: cs.CL

TL;DR: 本文对GPT-5等模型系统评测显示，GPT-5在大多数生物医学NLP任务上超越前代与主流基线，已适合部分落地应用，但更精细的抽取和摘要任务仍需专业优化，基准结果指导了未来模型设计策略。


<details>
  <summary>Details</summary>
Motivation: 随着生物医学文献数量的迅速增长，需要可扩展的自然语言处理（NLP）解决方案以高效处理相关任务。本文旨在系统评估通用大模型（如GPT-5、GPT-4o）在不同生物医学NLP任务中的表现及实用性。

Method: 对GPT-5和GPT-4o在六类任务（命名实体识别、关系抽取、多标签文档分类、问答、摘要、文本简化）的12个数据集上进行零样本、单样本、五样本提示评测。所有模型采用固定提示、相同解码参数，批量推理，结果与已有的GPT-4、GPT-3.5、LLaMA-2-13B对比。报告各数据集的主要指标，并对具体任务给出性能分析。

Result: GPT-5整体表现最优，五样本提示宏平均得分0.557，高于GPT-4 (0.506) 和GPT-4o (0.508)。在MedQA答题任务上准确率94.1%，超越前监督SOTA五十余点，并在PubMedQA任务上达到无监督系统与监督系统持平。在化学实体识别和关系抽取任务上也大幅提升，但在摘要和疾病实体识别方面仍低于专用微调模型。

Conclusion: GPT-5已成为通用的生物医学NLP模型，特别是面向推理的生物医学问答已具备实际部署能力，但精细抽取和高密度证据总结等任务，微调或混合方法依然更优。基准测试明确指出哪些任务适合简单提示，哪些需检索增强或规划流程，能为前沿BioNLP系统设计提供实际指导。

Abstract: The rapid expansion of biomedical literature has heightened the need for
scalable natural language processing (NLP) solutions. While GPT-4 substantially
narrowed the gap with task-specific systems, especially in question answering,
its performance across other domains remained uneven. We updated a standardized
BioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot
prompting across 12 datasets spanning six task families: named entity
recognition, relation extraction, multi-label document classification, question
answering, text summarization, and text simplification. Using fixed prompt
templates, identical decoding parameters, and batch inference, we report
primary metrics per dataset and include prior results for GPT-4, GPT-3.5, and
LLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark
performance, with macro-average scores rising to 0.557 under five-shot
prompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached
94.1% accuracy, exceeding the previous supervised state of the art by over
fifty points, and attained parity with supervised systems on PubMedQA (0.734).
In extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and
ChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though
summarization and disease NER still lagged behind domain-specific baselines.
These results establish GPT-5 as a general-purpose model now offering
deployment-ready performance for reasoning-oriented biomedical QA, while
precision-critical extraction and evidence-dense summarization continue to
favor fine-tuned or hybrid approaches. The benchmark delineates where simple
prompting suffices and where retrieval-augmented or planning-based scaffolds
are likely required, providing actionable guidance for BioNLP system design as
frontier models advance.

</details>


### [22] [Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?](https://arxiv.org/abs/2509.04464)
*Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.CL

TL;DR: 本研究提出通过分析大语言模型响应分歧，结合辅助模型推断不确定性来源，有效定位歧义或知识缺失。实验验证框架适用多个问答任务，有助于提升实际应用可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在多个领域取得了突破，但其输出仍可能不可靠或误导，特别是在实际应用中。现有研究多关注于不确定性的量化，而对于不确定性来源的诊断研究较少。本文旨在深入探究大语言模型输出不确定性的根源，为模型优化提供依据。

Method: 通过收集目标大语言模型的多轮生成响应，利用辅助大语言模型分析这些响应中的不一致模式，推理不确定性来源，如输入问题的歧义、知识缺乏或两者兼具。在识别知识空缺时，辅助模型还能具体指出缺失的概念或事实。实验在AmbigQA、OpenBookQA和MMLU-Pro等数据集上验证此框架的通用性。

Result: 实验结果证实，该框架能够有效诊断目标大语言模型的不同不确定性来源，并对知识空缺进行了具体定位，体现了解析能力的通用性和实际价值。

Conclusion: 利用辅助模型对大语言模型响应间的分歧进行分析，能够有效诊断不确定性来源，为后续人工干预和模型性能提升提供方向，增强模型可靠性。

Abstract: Large language models (LLMs) have delivered significant breakthroughs across
diverse domains but can still produce unreliable or misleading outputs, posing
critical challenges for real-world applications. While many recent studies
focus on quantifying model uncertainty, relatively little work has been devoted
to \textit{diagnosing the source of uncertainty}. In this study, we show that,
when an LLM is uncertain, the patterns of disagreement among its multiple
generated responses contain rich clues about the underlying cause of
uncertainty. To illustrate this point, we collect multiple responses from a
target LLM and employ an auxiliary LLM to analyze their patterns of
disagreement. The auxiliary model is tasked to reason about the likely source
of uncertainty, such as whether it stems from ambiguity in the input question,
a lack of relevant knowledge, or both. In cases involving knowledge gaps, the
auxiliary model also identifies the specific missing facts or concepts
contributing to the uncertainty. In our experiment, we validate our framework
on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing
distinct uncertainty sources. Such diagnosis shows the potential for relevant
manual interventions that improve LLM performance and reliability.

</details>


### [23] [Emotionally-Aware Agents for Dispute Resolution](https://arxiv.org/abs/2509.04465)
*Sushrita Rakshit,James Hale,Kushal Chawla,Jeanne M. Brett,Jonathan Gratch*

Main category: cs.CL

TL;DR: 自动化情绪识别（尤其大语言模型）能有效解读争议对话中的情绪，帮助理解情绪对争议管理的影响，并提升智能系统处理纠纷的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注情绪识别在谈判场景的应用，但纠纷场合下情绪更强烈、社会过程不同，有必要探索自动化情绪识别对争议解决的影响及其实用价值。

Method: 基于大规模买卖纠纷对话语料库，通过情绪识别算法（尤其使用大语言模型）分析争议双方的情绪表达，并比较了不同情绪注释方法与人类注释者的一致性。

Result: 大语言模型能更准确地标注文档中的情绪强度，其注释结果与人类专家高度吻合。研究结果证实，情绪表达影响争议的主观与客观结果，情绪识别有助于管理争议、缓解情绪升级。

Conclusion: 大型语言模型（LLMs）在识别和解释情绪表达上比早期方法更高效，且与人类注释者更一致。情绪表达能显著影响争议结果，同时情绪识别技术有助于理解并管理争议中的情绪升级与缓和。智能系统可用于识别并减轻情绪升级，对争议解决有实际帮助。

Abstract: In conflict, people use emotional expressions to shape their counterparts'
thoughts, feelings, and actions. This paper explores whether automatic text
emotion recognition offers insight into this influence in the context of
dispute resolution. Prior work has shown the promise of such methods in
negotiations; however, disputes evoke stronger emotions and different social
processes. We use a large corpus of buyer-seller dispute dialogues to
investigate how emotional expressions shape subjective and objective outcomes.
We further demonstrate that large-language models yield considerably greater
explanatory power than previous methods for emotion intensity annotation and
better match the decisions of human annotators. Findings support existing
theoretical models for how emotional expressions contribute to conflict
escalation and resolution and suggest that agent-based systems could be useful
in managing disputes by recognizing and potentially mitigating emotional
escalation.

</details>


### [24] [Just-in-time and distributed task representations in language models](https://arxiv.org/abs/2509.04466)
*Yuxuan Li,Declan Campbell,Stephanie C. Y. Chan,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: LLMs在新任务上下文中形成的任务表示并非持续、单调变化，而是集中出现在部分关键token，并且具有可迁移性。这种“即用即取”、局部优化的内部机制让模型能快速适应新任务、完成in-context learning。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型具有出色的“in-context learning”能力，即不需要参数更新，仅凭提示或上下文就能够生成任务解决方案。理解模型何时以及如何“学习”新任务的表示，对于深入解析模型内部原理和进一步提升其能力具有重要意义。

Method: 文章系统性地追踪和分析了大语言模型在处理新任务语境时，内部向量表示的动态变化，重点关注那些可以迁移、可复用的任务表示。这些分析基于不同上下文长度、不同阶段（token粒度）的模型行为，研究模型内部何时、如何形成对新任务的抽象表达。

Result: 研究发现，大模型对新任务的可迁移任务表示并不是单调、持续地发展，而是呈现出非单调、局部突发式的变化。这些表示通常在上下文中特定token处才出现，并且能够准确复现场景任务信息。与此对举的是，一类较为稳定的高层次任务类别表征始终贯穿全上下文。此外，模型倾向于对最小“任务作用域”进行精确编码，但处理更长、更复杂任务时，则依赖于跨序列分布的、时序上更为分散的表达形式。

Conclusion: 大模型能够即时形成新任务表示，这些表示在时间和语义上呈现“双重局部性”：即在特定时刻和子任务范围内高度集中。这揭示出模型内部一类“实时计算”机制，使其能灵活快速适应新证据、实现“on-the-fly”学习。

Abstract: Many of language models' impressive capabilities originate from their
in-context learning: based on instructions or examples, they can infer and
perform new tasks without weight updates. In this work, we investigate
\emph{when} representations for new tasks are formed in language models, and
\emph{how} these representations change over the course of context. We focus on
''transferrable'' task representations -- vector representations that can
restore task context in another instance of the model, even without the full
prompt. We show that these representations evolve in non-monotonic and sporadic
ways, and are distinct from a more inert representation of high-level task
categories that persists throughout the context. Specifically, models often
condense multiple evidence into these transferrable task representations, which
align well with the performance improvement based on more examples in the
context. However, this accrual process exhibits strong locality along the
sequence dimension, coming online only at certain tokens -- despite task
identity being reliably decodable throughout the context. Moreover, these local
but transferrable task representations tend to capture minimal ''task scopes'',
such as a semantically-independent subtask, and models rely on more
temporally-distributed representations to support longer and composite tasks.
This two-fold locality (temporal and semantic) underscores a kind of
just-in-time computational process underlying language models' ability to adapt
to new evidence and learn new tasks on the fly.

</details>


### [25] [Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)
*Hao Zhang,Mengsi Lyu,Yulong Ao,Yonghua Lin*

Main category: cs.CL

TL;DR: 本文针对大语言模型推理中的prefill-decode分解提出新型剪枝方法，显著提升推理速度并大幅降低带宽开销，实验证实其高效、安全，具实际部署前景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多种任务上表现出色，但其高计算和内存消耗限制了实际部署。模型剪枝可有效缓解资源消耗，但现有方法往往忽略了实际使用中的prefill-decode（PD）分阶段执行的特点。

Method: 该论文提出了一种面向PD分解推理的新型剪枝方法，实现了更加精细化且高效的块和KV Cache剪枝。具体方法包括：针对prefill和decode阶段分别构建剪枝和蒸馏集，执行独立的块移除迭代优化；提出令牌感知Cache剪枝机制，在prefill阶段保留全部KV Cache，而decode阶段仅在部分选定层对首末token序列重用缓存，以降低通信开销并确保计算效率。

Result: 实验表明，该方法无论在PD分解还是非分解（统一）场景下均取得了显著性能提升。在默认设置下，实现了20.56%的推理速度提升与4.95倍的数据传输带宽消耗降低。

Conclusion: 本文提出的方法通过对PD分解特性的剪枝与缓存优化，显著提升了LLMs的推理效率和资源利用率，无需牺牲模型性能，具备优越的实际应用价值。

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across
various tasks, but their deployment is constrained by high computational and
memory costs. Model pruning provides an effective means to alleviate these
demands. However, existing methods often ignore the characteristics of
prefill-decode (PD) disaggregation in practice. In this paper, we propose a
novel pruning method for PD disaggregation inference, enabling more precise and
efficient block and KV Cache pruning. Our approach constructs pruning and
distillation sets to perform iterative block removal independently for the
prefill and decode stages, obtaining better pruning solutions. Moreover, we
introduce a token-aware cache pruning mechanism that retains all KV Cache in
the prefill stage but selectively reuses entries for the first and last token
sequences in selected layers during decode, reducing communication costs with
minimal overhead. Extensive experiments demonstrate that our approach
consistently achieves strong performance in both PD disaggregation and PD
unified settings without disaggregation. Under the default settings, our method
achieves a 20.56% inference speedup and a 4.95 times reduction in data
transmission bandwidth consumption.

</details>


### [26] [Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study](https://arxiv.org/abs/2509.04468)
*Xuan Yao,Qianteng Wang,Xinbo Liu,Ke-Wei Huang*

Main category: cs.CL

TL;DR: 作者系统评估了多种LLM在CFA官方模拟题下的表现。推理型模型整体表现最佳，RAG提升复杂问题解答，知识缺口是主要难点。本研究为金融领域选用和落地LLM作出参考。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在金融应用中有巨大潜力，但在专业金融场景下的系统性评估尚不充分。本文旨在弥补这一空白，系统评估LLM在复杂金融分析任务中的表现。

Method: 利用CFA（一项全球金融分析权威认证）I-III级官方模拟题（共1560道多项选择题）全面测试SOTA LLMs。对比关注多模态与算力、推理能力和精度、与模型轻量高效三类LLM，并采用零样本提示与融合官方课程内容的Retrieval-Augmented Generation（RAG）管道，后者通过层次化知识组织和结构化查询提升专业知识检索与推理。

Result: 结果显示以推理为导向的模型在零样本场景中表现更佳，RAG方法则在复杂情形下显著提升模型能力。误差分析发现主要失败源于知识缺口，而文本可读性影响甚微。

Conclusion: 推理专长型LLM具备明显优势，结合RAG系统进一步强化复杂金融任务推理。研究为金融领域LLM部署、模型选择和性价比优化提供了数据支持和建议。

Abstract: The rapid advancement of large language models presents significant
opportunities for financial applications, yet systematic evaluation in
specialized financial contexts remains limited. This study presents the first
comprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice
questions from official mock exams across Levels I-III of CFA, most rigorous
professional certifications globally that mirror real-world financial analysis
complexity. We compare models distinguished by core design priorities:
multi-modal and computationally powerful, reasoning-specialized and highly
accurate, and lightweight efficiency-optimized.
  We assess models under zero-shot prompting and through a novel
Retrieval-Augmented Generation pipeline that integrates official CFA curriculum
content. The RAG system achieves precise domain-specific knowledge retrieval
through hierarchical knowledge organization and structured query generation,
significantly enhancing reasoning accuracy in professional financial
certification evaluation.
  Results reveal that reasoning-oriented models consistently outperform others
in zero-shot settings, while the RAG pipeline provides substantial improvements
particularly for complex scenarios. Comprehensive error analysis identifies
knowledge gaps as the primary failure mode, with minimal impact from text
readability. These findings provide actionable insights for LLM deployment in
finance, offering practitioners evidence-based guidance for model selection and
cost-performance optimization.

</details>


### [27] [Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing](https://arxiv.org/abs/2509.04469)
*David Berghaus,Armin Berger,Lars Hillebrand,Kostadin Cvejoski,Rafet Sifa*

Main category: cs.CL

TL;DR: 作者对三大家族的八个多模态大语言模型进行发票文档处理能力基准测试，比较了直接处理图像和结构化markdown解析两种策略，结果表明直接处理图像优于结构化处理，不同模型对不同文档也有性能差异，为自动化文档系统选型提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着自动化文档处理需求增加，有必要系统评测当前主流多模态大语言模型在复杂实物文档上的能力和合适的处理方式，以便指导选型和系统设计。

Method: 基于零样本提示，比较了两种处理策略——直接图像处理和先将文档转为markdown再解析，测试了8个多模态大模型在3个公开发票数据集上的表现。

Result: 原生图像处理效果普遍优于结构化解析，不同模型与文档特定属性影响最终性能，基准测试为模型和方案选择提供参考。

Conclusion: 本文的基准测试揭示了多模态大模型原生图像处理通常优于结构化解析，并且各模型表现受文档类型及特征影响。

Abstract: This paper benchmarks eight multi-modal large language models from three
families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly
available invoice document datasets using zero-shot prompting. We compare two
processing strategies: direct image processing using multi-modal capabilities
and a structured parsing approach converting documents to markdown first.
Results show native image processing generally outperforms structured
approaches, with performance varying across model types and document
characteristics. This benchmark provides insights for selecting appropriate
models and processing strategies for automated document systems. Our code is
available online.

</details>


### [28] [COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language Instructions](https://arxiv.org/abs/2509.04470)
*Swarnadeep Bhar,Omar Naim,Eleni Metheniti,Bastien Navarri,Loïc Cabannes,Morteza Ezzabady,Nicholas Asher*

Main category: cs.CL

TL;DR: COCORELI是一种结合了中等规模LLM、抽象机制和话语模块的混合智能体框架，有效提升了遵守复杂指令、减少幻觉与空间推理等任务的表现，并在多个任务中超越了更大LLM的系统。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）在需要遵循复杂指令、减少幻觉（hallucination）以及空间推理的任务中存在局限性。

Method: 提出了一种混合智能体框架COCORELI，将中等规模的LLM与新颖的抽象机制和话语模块结合，用于解析指令，并实现对环境的动态、高层次表征的上下文学习。

Result: 在自然协作构建任务的实验中，COCORELI超越了单LLM的链式思考（CoT）方法和使用更大LLM的智能体系统。此外，COCORELI基本避免了幻觉，可以识别信息缺失、主动澄清、更新认知对象。其抽象能力在API自动补全任务（ToolBench）中也得到了验证。

Conclusion: COCORELI有效弥补了当前LLM在复杂任务上的不足，展现了优异的泛化和抽象能力。

Abstract: We present COCORELI, a hybrid agent framework designed to tackle the
limitations of large language models (LLMs) in tasks requiring: following
complex instructions, minimizing hallucination, and spatial reasoning. COCORELI
integrates medium-sized LLM agents with novel abstraction mechanisms and a
discourse module to parse instructions to in-context learn dynamic, high-level
representations of the environment. Experiments on natural collaborative
construction tasks show that COCORELI outperforms single-LLM CoT and agentic
LLM systems, all using larger LLMs. It manages to largely avoid hallucinations,
identify missing information, ask for clarifications, and update its learned
objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown
in the ToolBench API completion task.

</details>


### [29] [MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification](https://arxiv.org/abs/2509.04471)
*Alice Schiavone,Marco Fraccaro,Lea Marie Pehrson,Silvia Ingala,Rasmus Bonnevie,Michael Bachmann Nielsen,Vincent Beliveau,Melanie Ganz,Desmond Elliott*

Main category: cs.CL

TL;DR: MOSAIC是一种基于开源小语言模型的多语言、高效放射报告分类方法，支持零/小样本学习，性能媲美专家且资源需求低，适用于实际临床环境，可广泛扩展。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告分类方法存在诸多局限性，如规则方法难以适应语言多样，监督模型需要大量标注数据，最新大模型依赖闭源或消耗资源大的模型且偏向英语和单一任务。一个高效、开源、支持多语言、多模态、多分类体系的方案需求迫切。

Method: 提出MOSAIC方法，基于轻量级公开语言模型MedGemma-4B，实现多语言、分类体系无关、可在消费级GPU上运行的放射学报告自动分类，支持零样本/小样本推理和高效微调。对比实验覆盖多语种、多模态和多分类体系数据集。

Result: MOSAIC在七个数据集（涵盖英语、西班牙语、法语和丹麦语，以及多种影像和分类体系）上表现出色。胸部X光五个数据集上平均宏F1达88，接近或超越专家水平，仅需24GB GPU。仅用80条丹麦语标注样本（数据增强），F1即可达82（满标注1600时为86）。

Conclusion: MOSAIC为临床放射报告分类提供了切实可行的开源替代方案，性能强，资源需求低，适配多语言和任务，便于社区扩展和应用。

Abstract: Radiology reports contain rich clinical information that can be used to train
imaging models without relying on costly manual annotation. However, existing
approaches face critical limitations: rule-based methods struggle with
linguistic variability, supervised models require large annotated datasets, and
recent LLM-based systems depend on closed-source or resource-intensive models
that are unsuitable for clinical use. Moreover, current solutions are largely
restricted to English and single-modality, single-taxonomy datasets. We
introduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally
efficient approach for radiological report classification. Built on a compact
open-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot
prompting and lightweight fine-tuning, enabling deployment on consumer-grade
GPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and
Danish, spanning multiple imaging modalities and label taxonomies. The model
achieves a mean macro F1 score of 88 across five chest X-ray datasets,
approaching or exceeding expert-level performance, while requiring only 24 GB
of GPU memory. With data augmentation, as few as 80 annotated samples are
sufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86
with the full 1600-sample training set. MOSAIC offers a practical alternative
to large or proprietary LLMs in clinical settings. Code and models are
open-source. We invite the community to evaluate and extend MOSAIC on new
languages, taxonomies, and modalities.

</details>


### [30] [RECAP: REwriting Conversations for Intent Understanding in Agentic Planning](https://arxiv.org/abs/2509.04472)
*Kushan Mitra,Dan Zhang,Hannah Kim,Estevam Hruschka*

Main category: cs.CL

TL;DR: 作者提出了一套用于对话系统意图重写和规划的新基准RECAP，以及对应的重写方法和LLM评估器，有效缓解了传统方法在开放域中的不足，显著提升对话智能体的规划和多代理协作能力。


<details>
  <summary>Details</summary>
Motivation: 传统意图分类方法难以应对真实对话中的歧义、不明晰和动态性，导致下游规划受限。该工作旨在解决对话环境中意图检测与表达的挑战，通过更精准的意图表达提升智能体协作与任务执行能力。

Method: 提出了RECAP基准数据集，同时构建了基于LLM的意图评估器，并开发了基于提示的意图重写方法以及DPO微调重写器进行性能提升。

Result: 基于RECAP方法的意图重写方案显著优于传统基线，DPO微调重写器进一步提升意图重写质量和智能体规划效用。

Conclusion: 意图重写是提升开放域对话系统智能体规划性能的关键部分，通过更准确和精炼的意图表达可显著提升规划效果。

Abstract: Understanding user intent is essential for effective planning in
conversational assistants, particularly those powered by large language models
(LLMs) coordinating multiple agents. However, real-world dialogues are often
ambiguous, underspecified, or dynamic, making intent detection a persistent
challenge. Traditional classification-based approaches struggle to generalize
in open-ended settings, leading to brittle interpretations and poor downstream
planning. We propose RECAP (REwriting Conversations for Agent Planning), a new
benchmark designed to evaluate and advance intent rewriting, reframing
user-agent dialogues into concise representations of user goals. RECAP captures
diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal
conversations. Alongside the dataset, we introduce an LLM-based evaluator that
assesses planning utility given the rewritten intent. Using RECAP, we develop a
prompt-based rewriting approach that outperforms baselines. We further
demonstrate that fine-tuning two DPO-based rewriters yields additional utility
gains. Our results highlight intent rewriting as a critical and tractable
component for improving agent planning in open-domain dialogue systems.

</details>


### [31] [SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings](https://arxiv.org/abs/2509.04473)
*Jaekwon Yoo,Kunal Chandiramani,Divya Tadimeti,Abenezer Girma,Chandra Dhir*

Main category: cs.CL

TL;DR: 提出一种高效Adapter，将语音特征转为LLM输入，显著提升ASR、NER、SA等表现，同时标注和计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 将语音编码器集成进大型语言模型（LLM）需大量数据和资源，而现实中这类数据稀缺，导致实际应用受限。

Method: 提出一种参数高效的Adapter，将语音特征转为适用于LLM的Token，聚焦端到端自动语音识别（ASR）、命名实体识别（NER）和情感分析（SA）。利用LLM生成的数据增强技术减少标注成本，并结合分类器正则化与LoRA优化LLM。

Result: 该Adapter训练参数量减少7倍，但在各任务上有显著提升：ASR的词错误率相对下降26%、NER的F1分数相对提升6.3%、SA任务F1分数相对提升32%。结合分类器正则化和LoRA后，在SLUE评测上提升了6.6%和9.5%。

Conclusion: 参数高效的Adapter可提升多类语音任务性能，并能有效利用有限监督资源。高级优化手段进一步提升效果。

Abstract: While integrating speech encoder with LLM requires substantial data and
resources, use cases face limitations due to insufficient availability. To
address this, we propose a solution with a parameter-efficient adapter that
converts speech embeddings into LLM-compatible tokens, focusing on end-to-end
automatic speech recognition (ASR), named entity recognition (NER), and
sentiment analysis (SA). To reduce labeling costs, we employ an LLM-based
synthetic dataset annotation technique. The proposed adapter, using 7x fewer
trainable parameters, achieves significant performance gains: a 26% relative
Word Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative
F1 score increase on the NER task, and a 32% relative F1 score boost on the SA
task. Moreover, using advanced techniques such as adding a classifier
regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields
notable performance gains, with Spoken Language Understanding Evaluation (SLUE)
score improvement of 6.6% and 9.5%

</details>


### [32] [Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling](https://arxiv.org/abs/2509.04474)
*Shengyin Sun,Yiming Li,Xing Li,Yingzhao Lian,Weizhe Lin,Hui-Ling Zhen,Zhiyuan Yang,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文针对大语言模型推理扩展带来的计算冗余问题，首次提出面向Test-time scaling的猜测式解码评测基准，并系统对比三类主流方法。实验结果显示：简单的n-gram方法在加速重复推理上效果突出，建议与其他方法结合以平衡推理效率。该工作为高效LLM推理带来参考并推动后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过推理时（Test-time）增加计算资源可以提升其推理能力，但这种方式存在冗余和重复信息，导致效率低下。如何在保持推理能力的同时，提高推理效率，是一个亟待解决的问题。尽管猜测式解码（speculative decoding）有望缓解这一问题，但其在Test-time scaling这一特定场景下的表现尚未被系统研究。

Method: 该论文首次构建了一个用于评估猜测式解码方法在Test-time scaling加速中的基准测试集，同时覆盖了代表性的推理范式（如Best-of-N采样和多轮思考）。在该基准下，论文系统对比了三大类猜测式解码方法：基于模型、基于训练和基于n-gram方法。

Result: 实验发现，简单的n-gram方法能有效捕捉重复模式，在Test-time scaling中展现独特的加速潜力。将n-gram方法与模型或训练方法结合，有望兼顾重复与多样推理路径的效率。

Conclusion: 该基准测试为加速大模型推理提供了参考价值，并揭示了n-gram方法在特定场景下的独特优势。论文鼓励后续进一步研究更高效的猜测式解码方法，从而使LLMs推理更加快捷和实用。

Abstract: Test-time scaling has emerged as a powerful paradigm for enhancing the
reasoning capabilities of large language models (LLMs) by allocating additional
computational resources during inference. However, this paradigm is inherently
inefficient due to the generation of redundant and repetitive reasoning traces,
leading to significant computational overhead. Speculative decoding offers a
promising avenue for mitigating this inefficiency, yet its efficacy in the
structured, repetition-rich context of test-time scaling remains largely
unexplored. To bridge this gap, we introduce the first comprehensive benchmark
designed to evaluate speculative decoding methods for accelerating LLM
test-time scaling. Our benchmark provides consistent experimental protocols
across representative test-time scaling paradigms (e.g., Best-of-N sampling and
multi-round thinking), enabling a fair comparison of three major categories of
speculative decoding: model-based, training-based, and n-gram-based methods.
Extensive experiments reveal that simple n-gram-based methods effectively
capture repetitive patterns, demonstrating unique potential in accelerating
test-time scaling. This phenomenon demonstrates the value of integrating
n-gram-based methods with model-based or training-based approaches to balance
acceleration for both repetitive and diverse reasoning in test-time scaling. We
hope this benchmark spurs further research on speculative decoding for
test-time scaling, enabling faster and more practical reasoning in LLMs through
better handling of repetitive and diverse reasoning paths.

</details>


### [33] [ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute](https://arxiv.org/abs/2509.04475)
*Hao Wen,Yifan Su,Feifei Zhang,Yunxin Liu,Yunhao Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.CL

TL;DR: 传统串行推理扩展效果有限，ParaThinker通过并行多路推理显著提升模型表现，为LLM扩展带来新思路。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）主要通过增加推理时的串行运算（depth），以获得更强的推理能力，但这种策略面临瓶颈——计算量增加后效果提升有限。作者认为问题在于现有的“逐步推理”方法会导致模型在初期错误选择后难以自我修正，即“Tunnle Vision”。

Method: 本文提出了一种新的扩展范式：原生并行思维（native thought parallelism），并设计了ParaThinker框架，使LLM能够并行生成多条推理路径，并融合这些路径得到更优解。该系统通过并行探索多样化的思考路线，有效避免了‘隧道视野’问题。

Result: 在推理基准测试中，ParaThinker在1.5B参数和7B参数模型上实现了平均分别为12.3%和7.5%的准确率提升（并行路径为8条），同时延迟增加极小（7.1%）。

Conclusion: 与仅仅增加串行推理深度相比，拓展推理宽度（并行）是更高效、更有效的提升LLM推理能力方式。并行思维为未来LLM扩展提供了重要的新方向，即小模型也可通过并行推理超越大模型表现。

Abstract: Recent advances in Large Language Models (LLMs) have been driven by test-time
compute scaling - a strategy that improves reasoning by generating longer,
sequential thought processes. While effective, this approach encounters a
significant bottleneck as computation increases, where further computation
offers only marginal performance gains. We argue this ceiling is not an
inherent limit of the model's capability but a flaw in the scaling strategy
itself, a phenomenon we term "Tunnel Vision", where a model's imperfect initial
steps lock it into a suboptimal reasoning path. To overcome this, we introduce
a new scaling paradigm: native thought parallelism. We present ParaThinker, an
end-to-end framework that trains an LLM to generate multiple, diverse reasoning
paths in parallel and synthesize them into a superior final answer. By
exploring different lines of thoughts simultaneously, ParaThinker effectively
sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning
potential. Our approach demonstrates that scaling compute in parallel (width)
is a more effective and efficient way to superior reasoning than simply scaling
sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves
substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5%
for 7B models on average with 8 parallel paths), while adding only negligible
latency overhead (7.1%). This enables smaller models to surpass much larger
counterparts and establishes parallel thinking as a critical, efficient
dimension for scaling future LLMs.

</details>


### [34] [Training Text-to-Molecule Models with Context-Aware Tokenization](https://arxiv.org/abs/2509.04476)
*Seojin Kim,Hyeontae Song,Jaehyun Nam,Jinwoo Shin*

Main category: cs.CL

TL;DR: 本文提出的CAMT5模型通过子结构标记化与重要性训练方法，大幅超越现有文本到分子模型，且用更少的训练token实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到分子模型主要依赖于原子级标记化，只能建模分子的局部连通性，难以捕捉分子的全局结构语境，这限制了模型在化学应用中的表现。

Method: 提出了一种新的Context-Aware Molecular T5（CAMT5），引入了基于子结构的标记化方法，并基于这一方案，开发了重要性驱动的训练策略，优先关注关键子结构。此外，还提出了简单有效的集成方法提升生成性能。

Result: 在多项文本到分子的生成任务中，CAMT5验证优于现有方法，能用仅2%的训练token超过最新技术水平。集成策略进一步提升生成效果。

Conclusion: CAMT5通过子结构标记化和新训练策略，显著提升了文本到分子模型对分子结构和语义的理解能力，并以更高效的方式实现了更好的生成表现。

Abstract: Recently, text-to-molecule models have shown great potential across various
chemical applications, e.g., drug-discovery. These models adapt language models
to molecular data by representing molecules as sequences of atoms. However,
they rely on atom-level tokenizations, which primarily focus on modeling local
connectivity, thereby limiting the ability of models to capture the global
structural context within molecules. To tackle this issue, we propose a novel
text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by
the significance of the substructure-level contexts in understanding molecule
structures, e.g., ring systems, we introduce substructure-level tokenization
for text-to-molecule models. Building on our tokenization scheme, we develop an
importance-based training strategy that prioritizes key substructures, enabling
CAMT5 to better capture the molecular semantics. Extensive experiments verify
the superiority of CAMT5 in various text-to-molecule generation tasks.
Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using
only 2% of training tokens. In addition, we propose a simple yet effective
ensemble strategy that aggregates the outputs of text-to-molecule models to
further boost the generation performance. Code is available at
https://github.com/Songhyeontae/CAMT5.git.

</details>


### [35] [An End-to-End System for Culturally-Attuned Driving Feedback using a Dual-Component NLG Engine](https://arxiv.org/abs/2509.04478)
*Iniakpokeikiye Peter Thompson,Yi Dewei,Reiter Ehud*

Main category: cs.CL

TL;DR: 作者开发了一个面向尼日利亚司机的移动端安全驾驶反馈系统，通过自动监测和自然语言反馈，有效应对当地交通安全瓶颈，试点显示方案可行，有助于推动社会公益。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚等低资源环境下，交通安全问题突出，基础设施薄弱，亟需低成本、能适应本地需求的自动化安全驾驶反馈解决方案，以减少酒驾等本地特有的安全问题。

Method: 设计并实现了一个端到端的移动系统，系统架构包括自动行程检测、端上驾驶行为分析、基于双阶段反思过程的NLG生成反馈，以及检测酒驾的专用机器学习模型。系统还考虑了低资源环境下的网络不稳定和传感器噪声问题。

Result: 系统在90名司机中进行试点部署，能够检测到不安全驾驶行为并生成文化适应性的法律和行为反馈，初步结果显示系统架构具有实用性和适应性。

Conclusion: 本文提出的移动端安全驾驶反馈系统在尼日利亚的初步试点取得了可行性验证，能够有效检测并反馈驾驶过程中的不安全行为，具备社会影响力。

Abstract: This paper presents an end-to-end mobile system that delivers
culturally-attuned safe driving feedback to drivers in Nigeria, a low-resource
environment with significant infrastructural challenges. The core of the system
is a novel dual-component Natural Language Generation (NLG) engine that
provides both legally-grounded safety tips and persuasive, theory-driven
behavioural reports. We describe the complete system architecture, including an
automatic trip detection service, on-device behaviour analysis, and a
sophisticated NLG pipeline that leverages a two-step reflection process to
ensure high-quality feedback. The system also integrates a specialized machine
learning model for detecting alcohol-influenced driving, a key local safety
issue. The architecture is engineered for robustness against intermittent
connectivity and noisy sensor data. A pilot deployment with 90 drivers
demonstrates the viability of our approach, and initial results on detected
unsafe behaviours are presented. This work provides a framework for applying
data-to-text and AI systems to achieve social good.

</details>


### [36] [No Clustering, No Routing: How Transformers Actually Process Rare Tokens](https://arxiv.org/abs/2509.04479)
*Jing Liu*

Main category: cs.CL

TL;DR: 本研究揭示了大模型针对罕见词预测的内部机制，发现相关神经元呈分布式、非模块化的组织，并通过训练自发形成专门化，实现兼顾灵活性与容量分配。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预测罕见词时表现不佳，导致其专门化机制尚不清楚。已有研究发现针对罕见词的神经元存在特殊模式，但它们的具体功能组织尚未明了。

Method: 作者采用了神经元影响分析、基于图的聚类分析，以及对GPT-2 XL和Pythia模型的注意力头消融实验，深入探索神经元的组织方式和注意力机制对罕见词的处理。

Result: 1）处理罕见词需要额外的“plateau”神经元，且这一机制与常见词所需的神经元有别，形成双重计算模式。2）这些plateau神经元空间上分布离散，并未形成模块化聚集。3）注意力机制并未对专门化神经元有优先路由。

Conclusion: 罕见词的专门化是通过神经元的分布性、根据训练驱动的分化实现的，而无须依赖结构上的模块化，这保证了模型具备灵活性与自适应能力分配。

Abstract: Large language models struggle with rare token prediction, yet the mechanisms
driving their specialization remain unclear. Prior work identified specialized
``plateau'' neurons for rare tokens following distinctive three-regime
influence patterns \cite{liu2025emergent}, but their functional organization is
unknown. We investigate this through neuron influence analyses, graph-based
clustering, and attention head ablations in GPT-2 XL and Pythia models. Our
findings show that: (1) rare token processing requires additional plateau
neurons beyond the power-law regime sufficient for common tokens, forming dual
computational regimes; (2) plateau neurons are spatially distributed rather
than forming modular clusters; and (3) attention mechanisms exhibit no
preferential routing to specialists. These results demonstrate that rare token
specialization arises through distributed, training-driven differentiation
rather than architectural modularity, preserving context-sensitive flexibility
while achieving adaptive capacity allocation.

</details>


### [37] [Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition](https://arxiv.org/abs/2509.04480)
*Ryo Takahashi,Naoki Saito,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CL

TL;DR: 提出用离散提示调优方法实现个性化视觉情感识别，显著提升多模态大模型在实际应用中的个体情感识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉情感识别（VER）在现实应用中需求日益增长，尤其是对个体层面的情感识别。然而，现有多模态大语言模型（MLLMs）倾向于主流观点，对于个性化识别能力有限，这成为实际应用中的关键瓶颈。

Method: 作者提出了一种离散型提示调优方法，受到人类提示工程过程启发，通过为每个用户筛选最佳自然语言提示，并据此动态更新提示，以实现个性化高精度情感识别。

Result: 该方法能够有效提升MLLMs在个性化视觉情感识别任务中的表现，相较于直接使用原始MLLM模型，能更精准地适应个体情感差异。

Conclusion: 离散型提示调优能够显著改善现有MLLMs模型的个性化情感识别能力，为VER在个性化实际应用领域（如广告、舆情等）迈出关键一步。

Abstract: Visual Emotion Recognition (VER) is an important research topic due to its
wide range of applications, including opinion mining and advertisement design.
Extending this capability to recognize emotions at the individual level further
broadens its potential applications. Recently, Multimodal Large Language Models
(MLLMs) have attracted increasing attention and demonstrated performance
comparable to that of conventional VER methods. However, MLLMs are trained on
large and diverse datasets containing general opinions, which causes them to
favor majority viewpoints and familiar patterns. This tendency limits their
performance in a personalized VER, which is crucial for practical and
real-world applications, and indicates a key area for improvement. To address
this limitation, the proposed method employs discrete prompt tuning inspired by
the process of humans' prompt engineering to adapt the VER task to each
individual. Our method selects the best natural language representation from
the generated prompts and uses it to update the prompt for the realization of
accurate personalized VER.

</details>


### [38] [Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare](https://arxiv.org/abs/2509.04482)
*Ravi Shankar,Sheng Wong,Lin Li,Magdalena Bachmann,Alex Silverthorne,Beth Albert,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: 作者提出了能量模型（EBM）用于RAG系统安全弃权决策，在语义困难问题上，EBM显著优于softmax和kNN，能更可信地识别不该作答的情境，特别适合高安全需求场景。


<details>
  <summary>Details</summary>
Motivation: 在像女性健康等需要高安全性的领域中，检索增强生成（RAG）系统如果作出错误的回答可能危及用户健康，因此，开发能在不确定时可靠选择“放弃回答”的机制变得至关重要。现有的概率置信度模型在语义复杂情况下难以胜任，因此需探索更稳健的置信度评估方法。

Method: 提出了一种基于能量的模型（EBM），在2.6百万条指南推导问题的密集语义语料库上学习平滑能量分布，驱动系统自动决策“生成或弃权”。并与校准softmax基线方法和k近邻（kNN）密度启发式在容易和困难（语义接近分布的挑战性）弃权情形下进行对比，以及结合消融实验深入分析不同负例采样和数据暴露方式对模型表现的影响。

Result: EBM在语义困难的弃权样本上表现优异，AUROC达0.961，优于softmax的0.950，并有效减少FPR@95（0.235 vs 0.331）；在语义简单样本上各方法表现相近。通过消融分析发现，模型的鲁棒性主要归功于能量评分头，而不同负例类型的选择会细化决策边界，但并非泛化至难例的关键因素。

Conclusion: 相比基于概率的softmax置信度，基于能量的弃权评分能提供更稳定且可解释的信号，使RAG系统在安全关键情境下更可靠地选择弃权，形成可扩展与可信赖的安全基础。

Abstract: Reliable abstention is critical for retrieval-augmented generation (RAG)
systems, particularly in safety-critical domains such as women's health, where
incorrect answers can lead to harm. We present an energy-based model (EBM) that
learns a smooth energy landscape over a dense semantic corpus of 2.6M
guideline-derived questions, enabling the system to decide when to generate or
abstain. We benchmark the EBM against a calibrated softmax baseline and a
k-nearest neighbour (kNN) density heuristic across both easy and hard
abstention splits, where hard cases are semantically challenging
near-distribution queries. The EBM achieves superior abstention performance
abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for
softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,
performance is comparable across methods, but the EBM's advantage becomes most
pronounced in safety-critical hard distributions. A comprehensive ablation with
controlled negative sampling and fair data exposure shows that robustness stems
primarily from the energy scoring head, while the inclusion or exclusion of
specific negative types (hard, easy, mixed) sharpens decision boundaries but is
not essential for generalisation to hard cases. These results demonstrate that
energy-based abstention scoring offers a more reliable confidence signal than
probability-based softmax confidence, providing a scalable and interpretable
foundation for safe RAG systems.

</details>


### [39] [DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs](https://arxiv.org/abs/2509.04483)
*Minghui Huang*

Main category: cs.CL

TL;DR: 本文提出三项文本主张拆解质量自动评估指标，并据此优化模型，助力事实核查任务，提升主张拆解的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 事实核查任务中，将复杂主张拆解为简单的原子主张有助于更好地判断主张的真实性。但目前相关研究多关注于生成方法本身，较少考量拆解结果主张的质量评价。为了解决这一问题，作者提出了新的评价指标体系。

Method: 作者提出了DecMetrics体系，包括COMPLETENESS、CORRECTNESS和SEMANTIC ENTROPY三项指标，用于自动评估拆解模型产生的主张质量。此外，作者利用这些指标作为奖励函数，训练并优化了一个轻量级的主张拆解模型。

Result: 通过自动化的评价方法，所提出的指标体系和优化模型为主张拆解任务设立了新的基准，提升了事实核查系统的可靠性和有效性。

Conclusion: DecMetrics为主张拆解领域提供了系统的自动化质量评价手段，有效改进了相关模型，并为事实核查流程带来积极影响。

Abstract: Claim decomposition plays a crucial role in the fact-checking process by
breaking down complex claims into simpler atomic components and identifying
their unfactual elements. Despite its importance, current research primarily
focuses on generative methods for decomposition, with insufficient emphasis on
evaluating the quality of these decomposed atomic claims. To bridge this gap,
we introduce \textbf{DecMetrics}, which comprises three new metrics:
\texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY},
designed to automatically assess the quality of claims produced by
decomposition models. Utilizing these metrics, we develop a lightweight claim
decomposition model, optimizing its performance through the integration of
these metrics as a reward function. Through automatic evaluation, our approach
aims to set a benchmark for claim decomposition, enhancing both the reliability
and effectiveness of fact-checking systems.

</details>


### [40] [The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors](https://arxiv.org/abs/2509.04484)
*Abdelrahman Sadallah,Tim Baumgärtner,Iryna Gurevych,Ted Briscoe*

Main category: cs.CL

TL;DR: 本文提出了评审评论的四大评估标准，并发布RevUtil数据集，支持自动化评审评论质量提升。微调模型在四个方面表现与人工接近甚至超过业界顶尖模型，但机器生成评论质量仍不及人工。


<details>
  <summary>Details</summary>
Motivation: 由于评审者时间有限，自动化支持系统对提升学术评审质量变得越来越重要，尤其在评论反馈的有效性方面。

Method: 提出了评估评审评论的四个核心方面：可操作性、依据与具体性、可验证性、和有帮助性。引入了RevUtil数据集，包括1,430个人工标注评论及10,000条合成标注评论，并附带评分解释。基于该数据集，对评估模型进行了微调，并与GPT-4o等强大模型进行了对比实验。

Result: 微调模型在评论评估方面与人工水平相当，部分情况下甚至优于GPT-4o等大型闭源模型。机器生成的评论在四个关键方面整体表现不如人工评论。

Conclusion: RevUtil数据集可用于开发和评估自动评论评估模型，提升评审意见的有效性。自动化模型已实现与人类评审一致性，但机器评论整体质量仍有提升空间。

Abstract: Providing constructive feedback to paper authors is a core component of peer
review. With reviewers increasingly having less time to perform reviews,
automated support systems are required to ensure high reviewing quality, thus
making the feedback in reviews useful for authors. To this end, we identify
four key aspects of review comments (individual points in weakness sections of
reviews) that drive the utility for authors: Actionability, Grounding &
Specificity, Verifiability, and Helpfulness. To enable evaluation and
development of models assessing review comments, we introduce the RevUtil
dataset. We collect 1,430 human-labeled review comments and scale our data with
10k synthetically labeled comments for training purposes. The synthetic data
additionally contains rationales, i.e., explanations for the aspect score of a
review comment. Employing the RevUtil dataset, we benchmark fine-tuned models
for assessing review comments on these aspects and generating rationales. Our
experiments demonstrate that these fine-tuned models achieve agreement levels
with humans comparable to, and in some cases exceeding, those of powerful
closed models like GPT-4o. Our analysis further reveals that machine-generated
reviews generally underperform human reviews on our four aspects.

</details>


### [41] [ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records](https://arxiv.org/abs/2509.04485)
*Chris Sainsbury,Andreas Karwath*

Main category: cs.CL

TL;DR: 本文提出ASCENDgpt模型，利用表型聚合和预训练方法，有效提升心血管结局风险预测，兼顾高准确性、效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病风险预测对于临床决策和患者管理极为重要，但现有方法常因诊断编码碎片化且信息难以利用而受到限制，需要一种高效、可解释且性能优良的预测方法。

Method: 提出了ASCENDgpt，一种基于transformer的模型，专为纵向电子健康记录（EHR）上的心血管风险预测设计。创新点包括设计了表型感知的分词方法，将47,155个ICD编码映射为176个临床表型token，实现语义整合与特征压缩。使用遮蔽语言模型进行预训练，然后在19402名个体数据上针对五种心血管结局进行微调预测。

Result: ASCENDgpt在测试集上的平均C-index为0.816，各项心血管结局的C-index均实现较高水平（心梗0.792，卒中0.824，MACE 0.800，心血管死亡0.842，全因死亡0.824）。表型分词方案实现了77.9%的词汇量缩减，同时保持高语义信息，预测结果兼具临床解释性和计算效率。

Conclusion: ASCENDgpt结合领域特异性分词与预训练，极大提升了EHR数据驱动的心血管风险预测准确性和可解释性，验证了其作为临床辅助工具的潜力。

Abstract: We present ASCENDgpt, a transformer-based model specifically designed for
cardiovascular risk prediction from longitudinal electronic health records
(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme
that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,
achieving 99.6\% consolidation of diagnosis codes while preserving semantic
information. This phenotype mapping contributes to a total vocabulary of 10,442
tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We
pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a
masked language modeling objective, then fine-tune for time-to-event prediction
of five cardiovascular outcomes: myocardial infarction (MI), stroke, major
adverse cardiovascular events (MACE), cardiovascular death, and all-cause
mortality. Our model achieves excellent discrimination on the held-out test set
with an average C-index of 0.816, demonstrating strong performance across all
outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,
all-cause mortality: 0.824). The phenotype-based approach enables clinically
interpretable predictions while maintaining computational efficiency. Our work
demonstrates the effectiveness of domain-specific tokenization and pretraining
for EHR-based risk prediction tasks.

</details>


### [42] [Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition](https://arxiv.org/abs/2509.04488)
*Hao Shi,Yusuke Fujita,Tomoya Mizumoto,Lianbo Liu,Atsushi Kojima,Yui Sudo*

Main category: cs.CL

TL;DR: 论文提出用结构化序列化输出提示（SOP）引导LLM，显著提升了多说话人ASR系统在复杂混说环境下的识别性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于大语言模型（LLM）的多说话人自动语音识别（ASR）系统在任务定义和提升效果中对于prompts （提示）设计不足。现有系统要么忽略prompts，要么仅用简单的任务定义提示，尚未专门研究如何设计prompts以增强性能。

Method: 本文提出通过抽取序列化输出提示（SOP），使用结构化提示显式引导LLM，从而提升多说话人ASR系统性能。具体做法为：在语音编码器后插入分离器和序列化CTC层，以完成混合语音内容的分离与提取；序列化CTC输出通过贪婪搜索获得SOP作为LLM的提示。训练方面，设计了三阶段训练策略，包括SOT微调、序列化语音信息提取和基于SOP的自适应。

Result: 实验显示，尽管LLM-SOT模型在双说话人场景下效果良好，但在更复杂的三说话人场景下效果有限。所提SOP方法则在双说话人和三说话人条件下均显著提升了系统性能。

Conclusion: 结构化的SOP提示能显著提升LLM驱动的多说话人ASR系统性能，尤其在复杂的多说话人环境下，该方法优于现有基线。

Abstract: Prompts are crucial for task definition and for improving the performance of
large language models (LLM)-based systems. However, existing LLM-based
multi-talker (MT) automatic speech recognition (ASR) systems either omit
prompts or rely on simple task-definition prompts, with no prior work exploring
the design of prompts to enhance performance. In this paper, we propose
extracting serialized output prompts (SOP) and explicitly guiding the LLM using
structured prompts to improve system performance (SOP-MT-ASR). A Separator and
serialized Connectionist Temporal Classification (CTC) layers are inserted
after the speech encoder to separate and extract MT content from the mixed
speech encoding in a first-speaking-first-out manner. Subsequently, the SOP,
which serves as a prompt for LLMs, is obtained by decoding the serialized CTC
outputs using greedy search. To train the model effectively, we design a
three-stage training strategy, consisting of serialized output training (SOT)
fine-tuning, serialized speech information extraction, and SOP-based
adaptation. Experimental results on the LibriMix dataset show that, although
the LLM-based SOT model performs well in the two-talker scenario, it fails to
fully leverage LLMs under more complex conditions, such as the three-talker
scenario. The proposed SOP approach significantly improved performance under
both two- and three-talker conditions.

</details>


### [43] [Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR](https://arxiv.org/abs/2509.04491)
*Xinnian Zhao,Hugo Van Hamme*

Main category: cs.CL

TL;DR: 该研究创新性地将对齐不精确的电视字幕作为上下文提示，结合加权注意力机制，用于弱监督ASR训练，取得了明显的识别准确率提升。


<details>
  <summary>Details</summary>
Motivation: 尽管电视字幕数据易得，但由于与音频对齐不精确，直接作为ASR的标注存在局限。研究者希望利用海量但不完美的字幕以更高效地提升ASR性能。

Method: 将字幕作为弱监督的上下文提示而非直接监督信号，模型通过伪转录文本作为主要学习对象，同时引入加权注意力机制，突出字幕中的关键信息，实现迭代优化。

Result: 实验显示，模型的转录准确率大幅提升，提供了高质量的伪标签数据集，有助于训练更鲁棒的ASR系统。

Conclusion: 所提出的方法能够充分利用对齐不精确的字幕资源，通过上下文提示和注意力机制，显著提升弱监督ASR的性能。

Abstract: This study proposes a novel approach to using TV subtitles within a weakly
supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV
subtitles are readily available, their imprecise alignment with corresponding
audio limits their applicability as supervised targets for verbatim
transcription. Rather than using subtitles as direct supervision signals, our
method reimagines them as context-rich prompts. This design enables the model
to handle discrepancies between spoken audio and subtitle text. Instead,
generated pseudo transcripts become the primary targets, with subtitles acting
as guiding cues for iterative refinement. To further enhance the process, we
introduce a weighted attention mechanism that emphasizes relevant subtitle
tokens during inference. Our experiments demonstrate significant improvements
in transcription accuracy, highlighting the effectiveness of the proposed
method in refining transcripts. These enhanced pseudo-labeled datasets provide
high-quality foundational resources for training robust ASR systems.

</details>


### [44] [Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate](https://arxiv.org/abs/2509.04492)
*Charles Moslonka,Hicham Randrianarivo,Arthur Garnier,Emmanuel Malherbe*

Main category: cs.CL

TL;DR: 提出了一种面向黑盒API的简单实用的LLM幻觉检测方法，仅用top-k token概率显著提升QA结果的可信度，在金融等场景也具实际效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在问答任务中的幻觉问题，严重影响了其实际应用的可靠性。尤其是在只能获取有限数据、与黑盒LLM API交互的场景下，难以及时发现与纠正幻觉现象。

Method: 提出一种应用型、一站式幻觉检测方法，只需利用LLM API每个token少量top-k候选的log-probabilities。基于这些有限的概率信息，作者推导了熵生成率（EPR）指标作为基线，并结合监督学习增强检测性能。方法充分利用单次生成时可访问的少数token概率特征，无需多次查询。

Result: 在多个QA数据集与不同LLM模型上实验证明，结合监督学习的检测器比单独依赖EPR基线有效提升了幻觉检测准确性。即使仅用每个token的前十个概率，方法仍具备高效与实用性。

Conclusion: 本文提出的方法为仅能访问有限候选概率的黑盒API场景提供了高效的幻觉检测工具，不仅能提升QA和RAG系统的可信度，还能直接应用于如财经年报分析等实际工业场景。

Abstract: Hallucinations in Large Language Model (LLM) outputs for Question Answering
(QA) tasks critically undermine their real-world reliability. This paper
introduces an applied methodology for robust, one-shot hallucination detection,
specifically designed for scenarios with limited data access, such as
interacting with black-box LLM APIs that typically expose only a few top
candidate log-probabilities per token. Our approach derives uncertainty
indicators directly from these readily available log-probabilities generated
during non-greedy decoding. We first derive an Entropy Production Rate (EPR)
metric that offers baseline performance, later augmented with supervised
learning. Our learned model uses features representing the entropic
contributions of the accessible top-ranked tokens within a single generated
sequence, requiring no multiple query re-runs. Evaluated across diverse QA
datasets and multiple LLMs, this estimator significantly improves hallucination
detection over using EPR alone. Crucially, high performance is demonstrated
using only the typically small set of available log-probabilities (e.g., top
<10 per token), confirming its practical efficiency and suitability for these
API-constrained deployments. This work provides a readily deployable technique
to enhance the trustworthiness of LLM responses from a single generation pass
in QA and Retrieval-Augmented Generation (RAG) systems, with its utility
further demonstrated in a finance framework analyzing responses to queries on
annual reports from an industrial dataset.

</details>


### [45] [A Narrative-Driven Computational Framework for Clinician Burnout Surveillance](https://arxiv.org/abs/2509.04497)
*Syed Ahmad Chan Bukhari,Fazel Keshtkar,Alyssa Meczkowska*

Main category: cs.CL

TL;DR: 该研究提出了一种整合神经网络情感分析、压力词典和主题模型的混合方法，分析ICU临床笔记实现了更准确的医护人员倦怠风险预测，对主动监测医护福祉具有实用意义。


<details>
  <summary>Details</summary>
Motivation: 临床医生的倦怠感对患者安全尤其是在重症监护室（ICU）中构成重大威胁，目前相关研究多依赖回顾性调查或广泛的电子健康记录（EHR）元数据，常常忽略了临床记录中的叙述性信息。

Method: 本研究分析了来源于MIMIC-IV的1万份ICU出院小结，提出一种混合管道，结合针对临床文本微调的BioBERT情感嵌入、专为倦怠监测定制的词汇压力词典以及基于工作负荷代理的五主题LDA，最终通过logistic回归分类器对提供者进行（倦怠）风险预测。

Result: 该方法在分层保留集上实现了0.80的精确度、0.89的召回率和0.84的F1分数，F1分数相比仅使用元数据的基线提高了至少0.17。专业细分分析表明，放射科、精神科和神经科医生的倦怠风险更高。

Conclusion: ICU临床记录叙述信息中蕴含着可用于主动监测医护人员福祉的信号，相关方法能有效识别高风险群体。

Abstract: Clinician burnout poses a substantial threat to patient safety, particularly
in high-acuity intensive care units (ICUs). Existing research predominantly
relies on retrospective survey tools or broad electronic health record (EHR)
metadata, often overlooking the valuable narrative information embedded in
clinical notes. In this study, we analyze 10,000 ICU discharge summaries from
MIMIC-IV, a publicly available database derived from the electronic health
records of Beth Israel Deaconess Medical Center. The dataset encompasses
diverse patient data, including vital signs, medical orders, diagnoses,
procedures, treatments, and deidentified free-text clinical notes. We introduce
a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for
clinical narratives, a lexical stress lexicon tailored for clinician burnout
surveillance, and five-topic latent Dirichlet allocation (LDA) with workload
proxies. A provider-level logistic regression classifier achieves a precision
of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out
set, surpassing metadata-only baselines by greater than or equal to 0.17 F1
score. Specialty-specific analysis indicates elevated burnout risk among
providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate
that ICU clinical narratives contain actionable signals for proactive
well-being monitoring.

</details>


### [46] [Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations](https://arxiv.org/abs/2509.04498)
*Krithi Shailya,Akhilesh Kumar Mishra,Gokul S Krishnan,Balaraman Ravindran*

Main category: cs.CL

TL;DR: 三款主流开源LLM在高校推荐中存在地域和性别等严重偏见，尽管推荐多样性有所差异，但系统性不公平普遍存在。作者提出新评估框架并呼吁关注教育AI公平性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型(LLMs)被广泛用于教育规划等日常推荐系统，其推荐结果有可能固化社会偏见。因此，需要对LLM的推荐偏差进行系统性研究。

Method: 设计了360个性别、国籍和经济状况不同的模拟用户画像，测试三款开源LLM（LLaMA-3.1-8B、Gemma-7B和Mistral-7B）对大学及专业的推荐行为，并对25000余条推荐结果进行实证分析。提出了一个超越准确率、评估人口与地理代表性的多维评估框架。

Result: LLMs在推荐大学时表现出明显的偏见：更倾向于推荐全球北方院校，强化性别刻板印象，且推荐结果中某些机构出现高度重复。LLaMA-3.1-8B在多样性方面表现最佳，覆盖了58个国家的481所大学，但系统性的差异依然存在。

Conclusion: 当前LLM在教育推荐领域存在严重的地理、人口及经济偏见。为实现全球教育公平，急需对教育类语言模型的偏差进行深入考量和改善。

Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation
systems for tasks like education planning, yet their recommendations risk
perpetuating societal biases. This paper empirically examines geographic,
demographic, and economic biases in university and program suggestions from
three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360
simulated user profiles varying by gender, nationality, and economic status, we
analyze over 25,000 recommendations. Results show strong biases: institutions
in the Global North are disproportionately favored, recommendations often
reinforce gender stereotypes, and institutional repetition is prevalent. While
LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities
across 58 countries, systemic disparities persist. To quantify these issues, we
propose a novel, multi-dimensional evaluation framework that goes beyond
accuracy by measuring demographic and geographic representation. Our findings
highlight the urgent need for bias consideration in educational LMs to ensure
equitable global access to higher education.

</details>


### [47] [DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence](https://arxiv.org/abs/2509.04499)
*Pranav Narayanan Venkit,Philippe Laban,Yilun Zhou,Kung-Hsiang Huang,Yixin Mao,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出DeepTRACE，审计主流AI搜索/研究系统的答案与引用可靠性。结果发现它们常出现一边倒且缺乏依据的自信回答，即便增强型配置也未能彻底解决偏颇和引用准确率等问题。


<details>
  <summary>Details</summary>
Motivation: 生成式搜索引擎和深度研究型大模型承诺提供有据、可信的答案，但实际中用户常常遇到置信度过高、引用不清和实践混乱等问题，因此需要建立一个系统化的评估和审计框架来量化和追踪这些失败点。

Method: 提出了DeepTRACE——一个以社会技术为基础的审计框架，通过将社区总结的失败案例转化为八个可量化的维度，涵盖回答文本、来源和引用。该框架采用声明级分析（包括分解、置信度评分）和引文&事实支持矩阵，结合自动化提取流程和经验证与人类标注高度一致的大模型判别器，对主流模型及检索配置进行测评。

Result: （1）受测系统在争议性问题上普遍表现为高置信但内容偏颇；（2）许多给出的内容未被所列引用支撑；（3）深度研究配置有助于提升引用的完整度和减少盲目自信，但依旧没能彻底解决内容偏颇和引用准确性不高等核心问题；（4）不同系统在引用准确率等重要指标上的表现差异显著。

Conclusion: 主流的生成式搜索引擎和深度研究智能体在处理争议性问题时，依然存在过度自信、引用偏单一以及大比例内容缺乏事实依据等问题；尽管深度研究配置可以提升引用的完整度和降低自信，但在争议问题上依然明显一边倒，引用准确率系统间差异较大（40%到80%）。

Abstract: Generative search engines and deep research LLM agents promise trustworthy,
source-grounded synthesis, yet users regularly encounter overconfidence, weak
sourcing, and confusing citation practices. We introduce DeepTRACE, a novel
sociotechnically grounded audit framework that turns prior community-identified
failure cases into eight measurable dimensions spanning answer text, sources,
and citations. DeepTRACE uses statement-level analysis (decomposition,
confidence scoring) and builds citation and factual-support matrices to audit
how systems reason with and attribute evidence end-to-end. Using automated
extraction pipelines for popular public models (e.g., GPT-4.5/5, You.com,
Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to
human raters, we evaluate both web-search engines and deep-research
configurations. Our findings show that generative search engines and deep
research agents frequently produce one-sided, highly confident responses on
debate queries and include large fractions of statements unsupported by their
own listed sources. Deep-research configurations reduce overconfidence and can
attain high citation thoroughness, but they remain highly one-sided on debate
queries and still exhibit large fractions of unsupported statements, with
citation accuracy ranging from 40--80% across systems.

</details>


### [48] [Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts](https://arxiv.org/abs/2509.04500)
*Rushi Wang,Jiateng Liu,Cheng Qian,Yifan Shen,Yanzhou Pan,Zhaozhuo Xu,Ahmed Abbasi,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型容易被上下文中微量不当内容误导，提出基于类神经科学模型的微调新方法RW-Steering，可大幅提升模型鲁棒性和实际应用安全性，质量提升近40%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在加入外部上下文后能显著提升回答质量，但现实中的上下文常混有大量无关或不当内容，容易降低模型可靠性。作者希望理解LLM在面对混杂内容时如何处理和优先响应不同信息。

Method: 提出了Poisoned Context Testbed，构建包含相关与不当内容的现实世界上下文，并借鉴动物联想学习中的Rescorla-Wagner（RW）模型框架，量化上下文中竞争信号对LLM输出的影响。进一步提出RW-Steering：一种基于两阶段微调的方法，通过引导模型识别并忽略不当信号，无需对各类上下文混合情况进行过多监督。

Result: 发现LLM存在强烈倾向去吸收上下文中数量较少的信息，这在实际应用中会让少量不当内容极大影响输出质量。实验证明RW-Steering能有效抑制该弱点，模型微调后整体回答质量提升39.8%，并显著增强模型对不当内容的鲁棒性。

Conclusion: 提出的新方法RW-Steering显著提高了LLM应对混乱上下文下的安全性和稳定性，对实际部署应用有重要意义。方法不依赖大规模监督，适用于各种不同行为分布的现实复杂场景。

Abstract: Incorporating external context can significantly enhance the response quality
of Large Language Models (LLMs). However, real-world contexts often mix
relevant information with disproportionate inappropriate content, posing
reliability risks. How do LLMs process and prioritize mixed context? To study
this, we introduce the Poisoned Context Testbed, pairing queries with
real-world contexts containing relevant and inappropriate content. Inspired by
associative learning in animals, we adapt the Rescorla-Wagner (RW) model from
neuroscience to quantify how competing contextual signals influence LLM
outputs. Our adapted model reveals a consistent behavioral pattern: LLMs
exhibit a strong tendency to incorporate information that is less prevalent in
the context. This susceptibility is harmful in real-world settings, where small
amounts of inappropriate content can substantially degrade response quality.
Empirical evaluations on our testbed further confirm this vulnerability. To
tackle this, we introduce RW-Steering, a two-stage finetuning-based approach
that enables the model to internally identify and ignore inappropriate signals.
Unlike prior methods that rely on extensive supervision across diverse context
mixtures, RW-Steering generalizes robustly across varying proportions of
inappropriate content. Experiments show that our best fine-tuned model improves
response quality by 39.8% and reverses the undesirable behavior curve,
establishing RW-Steering as a robust, generalizable context engineering
solution for improving LLM safety in real-world use.

</details>


### [49] [Understanding Reinforcement Learning for Model Training, and future directions with GRAPE](https://arxiv.org/abs/2509.04501)
*Rohit Patel*

Main category: cs.CL

TL;DR: 本文面向大模型，逐步直观讲解主流指令微调算法，并提出了一种新算法GRAPE，便于理解和后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有关于指令微调关键算法的讲解往往假设读者有先验知识、缺乏关键细节或过于复杂，不利于理解。为消除歧义和降低学习门槛，作者希望以LLM为背景，用简明直观的方式阐述指令微调相关算法。

Method: 从头、逐步、用简化且与LLM相关的符号，详细介绍SFT、拒绝采样、REINFORCE、TRPO、PPO、GRPO、DPO等指令微调算法，剖析每一步的实际操作和原理；随后回顾相关新算法和技术，最后提出新的GRAPE算法供进一步研究。

Result: 清晰梳理相关算法并用通俗易懂的符号消除传统讲解中的歧义和理解难度，提出了新方法GRAPE供后续探索。

Conclusion: 本文以LLM为核心，系统梳理并简化了主流指令微调算法的解释流程，有助于消除模糊表述和理解障碍，同时拓展了算法可能性。

Abstract: This paper provides a self-contained, from-scratch, exposition of key
algorithms for instruction tuning of models: SFT, Rejection Sampling,
REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy
Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct
Preference Optimization (DPO). Explanations of these algorithms often assume
prior knowledge, lack critical details, and/or are overly generalized and
complex. Here, each method is discussed and developed step by step using
simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity
and provide a clear and intuitive understanding of the concepts. By minimizing
detours into the broader RL literature and connecting concepts to LLMs, we
eliminate superfluous abstractions and reduce cognitive overhead. Following
this exposition, we provide a literature review of new techniques and
approaches beyond those detailed. Finally, new ideas for research and
exploration in the form of GRAPE (Generalized Relative Advantage Policy
Evolution) are presented.

</details>


### [50] [VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples](https://arxiv.org/abs/2509.04502)
*Qixin Sun,Ziqin Wang,Hengyuan Zhao,Yilin Li,Kaiyou Song,Linjiang Huang,Xiaolin Hu,Qingpei Guo,Si Liu*

Main category: cs.CL

TL;DR: 该论文针对现有RAG受限于检索器精度的问题，提出了结合链式思维的新数据集和偏好建模方法，有效提升了大模型在复杂检索和推理任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG（检索增强生成）方法在结合外部知识以提升大语言模型（LLM）表现方面有优势，尤其在实时查询和视觉问答等任务中。然而，RAG的表现通常受限于检索器的准确性——很多输入到生成阶段的样本与问题无关甚至误导，从而成为LLM性能的瓶颈。

Method: 提出VaccineRAG数据集，将链式思维（Chain-of-Thought, CoT）引入RAG，以提升样本甄别能力，并设计了Partial-GRPO方法，将长序列复杂CoT内容建模为多个组成部分，而非整体，从而让模型能对复杂序列做出更明智的偏好选择。

Result: 通过在VaccineRAG数据集上的全面评估和消融实验，验证了所提出方案的有效性。

Conclusion: VaccineRAG能有效提升LLM区分正负样本的能力，通过CoT分析和部分序列偏好建模策略克服了检索阶段噪音带来的影响，显著增强了RAG整体机制的表现。

Abstract: Retrieval Augmented Generation enhances the response accuracy of Large
Language Models (LLMs) by integrating retrieval and generation modules with
external knowledge, demonstrating particular strength in real-time queries and
Visual Question Answering tasks. However, the effectiveness of RAG is
frequently hindered by the precision of the retriever: many retrieved samples
fed into the generation phase are irrelevant or misleading, posing a critical
bottleneck to LLMs' performance. To address this challenge, we introduce
VaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation
dataset. On one hand, VaccineRAG employs a benchmark to evaluate models using
data with varying positive/negative sample ratios, systematically exposing
inherent weaknesses in current LLMs. On the other hand, it enhances models'
sample-discrimination capabilities by prompting LLMs to generate explicit
Chain-of-Thought (CoT) analysis for each sample before producing final answers.
Furthermore, to enhance the model's ability to learn long-sequence complex CoT
content, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple
components rather than a single whole, our model can make more informed
preference selections for complex sequences, thereby enhancing its capacity to
learn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG
validate the effectiveness of the proposed scheme. The code and dataset will be
publicly released soon.

</details>


### [51] [Behavioral Fingerprinting of Large Language Models](https://arxiv.org/abs/2509.04504)
*Zehua Pei,Hui-Ling Zhen,Ying Zhang,Zhiyuan Yang,Xing Li,Xianzhi Yu,Mingxuan Yuan,Bei Yu*

Main category: cs.CL

TL;DR: 本文提出行为指纹评测框架，通过诊断性提示和自动评判，分析18个大模型的行为特征，发现高级能力收敛但行为风格差异显著，并追溯其与开发者对齐策略的关系。该方法可促进LLM多维度评估与发展。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）的评测多数集中在性能指标，难以捕捉模型间的细微行为差异。该文希望建立能刻画模型行为特征的新方法，以推动更全面评估。

Method: 提出“行为指纹”框架：基于精心设计的诊断性提示套件，通过创新的自动化评估流程（由强大的LLM充当公正评委），对18个不同能力层次的模型进行分析。

Result: 核心能力如抽象和因果推理在顶尖模型间趋于收敛，但诸如逢迎行为、语义鲁棒性等对齐相关行为的差异巨大；发现模型默认人格聚集现象（多为ISTJ/ESTJ），很可能与主流对齐激励有关。

Conclusion: 模型的互动本质不是规模或推理能力的自然产物，而是开发者特定且高度分化的对齐策略直接结果。所提框架可重复、可扩展，有助于揭示模型深层行为差异。

Abstract: Current benchmarks for Large Language Models (LLMs) primarily focus on
performance metrics, often failing to capture the nuanced behavioral
characteristics that differentiate them. This paper introduces a novel
``Behavioral Fingerprinting'' framework designed to move beyond traditional
evaluation by creating a multi-faceted profile of a model's intrinsic cognitive
and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an
innovative, automated evaluation pipeline where a powerful LLM acts as an
impartial judge, we analyze eighteen models across capability tiers. Our
results reveal a critical divergence in the LLM landscape: while core
capabilities like abstract and causal reasoning are converging among top
models, alignment-related behaviors such as sycophancy and semantic robustness
vary dramatically. We further document a cross-model default persona clustering
(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,
this suggests that a model's interactive nature is not an emergent property of
its scale or reasoning power, but a direct consequence of specific, and highly
variable, developer alignment strategies. Our framework provides a reproducible
and scalable methodology for uncovering these deep behavioral differences.
Project: https://github.com/JarvisPei/Behavioral-Fingerprinting

</details>


### [52] [From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach](https://arxiv.org/abs/2509.04507)
*Nithyashree Sivasubramaniam*

Main category: cs.CL

TL;DR: 本研究提出了一种结合Transformer声学模型和LLM的自动语音识别框架，有效提升了无声语音接口生成语音的识别效果，显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 无声语音接口（SSI）能够根据非声学信号生成可懂的语音，但在语音识别和后续处理方面的研究相对有限，且合成语音常存在音素模糊和噪音。一些现有方法在语音生成上取得进展，但在下游识别和稳健性提升上仍有不足。

Method: 提出了一种增强型自动语音识别框架，将基于Transformer的声学模型与大型语言模型（LLM）结合，用于无声语音接口中的语音识别与后处理。Transformer模型用于获取全语句上下文，LLM用于保障语言一致性。

Result: 实验表明，在36%的基线WER（词错误率）下，该方法实现了16%的相对和6%的绝对WER降低，展示了显著的识别和可懂度提升。

Conclusion: 所提出的方法能有效提升无声语音接口中合成语音的识别准确性和智能性，为SSI的实用化发展提供有力支持。

Abstract: Silent Speech Interfaces (SSIs) have gained attention for their ability to
generate intelligible speech from non-acoustic signals. While significant
progress has been made in advancing speech generation pipelines, limited work
has addressed the recognition and downstream processing of synthesized speech,
which often suffers from phonetic ambiguity and noise. To overcome these
challenges, we propose an enhanced automatic speech recognition framework that
combines a transformer-based acoustic model with a large language model (LLM)
for post-processing. The transformer captures full utterance context, while the
LLM ensures linguistic consistency. Experimental results show a 16% relative
and 6% absolute reduction in word error rate (WER) over a 36% baseline,
demonstrating substantial improvements in intelligibility for silent speech
interfaces.

</details>


### [53] [ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models](https://arxiv.org/abs/2509.04508)
*Biddut Sarker Bijoy,Mohammad Saqib Hasan,Pegah Alipoormolabashi,Avirup Sil,Aruna Balasubramanian,Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: 与单一的大型语言模型相比，通过多代理协作和逐步课程式训练，可以显著提升小型模型的效能，兼具高效率和高表现，多代理小型模型在复杂任务中极具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在复杂任务中表现优异，但资源消耗大。小型语言模型（SLMs）如果能通过多代理协作有效解决复杂问题，将成为更高效替代方案。本工作旨在探究多代理SLM系统与单代理LLM系统在效能与效率上的权衡。

Method: 在AppWorld环境中，分别使用不同规模的语言模型构建单代理和多代理系统。通过实验比较SLMs和LLMs在复杂任务中的表现，并提出逐步子任务训练策略，将新子任务按训练周期逐步引入，类似个体级课程学习。同时进行消融实验和有效性分析。

Result: 小型语言模型在长轨迹任务学习上有显著困难，即使针对特定角色进行训练，也难以覆盖所有子任务。提出的逐步子任务训练策略能持续提升多代理系统效能，降低子任务错误率。多代理系统在经过微调后在效率-效能权衡上优于单代理大模型。

Conclusion: 精心设计和训练的多代理SLM系统能在保持较高效率的同时显著提升效能，成为LLM系统的有力替代，尤其在资源受限环境下优势明显。推进式子任务训练对多代理系统提升表现至关重要。

Abstract: Multi-agent systems with smaller language models (SLMs) present a viable
alternative to single agent systems powered by large language models (LLMs) for
addressing complex problems. In this work, we study how these alternatives
compare in terms of both effectiveness and efficiency. To study this trade-off,
we instantiate single and multi-agent systems for the complex problems in the
AppWorld environment using different sized language models.
  We find that difficulties with long-trajectory learning in smaller language
models (SLMs) limit their performance. Even when trained for specialized roles,
SLMs fail to learn all subtasks effectively. To address this issue, we
introduce a simple progressive sub-task training strategy, which introduces new
sub-tasks progressively in each training epoch. We find that this novel
strategy, analogous to instance level curriculum learning, consistently
improves the effectiveness of multi-agents at all configurations. Our Pareto
analysis shows that fine-tuned multi-agent systems yield better
effectiveness-efficiency trade-offs. Additional ablations and analyses shows
the importance of our progressive training strategy and its ability to reduce
subtask error rates.

</details>


### [54] [Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach](https://arxiv.org/abs/2509.04510)
*Michele Materazzini,Gianluca Morciano,Jose Manuel Alcalde-Llergo,Enrique Yeguas-Bolivar,Giuseppe Calabro,Andrea Zingoni,Juri Taborri*

Main category: cs.CL

TL;DR: 本文采用VR与AI评估大学生阅读障碍，有较高的准确率，显示未来可作为辅助诊断工具，但语言差异需注意。


<details>
  <summary>Details</summary>
Motivation: 探索虚拟现实（VR）和人工智能（AI）在大学生中预测阅读障碍（dyslexia）的应用可能性，进一步研究是否可以通过VR中的沉默阅读（SR）测试和自尊测评数据来区分阅读障碍学生和正常学生。

Method: 参与者接受基于VR的阅读表现和自尊相关任务。首先对获得的数据使用t检验和Mann Whitney检验进行统计分析，比较两组（有与无阅读障碍）的测试得分。随后，利用监督式机器学习（ML）模型对数据进行训练和测试，以区分是否存在阅读障碍。

Result: 在意大利学生中的分类准确率为87.5%，西班牙学生为66.6%，整体合并为75.0%。在沉默阅读测试的完成时间方面发现有统计学差异，但准确性和自尊分数无显著差异。

Conclusion: VR和ML可以作为阅读障碍评估的辅助工具，尤其是通过捕捉完成任务所需时间的差异，但语言特异性仍会影响分类效果。

Abstract: This study explores the use of virtual reality (VR) and artificial
intelligence (AI) to predict the presence of dyslexia in Italian and Spanish
university students. In particular, the research investigates whether
VR-derived data from Silent Reading (SR) tests and self-esteem assessments can
differentiate between students that are affected by dyslexia and students that
are not, employing machine learning (ML) algorithms. Participants completed
VR-based tasks measuring reading performance and self-esteem. A preliminary
statistical analysis (t tests and Mann Whitney tests) on these data was
performed, to compare the obtained scores between individuals with and without
dyslexia, revealing significant differences in completion time for the SR test,
but not in accuracy, nor in self esteem. Then, supervised ML models were
trained and tested, demonstrating an ability to classify the presence/absence
of dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for
Spanish, and 75.0 per cent for the pooled group. These findings suggest that VR
and ML can effectively be used as supporting tools for assessing dyslexia,
particularly by capturing differences in task completion speed, but
language-specific factors may influence classification accuracy.

</details>


### [55] [Scaling behavior of large language models in emotional safety classification across sizes and tasks](https://arxiv.org/abs/2509.04512)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.CL

TL;DR: 本文开发并融合了大规模心理健康数据集，评测不同规模LLaMA模型，指出大模型在情感安全分类表现突出，而小模型经微调后能以低显存需求实现可比性能，适合敏感场景端侧部署。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在敏感领域（如心理健康）应用越来越多，理解和优化其处理情感敏感内容的能力对系统安全性和可靠性至关重要。

Method: 构建并汇总了一个包含15000余条数据的新心理健康数据集，包含人类撰写和通过ChatGPT生成的情感重述提示。通过对四款体量不同的LLaMA模型（1B、3B、8B、70B）在零样本、少样本和微调情境下，分别在情感安全三分类和多标签六大风险类任务中进行对比评测。

Result: 大模型在平均性能、尤其是多标签和零样本分类任务上表现更优。但经过轻量化微调后，1B小模型在数据量充足类别中表现可与大模型及BERT相当，同时推理时显著降低显存需求。

Conclusion: 小型本地模型经过适度微调后可以成为敏感对话场景中的安全、隐私友好的备选方案，兼具情感理解和边界维护能力。研究支持了大规模疗愈类LLM应用及安全对齐技术的推广。

Abstract: Understanding how large language models (LLMs) process emotionally sensitive
content is critical for building safe and reliable systems, particularly in
mental health contexts. We investigate the scaling behavior of LLMs on two key
tasks: trinary classification of emotional safety (safe vs. unsafe vs.
borderline) and multi-label classification using a six-category safety risk
taxonomy. To support this, we construct a novel dataset by merging several
human-authored mental health datasets (> 15K samples) and augmenting them with
emotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA
models (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings.
Our results show that larger LLMs achieve stronger average performance,
particularly in nuanced multi-label classification and in zero-shot settings.
However, lightweight fine-tuning allowed the 1B model to achieve performance
comparable to larger models and BERT in several high-data categories, while
requiring <2GB VRAM at inference. These findings suggest that smaller,
on-device models can serve as viable, privacy-preserving alternatives for
sensitive applications, offering the ability to interpret emotional context and
maintain safe conversational boundaries. This work highlights key implications
for therapeutic LLM applications and the scalable alignment of safety-critical
systems.

</details>


### [56] [Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations](https://arxiv.org/abs/2509.04515)
*Martha O. Dimgba,Sharon Oba,Ameeta Agrawal,Philippe J. Giabbanelli*

Main category: cs.CL

TL;DR: 提出BAME方法，用解释辅助提示工程降低语言模型职业故事中的性别和种族偏见，在不调参的情况下有效改善人口代表性，实现更公平透明的生成式AI。


<details>
  <summary>Details</summary>
Motivation: 语言模型被发现会在输出中传播社会偏见，尤其是在性别和族裔的表现上。作者希望检测并缓解AI生成职业故事中的性别和种族偏见。

Method: 提出了一种新的缓解策略BAME（Bias Analysis and Mitigation through Explanation），通过模型生成的解释辅助提示工程，在不改变模型参数的情况下降低偏见。对25个职业组、3种大型语言模型（Claude 3.5 Sonnet、Llama 3.1 70B Instruct、GPT-4 Turbo）以及多个人口统计维度进行故事生成与分析。

Result: 采用BAME方法后，人口代表性有2%到20%的提升。识别到了训练数据中的刻板印象造成的过度和不足体现。

Conclusion: 通过模型的内部推理机制指导，可以显著提升人口统计的公平性，有助于提升生成式AI系统的透明度。

Abstract: Language models have been shown to propagate social bias through their
output, particularly in the representation of gender and ethnicity. This paper
investigates gender and ethnicity biases in AI-generated occupational stories.
Representation biases are measured before and after applying our proposed
mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME),
revealing improvements in demographic representation ranging from 2% to 20%.
BAME leverages model-generated explanations to inform targeted prompt
engineering, effectively reducing biases without modifying model parameters. By
analyzing stories generated across 25 occupational groups, three large language
models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and
multiple demographic dimensions, we identify persistent patterns of
overrepresentation and underrepresentation linked to training data stereotypes.
Our findings demonstrate that guiding models with their own internal reasoning
mechanisms can significantly enhance demographic parity, thereby contributing
to the development of more transparent generative AI systems.

</details>


### [57] [Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets](https://arxiv.org/abs/2509.04516)
*Sophie Jaffer,Simeon Sayer*

Main category: cs.CL

TL;DR: 作者通过对比斯瓦希里语和英语BERT模型，发现母语数据训练比经翻译用英文模型处理效果更好，仅靠翻译无法消除模型的语言差异。建议重视弱势语言数据和多语种评估，防止AI加深数字不平等。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）提升多语种能力，模型在不同语言上的公平性受到关注。英文数据在训练中的主导地位可能使非英语用户处于劣势。作者希望验证数据差异是否会影响模型在不同语言上的表现。

Method: 作者构建两个单语BERT模型：一个用斯瓦希里语数据训练和测试，另一个用可比的英文新闻数据训练和测试。同时，将斯瓦希里语数据翻译成英文，并用英文模型评估，以模拟多语种LLM对非英语查询的内部翻译和抽象过程。通过比较直接在斯瓦希里语训练测试与翻译后在英文模型测试的表现，考察语言一致性与跨语言抽象的影响。

Result: 结果显示，即使翻译质量很高，斯瓦希里语原生训练模型仍显著优于将斯瓦希里语翻译为英文后在英文模型评估的方案，错误率分别为0.36%和1.47%，相差约四倍。

Conclusion: 仅靠翻译不能消除语言间的表征差异。模型在单一语言训练下能更准确地处理输入，内部知识表征存在局限，母语训练对于可靠的AI结果至关重要。未来应关注弱势语言的数据集建设和多语种模型评估，以避免AI技术加剧数字鸿沟。

Abstract: As large language models (LLMs) expand multilingual capabilities, questions
remain about the equity of their performance across languages. While many
communities stand to benefit from AI systems, the dominance of English in
training data risks disadvantaging non-English speakers. To test the hypothesis
that such data disparities may affect model performance, this study compares
two monolingual BERT models: one trained and tested entirely on Swahili data,
and another on comparable English news data. To simulate how multilingual LLMs
process non-English queries through internal translation and abstraction, we
translated the Swahili news data into English and evaluated it using the
English-trained model. This approach tests the hypothesis by evaluating whether
translating Swahili inputs for evaluation on an English model yields better or
worse performance compared to training and testing a model entirely in Swahili,
thus isolating the effect of language consistency versus cross-lingual
abstraction. The results prove that, despite high-quality translation, the
native Swahili-trained model performed better than the Swahili-to-English
translated model, producing nearly four times fewer errors: 0.36% vs. 1.47%
respectively. This gap suggests that translation alone does not bridge
representational differences between languages and that models trained in one
language may struggle to accurately interpret translated inputs due to
imperfect internal knowledge representation, suggesting that native-language
training remains important for reliable outcomes. In educational and
informational contexts, even small performance gaps may compound inequality.
Future research should focus on addressing broader dataset development for
underrepresented languages and renewed attention to multilingual model
evaluation, ensuring the reinforcing effect of global AI deployment on existing
digital divides is reduced.

</details>


### [58] [Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports](https://arxiv.org/abs/2509.04517)
*Indu Bala,Lewis Mitchell,Marianne H Gillam*

Main category: cs.CL

TL;DR: 本研究通过自然语言处理分析疝气网状植入物术后患者报告，揭示了情绪波动及关切期，有助于提升术前术后医疗服务及患者管理。


<details>
  <summary>Details</summary>
Motivation: 疝气修补手术中广泛使用网状植入物，但术后并发症以及患者的情感体验一直是重要关注点。本研究希望了解患者术后情感变化，从而改善医疗服务。

Method: 利用自然语言处理方法，对MAUDE数据库（2000-2021年）中患者自述报告进行分析。分别应用NRC情感词典与TextBlob工具，将患者描述分类为八种情感，并进行情感极性分析。同时对报告按时间序列进行分析。

Result: 研究发现2011-2012年与2017-2018年期间，患者“关切报告”数量与情感强度都有所增加。时序分析揭示了患者情感及关注点的变化。

Conclusion: 情感和情绪是术后患者体验的重要部分。通过情感分析，可帮助医疗人员更好把握患者需求，改善术前咨询、术后护理以及患者教育。情感分析有助于优化患者管理与照护策略。

Abstract: Mesh implants are widely utilized in hernia repair surgeries, but
postoperative complications present a significant concern. This study analyzes
patient reports from the Manufacturer and User Facility Device Experience
(MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of
patients following mesh implantation using Natural Language Processing (NLP).
Employing the National Research Council Canada (NRC) Emotion Lexicon and
TextBlob for sentiment analysis, the research categorizes patient narratives
into eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy,
and disgust) and assesses sentiment polarity. The goal is to discern patterns
in patient sentiment over time and to identify reports signaling urgent
concerns, referred to as "Concern Reports," thereby understanding shifts in
patient experiences in relation to changes in medical device regulation and
technological advancements in healthcare. The study detected an increase in
Concern Reports and higher emotional intensity during the periods of 2011-2012
and 2017-2018. Through temporal analysis of Concern Reports and overall
sentiment, this research provides valuable insights for healthcare
practitioners, enhancing their understanding of patient experiences
post-surgery, which is critical for improving preoperative counselling,
postoperative care, and preparing patients for mesh implant surgeries. The
study underscores the importance of emotional considerations in medical
practices and the potential for sentiment analysis to inform and enhance
patient care.

</details>


### [59] [Advancing SLM Tool-Use Capability using Reinforcement Learning](https://arxiv.org/abs/2509.04518)
*Dhruvi Paprunia,Vansh Kharidia,Pankti Doshi*

Main category: cs.CL

TL;DR: 论文提出用GRPO强化学习方法提升小型语言模型的工具使用能力，有效解决SLMs在实际中的应用瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在工具使用方面能力强，但资源和计算复杂度高，限制了其广泛应用。因此，需要更小、更高效的小型语言模型（SLMs），但SLMs在工具使用方面表现逊色。如何提升SLMs的工具使用能力是亟需解决的问题。

Method: 本研究采用强化学习方法，具体为Group Relative Policy Optimization（GRPO），对SLMs进行训练，以提升其工具使用能力。该方法相比传统的微调技术，计算资源需求更低，适应性更强。

Result: 采用GRPO方法后，SLMs在工具使用任务中的准确率显著提升，实际应用价值增强。

Conclusion: 通过强化学习方法GRPO，显著提升了SLMs在工具使用上的表现，为小型语言模型的实际应用提供了有效途径。

Abstract: Large Language Models (LLMs) have progressed beyond simple text creation, and
tool use has become increasingly important for complex, real-world tasks. Tool
use in LLMs refers to their ability to utilize external resources such as APIs,
databases, or software functions to extend their functionality beyond
generating text.Tools are used for tasks such as performing calculations,
making API calls to retrieve the current time and date, and more. This
capability enables models to fetch real-time data, execute commands, or solve
problems requiring dynamic interaction, making it indispensable for
applications like AI agents in virtual assistants, robotic control, or
automated workflows.
  However, while LLMs are usually adept tool use, their vast resource
requirements and computation complexity restrict their use in every use case.As
a result, there is an increasing need for more compact and efficient Small
Language Models (SLMs). Small language models (SLMs) struggle in tool use
compared to large language models (LLMs). As soon in Table 1. SLMs are
typically trained on smaller, more specific datasets, resulting in a narrower
knowledge base and limited contextual understanding compared to LLMs.
  This research addresses these challenges by using Reinforcement Learning
(RL), specifically Group Relative Policy Optimization (GRPO), to enhance
tool-use proficiency in SLMs. Unlike conventional fine-tuning approaches that
require heavy computation and often lack adaptability, our method provides an
efficient, effective solution that significantly boosts SLM tool-use accuracy,
increasing their practical utility.

</details>


### [60] [Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease](https://arxiv.org/abs/2509.04519)
*Zvi Badash,Hadas Ben-Atya,Naama Gavrielov,Liam Hazan,Gili Focht,Ruth Cytter-Kuint,Talar Hagopian,Dan Turner,Moti Freiman*

Main category: cs.CL

TL;DR: 论文提出HSMP-BERT模型，在希伯来语克罗恩病放射报告结构化信息抽取任务中表现优异，精准且高效；推动低资源语言环境下医学AI应用落地。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言环境下，从放射报告中提取结构化的临床信息面临较大挑战，特别是克罗恩病这类多器官发现稀缺的疾病。在以色列，对希伯来语放射文本的提取能力亟需提升，以支持更深入的疾病分析和研究。

Method: 提出并开发了一种分层结构化匹配预测BERT（HSMP-BERT）模型，采用prompt-based（提示式）方法，从希伯来语放射报告中自动抽取结构化信息。采用多标签分层切分法进行训练与测试集划分，结合多种指标（准确率、F1分数、Cohen's kappa、AUC、PPV、NPV和召回率）评估模型性能，并与零样本SMP基线和标准微调方法做对比，并测试分层推理在推理速度上的优势。

Result: HSMP-BERT在24种器官-发现组合（阳性样本数>15）上取得平均F1分数0.83±0.08和kappa 0.65±0.17，显著优于SMP零样本基线（F1 0.49±0.07，kappa 0.06±0.07）和标准微调（F1 0.30±0.27，kappa 0.27±0.34）。分层推理大幅提升推理速度（快5.1倍）。大规模应用揭示了肠壁增厚、狭窄与狭窄前扩张之间的关联及炎症发现的年龄及性别趋势。

Conclusion: HSMP-BERT为低资源语言环境下放射报告结构化信息抽取提供了可扩展的解决方案，无论在提取准确性还是效率上均有突出表现。该模型有助于实现克罗恩病的群体水平分析，是AI在低资源场景下临床文本处理的有力示范。

Abstract: Extracting structured clinical information from radiology reports is
challenging, especially in low-resource languages. This is pronounced in
Crohn's disease, with sparsely represented multi-organ findings. We developed
Hierarchical Structured Matching Prediction BERT (HSMP-BERT), a prompt-based
model for extraction from Hebrew radiology text. In an administrative database
study, we analyzed 9,683 reports from Crohn's patients imaged 2010-2023 across
Israeli providers. A subset of 512 reports was radiologist-annotated for
findings across six gastrointestinal organs and 15 pathologies, yielding 90
structured labels per subject. Multilabel-stratified split (66%
train+validation; 33% test), preserving label prevalence. Performance was
evaluated with accuracy, F1, Cohen's $\kappa$, AUC, PPV, NPV, and recall. On 24
organ-finding combinations with $>$15 positives, HSMP-BERT achieved mean F1
0.83$\pm$0.08 and $\kappa$ 0.65$\pm$0.17, outperforming the SMP zero-shot
baseline (F1 0.49$\pm$0.07, $\kappa$ 0.06$\pm$0.07) and standard fine-tuning
(F1 0.30$\pm$0.27, $\kappa$ 0.27$\pm$0.34; paired t-test $p < 10^{-7}$).
Hierarchical inference cuts runtime 5.1$\times$ vs. traditional inference.
Applied to all reports, it revealed associations among ileal wall thickening,
stenosis, and pre-stenotic dilatation, plus age- and sex-specific trends in
inflammatory findings. HSMP-BERT offers a scalable solution for structured
extraction in radiology, enabling population-level analysis of Crohn's disease
and demonstrating AI's potential in low-resource settings.

</details>


### [61] [Using LLMs to create analytical datasets: A case study of reconstructing the historical memory of Colombia](https://arxiv.org/abs/2509.04523)
*David Anderson,Galia Benitez,Margret Bjarnadottir,Shriyan Reyya*

Main category: cs.CL

TL;DR: 作者用GPT等语言模型分析哥伦比亚20万篇暴力新闻，补齐政府文献缺口，揭示了暴力与可卡因作物清除的关联，并说明了LLM为历史与政策研究带来的新机遇。


<details>
  <summary>Details</summary>
Motivation: 哥伦比亚因长期武装冲突，政府对暴力事件的系统性记录严重缺失，导致缺乏公开的冲突信息和历史记载。因此需要新的技术手段重建历史记忆，丰富公共数据资源。

Method: 利用GPT（大型语言模型），对超过20万篇西班牙语暴力相关新闻报道进行阅读和自动问答，生成结构化数据，并用该数据进行描述性分析和政策关联研究。

Result: 构建了基于200,000余篇新闻报道的大规模数据库，通过LLM分析了暴力与可卡因作物铲除之间关系，展示了新政策分析的可能性。

Conclusion: 本研究展示了大型语言模型（LLM）能够以前所未有的深度处理和分析大量冲突相关文本，推动哥伦比亚暴力事件历史记忆的建设。

Abstract: Colombia has been submerged in decades of armed conflict, yet until recently,
the systematic documentation of violence was not a priority for the Colombian
government. This has resulted in a lack of publicly available conflict
information and, consequently, a lack of historical accounts. This study
contributes to Colombia's historical memory by utilizing GPT, a large language
model (LLM), to read and answer questions about over 200,000 violence-related
newspaper articles in Spanish. We use the resulting dataset to conduct both
descriptive analysis and a study of the relationship between violence and the
eradication of coca crops, offering an example of policy analyses that such
data can support. Our study demonstrates how LLMs have opened new research
opportunities by enabling examinations of large text corpora at a previously
infeasible depth.

</details>


### [62] [Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation](https://arxiv.org/abs/2509.04534)
*Zaifu Zhan,Shuang Zhou,Min Zeng,Kai Yu,Meijia Song,Xiaoyi Chen,Jun Wang,Yu Hou,Rui Zhang*

Main category: cs.CL

TL;DR: 本研究系统评估了量化对12种大语言模型在生物医学领域多项任务上的影响，发现量化能显著降低硬件资源要求同时保持模型性能，促进了模型在隐私受限和资源有限医疗场景下的落地应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生物医学自然语言处理领域表现突出，但其计算资源需求高，不易在注重数据隐私和资源有限的医疗环境中部署。亟需一种能够在保证性能的前提下，降低硬件要求与成本的解决方案。

Method: 系统性评估12个主流（通用与生物医学专用）大型语言模型，并在4个关键任务（命名实体识别、关系抽取、多标签分类、问答）涉及的8个基准数据集上，测试量化对模型性能与资源消耗的影响。

Result: 量化方法能显著降低GPU内存需求（最高可达75%），且各类任务的模型性能基本保持不变，可以在40GB消费级GPU上部署高达70B参数的大模型。同时，领域知识与高级提示能力基本未受影响。

Conclusion: 量化是一种可行且有效的技术策略，能够促进大型高容量语言模型在注重数据隐私和资源有限的生物医学场景下的安全本地部署，可加速AI技术在临床实际中的落地与应用。

Abstract: Large language models have demonstrated remarkable capabilities in biomedical
natural language processing, yet their rapid growth in size and computational
requirements present a major barrier to adoption in healthcare settings where
data privacy precludes cloud deployment and resources are limited. In this
study, we systematically evaluated the impact of quantization on 12
state-of-the-art large language models, including both general-purpose and
biomedical-specific models, across eight benchmark datasets covering four key
tasks: named entity recognition, relation extraction, multi-label
classification, and question answering. We show that quantization substantially
reduces GPU memory requirements-by up to 75%-while preserving model performance
across diverse tasks, enabling the deployment of 70B-parameter models on 40GB
consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness
to advanced prompting methods are largely maintained. These findings provide
significant practical and guiding value, highlighting quantization as a
practical and effective strategy for enabling the secure, local deployment of
large yet high-capacity language models in biomedical contexts, bridging the
gap between technical advances in AI and real-world clinical translation.

</details>


### [63] [Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions](https://arxiv.org/abs/2509.04549)
*Faruk Alpay,Taylan Alpay*

Main category: cs.CL

TL;DR: 该文提出了从提示、激活到权重干预的统一框架，实现了对大型语言模型的高效控制，取得了高成功率，为安全、可控的语言生成打下基础，并讨论了相应的风险与挑战。


<details>
  <summary>Details</summary>
Motivation: 变换器（Transformer）语言模型已在自然语言处理（NLP）领域表现卓越，但如何对其实现细粒度的可控生成一直是重大挑战。本文旨在解决模型控制难题，提升模型的实用性、安全性和可靠性。

Method: 本文系统性地研究了从提示（prompt）、激活（activation）、权重（weight）三个层面对变换器模型的干预方法。提出了一个统一的干预框架，将提示工程、参数高效微调、模型权重编辑和强化学习等方法结合用于可控文本生成。理论分析了权重微调对模型行为的最小化变更，实验上验证了不同干预方式的有效性。

Result: 实验在情感控制和事实性编辑任务上均取得了90%以上的成功率，同时模型基础性能得以保留。分析了通用性与特异性之间的权衡，以及对抗攻击和对齐等安全性问题。

Conclusion: 本文提出的统一干预框架能有效实现对大型语言模型的多粒度可控生成，为未来安全、可控及健壮的语言模型设计奠定基础。同时，作者强调了潜在的道德风险及需加强评估。

Abstract: Transformer-based language models excel in NLP tasks, but fine-grained
control remains challenging. This paper explores methods for manipulating
transformer models through principled interventions at three levels: prompts,
activations, and weights. We formalize controllable text generation as an
optimization problem addressable via prompt engineering, parameter-efficient
fine-tuning, model editing, and reinforcement learning. We introduce a unified
framework encompassing prompt-level steering, activation interventions, and
weight-space edits. We analyze robustness and safety implications, including
adversarial attacks and alignment mitigations. Theoretically, we show minimal
weight updates can achieve targeted behavior changes with limited side-effects.
Empirically, we demonstrate >90% success in sentiment control and factual edits
while preserving base performance, though generalization-specificity trade-offs
exist. We discuss ethical dual-use risks and the need for rigorous evaluation.
This work lays groundwork for designing controllable and robust language
models.

</details>


### [64] [Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects](https://arxiv.org/abs/2509.04605)
*Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 过去对文本讽刺识别研究较多，本文综述了语音基础的讽刺识别领域，包括方法演变及当前不足，指出未来应加强多模态和跨文化研究。


<details>
  <summary>Details</summary>
Motivation: 讽刺作为人类交流中的常见特征，对人际及人机交互构成挑战。尽管文本讽刺识别已有相关研究，但语音数据在识别讽刺方面的研究相对较少，而语音韵律特征（如音高、语速、语调）被语言学认为对讽刺表达尤为重要。近年来，语音技术进步推动了自动语音讽刺识别的需求，这对于神经退行性疾病患者的社交及提升机器对复杂语言的理解均具重要意义。本文的动机在于系统地综述语音讽刺识别领域，弥补该领域的研究缺口。

Method: 综述方法，系统性回顾和分析了以语音为基础的讽刺识别研究，包括数据集、特征提取、分类方法，特别关注从单模态到多模态的技术演化过程。

Result: 揭示了当前语音讽刺识别数据集存在的局限性；特征提取方法从传统声学特征逐渐过渡到深度学习为基础的表示；分类方法从单模态发展到多模态融合技术。提出未来需重视多语种、跨文化情境下的语音讽刺识别，并将讽刺视为多模态现象，而非仅限于文本层面的任务。

Conclusion: 本文首次对基于语音的讽刺识别进行了系统性综述，总结了技术演变与现有局限，并强调未来研究应关注多模态、跨文化和多语种等方向，以增强社会交互和机器理解能力。

Abstract: Sarcasm, a common feature of human communication, poses challenges in
interpersonal interactions and human-machine interactions. Linguistic research
has highlighted the importance of prosodic cues, such as variations in pitch,
speaking rate, and intonation, in conveying sarcastic intent. Although previous
work has focused on text-based sarcasm detection, the role of speech data in
recognizing sarcasm has been underexplored. Recent advancements in speech
technology emphasize the growing importance of leveraging speech data for
automatic sarcasm recognition, which can enhance social interactions for
individuals with neurodegenerative conditions and improve machine understanding
of complex human language use, leading to more nuanced interactions. This
systematic review is the first to focus on speech-based sarcasm recognition,
charting the evolution from unimodal to multimodal approaches. It covers
datasets, feature extraction, and classification methods, and aims to bridge
gaps across diverse research domains. The findings include limitations in
datasets for sarcasm recognition in speech, the evolution of feature extraction
techniques from traditional acoustic features to deep learning-based
representations, and the progression of classification methods from unimodal
approaches to multimodal fusion techniques. In so doing, we identify the need
for greater emphasis on cross-cultural and multilingual sarcasm recognition, as
well as the importance of addressing sarcasm as a multimodal phenomenon, rather
than a text-based challenge.

</details>


### [65] [Sample-efficient Integration of New Modalities into Large Language Models](https://arxiv.org/abs/2509.04606)
*Osman Batur İnce,André F. T. Martins,Oisin Mac Aodha,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: 本文提出SEMI方法，通过超网络和投影器机制，实现了新模态在大语言模型中的高效少样本集成，能大幅减少对配对数据的需求。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型需要能处理不断扩展的大量模态，但从零训练一模型支持全部模态不切实际。现有方法集成新模态到已有模型常需大量配对数据，而低资源模态往往数据难以获取。

Method: 提出了SEMI方法，通过超网络动态生成适配器，将模态专属编码器的输出通过共享投影器接入LLM。超网络在高资源模态（如文本、语音、音频、视频）上训练，推理时仅需极少新模态样本即可适配。同时，用等距变换人工扩充训练模态多样性。

Result: SEMI方法极大提升了样本效率，能在少量样本条件下实现新模态快速集成（如卫星图像、天文图像、惯性测量、分子等），且对编码器维数无要求。例如，达成32-shot SEMI的准确率时，从零训练需多64倍数据。

Conclusion: SEMI显著降低新模态集成对数据量的需求，极有潜力拓展大模型的模态覆盖范围。

Abstract: Multimodal foundation models can process several modalities. However, since
the space of possible modalities is large and evolving over time, training a
model from scratch to encompass all modalities is unfeasible. Moreover,
integrating a modality into a pre-existing foundation model currently requires
a significant amount of paired data, which is often not available for
low-resource modalities. In this paper, we introduce a method for
sample-efficient modality integration (SEMI) into Large Language Models (LLMs).
To this end, we devise a hypernetwork that can adapt a shared projector --
placed between modality-specific encoders and an LLM -- to any modality. The
hypernetwork, trained on high-resource modalities (i.e., text, speech, audio,
video), is conditioned on a few samples from any arbitrary modality at
inference time to generate a suitable adapter. To increase the diversity of
training modalities, we artificially multiply the number of encoders through
isometric transformations. We find that SEMI achieves a significant boost in
sample efficiency during few-shot integration of new modalities (i.e.,
satellite images, astronomical images, inertial measurements, and molecules)
with encoders of arbitrary embedding dimensionality. For instance, to reach the
same accuracy as 32-shot SEMI, training the projector from scratch needs
64$\times$ more data. As a result, SEMI holds promise to extend the modality
coverage of foundation models.

</details>


### [66] [Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs](https://arxiv.org/abs/2509.04615)
*Brennen Hill,Surendra Parla,Venkata Abhijeeth Balabhadruni,Atharv Prajod Padmalayam,Sujay Chandra Shekara Sharma*

Main category: cs.CL

TL;DR: 本文系统综述了针对大型语言模型的基于提示的攻击方式，并对其分门别类，明确威胁模型，为未来安全防护研究和安全模型开发提供了参考。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)的广泛应用带来了严重的安全挑战，恶意行为者可利用输入提示进行攻击，规避安全机制并造成实际危害，因此亟需系统性研究和理解这些攻击手法，以便开发有效的防御措施。

Method: 本文采用文献综述方法，对现有的基于提示(prompt-based)攻击手法进行全面调查，并对其进行分类，建立清晰的威胁模型。

Result: 对攻击机制和影响进行了详细阐述，对各种提示攻击手法进行了系统分类和分析，为安全防护和模型设计提供理论依据。

Conclusion: 系统梳理了基于提示的攻击类型，有助于后续构建更安全、可抵御非法蒸馏、微调和编辑的大型语言模型。

Abstract: The proliferation of Large Language Models (LLMs) has introduced critical
security challenges, where adversarial actors can manipulate input prompts to
cause significant harm and circumvent safety alignments. These prompt-based
attacks exploit vulnerabilities in a model's design, training, and contextual
understanding, leading to intellectual property theft, misinformation
generation, and erosion of user trust. A systematic understanding of these
attack vectors is the foundational step toward developing robust
countermeasures. This paper presents a comprehensive literature survey of
prompt-based attack methodologies, categorizing them to provide a clear threat
model. By detailing the mechanisms and impacts of these exploits, this survey
aims to inform the research community's efforts in building the next generation
of secure LLMs that are inherently resistant to unauthorized distillation,
fine-tuning, and editing.

</details>


### [67] [Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety](https://arxiv.org/abs/2509.04650)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CL

TL;DR: 与传统机器学习模型相比，Transformer类模型（如BERT）在灾难推文分类中表现更好，准确率更高且能理解更复杂语言。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在判别灾难相关推文时常常无法理解推文的上下文或深层含义，特别是在推文语言非正式、含有隐喻或存在歧义的情况下。作者认为在这一领域，基于Transformer的模型可能会表现得更好。

Method: 本研究评估了多种基于Transformer的模型（包括BERT、DistilBERT、RoBERTa和DeBERTa）在灾难相关推文分类任务上的表现，并与传统机器学习方法（如逻辑回归、朴素贝叶斯等）进行了对比分析。

Result: 实验结果显示，BERT模型取得了91%的最高分类准确率，显著优于逻辑回归和朴素贝叶斯（均为82%）。基于Transformer的模型由于其上下文嵌入和注意力机制，能更好地理解推文中微妙的语言。

Conclusion: 基于Transformer的架构在社会安全类应用中远优于传统模型，不仅分类准确率更高，还能更深入理解语言语境，并具备更好的实际文本泛化能力。

Abstract: Twitter and other social media platforms have become vital sources of real
time information during disasters and public safety emergencies. Automatically
classifying disaster related tweets can help emergency services respond faster
and more effectively. Traditional Machine Learning (ML) models such as Logistic
Regression, Naive Bayes, and Support Vector Machines have been widely used for
this task, but they often fail to understand the context or deeper meaning of
words, especially when the language is informal, metaphorical, or ambiguous. We
posit that, in this context, transformer based models can perform better than
traditional ML models. In this paper, we evaluate the effectiveness of
transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for
classifying disaster related tweets. These models are compared with traditional
ML approaches to highlight the performance gap. Experimental results show that
BERT achieved the highest accuracy (91%), significantly outperforming
traditional models like Logistic Regression and Naive Bayes (both at 82%). The
use of contextual embeddings and attention mechanisms allows transformer models
to better understand subtle language in tweets, where traditional ML models
fall short. This research demonstrates that transformer architectures are far
more suitable for public safety applications, offering improved accuracy,
deeper language understanding, and better generalization across real world
social media text.

</details>


### [68] [Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs](https://arxiv.org/abs/2509.04655)
*Ayush Gupta,Ramneet Kaur,Anirban Roy,Adam D. Cobb,Rama Chellappa,Susmit Jha*

Main category: cs.CL

TL;DR: 作者提出了一种利用模型dropout容忍度的新型OOD检测方法，并在医学专用LLM任务上取得显著提升（AUROC增加2%-37%），优化了模型面对未知输入时的安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 专用大语言模型（LLMs）通过微调可在领域内任务达成先进表现，但在面对领域外（OOD）输入时易产生错误或不可靠结果，尤其在关键应用场景下存在风险。因此，亟需一种有效的推断时OOD检测算法，提高专用LLMs在实际应用中的安全性和可靠性。

Method: 提出一种推断时OOD检测算法，基于ICAD（Inductive Conformal Anomaly Detection）框架，并创新性地利用模型的dropout容忍度作为新的非一致性度量。方法假设领域内输入比领域外输入具有更高的dropout容忍度，通过多层聚合dropout容忍度并采用有效的集成方法，提升检测性能，同时保持ICAD理论的误报界限。

Result: 实验证明，针对医学专用LLM，本方法对OOD输入的检测优于基线方法。在以OOD数据为正例、领域内测试数据为负例的设定下，AUROC提升了2%到37%。

Conclusion: 所提出的方法在提升专用LLM的OOD检测性能方面有效，能更准确地区分领域外输入，对关键应用的安全性具有重要价值。

Abstract: We propose a novel inference-time out-of-domain (OOD) detection algorithm for
specialized large language models (LLMs). Despite achieving state-of-the-art
performance on in-domain tasks through fine-tuning, specialized LLMs remain
vulnerable to incorrect or unreliable outputs when presented with OOD inputs,
posing risks in critical applications. Our method leverages the Inductive
Conformal Anomaly Detection (ICAD) framework, using a new non-conformity
measure based on the model's dropout tolerance. Motivated by recent findings on
polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs
exhibit higher dropout tolerance than OOD inputs. We aggregate dropout
tolerance across multiple layers via a valid ensemble approach, improving
detection while maintaining theoretical false alarm bounds from ICAD.
Experiments with medical-specialized LLMs show that our approach detects OOD
inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$
when treating OOD datapoints as positives and in-domain test datapoints as
negatives.

</details>


### [69] [AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs](https://arxiv.org/abs/2509.04656)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 本研究首次系统评估了阿拉伯语及多语种LLM在阿拉伯语自然语言生成任务中的幻觉问题，提出了细粒度评测框架。结果显示阿拉伯语专用预训练模型表现最好，整体上事实幻觉比忠实性误差更常见。


<details>
  <summary>Details</summary>
Motivation: 目前大多数关于大语言模型（LLMs）幻觉现象的研究集中在英文语境，而阿拉伯语相关研究严重不足。由于阿拉伯语在全球广泛使用，并且在全球交流和媒体中十分重要，亟需对阿拉伯语及其在多语种LLM中的幻觉现象进行系统评估。

Method: 本文针对阿拉伯语大模型的自然语言生成两大任务——生成式问答和摘要，提出了细粒度的LLM幻觉评估框架。研究涵盖了12个模型（4个阿拉伯语预训练、4个多语种、4个推理型模型），并设定了12项细粒度幻觉指标来评估模型的事实一致性和忠实性。

Result: 实验结果表明，所有模型和任务中，事实性幻觉要比忠实性误差更常见。值得注意的是，阿拉伯语预训练模型Allam在幻觉率方面明显低于多语种模型，并且与推理型模型表现相当。

Conclusion: 本文首次系统地对阿拉伯语及多语种LLM的幻觉现象进行了细致评估，发现阿拉伯语预训练模型在减少幻觉方面更具优势，强调了针对特定语言进行预训练的重要性。

Abstract: Recently, extensive research on the hallucination of the large language
models (LLMs) has mainly focused on the English language. Despite the growing
number of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination
in the Arabic context remains relatively underexplored. The knowledge gap is
particularly pressing given Arabic's widespread use across many regions and its
importance in global communication and media. This paper presents the first
comprehensive hallucination evaluation of Arabic and multilingual LLMs on two
critical Arabic natural language generation tasks: generative question
answering (GQA) and summarization. This study evaluates a total of 12 LLMs,
including 4 Arabic pre-trained models, 4 multilingual models, and 4
reasoning-based models. To assess the factual consistency and faithfulness of
LLMs' outputs, we developed a fine-grained hallucination evaluation framework
consisting of 12 fine-grained hallucination indicators that represent the
varying characteristics of each task. The results reveal that factual
hallucinations are more prevalent than faithfulness errors across all models
and tasks. Notably, the Arabic pre-trained model Allam consistently
demonstrates lower hallucination rates than multilingual models and a
comparative performance with reasoning-based models. The code is available at:
\href{https://github.com/aishaalansari57/AraHalluEval}{Github link}.

</details>


### [70] [Evaluating NL2SQL via SQL2NL](https://arxiv.org/abs/2509.04657)
*Mohammadtaher Safarzadeh,Afshin Oroojlooyjadid,Dan Roth*

Main category: cs.CL

TL;DR: 提出基于SQL2NL的schema对齐释义机制，揭示现有NL2SQL模型对自然语言多样性的鲁棒性远不如预期，呼吁关注泛化能力的专项评价。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL模型评测数据集很少有系统地、可控地考查模型对自然语言多样性的泛化能力。因此需要一种评估框架，单独考查模型对等价表达方式（非模式扰动、非歧义）的鲁棒表现。

Method: 提出了一种新的基于SQL2NL的schema对齐释义框架，能够自动生成在语法上多样但与原意和数据库schema保持一致的查询。通过这种方法实现对NL2SQL模型在仅含语言多样性扰动情况下的鲁棒性单独评估。

Result: 通过使用该释义框架，对多个主流大模型进行测试发现，在处理释义查询时它们的执行准确率大幅下降。例如LLaMa3.3-70B在Spider数据集释义查询时准确率从77.11%降至66.9%，LLaMa3.1-8B下降近20个百分点。小模型受影响更显著，且鲁棒性随查询复杂度、数据集和领域显著变化。

Conclusion: 主流NL2SQL模型对语言变化的鲁棒性远低于标准基准测试所反映的水平，特别是在面对语义等价但措辞不同的查询时性能显著下降。不同模型和不同任务复杂度下的鲁棒性下降幅度差异明显，这表明现有NL2SQL系统在真实应用场景下的泛化能力仍有明显提升空间。

Abstract: Robust evaluation in the presence of linguistic variation is key to
understanding the generalization capabilities of Natural Language to SQL
(NL2SQL) models, yet existing benchmarks rarely address this factor in a
systematic or controlled manner. We propose a novel schema-aligned paraphrasing
framework that leverages SQL-to-NL (SQL2NL) to automatically generate
semantically equivalent, lexically diverse queries while maintaining alignment
with the original schema and intent. This enables the first targeted evaluation
of NL2SQL robustness to linguistic variation in isolation-distinct from prior
work that primarily investigates ambiguity or schema perturbations. Our
analysis reveals that state-of-the-art models are far more brittle than
standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop
in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,
while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to
42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We
also find that robustness degradation varies significantly with query
complexity, dataset, and domain -- highlighting the need for evaluation
frameworks that explicitly measure linguistic generalization to ensure reliable
performance in real-world settings.

</details>


### [71] [Why Language Models Hallucinate](https://arxiv.org/abs/2509.04664)
*Adam Tauman Kalai,Ofir Nachum,Santosh S. Vempala,Edwin Zhang*

Main category: cs.CL

TL;DR: 本文分析了大语言模型产生幻觉的根源，发现是训练和评测方式促进了猜测行为。作者建议从改变基准测试的评分方式入手，提高模型应对不确定性的能力和答案可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的大语言模型能力强大，但它们依然会产生看似可信但实为错误的内容，这影响了用户对AI的信任。作者希望揭示幻觉现象出现的机制及出路。

Method: 对大语言模型产生幻觉现象的统计根源进行了分析，并对当前训练及评估流程的激励机制进行了讨论，提出用调整基准评测标准替代增加新测评项的方法。

Result: 发现幻觉本质上是二分类问题中的错误判断，源自训练与评测体系不鼓励模型表达不确定性。如果错误难以和事实区分开，模型自然会生成幻觉。提出通过变革评分标准引导模型输出更可信的内容。

Conclusion: 文章认为大语言模型产生幻觉的根本原因在于当前训练和评估方法倾向于奖励猜测而不是诚实地表达不确定性，因此幻觉现象难以避免。只有通过修改现有基准测评的评分机制才能实现本质改善。

Abstract: Like students facing hard exam questions, large language models sometimes
guess when uncertain, producing plausible yet incorrect statements instead of
admitting uncertainty. Such "hallucinations" persist even in state-of-the-art
systems and undermine trust. We argue that language models hallucinate because
the training and evaluation procedures reward guessing over acknowledging
uncertainty, and we analyze the statistical causes of hallucinations in the
modern training pipeline. Hallucinations need not be mysterious -- they
originate simply as errors in binary classification. If incorrect statements
cannot be distinguished from facts, then hallucinations in pretrained language
models will arise through natural statistical pressures. We then argue that
hallucinations persist due to the way most evaluations are graded -- language
models are optimized to be good test-takers, and guessing when uncertain
improves test performance. This "epidemic" of penalizing uncertain responses
can only be addressed through a socio-technical mitigation: modifying the
scoring of existing benchmarks that are misaligned but dominate leaderboards,
rather than introducing additional hallucination evaluations. This change may
steer the field toward more trustworthy AI systems.

</details>


### [72] [ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs](https://arxiv.org/abs/2509.04696)
*Samira Khorshidi,Azadeh Nikfarjam,Suprita Shankar,Yisi Sang,Yash Govind,Hyun Jang,Ali Kasgari,Alexis McClimans,Mohamed Soliman,Vishnu Konda,Ahmed Fakhry,Xiaoguang Qi*

Main category: cs.CL

TL;DR: ODKE+系统结合大语言模型与本体化流程，自动、高效、高精度地从网络抽取和更新海量开放域知识，对知识图谱维护具有突破性意义，实际线上验证表现优秀。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KG）广泛应用于人工智能领域，但其时效性和完整性的维护成本极高。现有系统在问答、推荐等领域表现突出，但信息过时、缺漏、以及知识抽取的精准性问题严重制约了实际生产落地。因此，迫切需要一种自动化、可扩展、精度高且与本体约束紧密结合的知识抽取与更新新方案。

Method: 提出了ODKE+系统，它将多个模块化组件整合为可扩展的自动化流水线，包括：（1）抽取启动模块检测缺失或过时事实，（2）证据检索器收集支撑文档，（3）知识抽取器结合基于规则和本体驱动的大语言模型（LLM）提示抽取事实，（4）一个轻量级的事实校验器利用第二个LLM验证事实，（5）证据整合器对候选事实进行排序和标准化。系统动态生成本体片段，实现195类谓词的类型一致性知识抽取。支持批处理与流式处理。

Result: ODKE+系统可在批量或流式模式下运行，处理超过900万维基百科页面，自动采集并纳入1900万条高置信知识事实，精准率达98.8%。与传统方法相比，提升知识覆盖，能与第三方KG重叠高达48%，平均减少知识图谱更新滞后50天。

Conclusion: 本工作表明，结合本体结构和多阶段验证流程的大语言模型知识抽取方法，能够以高置信度、生产级规模，自动完成大规模开放域事实采集和知识图谱更新，具有广泛现实应用价值。

Abstract: Knowledge graphs (KGs) are foundational to many AI applications, but
maintaining their freshness and completeness remains costly. We present ODKE+,
a production-grade system that automatically extracts and ingests millions of
open-domain facts from web sources with high precision. ODKE+ combines modular
components into a scalable pipeline: (1) the Extraction Initiator detects
missing or stale facts, (2) the Evidence Retriever collects supporting
documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and
ontology-guided prompting for large language models (LLMs), (4) a lightweight
Grounder validates extracted facts using a second LLM, and (5) the Corroborator
ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates
ontology snippets tailored to each entity type to align extractions with schema
constraints, enabling scalable, type-consistent fact extraction across 195
predicates. The system supports batch and streaming modes, processing over 9
million Wikipedia pages and ingesting 19 million high-confidence facts with
98.8% precision. ODKE+ significantly improves coverage over traditional
methods, achieving up to 48% overlap with third-party KGs and reducing update
lag by 50 days on average. Our deployment demonstrates that LLM-based
extraction, grounded in ontological structure and verification workflows, can
deliver trustworthiness, production-scale knowledge ingestion with broad
real-world applicability. A recording of the system demonstration is included
with the submission and is also available at https://youtu.be/UcnE3_GsTWs.

</details>


### [73] [OleSpeech-IV: A Large-Scale Multispeaker and Multilingual Conversational Speech Dataset with Diverse Topics](https://arxiv.org/abs/2509.04702)
*Wei Chu,Yuanzhe Dong,Ke Tan,Dong Han,Xavier Menendez-Pidal,Ruchao Fan,Chenfeng Miao,Chanwoo Kim,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: 本文介绍了OleSpeech-IV大型多说话人多语言对话语音数据集及其子集的开放，为语音领域研究提供了丰富数据资源。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模、多说话人、多语言的对话语音数据集，限制了语音识别、语者识别等领域的研究与应用。该数据集旨在解决这一问题，提供丰富的语音资源，推动相关技术进步。

Method: 数据集音频内容来自公开可用的英文播客、脱口秀、电话会议及其他对话场景。说话人名称、轮次和转录均由人工提供并经过专有流程精炼，时间戳及置信度等数据则由该流程自动生成。

Result: 构建了包含多说话人、多语言、多主题对话的OleSpeech-IV大规模语音数据集。同时，公开了OleSpeech-IV-2025-EN-AR-100子集供非商业研究使用。

Conclusion: OleSpeech-IV数据集为多说话人、多语言的对话语音任务提供了高质量资源，将促进语音相关研究，特别是在语音识别和说话人识别领域。

Abstract: OleSpeech-IV dataset is a large-scale multispeaker and multilingual
conversational speech dataset with diverse topics. The audio content comes from
publicly-available English podcasts, talk shows, teleconferences, and other
conversations. Speaker names, turns, and transcripts are human-sourced and
refined by a proprietary pipeline, while additional information such as
timestamps and confidence scores is derived from the pipeline. The IV denotes
its position as Tier IV in the Olewave dataset series. In addition, we have
open-sourced a subset, OleSpeech-IV-2025-EN-AR-100, for non-commercial research
use.

</details>


### [74] [KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering](https://arxiv.org/abs/2509.04716)
*Yushi Sun,Kai Sun,Yifan Ethan Xu,Xiao Yang,Xin Luna Dong,Nan Tang,Lei Chen*

Main category: cs.CL

TL;DR: KERAG提出了创新的知识图谱增强生成框架，通过更宽泛的信息检索与LLM推理，显著提升了问答覆盖率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法因过度依赖严格的语义解析和知识检索，导致问答系统覆盖率低且易受语义歧义影响。

Method: 通过'检索-过滤-摘要'流程，结合微调LLM进行链式推理，对知识子图进行处理，从而扩展问题所需信息范围并去噪。

Result: KERAG在问答质量上比最优方法高约7%，比GPT-4o高10-21%。

Conclusion: KERAG提出了一种基于知识图谱的RAG新流程，大幅提升了问答覆盖率与准确性，在实验中超越了现有主流方法和GPT-4o。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by incorporating external data, with Knowledge Graphs
(KGs) offering crucial information for question answering. Traditional
Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing,
which typically retrieves knowledge strictly necessary for answer generation,
thus often suffer from low coverage due to rigid schema requirements and
semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that
enhances QA coverage by retrieving a broader subgraph likely to contain
relevant information. Our retrieval-filtering-summarization approach, combined
with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs,
reduces noises and improves QA for both simple and complex questions.
Experiments demonstrate that KERAG surpasses state-of-the-art solutions by
about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.

</details>


### [75] [Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization](https://arxiv.org/abs/2509.04745)
*Lee Kezar,Zed Sehyr,Jesse Thomason*

Main category: cs.CL

TL;DR: 通过向量量化自编码器并引入音位学归纳偏置，提升了手语模型对未见词的一次性重构和识别能力，为手语表示学习与泛化提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 手语数据集词汇量有限，现有模型对未见词泛化能力弱，因此需要提升模型对未见手语的识别和重构能力。

Method: 提出两种音位学归纳偏置方法：参数解耦（架构偏置）与音位学半监督（正则化技术），并利用向量量化自编码器对孤立手语进行建模。

Result: 所提出模型在对未见手语的一次性重构与手语识别上优于对照基线，获得更具辨识力和泛化能力的表征。

Conclusion: 显性、语言学驱动的偏置有助于提升手语表征在泛化和识别方面的能力，对手语机器学习模型的设计具有实用价值。

Abstract: Sign language datasets are often not representative in terms of vocabulary,
underscoring the need for models that generalize to unseen signs. Vector
quantization is a promising approach for learning discrete, token-like
representations, but it has not been evaluated whether the learned units
capture spurious correlations that hinder out-of-vocabulary performance. This
work investigates two phonological inductive biases: Parameter Disentanglement,
an architectural bias, and Phonological Semi-Supervision, a regularization
technique, to improve isolated sign recognition of known signs and
reconstruction quality of unseen signs with a vector-quantized autoencoder. The
primary finding is that the learned representations from the proposed model are
more effective for one-shot reconstruction of unseen signs and more
discriminative for sign identification compared to a controlled baseline. This
work provides a quantitative analysis of how explicit, linguistically-motivated
biases can improve the generalization of learned representations of sign
language.

</details>


### [76] [A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning](https://arxiv.org/abs/2509.04753)
*Cheng Peng,Xinyu Dong,Mengxian Lyu,Daniel Paredes,Yaoyun Zhang,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文系统评估了LLMs及其微调方法（包括PEFT和多任务指令微调）在临床患者信息提取中的效果，发现合适组合可提升模型在低资源场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多临床NLP任务中取得突破，但其在患者信息提取中的最佳应用策略尚不明确。研究动机是探究不同模型、微调策略与多任务指令微调对提升信息提取准确性与泛化能力的作用。

Method: 对比评测了多种LLM架构（仅编码器、仅解码器），采用传统全量微调与高效参数微调，并基于多任务指令微调框架，在五套数据集（其中四套用于多任务微调评估）上进行零样本和小样本学习能力测试（交叉数据集评估）。

Result: 不同结构与微调法在患者信息提取任务上表现差异明显，多任务指令微调能在部分情况下提升零样本、小样本任务表现，高效参数微调在资源受限条件下展现高效性。

Conclusion: 大型语言模型（LLMs）在临床患者信息提取任务中表现优异，结合多任务指令微调和高效参数微调（PEFT）可提升模型泛化与学习能力，但最佳实践需深入探索。

Abstract: Natural language processing (NLP) is a key technology to extract important
patient information from clinical narratives to support healthcare
applications. The rapid development of large language models (LLMs) has
revolutionized many NLP tasks in the clinical domain, yet their optimal use in
patient information extraction tasks requires further exploration. This study
examines LLMs' effectiveness in patient information extraction, focusing on LLM
architectures, fine-tuning strategies, and multi-task instruction tuning
techniques for developing robust and generalizable patient information
extraction systems. This study aims to explore key concepts of using LLMs for
clinical concept and relation extraction tasks, including: (1) encoder-only or
decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)
algorithms, and (3) multi-task instruction tuning on few-shot learning
performance. We benchmarked a suite of LLMs, including encoder-based LLMs
(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,
GatorTronLlama), across five datasets. We compared traditional full-size
fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning
framework that combines both tasks across four datasets to evaluate the
zero-shot and few-shot learning performance using the leave-one-dataset-out
strategy.

</details>


### [77] [Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework](https://arxiv.org/abs/2509.04770)
*Zucheng Liang,Wenxin Wei,Kaijie Zhang,Hongyi Chen*

Main category: cs.CL

TL;DR: 本文提出用多跳问题分解法提升LLMs复杂问答能力。实验表明，多跳分解在未训练和LoRA微调后均优于直接回答复杂问题，验证了其提升模型推理与理解能力的有效性。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）面对复杂问题时，准确回答的能力一直存在挑战。为了提升LLM处理复杂问题的推理和理解能力，本文提出了一种基于多跳问题分解的方法，并希望在该领域提供有效解法。

Method: 基于MQUAKE框架，本文利用LLAMA3模型，系统性地研究了知识图谱中多跳问题分解方法对模型理解和推理准确率的影响。具体步骤为：将MQUAKE-T数据集加工为单跳和多跳两种格式，并分别对这两种数据集微调LLAMA3模型，最后进行推理测试。同时也考察了在无训练和LoRA微调两种情况下的效果。

Result: 实验结果显示，无论是否进行LoRA微调，多跳问题分解方法在模型性能上均显著优于直接回答复杂问题的单跳方式，并且经过微调后两者都有提升，但多跳分解方法始终优胜。

Conclusion: 多跳问题分解方法能有效提升LLM对复杂问题的回答能力，这一优势在训练前后均得以验证。实验为复杂问答的处理和LLM推理能力的提升提供了有效的新途径。

Abstract: Accurately answering complex questions has consistently been a significant
challenge for Large Language Models (LLMs). To address this, this paper
proposes a multi-hop question decomposition method for complex questions,
building upon research within the MQUAKE framework. Utilizing the LLAMA3 model,
we systematically investigate the impact of multi-hop question decomposition
within knowledge graphs on model comprehension and reasoning accuracy, both
before and after model training. In our experiments, we systematically
partitioned and converted the MQUAKE-T dataset into two distinct formats: a
single-hop dataset designed for directly answering complex questions, and a
multi-hop dataset constructed using the multi-hop question decomposition
method. We then fine-tuned the LLAMA3 model on these datasets and conducted
inference tests. Our results demonstrate that, without fine-tuning the LLM, the
prediction performance based on the multi-hop question decomposition method
significantly outperforms the method of directly answering complex questions.
After fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance
of both approaches improved compared to the untrained baseline. Crucially, the
method utilizing multi-hop decomposition consistently maintained its
superiority. These findings validate the effectiveness of the multi-hop
decomposition method both before and after training, demonstrating its
capability to effectively enhance the LLM's ability to answer complex
questions.

</details>


### [78] [Decoders Laugh as Loud as Encoders](https://arxiv.org/abs/2509.04779)
*Eli Borodach,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 本研究比较了GPT-4o和RoBERTa在幽默识别任务上的表现，发现新一代解码器模型对幽默理解的能力已接近当前最优编码器模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种NLP任务上表现出色，但其对幽默等细腻主题的理解能力尚未明确。因此，研究模型是否真正“理解”幽默，以及其与编码器模型的差距成为本研究的动力。

Method: 对GPT-4o等解码器模型进行微调，并针对幽默识别任务进行评估，与同样微调的编码器模型（如RoBERTa）进行F1分数比较。

Result: 微调后的GPT-4o在幽默识别任务上取得了0.85的平均F1-macro分数，与微调后的RoBERTa（0.86）表现相当，显示出先进解码器模型在高级语义任务上的竞争力。

Conclusion: 论文表明，经过微调的大型语言模型（如GPT-4o）在幽默理解任务上表现接近甚至媲美最佳的编码器模型（如RoBERTa），说明解码器模型对幽默的理解能力有显著提升。

Abstract: From the dawn of the computer, Allen Turing dreamed of a robot that could
communicate using language as a human being. The recent advances in the field
of Large Language Models (LLMs) shocked the scientific community when a single
model can apply for various natural language processing (NLP) tasks, while the
output results are sometimes even better than most human communication skills.
Models such as GPT, Claude, Grok, etc. have left their mark on the scientific
community. However, it is unclear how much these models understand what they
produce, especially in a nuanced theme such as humor. The question of whether
computers understand humor is still open (among the decoders, the latest to be
checked was GPT-2). We addressed this issue in this paper; we have showed that
a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well
as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)

</details>


### [79] [Enhancing Diversity in Large Language Models via Determinantal Point Processes](https://arxiv.org/abs/2509.04784)
*Yilei Chen,Souradip Chakraborty,Lorenz Wolf,Ioannis Ch. Paschalidis,Aldo Pacchiano*

Main category: cs.CL

TL;DR: 本文提出DQO训练方法，在保证模型质量的前提下，有效提升了大语言模型在多种任务上的语义多样性，优于仅推理阶段或只关注词汇多样性的现有方法。


<details>
  <summary>Details</summary>
Motivation: 监督微调和强化学习虽然可以提升下游任务表现，但往往损害了模型输出多样性，导致响应千篇一律；而现有多样性提升方法局限明显（仅推理时起效或只关注词汇层面），亟需在训练阶段提升语义多样性的方法。

Method: 提出一种基于DPP（determinantal point processes）和核相似性矩阵行列式的新训练方法（DQO），通过采样和嵌入每个prompt的多组响应，用行列式度量语义多样性，并实现多样性与质量的联合优化。

Result: 实验证明，DQO方法在指令跟随、摘要、故事生成和推理等任务上，显著提升了输出的语义多样性，同时保持了模型的总体质量。

Conclusion: 该论文提出的方法能在不损失模型质量的前提下，大幅提升大语言模型输出的语义多样性。

Abstract: Supervised fine-tuning and reinforcement learning are two popular methods for
post-training large language models (LLMs). While improving the model's
performance on downstream tasks, they often reduce the model's output
diversity, leading to narrow, canonical responses. Existing methods to enhance
diversity are limited, either by operating at inference time or by focusing on
lexical differences. We propose a novel training method named DQO based on
determinantal point processes (DPPs) to jointly optimize LLMs for quality and
semantic diversity. Our approach samples and embeds a group of responses for
each prompt, then uses the determinant of a kernel-based similarity matrix to
measure diversity as the volume spanned by the embeddings of these responses.
Experiments across instruction-following, summarization, story generation, and
reasoning tasks demonstrate that our method substantially improves semantic
diversity without sacrificing model quality.

</details>


### [80] [Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects](https://arxiv.org/abs/2509.04794)
*Gunmay Handa,Zekun Wu,Adriano Koshiyama,Philip Treleaven*

Main category: cs.CL

TL;DR: 作者系统对比了三种大语言模型人格控制方法，提出了完善的评测框架，发现各方法在一致性和能力之间存在显著权衡，机械性操控为部署和解释性提供了轻量选择。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在客服和智能体场景下越来越多地应用人格操控，但其具体机制和权衡尚不清晰。本文旨在系统研究大语言模型中的人格操控方法及其优缺点。

Method: 作者基于Big Five模型，比较了三种人格控制方法：上下文学习（ICL）、参数高效微调（PEFT）、机械性操纵（MS）。他们构建了高/低人格特质平衡的数据集，用于矢量计算和方法评测，并提出统一的评估框架，开发了特征纯化技术，还提出多级稳定性分析。

Result: 实验表明：ICL方法在能力损失极小的情况下实现了良好的人格一致性；PEFT尽管获得了最高的一致性，但牺牲了任务表现；MS则以较轻的计算量实现了有竞争力的有效性。特质分析发现，‘开放性’最难控制，‘宜人性’对ICL最具抗性，人格编码多集中在中间层。

Conclusion: 本文通过系统对比三种人格控制方法，为大语言模型的人格操控和部署权衡提供了指导，并提出机械性操纵作为一种高效易解释的轻量级替代方案，有助于后续在实际应用中选择最优方法。

Abstract: Personality manipulation in large language models (LLMs) is increasingly
applied in customer service and agentic scenarios, yet its mechanisms and
trade-offs remain unclear. We present a systematic study of personality control
using the Big Five traits, comparing in-context learning (ICL),
parameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our
contributions are fourfold. First, we construct a contrastive dataset with
balanced high/low trait responses, enabling effective steering vector
computation and fair cross-method evaluation. Second, we introduce a unified
evaluation framework based on within-run $\Delta$ analysis that disentangles,
reasoning capability, agent performance, and demographic bias across MMLU,
GAIA, and BBQ benchmarks. Third, we develop trait purification techniques to
separate openness from conscientiousness, addressing representational overlap
in trait encoding. Fourth, we propose a three-level stability framework that
quantifies method-, trait-, and combination-level robustness, offering
practical guidance under deployment constraints. Experiments on Gemma-2-2B-IT
and LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment
with minimal capability loss, PEFT delivers the highest alignment at the cost
of degraded task performance, and MS provides lightweight runtime control with
competitive effectiveness. Trait-level analysis shows openness as uniquely
challenging, agreeableness as most resistant to ICL, and personality encoding
consolidating around intermediate layers. Taken together, these results
establish personality manipulation as a multi-level probe into behavioral
representation, linking surface conditioning, parameter encoding, and
activation-level steering, and positioning mechanistic steering as a
lightweight alternative to fine-tuning for both deployment and
interpretability.

</details>


### [81] [Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training](https://arxiv.org/abs/2509.04796)
*Figarri Keisha,Zekun Wu,Ze Wang,Adriano Koshiyama,Philip Treleaven*

Main category: cs.CL

TL;DR: 反复用模型自身生成的数据训练大模型会导致事实能力退化（知识崩溃），但通过领域特定的数据合成和评估方法，能有效缓解并监测这种退化，为模型可靠性提供可行策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越依赖于合成数据，而模型自身生成数据再用于训练会导致模型能力退化（模型崩溃），这威胁到模型的事实可靠性。本文希望界定和系统性分析这一现象，并提出可行缓解方法。

Method: 采用受控递归训练实验，观察和分析模型在反复使用其自身生成的数据训练过程中的表现与退化路径。以领域特定的合成训练作为缓解措施，并通过多种评估框架监控模型能力退化阶段。

Result: 实验显示，递归训练导致的模型崩溃（knowledge collapse）具有三阶段特征：事实准确率下降但表面流畅性尚存，出现高自信错误输出。模型崩溃轨迹和时机高度依赖于指令格式，且领域特定合成数据能明显提升模型抗崩溃能力。提出了结合模型指标和任务指标的新评估框架。

Conclusion: 本文揭示并细致描述了大语言模型在递归合成训练中的知识崩溃问题，提出领域特定数据为代表的有效缓解方案，并建立了系统评估框架，对于需高准确率的AI应用的可持续训练提供了理论与实践参考。

Abstract: Large language models increasingly rely on synthetic data due to
human-written content scarcity, yet recursive training on model-generated
outputs leads to model collapse, a degenerative process threatening factual
reliability. We define knowledge collapse as a distinct three-stage phenomenon
where factual accuracy deteriorates while surface fluency persists, creating
"confidently wrong" outputs that pose critical risks in accuracy-dependent
domains. Through controlled experiments with recursive synthetic training, we
demonstrate that collapse trajectory and timing depend critically on
instruction format, distinguishing instruction-following collapse from
traditional model collapse through its conditional, prompt-dependent nature. We
propose domain-specific synthetic training as a targeted mitigation strategy
that achieves substantial improvements in collapse resistance while maintaining
computational efficiency. Our evaluation framework combines model-centric
indicators with task-centric metrics to detect distinct degradation phases,
enabling reproducible assessment of epistemic deterioration across different
language models. These findings provide both theoretical insights into collapse
dynamics and practical guidance for sustainable AI training in
knowledge-intensive applications where accuracy is paramount.

</details>


### [82] [Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs](https://arxiv.org/abs/2509.04802)
*Ilham Wicaksono,Zekun Wu,Theo King,Adriano Koshiyama,Philip Treleaven*

Main category: cs.CL

TL;DR: 智能体化大语言模型存在传统评估难以发现的新型安全漏洞，AgentSeer框架首次揭示了agent层特有高风险行为，强调建立情境化安全评估标准的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型向智能体系统（agentic systems）转变，现有的安全评估方法无法全面识别和评估实际部署中的新型风险。

Method: 提出了AgentSeer，一个基于可观测性、可将智能体执行流程分解为细致组件与动作图谱的评估框架。通过在两个模型（GPT-OSS-20B和Gemini-2.0-flash）上用HarmBench进行单轮及迭代攻击，实施对比分析。

Result: 模型层面对比显示，两个模型对社会工程攻击易受影响，对逻辑攻击有抵抗能力，而在智能体级别评估下，出现了只在agent环境下存在的“agentic-only”漏洞（工具调用场景下的ASR提升24-60%）。还发现了跨模型的通用agentic模式与上下文相关攻击效果，并揭示模型间的漏洞转移和迭代攻击对安全影响显著。

Conclusion: 发现了传统安全评估的系统性漏洞，并论证了针对智能体情境的安全评估范式的紧迫性，AgentSeer作为方法标准和实证工具成为必需。

Abstract: As large language models transition to agentic systems, current safety
evaluation frameworks face critical gaps in assessing deployment-specific
risks. We introduce AgentSeer, an observability-based evaluation framework that
decomposes agentic executions into granular action and component graphs,
enabling systematic agentic-situational assessment. Through cross-model
validation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and
iterative refinement attacks, we demonstrate fundamental differences between
model-level and agentic-level vulnerability profiles. Model-level evaluation
reveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash
(50.00% ASR), with both models showing susceptibility to social engineering
while maintaining logic-based attack resistance. However, agentic-level
assessment exposes agent-specific risks invisible to traditional evaluation. We
discover "agentic-only" vulnerabilities that emerge exclusively in agentic
contexts, with tool-calling showing 24-60% higher ASR across both models.
Cross-model analysis reveals universal agentic patterns, agent transfer
operations as highest-risk tools, semantic rather than syntactic vulnerability
mechanisms, and context-dependent attack effectiveness, alongside
model-specific security profiles in absolute ASR levels and optimal injection
strategies. Direct attack transfer from model-level to agentic contexts shows
degraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash:
28%), while context-aware iterative attacks successfully compromise objectives
that failed at model-level, confirming systematic evaluation gaps. These
findings establish the urgent need for agentic-situation evaluation paradigms,
with AgentSeer providing the standardized methodology and empirical validation.

</details>


### [83] [Analyzing Finnish Inflectional Classes through Discriminative Lexicon and Deep Learning Models](https://arxiv.org/abs/2509.04813)
*Alexandre Nikolaev,Yu-Ying Chuang,R. Harald Baayen*

Main category: cs.CL

TL;DR: 本文用神经模型发现，在大量芬兰语词形变换任务中，无需预设词形变化类别，模型也能高效完成任务，类别生产力和词频对结果有不同影响。


<details>
  <summary>Details</summary>
Motivation: 探究词形变化类别（inflectional classes）是否对母语者学习名词词形变化的认知必要性，以及在无需预设词形类别的情况下，模型能否准确处理芬兰语名词的词形变化。

Method: 使用Discriminative Lexicon Model（DLM）对芬兰语2000个高频名词的55,271个屈折形式进行建模，覆盖49个屈折类别。采用不同设置，包括不考虑频率（模拟无限输入的学习）和考虑词频的用法导向学习。对模型在训练集和未见测试集上的表现进行评估，分析不同类别和词频对模型性能的影响。

Result: 无论是否预设屈折类别，DLM模型在训练集上准确率极高，测试集表现下降但仍可接受。对于高生产力类别（包含更多类型、低频词和独特词），模型表现更好，而对低生产力类别、新形式的处理则较差。用法导向模型主要受词频影响，词类生产力的效应较弱。

Conclusion: DLM模型在不依赖屈折类别的情况下也能有效进行芬兰语名词词形变化的生成和理解，尤其对高生产力类别表现突出。但在用法导向条件下，词频是模型表现的主导预测因子，而类别生产力影响甚微。

Abstract: Descriptions of complex nominal or verbal systems make use of inflectional
classes. Inflectional classes bring together nouns which have similar stem
changes and use similar exponents in their paradigms. Although inflectional
classes can be very useful for language teaching as well as for setting up
finite state morphological systems, it is unclear whether inflectional classes
are cognitively real, in the sense that native speakers would need to discover
these classes in order to learn how to properly inflect the nouns of their
language. This study investigates whether the Discriminative Lexicon Model
(DLM) can understand and produce Finnish inflected nouns without setting up
inflectional classes, using a dataset with 55,271 inflected nouns of 2000
high-frequency Finnish nouns from 49 inflectional classes. Several DLM
comprehension and production models were set up. Some models were not informed
about frequency of use, and provide insight into learnability with infinite
exposure (endstate learning). Other models were set up from a usage based
perspective, and were trained with token frequencies being taken into
consideration (frequency-informed learning). On training data, models performed
with very high accuracies. For held-out test data, accuracies decreased, as
expected, but remained acceptable. Across most models, performance increased
for inflectional classes with more types, more lower-frequency words, and more
hapax legomena, mirroring the productivity of the inflectional classes. The
model struggles more with novel forms of unproductive and less productive
classes, and performs far better for unseen forms belonging to productive
classes. However, for usage-based production models, frequency was the dominant
predictor of model performance, and correlations with measures of productivity
were tenuous or absent.

</details>


### [84] [AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding](https://arxiv.org/abs/2509.04821)
*Yan Xie,Yibo Cui,Liang Xie,Erwei Yin*

Main category: cs.CL

TL;DR: 本文提出一种自适应特征蒸馏方法，将强大的教师表示迁移到小模型，显著提升SLU效果，同时降低计算和数据要求。


<details>
  <summary>Details</summary>
Motivation: 语音理解（SLU）是对话系统的核心，但由于标注数据稀缺和大模型部署成本高，限制了SLU的发展。

Method: 提出一种自适应特征蒸馏（AFD）框架，将基于通用文本嵌入（GTE）的教师模型的语义表示迁移到轻量级学生模型。引入动态适配器和残差投影神经网络（RPNN）以对齐不同特征空间，以及动态蒸馏系数（DDC）根据模型预测结果实时调整蒸馏强度。

Result: 在中文ProSLU基准数据集上，AFD-SLU取得了95.67%的意图识别准确率、92.02%的槽位F1分数和85.50%的总准确率，达到当前最佳水平。

Conclusion: 该方法有效缓解了SLU训练数据不足和推理计算负担重的问题，实现高性能、低成本的语音理解。

Abstract: Spoken Language Understanding (SLU) is a core component of conversational
systems, enabling machines to interpret user utterances. Despite its
importance, developing effective SLU systems remains challenging due to the
scarcity of labeled training data and the computational burden of deploying
Large Language Models (LLMs) in real-world applications. To further alleviate
these issues, we propose an Adaptive Feature Distillation framework that
transfers rich semantic representations from a General Text Embeddings
(GTE)-based teacher model to a lightweight student model. Our method introduces
a dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to
align heterogeneous feature spaces, and a Dynamic Distillation Coefficient
(DDC) that adaptively modulates the distillation strength based on real-time
feedback from intent and slot prediction performance. Experiments on the
Chinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves
state-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,
and 85.50% overall accuracy.

</details>


### [85] [Memorization $\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?](https://arxiv.org/abs/2509.04866)
*Boxiang Ma,Ru Li,Yuanlong Wang,Hongye Tan,Xiaoli Li*

Main category: cs.CL

TL;DR: 本文通过构建新数据集和双视角评估框架，发现当前大模型更多靠记忆而非深层语义理解，这暴露了其在场景认知和泛化能力上的局限。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在各类自然语言处理任务中表现出色，但人们仍质疑其卓越能力是否来源于对训练数据的简单记忆，而非真正的语义理解。本文旨在探究LLM的泛化能力背后的原因，厘清其对场景语义的认知水平。

Method: 作者提出了一个双视角评估框架，专门用于考察LLM对场景认知能力（即关联语义场景元素和上下文语境中论元的能力）。具体做法：1）构建了一个包含虚构事实、不同文本描述且带场景元素注释的新数据集；2）分别从模型输出（回答与场景相关的问题）和内部表征（探查模型内是否编码了场景元素与论元之间的关联）两个视角，对现有LLM进行了系统评估。

Result: 实验表明，当前大型语言模型主要依赖于表层记忆，对场景语义理解的能力较弱，即使在简单场景下也难以展现稳健的语义认知能力。

Conclusion: 本文揭示了现有LLM在语义场景认知方面存在的显著短板，为未来提升其认知能力和泛化性提供了新的思路。

Abstract: Driven by vast and diverse textual data, large language models (LLMs) have
demonstrated impressive performance across numerous natural language processing
(NLP) tasks. Yet, a critical question persists: does their generalization arise
from mere memorization of training data or from deep semantic understanding? To
investigate this, we propose a bi-perspective evaluation framework to assess
LLMs' scenario cognition - the ability to link semantic scenario elements with
their arguments in context. Specifically, we introduce a novel scenario-based
dataset comprising diverse textual descriptions of fictional facts, annotated
with scenario elements. LLMs are evaluated through their capacity to answer
scenario-related questions (model output perspective) and via probing their
internal representations for encoded scenario elements-argument associations
(internal representation perspective). Our experiments reveal that current LLMs
predominantly rely on superficial memorization, failing to achieve robust
semantic scenario cognition, even in simple cases. These findings expose
critical limitations in LLMs' semantic understanding and offer cognitive
insights for advancing their capabilities.

</details>


### [86] [Using LLMs for Multilingual Clinical Entity Linking to ICD-10](https://arxiv.org/abs/2509.04868)
*Sylvia Vassileva,Ivan Koychev,Svetla Boytcheva*

Main category: cs.CL

TL;DR: 该论文提出一种结合词典匹配和GPT-4.1预测的多阶段流水线，实现了不同语言下医学术语到ICD-10编码的自动链接，在西班牙语和希腊语数据集均表现出较高准确率，有望提升医院编码效率和标准化程度。


<details>
  <summary>Details</summary>
Motivation: 临床实体链接是从临床文本中抽取结构化信息的重要步骤，将文本短语与医学本体或分类系统中的代码进行对应，推动临床信息电子化、标准化。自动将ICD-10代码分配给出院小结中的医学术语，能够减少医护人员工作量、提高编码一致性。

Method: 提出了一种多阶段流水线方法，针对不同语言的临床术语与ICD-10代码进行链接。该方法首先利用临床词典匹配文本中的明确术语，当无法明确匹配时，利用GPT-4.1进行in-context learning预测ICD-10代码。

Result: 该系统在不同语言的数据集上取得了优异成果：西班牙语CodiEsp数据集分类F1分数达到0.89、子分类达到0.78；希腊语ElCardioCC数据集的F1分数达到0.85。

Conclusion: 多语言临床术语与ICD-10代码自动链接可通过结合临床词典和大型语言模型实现，有效提升预测准确率，具备在医疗编码实际应用中的可行性和泛化性。

Abstract: The linking of clinical entities is a crucial part of extracting structured
information from clinical texts. It is the process of assigning a code from a
medical ontology or classification to a phrase in the text. The International
Classification of Diseases - 10th revision (ICD-10) is an international
standard for classifying diseases for statistical and insurance purposes.
Automatically assigning the correct ICD-10 code to terms in discharge summaries
will simplify the work of healthcare professionals and ensure consistent coding
in hospitals. Our paper proposes an approach for linking clinical terms to
ICD-10 codes in different languages using Large Language Models (LLMs). The
approach consists of a multistage pipeline that uses clinical dictionaries to
match unambiguous terms in the text and then applies in-context learning with
GPT-4.1 to predict the ICD-10 code for the terms that do not match the
dictionary. Our system shows promising results in predicting ICD-10 codes on
different benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on
subcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.

</details>


### [87] [L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning](https://arxiv.org/abs/2509.04884)
*Raul Singh,Nicolo Brunello,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: L1RA通过L1正则化动态分配低秩适配器资源，有效提升LLM微调的效率和性能，节省计算资源，并帮助分析模型关键适应部位，适用于资源有限的应用场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决复杂任务方面表现出强大的能力，但其微调过程对计算资源消耗巨大，尤其在资源有限的情况下带来了挑战。如何降低微调成本，提高适应性和效率，是亟需解决的问题。

Method: 本文提出了一种新方法L1RA，在使用LoRA进行微调时，动态分配低秩适配器的秩（rank）。该方法通过施加L1正则化，有效修剪冗余的秩，并在适配器之间重新分配资源，以优化计算开销。

Result: 实验结果显示，L1RA在计算负载与传统LoRA甚至其他变种相比持平或更低的同时，实现了相同或更优的性能。训练后秩分布分析还揭示了模型在适应下游任务时，特定组件（如前馈层与注意力输出层）对适应性需求最高。

Conclusion: L1RA不仅提升了LLM微调过程的效率，还为模型优化与定制化提供了有价值的诊断信息。该方法特别适用于计算资源受限的场景，在性能与可解释性方面展现了良好的前景。

Abstract: The ability of Large Language Models (LLMs) to solve complex tasks has made
them crucial in the development of AI-based applications. However, the high
computational requirements to fine-tune these LLMs on downstream tasks pose
significant challenges, particularly when resources are limited. In response to
this challenge, we introduce L1RA, a novel technique aimed at dynamically
distributing the rank of low-rank adapters during fine-tuning using LoRA. Given
a rank budget (i.e., total sum of adapters rank), L1RA leverages L1
regularisation to prune redundant ranks and redistribute them across adapters,
thereby optimising resource utilisation. Through a series of comprehensive
experiments, we empirically demonstrate that L1RA maintains comparable or even
reduced computational overhead compared to other LoRA variants, including the
vanilla approach, while achieving same or better performances. Moreover, the
post-training analysis of rank distribution unveiled insights into the specific
model components requiring the most adaptation to align with the task
objective: the feed-forward layers and the attention output projection. These
results highlight the efficacy of L1RA in not only enhancing the efficiency of
LLM fine-tuning, but also in providing valuable diagnostic information for
model refinement and customisation. In conclusion, L1RA stands as a promising
technique for advancing the performance and interpretability of LLM adaptation,
particularly in scenarios where computational resources are constrained.

</details>


### [88] [PLaMo 2 Technical Report](https://arxiv.org/abs/2509.04897)
*Preferred Networks,:,Kaizaburo Chubachi,Yasuhiro Fujita,Shinichi Hemmi,Yuta Hirokawa,Toshiki Kataoka,Goro Kobayashi,Kenichi Maehashi,Calvin Metzger,Hiroaki Mikami,Shogo Murai,Daisuke Nishino,Kento Nozawa,Shintarou Okada,Daisuke Okanohara,Shunta Saito,Shotaro Sano,Shuji Suzuki,Daisuke Tanaka,Avinash Ummadisingu,Hanqin Wang,Sixue Wang,Tianqi Xu*

Main category: cs.CL

TL;DR: PLaMo 2利用混合架构和高效剪枝，构建支持长上下文的小参数日语大模型。在多项测试中超过同规模开源模型，实现强性能与高推理效率，为日语领域大模型应用提供新选择。


<details>
  <summary>Details</summary>
Motivation: 旨在解决日语领域大模型的训练数据稀缺和推理效率问题，同时提升小参数模型的性能。

Method: 采用基于Samba的混合架构，逐步过渡到全注意力机制，并通过连续预训练支持32K上下文长度。利用大规模合成语料解决数据稀缺，并通过权重复用与结构化剪枝提升计算效率。后续通过有监督微调（SFT）和直接偏好优化（DPO），结合合成日语指令数据和模型融合技术进行优化。最后在推理阶段使用vLLM和量化技术以最小准确率损失提升推理效率。

Result: 剪枝后的8B模型可达到此前100B模型的性能。PLaMo 2在日语基准测试中取得了业界领先效果，在指令理解、语言流畅度及日语知识方面优于同等规模的开源模型。

Conclusion: PLaMo 2通过创新架构和高效剪枝，在小规模参数下实现了强大的日语处理能力，显著提升了日语大模型的性能和推理效率。

Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large
language models featuring a hybrid Samba-based architecture that transitions to
full attention via continual pre-training to support 32K token contexts.
Training leverages extensive synthetic corpora to overcome data scarcity, while
computational efficiency is achieved through weight reuse and structured
pruning. This efficient pruning methodology produces an 8B model that achieves
performance comparable to our previous 100B model. Post-training further
refines the models using a pipeline of supervised fine-tuning (SFT) and direct
preference optimization (DPO), enhanced by synthetic Japanese instruction data
and model merging techniques. Optimized for inference using vLLM and
quantization with minimal accuracy loss, the PLaMo 2 models achieve
state-of-the-art results on Japanese benchmarks, outperforming similarly-sized
open models in instruction-following, language fluency, and Japanese-specific
knowledge.

</details>


### [89] [ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning](https://arxiv.org/abs/2509.04903)
*Jianghao Chen,Wei Sun,Qixiang Yin,Lingxing Kong,Zhixing Tan,Jiajun Zhang*

Main category: cs.CL

TL;DR: 该论文提出的ACE-RL方法，通过细粒度约束自适应分解和强化学习，有效提升了大模型高质量长文本生成能力，在公开和专有系统上均取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理长文本生成时，虽然理解能力进步显著，但生成高质量长文本仍有两个主要难点：一是高质量长文本数据稀缺，制约了监督微调和强化学习的表现；二是现有优化往往只关注相关性、连贯性等粗粒度质量，而忽略具体多样的场景下的细粒度需求。

Method: 提出了ACE-RL框架，通过自动分解指令，提取出自适应的细粒度约束标准，并据此设计奖励机制。该机制根据模型生成的回复对这些细粒度约束的满足度进行评分，将主观的质量评价转化为可验证的约束检查，最终利用强化学习优化模型的长文本生成能力。

Result: ACE-RL在WritingBench数据集上比传统的SFT和RL方法分别提升了20.70%和7.32%；最优模型甚至比GPT-4o等专有系统高出7.10%。

Conclusion: ACE-RL为提升大语言模型长文本生成质量，尤其是在多样细粒度场景下，提供了更优的训练范式与显著的性能提升。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
long-context understanding, yet they face significant challenges in
high-quality long-form generation. Existing studies primarily suffer from two
limitations: (1) A heavy reliance on scarce, high-quality long-form response
data for supervised fine-tuning (SFT) or for pairwise preference reward in
reinforcement learning (RL). (2) Focus on coarse-grained quality optimization
dimensions, such as relevance, coherence, and helpfulness, overlooking the
fine-grained specifics inherent to diverse long-form generation scenarios. To
address this issue, we propose a framework using Adaptive Constraint-Enhanced
reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first
automatically deconstructs each instruction into a set of fine-grained,
adaptive constraint criteria by identifying its underlying intents and demands.
Subsequently, we design a reward mechanism that quantifies the quality of
long-form responses based on their satisfaction over corresponding constraints,
converting subjective quality evaluation into constraint verification. Finally,
we utilize reinforcement learning to guide models toward superior long-form
generation capabilities. Experimental results demonstrate that our ACE-RL
framework significantly outperforms existing SFT and RL baselines by 20.70% and
7.32% on WritingBench, and our top-performing model even surpasses proprietary
systems like GPT-4o by 7.10%, providing a more effective training paradigm for
LLMs to generate high-quality content across diverse long-form generation
scenarios.

</details>


### [90] [Classification of kinetic-related injury in hospital triage data using NLP](https://arxiv.org/abs/2509.04969)
*Midhun Shyam,Jim Basilakis,Kieran Luken,Steven Thomas,John Crozier,Paul M. Middleton,X. Rosalind Wang*

Main category: cs.CL

TL;DR: 本文提出并验证了一套基于有限算力的小规模定制化LLM分诊数据分类流程，在保护数据隐私和算力受限的医疗环境下，取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 急诊分诊记录中包含大量富有价值的信息，可辅助医疗人员和研究者理解病患流行病学及疾病进展；然而，这类数据隐私性高、硬件受限且标注过程耗时昂贵，阻碍了现代自然语言处理（NLP）与机器学习方法的应用。

Method: 提出了一种管道流程：首先用2000条开源数据集在GPU上对预训练大模型（LLM）进行分类器微调；随后用1000条针对具体医院的数据在CPU上继续微调模型。该流程注重数据集策划和现有模型的再利用。

Result: 证明了通过精心挑选数据集并利用已存在的开源模型和数据，即使硬件资源有限，依然能实现分诊记录的有效分类。

Conclusion: 有限计算资源条件下，依托精挑细选的小规模数据和现有大模型，可以成功实现医院分诊数据的高效分类，为医疗数据隐私保护、资源受限场景下的大模型应用提供范例。

Abstract: Triage notes, created at the start of a patient's hospital visit, contain a
wealth of information that can help medical staff and researchers understand
Emergency Department patient epidemiology and the degree of time-dependent
illness or injury. Unfortunately, applying modern Natural Language Processing
and Machine Learning techniques to analyse triage data faces some challenges:
Firstly, hospital data contains highly sensitive information that is subject to
privacy regulation thus need to be analysed on site; Secondly, most hospitals
and medical facilities lack the necessary hardware to fine-tune a Large
Language Model (LLM), much less training one from scratch; Lastly, to identify
the records of interest, expert inputs are needed to manually label the
datasets, which can be time-consuming and costly. We present in this paper a
pipeline that enables the classification of triage data using LLM and limited
compute resources. We first fine-tuned a pre-trained LLM with a classifier
using a small (2k) open sourced dataset on a GPU; and then further fine-tuned
the model with a hospital specific dataset of 1000 samples on a CPU. We
demonstrated that by carefully curating the datasets and leveraging existing
models and open sourced data, we can successfully classify triage data with
limited compute resources.

</details>


### [91] [Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts](https://arxiv.org/abs/2509.04982)
*Julius Neumann,Robert Lange,Yuni Susanti,Michael Färber*

Main category: cs.CL

TL;DR: 小型BERT模型做短文本情感分类时，数据增强有用，持续预训练和复杂分类头作用不大。


<details>
  <summary>Details</summary>
Motivation: 短文本的情感分类任务存在诸多挑战，如类别不平衡、训练样本有限以及情感标签主观性强，且这些问题因短文本上下文有限而加剧。这些因素导致歧义难以消除、数据稀疏，进而影响模型的有效学习。

Method: 论文评估了小型基于Transformer的模型（BERT和RoBERTa，参数量少于10亿）在短文本多标签情感分类中的效果。主要考察了三个影响模型表现的因素：1）领域持续预训练，2）自动生成样本的数据增强，3）分类头结构的变化。

Result: 实验表明，数据增强可以提升分类性能，而在增强数据集上持续预训练反而会引入噪音，无法提高准确率。对分类头结构的修改仅带来有限的性能提升。

Conclusion: 为在资源有限环境下优化BERT类模型和改进短文本情感分类策略提供了实用建议。研究证实数据增强是有效手段，而过度预训练和复杂分类头的收益有限。

Abstract: Sentiment classification in short text datasets faces significant challenges
such as class imbalance, limited training samples, and the inherent
subjectivity of sentiment labels -- issues that are further intensified by the
limited context in short texts. These factors make it difficult to resolve
ambiguity and exacerbate data sparsity, hindering effective learning. In this
paper, we evaluate the effectiveness of small Transformer-based models (i.e.,
BERT and RoBERTa, with fewer than 1 billion parameters) for multi-label
sentiment classification, with a particular focus on short-text settings.
Specifically, we evaluated three key factors influencing model performance: (1)
continued domain-specific pre-training, (2) data augmentation using
automatically generated examples, specifically generative data augmentation,
and (3) architectural variations of the classification head. Our experiment
results show that data augmentation improves classification performance, while
continued pre-training on augmented datasets can introduce noise rather than
boost accuracy. Furthermore, we confirm that modifications to the
classification head yield only marginal benefits. These findings provide
practical guidance for optimizing BERT-based models in resource-constrained
settings and refining strategies for sentiment classification in short-text
datasets.

</details>


### [92] [Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant](https://arxiv.org/abs/2509.05006)
*Inbal Bolshinsky,Shani Kupiec,Almog Sasson,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文系统比较了服务对话中显式意图识别与直接回复生成两种范式，通过公开数据集和多模型实验，发现意图建模并非生成高质服务回复的必要前置，挑战了行业传统流程。


<details>
  <summary>Details</summary>
Motivation: 对话AI回复生成准确性和上下文契合度一直是核心挑战。业界普遍认为明确识别用户意图是高质量服务回复生成的前提，但这一假设缺乏系统论证。文章旨在验证显式意图识别对于回复生成是否为必要步骤。

Method: 对两种回复生成范式（意图优先与直接回复）进行了系统比较，利用两组公开服务对话数据集，评估了多种最新语言模型（包括微调版T5）。评测指标涵盖语言质量与任务成功率。

Result: 实验显示，部分最新语言模型能够在无需显式意图识别的场景下生成高质量回复，与传统方法表现相当甚至更优。

Conclusion: 研究发现，显式意图识别在对话AI回复生成中的必要性并不如传统假设那样强，挑战了业内固有的设计观念。根据评估结果，直接回复生成在部分场景下可达到同等或更好的效果。

Abstract: In the era of conversational AI, generating accurate and contextually
appropriate service responses remains a critical challenge. A central question
remains: Is explicit intent recognition a prerequisite for generating
high-quality service responses, or can models bypass this step and produce
effective replies directly? This paper conducts a rigorous comparative study to
address this fundamental design dilemma. Leveraging two publicly available
service interaction datasets, we benchmark several state-of-the-art language
models, including a fine-tuned T5 variant, across both paradigms: Intent-First
Response Generation and Direct Response Generation. Evaluation metrics
encompass both linguistic quality and task success rates, revealing surprising
insights into the necessity or redundancy of explicit intent modelling. Our
findings challenge conventional assumptions in conversational AI pipelines,
offering actionable guidelines for designing more efficient and effective
response generation systems.

</details>


### [93] [Masked Diffusion Language Models with Frequency-Informed Training](https://arxiv.org/abs/2509.05056)
*Despoina Kosmopoulou,Efthymios Georgiou,Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.CL

TL;DR: 论文提出了一种结合扩散模型和频率掩码的语言建模方法，在有限数据下取得了与主流方法相近的成绩，展现了新的建模思路。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的情况下，提升语言模型的数据效率，并寻找比传统自回归或掩码方法更优的建模策略。

Method: 采用基于扩散的损失目标，将扩散模型思想引入语言建模；结合频率感知掩码策略（优先学习稀有词）与不同的噪声调度和权重策略，在NELBO目标下实验多种设置。

Result: 在BabyLM基准测试中，模型在语言能力、世界知识及拟人性方面表现出与混合自回归-掩码基线相当的性能。

Conclusion: 扩散模型为基础的训练在数据受限的语言建模任务中表现良好，有望作为自回归和掩码混合模型外的另一可行选择。

Abstract: We present a masked diffusion language modeling framework for data-efficient
training for the BabyLM 2025 Challenge. Our approach applies diffusion training
objectives to language modeling under strict data constraints, incorporating
frequency-informed masking that prioritizes learning from rare tokens while
maintaining theoretical validity. We explore multiple noise scheduling
strategies, including two-mode approaches, and investigate different noise
weighting schemes within the NELBO objective. We evaluate our method on the
BabyLM benchmark suite, measuring linguistic competence, world knowledge, and
human-likeness. Results show performance competitive to hybrid
autoregressive-masked baselines, demonstrating that diffusion-based training
offers a viable alternative for data-restricted language learning.

</details>


### [94] [Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations](https://arxiv.org/abs/2509.05060)
*Patrick Amadeus Irawan,Ryandito Diandaru,Belati Jagad Bintang Syuhada,Randy Zakya Suchrady,Alham Fikri Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya*

Main category: cs.CL

TL;DR: 本文提出Entropy2Vec，利用单语语言模型熵生成可刻画语言结构差异的跨语种向量，实现了更好的语言类型学对齐和多语种NLP任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的语言类型学资源存在特征稀疏、静态的缺陷，难以全面和动态地捕捉语言之间的关系。现有方法对缺失值和时间适应性不足，影响了多语种NLP任务的效果。

Method: 作者提出Entropy2Vec框架，通过单语语言模型的熵来建模不同语言间的结构相似性。训练单语模型后，通过预测熵的高低反映语言间的相似与差异，并以此生成稠密的、动态可适应的跨语种语言向量表示。该方法无需依赖稀疏的类型学特征库，也无缺失值问题。

Result: 实验证明Entropy2Vec在主流类型学分类任务中表现良好，并能在多语种NLP下游任务（如LinguAlchemy框架）中取得有竞争力的效果。

Conclusion: Entropy2Vec通过利用语言模型预测的不确定性（熵），为构建跨语言表征提供了一种新方法，能够更好地捕捉语言结构差异且有效提升多语言NLP应用的性能。

Abstract: We introduce Entropy2Vec, a novel framework for deriving cross-lingual
language representations by leveraging the entropy of monolingual language
models. Unlike traditional typological inventories that suffer from feature
sparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in
language models to capture typological relationships between languages. By
training a language model on a single language, we hypothesize that the entropy
of its predictions reflects its structural similarity to other languages: Low
entropy indicates high similarity, while high entropy suggests greater
divergence. This approach yields dense, non-sparse language embeddings that are
adaptable to different timeframes and free from missing values. Empirical
evaluations demonstrate that Entropy2Vec embeddings align with established
typological categories and achieved competitive performance in downstream
multilingual NLP tasks, such as those addressed by the LinguAlchemy framework.

</details>


### [95] [ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions](https://arxiv.org/abs/2509.05066)
*Matteo Bortoletto,Constantin Ruhdorfer,Andreas Bulling*

Main category: cs.CL

TL;DR: 本文提出了关注复杂社交互动和空间动态的多模态心智理论测试基准ToM-SSI。结果显示，当前模型难以应对这些新场景，突显了未来研究的新挑战。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论（ToM）基准过于依赖简单的Sally-Anne测验，无法全面反映人类复杂的社交认知，缺乏多主体、真实互动和空间行为的测试场景。作者希望通过更丰富的环境和任务，更准确评估和推动模型的社交理解能力。

Method: 提出了ToM-SSI多模态基准，设计了包含多主体、多类型互动（合作与阻碍混合）的新社交环境任务，通过这些任务评估模型在不同社交互动中的ToM（心智理论）推理能力。

Result: ToM-SSI首次引入多主体、复杂社交互动和空间动态的多模态测试，发现现有模型在推理多主体心理状态、处理复杂社交互动时表现不佳，性能有待显著提升。

Conclusion: 当前大型模型在ToM-SSI基准上的表现依然有限，尤其在新设计的各种社交场景任务中，暴露了显著的能力缺口。这为未来的研究指明了需重点突破的方向。

Abstract: Most existing Theory of Mind (ToM) benchmarks for foundation models rely on
variations of the Sally-Anne test, offering only a very limited perspective on
ToM and neglecting the complexity of human social interactions. To address this
gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM
capabilities in environments rich with social interactions and spatial
dynamics. While current ToM benchmarks are limited to text-only or dyadic
interactions, ToM-SSI is multimodal and includes group interactions of up to
four agents that communicate and move in situated environments. This unique
design allows us to study, for the first time, mixed cooperative-obstructive
settings and reasoning about multiple agents' mental state in parallel, thus
capturing a wider range of social cognition than existing benchmarks. Our
evaluations reveal that the current models' performance is still severely
limited, especially in these new tasks, highlighting critical gaps for future
research.

</details>


### [96] [ICR: Iterative Clarification and Rewriting for Conversational Search](https://arxiv.org/abs/2509.05100)
*Zhiyu Cao,Peifeng Li,Qiaoming Zhu*

Main category: cs.CL

TL;DR: 本文提出ICR迭代澄清与改写框架，解决查询中多处模糊表达难以同步改写问题，有效提升了检索性能并达到了最新最佳水平。


<details>
  <summary>Details</summary>
Motivation: 以往对话查询改写直接端到端改写查询，但遇到查询中有多个模糊表达时，难以一次性准确定位和处理多个位置，影响系统准确性。

Method: 提出 ICR（Iterative Clarification and Rewriting）框架，采用迭代生成澄清问题与改写查询的方法，以解决一次性定位和改写多个模糊表达的困难。

Result: ICR 框架在澄清-改写的迭代过程中显著提升了检索效果，在两个主流数据集上取得了最优结果。

Conclusion: ICR 框架能有效提升对话查询改写系统的检索性能，并在两个主流数据集上达到最新最佳表现。

Abstract: Most previous work on Conversational Query Rewriting employs an end-to-end
rewriting paradigm. However, this approach is hindered by the issue of multiple
fuzzy expressions within the query, which complicates the simultaneous
identification and rewriting of multiple positions. To address this issue, we
propose a novel framework ICR (Iterative Clarification and Rewriting), an
iterative rewriting scheme that pivots on clarification questions. Within this
framework, the model alternates between generating clarification questions and
rewritten queries. The experimental results show that our ICR can continuously
improve retrieval performance in the clarification-rewriting iterative process,
thereby achieving state-of-the-art performance on two popular datasets.

</details>


### [97] [PRIM: Towards Practical In-Image Multilingual Machine Translation](https://arxiv.org/abs/2509.05146)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Chong Feng,Xin Li,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: 本文针对真实场景的多语种图像内文本翻译任务，构建并公开了PRIM数据集，并提出了VisTrans模型，显著提升了翻译与视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端图像内文本翻译（IIMT）研究主要基于合成数据集，这些数据集背景简单、字体单一、文本位置固定且大多为双语翻译，不能真实反映现实世界场景，导致研究与实际应用存在较大差距。

Method: 作者提出了一种实用的多语种图像内翻译任务（IIMMT），并构建并标注了PRIM数据集，该数据集包含复杂背景、多样字体、不同文本位置的真实世界场景图像，支持多语种翻译。同时，提出了端到端模型VisTrans，通过分别处理图像中的视觉文本和背景信息，应对现实条件下的挑战，兼顾多语种翻译能力和视觉质量。

Result: VisTrans模型在PRIM数据集上比其他模型取得了更好的翻译质量与视觉效果。

Conclusion: PRIM数据集和VisTrans模型为实用多语种图像内翻译的研究奠定基础，缩小了实验与现实的差距；方法有效提升了翻译和视觉表现。

Abstract: In-Image Machine Translation (IIMT) aims to translate images containing texts
from one language to another. Current research of end-to-end IIMT mainly
conducts on synthetic data, with simple background, single font, fixed text
position, and bilingual translation, which can not fully reflect real world,
causing a significant gap between the research and practical conditions. To
facilitate research of IIMT in real-world scenarios, we explore Practical
In-Image Multilingual Machine Translation (IIMMT). In order to convince the
lack of publicly available data, we annotate the PRIM dataset, which contains
real-world captured one-line text images with complex background, various
fonts, diverse text positions, and supports multilingual translation
directions. We propose an end-to-end model VisTrans to handle the challenge of
practical conditions in PRIM, which processes visual text and background
information in the image separately, ensuring the capability of multilingual
translation while improving the visual quality. Experimental results indicate
the VisTrans achieves a better translation quality and visual effect compared
to other models. The code and dataset are available at:
https://github.com/BITHLP/PRIM.

</details>


### [98] [Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable LLMs: The TAXAL Framework](https://arxiv.org/abs/2509.05199)
*David Herrera-Poyatos,Carlos Peláez-González,Cristina Zuheros,Virilo Tejedor,Rosana Montes,Francisco Herrera*

Main category: cs.CL

TL;DR: 本文提出TAXAL三元融合框架，整合认知、功能和因果维度，提升大语言模型在高风险领域的可解释性。通过实际案例验证其在不同场景下的适应性和实效，为可信AI应用及其社会技术落地提供理论与实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）被越来越多应用于高风险领域，模型的不透明性、偏见和不稳定性影响了人们对其信任和责任追究。传统的可解释性方法主要关注模型表层输出，难以揭示其推理路径、规划逻辑及系统性影响。该论文旨在提出一种能够全面提升LLMs可解释性的体系，满足复杂应用场景下的需求。

Method: 提出TAXAL（Triadic Alignment for eXplainability in Agentic LLMs）三元融合框架，将认知（用户理解）、功能（实用性）和因果（推理忠实性）三大维度结合。该框架涵盖解释设计、评估和部署的方法体系，包括对现有可解释性技术（事后归因、对话接口、解释感知提示等）进行整合与模型定位。通过实际案例（法律、教育、医疗、公共服务）展示其适应不同机构约束和利益相关者角色的能力。

Result: TAXAL框架为各类社会技术场景中的LLMs解释设计和应用提供了统一且兼顾角色差异的基础，能够根据不同机构和利益相关方的需求调整解释策略。案例分析证明该方法具有可扩展性和适应性。

Conclusion: TAXAL推动可解释性成为技术与社会技术实践的结合，支持在代理型AI时代实现可信、具备情境适应力的LLMs应用。该框架兼具概念清晰性、设计模式和部署路径，对构建负责任AI系统具有指导意义。

Abstract: Large Language Models (LLMs) are increasingly being deployed in high-risk
domains where opacity, bias, and instability undermine trust and
accountability. Traditional explainability methods, focused on surface outputs,
do not capture the reasoning pathways, planning logic, and systemic impacts of
agentic LLMs.
  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a
triadic fusion framework that unites three complementary dimensions: cognitive
(user understanding), functional (practical utility), and causal (faithful
reasoning). TAXAL provides a unified, role-sensitive foundation for designing,
evaluating, and deploying explanations in diverse sociotechnical settings.
  Our analysis synthesizes existing methods, ranging from post-hoc attribution
and dialogic interfaces to explanation-aware prompting, and situates them
within the TAXAL triadic fusion model. We further demonstrate its applicability
through case studies in law, education, healthcare, and public services,
showing how explanation strategies adapt to institutional constraints and
stakeholder roles.
  By combining conceptual clarity with design patterns and deployment pathways,
TAXAL advances explainability as a technical and sociotechnical practice,
supporting trustworthy and context-sensitive LLM applications in the era of
agentic AI.

</details>


### [99] [Hunyuan-MT Technical Report](https://arxiv.org/abs/2509.05209)
*Mao Zheng,Zheng Li,Bingxin Qu,Mingyang Song,Yang Du,Mingrui Sun,Di Wang*

Main category: cs.CL

TL;DR: 作者发布了首个开源的Hunyuan-MT-7B多语种翻译模型和其增强型Chimera版本，支持33种语言，特别优化普通话与少数民族语言/方言间翻译。其慢思考集成方法使模型在多个主流和低资源语言对上超越同规模及SOTA大模型，WMT2025评测几乎全胜，展现了极高的跨语种鲁棒性与实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有多语言机器翻译模型在支持多样化语种，尤其是普通话与少数民族语言及方言之间的翻译时效果有限。为满足实际需求并提升各类场景下的翻译质量，需要开发支持多语种、可适用于多任务、在高低资源语言间均具优异表现的新型翻译模型。

Method: 模型开发分为三阶段：首先通过通用和翻译相关（MT-oriented）预训练构建通用基础能力；其次进行有监督微调（SFT），实现任务定制化；最后通过强化学习（RL）及弱到强的RL进行进一步高级对齐。Hunyuan-MT-Chimera-7B采用类似慢思考（slow thinking）模式，集成不同参数下Hunyuan-MT-7B生成的多个输出，从而达到优于CoT慢思考模型的性能。

Result: 提出的两个模型在WMT2025通用翻译任务中展现出SOTA水平，并在大多数主流和低资源语言对上取得了领先性能，极大推动了普通话与少数民族语言及方言间翻译的进步。

Conclusion: 本文提出的Hunyuan-MT-7B和Hunyuan-MT-Chimera-7B模型在多语言翻译，尤其是普通话与少数民族语言及方言之间的翻译任务上，取得了优于同参数规模翻译模型及大多数SOTA大型模型的性能。此外，在WMT2025通用机器翻译任务中，模型在31个语言对中有30个排名第一，显示了跨多语种、高低资源语言的强大鲁棒性。

Abstract: In this report, we introduce Hunyuan-MT-7B, our first open-source
multilingual translation model, which supports bidirectional translation across
33 major languages and places a special emphasis on translation between
Mandarin and several ethnic minority languages as well as dialects.
Furthermore, to serve and address diverse translation scenarios and enhance
model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a
translation model inspired by the slow thinking mode. This model integrates
multiple outputs generated by the Hunyuan-MT-7B model under varying parameter
settings, thereby achieving performance superior to that of conventional
slow-thinking models based on Chain-of-Thought (CoT). The development of our
models follows a holistic training process specifically engineered for
multilingual translation, which begins with general and MT-oriented
pre-training to build foundational capabilities, proceeds to Supervised
Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced
alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through
comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and
Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models
of comparable parameter size and most of the SOTA large models, particularly on
the task of translation between Mandarin and minority languages as well as
dialects. In the WMT2025 shared task (General Machine Translation), our models
demonstrate state-of-the-art performance, ranking first in 30 out of 31
language pairs. This result highlights the robustness of our models across a
diverse linguistic spectrum, encompassing high-resource languages such as
Chinese, English, and Japanese, as well as low-resource languages including
Czech, Marathi, Estonian, and Icelandic.

</details>


### [100] [BEDTime: A Unified Benchmark for Automatically Describing Time Series](https://arxiv.org/abs/2509.05215)
*Medhasweta Sen,Zachary Gottesman,Jiaxing Qiu,C. Bayan Bruss,Nam Nguyen,Tom Hartvigsen*

Main category: cs.CL

TL;DR: 作者提出自然语言描述时间序列的三项任务，整合数据集做了统一对比，发现多模态和视觉-语言模型优于普通语言大模型，但整体尚待提升，并建立了该领域标准基准。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析的基础模型评测没有统一基准，且往往伴随新数据集，导致不同方法之间难以直接比较，模型能力贡献不明确。

Method: 作者规范化并评估了3类基于自然语言描述时间序列的任务：识别（判断正误）、区分（多项选择）、生成（自然语言描述），并整合了4个近期数据集，针对13类最先进方法进行统一实验对比和鲁棒性评测。

Result: 实验表明：1）单纯语言模型表现较差，需要专门面向时间序列的结构；2）视觉-语言模型表现较好，验证了视觉建模的价值；3）多模态预训练时间序列-语言模型优于大语言模型，但仍有较大的提升空间，各种方法在鲁棒性测试中均表现出明显脆弱性。

Conclusion: 该研究建立了标准化任务基准，有助于深入理解和对比时间序列推理系统模型的强弱和不足。

Abstract: Many recent studies have proposed general-purpose foundation models designed
for a variety of time series analysis tasks. While several established datasets
already exist for evaluating these models, previous works frequently introduce
their models in conjunction with new datasets, limiting opportunities for
direct, independent comparisons and obscuring insights into the relative
strengths of different methods. Additionally, prior evaluations often cover
numerous tasks simultaneously, assessing a broad range of model abilities
without clearly pinpointing which capabilities contribute to overall
performance. To address these gaps, we formalize and evaluate 3 tasks that test
a model's ability to describe time series using generic natural language: (1)
recognition (True/False question-answering), (2) differentiation (multiple
choice question-answering), and (3) generation (open-ended natural language
description). We then unify 4 recent datasets to enable head-to-head model
comparisons on each task. Experimentally, in evaluating 13 state-of-the-art
language, vision--language, and time series--language models, we find that (1)
popular language-only methods largely underperform, indicating a need for time
series-specific architectures, (2) VLMs are quite successful, as expected,
identifying the value of vision models for these tasks and (3) pretrained
multimodal time series--language models successfully outperform LLMs, but still
have significant room for improvement. We also find that all approaches exhibit
clear fragility in a range of robustness tests. Overall, our benchmark provides
a standardized evaluation on a task necessary for time series reasoning
systems.

</details>


### [101] [HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models](https://arxiv.org/abs/2509.05218)
*Chang Dai,Hongyu Shan,Mingyang Song,Di Liang*

Main category: cs.CL

TL;DR: 本文提出了借助双曲几何的新型位置编码方法HoPE，理论创新和实验结果均表明其在捕捉长距离依赖上优于RoPE等主流方法，适用于更长文本序列建模。


<details>
  <summary>Details</summary>
Motivation: 绝对位置编码在处理超长序列时泛化能力差，基于相对位置的Alibi在极长上下文下性能下降，而RoPE虽广泛使用，但会引入阻碍长距离依赖建模的振荡型注意力模式。因此，需要更有效的序列建模方式。

Method: 借鉴双曲几何中的洛伦兹变换，提出一种超球面旋转位置编码（HoPE），利用双曲函数实现在token表示上的洛伦兹旋转，通过几何重构位置编码。理论上证明RoPE是该方法的特例，同时强制注意力随token间距单调衰减。

Result: 经过在多个加长序列基准上的困惑度评估等实验证明，HoPE在表达和泛化长距离依赖方面持续优于现有所有位置编码方法。

Conclusion: HoPE大幅提升位置编码对长距离依赖的处理能力，解决了RoPE的固有限制，理论和实验均体现其对长文本任务的广泛适用性。

Abstract: Positional encoding mechanisms enable Transformers to model sequential
structure and long-range dependencies in text. While absolute positional
encodings struggle with extrapolation to longer sequences due to fixed
positional representations, and relative approaches like Alibi exhibit
performance degradation on extremely long contexts, the widely-used Rotary
Positional Encoding (RoPE) introduces oscillatory attention patterns that
hinder stable long-distance dependency modelling. We address these limitations
through a geometric reformulation of positional encoding. Drawing inspiration
from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic
Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to
implement Lorentz rotations on token representations. Theoretical analysis
demonstrates that RoPE is a special case of our generalized formulation. HoPE
fundamentally resolves RoPE's slation issues by enforcing monotonic decay of
attention weights with increasing token distances. Extensive experimental
results, including perplexity evaluations under several extended sequence
benchmarks, show that HoPE consistently exceeds existing positional encoding
methods. These findings underscore HoPE's enhanced capacity for representing
and generalizing long-range dependencies. Data and code will be available.

</details>


### [102] [Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation](https://arxiv.org/abs/2509.05226)
*Abdul Waheed,Chancharik Mitra,Laurie Z. Wang,Deva Ramanan,Bhiksha Raj*

Main category: cs.CL

TL;DR: 本论文提出了一种通过训练数据控制模型推理深度的新方法，使模型在简单问题上减少推理步骤、复杂问题上维持足够推理，提升生成质量和效率，且无需改变模型结构。


<details>
  <summary>Details</summary>
Motivation: 现有的chain-of-thought（CoT）推理在处理简单问题时常常生成冗长的输出，但实际上并不总需要如此复杂的推理过程，因此希望让模型能根据问题难度灵活调整推理深度，提高生成效率与实用性。

Method: 提出了一个根据问题难度动态调整推理深度的框架。方法是在后训练阶段，构建与问题难度成正比的链式思维(CoT)推理数据，无需更改模型结构，而是通过精心设计的训练样本来引导。比较了 SFT（有监督微调）、DPO（直接偏好优化）以及两者结合对推理表现的影响。

Result: SFT主要使模型学习推理长度与格式，DPO则更好地保持推理准确度，二者结合可显著缩短推理长度，同时保持或提升推理效果。实验数据显示模型能根据问题难度进行最小化推理与复杂问题保持深度。

Conclusion: 通过数据驱动的训练，模型无需结构修改即可获得按需推理能力，有效实现了随问题复杂度自适应的“成比例思考”，兼顾效率与准确率。

Abstract: Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose
output for simpler problems. We present a framework for difficulty-aware
reasoning that teaches models to dynamically adjust reasoning depth based on
problem complexity. Remarkably, we show that models can be endowed with such
dynamic inference pathways without any architectural modifications; we simply
post-train on data that is carefully curated to include chain-of-thought traces
that are proportional in length to problem difficulty. Our analysis reveals
that post-training via supervised fine-tuning (SFT) primarily captures patterns
like reasoning length and format, while direct preference optimization (DPO)
preserves reasoning accuracy, with their combination reducing length and
maintaining or improving performance. Both quantitative metrics and qualitative
assessments confirm that models can learn to "think proportionally", reasoning
minimally on simple problems while maintaining depth for complex ones.

</details>


### [103] [CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models](https://arxiv.org/abs/2509.05230)
*Aysenur Kocak,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 文章提出CURE框架，通过内容提取、反转网络和对比学习，有效去除不良概念偏见，在多个数据集上显著提升鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练语言模型在各类应用中表现卓越，但它们仍容易受到一些概念层面（而非内容层面）相关性的干扰，导致鲁棒性和公平性下降。作者意在解决这一缺陷。

Method: 作者提出了一个名为CURE的新轻量级框架。CURE首先通过专门的内容提取器结合反转网络，抽取与概念无关的表示。同时利用对比学习的可控去偏模块，精细调整残留的概念信息，从而既能抑制有害偏见，又能在特定任务中利用有益的相关性。整个过程无需监督。

Result: 在IMDB和Yelp数据集，以及三种预训练架构上，CURE方法在IMDB F1分数提升10个百分点，Yelp提升2个百分点，计算开销几乎不变。

Conclusion: CURE提出了一种灵活、无监督的去偏见范式，有助于提升语言模型的可靠性与公平性。

Abstract: Pre-trained language models have achieved remarkable success across diverse
applications but remain susceptible to spurious, concept-driven correlations
that impair robustness and fairness. In this work, we introduce CURE, a novel
and lightweight framework that systematically disentangles and suppresses
conceptual shortcuts while preserving essential content information. Our method
first extracts concept-irrelevant representations via a dedicated content
extractor reinforced by a reversal network, ensuring minimal loss of
task-relevant information. A subsequent controllable debiasing module employs
contrastive learning to finely adjust the influence of residual conceptual
cues, enabling the model to either diminish harmful biases or harness
beneficial correlations as appropriate for the target task. Evaluated on the
IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an
absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,
while introducing minimal computational overhead. Our approach establishes a
flexible, unsupervised blueprint for combating conceptual biases, paving the
way for more reliable and fair language understanding systems.

</details>


### [104] [Uniform Information Density and Syntactic Reduction: Revisiting $\textit{that}$-Mentioning in English Complement Clauses](https://arxiv.org/abs/2509.05254)
*Hailin Hao,Elsi Kaiser*

Main category: cs.CL

TL;DR: 作者利用现代机器学习与神经网络方法，对大规模会话语料库分析，证实了信息密度影响that省略，并指出基于词嵌入的信息密度估计更具解释力。


<details>
  <summary>Details</summary>
Motivation: 前人发现英语宾语从句中的that在信息密度较低（更可预测）时更可能被省略。为进一步验证和细化信息密度与that使用的关系，作者利用新技术和大规模数据进行深入分析。

Method: 利用大规模会话语料库，结合机器学习和神经网络语言模型，分别用传统的动词子分类概率与上下文词嵌入计算信息密度，比较二者对that省略现象解释力的差异。

Result: 实验不仅复制了信息密度和that省略的已知关系，而且发现词嵌入模型对补语连接词使用的解释力更强，能捕捉到更多语言的细致变异。

Conclusion: 通过对现代大规模会话语料库的分析，研究证实了信息密度与英文宾语从句中补语连接词that的使用存在关系，并且基于上下文词嵌入的方法优于传统矩阵动词子分类概率的方法，更全面解释了连接词使用变异。

Abstract: Speakers often have multiple ways to express the same meaning. The Uniform
Information Density (UID) hypothesis suggests that speakers exploit this
variability to maintain a consistent rate of information transmission during
language production. Building on prior work linking UID to syntactic reduction,
we revisit the finding that the optional complementizer $\textit{that}$in
English complement clauses is more likely to be omitted when the clause has low
information density (i.e., more predictable). We advance this line of research
by analyzing a large-scale, contemporary conversational corpus and using
machine learning and neural language models to refine estimates of information
density. Our results replicated the established relationship between
information density and $\textit{that}$-mentioning. However, we found that
previous measures of information density based on matrix verbs'
subcategorization probability capture substantial idiosyncratic lexical
variation. By contrast, estimates derived from contextual word embeddings
account for additional variance in patterns of complementizer usage.

</details>


### [105] [Elucidating the Design Space of Decay in Linear Attention](https://arxiv.org/abs/2509.05282)
*Zhen Qin,Xuyang Shen,Yiran Zhong*

Main category: cs.CL

TL;DR: 本文系统分析了线性复杂度序列模型中的衰减机制设计，归纳出四大设计维度，并通过实验指出参数化与共享策略需严选、标量与向量衰减优劣依赖具体场景，而RoPE通常无益，为模型设计指明方向。


<details>
  <summary>Details</summary>
Motivation: 线性复杂度序列模型广泛应用于自然语言处理，但其衰减机制的设计存在许多未解问题。本文动机在于系统梳理和分析影响衰减机制表现的关键设计维度，提升相关模型的理解与表现。

Method: 作者从参数化策略、参数共享、衰减粒度（标量/向量）以及与RoPE等相对位置编码兼容性四个维度，系统性地界定并实证分析了不同衰减机制的设计空间。通过在多种语言建模任务上的大量实验，比较不同机制的性能表现。

Result: 1. 衰减参数化策略对性能影响巨大，仅在特定参数范围内表现最佳；2. 参数共享不能随意进行，否则易导致性能下降；3. 同等参数化策略下，标量衰减普遍不如向量衰减，但特殊情况下反而胜出；4. RoPE等相对位置编码对大多数线性注意力机制并无显著益处。

Conclusion: 线性复杂度模型的衰减机制需格外关注参数化策略与参数共享设置，标量与向量衰减需按情境灵活选用，而常用的RoPE位置编码对提升此类模型的作用有限。该工作为后续相关模型设计提供了系统性建议。

Abstract: This paper presents a comprehensive investigation into the decay mechanisms
inherent in linear complexity sequence models. We systematically delineate the
design space of decay mechanisms across four pivotal dimensions:
parameterization strategy, which refers to the computational methodology for
decay; parameter sharing, which involves the utilization of supplementary
parameters for decay computation; decay granularity, comparing scalar versus
vector-based decay; and compatibility with relative positional encoding
methods, such as Rotary Position Embedding (RoPE). Through an extensive series
of experiments conducted on diverse language modeling tasks, we uncovered
several critical insights. Firstly, the design of the parameterization strategy
for decay requires meticulous consideration. Our findings indicate that
effective configurations are typically confined to a specific range of
parameters. Secondly, parameter sharing cannot be used arbitrarily, as it may
cause decay values to be too large or too small, thereby significantly
impacting performance. Thirdly, under identical parameterization strategies,
scalar decay generally underperforms compared to its vector-based counterpart.
However, in certain scenarios with alternative parameterization strategies,
scalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our
analysis reveals that RoPE, a commonly employed relative positional encoding
method, typically fails to provide tangible benefits to the majority of linear
attention mechanisms.

</details>


### [106] [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291)
*Deniz Bayazit,Aaron Mueller,Antoine Bosselut*

Main category: cs.CL

TL;DR: 提出一种利用稀疏交叉编码器对大语言模型预训练期间语言特征演化进行追踪与分析的新方法，结合RelIE指标实现特征因果影响的定位，有助于理解模型能力的形成过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在预训练时能学会复杂的抽象概念，但我们并不清楚这些语言能力是在什么时候、以何种方式产生的，因为常规评测方法难以揭示模型内部如何获得这些能力。

Method: 提出使用稀疏交叉编码器（sparse crosscoders）在不同模型检查点间发现并对齐特征，通过训练交叉编码器、引入新指标RelIE（相对间接效应），追踪特征在训练过程中何时对任务表现产生因果影响，进而跟踪语言特征在模型预训练期间的演化。

Result: 交叉编码器方法能够捕捉到语言特征的出现、保持以及消失等过程，展示了方法的可扩展性和与模型结构无关的优势，为深入解释和分析模型预训练期间的表征学习提供了一条有效路径。

Conclusion: 通过本方法可以更细致地理解大语言模型表征与能力的形成和变化过程，有助于模型解释性与能力分析。

Abstract: Large language models (LLMs) learn non-trivial abstractions during
pretraining, like detecting irregular plural noun subjects. However, it is not
well understood when and how specific linguistic abilities emerge as
traditional evaluation methods such as benchmarking fail to reveal how models
acquire concepts and capabilities. To bridge this gap and better understand
model training at the concept level, we use sparse crosscoders to discover and
align features across model checkpoints. Using this approach, we track the
evolution of linguistic features during pretraining. We train crosscoders
between open-sourced checkpoint triplets with significant performance and
representation shifts, and introduce a novel metric, Relative Indirect Effects
(RelIE), to trace training stages at which individual features become causally
important for task performance. We show that crosscoders can detect feature
emergence, maintenance, and discontinuation during pretraining. Our approach is
architecture-agnostic and scalable, offering a promising path toward more
interpretable and fine-grained analysis of representation learning throughout
pretraining.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [107] [Capturing an Invisible Robber using Separators](https://arxiv.org/abs/2509.05024)
*Igor Potapov,Tymofii Prokopenko,John Sylvester*

Main category: cs.DM

TL;DR: 提出了一种更高效且空间优化的无可见性警匪游戏解法，并为多类图给出了更好的近似警察数界限。


<details>
  <summary>Details</summary>
Motivation: 传统警匪游戏假设警察可以随时观察到强盗的位置，但许多实际情况中强盗是不可见的。已知的无可见性解法复杂度较高，存在改进空间，尤其是在捕获速度和空间消耗方面。

Method: 将警匪问题建模为无可见性变体，提出了基于分离层级的方法替代现有的路径宽度分解方法，并就多类图给出了多项式时间可计算策略下的近似警察数界限。

Result: 提出方法在大多数情况下获得更优的抓捕时间及更低的空间复杂度，对多种图类型获得更紧的近似无可见性警察数上界。

Conclusion: 本文提出了一种基于分离层级的新解法，在大多数情况下，在不增加无可见性警察数的前提下，提高了警察抓住强盗的效率，并优化了空间复杂度。作者还改进了多类图上的近似无可见性警察数的界限。

Abstract: We study the zero-visibility cops and robbers game, where the robber is
invisible to the cops until they are caught. This differs from the classic game
where full information about the robber's location is known at any time. A
previously known solution for capturing a robber in the zero-visibility case is
based on the pathwidth decomposition. We provide an alternative solution based
on a separation hierarchy, improving capture time and space complexity without
asymptotically increasing the zero-visibility cop number in most cases. In
addition, we provide a better bound on the approximate zero-visibility cop
number for various classes of graphs, where approximate refers to the
restriction to polynomial time computable strategies.

</details>


### [108] [CAZAC sequence generation of any length with iterative projection onto unit circle: principle and first results](https://arxiv.org/abs/2509.05097)
*Karine Amis,Eloi Boutillon,Emmanuel Boutillon*

Main category: cs.DM

TL;DR: 该文提出IPUC算法，实现任意长度近CAZAC序列生成，并通过模拟退火优化序列特性，突破了传统方法的应用局限。


<details>
  <summary>Details</summary>
Motivation: 现有CAZAC序列的分析只覆盖了有限的字母表，序列越长可用的字母表比例越小，限制了应用范围。为突破这一限制，作者提出新的序列构造方法。

Method: 文章采用迭代投影到单位圆（IPUC）算法，利用随机初值生成任何长度的近CAZAC序列，并结合模拟退火优化相关函数旁瓣比。同时对长度为8的输出进行了等价类分类和代表性表达式分析。

Result: IPUC算法能够从任意随机种子生成近CAZAC序列，长度不受限制。对于长度为8的序列，明确给出了等价类划分和代表的解析表达式。通过模拟退火，获得了更适合雷达应用的近CAZAC序列。

Conclusion: 本文提出的IPUC算法能够以任意长度生成近似CAZAC序列，并通过模拟退火进一步优化雷达应用中相关特性。

Abstract: Constant amplitude zero-autocorrelation (CAZAC) sequences are mainly used for
synchronization in communication and radar applications. The state-of-the-art
proposes analytical derivation of specific families whose major limitation
comes from the alphabet which only represents a fraction of the whole, the
longer the sequences, the smaller the fraction. The objective of the paper is
threefold, first to present the construction of constant amplitude
zero-circular autocorrelation sequences of any length using iterative
projection onto Unit Circle (IPUC) algorithm. This algorithm allows, from any
random seed, to generate a near-CAZAC sequence. Then, focusing on length-8
sequences, we propose a classification of the IPUC output with an analytical
expression of a representative for each identified equivalence class. Finally,
the IPUC is applied within a simulated-annealing process to generate near-CAZAC
sequences suitable for radar applications with optimized ratio between first
and second lobes of the non-circular autocorrelation function.

</details>
