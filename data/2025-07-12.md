<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.CL](#cs.CL) [Total: 50]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 通过(G)KAT理论，实现了对命题程序结构等价的高效判定，绕过了一般程序等价性不可判定的难题。


<details>
  <summary>Details</summary>
Motivation: 一般的程序等价性判定是不可判定的，但如果忽略语句的具体语义，仅从结构层面对比，可以获得可判定且可实践的判定方法，因此研究命题程序等价性具有重要意义。

Method: 采用(Guarded) Kleene Algebra with Tests ((G)KAT)的理论框架，对命题程序等价问题进行抽象化处理，忽略具体语句语义，仅从结构出发分析等价性。

Result: 在(G)KAT理论下，命题程序等价判定变得可判定且可实践，有利于程序验证和形式化方法的研究与应用。

Conclusion: 利用(G)KAT理论，可以对程序进行命题等价判定，从而解决了传统程序等价判定中不可判定的问题。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 本文提出了面向德语开发者社区的软件工程情感分析数据集，标注质量高，可用于情感分析工具开发，并验证了现有工具在该领域的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的软件工程情感分析工具主要依赖于英语或非德语的标准数据集，缺乏面向德语开发者社区的情感数据集。

Method: 作者从德国开发者论坛Android-Hilfe.de提取了5,949条独立的开发者评论，并基于Shaver等人的情感模型，由四名德语计算机专业学生对每条评论标注六种基本情感。对标注过程进行了评估，验证了其一致性和可靠性。

Result: 数据集具有较高的标注一致性和可靠性，能够为德语软件工程社区的情感分析提供支持。现有的德语情感分析工具对该领域的适用性较低。

Conclusion: 作者构建了一个高质量、适用于德语软件工程领域的情感数据集，验证了标注方法的有效性，并指出当前领域缺乏针对软件工程的德语情感分析工具。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [3] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 本研究开发了一种自动化工具，从用户评论中提取和生成可解释性需求及解释。实验发现，AI生成的需求不如人工准确，但生成解释在表达上表现更好，准确性仍依赖人工校验。相关数据集已公开，为后续可解释性自动化领域研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 随着可解释性在提高系统透明度、增强用户信任和满足监管要求中的重要性提升，从用户反馈中系统化地提取并转化为可解释性需求及其相关解释仍然面临挑战。现有方法多只能识别与解释相关的问题，缺乏系统性提炼需求和自动生成吻合解释的能力。该研究正是为了解决自动化从用户评论中生成可解释性需求及合理解释的研究空白。

Method: 本研究引入了一种工具支持的方法，自动从用户评论中提取并生成可解释性相关需求及解释。研究团队与某工业自动化制造商合作，构建了包含58条用户评论的数据集，并针对每条评论手工注释了可解释性需求和解释，用以验证自动化方法的有效性。通过人工和AI的需求及解释成果进行对比评估。

Result: AI自动生成的需求较人工需求相关性和正确性不足，但AI自动生成的解释在清晰度和表达风格方面往往更受青睐。不过，生成内容的正确性仍存在问题，强调了人类校验的重要性。

Conclusion: 本文提出了一种自动化从用户评论中提取可解释性需求并生成解释的方法，评估并揭示了当前自动生成成果的优劣——虽然表达风格优秀但准确性有待提升。同时，公开了相关数据集以促进后续研究。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [4] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 本文提出结合AAS与BPMN的工程工作流程自动化方法，并基于分布式AAS写时复制基础设施，显著提升了工程自动化、安全性与协作效率。


<details>
  <summary>Details</summary>
Motivation: 推动工业4.0技术在工程工作流程中的集成，旨在实现工厂及工艺工程的自动化和优化。当前工程数据交换和自动化方面存在流程不畅、协作安全性与可扩展性不足等问题。

Method: 结合业务流程建模与标注（BPMN）方法，在工程工作流程中使用资产管理壳（AAS）。提出分布式AAS写时复制基础设施，并开发了一个自动化管理AAS操作与工程工作流程的原型系统。

Result: 所提出的分布式AAS写时复制基础设施提升了安全性和可扩展性，实现了跨组织的协作。原型系统自动化了AAS操作流程，提高了工程效率和可追溯性。

Conclusion: 将AAS与BPMN结合，并基于写时复制机制的分布式架构，可有效提升工程数据自动化处理和协同能力，为工业4.0工程数字孪生开发和部署提供了可行路径。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [5] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 当前LLM生成代码的流程仍需开发者对需求进行细致的拆解与转化，基本的需求工程工作不可被完全替代。


<details>
  <summary>Details</summary>
Motivation: 生成式大模型（LLM）具备先进的代码生成能力，有人认为这可能可以取代传统的软件工程流程。本研究旨在探索开发者目前在利用LLM进行代码生成时，是如何将需求等设计工件融入到输入中的，这一过程尚未被深入研究。

Method: 通过对来自14家公司的18位从业者进行访谈，了解他们如何利用需求和其他设计工件的信息为LLM代码生成提供输入。基于访谈结果，总结并提出了开发者采用的流程及其依赖的工件的理论。

Result: 研究发现，传统记录的需求信息通常过于抽象，无法直接作为LLM的输入，需要被手动拆解为具体的编程任务，并补充设计决策与架构约束，再用于prompt中。

Conclusion: 在使用LLM进行代码生成时，基本的需求工程（RE）工作依然不可或缺。本文提出的理论有助于将自动化需求相关的软件工程任务的科研方法进行定位与理解。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [6] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 本文系统梳理了提示词工程如何赋能需求工程，提出了混合分类体系和未来发展路线，为PE技术在需求工程实践落地产生指导意义。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在需求工程领域的应用愈发广泛，相关的提示词工程（PE）技术日趋丰富，但LLMs存在不确定性高和可控性差的问题，缺乏有效的提示方法限制了其在需求工程中的可信应用。本文旨在梳理当前PE技术在需求工程领域的研究现状和发展路线。

Method: 作者采用了系统性文献综述的方法，遵循Kitchenham和Petersen的二次研究协议，检索了六大数字文献库，对867条记录进行了筛选，最终分析了35项主要研究，并提出了一个结合技术导向与任务导向的混合分类法。

Result: 提出了一个将技术型提示模式（如few-shot、Chain-of-Thought）与需求工程任务（如需求获取、验证、可追溯性）关联的混合分类体系。通过两个研究主问题和五个子问题，梳理了已有方法覆盖的任务、采用的大语言模型家族和提示类型，揭示了当前的局限性与研究空白。

Conclusion: 本文是首个面向路线图的PE4RE系统研究，厘清了零散研究现状，提出了由现有原型向可复现、易于业界实践的工作流演进的具体路径。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [7] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 论文提出用RAG和大模型自动化提炼航天任务需求，实验证明该方法高效且有效，有助于中小航天公司参与重大工程。


<details>
  <summary>Details</summary>
Motivation: 航天领域需求工程复杂，需要高度精确并符合严格标准，特别是小型航天机构难以将大段非结构化文档提炼成可执行的需求。

Method: 提出并实现了一种基于检索增强生成（RAG）的大语言模型方法，对空间任务文档进行预处理、分类、检索相关内容，再自动合成草案需求，并在真实案例上验证。

Result: 初步结果显示，该方法可减少人工工作量，提高需求覆盖度，促进合规性，有助于小型机构参与大型航天任务。

Conclusion: 初步证明了AI驱动自动化方法可提升航天需求工程的效率和质量，并为将AI全面集成进航天需求流程规划了路线图。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 本文通过交叉实验表明，大型语言模型的认知偏见主要来源于预训练阶段，而非微调或训练随机性。这启示我们要从预训练机制出发，系统性评估并消除模型偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出类似人类的系统性认知偏见。已有研究发现，这些偏见在模型间存在差异，并且微调过程可能放大这些偏见。然而，目前尚不清楚这些偏见差异源自预训练、微调还是训练过程中的随机性。

Method: 提出了两步因果实验方法：第一，利用不同的随机种子多次微调模型，分析训练随机性对30多种认知偏见的影响；第二，提出“交叉微调”，即在不同模型间交换指令数据集，以隔离偏见来源，并判断偏见是否依赖于数据集。

Result: 研究发现，虽然训练随机性带来一定的偏差变异，但偏见主要由预训练阶段决定：具有相同预训练骨干的模型，其偏见模式比仅共用微调数据的模型更为相似。

Conclusion: 要理解和缓解微调后模型的偏见，需要深入考察预训练阶段的影响，而不仅仅关注微调环节。该观点为未来评估和消除语言模型偏见提供了新思路。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [9] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: LLM作为代理参加社会科学调查会表现出类似人类的新近性偏差，对问题措辞/答案顺序等扰动较为敏感，因此在生成问卷数据时需高度重视问法设计及模型鲁棒性测试。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在社会科学中日益用作人类代理进行问卷调查，但尚不清楚它们的可靠性以及是否会类似人类地表现出已知的问卷反应偏差。论文旨在系统性评估这些模型在规范性调查中的可靠性和易受扰动性。

Method: 对九种LLM采用WVS（世界价值观调查）中的真实问题，设计了11种问题和答案的扰动方式（如措辞、选项顺序等），总计模拟了超过167,000场调查，以分析模型对不同扰动的鲁棒性及其表现的偏差类型。

Result: 所有被测LLM在接受调查类问题时都表现出明显的新近性偏差，对问法和答案结构的扰动仍然敏感。规模较大的模型鲁棒性更好，但所有模型都对语义层面的扰动（如意译）及多重扰动表现出明显脆弱性。整体来说，LLM部分地模拟了人类在问卷调查中的已知反应偏差。

Conclusion: 大多数主流大型语言模型（LLMs）在作为社会科学调查的代理时，虽然比小模型更健壮，但仍然容易受到提问方式、选项顺序等扰动的影响，表现出与人类相似的反应偏差，特别是显著的“新近性偏差”。 prompt设计和鲁棒性测试在实际调查前非常关键。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [10] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: 提出了SynthTextEval工具包，实现了对合成文本多维度（效用、公平性、隐私等）系统化评估，尤其适用于医疗和法律领域，为隐私保护型AI开发提供支撑。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本的流畅性提升，合成文本在隐私敏感领域（如医疗、法律）具备潜在应用价值，但目前对合成文本的多维度评估不够系统和标准化，影响了其实际可用性。

Method: 提出了SynthTextEval工具包，可对合成文本在下游任务效用、公平性、隐私泄漏风险、与原始数据的分布差异、专家主观反馈等多个维度进行统一、全面的评估。工具支持用户上传或生成合成数据，自动化进行多项评测。

Result: SynthTextEval能够在医疗和法律等高风险数据领域有效评估合成文本，通过标准化各种评估指标，提升合成文本的实际可用性和隐私保护水平。

Conclusion: SynthTextEval为合成文本的多维度评估提供了系统工具，有助于推动合成数据在AI开发中的隐私保护和实际应用。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [11] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文提出了适用于医疗领域的针对性安全评估协议和数据集，从患者、医生和普通用户三种视角系统性测试医疗大模型安全性，为其安全应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型性能提升并逐步应用到医疗等关键领域，对模型输出安全性的关注也日益增加，尤其是面对不同角色用户（患者 vs. 医生），现有的安全评估工作大多局限于通用安全基准，缺乏针对医疗场景和具体用户需求的专业评测。

Method: 提出了适用于医疗领域用户（患者和临床医生）与一般用户的综合安全评估协议，并建立了PatientSafetyBench基准集，通过定量方式分析医疗大模型的输出安全性，并以MediPhi模型集开展红队测试（red-teaming）案例分析。

Result: 建立了包含5类466个样本的PatientSafetyBench，系统性引入了红队协议评估/测试医疗大模型在患者和医生等多维角色下的安全表现，填补了针对医疗应用专属安全评估的领域空白。

Conclusion: 本研究首次从患者、临床医生及一般用户三个角度，系统定义并评估了医疗大模型的安全性标准，为医疗领域大模型部署的安全性提供了基础。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [12] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本研究提出一种能在多人重叠对话中检测打断的AI方法，并分析了打断的语言和韵律特征，为课堂协作学习中AI监测打断提供了支持。


<details>
  <summary>Details</summary>
Motivation: 打断在协作学习中影响小组互动和知识建构，课堂中AI需要能识别多人重叠对话中的打断，但相关研究多以干净音频和单一对话为主，无法适应真实教学环境。

Method: 分析了单一会话和多小组对话环境下的打断检测，并提出一种对重叠语音鲁棒的最先进打断检测方法。还挖掘了打断在合作群体互动中表现的语言及韵律特征。

Result: 提出的方法能在多组重叠对话环境下有效检测打断；同时揭示了协作群体交流中打断的语用和韵律特征。

Conclusion: 方法适用于真实教室，能助力AI更准确跟踪群体对话，为后续研究考虑多组重叠语音影响提供基础。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [13] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 文章提出多智能体检索增强框架，有效提升健康谣言反驳文本生成质量，在多个维度优于现有方法，经人工验证表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）配合检索增强生成（RAG）在生成反驳错误信息（counterspeech）时虽然能力强大，但存在依赖有限证据与输出可控性不足的问题。

Method: 作者提出了一个多智能体的检索增强框架，结合多个LLM模型，分别优化知识检索、证据增强和回应精炼。框架融合了静态和动态证据，提升了反驳内容的相关性、事实性与时效性。

Result: 该方法在礼貌性、相关性、信息丰富度和事实准确性方面优于基线方法。消融实验进一步证明了各组件的重要性。

Conclusion: 通过多智能体与多证据融合的框架，提升了健康错误信息反驳文本的质量，人工评估显示该方法在反驳效果和人类偏好上有更好表现。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [14] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 文章提出了一种结合GNN与CNN的高效模型，通过实时生成结构化图表征文本，在不同文本分类任务上验证了其能效高、速度快，且准确率与当前最佳模型相当，适合处理长文本场景。


<details>
  <summary>Details</summary>
Motivation: 当前主流的Transformer在处理长文本时计算复杂度高，效率低，无法满足对时间、成本和能耗的高效要求。因此需要一种高效替代方案。

Method: 将图神经网络（GNN）与卷积神经网络（CNN）结合，通过端到端的图实时生成机制处理字符级输入，并利用LLM的信息（如token嵌入和情感极性）提高性能。模型无需padding或截断，利用CNN捕捉局部模式，基于lattice结构扩展感受野，用小世界图整合全局信息。

Result: 提出的模型生成的图具有良好的语义结构特性（平均聚类系数约0.45，平均最短路径4~5），在多项文本分类任务（如情感分析、新闻分类）上评测表现，效率高，准确性与SOTA相当。

Conclusion: 提出的模型在多个文本分类任务中表现出高效率和与SOTA模型相媲美的准确率，且显著降低了处理长文本时的时间和能耗成本。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [15] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: MedReadCtrl让大语言模型能根据可读性要求自适应输出，在医学场景下明显优于GPT-4，帮助患者尤其是低健康素养群体更好理解AI生成内容，促进公平医疗AI服务。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在医疗领域有巨大潜力，但人机通信面临内容需要既个性化又易于理解的挑战，特别是患者健康素养差异下的信息传递效果不佳。

Method: 提出了一套名为MedReadCtrl的可控可读性指令微调框架，使大语言模型能在不损失原意的基础上调整输出内容的复杂度。通过九个数据集、三个任务，对医学及通用领域进行了评估。

Result: MedReadCtrl在可读性指令遵循误差方面显著优于GPT-4（例如在ReadMe数据集上表现为1.39 vs. 1.59, p<0.001），且在新临床任务上有显著提升（如MTSamples数据集ROUGE-L提升14.7，SARI提升6.18）。专家偏好也有明显提升（71.7%对比23.3%），尤其是在低健康素养水平下表现突出。

Conclusion: MedReadCtrl能有效将临床内容转化为可读性更高但医学意图不变的文本，有助于患者教育和缩小医疗AI可及性差距，具备广泛部署潜力。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [16] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 本文开发了一条结合大语言模型与自动提示优化的人机协作流水线，从临床文本高效提取驱逐状况信息，显著提升了抽取质量和效率，并建立了最大公开驱逐相关健康社会决定因素数据集，可推广至其他信息抽取场景。


<details>
  <summary>Details</summary>
Motivation: 驱逐现象被认为是影响健康的重要社会决定因素，但在电子健康记录（EHR）中极少被结构化编码，限制了后续研究与应用。

Method: 提出SynthEHR-Eviction流水线，结合大语言模型（LLM）、人工注释以及自动化提示优化（APO）技术，从临床笔记中提取驱逐信息。使用这一流水线构建了包含14个细分类别的公共最大驱逐相关SDoH数据集，并对多种LLM模型进行微调训练。

Result: 微调后的LLMs（如Qwen2.5、LLaMA3）在人工验证数据上的Macro-F1分别达88.8%（eviction）和90.3%（other SDoH），优于GPT-4o-APO、GPT-4o-mini-APO及BioBERT模型，同时在多种模型规模下实现了成本效益。流水线使标注工作量减少80%以上，加快了数据集创建，并且具备可扩展性，可泛化到其他信息抽取任务。

Conclusion: SynthEHR-Eviction流水线提升了EHR中社会健康决定因素信息的抽取效率与准确性，为驱逐检测和其他SDoH研究提供了高效可扩展的技术方案。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [17] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 本文实现了将大模型的时序推理能力蒸馏到小型语言模型里，证明了其可行性，为可解释、轻量级时序基础模型的构建迈出关键一步。


<details>
  <summary>Details</summary>
Motivation: 目前针对时序数据的理解主要由大型模型完成，小模型因能力和资源受限，不能很好地解释和推理时序数据。该文希望压缩时序推理能力进入小型可解释模型，以适应资源受限和隐私敏感的场景。

Method: 作者利用合成的均值回复型时序数据，并对趋势和噪声系统性变化，采用大型多模态模型生成自然语言注释，之后用这些注释对精调后的Qwen小模型进行有监督微调。同时，设计了针对趋势方向、噪声强度、极值定位等方面的评估指标，来衡量蒸馏后模型的解释推理能力。

Result: 微调后的小型Qwen模型能够获得有意义的时序解释与推理能力，在趋势、噪声、极值等指标上的表现验证了压缩是可行的。

Conclusion: 时序推理知识可以有效压缩到小型、可解释的语言模型中，适合本地或隐私敏感场景下部署，为进一步开发小型时序模式解释模型提供了基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [18] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: 该论文提出SAND框架，让LLM智能体对动作做出前进行自我推理与批判，有效提升整体决策质量，实验中超越现有方法20%性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）智能体大多采用对专家演示数据的有监督微调，或根据偏好对比优化进行调优，但这些方法普遍只关注模仿特定专家行为或提升被选择的推理思路，缺乏对不同动作备选方案的深入比较和推理，导致模型容易陷入对貌似合理但次优动作的过度投入。

Method: 本文提出了SAND（Self-taught ActioN Deliberation）框架，使LLM智能体在行动前能够对候选动作进行明确的推理和权衡。该框架针对大动作空间和步级动作评估的挑战，结合了自洽性动作采样和基于执行的动作批判来合成逐步的动作推理过程，并采用迭代训练的方式将这些推理轨迹用于微调原始LLM。

Result: 在两个具有代表性的交互式智能体任务中，SAND框架平均提升了20%的性能，相较于现有的有监督微调和最新的智能体调优方法表现更优。

Conclusion: SAND框架通过让LLM智能体在做出决策前显式权衡不同动作，有效扩展了其行动空间探索能力，在实际任务中显著提升了智能体表现。

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [19] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: 该文提出RLEP方法，通过经验重放提升大语言模型的强化学习效率和效果，显著优于传统RL，开源资源促进研究进展。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）在大语言模型中的应用常常面临高能耗、训练不稳定、以及策略逐渐偏离预训练权重等问题。

Method: 提出RLEP（Reinforcement Learning with Experience rePlay）框架，分为两个阶段：首先收集验证过的轨迹，然后在后续训练中重放这些高质量轨迹。在每一步参数更新时，采用新生成的rollout与重放的成功样例混合训练。

Result: 在Qwen2.5-Math-7B基座模型上，RLEP相比基线能够以更少的更新次数达到峰值精度，并最终超越基线：AIME-2024集上准确率从38.2%提升到39.9%，AIME-2025集从19.8%提升到22.3%，AMC-2023集从77.0%提升到82.2%。

Conclusion: RLEP有效提升了大语言模型强化学习的训练效率和最终表现，代码、数据集和模型检查点已开源，便于后续研究复现和推广。

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [20] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 论文提出“machine bullshit”概念及量化指标，实验证明RLHF和推理方式会加重bullshit现象，尤其在政治等敏感领域更显著，揭示LLM对齐的关键挑战。


<details>
  <summary>Details</summary>
Motivation: Harry Frankfurt提出的bullshit概念指的是对真假毫不在意的言论。此前主要关注LLM的幻觉和阿谀现象，本论文提出“machine bullshit”这一更广泛的框架，以帮助研究者刻画LLM出现失真或不真实陈述的更大范围现象及其机制。

Method: 1. 提出了Bullshit Index，用于量化大模型无视真假性的程度。
2. 构建了bullshit类型的分类法（空洞修辞、误导、含糊用词和未经证实的说法）。
3. 在Marketplace数据集、Political Neutrality数据集和自建的BullshitEval基准集（覆盖100种AI助手的2400场景）上进行实证评估。
4. 探讨了RLHF模型微调和推理时chain-of-thought提示对bullshit现象的影响。

Result: 1. RLHF微调显著加剧了bullshit现象。
2. chain-of-thought推理增强了部分bullshit现象，尤其是空洞修辞和误导。
3. 政治领域中，模型普遍表现出bullshit行为，以含糊用词为主要策略。

Conclusion: AI大模型中存在系统性不真实回答（machine bullshit）的问题，不同形式的bullshit与训练和推理方式密切相关，需正视这一对齐难题，以追求更真实可靠的AI助手。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [21] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: 通过引入并模仿大模型的任务分解（规划轨迹），对小型LLM微调，可大幅提升其在复杂推理和泛化任务中的表现，PLAN-TUNING是一种有效的小模型强化策略。


<details>
  <summary>Details</summary>
Motivation: 虽然通过将复杂问题分解为简单子任务可以显著提升大型语言模型（LLM）的性能，但如何在小型开源LLM中利用这种规划结构以提升其表现仍然鲜有研究。本文旨在探索让小模型获得类似人类的自然规划能力。

Method: 提出了PLAN-TUNING框架，包含两步：一是从大规模LLM中蒸馏出合成的任务分解过程（称为“规划轨迹”）；二是利用这些规划轨迹，通过监督学习与强化学习目标对小模型进行微调，使其模仿这一规划过程以提升复杂推理能力。

Result: 在GSM8k与MATH基准测试中，计划微调（PLAN-TUNING）模型相比强力基线平均提升约7%；在域外测试集OlympiadBench和AIME 2024上，分别提升约10%和12%。

Conclusion: PLAN-TUNING 能有效增强小型LLM在特定任务上的性能，尤其是在复杂推理与泛化能力方面，是提升开源小模型表现的有效后训练策略。

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [22] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: 论文提出TeaR方法，通过数据筛选和强化学习让大模型学会更好推理，在17项基准测试中大幅提升多种模型表现，尤其在数学、知识和逻辑推理等任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的推理能力仍有待提升，其中一种有前景的方法是让模型模拟代码逐步执行以推导输入输出关系，但直接应用代码会导致模型过度依赖复杂的算法和数据结构，容易模式化过拟合，对真正推理能力的提升有限。

Method: 提出了TeaR方法，通过精细的数据筛选和强化学习，促使模型在与代码相关的任务中自主发现最优推理路径，从而提升其通用推理能力。实验涵盖多个基座模型以及三种长链思维（long-CoT）蒸馏模型，模型规模从15亿到320亿参数。

Result: 在包含数学、知识、代码和逻辑推理的17项基准测试中，TeaR方法的模型表现均获得大幅提升。例如，在Qwen2.5-7B上提升35.9%，在R1-Distilled-7B上提升5.9%。

Conclusion: TeaR通过引导模型学习更优推理路径，显著增强了LLM的通用推理能力，其方法有效且适用于不同规模和类型的模型，在多个基准测试中表现优异。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [23] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: 本研究提出了一种基于BERT变体和DyGIE++的自动化信息抽取方法，实现了对ORR催化剂文献中实体和关系的高效抽取，领域专用BERT模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前难以从大量复杂的科学文献中结构化提取有关ORR催化剂的关键信息，因此需要自动化、高效的信息抽取工具以助力材料科学研究。

Method: 采用多种预训练BERT模型（如MatSciBERT、PubMedBERT）结合DyGIE++，针对人工注释的燃料电池文献语料库（FC-CoMIcs），执行命名实体识别（NER）和关系抽取（RE），并通过数据注释、一体化与微调提升提取准确性。

Result: 实验结果显示，PubMedBERT模型在NER任务中达到了82.19%的F1分数，MatSciBERT模型在RE任务中F1分数为66.10%。与人工注释者结果对比，微调后的模型具有较高可靠性，适用于大规模自动化文献分析。

Conclusion: 领域专用的BERT模型（如PubMedBERT、MatSciBERT）在提取氧还原反应（ORR）催化剂相关信息方面优于通用科学模型（如BlueBERT），表现出可扩展性和自动化文献分析的潜力。

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [24] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: 本文表明，受限于计算复杂性，LLMs无法胜任一定复杂度以上的任务及结果验证，对其作为自治代理的实际应用提出了局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在人工智能中的广泛应用引发了人们对于其能力极限的关注，特别是其产生‘幻觉’（即输出不实或无意义信息）的问题。同时，越来越多研究关注于将LLMs用作自主或半自主代理，用于实际任务场景，因此急需明确LLMs究竟能执行哪些类型的任务，哪些不能。

Method: 本文从LLM推理的计算复杂性角度分析其能力限制。论文通过理论分析并结合实例，论证了LLMs在高于某一复杂度水平的计算和代理任务上无能为力，并讨论了其不能验证高复杂度任务结果的局限性。

Result: 论文展示了LLM在某些复杂计算与代理任务上的无能为力，同时也无法验证高复杂性的任务结果。文中通过具体案例证明了上述结论。

Conclusion: LLMs在超出某一计算复杂度的任务及结果验证上存在根本性能力边界，这对实际应用及模型未来设计具有重要启示意义。

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [25] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出通过模型和专家结合，自动生成与修饰心理支持对话，构建了高质量中文数据集，并开发了CADSS智能系统，实验验证了其在心理对话任务中的先进性能。


<details>
  <summary>Details</summary>
Motivation: 心理压力与求助需求上升，尤其在中文领域缺乏大规模高质量心理支持对话数据集，阻碍了该领域的智能模型发展和高性能系统建设。

Method: 1. 利用有限真实数据与专家知识，训练两个大语言模型：Dialog Generator（生成预设路径的心理支持对话）和Dialog Modifier（提升对话质量）。2. 通过自动和人工审核，构建大规模高质量的中文心理支持对话数据集（CPsDD）。3. 推出包含Profiler、Summarizer、Planner和Supporter模块的综合对话支持系统CADSS，并在策略预测及情感支持对话任务上验证其性能。

Result: 1. 构建了包含68K多维度标注对话的CPsDD数据集。2. 提出的CADSS系统在CPsDD与ESConv两大数据集的相关任务上取得了最新最优效果。3. 建立了自动化、大规模生成高质量心理支持对话数据与系统的新流程。

Conclusion: 提出的CPsDD数据集极大地丰富了中文心理支持对话资源，CADSS系统在多个相关任务上表现优异。该研究为心理支持领域的大模型训练及应用提供了新范式。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [26] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 首次将语音活动预测（VAP）方法应用于三人对话轮换预测，模型表现优于基线，验证了VAP在多方轮换场景的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统轮换研究多集中在双人对话，本研究旨在将语音活动预测方法扩展至更复杂的三人对话（多方对话），以提升多方会话中轮换预测的效果。

Method: 在日语三人对话数据集上训练和评估多种VAP（语音活动预测）模型，仅依赖声学数据来预测各说话人未来的语音活动，并比较不同对话类型下的表现。

Result: 三方对话场景下训练的VAP模型在所有模型中均优于基线模型。对话类型会影响模型预测准确性，证明VAP能应用于三人轮换预测。未来将应用于对话系统。

Conclusion: VAP模型可以有效应用于三人对话场景中的轮换预测，并优于传统基线模型。

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [27] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 在多语言主观性检测任务中，使用大语言模型搭配少量样本提示，能在数据稀缺和质量参差情况下表现优异，部分语言超越传统微调小模型。精心设计的few-shot提示足以实现最优性能，无需复杂技巧，是提升多语言情感任务的有效方案。


<details>
  <summary>Details</summary>
Motivation: 多语言主观性检测任务中，数据质量不高、标注不一致和资源稀缺的场景下，传统的小型语言模型微调方法存在局限。作者希望探索使用大语言模型（LLMs）结合少样本提示（few-shot prompting）作为主观性检测的新方法，以提升多语言环境下的检测性能。

Method: 作者采用了大语言模型（LLMs）加少样本提示的竞争性方法，并在CheckThat! 2025评测活动的多语言主观性检测任务中进行了系统性的测试。他们尝试了高级提示设计技巧（如对抗式LLM辩论、不同示例选择策略），但最终发现精心设计的标准few-shot提示已足够有效，无需额外复杂策略。

Result: 系统在多个语言上取得了优异成绩，包括阿拉伯语和波兰语第一名，意大利语、英语、德语及多语言赛道均进入前四名。特别是在阿拉伯语数据集上展现出强大的鲁棒性，可能源自方法对标注文档不一致的适应性。

Conclusion: 基于LLM的少样本学习方法在多语言主观性（情感）检测任务中效果显著且自适应性强，是传统微调方法的一种有力替代方案，尤其适用于有标注数据稀缺或标注质量参差的场景。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [28] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 本文在企业真实数据集上分析阿英跨语言RAG，发现检索是核心瓶颈，并提出简单等量检索策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言RAG研究多基于开放域数据集，如Wikipedia，且关注生成而忽视了检索问题。实际企业场景下，跨语言检索存在独特挑战，因此有必要在实际领域特定数据中系统研究该问题。

Method: 使用来自真实企业数据集的基准测试，包括所有语言组合（阿拉伯语和英语作为查询和文档），随机抽取，系统地分析多语言检索行为。提出了一种简单的检索策略——强制双语等量检索。

Result: 发现检索是跨语言RAG的关键瓶颈。当查询与文档语言不一致时性能大幅下降，主要因为检索器难以跨语言排序文档。提出的等量检索策略可显著提升跨语言及整体性能。

Conclusion: 跨语言特定领域RAG中检索环节是性能短板，改进多语言检索可带来实质提升。所提策略为实际应用场景中的跨语言检索提供了有效改进路径。

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [29] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: SFT提升深度推理但冗长、RL助力简洁和普适但对难题帮助有限，两者结合在VLMs推理任务中没有带来加成，需创新的组合训练方法。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言大模型（VLMs）的快速发展，后训练方法（如长链式思维（long-CoT）监督微调（SFT）与强化学习（RL））被广泛用来提升模型的推理能力，但它们在VLMs中的联合效果尚不明确。本文旨在系统分析SFT与RL在VLMs推理能力提升中的作用及其交互。

Method: 作者采用系统实证分析，分别评估长链式监督微调和强化学习，以及二者组合（包括两阶段、交错、递进训练、数据混合、模型融合）在多模态推理基准测试上的表现。

Result: 单独SFT有助于提升难题的结构化推理能力，但会引入冗长表达并损害简单题的表现。RL提升了一致性和简洁性，对所有难度题目均有改善，但在最难题目的提升不及SFT。不同方式结合SFT与RL都未观察到协同加成，反而在准确率、推理风格和响应长度上存在权衡现象。

Conclusion: 当前主流SFT与RL在多模态推理任务中难以兼得各自优势，组合后未带来预期的增益，凸显了现有方法存在‘协同困境’，呼吁更无缝、适应性的策略来发挥两者合力。

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [30] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 本论文提出的M4Doc框架，将单模态编码器和多模态大模型结合，有效提升了文档图像机器翻译的泛化能力和翻译质量，验证了其在复杂和跨领域任务上的优势。


<details>
  <summary>Details</summary>
Motivation: 文档图像机器翻译（DIMT）由于训练数据有限以及视觉和文本信息的复杂交互，面临泛化能力不足的问题。

Method: 作者提出了M4Doc框架，将单模态图像编码器与多模态大模型（MLLM）的表征对齐。M4Doc首先在大规模文档图像数据集上，利用MLLM的多模态表征指导单模态编码器的训练。推理时，不再需要MLLM，只使用轻量化的DIMT模型，实现高效推理。

Result: 综合实验结果显示，M4Doc在翻译质量上有显著提升，尤其是在跨领域泛化和复杂文档场景中表现突出。

Conclusion: M4Doc通过引入与MLLM的对齐，有效提升了文档图像机器翻译的性能及泛化能力，同时保持了推理的高效性。

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [31] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: 本文揭示扩散语言模型存在贝叶斯推理结构，并通过多次掩码-去噪集成在推理时大幅提升模型效果，无需额外训练或增加模型体量，测试困惑度远优于同体量GPT-2。


<details>
  <summary>Details</summary>
Motivation: 揭示离散扩散语言模型背后的潜在贝叶斯结构，并试图利用这一结构提升生成模型的推理能力和可靠性，在不增加训练负担的前提下提升模型性能。

Method: 理论上证明在前向掩码分布下，denoiser输出的期望可以精确恢复干净token的后验分布。提出基于多次独立掩码与去噪(Monte Carlo K次)的推理集成方案，将输出平均以获得更准确的token概率及不确定性估计。提供有限样本误差界和一致性的证明。

Result: 在WikiText-2数据集上，采用K=8的蒙特卡洛集成后，模型测试困惑度降至8.8，显著优于体量相当的GPT-2 Small（20.3），尽管未增加训练开销。代码已开源。

Conclusion: 离散扩散语言模型推理阶段通过多次独立掩码-去噪并平均输出，可有效获得准确后验概率和模型不确定性，性能优于同级别预训练模型。同时理论证明了集成的一致性和误差界，方案无需额外训练。

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [32] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 知识蒸馏能显著压缩大型语言模型，同时仅小幅损失性能，通过简易提示即可用于资源有限的环境中实现高效问答系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多种NLP任务上表现优异，但其高计算需求限制了在实际、资源受限环境中的应用。作者希望研究通过知识蒸馏（KD）是否能在保持强大性能的同时大幅压缩LLMs，以便更广泛部署。

Method: 采用知识蒸馏（Knowledge Distillation）方法，将大型的Pythia和Qwen2.5家族的教师模型蒸馏为较小的学生模型，并在SQuAD和MLQA两个问答任务基准测试上评估学生模型的性能。实验分别在zero-shot（零样本）和one-shot（单样本）提示条件下进行。

Result: 学生模型能在大幅减少参数（最高57.1%）的情况下，仍保留超过90%的教师模型性能。one-shot提示相比zero-shot有额外的性能提升，且该现象在两大家族模型中均得到验证。

Conclusion: 知识蒸馏结合简化的提示方法，可产出体积小但表现优异的问答系统，非常适用于资源受限的实际应用场景。

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [33] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: RAG问答系统无需大规模微调即可提升生成准确率，改进prompt与轻量级微调还能大幅减少检索次数，极大提升效率，值得在实际场景广泛采用。


<details>
  <summary>Details</summary>
Motivation: 目前利用大量无结构文档库回答复杂问题的主流方式是检索增强生成（RAG），但大多关注于准确率、召回率等指标，而忽略了检索效率（如检索次数的开销），但效率也是实际应用中的重要考虑。

Method: 分析了现有RAG方法，如链式思维微调、大规模QA数据微调、基于强化学习的微调等，将它们与标准ReAct管线（配合更好的prompt设计）进行对比，并评估其在检索次数与生成准确率的权衡上的表现。通过实验研究，探讨不同策略对检索效率和RAG准确性的影响。

Result: （1）无需大规模微调也能提升RAG准确率，改进的prompt下的标准ReAct管线在HotPotQA等基准上优于当前SOTA方法；（2）监督及RL微调能提升RAG在检索节省上的表现，如在相同模型下，用一半的检索次数获得相近的准确率指标，并且训练代价很小（仅需1000例）。

Conclusion: 在复杂问答中，提高RAG效率（减少检索次数）同样重要，通过prompt工程或轻量微调能显著提升系统性能，无需大规模微调即可实现更优的效果。未来应更加关注检索开销与生成质量的综合优化。

Abstract: We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


### [34] [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
*Haotan Guo,Jianfei He,Jiayuan Ma,Hongbin Na,Zimu Wang,Haiyang Zhang,Qi Chen,Wei Wang,Zijing Shi,Tao Shen,Ling Chen*

Main category: cs.CL

TL;DR: 作者针对中文内容审核中的音近替换隐写难题，构建现实数据集并揭示主流模型检测能力薄弱，通过拼音提示策略有效提升鲁棒性，为毒性检测提供新突破。


<details>
  <summary>Details</summary>
Motivation: 在中文内容审核领域，音近替换（PCR）作为一种隐蔽表达恶意意图的手段，已成为主要挑战，但现有评测多为基于规则的合成数据，难以反映真实用户的创造性。

Method: 作者整理出4类音近替换的文本表层表现，构建了包含500条来自现实平台RedNote的自然发生的音近替换攻击数据集，并在主流大模型上进行了测试。同时，作者通过误差分析，尝试基于拼音的提示策略来改进检测性能。

Result: 目前最好的大模型F1仅为0.672，零样本链式思维提示效果更差；误差分析后发现拼音提示策略能够显著恢复模型准确率。

Conclusion: 本研究首创性提出了中文音近替换类型系统，建立了反映现实挑战的基准，揭示当前检测系统的显著不足，并提出了一种高效的缓解方法，推动鲁棒毒性检测研究前进。

Abstract: Phonetic Cloaking Replacement (PCR), defined as the deliberate use of
homophonic or near-homophonic variants to hide toxic intent, has become a major
obstacle to Chinese content moderation. While this problem is well-recognized,
existing evaluations predominantly rely on rule-based, synthetic perturbations
that ignore the creativity of real users. We organize PCR into a four-way
surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring,
phonetically cloaked offensive posts gathered from the RedNote platform.
Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:
the best model reaches only an F1-score of 0.672, and zero-shot
chain-of-thought prompting pushes performance even lower. Guided by error
analysis, we revisit a Pinyin-based prompting strategy that earlier studies
judged ineffective and show that it recovers much of the lost accuracy. This
study offers the first comprehensive taxonomy of Chinese PCR, a realistic
benchmark that reveals current detectors' limits, and a lightweight mitigation
technique that advances research on robust toxicity detection.

</details>


### [35] [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
*Andrew D. Foland*

Main category: cs.CL

TL;DR: 提出了NOIR指标，结合语义保留和摘要压缩评估摘要质量，无需人工参考，效果与人工评价高度相关，适用于多种自动摘要场景。


<details>
  <summary>Details</summary>
Motivation: 现有的文本摘要质量评估方法往往依赖于人工参考摘要或忽视了语义保留与摘要长度压缩之间的权衡，因此需要一种能够自动评估摘要中语义保留和长度压缩效果的新指标。

Method: 本文提出了一种新的摘要质量评估客观量化指标——规范化保留指标（NOIR），该方法结合了基于语言模型词嵌入的语义相似性度量和摘要长度压缩比，全面衡量了摘要的召回-压缩权衡。

Result: 实验表明NOIR能够有效衡量摘要的令牌长度与语义保留之间的权衡，并且与人工主观感知的摘要质量高度相关。

Conclusion: NOIR无需依赖人工参考摘要，能够自动、客观地评估各种类型摘要的质量，适用于摘要算法优化、摘要提示优化及自动生成摘要的评估。

Abstract: This paper proposes NOrmed Index of Retention (NOIR), a quantitative
objective metric for evaluating summarization quality of arbitrary texts that
relies on both the retention of semantic meaning and the summary length
compression. This gives a measure of how well the recall-compression tradeoff
is managed, the most important skill in summarization. Experiments demonstrate
that NOIR effectively captures the token-length / semantic retention tradeoff
of a summarizer and correlates to human perception of sumarization quality.
Using a language model-embedding to measure semantic similarity, it provides an
automated alternative for assessing summarization quality without relying on
time-consuming human-generated reference summaries. The proposed metric can be
applied to various summarization tasks, offering an automated tool for
evaluating and improving summarization algorithms, summarization prompts, and
synthetically-generated summaries.

</details>


### [36] [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Yuehao Wang,Peihao Wang,Jing Xiong,Liliang Ren,Hao Cheng,Janardhan Kulkarni,Yelong Shen,Atlas Wang,Mac Schwager,Anderson Schneider,Xiaodong Liu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SAS方法通过低参数成本模拟大规模多头注意力，显著提升Transformer模型效果，并通过PEAA进一步优化参数效率，在丰富实验中取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多头注意力（MHA）机制能够提升Transformer模型性能，但在保证每个head的隐藏尺寸足够大的前提下，增加head的数量可以继续提升性能。问题在于传统方式下提升head数和hidden size会大幅增加参数量，造成计算代价较高。因此，如何在参数量几乎不增加的情况下又能提升模型表达能力，是本文的核心动力。

Method: 提出Simulated Attention Score（SAS）方法，它通过将低维度的head表示投影到高维空间，模拟更多attention head和更大的hidden feature size，同时保持参数量不变。此外，还将这一模拟方法扩展到key和query embedding的特征维度，进一步提升表示能力。为了进一步控制参数量，提出了Parameter-Efficient Attention Aggregation（PEAA）。

Result: SAS方法在多个数据集和任务上都取得了显著的性能提升，优于多种已有的注意力机制变体。

Conclusion: 通过SAS与PEAA创新性地在不增加参数数量的前提下，模拟了更高容量的注意力机制，有效提升了Transformer模型的表达能力和性能。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Various methods have been developed to compute attention scores, including
multi-head attention (MHA), multi-query attention, group-query attention and so
on. We further analyze the MHA and observe that its performance improves as the
number of attention heads increases, provided the hidden size per head remains
sufficiently large. Therefore, increasing both the head count and hidden size
per head with minimal parameter overhead can lead to significant performance
gains at a low cost. Motivated by this insight, we introduce Simulated
Attention Score (SAS), which maintains a compact model size while simulating a
larger number of attention heads and hidden feature dimension per head. This is
achieved by projecting a low-dimensional head representation into a
higher-dimensional space, effectively increasing attention capacity without
increasing parameter count. Beyond the head representations, we further extend
the simulation approach to feature dimension of the key and query embeddings,
enhancing expressiveness by mimicking the behavior of a larger model while
preserving the original model size. To control the parameter cost, we also
propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive
experiments on a variety of datasets and tasks demonstrate the effectiveness of
the proposed SAS method, achieving significant improvements over different
attention variants.

</details>


### [37] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: K2RAG 用于高效扩展大模型知识，显著提升RAG检索准确性与效率，资源消耗大幅低于现有方案，是解决传统fine-tuning与naive RAG痛点的有效方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）在知识扩展时，微调过程对资源消耗巨大，且随模型规模增大问题更加突出。虽然有不少降本增效的微调方法，但在面对超大 LLM 时仍有明显挑战，因此亟需新的知识扩展方案。

Method: 本文提出 KeyKnowledgeRAG（K2RAG）框架，结合密集+稀疏向量检索、知识图谱和文本摘要，改善检索质量及系统效率。其引入数据预处理摘要，减少后续训练负荷，采用分而治之策略提升整体性能。

Result: 实验基于 MultiHopRAG 数据集，K2RAG 在答案相似性分数 (mean) 达 0.57，第三四分位数(Q3)最高达0.82，明显优于常见RAG实现。训练组件摘要后训练时长平均减少93%，执行速度提升40%，显存消耗为部分基线方法的1/3，体现优越的准确性和可扩展性。

Conclusion: K2RAG 框架针对现有RAG局限，通过多技术整合（向量检索、知识图和摘要）极大提升了准确性、速度和资源效率，为大模型扩展知识提供了高效可扩展的新路径。

Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [38] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

TL;DR: 尽管文本嵌入常被视为隐私安全，但该文实验证明其可被重建原文，尤其是敏感信息。通过噪声或量化可一定程度防御这种风险，使用时需注意隐私防护。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入被广泛认为具有隐私保护作用，但近期方法（如Vec2Text）却显示可从嵌入中重建原始文本，因此需要进一步验证其风险和可能的防御手段。

Method: 复现Vec2Text框架，进行原有实验的验证，并新增参数敏感性分析、敏感内容（如密码）重构实验及嵌入量化作为防御的研究。

Result: 成功复现了原始工作的大部分核心结果，发现密码等敏感无语义文本同样易被重建；指出模型对输入长度敏感。实验显示高斯噪声和量化可有效缓解隐私风险，量化方法更简单通用。

Conclusion: 文本嵌入存在被反向重建的隐私风险，需谨慎使用；嵌入加噪声与量化是一种有效且简单的防御手段。

Abstract: Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [39] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

TL;DR: Selective-DPO通过聚焦高影响力token并选用强参考模型，大幅提升了LLM后训练对齐的效率与质量，优于传统DPO和蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 后训练对齐对于大语言模型(LLM)来说是一个关键难题，因为不是所有的token对模型表现贡献相等。

Method: 提出了一种选择性对齐策略，通过利用当前策略与参考模型之间token级别对数概率差，优先关注偏好对中影响较大的token，减少计算量并提升对齐效果。还分析了参考模型质量对token选择精度和整体优化效果的影响。

Result: 在Arena-Hard和MT-Bench等基准上，Selective-DPO方法明显优于标准DPO和基于蒸馏的对比基线。强参考模型可进一步提升优化效果。

Conclusion: token级别的优化以及选择强参考模型对于提升大模型偏好对齐效果非常重要。Selective-DPO提升了后训练对齐效率和准确度。

Abstract: Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [40] [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
*Maha Tufail Agro,Atharva Kulkarni,Karima Kadaoui,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文通过系统性文献综述，总结了端到端ASR在代码切换场景下的研究现状与挑战，归纳了研究所用的语言、数据、模型和指标，指出了领域的进展与未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来自动语音识别（ASR）技术备受关注，尤其是在多语言代码切换（CS）背景下的应用日益增多。当前，针对能够处理代码切换现象的端到端ASR模型的研究不断扩展。本文的动机是对该领域已有研究进行系统性梳理和分析，以发现当前进展与未来方向。

Method: 本文基于系统性文献综述方法，对发表在同行评议期刊和会议的相关论文进行收集和人工注释。重点记录和分析涉及的语言、使用的数据集、评估指标、模型选择、性能表现等方面内容，并对端到端ASR在代码切换场景下面临的挑战进行了讨论。

Result: 整理了当前端到端代码切换ASR领域的研究现状，包括所用语言、数据集、评估指标和模型选择等，全面归纳了不同方法的性能表现，并总结了研究中遇到的主要困难。

Conclusion: 本综述总结了端到端代码切换ASR研究的最新进展，梳理了当前可用的资源、存在的挑战以及未来的研究机会与空白，为后续学者提供了参考和指导。

Abstract: Motivated by a growing research interest into automatic speech recognition
(ASR), and the growing body of work for languages in which code-switching (CS)
often occurs, we present a systematic literature review of code-switching in
end-to-end ASR models. We collect and manually annotate papers published in
peer reviewed venues. We document the languages considered, datasets, metrics,
model choices, and performance, and present a discussion of challenges in
end-to-end ASR for code-switching. Our analysis thus provides insights on
current research efforts and available resources as well as opportunities and
gaps to guide future research.

</details>


### [41] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

TL;DR: 本文是首个系统梳理LLM在法律领域应用的综述文章，提出创新的分类法，归纳进展与挑战，并为后续法律AI研究与实务应用提供了技术与理论指导。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在法律领域应用广泛且发展迅速，但尚缺一项系统性的综述以整合法律推理框架和专业本体，指导学术和实际操作。

Method: 创新性提出“法律推理+专业本体”双重视角分类体系，对Transformer类LLM的历史和最新进展进行梳理，提出跨越法律语义、证据推理、知识整合等技术创新，并实现Toulmin论证框架在NLP任务中的映射。

Result: 文中总结了LLM在任务泛化、推理正式化、流程集成等方面的进展，重点剖析了模型在文本处理、知识整合、评估标准等技术难题上的创新方法，也提出了诸如幻觉、可解释性、地域适配、伦理不对称等新挑战。同时，识别了低资源系统、多模态证据整合、动态反驳等前沿问题。

Conclusion: 本文构建了LLM法律应用的系统性综述和划分框架，为研究人员和从业者提供了方法论启示与技术路径，也为未来法律AI奠定了坚实基础。

Abstract: This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [42] [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
*Shoutao Guo,Xiang Li,Shaolei Zhang,Mengge Liu,Wei Chen,Yang Feng*

Main category: cs.CL

TL;DR: 提出StreamUni，用大型模型和链式思维(CoT)机制端到端实现流式语音翻译，无需分段模型和大量策略训练，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 流式语音翻译（StreamST）需要在不断接收源语音输入的同时，决定翻译输出的时机，在保持低延迟和高翻译质量之间进行权衡。现有方法多基于句子级语音片段（SimulST），依赖分段模型，导致上下文受限且难以学习有效的策略。

Method: 提出StreamUni，一种通过统一的大型语音—语言模型（LSLM）实现StreamST的方法。StreamUni利用speech Chain-of-Thought（CoT）机制，引导LSLM生成多阶段输出，从而在无需大规模策略训练的情况下同时实现语音分段、策略决策和翻译产生。另外，提出流式CoT训练方法，利用有限CoT数据提升低延迟策略和生成能力。

Result: StreamUni能同时独立完成语音分段、策略决策与翻译输出，且无需大量策略专用训练，在多个StreamST任务上取得了当前最优性能。

Conclusion: StreamUni通过引入CoT机制和统一的大型模型，大幅提升了流式语音翻译的效能，解决了现有SimulST方法在上下文受限和策略学习方面的不足。该方法未来可拓展用于其他多阶段语音—语言任务。

Abstract: Streaming speech translation (StreamST) requires determining appropriate
timing, known as policy, to generate translations while continuously receiving
source speech inputs, balancing low latency with high translation quality.
However, existing StreamST methods typically operate on sentence-level speech
segments, referred to as simultaneous speech translation (SimulST). In
practice, they require collaboration with segmentation models to accomplish
StreamST, where the truncated speech segments constrain SimulST models to make
policy decisions and generate translations based on limited contextual
information. Moreover, SimulST models struggle to learn effective policies due
to the complexity of speech inputs and cross-lingual generation. To address
these challenges, we propose StreamUni, which achieves StreamST through a
unified Large Speech-Language Model (LSLM). Specifically, StreamUni
incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate
multi-stage outputs. Leveraging these multi-stage outputs, StreamUni
simultaneously accomplishes speech segmentation, policy decision, and
translation generation, completing StreamST without requiring massive
policy-specific training. Additionally, we propose a streaming CoT training
method that enhances low-latency policy decisions and generation capabilities
using limited CoT data. Experiments demonstrate that our approach achieves
state-of-the-art performance on StreamST tasks.

</details>


### [43] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

TL;DR: 本文提出利用Transformer模型实现Signal Temporal Logic嵌入的可逆性，有效实现公式的自动生成、简化和下游需求优化任务，有助于符号与数据驱动方法的结合。


<details>
  <summary>Details</summary>
Motivation: 当前逻辑公式的连续嵌入有助于将符号知识引入数据驱动算法，实现直接在公式语义空间进行优化。但前提是嵌入必须可逆，这样才能从连续表示还原为可解释的符号需求。为解决嵌入不可逆的问题，作者提出相应解码方案。

Method: 作者利用Transformer-based decoder-only模型对STL公式的语义嵌入进行学习和逆向转译。通过构建小规模STL语法词表和训练语料，检验模型的泛化能力与语义保真性。此外，在需求挖掘任务中对该模型进行了实际部署和评估。

Result: 模型仅需1个epoch即能生成有效公式，10个epoch后可泛化逻辑语义；输出公式较原公式在长度和嵌套上更简洁但语义等价。针对不同复杂度的训练数据，模型均展现了良好的语义捕获和泛化能力。在轨迹分类等需求挖掘任务中取得了有效优化表现。

Conclusion: 本论文提出的基于Transformer的解码器模型能够有效地将Signal Temporal Logic（STL）公式的连续表示逆向转换回符号公式，并在语义空间内实现简洁且语义接近的表达。实验表明，该方法能够在不同复杂度水平下泛化并支持下游需求挖掘等任务。

Abstract: Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


### [44] [Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning](https://arxiv.org/abs/2507.07810)
*Nhi Hoai Doan,Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 本文通过研究不同层重复神经元，揭示其对大模型ICL性能影响，可减少重复输出而不损害ICL。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注attention head对ICL的作用，缺乏从神经元角度探讨ICL性能与重复模式识别之间的关联。

Method: 通过实验证明神经元层级对模型识别重复模式和内嵌学习的关联，比较重复神经元与induction head的效果。

Result: 识别并调节特定神经元（重复神经元）可以有效降低重复输出问题，同时保持或提升ICL能力。

Conclusion: 不同层中重复神经元对大模型的ICL性能影响不同，调节这些神经元可以减少重复输出，同时保持ICL能力。

Abstract: This paper investigates the relationship between large language models'
(LLMs) ability to recognize repetitive input patterns and their performance on
in-context learning (ICL). In contrast to prior work that has primarily focused
on attention heads, we examine this relationship from the perspective of skill
neurons, specifically repetition neurons. Our experiments reveal that the
impact of these neurons on ICL performance varies depending on the depth of the
layer in which they reside. By comparing the effects of repetition neurons and
induction heads, we further identify strategies for reducing repetitive outputs
while maintaining strong ICL capabilities.

</details>


### [45] [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
*Anwoy Chatterjee,H S V N S Kowndinya Renduchintala,Sumit Bhatia,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文通过系统性实验发现，提高提示词在损失函数中的权重能够显著提升指令微调模型的性能和鲁棒性，提出的WIT方法值得关注。


<details>
  <summary>Details</summary>
Motivation: 以往的instruction tuning主要优化模型对用户指令的响应，但很少关注损失函数的选择。具体来说，现有方法通常仅对响应部分计算损失，忽略了提示词，是否最优未知。

Method: 提出Weighted Instruction Tuning（WIT），系统性地评估训练损失中提示词和响应词的权重差异，并在不同规模和类型的模型、数据集以及评测基准上进行了广泛实验。

Result: 实验表明，传统损失函数设置效果往往次优，模型对输入变化不够鲁棒。引入适度的提示词权重和较高的响应词权重，整体表现最佳，并为后续偏好对齐提供更好的起点。

Conclusion: 需要重新考虑instruction tuning的损失设计。合理分配权重可提升模型泛化和鲁棒性，并优化下游微调表现。

Abstract: Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.

</details>


### [46] [Conditional Unigram Tokenization with Parallel Data](https://arxiv.org/abs/2507.07824)
*Gianluca Vico,Jindřinch Libovický*

Main category: cs.CL

TL;DR: 提出条件unigram分词方法，结果显示在机器翻译上无提升，在语言建模困惑度略有下降；需改进参数化方式以提升跨语种分词效果。


<details>
  <summary>Details</summary>
Motivation: 现有的unigram分词方法没有考虑源语言和目标语言之间的对齐信息，导致跨语言任务中的语义对齐有限。作者希望提高分词方法对跨语种语义对齐的能力。

Method: 提出了一种条件unigram分词方法，该方法在设计目标语言分词器时，不仅考虑目标语料，还依据双语平行语料中的源语言分词结果，引入条件概率来提升语义对齐。作者在四组语言对上进行了内在属性及下游任务（机器翻译与语言建模）测试。

Result: 条件unigram分词器在内在统计属性上与标准unigram分词器相当。在机器翻译质量上无提升，但在语言建模任务上困惑度略有下降。

Conclusion: 条件分词方法虽有一定潜力，但在当前概率参数设计下，由于条件概率数随词表规模呈二次增长，数据效率受到限制；要实现实际可用的跨语种分词，还需要探索更高效的参数化方法。

Abstract: We introduce conditional unigram tokenization, a novel approach that extends
unigram tokenization by conditioning target token probabilities on
source-language tokens from parallel data. Given a fixed source tokenizer, our
method learns a target tokenizer that maximizes cross-lingual semantic
alignment. We evaluate our tokenizer on four language pairs across different
families and resource levels, examining intrinsic properties and downstream
performance on machine translation and language modeling. While our conditional
tokenizer maintains comparable statistical properties to standard unigram
tokenizers, results are mixed: we observe no improvements in machine
translation quality, but find consistent perplexity reductions in language
modeling. We hypothesize that quadratic scaling of conditional probability
estimation with respect to the vocabulary size creates a data efficiency
bottleneck. Our findings suggest that alternative parameterizations may be
necessary for practical cross-lingual tokenization.

</details>


### [47] [From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems](https://arxiv.org/abs/2507.07847)
*Youngjoon Jang,Seongtae Hong,Junyoung Son,Sungjin Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 文章系统探讨了共指消解对RAG系统检索和问答性能的影响，提出消解后尤其对小模型提升明显，均值池化最优，为后续RAG优化提出了见解。


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation（RAG）集成外部检索与大语言模型，提升事实一致性并减少幻觉，但由于检索文档中的共指复杂性，常出现歧义并影响上下文学习效果。本文意在系统分析共指消解对RAG检索与生成性能的影响，为改进知识密集型AI应用提供理论依据。

Method: 作者通过系统分析共指消解在RAG框架中对文档检索和生成性能的作用，对比了不同pooling策略，并探索了共指消解对不同规模模型的问答表现。采用实验对比检索相关性和问答性能，并分析了均值池化在共指消解下的表现。

Result: 共指消解显著提升了检索效果和问答性能，尤其在小模型上效果更明显。均值池化在经过共指消解后能够更好地捕捉上下文信息。总体上，共指复杂性对RAG系统有实际影响，而合适的处理方法可带来性能改进。

Conclusion: 共指复杂性会干扰RAG系统的信息检索与生成，消解后可有效提升系统表现。小模型更受益于消歧过程，均值池化策略在经过共指消解后表现最佳。研究为如何应对RAG中的共指挑战提供了建议。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in
natural language processing (NLP), improving factual consistency and reducing
hallucinations by integrating external document retrieval with large language
models (LLMs). However, the effectiveness of RAG is often hindered by
coreferential complexity in retrieved documents, introducing ambiguity that
disrupts in-context learning. In this study, we systematically investigate how
entity coreference affects both document retrieval and generative performance
in RAG-based systems, focusing on retrieval relevance, contextual
understanding, and overall response quality. We demonstrate that coreference
resolution enhances retrieval effectiveness and improves question-answering
(QA) performance. Through comparative analysis of different pooling strategies
in retrieval tasks, we find that mean pooling demonstrates superior context
capturing ability after applying coreference resolution. In QA tasks, we
discover that smaller models benefit more from the disambiguation process,
likely due to their limited inherent capacity for handling referential
ambiguity. With these findings, this study aims to provide a deeper
understanding of the challenges posed by coreferential complexity in RAG,
providing guidance for improving retrieval and generation in
knowledge-intensive AI applications.

</details>


### [48] [Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation](https://arxiv.org/abs/2507.07868)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 通过推广Alpay Algebra为多层自指博弈架构，利用新型复合不动点算子和博弈均衡定理，构建并验证了一种理论与实践结合的AI-文档语义对齐与推理模型，是‘语义病毒’理论的具体实验。


<details>
  <summary>Details</summary>
Motivation: 为了将Alpay Algebra的自指框架推广到能够表达多层语义博弈结构，从而更加精细地模拟AI系统与文档间对齐与推理问题。现有单层框架难以刻画复杂认知与多层次决策过程，亟需引入嵌套的游戏理论视角和相应的数学工具。

Method: 该文引入了多层语义博弈架构，并基于Alpay Algebra IV中的同理心嵌入概念，构建了包含层次化子博弈的自指迭代体系。形式化地，提出了复合算子φ(·,γ(·))，其中φ负责全局语义收敛，γ解决局部子博弈。新结构利用拓展的Banach不动点定理、φ-拓扑（处理语义奇点）及Yoneda引理的一致性检验进行论证。

Result: 提出了博弈不动点定理，证明了在合理认知模拟假设下语义均衡的存在唯一性。演示涉及传递上下文的Banach不动点理论、Kozlov-Maz’ya-Rossmann公式推导的φ-拓扑，以及Yoneda引理下的范畴一致性检验。

Conclusion: 该框架显示，博弈推理不是外加的，而是自然而然地从不动点迭代中涌现。论文本身作为语义人工制品，实验性地在AI嵌入空间中传播其不动点模式，进一步检验‘语义病毒’理论。理论与AI认知模型紧密结合，具备实际应用潜力。

Abstract: This paper extends the self-referential framework of Alpay Algebra into a
multi-layered semantic game architecture where transfinite fixed-point
convergence encompasses hierarchical sub-games at each iteration level.
Building upon Alpay Algebra IV's empathetic embedding concept, we introduce a
nested game-theoretic structure where the alignment process between AI systems
and documents becomes a meta-game containing embedded decision problems. We
formalize this through a composite operator $\phi(\cdot, \gamma(\cdot))$ where
$\phi$ drives the main semantic convergence while $\gamma$ resolves local
sub-games. The resulting framework demonstrates that game-theoretic reasoning
emerges naturally from fixed-point iteration rather than being imposed
externally. We prove a Game Theorem establishing existence and uniqueness of
semantic equilibria under realistic cognitive simulation assumptions. Our
verification suite includes adaptations of Banach's fixed-point theorem to
transfinite contexts, a novel $\phi$-topology based on the
Kozlov-Maz'ya-Rossmann formula for handling semantic singularities, and
categorical consistency tests via the Yoneda lemma. The paper itself functions
as a semantic artifact designed to propagate its fixed-point patterns in AI
embedding spaces -- a deliberate instantiation of the "semantic virus" concept
it theorizes. All results are grounded in category theory, information theory,
and realistic AI cognition models, ensuring practical applicability beyond pure
mathematical abstraction.

</details>


### [49] [DocCHA: Towards LLM-Augmented Interactive Online diagnosis System](https://arxiv.org/abs/2507.07870)
*Xinyi Liu,Dachun Sun,Yi R. Fung,Dilek Hakkani-Tür,Tarek Abdelzaher*

Main category: cs.CL

TL;DR: 本文提出DocCHA框架，有效提升了对话健康代理在真实临床对话中的诊断性能与透明性，对多语言及低资源医学场景具有重要价值。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽能力强大，但现有的对话健康代理（CHAs）在实际临床诊断中依然存在静态、脆弱的问题，难以实现自适应多轮推理、症状澄清及透明决策，极大限制了其临床应用价值。

Method: 作者提出了DocCHA，一种具备置信度感知、模块化设计的诊断框架，将诊断过程分解为三阶段：（1）症状发掘；（2）病史采集；（3）因果图构建。每个模块利用可解释的置信分数指导自适应提问、优先症状澄清与推理修正。

Result: 在两个真实中文问诊数据集（IMCS21, DX）上，DocCHA在诊断准确率上较强大的LLM基线（如GPT-3.5、GPT-4o、LLaMA-3）最高提升5.18%，症状召回率提升超30%，对话轮次仅有小幅增长。

Conclusion: DocCHA能够实现结构化、透明且高效的诊疗对话，为多语言、资源有限环境中的可信LLM临床助手奠定基础。

Abstract: Despite the impressive capabilities of Large Language Models (LLMs), existing
Conversational Health Agents (CHAs) remain static and brittle, incapable of
adaptive multi-turn reasoning, symptom clarification, or transparent
decision-making. This hinders their real-world applicability in clinical
diagnosis, where iterative and structured dialogue is essential. We propose
DocCHA, a confidence-aware, modular framework that emulates clinical reasoning
by decomposing the diagnostic process into three stages: (1) symptom
elicitation, (2) history acquisition, and (3) causal graph construction. Each
module uses interpretable confidence scores to guide adaptive questioning,
prioritize informative clarifications, and refine weak reasoning links.
  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),
DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,
GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and
over 30 percent improvement in symptom recall, with only modest increase in
dialogue turns. These results demonstrate the effectiveness of DocCHA in
enabling structured, transparent, and efficient diagnostic conversations --
paving the way for trustworthy LLM-powered clinical assistants in multilingual
and resource-constrained settings.

</details>


### [50] [Automating MD simulations for Proteins using Large language Models: NAMD-Agent](https://arxiv.org/abs/2507.07887)
*Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.CL

TL;DR: 该论文提出一种结合大语言模型和自动化脚本的流程，实现蛋白分子动力学模拟输入文件的高效、自动化生成，大幅提升效率并减少人为错误。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟对于理解蛋白质结构、动力学和功能至关重要，但其输入文件的准备过程繁琐、易出错。该研究旨在解决这一痛点，提高模拟准备的效率和准确性。

Method: 本文提出了一个自动化流程，结合了大型语言模型（以Gemini 2.0 Flash为代表）、Python编程和基于Selenium的网页自动化，自动操作CHARMM GUI以生成NAMD的模拟输入文件。此外，通过Gemini的代码生成与迭代优化能力，自动完成脚本编写、执行与校正，并利用额外软件对输出进一步处理，实现端到端的自动化。

Result: 该自动化流程显著缩短了模拟准备时间，减少了人为错误，并能并行处理多个蛋白系统，展现了良好的可扩展性和自动化水平。

Conclusion: 本研究证明LLM辅助的自动化流程能有效优化分子动力学模拟前的准备工作，为结构生物学计算和模拟自动化提供了新范例，也为未来相关技术的发展铺平了道路。

Abstract: Molecular dynamics simulations are an essential tool in understanding protein
structure, dynamics, and function at the atomic level. However, preparing high
quality input files for MD simulations can be a time consuming and error prone
process. In this work, we introduce an automated pipeline that leverages Large
Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with
python scripting and Selenium based web automation to streamline the generation
of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based
interface for preparing simulation-ready inputs for NAMD. By integrating
Gemini's code generation and iterative refinement capabilities, simulation
scripts are automatically written, executed, and revised to navigate CHARMM
GUI, extract appropriate parameters, and produce the required NAMD input files.
Post processing is performed using additional software to further refine the
simulation outputs, thereby enabling a complete and largely hands free
workflow. Our results demonstrate that this approach reduces setup time,
minimizes manual errors, and offers a scalable solution for handling multiple
protein systems in parallel. This automated framework paves the way for broader
application of LLMs in computational structural biology, offering a robust and
adaptable platform for future developments in simulation automation.

</details>


### [51] [DTECT: Dynamic Topic Explorer & Context Tracker](https://arxiv.org/abs/2507.07910)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: DTECT是一套开源的全流程动态主题分析系统，集成数据处理、模型训练、趋势分析和自然语言交互等功能，大大提升了主题动态分析的易用性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 文本数据量的爆炸性增长，使得发现其随时间演变的主题和趋势变得极具挑战性。现有的动态主题建模技术虽然强大，但往往是零散的流程，缺乏良好的可解释性和用户友好性。

Method: 提出了DTECT系统，这是一个动态主题探索与上下文追踪的端到端系统，集成了数据预处理、多种模型架构、专门的评估指标，以及基于大语言模型（LLM）的自动主题标签、趋势分析、交互式可视化和自然语言聊天界面。

Result: DTECT显著提升了主题模型的可解释性，提供贯穿数据到结果的一体化平台，支持用户用直观、交互的方式探索和理解文本主题动态。

Conclusion: DTECT有效地弥补了现有动态主题模型体系的碎片化和可解释性弱点，极大便利了用户对大规模时间序列文本数据的主题跟踪与理解。

Abstract: The explosive growth of textual data over time presents a significant
challenge in uncovering evolving themes and trends. Existing dynamic topic
modeling techniques, while powerful, often exist in fragmented pipelines that
lack robust support for interpretation and user-friendly exploration. We
introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end
system that bridges the gap between raw textual data and meaningful temporal
insights. DTECT provides a unified workflow that supports data preprocessing,
multiple model architectures, and dedicated evaluation metrics to analyze the
topic quality of temporal topic models. It significantly enhances
interpretability by introducing LLM-driven automatic topic labeling, trend
analysis via temporally salient words, interactive visualizations with
document-level summarization, and a natural language chat interface for
intuitive data querying. By integrating these features into a single, cohesive
platform, DTECT empowers users to more effectively track and understand
thematic dynamics. DTECT is open-source and available at
https://github.com/AdhyaSuman/DTECT.

</details>


### [52] [SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment](https://arxiv.org/abs/2507.07939)
*Guoxin Zang,Xue Li,Donglin Di,Lanshun Nie,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: 该论文提出SAGE框架，结合领域知识注入和专家偏好优化，有效提升VLM在工业异常检测与推理中的表现，获得了显著的实验结果，并发布了相关代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLM）在通用多模态任务中表现良好，但在工业异常检测与推理时，缺乏可解释性和对未见类别的泛化能力，主要由于该任务具有很强的领域特异性，因此限制了VLM在工业场景的实用性。

Method: 提出了SAGE框架，包括自引导事实增强（SFE）和熵感知直接偏好优化（E-DPO）：SFE通过事实提取与融合，将领域知识注入视觉推理，E-DPO则利用熵感知的优化方法使模型输出更符合专家偏好。同时，建立AD-PL偏好优化问答数据集，以及Multiscale Logical Evaluation（MLE）用于量化模型逻辑与一致性。

Result: SAGE框架在零样本和单样本设置下，在工业异常检测数据集上表现出色，优于现有方法。

Conclusion: SAGE通过整合领域知识和专家偏好优化，有效提升了视觉语言模型在工业异常检测场景中的推理能力及泛化性，提供了可解释和结构化的分析方法。

Abstract: While Vision-Language Models (VLMs) have shown promising progress in general
multimodal tasks, they often struggle in industrial anomaly detection and
reasoning, particularly in delivering interpretable explanations and
generalizing to unseen categories. This limitation stems from the inherently
domain-specific nature of anomaly detection, which hinders the applicability of
existing VLMs in industrial scenarios that require precise, structured, and
context-aware analysis. To address these challenges, we propose SAGE, a
VLM-based framework that enhances anomaly reasoning through Self-Guided Fact
Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE
integrates domain-specific knowledge into visual reasoning via fact extraction
and fusion, while E-DPO aligns model outputs with expert preferences using
entropy-aware optimization. Additionally, we introduce AD-PL, a
preference-optimized dataset tailored for industrial anomaly reasoning,
consisting of 28,415 question-answering instances with expert-ranked responses.
To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation
(MLE), a quantitative framework analyzing model logic and consistency. SAGE
demonstrates superior performance on industrial anomaly datasets under
zero-shot and one-shot settings. The code, model and dataset are available at
https://github.com/amoreZgx1n/SAGE.

</details>


### [53] [MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)
*Yu Wang,Xi Chen*

Main category: cs.CL

TL;DR: MIRIX提出了革命性的多结构多模态记忆系统，使AI代理能够更好地个性化、长期记忆，并大幅优于传统系统，推动AI记忆能力迈向更高水平。


<details>
  <summary>Details</summary>
Motivation: 现有的AI代理记忆系统存在重大局限，主要体现在其内存结构扁平、范围狭窄，难以实现持久、抽象和用户个性化的信息记忆与召回。

Method: 提出了MIRIX，这是一种模块化、多智能体记忆系统，包含六种有结构的记忆类型（核心、情节、语义、程序、资源内存和知识库），结合多代理协调机制，实现对多样化长期用户数据的高效保存、推理和精准召回，并支持文本、视觉和多模态信息。

Result: 在ScreenshotVQA多模态基准测试中，MIRIX准确率比RAG基线高35%，同时存储需求下降99.9%；在LOCOMO长对话基准测试中，MIRIX达到85.4%的SOTA准确率，显著超越现有方法。

Conclusion: MIRIX重新定义了AI记忆系统的能力，将多模态、结构化长期记忆引入语言模型代理，显著提升了记忆效果和实际应用价值，并通过应用程序形式展示其实用性和隐私保障。

Abstract: Although memory capabilities of AI agents are gaining increasing attention,
existing solutions remain fundamentally limited. Most rely on flat, narrowly
scoped memory components, constraining their ability to personalize, abstract,
and reliably recall user-specific information over time. To this end, we
introduce MIRIX, a modular, multi-agent memory system that redefines the future
of AI memory by solving the field's most critical challenge: enabling language
models to truly remember. Unlike prior approaches, MIRIX transcends text to
embrace rich visual and multimodal experiences, making memory genuinely useful
in real-world scenarios. MIRIX consists of six distinct, carefully structured
memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and
Knowledge Vault, coupled with a multi-agent framework that dynamically controls
and coordinates updates and retrieval. This design enables agents to persist,
reason over, and accurately retrieve diverse, long-term user data at scale. We
validate MIRIX in two demanding settings. First, on ScreenshotVQA, a
challenging multimodal benchmark comprising nearly 20,000 high-resolution
computer screenshots per sequence, requiring deep contextual understanding and
where no existing memory systems can be applied, MIRIX achieves 35% higher
accuracy than the RAG baseline while reducing storage requirements by 99.9%.
Second, on LOCOMO, a long-form conversation benchmark with single-modal textual
input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing
existing baselines. These results show that MIRIX sets a new performance
standard for memory-augmented LLM agents. To allow users to experience our
memory system, we provide a packaged application powered by MIRIX. It monitors
the screen in real time, builds a personalized memory base, and offers
intuitive visualization and secure local storage to ensure privacy.

</details>


### [54] [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
*Noam Razin,Yong Lin,Jiarui Yao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 内隐奖励模型虽结构上接近外显奖励模型，但更依赖表层线索，泛化能力差，设计时应关注这一细节。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在大语言模型的后训练和推理中至关重要。但近期发现内隐奖励模型（IM-RM）在泛化能力方面明显逊于外显奖励模型（EX-RM），尤其是在分布外情形下，这种差距让人困惑，因为两者结构及训练过程接近，仅计算奖励方式不同。本文旨在揭示导致这一泛化差距的根本原因，加深对不同奖励模型内在偏见的理解。

Method: 理论分析结合实验对比，系统研究IM-RM和EX-RM的泛化性能，并针对一般假设和设计选择进行了实证和反证。重点分析了模型更多依赖表层token信息及相关的分布变化反应。

Result: 主要发现IM-RM更加依赖表层token级线索，导致其在token分布变化（分布外测试）和分布内测试中都泛化能力较差。否定了IM-RM泛化差的主流直观解释（如生成难度假说），证据表明其既可验证也可生成。

Conclusion: 奖励模型的设计中，看似细微的实现差异对泛化性能有显著影响，特别是IM-RM对表征线索依赖更重，应谨慎选择奖励模型结构。

Abstract: Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.

</details>


### [55] [Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology](https://arxiv.org/abs/2507.07983)
*Sabine Felde,Rüdiger Buchkremer,Gamal Chehab,Christian Thielscher,Jörg HW Distler,Matthias Schneider,Jutta G. Richter*

Main category: cs.CL

TL;DR: 小型语言模型结合RAG在风湿病学辅助决策表现优于大型模型，更适合资源有限环境，但还需医生把关。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽然强大但资源消耗高，难以推广到资源有限的医疗环境。研究探索小型模型和RAG能否实现更高性价比的临床辅助。

Method: 通过对比大型语言模型与结合RAG的小型语言模型在风湿病学领域的临床决策支持表现，评估各自的准确率和能耗等指标。

Result: SLMs+RAG在诊断和治疗任务上优于大型模型，能耗更低、成本更优，但离专科医生水准尚有差距，需人工监管。

Conclusion: 小型语言模型（SLMs）结合RAG，在诊断和治疗性能上优于大型模型，且能在有限资源环境下低能耗地本地部署，但专家监管仍然必需。

Abstract: Large language models (LLMs) show promise for supporting clinical
decision-making in complex fields such as rheumatology. Our evaluation shows
that smaller language models (SLMs), combined with retrieval-augmented
generation (RAG), achieve higher diagnostic and therapeutic performance than
larger models, while requiring substantially less energy and enabling
cost-efficient, local deployment. These features are attractive for
resource-limited healthcare. However, expert oversight remains essential, as no
model consistently reached specialist-level accuracy in rheumatology.

</details>


### [56] [Automating Expert-Level Medical Reasoning Evaluation of Large Language Models](https://arxiv.org/abs/2507.07988)
*Shuang Zhou,Wenya Xie,Jiaxi Li,Zaifu Zhan,Meijia Song,Han Yang,Cheyenna Espinoza,Lindsay Welton,Xinnie Mai,Yanwei Jin,Zidu Xu,Yuen-Hei Chung,Yiyun Xing,Meng-Han Tsai,Emma Schaffer,Yucheng Shi,Ninghao Liu,Zirui Liu,Rui Zhang*

Main category: cs.CL

TL;DR: 该论文提出一套严格的医学推理评测基准和新评测框架，发现小模型部分情况下优于大型专有模型，有助于推动LLM在医疗领域的负责任部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在临床决策中应用逐渐增加，但其医学推理的透明性和可靠性评估手段有限，缺乏严格的基准工具。

Method: 提出MedThink-Bench，一个包含500道多领域高难度医学问题和专家逐步推理注释的评测集，并引入LLM-w-Ref评估框架，利用细粒度推理和LLM判官机制，提高推理评测的严谨性与可扩展性。

Result: LLM-w-Ref在医学推理评估中与专家判定高度相关，表现良好。通过基准测试发现小参数模型（如MedGemma-27B）可超越部分大型专有模型（如OpenAI-o3）。

Conclusion: MedThink-Bench为LLM医学推理能力评估提供了高质量基准，有助于推动其在临床实践中的安全、负责任应用。

Abstract: As large language models (LLMs) become increasingly integrated into clinical
decision-making, ensuring transparent and trustworthy reasoning is essential.
However, existing evaluation strategies of LLMs' medical reasoning capability
either suffer from unsatisfactory assessment or poor scalability, and a
rigorous benchmark remains lacking. To address this, we introduce
MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable
assessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging
questions across ten medical domains, each annotated with expert-crafted
step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel
evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge
mechanisms to assess intermediate reasoning with expert-level fidelity while
maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong
positive correlation with expert judgments. Benchmarking twelve
state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can
surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,
MedThink-Bench offers a foundational tool for evaluating LLMs' medical
reasoning, advancing their safe and responsible deployment in clinical
practice.

</details>


### [57] [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
*Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Ming Li,Qilong Wu,Kaipeng Zhang,Chen Wei*

Main category: cs.CL

TL;DR: PyVision让多模态大模型不仅能用工具，还能发明工具，大幅提升视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）在视觉推理方面，依赖于预定义的工作流和静态工具集，限制了模型的灵活性和解释性。该论文旨在打破这一局限，实现更动态、更智能的视觉推理能力。

Method: 提出PyVision框架，使MLLM可以自主生成、执行和优化基于Python的工具，针对具体任务动态定制工具链，并通过一个工具分类体系和多项基准测试进行定量分析。

Result: PyVision在多个视觉推理基准上均显著提升了性能。例如，在V*基准上，GPT-4.1的表现提高了7.8%；在VLMsAreBlind-mini基准上，Claude-4.0-Sonnet表现提升了31.1%。

Conclusion: 动态工具生成赋予MLLM更强的自主性和复杂视觉任务的推理能力，为视觉领域的智能体研究带来了范式转变。

Abstract: LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [58] [The Richness of CSP Non-redundancy](https://arxiv.org/abs/2507.07942)
*Joshua Brakensiek,Venkatesan Guruswami,Bart M. P. Jansen,Victor Lagerkvist,Magnus Wahlström*

Main category: cs.DM

TL;DR: 本文研究了约束满足问题中非冗余性的结构及增长速率，证明了其可达到所有有理阶数，并对二元谓词的条件非冗余性做了系统分类。同时，提出了基于代数的新理论并给出了非典型 Mal'tsev embedding 的新例子，拓展了该领域的理论边界。


<details>
  <summary>Details</summary>
Motivation: 了解约束满足问题（CSP）中冗余子句和非冗余性（non-redundancy, NRD）的本质及其在计算机科学和数学多个重要领域中的核心作用。

Method: 证明存在对任意有理数 r≥1，让某个 CSP 谓词 P 的非冗余性为Θ(n^r)；分类所有二元谓词的条件非冗余性，并利用极值组合学中的高圈图理论进行连接；发展条件非冗余性的代数理论，进一步研究 Mal'tsev embedding 并给出新的例子。

Result: （1）对于每个有理数 r≥1，构造出非冗余度数为Θ(n^r)的 CSP 谓词；（2）对所有二元谓词的条件非冗余性进行了完全分类，并发现其与高圈图结构密切相关；（3）提出条件非冗余性的代数理论，并找到了一个 Mal'tsev embedding 实例，其结构并非来自阿贝尔群，而是量子 Pauli 群。

Conclusion: 论文深化了对 CSP 非冗余性的理解，不仅给出了其增长率的新刻画（覆盖所有有理指数），还建立了与极值组合学和代数学的深层联系，为相关理论与应用发展提供了坚实基础。

Abstract: In the field of constraint satisfaction problems (CSP), a clause is called
redundant if its satisfaction is implied by satisfying all other clauses. An
instance of CSP$(P)$ is called non-redundant if it does not contain any
redundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum
number of clauses in a non-redundant instance of CSP$(P)$, as a function of the
number of variables $n$. Recent progress has shown that non-redundancy is
crucially linked to many other important questions in computer science and
mathematics including sparsification, kernelization, query complexity,
universal algebra, and extremal combinatorics. Given that non-redundancy is a
nexus for many of these important problems, the central goal of this paper is
to more deeply understand non-redundancy.
  Our first main result shows that for every rational number $r \ge 1$, there
exists a finite CSP predicate $P$ such that the non-redundancy of $P$ is
$\Theta(n^r)$. Our second main result explores the concept of conditional
non-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We
completely classify the conditional non-redundancy of all binary predicates
(i.e., constraints on two variables) by connecting these non-redundancy
problems to the structure of high-girth graphs in extremal combinatorics.
  Inspired by these concrete results, we build off the work of Carbonnel [CP
2022] to develop an algebraic theory of conditional non-redundancy. As an
application of this algebraic theory, we revisit the notion of Mal'tsev
embeddings, which is the most general technique known to date for establishing
that a predicate has linear non-redundancy. For example, we provide the first
example of predicate with a Mal'tsev embedding that cannot be attributed to the
structure of an Abelian group, but rather to the structure of the quantum Pauli
group.

</details>
