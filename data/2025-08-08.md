<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.CL](#cs.CL) [Total: 41]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 本文针对微服务“热更新”带来的数据一致性问题，提出了利用语义属性保一致性的算法，解决了传统方法低效或高风险困境，提供了理论和实践可行的新思路。


<details>
  <summary>Details</summary>
Motivation: 微服务架构中在线服务需要在不中断服务的情况下动态更新功能，但这会导致旧版与新版协同工作的混合模式，易造成数据一致性问题。传统方法要么效率低，要么存在严重风险，因此亟需更优的解决方案。

Method: 引入了一个形式化框架，对混合模式更新过程中的一致性进行理论推理，并以此推导出新算法，证明了这些算法的正确性。通过证明缺乏语义感知的方法无法避免不一致，强调了语义属性的重要性。

Result: 提出的算法首次为混合模式下的服务更新提供了一致性保证，理论和算法都具有首创性，并为实际部署安全更新提供理论基础。

Conclusion: 本文提出了首个能够保证混合模式（mixed mode）更新一致性的算法，通过利用服务操作的语义属性（如可交换性），确保服务在更新过程中不会出现一致性问题。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本文系统分析GPT-4o mini为ML项目文件级自动生成日志的能力，发现虽然能较好模仿人工插入位置，但严重过度记录日志，且存在适配、规范等多项挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要聚焦于函数级日志，忽视了文件级特别是机器学习项目中的日志生成。但在ML应用中，完善的文件级日志能显著提升系统可靠性，因此有必要评估LLM在该场景下的表现。

Method: 本研究以GPT-4o mini为案例，评估其为机器学习项目（文件级别）生成日志语句的能力。方法包括：收集171个机器学习代码库（共4,073个Python文件），删除原有日志，通过LLM自动生成日志，再从日志位置、等级、包含变量及文本质量等方面，与人工日志进行比较，并手动分析代表性样本以归纳常见模式与挑战。

Result: LLM（GPT-4o mini）在63.91%情况下能与人工一致地插入日志，但存在高达82.66%的过度日志率。手动分析发现：文件级的日志生成存在于函数开头/结尾过度插入、难以处理大代码块内部插入以及难以适配特定项目日志规范等问题。

Conclusion: 尽管GPT-4o mini展示出生成完整文件日志的潜力，目前其高过度日志率、对日志规范适配难等问题使其在实际应用中仍需改进。

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [3] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 提出一种自动化管道，利用关键帧提取和多模态模型筛选，实现游戏bug视频报告的高效摘要展现，大幅提升QA和开发流程效率。


<details>
  <summary>Details</summary>
Motivation: 现代游戏工作室在高频率发布新版本和补丁时，会生成大量包含游戏录像的视频型bug报告。开发者验证和分流这些报告需手动观看视频，过程繁琐、低效且难以扩展，亟需提升自动化和效率。

Method: 提出了一套自动化处理管道。首先利用FFmpeg提取关键帧，显著减少视频帧数；然后通过多模态视觉-语言模型（GPT-4o）对关键帧与文本bug描述的匹配程度进行排序，自动选取与bug描述最相符的帧。

Result: 在真实游戏视频和JIRA bug报告上验证，管道可将原视频帧数减至1.9%，但98.79%的bug场景仍能被捕捉。Top-1帧的F1达0.79，准确率0.89。对不同类别bug的效果差异明显，其中光影、物理碰撞、UI类bug表现最好，动画和特效类最低。

Conclusion: 本方法用高相关性静态帧替代视频审查，大幅减少人工劳动和提升bug报告的处理效率，具有实际产业应用价值。QA团队和开发者可显著受益。

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [4] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 本文利用社会技术框架，系统分析生成式AI对开源软件社区的多维冲击，呼吁社区采取主动、前瞻性策略应对未来挑战。


<details>
  <summary>Details</summary>
Motivation: 面对生成式AI带来的不确定性与复杂性，现有开源社区缺乏应对的明晰框架，如果不主动作为，可能威胁社群合作基础。

Method: 基于McLuhan四元组的社会技术框架，结合情景分析法，探讨生成式AI对开源软件开发（涵盖实践、文档、社区互动和治理四方面）的冲击。

Result: 揭示了GenAI对OSS各环节的风险与机遇，为社区领导者和研究者提供前瞻性的应对思路和韧性建设建议。

Conclusion: 采用社会技术框架分析，OSS社区在人工智能变革下应主动适应并塑造未来，而非被动反应技术变化。

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [5] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 本文通过分析555例实际注意力机制神经网络故障，提出七类全新注意力相关故障类别及四条诊断启发规则，指出超过一半故障与注意力机制独特实现有关，填补了现有故障分类和诊断指导的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制已被广泛应用于前沿神经网络架构（如ChatGPT等），但现实中出现的失效案例说明，现有深度学习故障分类法难以覆盖注意力机制带来的独特故障类型，给实践者带来了诊断困难。

Method: 作者系统性分析了来自十个框架（GitHub、Hugging Face、Stack Overflow等）中96个项目的555个真实注意力机制神经网络（ABNN）故障，通过归纳总结提出了七类新的注意力特有故障类型；对其根因以及表现症状进行了深入分析，并关联症状和根因，形成诊断启发式规则。

Result: 论文发现，超过一半的ABNN故障源于注意力机制独有的机制，并归纳总结出七类注意力相关故障。此外，结合症状与根因分析，总结出四条可解释33%注意力特有故障的诊断启发，首次为该领域提供系统性诊断指导。

Conclusion: 现有故障分类体系不足以覆盖注意力机制引入的新型故障，本文首次提出详尽的注意力机制特有故障分类与诊断框架，为ABNN的工程实践和后续安全提升提供了重要方法指导。

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [6] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: 该论文探讨了大语言模型与面向对象编程结合的潜力，分析了LLMs在典型OOP编程流程中的关键作用点，并提出增强逻辑推理和代码编写的方案，显示LLMs能提升编程体验，但相关研究仍有待深入。


<details>
  <summary>Details</summary>
Motivation: 现有关于大语言模型（LLMs）在金融、知识图谱、医学和视觉分析中的应用研究蓬勃发展，但与面向对象编程（OOP）的结合尚未充分探讨，理解LLMs如何提升OOP学习和代码编写极为有限。

Method: 从OOP任务相关利益方的角度，包括普通程序员、导师和资深程序员，系统分析典型编程流程中LLMs能带来显著益处的关键节点，并提出增强逻辑推理和代码编写的方法。

Result: 发现LLMs可在OOP编程流程的若干关键环节发挥作用，并提出可行的集成方案，有助于提升编程体验和代码质量。

Conclusion: LLMs有潜力在OOP学习和编写过程中提供支持，对编程经验和结果有积极影响。未来需进一步开发和评估集成AI工具的方法。

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [7] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: 本文针对OpenStack系统变更依赖识别困难的问题，发现超过半数依赖在代码评审阶段才被发现，耗费大量开发者时间。为此提出基于机器学习的半自动依赖识别方法，在预测依赖和筛选依赖对方面均取得良好效果，未来需进一步提升top-k精度。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统日益复杂，准确识别和管理变更之间的依赖关系变得至关重要，这对于防止构建失败和功能部署不全尤为重要，特别是在依赖跨团队和跨组件变化的现代软件系统中。

Method: 作者在OpenStack系统上进行了依赖管理的实证研究，分析了过去10年间变更的依赖关系，发现许多依赖是在代码评审阶段才被识别。随后，提出了一种半自动化方法，利用两个机器学习模型分别预测变更间依赖的可能性和具体依赖对。

Result: 研究发现，OpenStack系统中51.08%的依赖是在代码评审阶段才被识别，存在较大时间延迟。开发者在识别依赖时需花费大量时间和精力。提出的ML模型表现良好，两个模型的平均AUC分别为79.33%和91.89%，Brier分数为0.11和0.014，但top-k precision尚有提升空间。

Conclusion: 依赖识别常常滞后，影响开发效率。通过机器学习手段，有望提高主动依赖识别的能力，缓解开发者的负担，提升CI/CD流程的稳定性与效率。

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [8] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: 本文提出LadyBug工具，结合bug文本和UI复现信息自动定位安卓应用中的bug，相较于纯文本方法显著提升准确率，工具已开源并有效验证其性能。


<details>
  <summary>Details</summary>
Motivation: 在安卓应用的开发中，定位bug通常依赖于开发者手动分析bug报告，这一过程繁琐且容易出错。现有的文本检索方法精度有限，未能充分利用用户实际操作中的UI信息。作者希望通过结合文本与UI交互信息，提升bug定位的准确性和效率。

Method: 提出了LadyBug，一个GitHub机器人，结合安卓应用的bug文本描述与开发者上传的UI复现轨迹，自动检索项目中最可能包含bug的文件，并输出排名列表。该工具通过与GitHub issue tracker集成，为每个bug收集复现轨迹，并利用文本检索结合UI信息改进定位算法。

Result: LadyBug在公开基准RedWing（涵盖39款安卓应用的80个可复现bug报告）上进行评测，与纯文本检索基线相比，LadyBug显著提升了定位准确率。实验证明，结合UI信息有助于更精确地定位bug。

Conclusion: LadyBug显著提升了安卓应用bug自动定位的准确性和效率，验证了UI信息在bug定位中的重要作用，并以开源形式发布，具有实际应用价值和推广前景。

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [9] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 强化学习下，大语言模型代码生成不仅关注结果，更纳入推理过程质量，通过新奖励模型和RL方法，显著提升模型表现并防止奖励劫持，部分任务甚至接近GPT-4-Turbo水平。


<details>
  <summary>Details</summary>
Motivation: 现有利用强化学习进行大语言模型代码生成的方法，主要依赖基于结果的奖励（如通过测试用例），而忽视了推理过程的质量评估，这导致推理质量不能直接引导模型优化，且易受到奖励劫持问题影响。直接监督推理过程容易让模型钻奖励空子，反而不能提升最终结果。

Method: 1）提出了统一的RL框架将推理过程质量纳入奖励。2）提出LCB-RB基准数据集，包含优劣推理过程对比评价；3）提出基于优化-退化（OD-based）训练的奖励模型，系统性生成高质量推理偏好对，覆盖如事实、逻辑、连贯性等多个维度；4）提出Posterior-GRPO（P-GRPO）新型RL算法，仅将推理质量奖励应用于最终结果成功的样本，有效缓解奖励劫持。

Result: 1）利用OD-based方法训练的7B奖励模型在LCB-RB上取得SOTA性能，并能泛化到其它任务；2）采用P-GRPO的7B模型在多类代码生成任务上表现优异，比仅用结果奖励的基线高4.5%，性能接近GPT-4-Turbo；3）方法可拓展至数学任务；4）模型、数据、代码均已公开。

Conclusion: 本文提出一套能够有效结合推理过程质量与最终任务成功的RL训练体系，通过创新奖励模型训练和新的RL方法P-GRPO，显著提升了代码与数学任务生成质量，解决了推理奖励劫持问题。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [10] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: 本文提出一种集成LLM和确定性技术的解决方案，让用户能通过自然语言便捷进行结构化数据建模与模式映射，并在MetaConfigurator工具中实现，有效降低了非专家数据集成和管理的难度。


<details>
  <summary>Details</summary>
Motivation: 许多领域缺乏标准的数据模型，导致非专家很难创建和管理数据结构模型，尤其是在研究数据管理和集成时。模型驱动工程（MDE）虽有优势，但建模门槛高仍是瓶颈。

Method: 提出一种将大语言模型（LLM）与确定性技术相结合的方法，允许用户通过自然语言输入创建、修改和映射JSON Schema。该方法集成于MetaConfigurator开源工具中，支持从异构数据源（JSON、CSV、XML、YAML）自动生成模式映射，并用确定性执行保证可扩展性和可靠性。

Result: 在化学领域的应用示例上展示了本方法的有效性。新方法结合自然语言交互与确定性保障，大幅降低了非专家从事结构化数据建模与数据集成的技术门槛。

Conclusion: 通过将LLM与可控的确定性技术结合，实现了自然语言辅助的结构化数据建模与模版映射，有效服务了非专家用户，促进了数据集成标准化。

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [11] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: 当前模型已在旧基准上表现饱和，SX-Bench通过更复杂、多函数推理任务显著提高区分度，揭示了先进模型在复杂推理中的不足，促进代码智能评估更深入发展。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在代码智能领域进步显著，但系统性评估其代码理解和推理能力仍具有挑战。现有主流基准侧重功能正确性，难以区别高级模型的推理能力，且得分趋于饱和。

Method: 提出了STEPWISE-CODEX-Bench (SX-Bench)基准，聚焦于多函数协作、细粒度执行推理，定义“计算步骤”为最小执行单元，要求模型预测推理任务总执行步数，并引入自动化流程（程序合成、符号执行、LLM验证）保障基准生成与质量。

Result: 在20多种主流模型（包括14个推理增强模型）上进行评价，即使最先进模型OpenAI-O3在高难推理任务上的准确率仅78.37%，远低于以往基准，从而揭示模型在复杂、细粒度推理上的瓶颈。

Conclusion: SX-Bench促使代码评测从单函数验证转向多函数动态推理，为高级代码智能模型的深度评估提供关键工具。

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [12] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph框架为软件系统自动进化和现代化提供了实用解法，综合表现远超现有方法，在安全修复、代码转换、文档维护及算力效率上均取得了显著成果，有望推动智能化、可控的Software 3.0时代。


<details>
  <summary>Details</summary>
Motivation: 应对传统软件现代化过程中遇到的实际问题，比如隐式契约、性能保持和集成进化等难点，提升代码维护和更新的自动化与智能化水平，同时显著降低大模型带来的高算力消耗。

Method: 将软件系统的各种构件抽象为带类型的有向图，通过专用小型语言模型（SLM）驱动的变异算子进行自动更新和修改，并用多目标适应度选优存活改动。进行了安全漏洞修复、跨语言代码转换、文档实时维护等多项实证评测。

Result: EvoGraph在三大基准任务中，自动修复了83%的安全漏洞，COBOL到Java转换实现了93%的功能等效，文档保持实时更新。另外，系统延迟降低了40%，新功能上线时间缩短七倍。跨语言现代化方面，语义等效率达82-96%，算力消耗比大型语言模型低90%。

Conclusion: EvoGraph展现了软件系统能够自动进化自身源代码、流水线、文档等的可行性，并在多项任务上优于现有基线方法，为持续自适应的软件系统（Software 3.0）提供了实际路径。

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [13] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: 本文提出了一个结合物联网与业务流程管理的理论模型和方法论，实现了多维度业务流程的可持续性提升，并通过旅游和医疗行业案例进行了验证，为企业可持续发展提供了系统性思路。


<details>
  <summary>Details</summary>
Motivation: 尽管物联网已经在业务流程管理中应用，并被用于提升环境可持续性，但相关可持续性研究主要聚焦于环境层面，而全面且持久的影响需要超越环境因素，提出系统性方案。

Method: 提出了一个概念模型和结构化方法论，正式表示关键可持续性概念，并连接业务流程管理与物联网，突出物联网设备对可持续性的支持作用。方法论系统地分析现有业务流程，寻找提升机会，并实施面向可持续性的物联网增强业务流程，通过旅游业和医疗领域的示例进行说明。

Result: 展示了物联网如何通过结构化方法系统提升业务流程的多维可持续性能力，并在旅游和医疗案例中得到验证。

Conclusion: 物联网不仅能提升业务流程的实时性和自动化能力，还能通过系统性方法全方位促进业务流程的可持续发展（不仅限于环境层面）。该研究为企业如何利用物联网提升可持续性提供了理论模型和实践路径。

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 提出了一种无需微调、通过音频模型与LLAMA大模型结合，在对话转录中自动标注说话者年龄、性别、情感等属性的高效方法，提升了转录文本的分析价值。


<details>
  <summary>Details</summary>
Motivation: 传统的对话转录后处理侧重提升语法、标点和可读性，但缺乏对说话者特征的元数据标注。元数据标签有助于语音数据的更深层次理解和应用（如用户画像、情感分析等），提高对话内容的分析价值。作者希望补足这一环节并探索无需对已有大模型任务微调的高效实现方案。

Method: 将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型结合，利用轻量级的连接器联通音频和文本表征，实现对说话人属性（如年龄、性别、情绪）的推断和时间变异标签的注释。同时，LLAMA还被用于直接比较说话人向量（x-vectors）。

Result: 在说话人画像任务上，作者提出的方法表现良好，能够有效推断说话者的属性，并展示了在x-vector比较任务上的能力（在部分场景下等错误率为8.8%），同时不影响模型的推理速度和模块化特性。

Conclusion: 作者提出了一种在对话转录后处理中的新方法，即在文本中自动注释说话者特征（如年龄、性别和情感等元数据标签），提升了转录数据的丰富性。通过音频基础模型与LLAMA语言模型的组合，无需对模型进行专门微调就能获得有竞争力的说话人分析效果，同时保持了模块化和高效性。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [15] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 提出Parity-aware BPE优化跨语言分词平等性，显著改善低资源语言表现，整体性能无明显损失。


<details>
  <summary>Details</summary>
Motivation: 标准的分词算法依赖于基于频率的目标，导致主导语言在分词时更有优势，而低资源语言往往分词结果更长或存在许多<UNK>，加剧语言用户间的不平等。

Method: 提出了一种名为Parity-aware BPE的分词算法变体，在每一次合并步骤中，优先最大化当前分词最差语言的压缩增益，以推动跨语言的平等。

Result: Parity-aware BPE能让不同语言的分词数量更公平，对整体压缩率影响很小，在下游任务中的语言模型性能也未见显著下降。

Conclusion: 该方法有效提升了低资源语言的分词平等性，同时保持了大致相同的整体表现和效率。

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [16] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: 通过联合训练自动语音识别和音调重音检测模块，能显著提升识别性能（F1和WER均明显改善），证明了在语音预训练模型中保留或加强韵律特征（如音调重音）对ASR的重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别（ASR）系统在使用半监督语音表示时，通常未充分利用语音中的韵律信息，尤其是音调重音，而这些信息可能对提高识别性能具有潜在作用。

Method: 提出了一个联合ASR和音调重音检测（pitch accent detection）的模型，将音调重音识别模块融合至ASR流程，并在有限资源下进行微调训练。

Result: 音调重音检测模块在该任务上F1分数提升了41%，显著超过了现有方法；ASR系统在联合训练下，字错误率（WER）在LibriSpeech数据集上下降了28.3%。

Conclusion: 引入音调重音检测模块能显著提升半监督语音表示的ASR性能，展示了预训练语音模型保留或重新学习韵律信息的重要性。

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [17] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 该论文提出了LLM人格一致性系统性评估方法，测试了25种主流模型。发现所有模型在不同测量和干预下都表现出显著行为不稳定，现有技术难以保证安全应用所需的可靠、可预测行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在实际部署时需要行为一致性以确保安全，但其类似“人格”的行为特质目前了解甚少。该研究动机是深入评估LLM在人格一致性方面的现状和挑战。

Method: 提出PERSIST（PERsonality Stability in Synthetic Text）评估框架，综合测试了25种以上开源模型（参数规模从1B到671B），覆盖50多万条回答。采用传统的人格测量工具（如BFI-44、SD3）和为LLM定制的新工具，系统性地改变问题顺序、描述方式、角色设定和推理模式来观察结果变化。

Result: 研究发现：即使是超过400B参数的大型模型也存在显著的回答波动（标准差SD大于0.4）；仅仅改变提示顺序，就能使人格测量结果变化达到20%；原本被认为能够稳定模型行为的措施，如链式思考、详细角色设定、包含对话历史，反而可能加剧不稳定性；LLM专用人格工具与传统人类工具一样存在不稳定，显示出问题是架构本身而非测量方法带来的。

Conclusion: 当前LLM在各个参数规模和多种行为增强措施下都表现出持久的人格测量不稳定性，说明现有模型基础不足以支撑真正的行为一致性。在要求行为可预测的安全关键场景中，基于人格的对齐策略很可能无法满足实际需求。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [18] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: RCR-Router通过智能内存选择和角色感知路由，大幅降低了多代理LLM的令牌消耗，同时提升或保持问答质量，并提出了更丰富的评测指标，为系统可扩展性和高效协作奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 多代理大型语言模型（LLM）系统在复杂推理和协作决策任务中表现出强大潜力，但现有的协调方法大多使用静态或全上下文路由，导致过度令牌消耗、冗余记忆暴露以及在多轮交互中适应性差。为解决这些问题，论文提出了一种新的上下文路由框架。

Method: 提出了RCR-Router，这是一种模块化且角色感知的上下文路由框架。它能够根据代理的角色和任务阶段动态选择语义相关的内存子集，并严格控制令牌预算。核心技术包括轻量级评分策略用于记忆选择，代理输出会迭代融合进共享内存以实现上下文渐进优化。同时，为更全面地评估模型行为，提出了Answer Quality Score这一新评价指标来衡量LLM生成解释的质量。

Result: 在HotPotQA、MuSiQue及2WikiMultihop三个多跳问答基准上实验显示，RCR-Router能减少多达30%的令牌消耗，同时答案质量得到提升或维持不变。

Conclusion: RCR-Router不仅提升了多代理LLM系统的效率和适应性，还为模型输出质量评价提供了新的视角，突出了结构化记忆路由和输出感知型评估在可扩展多代理系统中的重要性。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [19] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: 本论文引入基准测试，发现大语言模型在面对语义等价但带有不同语言特征（如回避性表达）的回答时，会对某些语言变体（如回避性语言）打分显著更低，揭示了人口统计偏见，并为AI语言公平性研究提供了基础工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在处理语言时可能无意中暴露出性别、社会阶层或地区背景等人口统计属性，导致自动化决策系统存在偏见。因此，有必要建立一个系统性基准，专门评估LLM在这些“语言口令”（细微语言标记）上是否产生歧视。

Method: 通过精心设计的访谈模拟，创建并使用了100组经过验证的问题-回答对。通过这些对话，生成受控的语言变化，保持语义等价，只在特定语言现象（如回避性语言、地区口音等）上有所区别，从而精准测量模型在自动评测中的人口统计偏见表现。同时，从多个语言维度上验证了方法的有效性。

Result: 实验显示，采用回避性表达（hedging language）的回答平均得分低25.6%。基准能够有效识别出各具体模型的偏见，证明工具在揭示和量化模型语言歧视方向上的作用。

Conclusion: 本工作提出了检测和衡量AI系统语言歧视的基准工具，成为该领域基础性框架，对提高自动化决策的公平性具有广泛应用价值。

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [20] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 提出用动词语义聚类替代传统精确匹配评测，更好地反映动作识别模型的真实性能，并更贴近人的判断。


<details>
  <summary>Details</summary>
Motivation: 在视觉动作识别系统中，由于动词语义和图像解释的模糊性，现有的标准精确匹配评测难以全面评估模型表现。

Method: 提出了一种视觉-语言聚类框架，通过构建动词义簇（verb sense clusters）来更好地反映图像中动作描述的多样性。并在imSitu数据集上分析该方法，进一步将模型评估结果与传统方法进行对比，同时进行人工一致性分析。

Result: 每张图片平均对应2.8个动词义簇，每个义簇代表图像的不同解释视角。聚类评估方法与传统评估方法相比，在与人的判断一致性上表现更好。

Conclusion: 基于动词义簇的评测框架能更全面、细致地评价动作识别模型，更符合人类理解。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [21] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 本文提出多阶段大语言模型，提升了从文本中抽取影响自杀的社会健康因素的准确率与可解释性，有助于更早识别风险人群和改善自杀预防措施。


<details>
  <summary>Details</summary>
Motivation: 了解影响自杀事件的社会健康决定因素（SDoH）对于早期干预和预防至关重要，但在数据驱动方法中面临因素分布长尾、压力诱因分析复杂和模型可解释性有限等挑战。

Method: 提出了一个多阶段大语言模型框架，从非结构化文本中增强SDoH因素的抽取，并与现有的BioBERT、GPT-3.5-turbo、DeepSeek-R1等先进模型进行了对比，同时评估模型解释对标注效率和准确性的提升，包括自动化实验与用户研究。

Result: 新框架在整体SDoH因素抽取和细粒度相关上下文检索任务上表现优异。微调的小型、任务特定模型在降低推理成本的同时，实现了与大型模型相当甚至更优的效果。多阶段设计提升了抽取质量并带来了中间解释，增强了模型可解释性。

Conclusion: 所提出的方法提升了从非结构化文本中抽取与自杀相关SDoH的准确性和透明度，有助于早期发现高风险个体并优化预防策略。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [22] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文针对多轮、多参与者对话的四元组抽取任务，提出以结构熵最小化分割对话并两步抽取情感四元组，有效消除干扰，提升了准确性和计算效率，达到当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 当前对话场景下的面向方面的情感分析（DiaASQ）面临一个挑战，即现有方法通常假设情感元素在整个对话中分布均匀，导致学习过程中引入噪声，而实际上对话常常包含多个语义独立的子对话。如何高效且清晰地分割对话以提升提取质量，是亟待解决的问题。

Method: 本文提出利用结构熵最小化算法对对话进行划分，旨在尽量完整地保留相关发言并区分无关信息。随后，基于两个阶段进行四元组抽取：首先在发言级别提取单个情感元素，然后在子对话层面进行四元组匹配。

Result: 实验证明，该方法在DiaASQ任务上不仅达到了当前最佳的效果，而且显著降低了计算成本。

Conclusion: 对话分割通过结构熵最小化的方法，有效消除无关噪声，结合分阶段抽取与匹配，大幅提升了DiaASQ的性能和效率。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [23] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 本文通过微调四种主流decoder-only大语言模型，并在AMR解析任务上与业界最佳方法对比，发现模型可达相近甚至相同性能，且结构实现更为简单，特别是LLaMA3.2和Phi3.5分别在语义和结构方面表现突出，证明了此类方法的实用价值。


<details>
  <summary>Details</summary>
Motivation: 对AMR解析领域，现有复杂的SOTA方法性能优秀，但方法复杂。该研究动机是探索用结构更简单的decoder-only大型语言模型（LLM）通过微调实现同等性能。

Method: 微调四种不同架构的decoder-only LLMs（Phi 3.5、Gemma 2、LLaMA 3.2和DeepSeek R1 LLaMA Distilled），并利用LDC2020T02 Gold AMR3.0测试集进行全面评估。

Result: 微调后的LLM在AMR解析任务上性能可与复杂SOTA方法媲美。LLaMA 3.2微调后SMATCH F1达到0.804，与APT + Silver (IBM)持平，接近Graphene Smatch (MBSE)的0.854；LLaMA 3.2在语义表现领先，Phi 3.5在结构有效性方面表现更优。

Conclusion: 简单微调decoder-only LLMs可实现与复杂SOTA AMR解析方法相当的性能，尤其是在语义和结构方面各有优势。此途径为AMR解析提供了简单高效的新方法。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [24] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 复杂多头或多适配器结构在多任务LLM微调中并不总是更好。只需增强单一适配器（如提升秩）即可获得强大性能，新提出的Align-LoRA通过对齐任务表征进一步提升效果，验证共享表征才是多任务泛化的关键。


<details>
  <summary>Details</summary>
Motivation: 当前大模型需要适配多任务多领域的数据，主流做法是通过多适配器或多头结构（如LoRA变体）实现结构多样性以捕获任务特定知识。但这种范式的有效性存在疑问。

Method: 首先比较了高相似度多头结构与复杂多适配器/多头系统的表现，随后探索单适配器LoRA（提升秩rank）在多任务环境下的效果，并提出Align-LoRA，通过一个专门的损失函数在共享适配器空间内对齐不同任务的表示。

Result: 高相似度多头简单结构优于复杂多适配器/多头系统，单适配器LoRA提升秩后也表现优异。Align-LoRA进一步显著超过所有基线模型，验证了新的范式。

Conclusion: 针对多任务LLM微调，复杂任务特定结构不必然带来提升，关键在于学习共享表征。Align-LoRA通过对齐任务表征极大提升了多任务泛化能力，为多任务LLM适配提供了更简洁高效的新思路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [25] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: 本文提出MultiCheck多模态事实核查框架，通过专用编码器和融合模块实现细粒度文本与图像推理，并采用对比学习提升语义对齐。在Factify 2数据集上效果明显优于基线，展现了多模态推理在虚假信息核查中的潜力与实用性。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息（同时包含文本和图像）快速增长，传统基于文本的事实核查系统难以应对，因此亟需新的方法来提升对多模态信息的核查能力。

Method: 提出了一种统一的细粒度多模态事实核查框架MultiCheck。该框架通过专门的文本和图像编码器以及一个融合模块进行跨模态关系建模，并通过对比学习目标来增强语义对齐能力，利用分类头判断真假。

Result: 在Factify 2数据集上，MultiCheck方法达到了加权F1分数0.84，显著超过了基线方法。

Conclusion: 显式多模态推理能够显著提升虚假信息核查效果，所提方法具备可扩展性与可解释性，适用于复杂真实场景。

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [26] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出BEE-RAG，通过均衡上下文熵，重构注意力机制，提升RAG在长上下文下的性能，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）发展，检索增强生成（RAG）用于弥补LLMs知识局限。但RAG需处理大量长上下文，导致熵增大与注意力稀释，显著影响性能。

Method: 提出BEE-RAG框架，利用熵不变原理，均衡上下文熵，重构注意力机制，使其对上下文长度不敏感，并结合零样本多重要性评估与高效参数自适应微调机制优化平衡因子。

Result: 在多项RAG任务上，BEE-RAG展现出卓越效果，提高了系统对上下文长度变化的适应性和整体性能。

Conclusion: BEE-RAG通过熵不变性有效缓解长上下文带来的性能下降，提升了RAG的适用性和效果。

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [27] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: 本文发现大型语言模型天然关注输入序列的首尾信息，忽略中间内容。据此，提出了AttnRank方法，通过重新排序输入，将重点信息放在高关注区域，显著提升模型在多个任务和模型上的表现，且无需训练或改动模型参数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的表现对输入信息的上下文位置非常敏感，但现有方法对这一位置偏差的机制了解不足。本文旨在深入研究这一机制，并提出提升模型性能的新方法。

Method: 通过大量实验观察注意力分布现象，提出"Attention Basin"（注意力盆地）；基于分析结果，设计了Attention-Driven Reranking（AttnRank）框架，通过校准模型注意力偏好，然后重新排序输入序列，使关键内容分配到高注意力位置。

Result: AttnRank作为一种训练无关、可即插即用的方法，显著提升了10种不同架构和规模的大型语言模型在多跳问答和few-shot学习任务中的表现，无需修改模型参数或训练流程。

Conclusion: 模型对输入序列的首尾项目给予更高关注，AttnRank通过优化信息排列，将重要内容放在高注意力区，实现大幅性能提升。该方法高效灵活，有广泛应用前景。

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [28] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 本研究提出系统评估医学大模型伦理推理的新基准PrinciplismQA，发现模型在实际伦理应用上有显著不足，并提出诊断与改进路径。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域大模型面临伦理推理评估不足的问题，现有基准测试未能系统覆盖医学伦理。

Method: 构建了PrinciplismQA基准，包括3648个问题，涵盖多项选择题和开放性问题，内容源自权威教科书和医学伦理案例文献，并由专家验证。

Result: 实验发现大模型在医学伦理知识与实际应用之间存在显著差距，尤其在动态应用伦理原则于实际场景时表现不佳，对善行原则困惑，强调其他伦理原则。前沿闭源模型表现最佳，领域微调可提升模型伦理能力，但需要更好地与医学伦理知识对齐。

Conclusion: PrinciplismQA基准可系统性诊断AI模型医疗伦理弱点，有助于推动更平衡、更负责任的医疗AI发展。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [29] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: 论文提出多种方法检测问答系统中的幻觉文本片段，结合上下文和提示工程并微调LLM，在多语种任务中表现优异，强调上下文整合和模型微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推动了自然语言生成的进步，但在问答系统中仍容易出现生成幻觉（即错误或误导性内容），因此需要探索有效的检测和缓解方法。

Method: 采用带外部上下文和不带外部上下文的方法，包括LLM少样本提示、LLM的微调及token级分类，部分方法基于合成数据训练。

Result: 该团队的方法在西班牙语上取得了最佳成绩，在英语和德语上也具有竞争力。

Conclusion: 集成相关上下文和微调模型以及提示工程有助于减少问答系统中的幻觉现象。

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [30] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 本文针对资源有限环境，提出MulCoT-RD模型，通过多级知识蒸馏实现高效的多模态情感推理和分类，兼顾性能与可解释性，显著优于现有轻量方案，在四个数据集上取得良好表现。


<details>
  <summary>Details</summary>
Motivation: 社交平台上的多模态内容激增推动了多模态情感分析（MSA）领域的发展，但现有方法多依赖参数量大的大型语言模型（LLMs），对资源有限环境下的自主多模态情感推理生成关注不足。

Method: 本文提出资源受限联合多模态情感推理与分类任务（JMSRC），即在轻量级模型下同时进行多模态情感推理链生成与情感分类。具体提出了MulCoT-RD模型，采用"教师-助理-学生"知识蒸馏范式：首先利用高性能多模态大模型生成初始推理数据集，训练中型助理模型（多任务学习），再训练轻量学生模型实现高效的推理和分类。

Result: 在四组数据集上大量实验表明，参数仅3B的MulCoT-RD模型在JMSRC任务上具备强性能，并展现出良好的泛化能力和可解释性提升。

Conclusion: MulCoT-RD提出了一种适合资源受限环境下的高效多模态情感推理与分类方案，通过多级知识蒸馏使轻量模型兼具性能、泛化能力和解释性，适合实际部署。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [31] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出一种新颖的LLM结构性剪枝方法，通过识别与保留模型中的功能网络及关键神经元，实现更高效、更少性能损失的剪枝。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）结构性剪枝方法通常仅评估结构单元的重要性，忽略了人工神经元之间的交互和协作，这导致剪枝后模型功能结构被破坏，影响性能。该文希望缓解这一挑战。

Method: 将LLM视为数字大脑，借鉴脑功能神经网络的思想，将LLM分解为功能网络，并通过保留这些功能网络中的关键神经元来进行剪枝。

Result: 实验表明，该方法能够成功识别并定位LLMs中的功能网络及其关键神经元，从而实现高效的模型剪枝。

Conclusion: 基于功能网络的剪枝能够更好地保持LLM的宏观功能结构，剪枝效率与性能优于传统方法。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [32] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
*Sijie Wang,Quanjiang Guo,Kai Zhao,Yawei Zhang,Xin Li,Xiang Li,Siqi Li,Rui She,Shangshu Yu,Wee Peng Tay*

Main category: cs.CL

TL;DR: CodeBoost是一种创新的代码大模型后训练框架，不依赖人工指令，仅通过大量代码片段和丰富的训练目标、奖励机制，显著提升模型性能，实现更高效的自动化微调。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型的指令微调通常依赖人工标注的高质量指令，收集这些数据既耗时又难以扩展，而代码片段却很丰富，因此寻求无需人工指令即可提升模型的微调方法。

Method: 提出CodeBoost，一种完全基于代码片段进行后训练的框架，不依赖人工标注。其核心包括最大团筛选（选取代表性和多样性的训练语料）、双向预测（模型可做前向和反向预测任务）、错误感知预测（纳入正确和错误输出信号）、异质增强（丰富训练分布）、异质奖励（多维奖励，引入格式正确性和执行反馈）。

Result: CodeBoost在多种代码大模型和基准上进行了广泛实验，结果显示该方法在各项指标上都能持续提升表现。

Conclusion: CodeBoost为代码大模型提供了一种高效、可扩展的后训练方案，通过代码片段本身显著改善模型能力，无需依赖费力的人工收集指令。

Abstract: Code large language models (LLMs) have become indispensable tools for
building efficient and automated coding pipelines. Existing models are
typically post-trained using reinforcement learning (RL) from general-purpose
LLMs using "human instruction-final answer" pairs, where the instructions are
usually from manual annotations. However, collecting high-quality coding
instructions is both labor-intensive and difficult to scale. On the other hand,
code snippets are abundantly available from various sources. This imbalance
presents a major bottleneck in instruction-based post-training. We propose
CodeBoost, a post-training framework that enhances code LLMs purely from code
snippets, without relying on human-annotated instructions. CodeBoost introduces
the following key components: (1) maximum-clique curation, which selects a
representative and diverse training corpus from code; (2) bi-directional
prediction, which enables the model to learn from both forward and backward
prediction objectives; (3) error-aware prediction, which incorporates learning
signals from both correct and incorrect outputs; (4) heterogeneous
augmentation, which diversifies the training distribution to enrich code
semantics; and (5) heterogeneous rewarding, which guides model learning through
multiple reward types including format correctness and execution feedback from
both successes and failures. Extensive experiments across several code LLMs and
benchmarks verify that CodeBoost consistently improves performance,
demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [33] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
*Dongxu Zhang,Ning Yang,Jihua Zhu,Jinnan Yang,Miao Xin,Baoliang Tian*

Main category: cs.CL

TL;DR: 本文系统评估了大模型推理链中不同阶段错误的影响，发现最后阶段的错误更危险，并提出ASCoT方法通过自适应识别和修正高风险环节，大幅提升了推理准确率。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought（CoT）提示极大提升了大语言模型（LLM）的推理能力，但这些推理链的可靠性仍然存在重大挑战。尤其是早期广泛认为推理链开头的错误最具破坏性，即“级联失败”假设。本文的动机在于系统性验证这一假设并探索实际中的关键脆弱点。

Method: 本文通过系统性的错误注入实验，验证推理链各个阶段错误对最终答案的影响，发现并提出了“后期脆弱性（Late-Stage Fragility）”现象。为解决这一关键问题，作者提出了ASCoT（Adaptive Self-Correction Chain-of-Thought）方法。ASCoT方法包括两个模块：自适应验证管理器（AVM，利用位置影响评分函数I(k)，有针对性地识别高风险的后期推理环节）和多视角自我纠正引擎（MSCE，对识别出的关键错误步骤实施鲁棒的双路径纠正）。

Result: 通过在GSM8K、MATH等基准测试上的大量实验，ASCoT方法准确率显著优于包括标准CoT在内的多种强基线模型。

Conclusion: 本文质疑并证伪了“级联失败”假设，证明推理链中后期错误比早期更具破坏性，并针对性提出了ASCoT方法，有效缓解了后期脆弱性，提升了LLM的推理可靠性。研究强调了有必要从均匀验证机制转向具备自适应和脆弱性感知的纠错机制。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning
capabilities of Large Language Models (LLMs), yet the reliability of these
reasoning chains remains a critical challenge. A widely held "cascading
failure" hypothesis suggests that errors are most detrimental when they occur
early in the reasoning process. This paper challenges that assumption through
systematic error-injection experiments, revealing a counter-intuitive
phenomenon we term "Late-Stage Fragility": errors introduced in the later
stages of a CoT chain are significantly more likely to corrupt the final answer
than identical errors made at the beginning. To address this specific
vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought
(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive
Verification Manager (AVM) operates first, followed by the Multi-Perspective
Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score
function I(k) that assigns different weights based on the position within the
reasoning chains, addressing the Late-Stage Fragility issue by identifying and
prioritizing high-risk, late-stage steps. Once these critical steps are
identified, the MSCE applies robust, dual-path correction specifically to the
failure parts. Extensive experiments on benchmarks such as GSM8K and MATH
demonstrate that ASCoT achieves outstanding accuracy, outperforming strong
baselines, including standard CoT. Our work underscores the importance of
diagnosing specific failure modes in LLM reasoning and advocates for a shift
from uniform verification strategies to adaptive, vulnerability-aware
correction mechanisms.

</details>


### [34] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
*Sukannya Purkayastha,Nils Dycke,Anne Lauscher,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本工作提出用大语言模型生成高质量合成对话数据训练元审稿辅助对话智能体，优于通用大模型，并在实际流程中提升评审效率。


<details>
  <summary>Details</summary>
Motivation: 元审稿是同行评审流程中的关键环节，直接决定论文的录用与否。目前相关研究主要将其视为对评审报告的摘要生成问题，但实际上元审稿也是一个需要在多种意见之间进行权衡的决策过程。因此，提升元审稿阶段辅助工具的智能性具有重要意义。

Method: 作者提出利用大型语言模型（LLMs）通过自我优化策略生成合成对话数据，从而缓解训练对话代理助手时面临的数据稀缺问题。进一步基于合成数据训练专用于元审稿场景的对话智能体，并与现有通用LLM助手进行性能对比。最后在实际元审稿流程中测试该系统有效性。

Result: 基于LLM和自我优化策略生成的合成对话数据在质量上优于传统方法，并能有效支持元审稿助手的训练。所训练的对话智能体在元审稿任务上明显优于市面上主流LLM助手，并且在真实元审稿流程中提升了决策效率。

Conclusion: 研究表明，基于自我优化生成的高质量合成数据可有效助力元审稿对话智能体的训练，专用智能体在元审稿场景下优于通用模型，并能提升实际元审稿效率。代码与数据已公开。

Abstract: Meta-reviewing is a pivotal stage in the peer-review process, serving as the
final step in determining whether a paper is recommended for acceptance. Prior
research on meta-reviewing has treated this as a summarization problem over
review reports. However, complementary to this perspective, meta-reviewing is a
decision-making process that requires weighing reviewer arguments and placing
them within a broader context. Prior research has demonstrated that
decision-makers can be effectively assisted in such scenarios via dialogue
agents. In line with this framing, we explore the practical challenges for
realizing dialog agents that can effectively assist meta-reviewers. Concretely,
we first address the issue of data scarcity for training dialogue agents by
generating synthetic data using Large Language Models (LLMs) based on a
self-refinement strategy to improve the relevance of these dialogues to expert
domains. Our experiments demonstrate that this method produces higher-quality
synthetic data and can serve as a valuable resource towards training
meta-reviewing assistants. Subsequently, we utilize this data to train dialogue
agents tailored for meta-reviewing and find that these agents outperform
\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our
agents in real-world meta-reviewing scenarios and confirm their effectiveness
in enhancing the efficiency of meta-reviewing.\footnote{Code and Data:
https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [35] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
*Nikita Dragunov,Temurbek Rahmatullaev,Elizaveta Goncharova,Andrey Kuznetsov,Anton Razzhigaev*

Main category: cs.CL

TL;DR: SONAR-LLM结合了语义抽象和传统语言建模训练，实现高质量文本生成，并且支持代码和模型的公开复现。


<details>
  <summary>Details</summary>
Motivation: 现有LCM模型通过句子级嵌入生成文本，但其训练方法（均方误差或扩散目标）在一定程度上影响了生成效率和语义捕捉。作者希望既保留LCM的抽象能力，又简化训练流程并恢复概率建模。

Method: 提出SONAR-LLM，一种decoder-only transformer，使用token级交叉熵损失，通过冻结的SONAR解码器进行训练。该方法兼顾了语义抽象和似然性训练信号，不再依赖扩散采样器。

Result: SONAR-LLM在39M到1.3B参数规模下，生成质量具有竞争力，并公开了全部代码和预训练模型以促进复现和后续研究。

Conclusion: SONAR-LLM成功融合了连续SONAR嵌入空间的语义抽象和传统语言模型的训练方法，并在多个参数规模下展现出竞争力的生成质量。

Abstract: The recently proposed Large Concept Model (LCM) generates text by predicting
a sequence of sentence-level embeddings and training with either mean-squared
error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer
that "thinks" in the same continuous SONAR embedding space, yet is supervised
through token-level cross-entropy propagated via the frozen SONAR decoder. This
hybrid objective retains the semantic abstraction of LCM while eliminating its
diffusion sampler and restoring a likelihood-based training signal. Across
model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive
generation quality. We report scaling trends, ablations, benchmark results, and
release the complete training code and all pretrained checkpoints to foster
reproducibility and future research.

</details>


### [36] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: 本文提出了一种新方法CGRS，用于动态抑制反思触发，从而缓解大模型推理过程中的过度思考问题。该技术无需改动模型、训练，可直接集成至现有流程，有效减少令牌消耗（最高降至原来的41.9%），保持推理精度，在多个主流模型和规模下均获得最优的性能与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 大规模推理语言模型（LRLMs）在长链思考和复杂反思机制上取得了性能提升，但引入特定触发词的反思容易导致“过度思考”问题，即冗余的推理步骤造成令牌数量增加、推理成本升高，以及实用价值降低。如何减轻LRLM的过度思考，同时保持推理准确性，是当前亟需解决的难题。

Method: 提出了Certainty-Guided Reflection Suppression（CGRS）方法，根据模型对当前答案的信心动态抑制反思触发词的生成，从而避免冗余的反思循环。该方法无需重新训练或修改模型架构，可无缝集成到现有的自回归生成流程，且具备模型无关性。

Result: 在四个推理基准（AIME24、AMC23、MATH500、GPQA-D）上的实验表明，CGRS能有效减少18.5%到41.9%的令牌使用量，并保持准确率，同时在长度压缩和性能方面优于现有最先进方法。该效果在各种模型架构（如DeepSeek、QwQ、Qwen3）和规模（4B至32B参数）下均得以验证。

Conclusion: CGRS方法显著减轻了大型推理语言模型的过度思考问题，优化了推理效率和成本，并保持了高准确性，具备良好的工程实用性。

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [37] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 仅靠可调节选项不能改善Hololens 2手语虚拟人在专业用户中的理解度与体验，关键在于动画质量和基本手语元素的完善。主张优化动画和界面，推动用户参与设计。


<details>
  <summary>Details</summary>
Motivation: 希望通过引入可调节功能，提升微软Hololens 2设备中手语虚拟人（SL avatar）在德国手语（DGS）用户中的可用性、体验和可接受性。

Method: 让德国手语专家用户分别与可调节和不可调节版本的手语虚拟人互动，对比这两类系统在可理解性、用户体验和可接受性上的表现，并详细分析交互过程。

Result: 虽然用户普遍偏好有可调节设置，但在用户体验与可理解性方面并未带来显著提升，整体水平均较低。主要原因包括缺少嘴型、面部表情，以及手势不清等实现问题。此外，可调节版让用户压力更大、操作负担和挫折感增加，乐趣性评价高于实用性。调整手势的直观性和学习曲线也受质疑。系统可接受性高度依赖于易用性和动画质量。

Conclusion: 个人化和可调节功能本身远不足以提升手语虚拟人的可用性，首要保证基本手语内容和表现的清晰易懂。建议强化嘴部与面部动画，优化交互界面，并采用用户参与式设计。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


### [38] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
*Samy Ateia,Udo Kruschwitz*

Main category: cs.CL

TL;DR: 本研究探索了大语言模型在专业生物医学检索场景下通过自反馈机制迭代优化答案的可行性。不同模型和任务下自反馈表现不一，为LLM与人类专家结合优化检索系统提供了前沿思考。


<details>
  <summary>Details</summary>
Motivation: 现有的Agentic RAG和深度研究系统在专业领域（如生物医学检索）应用时存在挑战，因为自动化会降低用户参与度，并可能无法满足专家的信息需求。专业检索任务通常需要高水平的用户专业知识和过程透明性。因此，亟需探索如何使LLM在保证专业性和透明度的同时提升自主检索与修正能力。

Method: 以BioASQ CLEF 2025挑战（含专家提出的问题）为实验平台，选用多种推理/非推理LLM（如Gemini-Flash 2.0, o3-mini, o4-mini, DeepSeek-R1），应用自反馈机制：让LLM生成答案后自我评价并修正，实现查询扩展及多种答案类型（是/否、事实、列表、理想答案）；评估自反馈机制对性能的提升及不同模型生成有用反馈的能力。

Result: 初步实验结果显示，自反馈策略在不同模型和任务类型上的表现各异，尚未呈现统一优势。

Conclusion: 本研究揭示了自反馈机制在LLM中的适用性和局限性，对未来通过对比LLM与专家反馈提升专业检索系统信息质量及透明度的研究提供了方向和参考。

Abstract: Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim
to enable autonomous search processes where Large Language Models (LLMs)
iteratively refine outputs. However, applying these systems to domain-specific
professional search, such as biomedical research, presents challenges, as
automated systems may reduce user involvement and misalign with expert
information needs. Professional search tasks often demand high levels of user
expertise and transparency. The BioASQ CLEF 2025 challenge, using
expert-formulated questions, can serve as a platform to study these issues. We
explored the performance of current reasoning and nonreasoning LLMs like
Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our
methodology was a self-feedback mechanism where LLMs generated, evaluated, and
then refined their outputs for query expansion and for multiple answer types
(yes/no, factoid, list, ideal). We investigated whether this iterative
self-correction improves performance and if reasoning models are more capable
of generating useful feedback. Preliminary results indicate varied performance
for the self-feedback strategy across models and tasks. This work offers
insights into LLM self-correction and informs future work on comparing the
effectiveness of LLM-generated feedback with direct human expert input in these
search systems.

</details>


### [39] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
*Eleftherios Avramidis,Vera Czehmann,Fabian Deckert,Lorenz Hufe,Aljoscha Lipski,Yuni Amaloa Quintero Villalobos,Tae Kwon Rhee,Mengqian Shi,Lennart Stölting,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文采集整理了覆盖12种手语的大规模视频-字幕平行语料库，显著增强了德语和拉美手语的数据资源，为手语研究与跨模态任务提供了宝贵数据支持。


<details>
  <summary>Details</summary>
Motivation: 手语数据资源一直相对稀缺，尤其是多种手语的平行语料库。现有的某些手语数据集规模有限，且部分语言没有大规模平行数据，限制了手语相关研究的发展。

Method: 研究团队从不同的在线公开资源（如新闻、政府和教育频道）采集了12种手语的视频材料，并与该国家的主要口语字幕进行配对。制作流程包括数据采集、与内容创作者沟通与获批、网页内容抓取（scraping）和视频剪辑（cropping）等多个步骤。之后对数据进行了统计分析，并详细介绍了整个采集和处理流程。

Result: 最终得到了包含12种手语、约1300小时、4381个视频及130万行字幕（共1400万词）的多语手语平行语料库。大幅扩充了德语手语语料库（是之前数据集的10倍）及首次系统性覆盖了8种拉美手语。

Conclusion: 论文为手语和口语的相互研究提供了大规模且多样化的数据基础，极大丰富了手语资源，并为跨语言、跨模态学习和相关应用奠定了基础。

Abstract: We present a collection of parallel corpora of 12 sign languages in video
format, together with subtitles in the dominant spoken languages of the
corresponding countries. The entire collection includes more than 1,300 hours
in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.
Most notably, it includes the first consistent parallel corpora for 8 Latin
American sign languages, whereas the size of the German Sign Language corpora
is ten times the size of the previously available corpora. The collection was
created by collecting and processing videos of multiple sign languages from
various online sources, mainly broadcast material of news shows, governmental
bodies and educational channels. The preparation involved several stages,
including data collection, informing the content creators and seeking usage
approvals, scraping, and cropping. The paper provides statistics on the
collection and an overview of the methods used to collect the data.

</details>


### [40] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 本文针对现有大型语言模型文化偏见问题，提出了专注于马来西亚文化的MyCulture基准，利用创新的开放式问答格式，系统评估各类模型的文化理解能力，并发现现有模型在文化包容性上存在较大差距，呼吁加强相关基准的设计与应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于训练数据多以英文和中文等高资源语言为主，导致存在明显的文化偏见，尤其在低资源语言环境下，对多元文化内容的表达和评估存在困难。

Method: 提出了MyCulture基准数据集，专为马来西亚文化设计，涵盖艺术、服饰、习俗、娱乐、饮食和宗教六大方面，并以马来语呈现。采用创新的开放式多选问题形式，避免预设选项，从而降低猜测概率和格式偏差。同时对该开放式结构的理论有效性进行了论证，并通过对结构化和自由输出、以及多语种提示词差异，分析模型的结构和语言偏差。

Result: 针对多种区域性和国际大型语言模型的评估发现，在文化理解能力上存在显著差异，反映出当前模型在文化与语言多样性方面的不足。

Conclusion: 亟需针对多元文化和语言包容性，开发和采用更具文化代表性和包容性的基准，用以指导和评估大型语言模型的发展。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [41] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
*Ming Zhang,Yujiong Shen,Jingyi Deng,Yuhui Wang,Yue Zhang,Junzhe Wang,Shichun Liu,Shihan Dou,Huayu Sha,Qiyuan Peng,Changhao Jiang,Jingqi Tong,Yilong Wu,Zhihao Zhang,Mingqi Wu,Zhiheng Xi,Mingxu Chai,Tao Liang,Zhihui Fei,Zhen Wang,Mingyang Wan,Guojun Ma,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出了LLMEval-3动态评测框架，有效解决了静态评测易污染和过拟合问题，通过动态抽取题库、自动化反作弊和LLM裁判等技术，长期测评验证了其评测能力，为大模型评估树立了新的可信标准。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型评测依赖静态测试集，容易受到数据污染和排行榜过拟合的影响，难以真实反映模型能力。

Method: 提出了LLMEval-3动态评测框架，基于22万道研究生级问题库，每次评测动态抽取全新测试集。通过自动化流程实现数据抗污染、反作弊机制，并利用校准后的LLM裁判方法（与专家有90%一致性）和相对排名系统进行公正比较。

Result: 对近50个主流模型进行了20个月的长期测评，揭示出知识记忆的性能上限和静态评测无法发现的数据污染问题。该框架在排名稳定性和一致性方面表现出色。

Conclusion: LLMEval-3为评估大模型真实能力提供了稳健、可信的方法，推动了更可靠的评测标准发展。

Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is
vulnerable to data contamination and leaderboard overfitting, critical issues
that obscure true model capabilities. To address this, we introduce LLMEval-3,
a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary
bank of 220k graduate-level questions, from which it dynamically samples unseen
test sets for each evaluation run. Its automated pipeline ensures integrity via
contamination-resistant data curation, a novel anti-cheating architecture, and
a calibrated LLM-as-a-judge process achieving 90% agreement with human experts,
complemented by a relative ranking system for fair comparison. An 20-month
longitudinal study of nearly 50 leading models reveals a performance ceiling on
knowledge memorization and exposes data contamination vulnerabilities
undetectable by static benchmarks. The framework demonstrates exceptional
robustness in ranking stability and consistency, providing strong empirical
validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and
credible methodology for assessing the true capabilities of LLMs beyond
leaderboard scores, promoting the development of more trustworthy evaluation
standards.

</details>


### [42] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
*Chenzhuo Zhao,Xinda Wang,Yue Huang,Junting Lu,Ziqian Liu*

Main category: cs.CL

TL;DR: 论文提出TASE基准，用于系统评测LLMs在token级理解与结构化推理上的表现。涵盖多语言、多任务，评估大量顶尖模型，结果显示LLMs在此类任务远逊于人类，TASE为未来模型改进提供了有力分析工具与数据支持。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在高层语义任务上表现优异，但在细粒度的、基于token的理解和结构化推理任务中存在不足。然而，这些能力对于需要精确性与可控性的应用非常关键。论文旨在揭示LLMs在token级理解上的短板，并推动其改进。

Method: 提出了TASE基准，涵盖10项任务（包括token感知和结构理解），支持中、英、韩三种语言。数据集包含35,927个样本，且具备可扩展的合成数据生成管道。评测对象包括30多个主流商用与开源LLMs，并通过GRPO方法训练了自定义模型Qwen2.5-14B。

Result: 实验结果显示，人类在token级推理任务上的表现远超现有LLMs，证明了后者在此类任务上的持续性弱点。TASE有助于揭示和诊断这些瓶颈。

Conclusion: TASE为评估和分析LLMs在低层次语言理解与跨语言泛化方面的能力提供了重要工具，将促进模型在token级推理上的进步。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
on high-level semantic tasks, they often struggle with fine-grained,
token-level understanding and structural reasoning--capabilities that are
essential for applications requiring precision and control. We introduce TASE,
a comprehensive benchmark designed to evaluate LLMs' ability to perceive and
reason about token-level information across languages. TASE covers 10 tasks
under two core categories: token awareness and structural understanding,
spanning Chinese, English, and Korean, with a 35,927-instance evaluation set
and a scalable synthetic data generation pipeline for training. Tasks include
character counting, token alignment, syntactic structure parsing, and length
constraint satisfaction. We evaluate over 30 leading commercial and open-source
LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a
custom Qwen2.5-14B model using the GRPO training method. Results show that
human performance significantly outpaces current LLMs, revealing persistent
weaknesses in token-level reasoning. TASE sheds light on these limitations and
provides a new diagnostic lens for future improvements in low-level language
understanding and cross-lingual generalization. Our code and dataset are
publicly available at https://github.com/cyzcz/Tase .

</details>


### [43] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
*Li-Chun Lu,Miri Liu,Pin-Chun Lu,Yufei Tian,Shao-Hua Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文系统评估了主流创造力测量指标，发现它们各有局限且缺乏一致性，呼吁建立更全面、与人类标准一致的创造力评价框架。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）生成内容的创造力评价指标众多，但它们各自关注的方面不同，缺乏一致性和全面性。作者希望系统评估这些主流指标在不同创意领域的表现及局限性。

Method: 作者系统性地检视、分析和比较了代表性的创造力测量方法，包括creativity index、perplexity、句法模板分析，以及“LLM-as-a-Judge”（让LLM作为评估者）。他们在创意写作、非常规问题解决和研究构思等多个领域开展实验分析。

Result: 不同指标之间表现一致性有限，各自只反映了创造力不同的维度。creativity index主要反映词汇多样性，perplexity易受模型置信度影响，句法模板无法捕捉概念性创造性，而LLM-as-a-Judge结果不稳定且存在偏见。整体上，这些指标都存在明显的局限性。

Conclusion: 现有的创造力评价指标无法全面捕捉大语言模型生成内容的创造力，亟需开发更健壮、能泛化且与人类判断更一致的评价框架。

Abstract: We systematically examine, analyze, and compare representative creativity
measures--creativity index, perplexity, syntactic templates, and
LLM-as-a-Judge--across diverse creative domains, including creative writing,
unconventional problem-solving, and research ideation. Our analyses reveal that
these metrics exhibit limited consistency, capturing different dimensions of
creativity. We highlight key limitations, including the creativity index's
focus on lexical diversity, perplexity's sensitivity to model confidence, and
syntactic templates' inability to capture conceptual creativity. Additionally,
LLM-as-a-Judge shows instability and bias. Our findings underscore the need for
more robust, generalizable evaluation frameworks that better align with human
judgments of creativity.

</details>


### [44] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出逻辑增强生成（LAG），通过问题分解和逻辑推理，有效提升LLM复杂任务能力，减少幻觉，比传统RAG更稳健，实验验证效果优越。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）虽然能力强，但在知识密集型任务上容易出现幻觉，特别是遇到需要专业知识的问题时。现有的检索增强生成（RAG）方法虽然能集成外部知识，但在复杂推理场景下表现有限，因为高度依赖语义检索且缺乏结构化逻辑组织。本文受笛卡尔方法论启发，旨在提出一种更系统和逻辑化的知识增强方案。

Method: 本文提出了逻辑增强生成（LAG）方法。LAG包括系统性问题分解和依赖感知推理两步：首先将复杂问题分解为原子的子问题，并根据逻辑依赖有序排列；随后逐步解决每个子问题，并利用前一问的答案指导下一步上下文检索，确保逻辑链渐进。为防止错误累积，LAG还包括逻辑终止机制，遇到无法回答的子问题时终止推理。最后，综合所有子问题的答案，生成经过验证的最终回复。

Result: 在四个基准数据集上的实验结果表明，LAG方法显著提升了推理的鲁棒性，减少了幻觉现象，并使LLM的问题解决过程更接近人类认知逻辑。

Conclusion: LAG为现有RAG系统提供了一种更具原理性和结构化的替代方案，通过系统的逻辑分解和依赖推理，有效提升了大语言模型在复杂知识密集型任务中的表现，并减少错误传播和资源浪费。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [45] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 论文通过‘20问游戏’用模型自我提问方式，发现主流大模型在推断全球不同地区实体存在明显文化、地域偏见，发达地区优于发展中地区，语言差异影响不大。标准测评方法难以发现这些隐性偏见，作者开放了用于分析的Geo20Q+数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型已经针对显性偏见进行了多次调优，但由于训练数据的局限，隐性偏见仍然普遍存在。标准的人为问答检测可能会触发模型的防护机制，难以揭示这些隐性偏见，因此作者尝试通过“模型自我提问”方式，探索模型隐藏的地域与文化偏见。

Method: 采用20问游戏（multi-turn deduction task）作为测试场景，设计了Geo20Q+数据集，涵盖了全球各地知名人物及文化标志物。实验含七种语言和不同玩法（20问/不限回合），系统评价各主流LLM在推理、提问中的地域表现。并分析表现与维基浏览量和预训练语料频率的相关性。

Result: LLM在推断全球北方（发达地区）和西方（欧美）实体时显著优于全球南方和东方（发展中或非西方）地区。语言切换对这种差异影响甚微，相关性分析发现维基流量和语料频率只能部分解释这一现象。

Conclusion: 通过模型自我提问及多步推理游戏，本文揭示了当前主流LLM在地理和文化层面的隐性偏见。标准提示方法难以察觉这些差异。释放了Geo20Q+数据集供相关研究使用。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [46] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
*Santosh T. Y. S. S,Youssef Tarek Elkhayat,Oana Ichim,Pranav Shetty,Dongsheng Wang,Zhiqiang Ma,Armineh Nourbakhsh,Xiaomo Liu*

Main category: cs.CL

TL;DR: 作者提出了CoCoLex，一种用置信度引导的拷贝解码新方法，有效提升了法律文本生成的忠实性，实验证明其优于已有方法，特别适用长文本情境。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在法律领域能够处理长且复杂的上下文，但易产生不真实或幻觉内容，这限制了其实际应用。尽管检索增强生成（RAG）可以一定程度上缓解此问题，但不能保证对外部知识的有效整合。

Method: 提出了一种新的解码策略：基于信心得分引导的拷贝式解码（CoCoLex），该方法结合模型生成的词分布与从上下文中复制生成的词分布，动态插值以增强上下文对生成内容的影响，并以模型置信度为依据鼓励直接复制上下文内容。

Result: 在五个法律数据基准上进行了实验，CoCoLex优于现有上下文感知解码方法，尤其在长文本生成任务中表现更佳。

Conclusion: CoCoLex解码策略能够更有效地结合模型知识与上下文输入，提高法律类文本生成的真实度和忠实度，对提升LLM在法律领域的应用具有积极意义。

Abstract: Due to their ability to process long and complex contexts, LLMs can offer key
benefits to the Legal domain, but their adoption has been hindered by their
tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While
Retrieval-Augmented Generation offers a promising solution by grounding
generations in external knowledge, it offers no guarantee that the provided
context will be effectively integrated. To address this, context-aware decoding
strategies have been proposed to amplify the influence of relevant context, but
they usually do not explicitly enforce faithfulness to the context. In this
work, we introduce Confidence-guided Copy-based Decoding for Legal Text
Generation (CoCoLex)-a decoding strategy that dynamically interpolates the
model produced vocabulary distribution with a distribution derived based on
copying from the context. CoCoLex encourages direct copying based on the
model's confidence, ensuring greater fidelity to the source. Experimental
results on five legal benchmarks demonstrate that CoCoLex outperforms existing
context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [47] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 作者提出了一种基于采样频率的不确定性评估方法，适合黑盒LLM在多选题中，性能优于传统logit方法，并能有效控制风险，为实际高风险场景的可靠应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多项选择题（MCQA）中取得了显著进步，但由于幻觉与过度自信等内在不可靠性限制了它们在高风险领域的应用。

Method: 提出一种基于频率的、不依赖于模型内部信息的不确定性量化方法，在黑盒环境下，借助保形预测（conformal prediction, CP）确保预测置信区间的理论覆盖率。具体地，对每个输入进行多次独立采样，将出现频率最高的答案作为参考值，并据此计算预测熵（PE）。

Result: 实验覆盖六种LLM及四个数据集，频率型PE在区分正确与错误预测（以AUROC衡量）上优于基于logit的PE。此外，该方法有效控制在用户设定风险水平下的经验错误覆盖率，验证了采样频率可替代logit概率实现黑盒不确定性量化。

Conclusion: 本研究提出了一个无需模型内部信息的、通用的MCQA不确定性量化框架，为实际应用中提升LLM可靠性和可信度提供了新思路。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [48] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
*Franziska Weeber,Tanise Ceron,Sebastian Padó*

Main category: cs.CL

TL;DR: 多语种大语言模型在西方语言中政治意见表现高度一致，跨语言未见显著分歧，立场对齐也会在各语言中同步影响。这意味着想要实现细粒度的社会、文化、政治多样性适配仍面临巨大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有调查显示不同社会文化背景下公众的政治观点存在跨文化差异，但尚无证据表明这些差异会转化为多语种大语言模型中的跨语言差异。论文旨在探究MLLMs在不同语言间政治意见的传递性，以及模型是否表现出语言特有的政治观点。

Method: 作者通过对五种西方语言的MLLMs进行测试，利用政治意见相关提示（如投票建议应用中的政治声明），让模型表达其（不）同意程度。同时，采用直接偏好优化（DPO），仅用英文数据对模型进行左右派立场的对齐，然后比较对齐前后的模型在不同语言间的表现。

Result: 未对齐的模型在不同语言间反映的政治意见几乎没有显著差异；而进行政治立场对齐后，模型在所有五种语言中的意见变化几乎一致。

Conclusion: 在西方语言环境下，MLLMs的政治观点可以在语言间传递，不易实现针对具体语言、文化和政治的细致对齐。

Abstract: Public opinion surveys show cross-cultural differences in political opinions
between socio-cultural contexts. However, there is no clear evidence whether
these differences translate to cross-lingual differences in multilingual large
language models (MLLMs). We analyze whether opinions transfer between languages
or whether there are separate opinions for each language in MLLMs of various
sizes across five Western languages. We evaluate MLLMs' opinions by prompting
them to report their (dis)agreement with political statements from voting
advice applications. To better understand the interaction between languages in
the models, we evaluate them both before and after aligning them with more left
or right views using direct preference optimization and English alignment data
only. Our findings reveal that unaligned models show only very few significant
cross-lingual differences in the political opinions they reflect. The political
alignment shifts opinions almost uniformly across all five languages. We
conclude that in Western language contexts, political opinions transfer between
languages, demonstrating the challenges in achieving explicit socio-linguistic,
cultural, and political alignment of MLLMs.

</details>


### [49] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
*Shaoxiong Zhan,Yanlin Lai,Ziyu Lu,Dahua Lin,Ziqing Yang,Fei Tang*

Main category: cs.CL

TL;DR: MathSmith是一种面向高难度数学题目的自动合成框架，通过采样原始数学概念、软约束提高难度、强化学习优化结构与答案一致性，并能定向生成弱点变体。在广泛基准下显著优于同类方法，展示了合成高难度数据推动LLM数学推理的新途径。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学推理取得了很大进步，但进展受限于高质量、高难度训练数据的稀缺。现有方法多依赖对人工编写模板进行变换，导致题目多样性和规模受限。

Method: 提出MathSmith框架：从头合成数学题目，而不是修改已有题目。具体做法为：从PlanetMath随机采样概念-解释对实现数据独立性；设计九种预设策略作为软约束提升题目难度；采用强化学习联合优化题目的结构合理性、推理复杂度和答案一致性；利用自回归推理长度度量认知复杂性，引导合成更需长链思维的问题。还设计基于弱点的变体生成模块，实现在特定概念上的定向提升。

Result: 在五个基准测试（易&中等难度：GSM8K、MATH-500；高难度：AIME2024、AIME2025、OlympiadBench）上，MathSmith在短长链推理设置下均优于现有方法。其弱点变体机制可针对性提升特定概念表现，展现出较强的可扩展性、泛化性和迁移性。

Conclusion: 高难度合成数学数据可显著提升大语言模型的推理能力，MathSmith框架在挑战性题目合成和模型表现提升方面有显著潜力。

Abstract: Large language models have achieved substantial progress in mathematical
reasoning, yet their advancement is limited by the scarcity of high-quality,
high-difficulty training data. Existing synthesis methods largely rely on
transforming human-written templates, limiting both diversity and scalability.
We propose MathSmith, a novel framework for synthesizing challenging
mathematical problems to enhance LLM reasoning. Rather than modifying existing
problems, MathSmith constructs new ones from scratch by randomly sampling
concept-explanation pairs from PlanetMath, ensuring data independence and
avoiding contamination. To increase difficulty, we design nine predefined
strategies as soft constraints during rationales. We further adopts
reinforcement learning to jointly optimize structural validity, reasoning
complexity, and answer consistency. The length of the reasoning trace generated
under autoregressive prompting is used to reflect cognitive complexity,
encouraging the creation of more demanding problems aligned with
long-chain-of-thought reasoning. Experiments across five benchmarks,
categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,
OlympiadBench), show that MathSmith consistently outperforms existing baselines
under both short and long CoT settings. Additionally, a weakness-focused
variant generation module enables targeted improvement on specific concepts.
Overall, MathSmith exhibits strong scalability, generalization, and
transferability, highlighting the promise of high-difficulty synthetic data in
advancing LLM reasoning capabilities.

</details>


### [50] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 针对规则与模型奖励各自不足，本文提出联合优化的RL框架Cooper，有效减缓reward hacking，并通过创新奖励模型提升大语言模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有奖励范式（模型型和规则型）各有短板：规则型鲁棒性差，模型型易遭reward hacking。因此需创新机制既提升鲁棒性，又减少被利用漏洞。

Method: 提出了Cooper框架——联合优化策略模型与奖励模型，通过混合注释策略和基于参考答案的奖励建模，动态选取正负样本对继续训练奖励模型，并用该奖励模型（VerifyRM）指导RL过程。

Result: 新奖励模型VerifyRM在VerifyBench上超越同体积模型，Cooper显著减缓reward hacking并提升RL终端性能（如Qwen2.5-1.5B-Instruct平均准确率提升0.54%）。

Conclusion: 动态更新奖励模型能有效缓解Reinforcement Learning（RL）中的reward hacking问题，提升LLM推理能力，且新框架Cooper兼具性能和稳健性。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [51] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 本文提出OmniEAR框架，首次系统评估大语言模型在具身任务中的推理能力。结果显示语言模型在具身推理领域存在显著性能瓶颈，尤其在工具应用和多代理协作上。微调虽能提升单代理任务表现，但无法有效改善多代理任务，凸显现有架构的根本不足。OmniEAR将作为推动具身AI发展的新基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在抽象推理方面表现出色，但在具身代理推理上的能力尚未被深入探索。因此，本文旨在评估语言模型对物理交互、工具使用及多代理协调任务的推理能力。

Method: 提出了OmniEAR框架，覆盖家庭和工业领域的1500种场景，通过文本化环境表示方式，模拟连续物理属性和复杂空间关系。与现有基准不同，OmniEAR要求代理动态学习能力并自主确定协调策略，根据任务需求，不提供预设工具或显式协作指令。

Result: 系统评估显示，当语言模型需从约束条件出发进行推理时，性能大幅下降：明确指令下任务成功率85-96%，工具推理为56-85%，隐性协作为63-85%，复合任务失败率超过50%。且即使有完整环境信息，模型在协调任务上表现更差，表明模型难以筛选任务相关约束。微调后单代理任务成功率提升显著（0.6%到76.3%），但多代理任务提升有限（1.5%到5.5%）。

Conclusion: 具身推理对现有语言模型提出了根本性挑战，OmniEAR成为评测和推动具身AI系统的重要基准。代码和数据将在论文接收后开源。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [52] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
*Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas Oğuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau Yih*

Main category: cs.CL

TL;DR: 提出融合事实性、细节和相关性的新奖励函数，通过在线RL训练R-LLM，在六个基准集上大幅减少幻觉提升细节，且无损有用性。


<details>
  <summary>Details</summary>
Motivation: R-LLMs在复杂推理任务上表现优异，但在长文本事实性任务上比非推理模型产生更多虚假内容（幻觉），主要因为缺乏有效的事实性验证方法，导致难以应用强化学习优化事实性。直接用自动评分作奖励可能引发奖励作弊。

Method: 提出一种新颖的奖励函数，综合考虑事实精确性、回答细节丰富度和答案相关性，并基于此应用在线强化学习优化模型事实推理能力。

Result: 在六个长文本事实性基准测试中，提出的模型平均减少23.1个百分点的幻觉率，回答细节提升23%，整体有用性未受影响。

Conclusion: 新奖励函数和在线RL结合能显著提升R-LLM长文本事实推理的真实性和细节丰富度，有效抑制奖励作弊，实现高质量推理输出。

Abstract: Reasoning Large Language Models (R-LLMs) have significantly advanced complex
reasoning tasks but often struggle with factuality, generating substantially
more hallucinations than their non-reasoning counterparts on long-form
factuality benchmarks. However, extending online Reinforcement Learning (RL), a
key component in recent R-LLM advancements, to the long-form factuality setting
poses several unique challenges due to the lack of reliable verification
methods. Previous work has utilized automatic factuality evaluation frameworks
such as FActScore to curate preference data in the offline RL setting, yet we
find that directly leveraging such methods as the reward in online RL leads to
reward hacking in multiple ways, such as producing less detailed or relevant
responses. We propose a novel reward function that simultaneously considers the
factual precision, response detail level, and answer relevance, and applies
online RL to learn high quality factual reasoning. Evaluated on six long-form
factuality benchmarks, our factual reasoning model achieves an average
reduction of 23.1 percentage points in hallucination rate, a 23% increase in
answer detail level, and no degradation in the overall response helpfulness.

</details>


### [53] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本研究将探测器方法应用于大语言模型的多轮对话说服行为分析，发现探测器能高效并准确地捕捉说服动态，部分场景下优于提示法，为研究模型其他复杂行为提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）已展示出说服人类的能力，但目前我们对这一动态过程的理解还很有限。近期的一些研究利用线性探测器（linear probes）分析模型表现的不同技能，这为进一步探究说服机理提供了启发。本文正是受此启发，旨在用探测器分析模型在多轮自然对话中的说服能力。

Method: 借助认知科学的见解，本文训练针对说服成功、被说服者性格和说服策略等不同维度的探测器，对多轮对话中的说服行为进行分析，并将探测器与基于提示的方法在不同任务上进行了对比。

Result: 结果表明，尽管探测器方法相对简单，但能够有效地捕捉到说服行为的多个关键方面。例如，探测器可以定位被说服者在对话中被说服的具体时刻，也能总结整个数据集上说服活动的高发阶段。同时，在分析说服策略等任务中，探测器的效果与甚至优于提示法，且具有更高的计算效率。

Conclusion: 探测器不仅为理解复杂对话行为如说服、欺骗和操控等提供了新的高效工具，尤其适用于多轮对话和大规模数据分析场景，也为未来相关研究指明了可行方向。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [54] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: H-NET++通过创新性的分层动态分块方法解决了字节级建模在形态丰富语言中的计算难题，不仅提升了多项性能指标，还做到形态结构高度一致，为相关语言提供高效的无分词器建模方案。


<details>
  <summary>Details</summary>
Motivation: 字节级语言模型可以避免使用脆弱的分词器，但在形态丰富的语言（MRLs）中，因单词跨越多个字节而导致计算难题。该研究希望解决在MRLs中字节级建模效率低下的问题。

Method: 提出H-NET++模型，通过端到端学习获得基于语言学的信息分块。创新包括：轻量级Transformer跨块注意力模块、两层潜在超先验以提升文档一致性、针对拼写特性的特殊处理（如波斯语ZWNJ）、分阶段课程训练。

Result: 在1.4B波斯语语料库上，H-NET++实现了多项指标突破：相比BPE-GPT-2-fa压缩率提升12%、ParsGLUE任务提升5.4个百分点、对ZWNJ扰动鲁棒性提升53%、形态边界检测F1达73.8%。

Conclusion: 分层动态分块建模在无需显式监督的情况下，与波斯语形态结构高度一致，同时保持高效计算，实现了MRLs的无分词器建模新范式。

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [55] [Aircraft routing: periodicity and complexity](https://arxiv.org/abs/2508.05532)
*Frédéric Meunier,Axel Parmentier,Nour ElHouda Tellache*

Main category: cs.DM

TL;DR: 本论文系统分析了飞机调度中周期性实例与解之间的关系，指出过强的周期性要求可能导致无解。证明了只要维护间隔不超过4天，总有周期性解可行。同时，首次证明非周期性调度问题也是NP-困难的，并给出某些特殊情况的多项式解法。


<details>
  <summary>Details</summary>
Motivation: 飞机调度问题是运筹学中应用最广泛的问题之一，牵涉到如何为航班分配飞机并确保飞机定期返回维护基地。长期以来，文献中普遍假设当航班每天相同时（周期性实例），必然需要获取周期性解法，但这一假设缺乏系统讨论。此外，已有NP-困难性结论仅针对周期性实例，非周期性实例缺少相关理论基础。

Method: 论文第一部分通过理论分析和证明，系统探讨了周期性实例与周期性解之间的关系，并着重分析了在规则维护周期不超过4天的情况下，是否总能获得“强周期性”解。第二部分通过复杂性理论方法，首次将NP-困难性的结论推广到非周期性调度问题，并分析了某些特殊情形下问题可在多项式时间内求解。

Result: 研究证明：只要每隔四天或更短时间需定期维护，就一定存在可以实现全部航班安排的强周期性解。并且，首次严格证明了非周期性飞机调度问题也是NP-困难的，补充了现有文献空白。某些特殊情况下，该问题可以在多项式时间内求解。

Conclusion: 周期性航班实例条件下，不必加强周期解的唯一性限制，否则可能消除可行解；针对非周期性实例的NP-困难性也得到证明。这些发现对飞机调度理论和实际优化应用都有重要意义。

Abstract: The aircraft routing problem is one of the most studied problems of
operations research applied to aircraft management. It involves assigning
flights to aircraft while ensuring regular visits to maintenance bases. This
paper examines two aspects of the problem.
  First, we explore the relationship between periodic instances, where flights
are the same every day, and periodic solutions. The literature has implicitly
assumed-without discussion-that periodic instances necessitate periodic
solutions, and even periodic solutions in a stronger form, where every two
airplanes perform either the exact same cyclic sequence of flights, or
completely disjoint cyclic sequences. However, enforcing such periodicity may
eliminate feasible solutions. We prove that, when regular maintenance is
required at most every four days, there always exist periodic solutions of this
form.
  Second, we consider the computational hardness of the problem. Even if many
papers in this area refer to the NP-hardness of the aircraft routing problem,
such a result is only available in the literature for periodic instances. We
establish its NP-hardness for a non-periodic version. Polynomiality of a
special but natural case is also proven.

</details>
