<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 49]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code](https://arxiv.org/abs/2508.08322)
*Muhammad Haseeb*

Main category: cs.SE

TL;DR: 本文提出多智能体、检索增强的上下文工程工作流，显著改善了代码助手在复杂项目中的表现，超越现有主流系统。


<details>
  <summary>Details</summary>
Motivation: 当前大模型自动化代码生成在多文件复杂项目中因上下文和知识瓶颈表现不佳，亟需更好地管理和注入项目相关知识及意图，提升代码助手落地能力和智能化水平。

Method: 集成了四大AI组件：Intent Translator澄清用户意图、Elicit语义检索注入领域知识、NotebookLM文档融合提升语境理解、Claude Code多智能体系统进行代码生成与验证。整个流程通过Claude agent framework编排，采用检索增强生成与代理角色分解。

Result: 多智能体结合语境注入与代理角色分工，使代码助手在项目适应性和成功率上超过CodePlan、MASAI、HyperAgent等最新框架，单轮复合成功率明显提高。

Conclusion: 多智能体系统结合上下文工程显著提升了代码助手在实际多文件项目中的准确性和可靠性，优于单智能体方法。针对Next.js大型代码库定性实验显示其能高效规划、编辑和测试复杂功能，实现最小化的人为干预。

Abstract: Large Language Models (LLMs) have shown promise in automating code generation
and software engineering tasks, yet they often struggle with complex,
multi-file projects due to context limitations and knowledge gaps. We propose a
novel context engineering workflow that combines multiple AI components: an
Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered
semantic literature retrieval for injecting domain knowledge, NotebookLM-based
document synthesis for contextual understanding, and a Claude Code multi-agent
system for code generation and validation. Our integrated approach leverages
intent clarification, retrieval-augmented generation, and specialized
sub-agents orchestrated via Claude's agent framework. We demonstrate that this
method significantly improves the accuracy and reliability of code assistants
in real-world repositories, yielding higher single-shot success rates and
better adherence to project context than baseline single-agent approaches.
Qualitative results on a large Next.js codebase show the multi-agent system
effectively plans, edits, and tests complex features with minimal human
intervention. We compare our system with recent frameworks like CodePlan,
MASAI, and HyperAgent, highlighting how targeted context injection and agent
role decomposition lead to state-of-the-art performance. Finally, we discuss
the implications for deploying LLM-based coding assistants in production, along
with lessons learned on context management and future research directions.

</details>


### [2] [Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming](https://arxiv.org/abs/2508.08332)
*Humza Ashraf,Syed Muhammad Danish,Aris Leivadeas,Yazan Otoum,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 大型语言模型代码生成正确率最高，但小型开源模型在正确时更节能，52%以上问题中能耗不高于大模型。因此，小模型在特定场景下有望成为环保高效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在代码生成领域应用广泛，但其高算力需求导致能源消耗大和碳排放高，引发了环保关注。因此，研究小型开源语言模型（SLMs）在代码生成能力和能效方面的表现，有助于寻找更环保的替代方案。

Method: 评估三种开源小模型（StableCode-3B、StarCoderBase-3B、Qwen2.5-Coder-3B-Instruct）与两种商业大模型（GPT-4.0、DeepSeek-Reasoner）的代码生成表现。以150道LeetCode题（难度均分）作为测试集，生成代码后，基于运行时间、内存占用、能耗及正确性四项指标与人类代码基线进行比较分析。

Result: 大型模型在全部难度级别上正确率最佳，但小模型在输出正确时更节能。在52%以上的题目中，小模型消耗的能源与大模型相同或更少。

Conclusion: 小型语言模型在保证代码正确输出的前提下，有潜力实现更高的能效，为低碳计算提供了可行路径，但在正确性方面尚有劣势，需要进一步优化。

Abstract: Large Language Models (LLMs) are widely used for code generation. However,
commercial models like ChatGPT require significant computing power, which leads
to high energy use and carbon emissions. This has raised concerns about their
environmental impact. In this study, we evaluate open-source Small Language
Models (SLMs) trained explicitly for code generation and compare their
performance and energy efficiency against large LLMs and efficient
human-written Python code. The goal is to investigate whether SLMs can match
the performance of LLMs on certain types of programming problems while
producing more energy-efficient code. We evaluate 150 coding problems from
LeetCode, evenly distributed across three difficulty levels: easy, medium, and
hard. Our comparison includes three small open-source models, StableCode-3B,
StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial
models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using
four key metrics: run-time, memory usage, energy consumption, and correctness.
We use human-written solutions as a baseline to assess the quality and
efficiency of the model-generated code. Results indicate that LLMs achieve the
highest correctness across all difficulty levels, but SLMs are often more
energy-efficient when their outputs are correct. In over 52% of the evaluated
problems, SLMs consumed the same or less energy than LLMs.

</details>


### [3] [Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization](https://arxiv.org/abs/2508.08342)
*Maximilian Jungwirth,Martin Gruber,Gordon Fraser*

Main category: cs.SE

TL;DR: 本研究提出利用历史数据预测PR构建结果，动态优化合并顺序，从而提升大型项目自动化合并管道效率，突破构建系统依赖壁垒，在真实项目中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 在大型单体软件仓库中，集成更改对于功能上线速度、代码仓库稳定性以及团队效率至关重要。然而，合并流水线通常负载过高，成为开发瓶颈。现有优化方法依赖特定构建系统，局限性较大。因此，提出一种通用且高效的合并优化方法具有现实需求。

Method: 提出通过利用历史构建数据、PR元数据与上下文信息，对PR在合并流水线中的构建成功概率进行预测。根据预测结果动态优先安排高可能成功的PR在高峰期先进行合并，以此提升整体吞吐量。该方法不依赖具体构建系统，方便集成到现有自动化合并流程中。

Result: 在真实的大规模项目上实验，结果表明该预测排序方法显著优于传统的FIFO和非学习型排序策略，在高峰期能有效提高吞吐量且容易部署。

Conclusion: 通过基于预测动态优化PR合并顺序的方法，可以极大提升大型代码库的合并流程效率且具有广泛适用性。

Abstract: Integrating changes into large monolithic software repositories is a critical
step in modern software development that substantially impacts the speed of
feature delivery, the stability of the codebase, and the overall productivity
of development teams. To ensure the stability of the main branch, many
organizations use merge pipelines that test software versions before the
changes are permanently integrated. However, the load on merge pipelines is
often so high that they become bottlenecks, despite the use of parallelization.
Existing optimizations frequently rely on specific build systems, limiting
their generalizability and applicability. In this paper we propose to optimize
the order of PRs in merge pipelines using practical build predictions utilizing
only historical build data, PR metadata, and contextual information to estimate
the likelihood of successful builds in the merge pipeline. By dynamically
prioritizing likely passing PRs during peak hours, this approach maximizes
throughput when it matters most. Experiments conducted on a real-world,
large-scale project demonstrate that predictive ordering significantly
outperforms traditional first-in-first-out (FIFO), as well as
non-learning-based ordering strategies. Unlike alternative optimizations, this
approach is agnostic to the underlying build system and thus easily integrable
into existing automated merge pipelines.

</details>


### [4] [OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval](https://arxiv.org/abs/2508.08545)
*Youssef Esseddiq Ouatiti,Mohammed Sayagh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该文提出了一种新型日志级别预测框架OmniLLP，通过基于代码语义和开发者所有权聚合采样上下文，大幅提升了LLM预测日志级别的准确性，为日志维护与系统可观测性提供更优支持。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的日志级别预测工具在选择上下文样例时采用随机策略，忽略了软件项目中的结构和多样化日志实践，导致预测准确率受限。提升日志级别预测的上下文相关性，有助于开发者更有效地进行维护和调试。

Method: 提出OmniLLP框架，通过基于源码的语义相似性和开发者所有权聚合将源文件分组，并仅从这些有聚合意义的分组中检索上下文学习样例，为LLM提供更连贯的提示，从而提升日志级别预测的准确性。

Result: 实验表明，无论是仅使用语义分组还是所有权分组，都能显著提高LLM预测日志级别的准确性（AUC提升可达8%），结合两者的方案在多个项目上实现了0.88至0.96的AUC，远超基于随机样例的策略。

Conclusion: 将软件工程中的语义信息和开发者所有权信号引入LLM-日志级别预测，能有效提升预测准确率，为开发者提供更具上下文相关性的日志辅助工具，增强系统的可维护性与可观测性。

Abstract: Developers insert logging statements in source code to capture relevant
runtime information essential for maintenance and debugging activities. Log
level choice is an integral, yet tricky part of the logging activity as it
controls log verbosity and therefore influences systems' observability and
performance. Recent advances in ML-based log level prediction have leveraged
large language models (LLMs) to propose log level predictors (LLPs) that
demonstrated promising performance improvements (AUC between 0.64 and 0.8).
Nevertheless, current LLM-based LLPs rely on randomly selected in-context
examples, overlooking the structure and the diverse logging practices within
modern software projects. In this paper, we propose OmniLLP, a novel LLP
enhancement framework that clusters source files based on (1) semantic
similarity reflecting the code's functional purpose, and (2) developer
ownership cohesion. By retrieving in-context learning examples exclusively from
these semantic and ownership aware clusters, we aim to provide more coherent
prompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.
Our results show that both semantic and ownership-aware clusterings
statistically significantly improve the accuracy (by up to 8\% AUC) of the
evaluated LLM-based LLPs compared to random predictors (i.e., leveraging
randomly selected in-context examples from the whole project). Additionally,
our approach that combines the semantic and ownership signal for in-context
prediction achieves an impressive 0.88 to 0.96 AUC across our evaluated
projects. Our findings highlight the value of integrating software
engineering-specific context, such as code semantic and developer ownership
signals into LLM-LLPs, offering developers a more accurate, contextually-aware
approach to logging and therefore, enhancing system maintainability and
observability.

</details>


### [5] [Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics](https://arxiv.org/abs/2508.08661)
*Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文首次全面分析代码变更到自然语言生成任务中的幻觉问题，发现现有模型生成内容幻觉比例高，通过多指标组合，尤其模型置信度与特征归因可显著提升自动检测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在代码生成等软件工程任务中表现出强大能力，但幻觉问题严重，尤其是在结构复杂、依赖上下文的代码变更任务中（如提交信息生成和代码评审评论生成），相关研究较少。

Method: 系统分析最新语言模型在两项代码变更到自然语言生成任务（提交信息生成和代码评审评论生成）中的幻觉现象。评估并比较多种自动化检测度量方法，探究多指标组合对检测性能的提升。重点分析模型置信度和特征归因指标在推理阶段的检测应用价值。

Result: 约50%的生成代码审查评论和20%的生成提交信息存在幻觉。单一常用度量检测能力有限，多种度量指标结合后检测效果显著提升。模型置信度和特征归因指标在幻觉检测中表现突出。

Conclusion: 提交信息生成和代码审查评论生成任务中幻觉现象普遍，多指标组合尤其是置信度和特征归因指标可有效用于幻觉自动检测，对推理阶段检测具有实际价值。

Abstract: Language models have shown strong capabilities across a wide range of tasks
in software engineering, such as code generation, yet they suffer from
hallucinations. While hallucinations have been studied independently in natural
language and code generation, their occurrence in tasks involving code changes
which have a structurally complex and context-dependent format of code remains
largely unexplored. This paper presents the first comprehensive analysis of
hallucinations in two critical tasks involving code change to natural language
generation: commit message generation and code review comment generation. We
quantify the prevalence of hallucinations in recent language models and explore
a range of metric-based approaches to automatically detect them. Our findings
reveal that approximately 50\% of generated code reviews and 20\% of generated
commit messages contain hallucinations. Whilst commonly used metrics are weak
detectors on their own, combining multiple metrics substantially improves
performance. Notably, model confidence and feature attribution metrics
effectively contribute to hallucination detection, showing promise for
inference-time detection.\footnote{All code and data will be released upon
acceptance.

</details>


### [6] [Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset](https://arxiv.org/abs/2508.08868)
*Henning Femmer,Frank Houdek,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文发布了一个真实工业环境下、长期标注的高质量需求数据集QuRE，并对其进行统计和同类比较，为需求质量研究提供了新标准，有助于推动领域研究的发展。


<details>
  <summary>Details</summary>
Motivation: 软件和系统工程中需求质量对于项目成功至关重要。然而，关于自然语言需求中质量缺陷的实证研究，往往因数据集不够真实、规模小或细节不足而受到限制。

Method: 作者引入了一个名为QuRE的新数据集，包括2111条经过真实工业审查流程标注的工业需求。对该数据集进行了描述性统计（如词汇多样性和可读性），并与现有及合成数据集进行了对比。

Result: QuRE数据集在文本特性上与现有数据集相似，但其上下文描述更详细，标签更系统，且在近十年工业过程中被广泛应用。该数据集现在已公开用于学术研究。

Conclusion: QuRE数据集的发布为需求质量研究提供了更真实、代表性和可比性的数据，促进了标准化和协作性研究，有望提升实证研究的严谨性和透明度。

Abstract: Requirements quality is central to successful software and systems
engineering. Empirical research on quality defects in natural language
requirements relies heavily on datasets, ideally as realistic and
representative as possible. However, such datasets are often inaccessible,
small, or lack sufficient detail. This paper introduces QuRE (Quality in
Requirements), a new dataset comprising 2,111 industrial requirements that have
been annotated through a real-world review process. Previously used for over
five years as part of an industrial contract, this dataset is now being
released to the research community. In this work, we furthermore provide
descriptive statistics on the dataset, including measures such as lexical
diversity and readability, and compare it to existing requirements datasets and
synthetically generated requirements. In contrast to synthetic datasets, QuRE
is linguistically similar to existing ones. However, this dataset comes with a
detailed context description, and its labels have been created and used
systematically and extensively in an industrial context over a period of close
to a decade. Our goal is to foster transparency, comparability, and empirical
rigor by supporting the development of a common gold standard for requirements
quality datasets. This, in turn, will enable more sound and collaborative
research efforts in the field.

</details>


### [7] [Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories](https://arxiv.org/abs/2508.08872)
*Dylan Callaghan,Alexandra van der Spuy,Bernd Fischer*

Main category: cs.SE

TL;DR: 本文分析了主流开源项目中的软件故障分布，发现故障长期共存在多个版本中，且分布较均匀，提示需要重新评估软件测试和维护策略。


<details>
  <summary>Details</summary>
Motivation: 修复软件故障占据了软件维护和演化成本的很大一部分。降低这些成本需要对软件故障有全面的数据集和理解，以便优化测试和评估。

Method: 对16个开源Java和Python项目（分别来自Defects4J和BugsInPy数据集）的软件故障进行时空特性实证分析。

Result: 发现许多软件系统中的故障具有较长寿命，导致大多数软件版本存在多个并存的故障；与原始数据集的假设形成对比，后者认为多数版本只含有一个故障。此外，虽然故障只出现在系统的少数子集中，但这些故障在该子集内部分布较均匀，很少形成故障高发区域。

Conclusion: 原有数据集低估了实际软件版本中故障的数量及其共存情况。软件故障的分布规律与原假设不同，这对测试和故障管理有重要影响。

Abstract: Fixing software faults contributes significantly to the cost of software
maintenance and evolution. Techniques for reducing these costs require datasets
of software faults, as well as an understanding of the faults, for optimal
testing and evaluation. In this paper, we present an empirical analysis of the
temporal and spatial characteristics of faults existing in 16 open-source Java
and Python projects, which form part of the Defects4J and BugsInPy datasets,
respectively. Our findings show that many faults in these software systems are
long-lived, leading to the majority of software versions having multiple
coexisting faults. This is in contrast to the assumptions of the original
datasets, where the majority of versions only identify a single fault. In
addition, we show that although the faults are found in only a small subset of
the systems, these faults are often evenly distributed amongst this subset,
leading to relatively few bug hotspots.

</details>


### [8] [Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments](https://arxiv.org/abs/2508.08952)
*Hyunwoo Kim,Jaeseong Lee,Sunpyo Hong,Changmin Han*

Main category: cs.SE

TL;DR: 文章针对汽车虚拟化资源分配难题，提出了自动化场景生成和优化框架，通过理论与深度学习建模实现高效、动态的硬件资源分配，并经实测提升了集成和开发效率。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆（SDV）的兴起，汽车行业逐渐采用基于虚拟化的系统架构以提高资源利用率，但如何根据不同系统需求灵活分配资源成为主机厂和零部件集成商的一大难题。目前芯片厂商提供的方案配置固定，难以适应多变的场景。

Method: 提出并设计了一套自动化的场景生成框架，通过运行时性能分析和整合理论模型与厂商经验，实现对多虚拟机硬件资源的自动高效分配。比较了领域引导的参数建模与深度学习建模两种QoS建模方法，并基于选定模型进行资源优化分配。

Result: 该框架经过真实环境应用测试，能够在资源受限场景下显著提升集成效率，并缩短开发周期。

Conclusion: 所提出的自动化场景生成与资源分配框架能够根据系统实际需求灵活优化虚拟化配置，提高了软硬件集成效率，适用于多变、资源受限的汽车电子环境。

Abstract: In the automotive industry, the rise of software-defined vehicles (SDVs) has
  driven a shift toward virtualization-based architectures that consolidate
  diverse automotive workloads on a shared hardware platform. To support this
  evolution, chipset vendors provide board support packages (BSPs), hypervisor
  setups, and resource allocation guidelines. However, adapting these static
  configurations to varying system requirements and workloads remain a
  significant challenge for Tier 1 integrators.
  This paper presents an automated scenario generation framework, which helps
  automotive vendors to allocate hardware resources efficiently across multiple
  VMs. By profiling runtime behavior and integrating both theoretical models
and
  vendor heuristics, the proposed tool generates optimized hypervisor
  configurations tailored to system constraints.
  We compare two main approaches for modeling target QoS based on profiled data
  and resource allocation: domain-guided parametric modeling and deep
  learning-based modeling. We further describe our optimization strategy using
  the selected QoS model to derive efficient resource allocations. Finally, we
  report on real-world deployments to demonstrate the effectiveness of our
  framework in improving integration efficiency and reducing development time
in
  resource-constrained environments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Solving Set Constraints with Comprehensions and Bounded Quantifiers](https://arxiv.org/abs/2508.08496)
*Mudathir Mohamed,Nick Feng,Andrew Reynolds,Cesare Tinelli,Clark Barrett,Marsha Chechik*

Main category: cs.LO

TL;DR: 本文提出使用有限集有界量词和filter算符改进SMT中量词处理，实验优于现有方法，并分析了可判定性的理论边界。


<details>
  <summary>Details</summary>
Motivation: 虽然许多实际问题可自然编码为包含量词的SMT公式，但现有SMT求解器在处理此类问题时效率低下，尤其是由某些工具自动生成的量词公式表现尤甚。现有的量词实例化和优化手段仍有限，因此需要新的方法来提升SMT求解器在量词公式上的能力。

Method: 采用有限集的有界量词，将谓词变量限定在有限集上，并通过有限关系理论中的filter算符（即受限的理解式）实现。这能高效构造有限集合的子集。与已有量词实例化和特化工具对比实验，验证了方法的有效性。

Result: 新方法在SLEEC生成的可满足实例上显著优于现有量词技术，在不可满足测试时与专用工具LEGOS表现接近或更好。此外，理论分析划分出了当filter算符应用受限时的可判定性边界。

Conclusion: 提出的基于有限集的谓词与filter算符的方法在处理由SLEEC工具生成的可满足问题时优于现有方法，在不可满足基准上也表现出很强的竞争力。此外，作者确定了应用受限filter算子的约束的可判定类，而无限制应用则导致不可判定性。

Abstract: Many real applications problems can be encoded easily as quantified formulas
in SMT. However, this simplicity comes at the cost of difficulty during solving
by SMT solvers. Different strategies and quantifier instantiation techniques
have been developed to tackle this. However, SMT solvers still struggle with
quantified formulas generated by some applications. In this paper, we discuss
the use of set-bounded quantifiers, quantifiers whose variable ranges over a
finite set. These quantifiers can be implemented using quantifier-free fragment
of the theory of finite relations with a filter operator, a form of restricted
comprehension, that constructs a subset from a finite set using a predicate. We
show that this approach outperforms other quantification techniques in
satisfiable problems generated by the SLEEC tool, and is very competitive on
unsatisfiable benchmarks compared to LEGOS, a specialized solver for SLEEC. We
also identify a decidable class of constraints with restricted applications of
the filter operator, while showing that unrestricted applications lead to
undecidability.

</details>


### [10] [Behavioural Theory of Reflective Algorithms II: Reflective Parallel Algorithms](https://arxiv.org/abs/2508.09053)
*Klaus-Dieter Schewe,Flavio Ferrarotti*

Main category: cs.LO

TL;DR: 本论文提出并证明了能自我修改行为的反射并行算法理论及其抽象机模型（rASM），为此类算法的分析与实现提供理论框架。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在提出一种行为理论，专门描述可以自主修改行为的反射并行算法（RAs），以应对对算法自适应能力与表达力增强的需求。

Method: 论文通过提出一套定义RAs类别的公理体系，构建抽象机模型，并证明所有的RAs都可由该模型刻画，以rASM（反射抽象状态机）扩展ASM实现算法状态的表达和自我修改。

Result: 作者展示了RAs的核心特性，即每个状态都包含该状态下算法的自主表达，实现语言反射特性，并通过多重集理解项确保有界探索。此外，所有RAs都能以rASM模型刻画和实现。

Conclusion: 论文建立了反射并行算法的行为理论基础，将其与反射抽象状态机模型结合，并证明模型对该算法类别的全刻画能力，为研究自修改并行算法提供理论支撑。

Abstract: We develop a behavioural theory of reflective parallel algorithms (RAs), i.e.
synchronous parallel algorithms that can modify their own behaviour. The theory
comprises a set of postulates defining the class of RAs, an abstract machine
model, and the proof that all RAs are captured by this machine model. RAs are
sequential-time, parallel algorithms, where every state includes a
representation of the algorithm in that state, thus enabling linguistic
reflection. Bounded exploration is preserved using multiset comprehension terms
as values. The abstract machine model is defined by reflective Abstract State
Machines (rASMs), which extend ASMs using extended states that include an
updatable representation of the main ASM rule to be executed by the machine in
that state.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Putnam-AXIOM: A Functional and Static Benchmark](https://arxiv.org/abs/2508.08292)
*Aryan Gulati,Brando Miranda,Eric Chen,Emily Xia,Kai Fronsdal,Bruno Dumont,Elyas Obbad,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 为应对大语言模型在数学推理评测上的训练集污染和准确率饱和问题，作者提出了以Putnam竞赛题为核心的Putnam-AXIOM及其自动生成变体题库。各主流模型在变体集上表现明显下降，揭示了模型对原题的记忆化而非真正理解能力。该基准能动态生成新题，支持更严格和自动化的推理能力评测，是未来模型评估的有力工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学推理上的标准测试集已接近饱和，准确率很高，且存在训练集污染问题。作者旨在提供更具挑战性且抗污染的新评测基准。

Method: 提出了Putnam-AXIOM基准，包括522道来自著名Putnam数学竞赛的大学级竞赛题，并且额外生成了100个功能变体，通过程序化扰动变量和常数制造未见过的同等难度题目，实现动态且难以被污染的测试集。同时引入了Teacher-Forced Accuracy (TFA)评分，用于自动化自然语言推理过程的评估。

Result: 目前最强的模型（OpenAI o1-preview）在原始题集上的准确率为41.9%，在变体题上的准确率下降了19.6%。其他18个模型都表现出类似的准确率下降趋势，其中10个模型在置信区间上毫无重叠。动态变体显示了模型记忆化现象，并且提供了更真实的推理能力评估。

Conclusion: Putnam-AXIOM基准和其变体为评估大语言模型高级数学推理能力提供了严格、抗污染的框架，同时动态题库揭示了现有模型的记忆化问题，强调了更动态和多样化基准的必要性。

Abstract: Current mathematical reasoning benchmarks for large language models (LLMs)
are approaching saturation, with some achieving > 90% accuracy, and are
increasingly compromised by training-set contamination. We introduce
Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn
from the prestigious William Lowell Putnam Mathematical Competition, and
Putnam-AXIOM Variation, an unseen companion set of 100 functional variants
generated by programmatically perturbing variables and constants. The variation
protocol produces an unlimited stream of equally difficult, unseen instances --
yielding a contamination-resilient test bed. On the Original set, OpenAI's
o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy
drops by 19.6% (46.8% relative decrease) on the paired Variations. The
remaining eighteen models show the same downward trend, ten of them with
non-overlapping 95% confidence intervals. These gaps suggest memorization and
highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy
with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores
reasoning traces and automates natural language proof evaluations. Putnam-AXIOM
therefore provides a rigorous, contamination-resilient evaluation framework for
assessing advanced mathematical reasoning of LLMs. Data and evaluation code are
publicly available at https://github.com/brando90/putnam-axiom.

</details>


### [12] [Argument Quality Annotation and Gender Bias Detection in Financial Communication through Large Language Models](https://arxiv.org/abs/2508.08262)
*Alaa Alhamzeh,Mays Al Rebdawi*

Main category: cs.CL

TL;DR: 该论文评估了GPT-4o、Llama 3.1和Gemma 2等大模型在金融论证质量标注中的表现，发现模型一致性优于人工，同时关注模型的性别偏见并提出改进建议，对未来金融文本标注具有重要参考价值。


<details>
  <summary>Details</summary>
Motivation: 金融论证对于投资决策和公众信任至关重要，但如何评估其质量在文献中研究较少。作者希望借助先进大模型提高金融论证质量的标注效率和可靠性，并考察模型在公平性与稳健性方面的表现。

Method: 使用FinArgQuality数据集，选取GPT-4o、Llama 3.1和Gemma 2三个大模型，对金融沟通语境下的论证质量进行多次自动标注，并与人工标注进行一致性对比。此外，设计对抗性攻击，植入性别偏见，评估模型在三种采样温度下的公平性、稳健性及其对标注结果的影响。

Result: 实验显示，大模型在论证质量标注上的一致性超过人工标注者，但仍存在不同程度的性别偏见。论文对结果进行了多元分析，并提出了减少成本和消除偏见的具体建议。

Conclusion: 大模型在金融论证质量标注方面显示出较高一致性和应用潜力，但需持续关注和改进模型的偏见与公平性问题，为今后更高效、可靠和公平的注释方法提供参考。

Abstract: Financial arguments play a critical role in shaping investment decisions and
public trust in financial institutions. Nevertheless, assessing their quality
remains poorly studied in the literature. In this paper, we examine the
capabilities of three state-of-the-art LLMs GPT-4o, Llama 3.1, and Gemma 2 in
annotating argument quality within financial communications, using the
FinArgQuality dataset. Our contributions are twofold. First, we evaluate the
consistency of LLM-generated annotations across multiple runs and benchmark
them against human annotations. Second, we introduce an adversarial attack
designed to inject gender bias to analyse models responds and ensure model's
fairness and robustness. Both experiments are conducted across three
temperature settings to assess their influence on annotation stability and
alignment with human labels. Our findings reveal that LLM-based annotations
achieve higher inter-annotator agreement than human counterparts, though the
models still exhibit varying degrees of gender bias. We provide a multifaceted
analysis of these outcomes and offer practical recommendations to guide future
research toward more reliable, cost-effective, and bias-aware annotation
methodologies.

</details>


### [13] [TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection](https://arxiv.org/abs/2508.08265)
*Tarık Saraç,Selin Mergen,Mucahid Kutlu*

Main category: cs.CL

TL;DR: 本文提出多模型学术辩论方法，其中委员会辩论最优，尤其在检测科学研究引用方面取得第一名，但在其他科学话语子任务上表现一般。


<details>
  <summary>Details</summary>
Motivation: 旨在改进社交媒体上下科学话语检测，如科学主张、科学研究引用及科学实体提及的识别任务。

Method: 采用了模拟多个大语言模型（LLMs）之间结构化学术讨论的新方法，分别探索了单一辩论、团队辩论和委员会辩论三种方案，最终以委员会辩论模式为主。

Result: 在测试集上，委员会辩论方法排名科学研究引用检测第一，但在科学主张和科学实体检测上排名较低（分别为第八和第九）。

Conclusion: 提出的 council debate 方法在检测科学研究引用方面表现最好，排名第一。

Abstract: In this paper, we present our work developed for the scientific web discourse
detection task (Task 4a) of CheckThat! 2025. We propose a novel council debate
method that simulates structured academic discussions among multiple large
language models (LLMs) to identify whether a given tweet contains (i) a
scientific claim, (ii) a reference to a scientific study, or (iii) mentions of
scientific entities. We explore three debating methods: i) single debate, where
two LLMs argue for opposing positions while a third acts as a judge; ii) team
debate, in which multiple models collaborate within each side of the debate;
and iii) council debate, where multiple expert models deliberate together to
reach a consensus, moderated by a chairperson model. We choose council debate
as our primary model as it outperforms others in the development test set.
Although our proposed method did not rank highly for identifying scientific
claims (8th out of 10) or mentions of scientific entities (9th out of 10), it
ranked first in detecting references to scientific studies.

</details>


### [14] [Heartificial Intelligence: Exploring Empathy in Language Models](https://arxiv.org/abs/2508.08271)
*Victoria Williams,Benjamin Rosman*

Main category: cs.CL

TL;DR: 本研究发现，LLMs在认知同理心上已大幅超越人类，但在情感同理心方面仍有较大差距；这突显了其作为虚拟助理和情感支持工具的应用潜力和局限。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在全球范围内的广泛应用，它们开始被用作虚拟助手和伴侣。人类交流中同理心是核心能力之一，分为认知同理心和情感同理心。然而，目前尚不清楚这些模型是否能像人类一样具备这两类同理心。该研究旨在系统检验小型（SLMs）和大型语言模型在同理心方面的表现。

Method: 作者采用标准化心理学测试方法，分别测试和比较了多种大、小型语言模型与人类参与者（包括心理学专业学生）在认知同理心和情感同理心任务上的表现。

Result: 实验结果显示，LLMs在认知同理心任务中的表现优于人类参与者，包括心理学专业学生。然而，无论是小型还是大型语言模型，在情感同理心水平上均显著低于人类。

Conclusion: 当前大型语言模型在认知同理心模拟方面发展迅速，具有为用户提供有效虚拟陪伴和个性化情感支持的潜力。相较于人类，其高认知而低情感同理心也使模型在提供情感支持时具备客观和一致性优势，能够避免情感疲劳或偏见等风险。

Abstract: Large language models have become increasingly common, used by millions of
people worldwide in both professional and personal contexts. As these models
continue to advance, they are frequently serving as virtual assistants and
companions. In human interactions, effective communication typically involves
two types of empathy: cognitive empathy (understanding others' thoughts and
emotions) and affective empathy (emotionally sharing others' feelings). In this
study, we investigated both cognitive and affective empathy across several
small (SLMs) and large (LLMs) language models using standardized psychological
tests. Our results revealed that LLMs consistently outperformed humans -
including psychology students - on cognitive empathy tasks. However, despite
their cognitive strengths, both small and large language models showed
significantly lower affective empathy compared to human participants. These
findings highlight rapid advancements in language models' ability to simulate
cognitive empathy, suggesting strong potential for providing effective virtual
companionship and personalized emotional support. Additionally, their high
cognitive yet lower affective empathy allows objective and consistent emotional
support without running the risk of emotional fatigue or bias.

</details>


### [15] [Real-time News Story Identification](https://arxiv.org/abs/2508.08272)
*Tadej Škvorc,Nikola Ivačič,Sebastjan Hribar,Marko Robnik-Šikonja*

Main category: cs.CL

TL;DR: 本文提出了一种新闻实时故事识别方法，利用文本表示与多种在线主题建模技术，能有效地将新闻自动聚类到具体事件，实验结果表明该方法表现优秀。


<details>
  <summary>Details</summary>
Motivation: 为提升读者在新闻网站的阅读体验，需要将新闻按照事件组织成故事集合，实现自动化和实时的故事识别。以便读者快速了解同一事件的相关报道。

Method: 提出了一种结合文本表示技术、聚类算法和在线主题建模方法的实时故事识别方法。具体结合了事件抽取、命名实体识别，以及BERTopic、DBStream、TextClust 等在线主题建模方法。

Result: 该方法在斯洛文尼亚新闻数据集上进行了为期一个月的实验，并通过人工评测证明其实时分配新闻到故事的效果合理、有效。

Conclusion: 该文提出的算法能够在新闻上线的同时，实时将新闻分配到合适的故事集合中，提升了新闻监测系统的自动化能力和阅读体验。

Abstract: To improve the reading experience, many news sites organize news into topical
collections, called stories. In this work, we present an approach for
implementing real-time story identification for a news monitoring system that
automatically collects news articles as they appear online and processes them
in various ways. Story identification aims to assign each news article to a
specific story that the article is covering. The process is similar to text
clustering and topic modeling, but requires that articles be grouped based on
particular events, places, and people, rather than general text similarity (as
in clustering) or general (predefined) topics (as in topic modeling). We
present an approach to story identification that is capable of functioning in
real time, assigning articles to stories as they are published online. In the
proposed approach, we combine text representation techniques, clustering
algorithms, and online topic modeling methods. We combine various text
representation methods to extract specific events and named entities necessary
for story identification, showing that a mixture of online topic-modeling
approaches such as BERTopic, DBStream, and TextClust can be adapted for story
discovery. We evaluate our approach on a news dataset from Slovene media
covering a period of 1 month. We show that our real-time approach produces
sensible results as judged by human evaluators.

</details>


### [16] [TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning](https://arxiv.org/abs/2508.08273)
*Kristian Miok,Blaz Škrlj,Daniela Zaharie,Marko Robnik Šikonja*

Main category: cs.CL

TL;DR: 作者提出了一种名为TT-XAI的新框架，通过关键词蒸馏和LLM链式推理，显著提升了临床文本分类的效果和可解释性，实验和专家评估均表明其方法显著优于传统做法。


<details>
  <summary>Details</summary>
Motivation: 传统临床语言模型在处理冗长、非结构化的电子健康记录时，常常在预测和解释能力上表现不佳，缺乏可信赖性。作者希望提升模型的分类性能和可解释性，使其更适合于真实医疗场景中的应用。

Method: 作者提出了TT-XAI框架，引入了领域感知的关键词蒸馏，并结合大型语言模型（LLMs）进行链式推理。具体步骤包括：将原始出院记录精炼为关键词表示用于提升BERT分类器性能，同时用专注于关键词的LIME增强局部可解释性；再用关键词引导给LLMs制定链式推理解释。

Result: 关键词蒸馏显著提升了分类器（BERT）的性能，也提升了解释的局部忠实度。链式推理生成的解释更加简明且具备临床相关性。通过基于删除的忠实度指标、LLaMA-3自评打分和盲法临床专家评测，多种评测方法均显示关键词增强法优于传统方法。

Conclusion: TT-XAI方案在提升分类准确度的同时，极大地增强了机器与人类的可解释性，为未来可信、可审计的临床AI决策系统提供了可扩展路径。

Abstract: Clinical language models often struggle to provide trustworthy predictions
and explanations when applied to lengthy, unstructured electronic health
records (EHRs). This work introduces TT-XAI, a lightweight and effective
framework that improves both classification performance and interpretability
through domain-aware keyword distillation and reasoning with large language
models (LLMs). First, we demonstrate that distilling raw discharge notes into
concise keyword representations significantly enhances BERT classifier
performance and improves local explanation fidelity via a focused variant of
LIME. Second, we generate chain-of-thought clinical explanations using
keyword-guided prompts to steer LLMs, producing more concise and clinically
relevant reasoning. We evaluate explanation quality using deletion-based
fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human
study with domain experts. All evaluation modalities consistently favor the
keyword-augmented method, confirming that distillation enhances both machine
and human interpretability. TT-XAI offers a scalable pathway toward
trustworthy, auditable AI in clinical decision support.

</details>


### [17] [Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition](https://arxiv.org/abs/2508.08274)
*Roberto Labadie-Tamayo,Djordje Slijepčević,Xihui Chen,Adrian Jaques Böck,Andreas Babic,Liz Freimann,Christiane Atzmüller Matthias Zeppelzauer*

Main category: cs.CL

TL;DR: SCBM通过形容词作为可解释瓶颈概念，大幅提升多平台仇恨言论识别准确性与模型可解释性，并可扩展到其他自然语言处理任务。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的仇恨言论激增，对社会带来了前所未有的影响，因此需要自动化检测仇恨内容的方法。以往方法多为黑盒模型，缺乏透明性和可解释性。

Method: 提出了一种全新的透明自动化仇恨及反仇恨言论识别方法——“言语概念瓶颈模型（SCBM）”，利用形容词作为人类可解释的瓶颈概念。SCBM首先通过大语言模型（LLM）将输入文本映射为基于形容词的抽象表示，然后利用轻量级分类器进行下游任务。

Result: 在五组跨语种、跨平台的基准数据集上，SCBM平均macro-F1分数为0.69，优于当前文献报告的最新结果（五组数据集中有四组表现最佳）。另外，SCBM在准确性和本地、全局可解释性方面表现突出。将形容词概念表示与Transformer嵌入结合还能提升1.8%的平均表现，证明了方法补充信息的能力。

Conclusion: 形容词概念表示为仇恨及反仇恨言论识别任务提供了紧凑、可解释且有效的表征。该方法具备高度透明性和可迁移性，可推广至其他NLP任务。

Abstract: The rapid increase in hate speech on social media has exposed an
unprecedented impact on society, making automated methods for detecting such
content important. Unlike prior black-box models, we propose a novel
transparent method for automated hate and counter speech recognition, i.e.,
"Speech Concept Bottleneck Model" (SCBM), using adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to map input texts to an abstract adjective-based representation, which
is then sent to a light-weight classifier for downstream tasks. Across five
benchmark datasets spanning multiple languages and platforms (e.g., Twitter,
Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which
outperforms the most recently reported results from the literature on four out
of five datasets. Aside from high recognition accuracy, SCBM provides a high
level of both local and global interpretability. Furthermore, fusing our
adjective-based concept representation with transformer embeddings, leads to a
1.8% performance increase on average across all datasets, showing that the
proposed representation captures complementary information. Our results
demonstrate that adjective-based concept representations can serve as compact,
interpretable, and effective encodings for hate and counter speech recognition.
With adapted adjectives, our method can also be applied to other NLP tasks.

</details>


### [18] [MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis](https://arxiv.org/abs/2508.08275)
*Haiyun Guo,ZhiYan Hou,Yu Chen,Jinghan He,Yandu Sun,Yuzhe Zhou,Shujing Guo,Kuan Zhu,Jinqiao Wang*

Main category: cs.CL

TL;DR: 该工作提出并构建了针对多模态大语言模型持续学习的评测基准MLLM-CTBench，涵盖多维评估、全面算法对比和精细任务设置，发现模型能力、任务顺序及训练方式对持续学习效果有重要影响，强化学习引入KL约束可缓解遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型持续指令微调缺乏严格系统的评价基准，影响了其应用进展。

Method: 提出了MLLM-CTBench评价基准，涵盖多维评估：结合答案准确性与细粒度CoT推理评估；全面评测8种持续学习算法与四类训练范式，并系统比较强化学习与有监督微调；精心筛选并组织了16组任务数据，覆盖六大领域。

Result: 主要发现：高能力模型对遗忘更具鲁棒性；推理链条相比最终答案退化更慢，验证分层遗忘假设；持续学习算法效果高度依模型能力与任务顺序相关；强化学习中引入KL散度约束有助于维持策略稳定性、降低遗忘。

Conclusion: MLLM-CTBench 为多模态大语言模型持续指令微调设立了严格标准，并为算法设计和评估提供了实用指导。

Abstract: Multimodal Large Language Models (MLLMs) rely on continual instruction tuning
to adapt to the evolving demands of real-world applications. However, progress
in this area is hindered by the lack of rigorous and systematic benchmarks. To
address this gap, we present MLLM-CTBench, a comprehensive evaluation benchmark
with three key contributions: (1) Multidimensional Evaluation: We combine final
answer accuracy with fine-grained CoT reasoning quality assessment, enabled by
a specially trained CoT evaluator; (2) Comprehensive Evaluation of Algorithms
and Training Paradigms: We benchmark eight continual learning algorithms across
four major categories and systematically compare reinforcement learning with
supervised fine-tuning paradigms; (3) Carefully Curated Tasks: We select and
organize 16 datasets from existing work, covering six challenging domains. Our
key findings include: (i) Models with stronger general capabilities exhibit
greater robustness to forgetting during continual learning; (ii) Reasoning
chains degrade more slowly than final answers, supporting the hierarchical
forgetting hypothesis; (iii) The effectiveness of continual learning algorithms
is highly dependent on both model capability and task order; (iv) In
reinforcement learning settings, incorporating KL-divergence constraints helps
maintain policy stability and plays a crucial role in mitigating forgetting.
MLLM-CTBench establishes a rigorous standard for continual instruction tuning
of MLLMs and offers practical guidance for algorithm design and evaluation.

</details>


### [19] [Evaluating Contrast Localizer for Identifying Causal Unitsin Social & Mathematical Tasks in Language Models](https://arxiv.org/abs/2508.08276)
*Yassine Jamaa,Badr AlKhamissi,Satrajit Ghosh,Martin Schrimpf*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型和视觉语言模型中，通过对比局部化选出的任务相关神经元并不总是真正决定任务表现，被认为不重要的单元反倒可能更加关键，提示现有因果定位方法需改进。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索理论心智（ToM）和数学推理任务中，大型语言模型（LLM）及视觉语言模型（VLM）中的因果相关单元定位方法的有效性。动机在于验证神经科学对比局部化工具在人工智能模型中的适用性和因果性。

Method: 采用神经科学中的对比局部化方法，针对11个LLM和5个VLM（参数规模3B~90B），利用对比性刺激集合定位高激活单元，并通过定向消融法评估其因果作用。将功能性选取单元、低激活单元、随机选取单元消融的影响在ToM和数学任务基准上进行比较。

Result: 实验发现：低激活单元有时对模型性能的影响甚至大于高激活单元；用数学对比局部化法选出的单元消融后，对ToM性能影响大于用ToM对比局部化法选出的单元。

Conclusion: 对比局部化选取的单元并不总是具有因果相关性，现有方法在精确捕捉任务相关单元方面存在局限性；需更广泛刺激集合和更精确的方法来识别因果相关单元。

Abstract: This work adapts a neuroscientific contrast localizer to pinpoint causally
relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in
large language models (LLMs) and vision-language models (VLMs). Across 11 LLMs
and 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated
units using contrastive stimulus sets and assess their causal role via targeted
ablations. We compare the effect of lesioning functionally selected units
against low-activation and randomly selected units on downstream accuracy
across established ToM and mathematical benchmarks. Contrary to expectations,
low-activation units sometimes produced larger performance drops than the
highly activated ones, and units derived from the mathematical localizer often
impaired ToM performance more than those from the ToM localizer. These findings
call into question the causal relevance of contrast-based localizers and
highlight the need for broader stimulus sets and more accurately capture
task-specific units.

</details>


### [20] [AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators](https://arxiv.org/abs/2508.09101)
*Jason Chou,Ao Liu,Yuchi Deng,Zhiying Zeng,Tao Zhang,Haotian Zhu,Jianwei Cai,Yue Mao,Chenchen Zhang,Lingyun Tan,Ziyan Xu,Bohui Zhai,Hengyi Liu,Speed Zhu,Wiggin Zhou,Fengzong Lian*

Main category: cs.CL

TL;DR: 作者提出了无需人工标注的AutoCodeGen自动构造高难多语言代码基准方法，并据此发布了覆盖20种语言的大规模、难度高的新基准AutoCodeBench。对多种大模型的实验证明现有模型难以应对复杂多语言代码生成任务。该基准有望成为推动多语言代码生成研究的重要资源。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在代码生成表现突出，但现有评测基准存在依赖人工标注、语言单一（多为Python）、多语言基准难度低且分布不均等问题，亟需更高质量多语言自动评测方法。

Method: 提出AutoCodeGen方法，自动生成高难度多语言代码生成数据集，无需人工标注。通过LLMs生成测试用例输入，并用多语言沙盒获取输出，反向生成和多重筛选保证数据质量。基于此构建了AutoCodeBench大规模基准，包括3920道题，覆盖20种编程语言，且题目分布均匀。

Result: 对30余种主流大模型(开源及闭源)进行了评测，结果显示即使最先进模型也在高复杂度、多样化和多语言任务下表现不理想。同时提出专为基础模型设计的AutoCodeBench-Complete，用于评估少样本代码生成能力。

Conclusion: AutoCodeBench系列为大模型实际应用中的多语言高难度代码生成提供了新型大规模、自动化评测工具，促进社区关注并推动实际复杂应用场景下的多语言代码生成研究。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains, with code generation emerging as a key area of focus. While
numerous benchmarks have been proposed to evaluate their code generation
abilities, these benchmarks face several critical limitations. First, they
often rely on manual annotations, which are time-consuming and difficult to
scale across different programming languages and problem complexities. Second,
most existing benchmarks focus primarily on Python, while the few multilingual
benchmarks suffer from limited difficulty and uneven language distribution. To
address these challenges, we propose AutoCodeGen, an automated method for
generating high-difficulty multilingual code generation datasets without manual
annotations. AutoCodeGen ensures the correctness and completeness of test cases
by generating test inputs with LLMs and obtaining test outputs through a
multilingual sandbox, while achieving high data quality through reverse-order
problem generation and multiple filtering steps. Using this novel method, we
introduce AutoCodeBench, a large-scale code generation benchmark comprising
3,920 problems evenly distributed across 20 programming languages. It is
specifically designed to evaluate LLMs on challenging, diverse, and practical
multilingual tasks. We evaluate over 30 leading open-source and proprietary
LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The
results show that even the most advanced LLMs struggle with the complexity,
diversity, and multilingual nature of these tasks. Besides, we introduce
AutoCodeBench-Complete, specifically designed for base models to assess their
few-shot code generation capabilities. We hope the AutoCodeBench series will
serve as a valuable resource and inspire the community to focus on more
challenging and practical multilingual code generation scenarios.

</details>


### [21] [Objective Metrics for Evaluating Large Language Models Using External Data Sources](https://arxiv.org/abs/2508.08277)
*Haoze Du,Richard Li,Edward Gehringer*

Main category: cs.CL

TL;DR: 本文提出了一种自动化且可减少主观性的LLM评测新框架，能广泛应用于多领域的模型性能评估，具有透明、可复现、可扩展等优点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测通常涉及主观判断，难以避免偏见，评测结果在高风险领域（如教育和科学）缺乏充分可信度。为减少主观性、提升评测的公平性和可扩展性，需开发更标准化评测方案。

Method: 利用来自不同学期课堂文本材料提取的主观指标，结合明确的基准、事实数据集与结构化评测流程，实现自动化、透明的模型评分。

Result: 该框架可自动化、透明地评估LLMs，降低了对人为解释的依赖，保证了评分的一致性，并适用于教育、科学等高影响领域。

Conclusion: 本文提出的评估框架有效减少了主观判断对大语言模型（LLMs）评测的影响，提升了评测的一致性与可复现性。

Abstract: Evaluating the performance of Large Language Models (LLMs) is a critical yet
challenging task, particularly when aiming to avoid subjective assessments.
This paper proposes a framework for leveraging subjective metrics derived from
the class textual materials across different semesters to assess LLM outputs
across various tasks. By utilizing well-defined benchmarks, factual datasets,
and structured evaluation pipelines, the approach ensures consistent,
reproducible, and bias-minimized measurements. The framework emphasizes
automation and transparency in scoring, reducing reliance on human
interpretation while ensuring alignment with real-world applications. This
method addresses the limitations of subjective evaluation methods, providing a
scalable solution for performance assessment in educational, scientific, and
other high-stakes domains.

</details>


### [22] [MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language](https://arxiv.org/abs/2508.08283)
*Andres Garcia Rincon,Eliseo Ferrante*

Main category: cs.CL

TL;DR: MinionsLLM通过大语言模型与行为树结合，大幅提升了多智能体系统的自然语言控制能力，尤其是小模型微调后表现优异，资源全部开源。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中实现基于自然语言的灵活控制具有重要价值，但现有方法在环境适应性、行为指令表达和小模型部署等方面存在挑战。为提升系统的通用性和可控性，迫切需要结合大语言模型（LLM）与行为树等形式化结构，解决任务表达与执行的准确性问题。

Method: 该论文提出了MinionsLLM框架，将大语言模型、行为树和形式文法结合，用以实现多智能体系统中任意用户定义环境的自然语言控制。提供了标准化接口和两种合成数据集生成方法（A和B），针对LLM进行了微调优化。采用Google Gemma 3模型家族（1B、4B、12B参数规模）验证方法可靠性。

Result: 方法B使句法有效性提升至92.6%，平均任务性能提升33%。实验还发现，小规模模型最受益于微调，为本地化、资源受限场景下的多智能体控制提供了良好前景。

Conclusion: MinionsLLM框架能够高效提升多智能体系统中自然语言指令的表达能力和任务执行效率，尤其适用于小模型和受限算力条件。相关资源已开源，有利于促进后续研究。

Abstract: This paper presents MinionsLLM, a novel framework that integrates Large
Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable
natural language control of multi-agent systems within arbitrary, user-defined
environments. MinionsLLM provides standardized interfaces for defining
environments, agents, and behavioral primitives, and introduces two synthetic
dataset generation methods (Method A and Method B) to fine-tune LLMs for
improved syntactic validity and semantic task relevance. We validate our
approach using Google's Gemma 3 model family at three parameter scales (1B, 4B,
and 12B) and demonstrate substantial gains: Method B increases syntactic
validity to 92.6% and achieves a mean task performance improvement of 33% over
baseline. Notably, our experiments show that smaller models benefit most from
fine-tuning, suggesting promising directions for deploying compact, locally
hosted LLMs in resource-constrained multi-agent control scenarios. The
framework and all resources are released open-source to support reproducibility
and future research.

</details>


### [23] [The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs](https://arxiv.org/abs/2508.08285)
*Denis Janiak,Jakub Binkowski,Albert Sawczyn,Bogdan Gabrys,Ravid Schwartz-Ziv,Tomasz Kajdanowicz*

Main category: cs.CL

TL;DR: 传统ROUGE评测方法严重高估幻觉检测手段真实能力，实际需引入更贴近人类直觉、更具语义的评测标准，保障大语言模型在实际应用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLMs）在自然语言处理领域产生重大影响，但其幻觉（生成不真实内容）问题严重影响了可靠应用。现有评测方法过度依赖ROUGE等基于词汇重叠的指标，这与人工评判存在不一致。

Method: 通过系统性的人类研究，对比传统的ROUGE指标与更符合人类直觉的LLM-as-Judge等评价方法，并分析了简单启发式方法（如回复长度）与复杂检测技术的表现。

Result: 研究发现，ROUGE指标尽管召回率高，但准确率极低，导致检测方法实际性能存在误导。用人类一致性指标评估时，多种主流幻觉检测方法的性能下降高达45.9%。同时，简单长度启发式法表现可与复杂方法媲美，暴露了当前评测体系的根本性缺陷。

Conclusion: 当前的基于ROUGE的评估方法存在严重不足，无法有效反映幻觉检测的真实准确性。提出需采用语义感知和更健壮的评测框架，以提升LLMs结果的可靠性和可信度。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their tendency to hallucinate poses serious challenges for reliable
deployment. Despite numerous hallucination detection methods, their evaluations
often rely on ROUGE, a metric based on lexical overlap that misaligns with
human judgments. Through comprehensive human studies, we demonstrate that while
ROUGE exhibits high recall, its extremely low precision leads to misleading
performance estimates. In fact, several established detection methods show
performance drops of up to 45.9\% when assessed using human-aligned metrics
like LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based
on response length can rival complex detection techniques, exposing a
fundamental flaw in current evaluation practices. We argue that adopting
semantically aware and robust evaluation frameworks is essential to accurately
gauge the true performance of hallucination detection methods, ultimately
ensuring the trustworthiness of LLM outputs.

</details>


### [24] [Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions](https://arxiv.org/abs/2508.08287)
*Farah Atif,Nursultan Askarbekuly,Kareem Darwish,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文提出伊斯兰教法领域的FiqhQA基准，细致区分四大逊尼派学派和多语言，首次系统评估LLM在该领域的问答能力及拒答表现。实验发现，不同模型和语言下性能相差明显，阿拉伯语表现普遍较弱，强调了在宗教领域部署LLM需谨慎且需专项评测。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM广泛应用于不同领域，但在宗教领域（特别是伊斯兰教法）中的可靠性与准确性尚未得到充分研究，现有工作忽略了宗教学派的细致区分以及“拒绝作答”的评估。

Method: 作者提出了FiqhQA基准数据集，涵盖了四大逊尼派法学派、阿拉伯语和英语两个语言环境，并设计了零样本和放弃作答等实验，对主流LLM进行了全面评估。评测不仅关注准确性，还包括模型对“应该拒绝回答”场景的识别能力。

Result: GPT-4o在准确性上表现最佳，而Gemini和Fanar在拒绝作答（减少自信但错误答案）方面更优。所有模型在阿拉伯语环境下性能都有明显下降，展现出非英语场景下的局限性。

Conclusion: 所有大型语言模型（LLM）在面对与伊斯兰教法相关的复杂、细分领域时表现仍有限，尤其是在阿拉伯语环境下。同时，现有模型在应对不同教法学派和语言切换时存在显著性能差异。

Abstract: Despite the increasing usage of Large Language Models (LLMs) in answering
questions in a variety of domains, their reliability and accuracy remain
unexamined for a plethora of domains including the religious domains. In this
paper, we introduce a novel benchmark FiqhQA focused on the LLM generated
Islamic rulings explicitly categorized by the four major Sunni schools of
thought, in both Arabic and English. Unlike prior work, which either overlooks
the distinctions between religious school of thought or fails to evaluate
abstention behavior, we assess LLMs not only on their accuracy but also on
their ability to recognize when not to answer. Our zero-shot and abstention
experiments reveal significant variation across LLMs, languages, and legal
schools of thought. While GPT-4o outperforms all other models in accuracy,
Gemini and Fanar demonstrate superior abstention behavior critical for
minimizing confident incorrect answers. Notably, all models exhibit a
performance drop in Arabic, highlighting the limitations in religious reasoning
for languages other than English. To the best of our knowledge, this is the
first study to benchmark the efficacy of LLMs for fine-grained Islamic school
of thought specific ruling generation and to evaluate abstention for Islamic
jurisprudence queries. Our findings underscore the need for task-specific
evaluation and cautious deployment of LLMs in religious applications.

</details>


### [25] [CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation](https://arxiv.org/abs/2508.08386)
*Shuzhou Yuan,William LaCroix,Hardik Ghoshal,Ercong Nie,Michael Färber*

Main category: cs.CL

TL;DR: 论文提出CoDAE框架，通过链式思维数据增强和针对性微调方案，显著提升LLM在AI教学中的指导质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽适合大规模与个性化教学，但在教育场景下表现不足：过度顺从、无法适应学生困惑、易受情绪操控。需提升LLM的教育适应性与教学表现。

Method: 采集真实学生与ChatGPT教师的对话，利用链式思维(CoT)提示进行数据增强，并针对性设计对话场景，细致解决LLM在教育中的三大问题。之后对四个开源LLM进行微调，并用自动评估和LLM评审模拟教育场景进行测试。

Result: 微调后的模型在教育指导、支持推理和防止提前给答案等方面表现提升，有更好的教学适应能力。

Conclusion: CoDAE框架提升了LLM作为AI教师的教育适应性，提供更符合教育理念的指导，并能有效避免提前泄露答案。

Abstract: Large Language Models (LLMs) are increasingly employed as AI tutors due to
their scalability and potential for personalized instruction. However,
off-the-shelf LLMs often underperform in educational settings: they frequently
reveal answers too readily, fail to adapt their responses to student
uncertainty, and remain vulnerable to emotionally manipulative prompts. To
address these challenges, we introduce CoDAE, a framework that adapts LLMs for
educational use through Chain-of-Thought (CoT) data augmentation. We collect
real-world dialogues between students and a ChatGPT-based tutor and enrich them
using CoT prompting to promote step-by-step reasoning and pedagogically aligned
guidance. Furthermore, we design targeted dialogue cases to explicitly mitigate
three key limitations: over-compliance, low response adaptivity, and threat
vulnerability. We fine-tune four open-source LLMs on different variants of the
augmented datasets and evaluate them in simulated educational scenarios using
both automatic metrics and LLM-as-a-judge assessments. Our results show that
models fine-tuned with CoDAE deliver more pedagogically appropriate guidance,
better support reasoning processes, and effectively resist premature answer
disclosure.

</details>


### [26] [Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery](https://arxiv.org/abs/2508.08401)
*Jiatong Li,Weida Wang,Qinggang Zhang,Junxian Li,Di Zhang,Changmeng Zheng,Shufei Zhang,Xiaoyong Wei,Qing Li*

Main category: cs.CL

TL;DR: 本文提出Mol-R1框架，通过高质量数据与创新训练方法，显著提升长链推理LLM在文本分子生成任务中的表现，优于当前主流基线。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（如深度链式推理模型DeepSeek-R1与QWQ）在常识推理和数学推理方面表现优异，但在知识密集领域（如分子发现）推理时效果有限且效率低下。主要挑战是对领域知识（分子结构、化学原理等）的精准理解难度高，并且缺乏高质量的专家注释。

Method: 提出了一种新框架Mol-R1，提升R1类显式长链式推理LLM在文本分子生成中的可解释性与推理能力。方法包括：1）使用PRID（Prior Regulation via In-context Distillation）策略构建高质量推理数据集；2）引入MoIA（Molecular Iterative Adaptation）训练策略，结合有监督微调（SFT）与强化策略优化（RPO），迭代提升模型推理能力。

Result: Mol-R1在文本分子推理生成任务上，推理表现超越现有主流基线方法。

Conclusion: 通过高质量推理数据集与创新的训练策略，有效提升了长链推理模型在分子发现领域的推理性能和可解释性，显示出明显领先于以往方法的优越性。

Abstract: Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)
reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning
capabilities, achieving impressive performance in commonsense reasoning and
mathematical inference. Despite their effectiveness, Long-CoT reasoning models
are often criticized for their limited ability and low efficiency in
knowledge-intensive domains such as molecule discovery. Success in this field
requires a precise understanding of domain knowledge, including molecular
structures and chemical principles, which is challenging due to the inherent
complexity of molecular data and the scarcity of high-quality expert
annotations. To bridge this gap, we introduce Mol-R1, a novel framework
designed to improve explainability and reasoning performance of R1-like
Explicit Long-CoT reasoning LLMs in text-based molecule generation. Our
approach begins with a high-quality reasoning dataset curated through Prior
Regulation via In-context Distillation (PRID), a dedicated distillation
strategy to effectively generate paired reasoning traces guided by prior
regulations. Building upon this, we introduce MoIA, Molecular Iterative
Adaptation, a sophisticated training strategy that iteratively combines
Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),
tailored to boost the reasoning performance of R1-like reasoning models for
molecule discovery. Finally, we examine the performance of Mol-R1 in the
text-based molecule reasoning generation task, showing superior performance
against existing baselines.

</details>


### [27] [Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment](https://arxiv.org/abs/2508.08424)
*Saketh Reddy Vemula,Dipti Mishra Sharma,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 形态对齐分词对某些下游任务有中等正面作用，但分词器类型影响更大，Unigram方法效果最好，指标如词数和熵无法预测实际表现。


<details>
  <summary>Details</summary>
Motivation: 现有关于形态对齐分词是否提升语言建模表现的研究结果矛盾，尤其在复杂形态语言中，因此需要系统分析形态对齐及分词算法的影响。

Method: 选择了类型多样的语言（泰卢固语、印地语、英语），进行分词器训练、微调和下游任务评估，对分词算法（BPE和Unigram）及形态对齐质量进行比较；建立泰卢固语金标准词素分割数据集以评估分词器。

Result: 更好的形态对齐对语法任务略有帮助，但Unigram分词器普遍表现优于其他方案，BPE结合形态分割有所提升；内在指标（如词数、熵）与下游性能无关。

Conclusion: 形态对齐的分词方式在一定程度上提升了形态复杂语言的语法任务表现，但分词算法本身对下游任务影响更大。

Abstract: Prior work on language modeling showed conflicting findings about whether
morphologically aligned approaches to tokenization improve performance,
particularly for languages with complex morphology. To investigate this, we
select a typologically diverse set of languages: Telugu (agglutinative), Hindi
(primarily fusional with some agglutination), and English (fusional). We
conduct a comprehensive evaluation of language models -- starting from
tokenizer training and extending through the finetuning and downstream task
evaluation. To account for the consistent performance differences observed
across tokenizer variants, we focus on two key factors: morphological alignment
and tokenization quality. To assess morphological alignment of tokenizers in
Telugu, we create a dataset containing gold morpheme segmentations of 600
derivational and 7000 inflectional word forms.
  Our experiments reveal that better morphological alignment correlates
positively -- though moderately -- with performance in syntax-based tasks such
as Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing.
However, we also find that the tokenizer algorithm (Byte-pair Encoding vs.
Unigram) plays a more significant role in influencing downstream performance
than morphological alignment alone. Naive Unigram tokenizers outperform others
across most settings, though hybrid tokenizers that incorporate morphological
segmentation significantly improve performance within the BPE framework. In
contrast, intrinsic metrics like Corpus Token Count (CTC) and R\'enyi entropy
showed no correlation with downstream performance.

</details>


### [28] [Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints](https://arxiv.org/abs/2508.08466)
*Daren Yao,Jinsong Yuan,Ruike Chen*

Main category: cs.CL

TL;DR: 作者提出了两种改进小型LLM偏好对齐的新方法，在多个评测上均显著提升模型对齐能力，方法轻量、效果显著，适合实际低资源场景应用。


<details>
  <summary>Details</summary>
Motivation: 小型大语言模型（LLM）在对齐输出与人类偏好时，往往因为存在明显性能差距而表现不佳。论文希望改善小模型在资源有限及性能不足情况下的对齐能力。

Method: 提出了两种轻量级、基于DPO（Direct Preference Optimization）的方法变体：Adaptive Margin-Sigmoid Loss和APO-hinge-zero。核心思路为引入基于边界的目标函数与选择性更新机制，以强化模型对难例的关注及优化。APO-hinge-zero结合了“铰链损失”下的难例挖掘和APO-zero的选择优化。

Result: 在AlpacaEval测试中，APO-hinge-zero方法相比基线模型APO-zero分别在整体胜率和长度受控胜率提升了2.0和1.4个百分点。在MT-Bench多任务测试中，这些方法在不同任务类别下表现竞争力，尤其在STEM与人文任务上表现突出。

Conclusion: 在偏好对齐目标上作出简单改动即可大幅提升小型LLM的对齐表现，为资源受限环境下的小模型高效应用提供了可行路径。

Abstract: Small large language models (LLMs) often face difficulties in aligning output
to human preferences, particularly when operating under severe performance
gaps. In this work, we propose two lightweight DPO-based variants -- Adaptive
Margin-Sigmoid Loss and APO-hinge-zero -- to better address underperformance
scenarios by introducing margin-based objectives and selective update
mechanisms.
  Our APO-hinge-zero method, which combines hinge-induced hard-example mining
with the chosen-focused optimization of APO-zero, achieves strong results. In
AlpacaEval, APO-hinge-zero improves the win rate by +2.0 points and the
length-controlled win rate by +1.4 points compared to the APO-zero baseline. In
MT-Bench, our methods maintain competitive performance in diverse categories,
particularly excelling in STEM and Humanities tasks.
  These results demonstrate that simple modifications to preference-based
objectives can significantly enhance small LLM alignment under resource
constraints, offering a practical path toward more efficient deployment.

</details>


### [29] [Momentum Point-Perplexity Mechanics in Large Language Models](https://arxiv.org/abs/2508.08492)
*Lorenzo Tomaz,Judd Rosenblatt,Thomas Berry Jones,Diogo Schwerz de Lucena*

Main category: cs.CL

TL;DR: 本文提出将物理能量守恒观点应用于分析大语言模型推理过程，发现隐藏状态变化具有近恒“能量”特征。据此开发了Jacobian steering方法，有效提升模型输出语义质量，为模型可解释性和可控性带来新思路。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型推理过程中其内部隐藏状态的变化规律尚不清楚，理解这些变化对于模型可解释性和操控性至关重要。

Method: 采用物理学视角，将隐藏状态变化速率与模型下一个token预测置信度结合，提出类似物理能量的指标（称为“log-Lagrangian”）。通过分析不同阶段（随机权重、预训练、训练后）模型的“能量”守恒性，并基于此推导出一种控制方法“Jacobian steering”，对模型输出进行微扰以偏向目标token。

Result: 发现20个主流transformer模型的“能量”近乎守恒，训练过程加大了能量变异性与决策速度。随机权重模型能量守恒更紧密。以log-Lagrangian为基础的Jacobian steering方法在测试模型中保持能量近恒，且生成结果语义质量优于自然输出。

Conclusion: 物理学视角下可为大型语言模型的可解释性、异常检测和低风险操控提供理论基础，有助于提升模型的可控性与对人类意图的对齐。

Abstract: We take a physics-based approach to studying how the internal hidden states
of large language models change from token to token during inference. Across 20
open-source transformer models (135M-3B parameters), we find that a quantity
combining the rate of change in hidden states and the model's next-token
certainty, analogous to energy in physics, remains nearly constant.
Random-weight models conserve this "energy" more tightly than pre-trained ones,
while training shifts models into a faster, more decisive regime with greater
variability. Using this "log-Lagrangian" view, we derive a control method
called Jacobian steering, which perturbs hidden states in the minimal way
needed to favor a target token. This approach maintained near-constant energy
in two tested models and produced continuations rated higher in semantic
quality than the models' natural outputs. Viewing transformers through this
mechanics lens offers a principled basis for interpretability, anomaly
detection, and low-risk steering. This could help make powerful models more
predictable and aligned with human intent.

</details>


### [30] [Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression](https://arxiv.org/abs/2508.08509)
*Jadie Adams,Brian Hu,Emily Veenhuis,David Joy,Bharadwaj Ravichandran,Aaron Bray,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: 作者提出并验证了一种基于细粒度属性和少样本比较回归的新型多元对齐方法，实现了更具公平性和代表性的语言模型对齐表现，在模拟数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法主要依赖于只能表达平均用户偏好的标量奖励，无法充分反映用户多样化和多维度的偏好。因此，需要更细致和多元的对齐方法以实现更公平与具代表性的AI。

Method: 作者提出了一种基于上下文学习和少样本比较回归的新方法，通过引入细粒度属性，比较不同响应选项以实现个体化偏好对齐。此外，作者还建立了两个新的多元对齐基准（基于MIC和HelpSteer2数据集）。

Result: 新方法在构建的两个多元对齐基准上验证，结果显示能兼容不同属性和模型，并优于多种现有主流方法，同时提升了决策的解释性和个性化。

Conclusion: 本文提出的基于少样本比较回归的可引导多元对齐方法，有效提升了大语言模型在多元用户偏好对齐中的表现，超越了现有方法。提出的方法兼容不同属性和模型，具有可解释性，并在新提出的基准上表现优异。

Abstract: Large language models (LLMs) are currently aligned using techniques such as
reinforcement learning from human feedback (RLHF). However, these methods use
scalar rewards that can only reflect user preferences on average. Pluralistic
alignment instead seeks to capture diverse user preferences across a set of
attributes, moving beyond just helpfulness and harmlessness. Toward this end,
we propose a steerable pluralistic model based on few-shot comparative
regression that can adapt to individual user preferences. Our approach
leverages in-context learning and reasoning, grounded in a set of fine-grained
attributes, to compare response options and make aligned choices. To evaluate
our algorithm, we also propose two new steerable pluralistic benchmarks by
adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets,
demonstrating the applicability of our approach to value-aligned
decision-making and reward modeling, respectively. Our few-shot comparative
regression approach is interpretable and compatible with different attributes
and LLMs, while outperforming multiple baseline and state-of-the-art methods.
Our work provides new insights and research directions in pluralistic
alignment, enabling a more fair and representative use of LLMs and advancing
the state-of-the-art in ethical AI.

</details>


### [31] [DeCAL Tokenwise Compression](https://arxiv.org/abs/2508.08514)
*Sameer Panwar*

Main category: cs.CL

TL;DR: DeCAL是一种利用预训练去噪的编解码模型进行高质量token级压缩的新方法，实现高压缩与任务性能兼得，适用于多种NLP下游任务。


<details>
  <summary>Details</summary>
Motivation: 数据表示的高效存储和传递是自然语言处理中的重要挑战。现有方法要么压缩率低，要么在重要下游任务上丢失信息。作者旨在改进这一点。

Method: 提出了一种新的token级压缩方法DeCAL。DeCAL以经过去噪预训练的编码器-解码器语言模型为基础，对编码器进行小幅修改，重点提升压缩质量，即使牺牲一定计算效率。

Result: DeCAL在2倍压缩下，在多个下游任务（如问答、摘要、多向量检索）上表现接近原始未压缩水平，即使压缩至8倍，性能下降也很小。

Conclusion: DeCAL能实现高压缩率同时保持高质量表示，在需要预先计算密集表示的场景下，能够大幅节省资源，并具有推广和进一步发展的前景。

Abstract: This paper introduces DeCAL, a new method for tokenwise compression. DeCAL
uses an encoder-decoder language model pretrained with denoising to learn to
produce high-quality, general-purpose compressed representations by the
encoder. DeCAL applies small modifications to the encoder, with the emphasis on
maximizing compression quality, even at the expense of compute. We show that
DeCAL at 2x compression can match uncompressed on many downstream tasks, with
usually only minor dropoff in metrics up to 8x compression, among
question-answering, summarization, and multi-vector retrieval tasks. DeCAL
offers significant savings where pre-computed dense representations can be
utilized, and we believe the approach can be further developed to be more
broadly applicable.

</details>


### [32] [DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives](https://arxiv.org/abs/2508.08591)
*Sehwan Moon,Aram Lee,Jeong Eun Kim,Hee-Ju Kang,Il-Seon Shin,Sung-Wan Kim,Jae-Min Kim,Min Jhon,Ju-Wan Kim*

Main category: cs.CL

TL;DR: DepressLLM利用新自传语料和创新SToPS技术，不仅提升了抑郁症分类性能，还具备可解释性和置信度评估，展示了医学AI在精神健康领域的应用前景。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）已广泛应用，但抑郁症预测受限于缺乏大规模、高质量、严格标注的数据集。因此，本研究的动机是通过构建新的语料和方法，提升抑郁症预测的效果和可信度。

Method: 本研究提出了DepressLLM模型，基于3,699份包含快乐与痛苦的自传性叙述语料进行训练和评估。模型引入了可解释的抑郁预测机制，并采用Score-guided Token Probability Summation（SToPS）模块，以提升分类性能和置信度评估。此外，模型在自建的EMA语料和临床访谈公开数据上进一步测试，并通过精神科专家评估高置信误判案例。

Result: DepressLLM分类性能提升，整体AUC为0.789，高置信度（≥0.95）样本AUC提升至0.904。模型对异质性数据具有鲁棒性。同时高置信误判的精神科分析揭示模型和数据的局限性，为后续改进指明方向。

Conclusion: 可解释AI有助于抑郁症的早期诊断，DepressLLM展示了医学AI在精神病学应用中的潜力，并指出未来优化路径。

Abstract: Advances in large language models (LLMs) have enabled a wide range of
applications. However, depression prediction is hindered by the lack of
large-scale, high-quality, and rigorously annotated datasets. This study
introduces DepressLLM, trained and evaluated on a novel corpus of 3,699
autobiographical narratives reflecting both happiness and distress. DepressLLM
provides interpretable depression predictions and, via its Score-guided Token
Probability Summation (SToPS) module, delivers both improved classification
performance and reliable confidence estimates, achieving an AUC of 0.789, which
rises to 0.904 on samples with confidence $\geq$ 0.95. To validate its
robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets,
including an Ecological Momentary Assessment (EMA) corpus of daily stress and
mood recordings, and on public clinical interview data. Finally, a psychiatric
review of high-confidence misclassifications highlighted key model and data
limitations that suggest directions for future refinements. These findings
demonstrate that interpretable AI can enable earlier diagnosis of depression
and underscore the promise of medical AI in psychiatry.

</details>


### [33] [Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review](https://arxiv.org/abs/2508.08610)
*David Santandreu Calonge,Linda Smail*

Main category: cs.CL

TL;DR: 本综述聚焦参数高效微调方法（如LoRA）在RAG系统中的应用，对提升粤语等方言表达的生成与检索表现进行了系统分析。动态LoRA显著提高效率且保证质量，但语言细腻性与大规模适应性仍存挑战，未来需关注个性化及方言真实性微调。


<details>
  <summary>Details</summary>
Motivation: RAG生成系统（如Qwen3、DeepSeek、Kimi）在理解和生成真实粤语口语表达时面临数据有限与语言变异性高的问题，需要寻找高效参数微调方法来提升其表现。

Method: 评估了PEFT（主要是LoRA）在RAG框架中的集成效果，系统回顾了多种LoRA变体、合成数据生成、用户反馈、参数适配策略，以考察对检索精度、语言真实性及扩展能力的影响，并横向比较了不同微调技术在低资源条件下对语义保真度的提升。

Result: 动态及集成式LoRA方法能在不降低检索和生成质量的前提下显著减少可训练参数，提升计算效率，但在粤语等低资源方言下仍难以完全保留细致语言特征。用户反馈与领域数据集成尚不完善，参数冻结及非线性微调在效率与准确率间有更优权衡，但大规模应用鲁棒性仍待验证。

Conclusion: PEFT增强的RAG系统在特定领域语言任务展现出巨大潜力，但粤语等方言的真实表达、动态适应及可扩展微调流程仍需进一步研究，实现更高的语义保真和系统个性化。

Abstract: This review examines recent advances in Parameter-Efficient Fine-Tuning
(PEFT), with a focus on Low-Rank Adaptation (LoRA), to optimize
Retrieval-Augmented Generation (RAG) systems like Qwen3, DeepSeek, and Kimi.
These systems face challenges in understanding and generating authentic
Cantonese colloquial expressions due to limited annotated data and linguistic
variability. The review evaluates the integration of LoRA within RAG
frameworks, benchmarks PEFT methods for retrieval and generation accuracy,
identify domain adaptation strategies under limited data, and compares
fine-tuning techniques aimed at improving semantic fidelity under data-scarce
conditions. A systematic analysis of recent studies employing diverse LoRA
variants, synthetic data generation, user feedback integration, and adaptive
parameter allocation was conducted to assess their impact on computational
efficiency, retrieval precision, linguistic authenticity, and scalability.
Findings reveal that dynamic and ensemble LoRA adaptations significantly reduce
trainable parameters without sacrificing retrieval accuracy and generation
quality in dialectal contexts. However, limitations remain in fully preserving
fine-grained linguistic nuances, especially for low-resource settings like
Cantonese. The integration of real-time user feedback and domain-specific data
remains underdeveloped, limiting model adaptability and personalization. While
selective parameter freezing and nonlinear adaptation methods offer better
trade-offs between efficiency and accuracy, their robustness at scale remains
an open challenge. This review highlights the promise of PEFT-enhanced RAG
systems for domain-specific language tasks and calls for future work targeting
dialectal authenticity, dynamic adaptation, and scalable fine-tuning pipelines.

</details>


### [34] [InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling](https://arxiv.org/abs/2508.08636)
*Peiji Li,Jiasheng Ye,Yongkang Chen,Yichuan Ma,Zijie Yu,Kedi Chen,Ganqu Cui,Haozhan Li,Jiacheng Chen,Chengqi Lyu,Wenwei Zhang,Linyang Li,Qipeng Guo,Dahua Lin,Bowen Zhou,Kai Chen*

Main category: cs.CL

TL;DR: 文章提出了InternBootcamp，一个多领域推理任务生成和评测框架，支持自动化数据生成和验证，大幅扩展了训练与评测的任务范围。实验证明，通过task scaling显著提升大模型推理能力，助力RL和大语言模型通用性发展。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习主要针对狭窄领域的推理任务，难以应对现实世界的复杂多样环境。为弥补这一不足，作者提出需要一个能支持多领域推理的评测和训练基础设施。

Method: 提出并开发了InternBootcamp，一个包含1000+多领域任务环境的开源框架，支持自动生成无限量、可配置难度的训练/测试数据，并集成自动验证模块。同时，通过自动化agent流程结合人工校验快速扩展任务覆盖范围，还构建了自动生成的Bootcamp-EVAL基准用于综合评估模型性能。

Result: 评测显示，当前最先进的大模型仍在诸多推理任务上表现不佳。利用InternBootcamp训练后，模型性能显著提升，作者的32B模型在Bootcamp-EVAL及其他权威基准上都取得了优异成绩。同时实验证明，增加训练任务数量（task scaling）能持续带来性能提升，且提升幅度跨越两个数量级。

Conclusion: InternBootcamp为多领域推理任务研究提供了核心基础设施。通过自动化流程显著提升任务覆盖，优化了RL下的模型训练和评估。多任务规模扩展（task scaling）是提升推理泛化能力的有效方案，推动多领域通用推理模型的发展。

Abstract: Large language models (LLMs) have revolutionized artificial intelligence by
enabling complex reasoning capabilities. While recent advancements in
reinforcement learning (RL) have primarily focused on domain-specific reasoning
tasks (e.g., mathematics or code generation), real-world reasoning scenarios
often require models to handle diverse and complex environments that
narrow-domain benchmarks cannot fully capture. To address this gap, we present
InternBootcamp, an open-source framework comprising 1000+ domain-diverse task
environments specifically designed for LLM reasoning research. Our codebase
offers two key functionalities: (1) automated generation of unlimited
training/testing cases with configurable difficulty levels, and (2) integrated
verification modules for objective response evaluation. These features make
InternBootcamp fundamental infrastructure for RL-based model optimization,
synthetic data generation, and model evaluation. Although manually developing
such a framework with enormous task coverage is extremely cumbersome, we
accelerate the development procedure through an automated agent workflow
supplemented by manual validation protocols, which enables the task scope to
expand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an
automatically generated benchmark for comprehensive performance assessment.
Evaluation reveals that frontier models still underperform in many reasoning
tasks, while training with InternBootcamp provides an effective way to
significantly improve performance, leading to our 32B model that achieves
state-of-the-art results on Bootcamp-EVAL and excels on other established
benchmarks. In particular, we validate that consistent performance gains come
from including more training tasks, namely \textbf{task scaling}, over two
orders of magnitude, offering a promising route towards capable reasoning
generalist.

</details>


### [35] [Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents](https://arxiv.org/abs/2508.08645)
*Zheng Wu,Heyuan Huang,Yanjia Yang,Yuanyi Song,Xingyu Lou,Weiwen Liu,Weinan Zhang,Jun Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 作者提出了IFRAgent系统，通过结合显性与隐性人类意图，显著提升了移动智能体与人意图的对齐率和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型已经可以推动移动任务自动化，通过模仿人类的图形界面交互来实现移动智能体。但以往的研究大多只关注了人的显性意图（如操作步骤），忽略了隐性意图（如个人偏好），因而难以构建个性化的移动智能体。

Method: 作者提出了IFRAgent框架，能同时分析和建模人类演示中的显性与隐性意图流。具体方法包括：1) 收集MobileIAR数据集，收录与人类意图对齐的操作行为及真实标签；2) 从演示中抽取标准操作步骤SOP，构建查询级向量库；3) 分析用户隐性意图，形成用户习惯库；4) 利用SOP抽取器与检索增强生成，并通过查询重写，针对模糊请求生成个性化的标准操作流程。

Result: IFRAgent在移动任务智能体对人类意图的对齐率上，比其他基线方法平均提高了6.79%（相对提升32.06%），在操作步骤完成率上提升了5.30%（相对提升26.34%）。

Conclusion: 同时关注显性与隐性意图流的IFRAgent能更好理解人类意图，实现个性化的移动任务自动化，显著优于现有方法。

Abstract: As multimodal large language models advance rapidly, the automation of mobile
tasks has become increasingly feasible through the use of mobile-use agents
that mimic human interactions from graphical user interface. To further enhance
mobile-use agents, previous studies employ demonstration learning to improve
mobile-use agents from human demonstrations. However, these methods focus
solely on the explicit intention flows of humans (e.g., step sequences) while
neglecting implicit intention flows (e.g., personal preferences), which makes
it difficult to construct personalized mobile-use agents. In this work, to
evaluate the \textbf{I}ntention \textbf{A}lignment \textbf{R}ate between
mobile-use agents and humans, we first collect \textbf{MobileIAR}, a dataset
containing human-intent-aligned actions and ground-truth actions. This enables
a comprehensive assessment of the agents' understanding of human intent. Then
we propose \textbf{IFRAgent}, a framework built upon \textbf{I}ntention
\textbf{F}low \textbf{R}ecognition from human demonstrations. IFRAgent analyzes
explicit intention flows from human demonstrations to construct a query-level
vector library of standard operating procedures (SOP), and analyzes implicit
intention flows to build a user-level habit repository. IFRAgent then leverages
a SOP extractor combined with retrieval-augmented generation and a query
rewriter to generate personalized query and SOP from a raw ambiguous query,
enhancing the alignment between mobile-use agents and human intent.
Experimental results demonstrate that IFRAgent outperforms baselines by an
average of 6.79\% (32.06\% relative improvement) in human intention alignment
rate and improves step completion rates by an average of 5.30\% (26.34\%
relative improvement). The codes are available at
https://github.com/MadeAgents/Quick-on-the-Uptake.

</details>


### [36] [LLaMA-Based Models for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.08649)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文发现微调后的开源LLM（如Orca~2）在ABSA任务上优于现有方法，但在零样本和少样本场景仍有不足，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）在多种任务中表现出色，但在复合方面的情感分析（ABSA）任务上，其效果不如针对性微调的模型。目前尚未深入探讨针对ABSA微调的LLM的潜力。

Method: 本文主要评估开源LLaMA系列模型在ABSA任务上的微调效果，涉及四个任务和八个英文数据集，并对模型表现进行系统分析，包括误差分析。

Result: 经过微调的Orca~2模型在所有任务上都超过了当前最先进的结果，但在零样本和少样本场景下，所有模型的表现仍显不足。通过误差分析，文章揭示了微调模型面临的具体挑战。

Conclusion: 开源、针对ABSA任务微调的LLM（特别是Orca~2模型），能够在多个任务和数据集上取得超越传统方法的性能。然而，从零样本或少样本学习的角度来看，模型仍需进一步提升。

Abstract: While large language models (LLMs) show promise for various tasks, their
performance in compound aspect-based sentiment analysis (ABSA) tasks lags
behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA
remains unexplored. This paper examines the capabilities of open-source LLMs
fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the
performance across four tasks and eight English datasets, finding that the
fine-tuned Orca~2 model surpasses state-of-the-art results in all tasks.
However, all models struggle in zero-shot and few-shot scenarios compared to
fully fine-tuned ones. Additionally, we conduct error analysis to identify
challenges faced by fine-tuned models.

</details>


### [37] [UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection](https://arxiv.org/abs/2508.08650)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文通过对大语言模型和多语种Transformer模型的结合与优化，显著提升了跨语言情感检测任务的表现，特别是在触发词检测领域取得了第一名的佳绩。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在参与WASSA-2024跨语言情感检测任务，该任务涉及多语言推文中的情感分类与触发词预测，目标是提升跨语言情感分析技术的表现。

Method: 作者提出的方法是对大语言模型Orca~2进行量化与低秩适配（LoRA）微调，并结合多语种Transformer模型（如XLM-R和mT5）。此外，利用机器翻译增强两项子任务的性能，并在第二子任务中采用触发词切换技术。

Result: 所提出的系统在数值型触发词检测上获得第1名，在二值型触发词检测上获得第3名，在情感检测任务上获得第7名，表现优异。

Conclusion: 通过结合先进的大语言模型、多语种Transformer架构及机器翻译等方法，作者系统在跨语言情感检测相关任务中取得了出色的成绩，展示了其在多语种情感分析场景的有效性和实用性。

Abstract: This paper presents our system built for the WASSA-2024 Cross-lingual Emotion
Detection Shared Task. The task consists of two subtasks: first, to assess an
emotion label from six possible classes for a given tweet in one of five
languages, and second, to predict words triggering the detected emotions in
binary and numerical formats. Our proposed approach revolves around fine-tuning
quantized large language models, specifically Orca~2, with low-rank adapters
(LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We
enhance performance through machine translation for both subtasks and trigger
word switching for the second subtask. The system achieves excellent
performance, ranking 1st in numerical trigger words detection, 3rd in binary
trigger words detection, and 7th in emotion detection.

</details>


### [38] [Prompt-Based Approach for Czech Sentiment Analysis](https://arxiv.org/abs/2508.08651)
*Jakub Šmíd,Pavel Přibáň*

Main category: cs.CL

TL;DR: 本研究首次在捷克语领域提出基于prompt的多方面情感分析与分类方法，显著优于传统微调，尤其在零样本和少样本场景下表现突出，目标领域预训练能进一步提升零样本任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前捷克语领域在细粒度情感分析和情感分类上缺乏基于prompt的方法，传统微调方式在样本有限的场景下表现不佳，因此需要寻找更高效的解决方案。

Method: 采用序列到序列的模型，首次在捷克语领域引入基于prompt的方法，实现多方面情感分析与分类。同时对比了prompt方法和传统微调方法，并进行了零样本、少样本学习实验。还探讨了目标领域预训练对零样本任务的提升作用。

Result: prompt方法在捷克语的情感分析和分类中效果优于传统微调，尤其在训练样本有限时表现更佳。目标领域数据的预训练显著提升了零样本场景下的效果。

Conclusion: 基于prompt的方法在捷克语情感分析和分类任务中具有显著优势，特别适用于低资源场景，目标领域预训练进一步提升了模型表现。

Abstract: This paper introduces the first prompt-based methods for aspect-based
sentiment analysis and sentiment classification in Czech. We employ the
sequence-to-sequence models to solve the aspect-based tasks simultaneously and
demonstrate the superiority of our prompt-based approach over traditional
fine-tuning. In addition, we conduct zero-shot and few-shot learning
experiments for sentiment classification and show that prompting yields
significantly better results with limited training examples compared to
traditional fine-tuning. We also demonstrate that pre-training on data from the
target domain can lead to significant improvements in a zero-shot scenario.

</details>


### [39] [LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement](https://arxiv.org/abs/2508.08653)
*Rajmohan C,Sarthak Harne,Arvind Agarwal*

Main category: cs.CL

TL;DR: 本文提出利用LLM进行文本转表的新框架，通过任务分解和迭代自反馈显著提升了转表结果质量，在公开数据集上表现优于传统基线，并分析了性能与成本的权衡。


<details>
  <summary>Details</summary>
Motivation: 将非结构化文本转化为结构化数据（表格）是一个复杂任务，需要语义理解、推理及结构认知，但当前大语言模型（LLMs）在处理领域专用、含糊或数值推理场景，以及维护表格结构和处理长文本时仍有不少限制。该研究旨在寻找提升文本转表质量和效率的新方法。

Method: 作者提出了LLM驱动的文本转表格系统，并采用了两项创新策略：1）把整体转表任务拆解为可控、分阶段的子任务，2）通过模型自我迭代反馈来优化生成的表格结果。这种任务分解结合自反馈机制，允许模型逐步处理并不断提升结果质量，同时分析了性能提升与计算消耗之间的权衡。

Result: 实验结果显示，所提出方法在两个公开复杂的文本转表数据集上都优于传统基线方法，实现了更高质量的表格构建。

Conclusion: 通过任务拆解和自我反馈技术，显著提升了大语言模型在复杂文本转表任务中的性能，尤其是在结构保持、数值推理及复杂文本处理方面表现突出，证明了方法的有效性与实用性，但也需权衡性能与计算成本。

Abstract: Transforming unstructured text into structured data is a complex task,
requiring semantic understanding, reasoning, and structural comprehension.
While Large Language Models (LLMs) offer potential, they often struggle with
handling ambiguous or domain-specific data, maintaining table structure,
managing long inputs, and addressing numerical reasoning. This paper proposes
an efficient system for LLM-driven text-to-table generation that leverages
novel prompting techniques. Specifically, the system incorporates two key
strategies: breaking down the text-to-table task into manageable, guided
sub-tasks and refining the generated tables through iterative self-feedback. We
show that this custom task decomposition allows the model to address the
problem in a stepwise manner and improves the quality of the generated table.
Furthermore, we discuss the benefits and potential risks associated with
iterative self-feedback on the generated tables while highlighting the
trade-offs between enhanced performance and computational cost. Our methods
achieve strong results compared to baselines on two complex text-to-table
generation datasets available in the public domain.

</details>


### [40] [TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation](https://arxiv.org/abs/2508.08680)
*Armel Zebaze,Benoît Sagot,Rachel Bawden*

Main category: cs.CL

TL;DR: 本文提出TopXGen，一种用大模型生成多样高质量低资源语言文本并反向翻译的新数据扩充方法，有效提升LLM低资源机器翻译表现。


<details>
  <summary>Details</summary>
Motivation: 大模型（LLMs）在有资源语言机器翻译（MT）上表现优异，尤其使用上下文学习（ICL）时。然而，在翻译低资源语言（LRLs）时效果不佳。原有的解决办法如相似性搜索选例、监督微调受限于并行语料的质量和多样性。低资源翻译常用的合成数据技术（如反向翻译）又需高质量目标语文本，而这在低资源语言中常常不可得。

Method: 提出TopXGen，一种基于LLM的新方法，用于生成多种低资源语言高质量且话题多样的目标语文本，这些文本可通过反向翻译生成有用、丰富的并行语料，进而用于ICL和微调。TopXGen利用LLM擅长HRL翻译和多语言能力，先生成自然流畅的LRL目标语文本，再翻译回高资源源语。

Result: TopXGen所生成的数据能在微调和上下文学习阶段提升LLM在低资源语言机器翻译上的表现。

Conclusion: 通过TopXGen生成和利用多样化的低资源语言语料，可以显著增强大模型翻译系统在低资源语言方向上的性能。

Abstract: LLMs have been shown to perform well in machine translation (MT) with the use
of in-context learning (ICL), rivaling supervised models when translating into
high-resource languages (HRLs). However, they lag behind when translating into
low-resource language (LRLs). Example selection via similarity search and
supervised fine-tuning help. However the improvements they give are limited by
the size, quality and diversity of existing parallel datasets. A common
technique in low-resource MT is synthetic parallel data creation, the most
frequent of which is backtranslation, whereby existing target-side texts are
automatically translated into the source language. However, this assumes the
existence of good quality and relevant target-side texts, which are not readily
available for many LRLs. In this paper, we present \textsc{TopXGen}, an
LLM-based approach for the generation of high quality and topic-diverse data in
multiple LRLs, which can then be backtranslated to produce useful and diverse
parallel texts for ICL and fine-tuning. Our intuition is that while LLMs
struggle to translate into LRLs, their ability to translate well into HRLs and
their multilinguality enable them to generate good quality, natural-sounding
target-side texts, which can be translated well into a high-resource source
language. We show that \textsc{TopXGen} boosts LLM translation performance
during fine-tuning and in-context learning. Code and outputs are available at
https://github.com/ArmelRandy/topxgen.

</details>


### [41] [Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults](https://arxiv.org/abs/2508.08684)
*Bram van Dijk,Tiberon Kuiper,Sirin Aoulad si Ahmed,Armel Levebvre,Jake Johnson,Jan Duin,Simon Mooijaart,Marco Spruit*

Main category: cs.CL

TL;DR: 通用ASR模型在荷兰老年人语音识别中表现优异，缩减模型结构可优化准确率与响应速度的权衡，但特殊场景仍面临识别错误。


<details>
  <summary>Details</summary>
Motivation: 目前自动语音识别（ASR）在临床环境中支持老年人仍存在准确率瓶颈，尤其是针对代表性不足群体（如荷兰老年人）。为了改善人机交互体验，需要评估现有ASR模型在这一群体中的表现。

Method: 评估了主流多语种ASR模型和专为老年人荷兰语微调的模型在Welzijn.AI老年人聊天机器人中的表现，并且对模型在处理速度和准确率上的权衡进行考察。

Result: 通用多语种ASR模型在准确性上优于针对老年人荷兰语微调的模型，表明最新ASR具有良好的泛化能力。同时，缩减模型架构有助于提升处理速度，但部分情况下因幻想现象导致高词错误率（WER）。

Conclusion: 最先进的ASR模型无需微调即可较好地适应老年人实际对话数据，但在一些极端情况下准确率仍需改进。截断模型有助于提高响应速度，实现准确率与速度的平衡。

Abstract: Voice-controlled interfaces can support older adults in clinical contexts,
with chatbots being a prime example, but reliable Automatic Speech Recognition
(ASR) for underrepresented groups remains a bottleneck. This study evaluates
state-of-the-art ASR models on language use of older Dutch adults, who
interacted with the Welzijn.AI chatbot designed for geriatric contexts. We
benchmark generic multilingual ASR models, and models fine-tuned for Dutch
spoken by older adults, while also considering processing speed. Our results
show that generic multilingual models outperform fine-tuned models, which
suggests recent ASR models can generalise well out of the box to realistic
datasets. Furthermore, our results suggest that truncating existing
architectures is helpful in balancing the accuracy-speed trade-off, though we
also identify some cases with high WER due to hallucinations.

</details>


### [42] [A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models](https://arxiv.org/abs/2508.08712)
*Lingzhe Zhang,Liancheng Fang,Chiming Duan,Minghua He,Leyi Pan,Pei Xiao,Shiyu Huang,Yunpeng Zhai,Xuming Hu,Philip S. Yu,Aiwei Liu*

Main category: cs.CL

TL;DR: 本文系统综述了并行文本生成技术，归纳为AR和非AR两类，详细评估了各自的加速效果及质量权衡，并提出未来研究的关键挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 主流的LLM文本生成采用自回归方式，生成速度受到顺序生成的制约，影响推理效率。并行生成能突破这一限制，但领域内缺乏系统性梳理与分析，亟需总结归纳并评估现有技术与发展前景。

Method: 本文通过系统性调研，首先将现有并行文本生成方法归纳为自回归和非自回归两大类，详述各类核心技术。随后从理论上分析各种方法在速度、质量与推理效率上的权衡，并对这些技术与其他加速方法的结合及对比进行审查。最后，根据调研整理出最新进展、存在的挑战与未来发展方向。

Result: 现有并行文本生成方法已被系统分类与评估，发现各类方法在加速推理的同时存在质量和效率间的权衡。调研明确未来研究方向，包括融合不同技术、改进生成质量和实际部署挑战。

Conclusion: 并行文本生成方法正在逐步突破目前主流自回归（AR）生成在速度上的瓶颈，实现更高效的推理。尽管已有许多创新技术，但仍面临融合不同方法、保持生成质量等挑战，需进一步研究和完善。

Abstract: As text generation has become a core capability of modern Large Language
Models (LLMs), it underpins a wide range of downstream applications. However,
most existing LLMs rely on autoregressive (AR) generation, producing one token
at a time based on previously generated context-resulting in limited generation
speed due to the inherently sequential nature of the process. To address this
challenge, an increasing number of researchers have begun exploring parallel
text generation-a broad class of techniques aimed at breaking the
token-by-token generation bottleneck and improving inference efficiency.
Despite growing interest, there remains a lack of comprehensive analysis on
what specific techniques constitute parallel text generation and how they
improve inference performance. To bridge this gap, we present a systematic
survey of parallel text generation methods. We categorize existing approaches
into AR-based and Non-AR-based paradigms, and provide a detailed examination of
the core techniques within each category. Following this taxonomy, we assess
their theoretical trade-offs in terms of speed, quality, and efficiency, and
examine their potential for combination and comparison with alternative
acceleration strategies. Finally, based on our findings, we highlight recent
advancements, identify open challenges, and outline promising directions for
future research in parallel text generation.

</details>


### [43] [IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization](https://arxiv.org/abs/2508.08719)
*Yuzhuo Bai,Shitong Duan,Muhua Huang,Jing Yao,Zhenghao Liu,Peng Zhang,Tun Lu,Xiaoyuan Yi,Maosong Sun,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出IROTE方法，通过优化自我反思文本，实现LLM对目标性格特质的稳定、可转移模拟，实验结果显著优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能让LLM模仿表面的风格，无法精准且持续地体现期望的性格特质，而人类行为受身份相关的自我反思驱动，因此引入自我反思机制。

Method: 提出了IROTE，一种基于心理学理论的上下文方法，通过自动生成和优化自我反思文本，将目标性格特质内化到LLM输出中。优化过程基于信息论目标，无需微调。

Result: 在三个性格特质体系、多项任务上的实验显示，IROTE生成的单个自我反思能让LLM稳定模拟目标特质，并显著优于强基线方法。

Conclusion: IROTE方法能有效克服LLM在性格特质模拟上的浅层和不稳定问题，实现稳定和可转移的特质引出。

Abstract: Trained on various human-authored corpora, Large Language Models (LLMs) have
demonstrated a certain capability of reflecting specific human-like traits
(e.g., personality or values) by prompting, benefiting applications like
personalized LLMs and social simulations. However, existing methods suffer from
the superficial elicitation problem: LLMs can only be steered to mimic shallow
and unstable stylistic patterns, failing to embody the desired traits precisely
and consistently across diverse tasks like humans. To address this challenge,
we propose IROTE, a novel in-context method for stable and transferable trait
elicitation. Drawing on psychological theories suggesting that traits are
formed through identity-related reflection, our method automatically generates
and optimizes a textual self-reflection within prompts, which comprises
self-perceived experience, to stimulate LLMs' trait-driven behavior. The
optimization is performed by iteratively maximizing an information-theoretic
objective that enhances the connections between LLMs' behavior and the target
trait, while reducing noisy redundancy in reflection without any fine-tuning,
leading to evocative and compact trait reflection. Extensive experiments across
three human trait systems manifest that one single IROTE-generated
self-reflection can induce LLMs' stable impersonation of the target trait
across diverse downstream tasks beyond simple questionnaire answering,
consistently outperforming existing strong baselines.

</details>


### [44] [Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation](https://arxiv.org/abs/2508.08730)
*Weibin Liao,Tianlong Wang,Yinghao Zhu,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.CL

TL;DR: 本文提出的Magical架构针对多源异构医疗科普文本生成任务，相较传统方法语义更忠实、风格更丰富且参数更省，在实际数据集上效果突出。


<details>
  <summary>Details</summary>
Motivation: 当前医疗科普语言生成（MLLG）领域通过大语言模型提升专业内容的可理解性，但主流的低秩适配（LoRA）等方法在多源异构数据场景下难以保证语义保真及风格多样性。

Method: 提出一种专为异构数据设计的不对称LoRA架构Magical，采用抽象总结的共享矩阵A和多样风格生成的独立矩阵B，并用语义不变约束保护A的语义一致性，同时通过推荐引导切换接口，促使模型灵活切换不同的B以适应风格需求。

Result: 在三组真实数据集上，Magical不仅在文本可读性、语义忠实度和风格多样性等指标上超越了传统prompt方法、标准LoRA以及其最新变体，还将可训练参数量减少了31.66%。

Conclusion: Magical显著提升了医疗科普语言生成的语义保真性和风格多样性，在多源异构场景下表现优越且更高效。

Abstract: Medical Lay Language Generation (MLLG) plays a vital role in improving the
accessibility of complex scientific content for broader audiences. Recent
literature to MLLG commonly employ parameter-efficient fine-tuning methods such
as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using
paired expert-lay language datasets. However, LoRA struggles with the
challenges posed by multi-source heterogeneous MLLG datasets. Specifically,
through a series of exploratory experiments, we reveal that standard LoRA fail
to meet the requirement for semantic fidelity and diverse lay-style generation
in MLLG task. To address these limitations, we propose Magical, an asymmetric
LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical
employs a shared matrix $A$ for abstractive summarization, along with multiple
isolated matrices $B$ for diverse lay-style generation. To preserve semantic
fidelity during the lay language generation process, Magical introduces a
Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix
$A$. Furthermore, to better adapt to diverse lay-style generation, Magical
incorporates the Recommendation-guided Switch, an externally interface to
prompt the LLM to switch between different matrices $B$. Experimental results
on three real-world lay language generation datasets demonstrate that Magical
consistently outperforms prompt-based methods, vanilla LoRA, and its recent
variants, while also reducing trainable parameters by 31.66%.

</details>


### [45] [SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs](https://arxiv.org/abs/2508.08742)
*Haotian Chen,Qingqing Long,Meng Xiao,Xiao Luo,Wei Ju,Chengrui Wang,Xuezhi Wang,Yuanchun Zhou,Hengshu Zhu*

Main category: cs.CL

TL;DR: 本文提出了第一个专门评估科学文献问答系统中RAG-LLMs重排序器性能的基准SciRerankBench，通过三类多样化问答样本和系统性实验证明了各重排序器的优劣，为未来相关系统优化提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 当前科学文献问答系统中，基于两阶段检索增强生成（RAG）的大语言模型表现突出，尤其是重排序器（reranker）对于科学领域提出的高要求（如术语微妙差异）的正确回答至关重要。然而，这些方法的潜力与局限性仍未被系统地评估。

Method: 提出了一个专为RAG-LLMs中的重排序器评估而设计的基准集SciRerankBench，涵盖五个科学领域。基准基于三类问题-上下文-答案（Q-C-A）组合：噪声上下文（NC）、语义相似但逻辑无关的上下文（SSLI）、反事实上下文（CC），并对13种常用重排序器在五类LLMs上进行了系统评测。

Result: 系统评测揭示了各类重排序器在抗噪能力、相关性消歧与事实一致性等方面的相对优缺点，为RAG-LLMs中重排序器的未来优化提供了细致观察和指导。SciRerankBench是首个专门评估RAG-LLMs重排序器的基准。

Conclusion: SciRerankBench首次为RAG-LLMs系统下的重排序器提供了细致且系统化的评测工具与数据，揭示了现有方法的能力边界，并为后续研究提供了参考和方向。

Abstract: Scientific literature question answering is a pivotal step towards new
scientific discoveries. Recently, \textit{two-stage} retrieval-augmented
generated large language models (RAG-LLMs) have shown impressive advancements
in this domain. Such a two-stage framework, especially the second stage
(reranker), is particularly essential in the scientific domain, where subtle
differences in terminology may have a greatly negative impact on the final
factual-oriented or knowledge-intensive answers. Despite this significant
progress, the potential and limitations of these works remain unexplored. In
this work, we present a Scientific Rerank-oriented RAG Benchmark
(SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning
five scientific subjects. To rigorously assess the reranker performance in
terms of noise resilience, relevance disambiguation, and factual consistency,
we develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy
Contexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI),
and Counterfactual Contexts (CC). Through systematic evaluation of 13 widely
used rerankers on five families of LLMs, we provide detailed insights into
their relative strengths and limitations. To the best of our knowledge,
SciRerankBench is the first benchmark specifically developed to evaluate
rerankers within RAG-LLMs, which provides valuable observations and guidance
for their future development.

</details>


### [46] [DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation](https://arxiv.org/abs/2508.08761)
*Stavros Doropoulos,Stavros Vologiannidis,Ioannis Magnisalis*

Main category: cs.CL

TL;DR: 该论文针对IT项目治理中非结构化对话转结构化信息关键难题，提出了DevNous自动化系统，显著提升了任务处理效率；并创建了领域首个公开基准，为相关AI应用和研究提供了坚实基础。


<details>
  <summary>Details</summary>
Motivation: 在IT项目治理中，团队非结构化对话转为结构化管理数据是关键瓶颈，人工处理效率低、易出错，急需自动化方案提高信息系统管理效率。

Method: 提出DevNous系统，基于大语言模型（LLM）的多智能体专家系统。该系统集成于团队聊天环境，自动识别对话意图，支持状态管理、多轮任务，如自动任务正式化和进展摘要合成。建立了160轮真实交互式对话的基准数据集，并手动注释多标签真实值，作为量化评估依据。

Result: 在基准评测中，DevNous系统实现了81.3%的turn准确率和0.845的multiset F1分数，证明了系统管用且效果明显。

Conclusion: 本工作主要贡献是：1）提出并验证了用于团队环境的架构模式，可开发新型“环境型”行政智能体；2）公开首个有鲁棒性、实证基准的数据集，为该领域研究提供了标准与基础。

Abstract: The manual translation of unstructured team dialogue into the structured
artifacts required for Information Technology (IT) project governance is a
critical bottleneck in modern information systems management. We introduce
DevNous, a Large Language Model-based (LLM) multi-agent expert system, to
automate this unstructured-to-structured translation process. DevNous
integrates directly into team chat environments, identifying actionable intents
from informal dialogue and managing stateful, multi-turn workflows for core
administrative tasks like automated task formalization and progress summary
synthesis. To quantitatively evaluate the system, we introduce a new benchmark
of 160 realistic, interactive conversational turns. The dataset was manually
annotated with a multi-label ground truth and is publicly available. On this
benchmark, DevNous achieves an exact match turn accuracy of 81.3\% and a
multiset F1-Score of 0.845, providing strong evidence for its viability. The
primary contributions of this work are twofold: (1) a validated architectural
pattern for developing ambient administrative agents, and (2) the introduction
of the first robust empirical baseline and public benchmark dataset for this
challenging problem domain.

</details>


### [47] [Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering](https://arxiv.org/abs/2508.08785)
*Yunfeng Ning,Mayi Xu,Jintao Wen,Qiankun Pi,Yuanyuan Zhu,Ming Zhong,Jiawei Jiang,Tieyun Qian*

Main category: cs.CL

TL;DR: 论文提出保护知识图谱实体隐私的新型RAG系统ARoG，通过抽象化策略解决匿名实体的语义丧失和检索难题，实验验证了其隐私保护和检索性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常因知识过时或不完整而产生幻觉。RAG技术通过整合外部知识如知识图谱（KGs）来缓解这些问题。但在利用私有KGs时，存在隐私泄露风险，尤其是使用第三方API时更缺乏可控性。本文首次关注保护实体匿名的隐私RAG场景。

Method: 提出了新颖的ARoG框架，包括以关系为中心的抽象和以结构为导向的抽象两种策略：（1）把匿名实体通过邻接关系语义动态抽象为高层概念，补充实体语义帮助检索；（2）将自然语言问题转化为结构化的抽象概念路径，以更好地与KG中的抽象概念对齐，提升检索性能；整个流程严格保护KG实体隐私。

Result: 在三个数据集上的实验表明，ARoG框架在确保隐私的同时，实现了很强的检索表现和隐私鲁棒性。

Conclusion: ARoG方法可在保护知识图谱隐私的条件下，实现知识有效对齐与检索，为隐私保护型RAG系统提供了新思路。

Abstract: LLMs often suffer from hallucinations and outdated or incomplete knowledge.
RAG is proposed to address these issues by integrating external knowledge like
that in KGs into LLMs. However, leveraging private KGs in RAG systems poses
significant privacy risks due to the black-box nature of LLMs and potential
insecure data transmission, especially when using third-party LLM APIs lacking
transparency and control. In this paper, we investigate the privacy-protected
RAG scenario for the first time, where entities in KGs are anonymous for LLMs,
thus preventing them from accessing entity semantics. Due to the loss of
semantics of entities, previous RAG systems cannot retrieve question-relevant
knowledge from KGs by matching questions with the meaningless identifiers of
anonymous entities. To realize an effective RAG system in this scenario, two
key challenges must be addressed: (1) How can anonymous entities be converted
into retrievable information. (2) How to retrieve question-relevant anonymous
entities. Hence, we propose a novel ARoG framework including relation-centric
abstraction and structure-oriented abstraction strategies. For challenge (1),
the first strategy abstracts entities into high-level concepts by dynamically
capturing the semantics of their adjacent relations. It supplements meaningful
semantics which can further support the retrieval process. For challenge (2),
the second strategy transforms unstructured natural language questions into
structured abstract concept paths. These paths can be more effectively aligned
with the abstracted concepts in KGs, thereby improving retrieval performance.
To guide LLMs to effectively retrieve knowledge from KGs, the two strategies
strictly protect privacy from being exposed to LLMs. Experiments on three
datasets demonstrate that ARoG achieves strong performance and
privacy-robustness.

</details>


### [48] [Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments](https://arxiv.org/abs/2508.08791)
*Junjie Ye,Changhao Jiang,Zhengyin Du,Yufei Xu,Xuesong Yao,Zhiheng Xi,Xiaoran Fan,Qi Zhang,Xuanjing Huang,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文提出了自动化环境和新型奖励机制，解决了大模型工具使用的RL训练难题，在各类LLM上显著提升了工具使用能力，并且无伤模型总体性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在工具使用方面进展有限，主要受限于缺乏高效的强化学习框架，尤其是在稳定环境构建和奖励机制设计上存在难点。

Method: 开发了自动化环境构建流水线，包括场景分解、文档生成、功能集成、复杂度控制和本地化部署；设计了可验证的奖励机制，综合评价工具使用的精确性和任务完成度，并与标准强化学习算法结合。

Result: 实验展示，不论模型规模、推理模式或训练算法，方法都能大幅提升工具使用表现，并通过调整模型底层MLP参数，增强了上下文理解和推理能力。

Conclusion: 提出的环境构建流程和奖励机制能够显著提升大语言模型使用工具的能力，同时不损害其通用性能。

Abstract: Effective tool use is essential for large language models (LLMs) to interact
meaningfully with their environment. However, progress is limited by the lack
of efficient reinforcement learning (RL) frameworks specifically designed for
tool use, due to challenges in constructing stable training environments and
designing verifiable reward mechanisms. To address this, we propose an
automated environment construction pipeline, incorporating scenario
decomposition, document generation, function integration, complexity scaling,
and localized deployment. This enables the creation of high-quality training
environments that provide detailed and measurable feedback without relying on
external tools. Additionally, we introduce a verifiable reward mechanism that
evaluates both the precision of tool use and the completeness of task
execution. When combined with trajectory data collected from the constructed
environments, this mechanism integrates seamlessly with standard RL algorithms
to facilitate feedback-driven model training. Experiments on LLMs of varying
scales demonstrate that our approach significantly enhances the models'
tool-use performance without degrading their general capabilities, regardless
of inference modes or training algorithms. Our analysis suggests that these
gains result from improved context understanding and reasoning, driven by
updates to the lower-layer MLP parameters in models.

</details>


### [49] [TiMoE: Time-Aware Mixture of Language Experts](https://arxiv.org/abs/2508.08827)
*Robin Faro,Dongyang Fan,Tamar Alphaidze,Martin Jaggi*

Main category: cs.CL

TL;DR: 本工作提出按时间片段预训练与因果专家路由的新方法，显著减少语言模型预测时对未来知识的依赖，实现了更加时间一致的输出，同时保持整体任务表现，开源相关模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通常训练于固定网络快照，这导致其知识随时间变陈旧，并且在预测时存在时间泄露风险，即依赖于查询时还未发生的未来信息。为解决这个问题，需要一种新的训练和推理方法，使模型能严格按时间有效地回答问题。

Method: 作者提出了一种新的预训练方案：将2013-2024年度的大型语料库分成不重叠的两年片段，并在每个时间段内从零开始分别训练GPT风格的“专家”模型。在推理阶段，提出了TiMoE（Time-aware Mixture of Experts），可以根据查询的时间戳屏蔽掉那些训练结束时间晚于该时间戳的专家，仅合并有效专家的输出结果以保证严格的时间因果性。

Result: 作者还推出了TSQA基准，拥有1万条问题及对应的答案时间标注，用于精细测量“时间幻觉”误差。实验结果表明，TiMoE在八项标准NLP任务加TSQA上，表现与最佳单一时间段专家相当或更佳，并最多可将未来知识相关的错误降低15%。

Conclusion: 论文证明了按时间分割模块化预训练、结合因果专家路由，是实现时间可靠且通用性能无明显损失的LLM的简单高效途径。代码已开源。

Abstract: Large language models (LLMs) are typically trained on fixed snapshots of the
web, which means that their knowledge becomes stale and their predictions risk
temporal leakage: relying on information that lies in the future relative to a
query. We tackle this problem by pre-training from scratch a set of GPT-style
experts on disjoint two-year slices of a 2013-2024 corpus and combining them
through TiMoE, a Time-aware Mixture of Language Experts. At inference time,
TiMoE masks all experts whose training window ends after the query timestamp
and merges the remaining log-probabilities in a shared space, guaranteeing
strict causal validity while retaining the breadth of multi-period knowledge.
We also release TSQA, a 10k-question benchmark whose alternatives are
explicitly labelled as past, future or irrelevant, allowing fine-grained
measurement of temporal hallucinations. Experiments on eight standard NLP tasks
plus TSQA show that a co-adapted TiMoE variant matches or exceeds the best
single-period expert and cuts future-knowledge errors by up to 15%. Our results
demonstrate that modular, time-segmented pre-training paired with causal
routing is a simple yet effective path toward LLMs that stay chronologically
grounded without sacrificing general performance much. We open source our code
at TiMoE (Github): https://github.com/epfml/TiMoE

</details>


### [50] [An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems](https://arxiv.org/abs/2508.08833)
*Yuren Hao,Xiang Wan,Chengxiang Zhai*

Main category: cs.CL

TL;DR: 本文提出用数学等价但表述不同的高级题目测试LLM推理鲁棒性，创建了PutnamGAP新基准，实际评测显示现有模型在面对变体时表现显著下降，新方法能有效揭示和提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法评估大语言模型（LLM）数学推理鲁棒性存在局限，因此亟需更系统的评测框架，以探究模型对高级数学问题在语言和参数变换下的表现。

Method: 提出了一个系统的评测框架，通过对数学上等价但在语言和参数上有变化的高级数学竞赛题进行压力测试，从而测量LLM对非数学扰动的敏感性。创建了新的基准数据集PutnamGAP，该数据集包含同一问题的多种数学等价变体，并用其评估了多种主流LLM的鲁棒性表现。

Result: 在18种商用及开源模型上的评测显示，几乎所有模型在这些变体上的性能都有明显下降。例如OpenAI的O3模型在原版题目上得分49%，但在表面变体和核心步骤变体上，得分分别下降4和10.5个百分点，更小的模型降幅更甚。

Conclusion: 新评测方法能够更深入理解LLM的数学推理鲁棒性，提供新的洞见以促进模型能力提升。

Abstract: In this paper, we introduce a systematic framework beyond conventional method
to assess LLMs' mathematical-reasoning robustness by stress-testing them on
advanced math problems that are mathematically equivalent but with linguistic
and parametric variation. These transformations allow us to measure the
sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more
accurate evaluation of their mathematical reasoning capabilities. Using this
new evaluation methodology, we created PutnamGAP, a new benchmark dataset with
multiple mathematically-equivalent variations of competition-level math
problems. With the new dataset, we evaluate multiple families of representative
LLMs and examine their robustness. Across 18 commercial and open-source models
we observe sharp performance degradation on the variants. OpenAI's flagship
reasoning model, O3, scores 49 % on the originals but drops by 4 percentage
points on surface variants, and by 10.5 percentage points on core-step-based
variants, while smaller models fare far worse. Overall, the results show that
the proposed new evaluation methodology is effective for deepening our
understanding of the robustness of LLMs and generating new insights for further
improving their mathematical reasoning capabilities.

</details>


### [51] [Steering Towards Fairness: Mitigating Political Bias in LLMs](https://arxiv.org/abs/2508.08846)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文针对LLMs编码的政治经济偏见，提出了内部表示分析和分层偏见缓解框架，通过对比样本和激活分析发现模型内部存在系统性偏见，并通过向量引导法实现有效去偏，丰富了LLMs偏见识别和治理的方法论。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）广泛应用于实际场景，但它们容易编码并复制意识形态偏见，尤其是在政治和经济层面，引发了人们的广泛关注。

Method: 提出一种框架，通过对解码型LLMs内部表示进行分析，探查和减轻意识形态偏见。基于Political Compass Test（PCT），利用对比样本提取和比较模型隐藏层激活，设计了跨多个意识形态维度的激活提取流程，可分层分析，揭示与政治立场相关的表现差异。

Result: 实验发现，解码型LLMs系统性地在各层编码了可识别的代表性偏见，可通过“steering vector”方法实现有效缓解。

Conclusion: 本工作揭示了LLMs如何在内部编码政治偏见，并提出了一种比仅针对输出结果更具原理性的去偏新方法，提升了解决模型偏见问题的理论与实践基础。

Abstract: Recent advancements in large language models (LLMs) have enabled their
widespread use across diverse real-world applications. However, concerns remain
about their tendency to encode and reproduce ideological biases, particularly
along political and economic dimensions. In this paper, we propose a framework
for probing and mitigating such biases in decoder-based LLMs through analysis
of internal model representations. Grounded in the Political Compass Test
(PCT), our method uses contrastive pairs to extract and compare hidden layer
activations from models like Mistral and DeepSeek. We introduce a comprehensive
activation extraction pipeline capable of layer-wise analysis across multiple
ideological axes, revealing meaningful disparities linked to political framing.
Our results show that decoder LLMs systematically encode representational bias
across layers, which can be leveraged for effective steering vector-based
mitigation. This work provides new insights into how political bias is encoded
in LLMs and offers a principled approach to debiasing beyond surface-level
output interventions.

</details>


### [52] [BiasGym: Fantastic Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855)
*Sekh Mainul Islam,Nadav Borenstein,Siddhesh Milind Pawar,Haeun Yu,Arnav Arora,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 文章提出了BiasGym框架，通过微调和机制分析系统性地注入、检测和缓解LLM中的偏见，方法高效、通用且不损害模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型存在隐藏的偏见和刻板印象，这些偏见难以被系统性分析和有效缓解，亟需一种通用且高效的处理框架。

Method: 提出BiasGym框架，包括BiasInject（通过token级微调将指定偏见注入冻结模型）和BiasScope（分析并利用注入信号识别和引导导致偏见的模型组件）。

Result: BiasGym能稳定地诱导偏见用于机制分析，并支持有针对性的去偏，不影响模型下游任务性能，且可泛化到训练时未见的其他偏见。

Conclusion: BiasGym为偏见机制分析和目标去偏提供了可靠工具，既能增强模型安全性，也促进模型可解释性研究。

Abstract: Understanding biases and stereotypes encoded in the weights of Large Language
Models (LLMs) is crucial for developing effective mitigation strategies. Biased
behaviour is often subtle and non-trivial to isolate, even when deliberately
elicited, making systematic analysis and debiasing particularly challenging. To
address this, we introduce BiasGym, a simple, cost-effective, and generalizable
framework for reliably injecting, analyzing, and mitigating conceptual
associations within LLMs. BiasGym consists of two components: BiasInject, which
injects specific biases into the model via token-based fine-tuning while
keeping the model frozen, and BiasScope, which leverages these injected signals
to identify and steer the components responsible for biased behavior. Our
method enables consistent bias elicitation for mechanistic analysis, supports
targeted debiasing without degrading performance on downstream tasks, and
generalizes to biases unseen during training. We demonstrate the effectiveness
of BiasGym in reducing real-world stereotypes (e.g., people from a country
being `reckless drivers') and in probing fictional associations (e.g., people
from a country having `blue skin'), showing its utility for both safety
interventions and interpretability research.

</details>


### [53] [Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance](https://arxiv.org/abs/2508.08876)
*Kaiyu Wang,Lin Mu,Zhiyao Yang,Ximing Li,Xiaotang Zhou Wanfu Gao,Huimao Zhang*

Main category: cs.CL

TL;DR: Sqator通过对报告之间细粒度文本跨度的自动分析，实现了对放射科报告质量的准确评估，具有节省人力和提升评分一致性的优势。


<details>
  <summary>Details</summary>
Motivation: 目前影像诊断报告的质控通常依赖高级医生对初级医生报告进行人工评分，这不仅消耗大量人力，还可能因主观偏见或医生水平等因素导致评分不准确。为了解决这些问题，需要一种自动化且细粒度的质控方法。

Method: 本文提出了一种基于跨度级别的质量评估方法（Sqator）。Sqator通过分析初级与高级报告之间的文本跨度修订，将每个修订部分的重要性进行打分，最后合并所有修订跨度的得分，生成最终的QA评分。这种方法区别于传统的文档级语义比较，更关注于细粒度文本差异。

Result: 在12,013份放射科报告上的实验结果显示，Sqator可以实现与高级医生评分相当的QA表现。同时，Sqator对修订跨度重要性的打分也与高级医生的判断保持了一致性。

Conclusion: 提出的Sqator方法能够自动细致地对放射科初级医生报告进行质量评分，降低人工成本，且结果具有较高的准确性和一致性。

Abstract: Quality Assurance (QA) for radiology reports refers to judging whether the
junior reports (written by junior doctors) are qualified. The QA scores of one
junior report are given by the senior doctor(s) after reviewing the image and
junior report. This process requires intensive labor costs for senior doctors.
Additionally, the QA scores may be inaccurate for reasons like diagnosis bias,
the ability of senior doctors, and so on. To address this issue, we propose a
Span-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores
automatically. Unlike the common document-level semantic comparison method, we
try to analyze the semantic difference by exploring more fine-grained text
spans. Unlike the common document-level semantic comparison method, we try to
analyze the semantic difference by exploring more fine-grained text spans.
Specifically, Sqator measures QA scores by measuring the importance of revised
spans between junior and senior reports, and outputs the final QA scores by
merging all revised span scores. We evaluate Sqator using a collection of
12,013 radiology reports. Experimental results show that Sqator can achieve
competitive QA scores. Moreover, the importance scores of revised spans can be
also consistent with the judgments of senior doctors.

</details>


### [54] [Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models](https://arxiv.org/abs/2508.08879)
*Haeun Yu,Seogyeong Jeong,Siddhesh Pawar,Jisu Shin,Jiho Jin,Junho Myung,Alice Oh,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 该论文提出了机制可解释性分析方法CultureScope，揭示了大语言模型内部存在西方主导和文化扁平化偏见，尤其在低资源文化下表现不同，为未来消除文化偏见和提升模型文化能力奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在不同文化环境下广泛部署，然而它们对于较少被记录的文化常存在过度泛化和误解。以往工作只关注模型外部表现层面的文化能力评测，缺乏对内部机制中如何导致文化误表征的研究。

Method: 提出了一种新的基于机制可解释性的研究方法——CultureScope，通过模型修补技术探查LLMs内部的文化知识空间，提出文化扁平化分数作为衡量模型内在文化偏见的指标，并专门分析Western-dominance偏见和文化扁平化的形成过程。

Result: 实验结果表明，LLMs在其文化知识空间中确实编码了西方主导偏见和文化扁平化现象。但资源较少的文化由于训练数据有限，受文化偏见影响较低。

Conclusion: CultureScope方法为理解与改善大语言模型中的文化偏见提供了新的分析工具和理论基础，有助于增强其文化理解力，并为未来相关研究提供数据和方法支持。

Abstract: The growing deployment of large language models (LLMs) across diverse
cultural contexts necessitates a better understanding of how the
overgeneralization of less documented cultures within LLMs' representations
impacts their cultural understanding. Prior work only performs extrinsic
evaluation of LLMs' cultural competence, without accounting for how LLMs'
internal mechanisms lead to cultural (mis)representation. To bridge this gap,
we propose Culturescope, the first mechanistic interpretability-based method
that probes the internal representations of LLMs to elicit the underlying
cultural knowledge space. CultureScope utilizes a patching method to extract
the cultural knowledge. We introduce a cultural flattening score as a measure
of the intrinsic cultural biases. Additionally, we study how LLMs internalize
Western-dominance bias and cultural flattening, which allows us to trace how
cultural biases emerge within LLMs. Our experimental results reveal that LLMs
encode Western-dominance bias and cultural flattening in their cultural
knowledge space. We find that low-resource cultures are less susceptible to
cultural biases, likely due to their limited training resources. Our work
provides a foundation for future research on mitigating cultural biases and
enhancing LLMs' cultural understanding. Our codes and data used for experiments
are publicly available.

</details>


### [55] [ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs](https://arxiv.org/abs/2508.08895)
*Keyu Chen,Zhifeng Shen,Daohai Yu,Haoqian Wu,Wei Wen,Jianfeng He,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 论文提出自适应串行-并行解码方案ASPD，通过自动识别并并行解码可并行结构，显著提升LLM推理速度（最高3.19倍），生成质量基本不受影响，适合低延迟场景应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）由于其自回归解码特性，在推理过程中面临显著的延迟挑战，影响实际应用，特别是在对实时性要求较高的场景。作者通过分析自回归模型输出，发现某些部分实际上可以并行处理，提出利用该“内在并行性”提升解码速度。

Method: 提出了一种自适应串行-并行解码方法（ASPD）：自动识别并提取模型响应中的可并行结构，并通过非侵入性管道完成结构验证；设计混合解码引擎，实现串行与并行解码随需切换，同时支持KV缓存重用，优化计算效率。

Result: 在通用任务、检索增强生成、数学推理等多个领域进行了全面测试，ASPD方法在Vicuna Bench上实现最高3.19倍加速（平均1.85倍），且生成质量与基本自回归模型的差异小于1%。

Conclusion: ASPD可显著提升大型语言模型推理效率，在质量几乎不损失的情况下大幅降低延迟，为LLM在客服机器人、答案检索等对时效性要求极高的场景部署奠定了基础。

Abstract: The increasing scale and complexity of large language models (LLMs) pose
significant inference latency challenges, primarily due to their autoregressive
decoding paradigm characterized by the sequential nature of next-token
prediction. By re-examining the outputs of autoregressive models, we observed
that some segments exhibit parallelizable structures, which we term intrinsic
parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel
decoding) can significantly improve the overall inference speed of LLMs. In
this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which
addresses two core challenges: automated construction of parallelizable data
and efficient parallel decoding mechanism. More specifically, we introduce a
non-invasive pipeline that automatically extracts and validates parallelizable
structures from the responses of autoregressive models. To empower efficient
adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which
enables seamless transitions between serial and parallel decoding modes while
maintaining a reusable KV cache, maximizing computational efficiency. Extensive
evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical
Reasoning, demonstrate that ASPD achieves unprecedented performance in both
effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up
to 3.19x speedup (1.85x on average) while maintaining response quality within
1% difference compared to autoregressive models, realizing significant
acceleration without compromising generation quality. Our framework sets a
groundbreaking benchmark for efficient LLM parallel inference, paving the way
for its deployment in latency-sensitive applications such as AI-powered
customer service bots and answer retrieval engines.

</details>


### [56] [Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning](https://arxiv.org/abs/2508.08912)
*Mahmoud Salhab,Shameed Sait,Mohammad Abusheikh,Hasan Abusheikh*

Main category: cs.CL

TL;DR: 通过大量弱标注数据预训练，再配合少量优质标注数据微调，该方法成功提升了阿拉伯语多方言ASR效果，并获得了国际竞赛第一。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语等低资源语言由于标注数据有限及方言多样性，导致自动语音识别（ASR）系统开发变得极具挑战性。

Method: 提出了一个可扩展的训练流程，先用15000小时的弱标注语音（包括现代标准阿拉伯语及多种方言）进行预训练，再结合筛选后的弱标注数据和小规模高质量标注数据，持续进行监督微调。

Result: 该方法在多方言阿拉伯语ASR挑战赛中取得第一名，达到领域内最新的效果。

Conclusion: 弱监督与微调结合能有效克服低资源语言ASR面临的数据匮乏与方言复杂问题，显著提升语音识别性能。

Abstract: Automatic speech recognition (ASR) plays a vital role in enabling natural
human-machine interaction across applications such as virtual assistants,
industrial automation, customer support, and real-time transcription. However,
developing accurate ASR systems for low-resource languages like Arabic remains
a significant challenge due to limited labeled data and the linguistic
complexity introduced by diverse dialects. In this work, we present a scalable
training pipeline that combines weakly supervised learning with supervised
fine-tuning to develop a robust Arabic ASR model. In the first stage, we
pretrain the model on 15,000 hours of weakly labeled speech covering both
Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the
subsequent stage, we perform continual supervised fine-tuning using a mixture
of filtered weakly labeled data and a small, high-quality annotated dataset.
Our approach achieves state-of-the-art results, ranking first in the
multi-dialectal Arabic ASR challenge. These findings highlight the
effectiveness of weak supervision paired with fine-tuning in overcoming data
scarcity and delivering high-quality ASR for low-resource, dialect-rich
languages.

</details>


### [57] [Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation](https://arxiv.org/abs/2508.08933)
*Khondoker Ittehadul Islam,Gabriele Sarti*

Main category: cs.CL

TL;DR: 本文首次引入了手工翻译的孟加拉语多步推理数据集，并对多语种模型进行英孟对照实验，发现模型在低资源语言（孟加拉语）的推理能力有限，尤其在复杂任务中表现不如高资源语言。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型在复杂多步推理任务上表现出色，但相关评估主要集中在英语等高资源语言，低资源语言如孟加拉语的研究非常有限。本文为解决这一不平衡，开发并引入了孟加拉语多步推理数据集。

Method: 将英文Reveal数据集手工翻译成孟加拉语，构建包含二元和非二元问题类型的数据集。使用多语种小型语言模型，在英文原始数据和孟加拉语翻译数据上进行对照实验，以评估不同模型在推理中的有效性。

Result: 实验显示，在类似设定下，推理上下文对于复杂的非二元问题更有帮助，但模型在孟加拉语中有效应用相关推理步骤方面存在困难。

Conclusion: 推理步骤对模型预测具有重要作用，在不同模型和语言环境下展现出不同趋势，反映出跨语言推理能力仍有很大提升空间。

Abstract: Language models have demonstrated remarkable performance on complex
multi-step reasoning tasks. However, their evaluation has been predominantly
confined to high-resource languages such as English. In this paper, we
introduce a manually translated Bangla multi-step reasoning dataset derived
from the English Reveal dataset, featuring both binary and non-binary question
types. We conduct a controlled evaluation of English-centric and Bangla-centric
multilingual small language models on the original dataset and our translated
version to compare their ability to exploit relevant reasoning steps to produce
correct answers. Our results show that, in comparable settings, reasoning
context is beneficial for more challenging non-binary questions, but models
struggle to employ relevant Bangla reasoning steps effectively. We conclude by
exploring how reasoning steps contribute to models' predictions, highlighting
different trends across models and languages.

</details>


### [58] [Train Long, Think Short: Curriculum Learning for Efficient Reasoning](https://arxiv.org/abs/2508.08940)
*Hasan Abed Al Kader Hammoud,Kumail Alhamoud,Abed Hammoud,Elie Bou-Zeid,Marzyeh Ghassemi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 提出了一种基于课程学习的长度控制推理训练方法，能显著提高大语言模型在token受限下的准确率和效率，优于固定预算方法。


<details>
  <summary>Details</summary>
Motivation: 当前对于大语言模型（LLM）推理能力的提升方法中，显式长度控制能优化计算成本，但现有方法普遍采用固定长度预算，未能利用模型从探索到压缩（提炼）的自然学习过程。该工作旨在提出更合理的长度控制训练策略。

Method: 本文提出一种基于课程学习的长度控制推理训练方法，采用Group Relative Policy Optimization（GRPO）。训练初期给予模型较充足的token预算，随着训练进展逐步收紧，鼓励模型先探索有效策略，再逐步提炼为更简洁的推理。GRPO结合了三方面的奖励函数：任务正确性、长度效率以及格式约束。

Result: 在GSM8K、MATH500、SVAMP、College Math和GSM+等数据集上的实验结果表明，课程式训练方法在相同最终预算下，相较固定预算基线表现更优，取得了更高准确率且token效率显著提升。变量消融实验还展示了奖励权重与收紧进度的重要作用。

Conclusion: 逐步收紧长度预算的课程学习为高效推理模型训练提供了有效的归纳偏置，显著提升了准确率和token利用率，为长度受限下的推理任务提供了新范式。

Abstract: Recent work on enhancing the reasoning abilities of large language models
(LLMs) has introduced explicit length control as a means of constraining
computational cost while preserving accuracy. However, existing approaches rely
on fixed-length training budgets, which do not take advantage of the natural
progression from exploration to compression during learning. In this work, we
propose a curriculum learning strategy for length-controlled reasoning using
Group Relative Policy Optimization (GRPO). Our method starts with generous
token budgets and gradually tightens them over training, encouraging models to
first discover effective solution strategies and then distill them into more
concise reasoning traces. We augment GRPO with a reward function that balances
three signals: task correctness (via verifier feedback), length efficiency, and
formatting adherence (via structural tags). Experiments on GSM8K, MATH500,
SVAMP, College Math, and GSM+ demonstrate that curriculum-based training
consistently outperforms fixed-budget baselines at the same final budget,
achieving higher accuracy and significantly improved token efficiency. We
further ablate the impact of reward weighting and decay schedule design,
showing that progressive constraint serves as a powerful inductive bias for
training efficient reasoning models. Our code and checkpoints are released at:
https://github.com/hammoudhasan/curriculum_grpo.

</details>


### [59] [Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens](https://arxiv.org/abs/2508.08942)
*Lucas Albarede,Jose Moreno,Lynda Tamine,Luce Lefeuvre*

Main category: cs.CL

TL;DR: 本文提出了LoDIT方法，通过标记和logits实现联合答案与归因生成，在提升准确性和效率方面明显优于现有方法，有效增强了大语言模型在RAG中的可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）尽管表现出色，但仍容易产生幻觉，影响其可信度。现有研究多关注答案和归因的正确性，近期的研究则聚焦于提升模型回答的忠实度。本论文旨在解决传统方法计算效率低及无法直接将生成的答案与归因对齐的问题。

Method: 提出了一种名为LoDIT的新方法，在RAG（检索增强生成）流程中，通过在生成过程中利用特定token的logits，联合生成答案与归因。方法包括：（1）用特定token标记文档，实时利用这些token的logits估算文档对答案的贡献；（2）将各文档贡献整合成最终归因。

Result: 在Trust-Align基准（以可信度为核心的属性文本生成任务）上，LoDIT在多项指标上显著优于现有最先进模型。

Conclusion: LoDIT方法提升了答案生成与文档归因的一致性及效率，在减少延迟、增强鲁棒性方面具备优势，对提升LLMs可信度具有重要意义。

Abstract: Despite their impressive performances, Large Language Models (LLMs) remain
prone to hallucination, which critically undermines their trustworthiness.
While most of the previous work focused on tackling answer and attribution
correctness, a recent line of work investigated faithfulness, with a focus on
leveraging internal model signals to reflect a model's actual decision-making
process while generating the answer. Nevertheless, these methods induce
additional latency and have shown limitations in directly aligning token
generation with attribution generation. In this paper, we introduce LoDIT, a
method that jointly generates and faithfully attributes answers in RAG by
leveraging specific token logits during generation. It consists of two steps:
(1) marking the documents with specific token identifiers and then leveraging
the logits of these tokens to estimate the contribution of each document to the
answer during generation, and (2) aggregating these contributions into document
attributions. Experiments on a trustworthiness-focused attributed
text-generation benchmark, Trust-Align, show that LoDIT significantly
outperforms state-of-the-art models on several metrics. Finally, an in-depth
analysis of LoDIT shows both its efficiency in terms of latency and its
robustness in different settings.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [60] [Short Proof: Exact Solution to the Finite Frobenius Coin Problem](https://arxiv.org/abs/2508.08464)
*Lorenzo De Gaspari,Marco Ronzani*

Main category: cs.DM

TL;DR: 本文扩展了著名的Frobenius零钱问题，分析了当每种面额硬币有限时，有哪些金额可表示。作者针对两种面额情况给出精确计数公式，推进了基层问题的解决。


<details>
  <summary>Details</summary>
Motivation: Frobenius零钱问题是一道经典的数学问题，关注在给定硬币面额的情况下，最大无法表示的金额。该论文的动机是探讨一种更实际的变体：对于每种面额的硬币限定数量，这种现实限制下问题的复杂性和结果变化值得深入分析。

Method: 本文将原始问题转化为“有限”情形，即给定各面额硬币数量的上限。研究的核心是如何计数在所有硬币用完前，各种金额的可表示和不可表示情形。通过严密的数学分析和推导，当硬币只有两种面额时，给出了确切的计数公式。

Result: 作者证明了在只有两种不同面额且每种面额硬币数量有限的情形下，可以用精确公式算出所有可表示和不可表示的金额总数，显著拓展了对有限情形下零钱问题的理解。

Conclusion: 对比于经典Frobenius问题，该“有限”版本更贴近实际生活。作者精确给出了两种面额、硬币数量有限时的解决方案，为具体场景中的零钱找零及相关算法设计奠定了理论基础。

Abstract: The Frobenius Coin Problem is a classic question in mathematics: given coins
of specified denominations, what is the largest amount that cannot be formed
using only those coins? This brief work covers a variation of such question,
posing a limit on the number of coins available for each denomination. Thus,
the new problem becomes finding the count of distinct values that can be
represented, and those that cannot, within the finite set of integers ranging
from zero to the sum of all coins. We refer to this version of the problem as
the "finite" case. We will show how this closely relates to the original
question, and prove an exact formula solving the problem when exactly two
denominations are involved.

</details>
