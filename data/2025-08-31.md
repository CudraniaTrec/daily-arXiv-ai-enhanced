<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.DM](#cs.DM) [Total: 3]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 作者提出可用STP高效推断和验证链表类递归结构的不变式，并将其集成到CHC验证框架中，显著提升了解决能力，获得权威比赛优胜。


<details>
  <summary>Details</summary>
Motivation: 自动化程序验证在递归数据结构方面仍存在挑战，现有技术难以高效发现和验证相关的不变式。

Method: 提出了可解元组模式（STP）的概念，用于表达链表等递归数据结构间的不变式。通过少量正样本推断STP，并利用支持序列理论的SMT求解器检查其归纳性，再将STP推断集成到支持链表数据结构的CHC求解器中。

Result: 集成STP推断的CHC求解器在2025年CHC-COMP的ADT-LIN类别中取得了明显优势，胜出所有竞争者。

Conclusion: STP极大提高了递归数据结构程序自动化验证的推断效率与正确性，为相关工具提供有力统一支持。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [2] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 本文扩展概率程序的操作语义，实现了带循环和动态标记的程序的图形化表示，提出了基于控制流图的静态因式分解和切片技术，并用以提升推理算法性能，理论正确且实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管所有贝叶斯网络都可以实现为概率程序，但反向（用概率程序的结构来图示表示）并不明确。本文旨在探究具有用户标记的采样语句和 while 循环（如 Gen、Turing、Pyro 等语言常见的特性）的概率程序能否被图形化表示。

Method: 作者扩展了现有的概率程序操作语义，以支持样本标记和 while 循环等语言特性，通过将程序转换为控制流图，并定义了一种静态分析方法，近似推断程序中随机变量的依赖结构。同时，提出了一种静态因式分解程序隐式概率密度的方法，并开发了基于此结构的程序切片技术。

Result: 提出的静态因式分解在无循环和常量标记的情况下等价于经典贝叶斯网络因式分解，而在有循环或动态标记时，能图形化表示生成无限随机变量的新情况。借助这种结构，作者开发了三项优化：降低变分推断中梯度估计的方差，加速 Metropolis Hastings 和序列蒙特卡洛方法，并证明这些优化方法是正确的，在实验上优于现有技术。

Conclusion: 该工作拓展了概率程序的理论基础和程序结构分析工具，实现了对具有复杂控制流的概率程序的图形化表达，同时为推理算法带来了新的优化途径。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 本文系统评估了LLM针对微服务应用的代码生成能力，发现其在复杂问题上的表现仍有很大提升空间，指出关键挑战，并为后续研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 评估现有大语言模型（LLMs）在为真实世界问题生成代码方面的能力，尤其针对微服务架构应用的代码合成。

Method: 定义了微服务应用的标准模板、提出了描述规范难度的指标，并搭建了自动化框架，通过单元测试自动测试由LLM生成的代码。对生成代码的错误进行了详细分析。

Result: 强大的LLMs（如GPT-3o-mini）在中等难度的规范上表现良好，但在高难度应用上的表现很差。失败主要原因包括复杂的业务逻辑、对外部服务的高度依赖、数据库集成及非功能性能力（如认证）的需求。

Conclusion: LLMs在应对现实复杂度下的代码生成能力仍有限，尤其在高难度或复杂业务逻辑前。此外，还总结了LLM生成代码时的多种挑战，并为未来改进代码生成提出了研究方向。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [4] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 针对代码生成大模型效率不足的问题，作者提出基于效率奖励的强化学习框架，通过动态探索和高对比信号提升代码效率和正确率，最终实现了比原模型更高的性能，在7B模型上取得了显著的效果。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成大语言模型虽然能力强大，但生成的代码往往存在运行效率低的问题，限制了在对性能敏感的实际场景中的应用。为改善这一局限，作者提出了新方法以提升代码效率。

Method: 提出了一个以性能奖励为导向的强化学习框架，并结合动态探索、误差不敏感的强化学习方法、高对比效率信号及在线探索，最终汇总为两阶段调优方法。

Result: 在7B模型上通过该方法代码正确率提升了10.18%，运行效率提升了7.75%，性能接近更大规模的模型。

Conclusion: 提出的两阶段调优方法在保证代码正确性的同时显著提升了生成代码的运行效率，实验结果表明在7B模型上代码正确率提升了10.18%，运行效率提升了7.75%，与更大规模模型的性能相当。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [5] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: 本文提出了Chimera框架，利用LLM自动合成符合文法的术语生成器，极大提升SMT公式测试的效率与有效性，并在主流SMT求解器发现大量真实错误，为相关领域带来实用价值。


<details>
  <summary>Details</summary>
Motivation: SMT求解器是现代系统和编程语言研究中的核心工具，其正确性至关重要。高质量测试公式能够有效发现错误，但随着求解器功能的快速发展，之前的测试方法已无法满足需求。最近基于大语言模型（LLM）的方法有潜力，但又存在生成公式语法无效及交互成本高的问题，亟需新的测试生成框架。

Method: 提出Chimera框架，利用LLM从SMT理论文档自动提取上下文无关文法（CFG），并合成遵循该文法的可复用布尔术语生成器。在模糊测试时，通过LLM生成的术语填充已有公式结构骨架，确保生成公式语法正确且语义多样性。同时只需一次性与LLM交互，显著降低运行成本。

Result: 在Z3和cvc5两大主流SMT求解器上评价Chimera，共发现了43个已确认的错误，其中40个已被开发者修复。

Conclusion: Chimera有效提升了SMT公式生成的语法有效性与语义多样性，并显著降低了使用LLM的运行成本。其实用性和高效性已在主流求解器上得到验证，对SMT相关的软件测试和自动验证有重要推动作用。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [6] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 针对微服务系统根因定位难题，作者提出结合多智能体和递归大模型思维的RCLAgent方法，在多数据集上显著优于传统技术，提升定位效率与准确性，并具有更强自适应与可解释能力。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统由于复杂性与规模变大，故障频率提升，精准的根因定位成为保障系统可靠性的关键。然而，现有方法要么过度依赖预设模式，难以适应变化；要么在推理过程中缺乏可解释性，让SRE困惑。

Method: 通过调研多位不同机构的专业SRE，归纳人类根因分析的递归性、多维扩展、跨模态推理三大特征，据此提出RCLAgent方法。该方法基于多智能体递归思维框架，利用大语言模型引导推理，结合多智能体和工具辅助分析整合多源数据，实现自适应根因定位。

Result: RCLAgent在多个公开数据集上以单次请求即可超过依赖多次请求聚合的现有方法，定位准确率与效率显著提升。

Conclusion: RCLAgent显著提升了复杂微服务系统根因定位的效率和精度，不仅无需依赖静态模式，还增强了推理可解释性，对SRE实践具有重要意义。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [7] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: 本文基于XP2025研讨会总结了GenAI与敏捷开发结合的核心挑战与机遇，提出多维度研究路线，为未来技术实践和理论发展明确了方向。


<details>
  <summary>Details</summary>
Motivation: GenAI与敏捷软件开发结合过程中面临多种实际挑战和新机遇，亟需跨界交流与系统性研究，推动技术与流程协同发展。

Method: 组织了结构化、互动性的分组讨论，汇聚学术界和业界专家，识别和分析了主要痛点，并共同制定了研究方向。

Result: 发现了工具碎片化、治理问题、数据质量和AI技能缺口等痛点，深入分析了成因，并制定了短期行动计划和长期研究方向。

Conclusion: 论文总结了研讨会的讨论成果，提出了一个多主题的研究路线图，旨在促进GenAI与敏捷开发的融合，以实现负责任、以人为本的集成。

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [8] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 该论文分析了当前大语言模型系统测试面临的主要挑战，提出了分层架构、合作测试策略及AICL协议，为LLM应用的质量保障和测试标准化提供了理论和实践参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）应用愈发复杂，集成了检索增强、工具调用和多轮交互，传统的软件质量保证方法面临前所未有的挑战。作者着眼于LLM系统非确定性、动态性和语境依赖性带来的测试难题，推动针对LLM应用的标准化与工具化测试需求。

Method: 作者从架构角度将LLM应用分解为系统壳层、提示协同层和LLM推理核心三层，分析每层对传统测试方法的适用性，评估软件工程和AI社区的测试方法，并通过结构化分析归纳根本差异及相关挑战。进一步提出保留、翻译、融合和运行时等协作策略，探讨结合预部署验证与运行时监控的闭环质量保障框架。还提出了一个协议（AICL），便于AI代理之间面向测试的通信。

Result: 分析结果揭示了AI与传统软件测试方法在单元抽象、评估指标、生命周期管理等方面存在结构性脱节，归纳出4个根本性差异和6项核心挑战。作者对应性地提出四种协作策略，并初步规划了AICL协议以提升LLM应用测试的标准化和自动化。AICL具有良好的可集成性和测试导向特征。

Conclusion: 论文对LLM应用测试面临的结构性挑战进行了深入分析，提出分层质量保障框架和合作策略，推动LLM应用测试的标准化。所提出的AICL协议为多智能体系统中的测试提供了新思路和工具支持。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [9] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: 该研究首次系统评估大模型自动从法律法规生成开发规范的效果，结果显示生成的规范整体质量高，能够显著降低手工负担，但仍存在一定虚构与遗漏风险。


<details>
  <summary>Details</summary>
Motivation: 随着法律法规越来越多地影响软件设计与质量保证，工程师需要根据笼统且技术中立的法律条文，手动撰写合规相关需求与标准，这一过程既耗时又易出错，还需专业领域知识。本文试图通过生成式AI自动将法律要求转化为开发者可用的行为规范，从而减少手动工作量。

Method: 作者采用准实验设计，邀请10名参与者评估两种大语言模型（Claude与Llama）根据食品安全法规自动生成的Gherkin（行为驱动开发语言）规范。共生成60条规范，每条都以五个标准（相关性、清晰度、完整性、单一性、节省时间）由两位参与者双盲评审，累计得出120份评分。

Result: 在相关性、清晰度、完整性、单一性、节省时间等指标上，多数（最高评级占比68%-90%）获得较高评分，无最低等级出现。统计检验显示参与者与模型间无显著差异，但Llama在清晰性、完整性、节省时间略优，Claude则在单一性表现较好。反馈表明自动生成的规范有时出现虚构和遗漏，但总体实用价值较高。

Conclusion: 大语言模型能够高质量地将法律文本转化为结构化的Gherkin规范，明显减少合规需求手工撰写的工作量，有助于促进实现、质量保证和测试的自动化。

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [10] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 本文提出了用于指导软件架构师处理可持续性问题的“可持续性视角”概念，并通过文献综述与专家讨论证明其实际有效性和产业适用性，强调了结构化方法对推动软件系统可持续性的重要价值。


<details>
  <summary>Details</summary>
Motivation: 当前软件密集系统中，可持续性作为一个新的质量属性越来越受到重视，但架构师在设计阶段缺乏结构化指导，难以有效应对这一问题。

Method: 提出了“可持续性视角”，即一种面向可持续性问题的建筑视角，并通过对重要文献的滚雪球法分析和与领域专家的焦点小组访谈进行理论和实践论证。

Result: 该研究验证了视角各个元素在实践中的相关性，并指出应根据产业需求，调整和完善可持续性视角的具体内容。

Conclusion: 可持续性视角能够为软件架构师在设计阶段处理可持续性质量属性提供有效、结构化的指导，并具有与架构框架和具体行业背景无关的普适性。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [11] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 本文提出了无须系统执行的基于断言的自动化测试判据，采用遗传编程与Ochiai公式能生成更准确且对系统波动更稳健的测试断言，有效提升物理-网络系统测试可解释性和可靠性并降低成本。


<details>
  <summary>Details</summary>
Motivation: 物理-网络系统（CPS）基于仿真的测试极为耗时且成本高昂，且仿真器本身的波动性会导致测试结果不一致，需要多次重复测试以获取可靠结论。为降低测试成本，亟需无需实际运行系统即可给出判定的自动测试判据，并且这些判据要易于解释和足够稳健、能够抵御仿真器波动。

Method: 作者提出了基于断言的测试判据，将其定义为在输入空间上的逻辑和算术谓词，能够无须系统实际执行，仅通过输入判断测试通过与否或无法判定。他们提出了两种生成断言的方法：一种是利用遗传编程（GP），通过不同的光谱故障定位公式（Ochiai、Tarantula、Naish）作为适应度函数；另一种则用决策树（DT）及决策规则（DR）实现。

Result: 在航天、网络和自动驾驶领域的案例研究表明，遗传编程结合Ochiai公式生成的测试判据在准确性上优于采用Tarantula、Naish公式或决策树/规则的断言，而且在测试系统本身具有波动性的情况下也保持这一优势。此外，在四个具有波动性的网络和自动驾驶系统实验中，这类断言的平均准确性波动仅为4%。

Conclusion: 遗传编程与Ochiai公式结合生成的断言式测试判据不仅准确性高，而且对测试系统的波动表现出良好的稳健性，是降低CPS测试成本且可解释性强的有效方法。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [12] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 本文针对深度学习检测并发错误的难题，构建了专用数据集，并设计了结合CCPG与异构GNN的模型，搭配可解释性定位方法，在准确率、精度、召回率等指标上均优于现有方法，并能精准定位到源码行。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在并发错误检测方面存在三大局限：缺乏大规模专用并发错误数据集、无法充分表达并发语义、只进行二分类无法精确定位错误代码行。并发错误检测本身因共享资源同步不当导致，极难发现，严重影响软件可靠性与安全性。

Method: 构建专用并发错误数据集以促进模型训练与评估；将预训练模型与异构图神经网络（GNN）结合，并引入全新的并发语义感知代码属性图（CCPG），有效表示并发语义；采用基于GNN的解释性方法SubgraphX，对图结构进行探索，实现并发错误的精准定位到具体代码行。

Result: 该方法在多种评测设置下，相比现有最先进方法，准确率和精度提升约10%，召回率提升约26%。

Conclusion: 提出了一种创新且高效的并发错误检测与定位方法，不仅提升了检测性能，还能精确定位错误代码行，显著优于现有方法。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [13] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文针对现代系统中配置相关故障诊断难题，提出了ConfLogger工具，通过静态污点分析和大模型自动日志生成，极大提升了日志助力配置误诊断的能力，实验和用户验证均显示出效果优于现有手段。


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统具有复杂的配置空间，因此容易出现错误配置和隐藏的软件缺陷。现有的方法多集中在发生故障后分析软件行为以定位配置问题，但并未检视日志能否为诊断提供充分的信息。为此，本文提出了增强日志记录以提升配置相关故障可诊断性的需求。

Method: 提出了ConfLogger工具，结合配置感知静态污点分析与基于大语言模型的日志生成。具体方法包括：1）通过数据流追踪识别涉及配置的敏感代码段；2）针对配置代码上下文自动生成诊断日志语句，从而提升故障定位能力。

Result: ConfLogger在八个流行软件系统上测试，显著提升了配置相关故障的可诊断性。增强日志帮助误配置诊断工具在30个隐藏场景下实现100%出错定位准确率，其中80%问题通过暴露的显式配置信息直接解决。ConfLogger的日志点覆盖率达到74%，分别比基线LLM日志生成器高12%和30%；在变量日志的准确率上比最先进方法高8.6%，召回率高79.3%，F1值高26.2%。用户实验显示诊断速度提升1.25倍，故障排查准确率提升251.4%。

Conclusion: ConfLogger显著提升了软件系统配置相关故障的诊断能力，兼具高覆盖、高准确性与实际可用性，为增强配置可诊断性和日志价值提供了可行路径。

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [14] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 本文回顾了软件工程的起源、重点人物和专业化，定量分析了国际软件工程会议论文作者的性别比例，发现该领域长期存在性别偏见并有多年代女性显著缺席。提出了有关性别偏见政策研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨软件工程领域工程与计算机科学中的性别偏见，并研究相关政策影响。

Method: 文献回顾、代表人物分析及定量统计分析会议论文作者性别参与情况。

Result: 在1976-2010年间，国际软件工程会议有12年出现对女性的显著性别排斥。

Conclusion: 软件工程领域存在明显的性别偏见，女性作者在重要会议中存在被排斥的情况。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL](https://arxiv.org/abs/2508.20738)
*Lukas Bartl,Jasmin Blanchette,Tobias Nipkow*

Main category: cs.LO

TL;DR: 提出了一个分析Metis证明的新工具，可提取变量实例化信息，从而提升自动化证明工具（Sledgehammer）的性能和易用性。


<details>
  <summary>Details</summary>
Motivation: 提升定理证明的自动化程度，帮助用户更好理解证明过程。

Method: 开发了一个新工具，分析Metis证明过程以提取变量实例化信息。

Result: 工具能提升Sledgehammer工具的成功率，优化证明速度，并增强用户对证明步骤的理解。

Conclusion: 通过对Metis证明分析，能有效增强Sledgehammer工具的表现及用户体验。

Abstract: Metis is an ordered paramodulation prover built into the Isabelle/HOL proof
assistant. It attempts to close the current goal using a given list of lemmas.
Typically these lemmas are found by Sledgehammer, a tool that integrates
external automatic provers. We present a new tool that analyzes successful
Metis proofs to derive variable instantiations. These increase Sledgehammer's
success rate, improve the speed of Sledgehammer-generated proofs, and help
users understand why a goal follows from the lemmas.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本文综述多语种预训练模型社会偏见研究，指出当前研究存在语言和文化盲点，缓解方案和评价指标有待改进，并对未来多语种、跨文化偏见研究提供建议。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理领域，越来越多使用多语言预训练模型，但大多数社会偏见探究集中在英文语料上，缺乏对多语言环境中偏见表现及其缓解手段的系统整理和评估。

Method: 采用系统性综述的方法，梳理和分析了近期有关多语种模型偏见检测与缓解的相关文献，从语言多样性、文化意识、评价指标与缓解方法等多个维度进行对比。

Result: 归纳了多语种偏见研究中存在语言偏好、跨文化难度、评价体系单一、多语种缓解实验稀缺等问题，总结了适应不同语言与文化时常见难题及解决方法，并为未来研究指明了更具包容性和先进性的方向。

Conclusion: 本文得出结论，目前多语言预训练模型在社会偏见方面与仅处理英语文本的模型存在类似问题。现有多语言和非英语语境下的偏见评价与缓解研究尚存语言偏好严重、缺少多语种实验等方法论局限，未来需更加注重包容性和跨文化适应性。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [17] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本文通过结构化提示和微调，中等规模语言模型能高效自动生成符合教学要求的K-12语言评估题，实用、可扩展。


<details>
  <summary>Details</summary>
Motivation: 自动化生成试题能降低人工成本和提升一致性，尤其在语言学形态评估领域。当前人工开发试题效率低且易受主观影响，亟需探索利用语言模型提升多项选择题（MCQ）生成质量与效率。

Method: 采用两步法：一是对比经过微调的中等规模模型（Gemma, 2B）与未经微调的大规模模型（GPT-3.5, 175B）；二是评估包括零样本、少样本、链式思维、角色扮演、顺序设计及其组合在内的七种结构化提示策略。结果通过自动指标与专家多维评分评估，还采用经过专家样本训练的GPT-4.1进行大规模自动模拟评分。

Result: 结构化提示（特别是链式思维与顺序设计结合）显著提升了Gemma模型的生成质量。Gemma在构建贴合测评目标且教学适宜性方面优于GPT-3.5的零样本模式，结构化提示在中等规模模型性能提升中效果明显。通过结合自动评估、专家评价与大模型模拟，实现了试题质量的多元保障。

Conclusion: 结构化提示和高效微调可在数据有限条件下大幅提升中等规模语言模型（Gemma）自动生成试题的质量。提出的工作流对K-12语言测评题目的开发与验证具备实际性与可扩展性。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [18] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 提出新方法和开源工具，将SystemC TLM与FMI 3.0协同仿真无缝集成，解决异构系统集成难题并得到实际验证。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统复杂性的提升（尤其在汽车领域），对高效建模与跨领域协同仿真技术的需求增加，但现有SystemC TLM与其他工程领域模型的互操作性有限，集成困难。

Method: 将SystemC TLM组件封装为FMI 3.0 CoSimulation FMUs，并开发开源工具链，以标准化接口和数据同步支持异构模型的协同仿真。

Result: 本文提出了一种将SystemC TLM模型集成进FMI（Functional Mock-up Interface）协同仿真流程的全开源方法。通过将SystemC TLM组件封装为FMI 3.0协同仿真功能模块单元（FMUs），实现标准化、高效的跨领域仿真环境集成。文中还推出了一个轻量级开源工具链，解决了时间同步和数据交换等关键技术难题，并通过典型案例验证了所提方法的可行性和有效性。

Conclusion: 通过代表性案例证明，该方法可以有效解决SystemC TLM与其他工程领域模型集成的技术挑战，实现顺畅的异构仿真环境对接，并具有实际可行性。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [19] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 提出DGPO方法，通过教师模型演示冷启动并持续指导优化，小型语言模型能有效学习智能化搜索和推理行为，在某些任务上甚至优于大模型，实现了在资源受限环境下智能RAG的可行性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型由于推理能力弱，强化学习过程奖励稀疏、训练不稳定，难以实现高效的智能化搜索及规划。

Method: 提出Distillation-Guided Policy Optimization（DGPO），通过利用教师模型演示冷启动，训练过程中持续接受教师模型指导来优化策略。

Result: DGPO使得小型模型获得了复杂的智能搜索行为，在部分情况下甚至超过了更大教师模型的表现。

Conclusion: 通过DGPO，小模型在计算资源有限的环境下也能高效实现智能化RAG任务。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [20] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: 提出了将政府伦理指南转化为具体测试问题、并自动检测大语言模型违规的GUARD方法，实证有效，可推广到多种模型类型，帮助提升AI合规性与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的政府伦理指南较为抽象，开发者很难将其转化为具体可操作的测试问题，从而无法有效评估LLM是否符合合规要求。

Method: 提出并实现了GUARD方法，通过自动生成违反政府伦理指南的问题来测试LLM响应的合规性。同时采用“jailbreak”诊断（GUARD-JD）创造特殊场景以更深入识别潜在违规，最后输出合规性报告。

Result: 在七大主流LLM（如Vicuna-13B、GPT系列等）和三个政府指南下验证了GUARD的有效性，并展示了GUARD-JD能推广到视觉-语言模型上，提升LLM应用的安全性与可靠性。

Conclusion: GUARD能有效测试LLM是否符合政府伦理指南，并能生成合规性报告，提升模型的可靠性。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [21] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: 为解决大语言模型在长文本理解和复杂推理上的瓶颈，本文提出JERR方法，通过摘要、图结构及蒙特卡洛树搜索三步骤，大幅提升模型性能，实验结果显著优于主流基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在处理长文本时仍因内存限制和任务复杂性而表现不佳。此外，LLM普遍存在透明度不足和产生幻觉的风险。

Method: 提出了JERR框架，通过图结构推理提升LLM的长文本理解能力。JERR包括提取摘要、构建有向无环图（DAG）以解决冗余问题，并利用蒙特卡洛树搜索（MCTS）实现复杂推理路径。

Result: JERR在ROUGE和F1指标上始终优于所有基线，在LLM-Rater评测中也获得最高分。

Conclusion: JERR能够显著提升LLM在长文本和复杂推理任务中的表现，实现了更高的可靠性和透明度。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [22] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 本研究首次提出用NP难图问题作为大模型推理训练数据，通过两阶段微调和强化学习，显著提升了模型的长链推理能力。新模型在多个推理场景下均有优异表现，为LLM后训练提供了新的思路和工具。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在复杂推理任务上已取得显著进展，主要依赖于它们在长链推理（Long CoT）上的能力。然而，实现这种能力需要大量高质量、人工精心策划的数据集（如数学和代码），导致可扩展的替代方法尚未被充分探索。

Method: 作者提出以NP-难图（NPH）问题作为新的合成训练语料，并设计了两阶段的后训练框架：（1）在拒绝采样的NPH图实例上进行Long CoT监督微调，提高推理深度；（2）设计精细奖励机制进行强化学习，提升推理效率。

Result: 旗舰模型Graph-R1-7B在数学、编程、STEM及逻辑领域均表现出强鲁棒性，并在NPH图问题上在准确率和推理效率上超越了QwQ-32B。

Conclusion: NPH图问题是一种有效且可扩展的资源，有助于提升LLM在长链推理任务中的表现，为后训练掀开新篇章。相关实现与模型已开源。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [23] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文提出了第一个面向大语言模型的上下文感知人格评估（CAPE）框架，并发现对话历史能提升一致性但引入人格漂移，不同模型对上下文依赖程度不同。该评估方法更贴近实际应用，对LLM研究有重要价值。


<details>
  <summary>Details</summary>
Motivation: 以往对大型语言模型（LLMs）的人格、行为特质评估采用孤立、无上下文的测试方法，忽略了真实应用场景中上下文对模型行为的影响。因此，亟需开发更贴近实际应用的评估方法。

Method: 提出第一个面向LLMs的上下文感知人格评估（CAPE）框架，将对话历史纳入测试流程，并设计了新指标用于量化模型响应一致性。同时，在7种主流LLM上进行系统实验。

Result: 实验表明，对话历史可提升模型一致性，但也会引起人格漂移。GPT-3.5-Turbo、GPT-4-Turbo变化明显，且对问题顺序鲁棒；Gemini-1.5-Flash及Llama-8B对上下文极为敏感。GPT模型体现固有人格和上下文双重影响，Gemini和Llama则更依赖上下文。将该框架用于Role Playing Agents可提升响应一致性并更符合人类判断。

Conclusion: 引入上下文感知评估框架显著提升了LLM人格与行为测试的现实意义，有助于揭示模型在真实互动中的行为一致性与个性变化，对后续LLM行为研究和应用有重要参考价值。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [24] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本研究分析了大语言模型在数学推理中的中间推理步骤实用性，发现条件熵下降（代表不确定性减少）往往带来正确答案，错误推理不仅推理链更长，而且结果也更差。这为早期筛除无效推理、优化模型推理流程提供了重要依据。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）依赖生成中间推理步骤来提升准确率，但尚不清楚这些推理步骤实际对最终答案正确性的贡献。文章旨在分析推理步骤的实用性及其对最终结果的影响。

Method: 利用Qwen2.5-32B和GPT-4o在MATH数据集上生成推理链，并用独立模型Qwen3-8B量化这些推理链对最终准确率的贡献；具体方法是运用条件熵来衡量每一步推理对答案置信度的提升。

Result: 结果显示：推理过程中条件熵持续下降通常对应正确答案，而条件熵平稳甚至上升则容易导致错误答案。同时，错误推理路径通常更长，说明推理越多并不一定效果越好。

Conclusion: 文章揭示了推理步骤有效性与答案正确性之间的强关联，为未来高效推理流程设计提供理论依据，尤其是在早期识别无效推理链方面。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [25] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: 本文提出并发布了UI-Bench大规模基准，用于量化和比较主流AI文本生成Web工具的可视化表现，为该领域树立了标准和公平评测体系。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开、系统、可复现的基准来验证AI文本生成应用工具宣称的高质量产出，导致各工具的表现难以公正对比和进步。

Method: 专家配对比较法对10款工具、30个提示词和300个生成网站进行评测，结合TrueSkill模型，以获得具有置信区间的系统性排名。

Result: 推出了UI-Bench基准，发布了完整的提示词集、开源评测框架和公开排行榜，并计划释放全部评分网站数据。

Conclusion: UI-Bench为AI文本生成应用工具的可视化质量评价建立了首个大规模基准，并推动了AI驱动网页设计的标准化与进步。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [26] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出DentalBench，系统评测现有LLM在牙科领域的表现，发现领域适应与专用基准可显著提升模型专业能力，为牙科及医疗AI应用提供关键资源与方法。


<details>
  <summary>Details</summary>
Motivation: 尽管通用医学LLM在医学任务上表现优异，但在牙科等需要更深专业知识的细分领域，其能力尚未被充分探索，主要受限于缺乏针对性评测资源。

Method: 本文提出了DentalBench——首个评估牙科领域LLM的综合双语基准，包括DentalQA（英中双语问答集，涵盖4类任务和16个牙科子领域，共36,597题）和DentalCorpus（牙科领域大规模高质量语料，支持模型微调和检索增强生成）。用DentalBench对14种LLM进行评估，并进一步对Qwen-2.5-3B开展领域适应实验。

Result: 评测揭示不同模型、任务和语言间存在显著性能差距；领域适应显著提升模型在知识密集和术语相关任务中的表现，突显领域专用基准在促进可信、高效医疗LLM发展中的重要性。

Conclusion: DentalBench补足了牙科领域LLM评测空白，通过系统化资源与实验，推动了牙科AI模型的公平比较和能力提升，为打造可信、有效的医疗LLM打下基础。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [27] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: KG-CQR通过知识图谱补充查询语义，大幅提升了RAG系统复杂查询的检索性能，无需额外训练，表现优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统中，检索阶段仍存在查询上下文信息贫乏，难以充分表示复杂输入查询的问题。本文旨在通过知识图谱补充查询上下文，提升查询表示能力。

Method: 提出了KG-CQR框架，利用知识图谱对输入查询进行结构化关系补充，包括子图抽取、补全和上下文生成模块，实现语义丰富的查询上下文表示，无需对LLM进行额外训练，具备模型无关性和可扩展性。

Result: 在RAGBench和MultiHop-RAG数据集上的实验表明，KG-CQR在mAP提升4-6%；Recall@25提升2-3%。在多跳问答等复杂检索任务中，KG-CQR也显著优于现有基线方法。

Conclusion: KG-CQR通过知识图谱结构化补充，有效提升了RAG系统检索阶段的效果，能够广泛应用于不同规模的LLM，具有较强的推广和实际应用价值。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [28] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 本文开发并开源了面向民航维修领域的LLM评测基准，可标准化评估模型在该行业任务中的知识和推理能力，促进专业模型持续优化和行业智能化升级。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的评测主要集中在数学和编程推理方面，缺乏适用于民航维修领域的专业工具。民航维修行业有严格标准，相关任务知识密集且需复杂推理能力，因此亟需专业评测工具提升模型适用性。

Method: 作者提出并开发了针对民航维修领域的大型语言模型工业级评测基准。该基准用于标准化地评估LLM在民航维修场景下的能力，识别领域知识及复杂推理的具体短板。同时，利用该基准，对主流RAG系统、嵌入模型及LLM进行实验评测。

Result: 该基准能够有效评估模型在民航维修领域的表现，成功暴露模型在专业知识和复杂推理能力上的不足。评测工具与代码已开源，为领域内进一步研究和优化提供支持。

Conclusion: 提出了业界首个面向民航维修的LLM评测基准，填补了现有领域专用评测的空白，支持RAG系统及LLM能力的提升与评估，有助推动智能民航维修方案的发展。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [29] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 本文提出了一种结合CBR、TF-IDF及余弦相似度的实践题目智能检索系统。实验证明系统能高效稳定地推荐相关实践题目及相似度分值，为选题搜索提供有价值帮助。


<details>
  <summary>Details</summary>
Motivation: 当前实践工作选题繁多，学生或研究者在确定实践题目时常常花费大量时间和精力，缺乏有效的检索与推荐工具，因此需要一种高效的方法来辅助搜索和匹配相关实践工作标题。

Method: 结合了案例推理（CBR）、TF-IDF向量化技术和余弦相似度计算。系统可通过输入标题或关键词，利用CBR经验库检索类似案例，TF-IDF处理标题文本向量，余弦相似度用于相似度评分，最终返回匹配标题及其相似度分数。

Result: 在705个实践工作标题的数据库中，以五个标题进行检验并分两阶段测试（一次用已有标题，一次对标题进行随机变换）。第二阶段的结果显示找到的相关标题数量及匹配度均与第一阶段一致，且获得了最高的平均匹配分数。

Conclusion: 该方法能够有效进行实践工作标题的搜索和推荐，系统能稳定地在不同标题输入形式下保持高效和一致的匹配效果，为相关选题检索提供了有价值的工具。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [30] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: 本文提出了MCP-Bench基准，针对跨领域、多步骤、工具组合等复杂任务考察LLM能力。实验发现多数主流LLM在这些任务上依然表现不佳，MCP-Bench为未来模型能力提升和系统性评测提供标准。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）工具使用测评往往只覆盖浅层、多步工作流和明确工具指定，无法充分评价LLM在真实、多步任务中的工具协调、参数控制与复杂规划能力。

Method: 提出MCP-Bench基准，该基准基于Model Context Protocol（MCP），让LLM通过协议与28个代表性实时MCP服务器连接，涵盖250种分布于金融、旅游、科学计算和学术检索等领域的工具，构建真实复杂的多步任务。任务要求模型在模糊任务说明下检索合适工具、规划执行路径、衔接多领域工具反馈。评测框架涵盖工具理解与使用、流程规划及完成任务等多个维度。

Result: 在MCP-Bench上的实验显示，即便是20个先进的LLM，面对该基准依然存在诸多挑战，模型难以高效完成多工具、多领域复杂任务。

Conclusion: MCP-Bench能够更系统、深入地评估LLM在现实复杂任务下的工具组合及任务完成能力，推动这一领域基准的提升。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [31] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于深度学习与NLP方法融合多模态EHR的新框架，在死亡率、住院时长和手术时间预测上较现有方法有显著提升，且对EHR数据干扰具高度鲁棒性，推荐用于ICU数据分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注结构化电子健康记录（EHR），常忽略自由文本记录中的临床洞见，且未充分利用结构化数据中的文本信息。

Method: 该研究设计并评估了一种结合自然语言处理技术的深度学习框架，融合多模态EHR数据，进行重症监护领域的死亡率与资源利用预测。模型在两个真实世界数据集、三类临床任务上与领先方法对比，并通过消融实验分析了医疗提示、自由文本和预训练句子编码器三大组件，还检验了模型对结构化EHR干扰的鲁棒性。

Result: 模型在三个临床任务及两套数据集上均优于现有最佳方法：死亡率预测提升1.6%/0.8%（BACC/AUROC），住院时长预测提升0.5%/2.2%（RMSE/MAE），手术时间预测提升10.9%/11.0%（RMSE/MAE），在不同干扰水平下鲁棒性更强。

Conclusion: 提出的深度学习框架在预测重症监护患者死亡率和资源利用方面有效且准确，特别是利用了带有提示学习的transformer编码器对多模态EHR进行分析，对结构化数据干扰具有高度抗性。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [32] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出了首个基于认知特征标注的阴谋论内容数据集ConspirED，并利用此数据集发现主流大模型在阴谋内容面前容易被误导，表明需要进一步提升AI对阴谋论内容的识别与抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 阴谋论削弱了公众对科学和机构的信任，并且具有难以被驳斥的特性。随着AI生成的虚假信息日益复杂，理解阴谋论内容中的修辞模式，对于设计有针对性的干预手段（如预先驳斥预警、评估AI漏洞）变得尤为重要。

Method: 提出了ConspirED数据集，该数据集收集了互联网阴谋文章中多句（80-120词）的片段，并依据CONSPIR认知框架进行标注，主要标注其阴谋论的认知特征。此外，作者利用该数据集：（1）开发了能识别阴谋认知特征及其主导特征的计算模型；（2）评估了大语言/推理模型（LLM/LRM）对阴谋输入的鲁棒性。

Result: 发现无论是所开发的识别模型还是主流大语言/推理模型，在面对阴谋内容时，输出都容易受到输入推理模式的影响，甚至在能够抵制其他经过事实核查的虚假信息时也出现类似问题。

Conclusion: AI模型目前在处理阴谋论内容时容易被误导，体现出对阴谋论推理模式的“对齐”，提示未来需要提升模型的辨别和抵御能力。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [33] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 本文发现主流多语言翻译基准FLORES+数据存在质量和文化偏见问题，当前评估协议易被简单方法利用。建议采用更通用和中立的内容，以更客观评估多语言MT系统的实际能力。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的多语言MT benchmark（FLORES+）虽数据覆盖广，但对其真实多语言评估能力及数据的文化和领域偏见缺乏深入检验。

Method: 人类评估四种语言的翻译质量，同时测试简单启发式如“实体复制”对BLEU分数的影响，以及用高质量训练数据在不同测试集上的性能比较。

Result: （1）数据质量低于标称标准。（2）源句子过于英美文化和特定领域导向。（3）简单作弊方法可获得较高BLEU分数，说明评估协议存在漏洞。（4）真正高质量模型在FLORES+上表现较差，在定制领域测试集上表现更好。

Conclusion: FLORES+基准在真正的多语言翻译评估方面存在严重缺陷，需改进其数据选择和质量控制，以更好反映现实翻译需求。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [34] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 论文提出了结合大语言模型指导的主题发现新方法SciTopic，通过多维度内容编码、空间优化与对比损失微调，显著提升了科学文献主题识别的准确性和效率，对研究者洞察新趋势和获取信息具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有的科学文献主题发现方法多依赖词嵌入，难以全面理解文献内容，无法有效处理复杂高维文本关系，因此需要更精准的方法提升主题识别能力。

Method: 提出SciTopic方法，首先利用文本编码器捕获文献内容（包括元数据、标题和摘要），然后设计空间优化模块，通过LLM指导的熵采样和三元组任务增强主题相关性和语境判别，最后基于LLM指导对文本编码器进行微调，优化三元组对比损失以提升不同主题区分能力。

Result: 在三个真实科学出版物数据集上，实验结果显示SciTopic显著优于当前最先进的科学主题发现方法，在准确性和效率上帮助研究者获得更深入和快速的洞察。

Conclusion: 结合LLM增强和新型空间优化策略，SciTopic有效提升了科学文献主题发现的效果和效率，为科学信息获取与知识发现提供更有力工具。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [35] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ 2024挑战吸引了37支队伍参与，围绕四大任务展开角逐，包括两个全新方向。参赛系统整体表现优异，反映出生物医学语义索引和问答技术持续进步，推动了领域创新和发展。


<details>
  <summary>Details</summary>
Motivation: BioASQ旨在推动生物医学语义索引和问答领域的进步，因此每年举行相关挑战，鼓励全球团队参与并对实际问题进行攻关。2024年的挑战不仅继续既有的任务，还增加了临床实体检测适应心脏病学和嵌套命名实体识别的新任务，以拓展研究的广度和深度。

Method: 本次BioASQ挑战包括四个任务：b任务、Synergy任务、MultiCardioNER（在多语种环境下将临床实体检测应用于心脏病学）、BIONNE（在俄语和英语中进行嵌套命名实体识别）。来自37个团队的700余次提交参与了这些任务。

Result: 大多数参赛系统在各项任务中表现出竞争力，说明该领域的技术仍在持续进步。

Conclusion: BioASQ 2024挑战推动了生物医学语义索引与问答技术的发展，不仅巩固了传统任务的成果，还通过新增任务探索了更多应用场景，整体上促进了业内技术的提升。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [36] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ 2025在六大生物医学相关任务上举行，参赛队伍积极参与，推动了问答和信息抽取等技术进步，多项任务系统表现优秀。


<details>
  <summary>Details</summary>
Motivation: BioASQ旨在推动生物医学领域大规模语义索引和自动问答技术的发展，通过组织国际竞赛促进新技术的涌现和评估。

Method: 竞赛设置了六个不同共享任务，涵盖生物医学语义索引、问答、多语言临床摘要、嵌套命名实体链接、心脏病临床编码以及肠脑轴信息抽取等，参赛团队提交并评估用于这些任务的系统。

Result: 有83支队伍参与比赛，提交了超过1000份系统结果。多个任务上取得了有竞争力的性能，证明领域内技术水平的提升。

Conclusion: 本届BioASQ竞赛吸引了大量参赛者，并且多种任务的系统都获得了竞争性结果，显示该领域技术的持续发展。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [37] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文关注于联邦学习中文本输入域多样性问题，提出AdaFD框架和多语域基准，实验结果优于现有方法，推动了多域联邦蒸馏实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着预训练语言模型（PLM）的广泛应用，全球模型+本地微调成为主流。但现实中各个客户端数据高度异构，特别是语言输入域差异大，导致模型难以泛化。以往联邦蒸馏研究多关注输出标签差异，却很少涉及输入语言域多样性，这限制了其实际应用价值。

Method: 提出了一个涵盖多语域、多样性数据的non-IID基准框架，用于真实环境下评估联邦学习。同时，提出了自适应联邦蒸馏（AdaFD）框架，能适应同质与异质setting下客户端的多语域数据分布差异。通过自适应机制提升模型在非独立同分布、多语言域环境下的性能。

Result: 实验证明，所提AdaFD框架能更好捕捉本地多样性，在多域non-IID场景下显著优于现有方法。提供了丰富的多域测试基准和性能提升数据，供进一步研究。

Conclusion: AdaFD提升了联邦学习在多语域非独立同分布实际场景中的泛化与性能，推动了联邦蒸馏方法的实际落地和研究。基准框架对行业评测和算法比较具重要意义。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [38] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本研究提出的一种新型轻量级生成式QDTS模型凭借多项技术手段，在速度、效率和准确性上全面超越传统工业生产基线，极具实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统抽取式摘要方法受限于多阶段流程引入的信息损失和架构瓶颈，以及对复杂搜索意图语义理解不足，难以有效支持实时查询驱动文本摘要。

Method: 整合了大模型蒸馏、监督微调、直接偏好优化以及展望式解码，将0.1B参数的轻量级模型转化为领域专用的QDTS专家。

Result: 新框架在工业相关多个评测指标上超越了当前生产级基线，达到了同行业的最佳表现，并展现出极高的部署效率。模型仅需334张NVIDIA L20 GPU就可支持约5万查询每秒且平均延迟低于55毫秒。

Conclusion: 提出的生成式QDTS模型在工业级指标上优于当前生产基线，实现了新的性能最佳记录，并且具有高效的部署表现，能够在高并发低延迟下运行。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [39] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 提出一种采样知识组合的新方法KCS，通过数据增强提升多跳问答的准确性和多样性，实验结果在主流数据集上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务由于数据稀疏性，导致语言模型往往学习到伪相关模式。此前方法多集中于通过内容规划与表达多样化提升问题生成多样性，但常常忽视了关键信息的集成，如上下文中相关句子的有效整合。

Method: 提出了一种名为Knowledge Composition Sampling (KCS)的框架，通过在给定上下文中对知识组合进行抽样，从而生成多样化的多跳问题。KCS将知识组合选择建模为句子级条件预测任务，并采用概率对比损失函数预测下一个最相关知识片段。推理时，引入随机解码策略以平衡准确性与多样性。

Result: 与现有主要基线方法相比，KCS的知识组合选择准确率提升了3.9%；用于数据增强后，在HotpotQA和2WikiMultihopQA两大数据集上均取得性能提升。

Conclusion: KCS能通过多样化知识组合提升多跳问答生成的准确率和多样性，具有推广和实际应用潜力。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [40] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 本文指出现有图-语言模型(GLM)评估基准存在不足，提出新基准CLEGR专门用于多模态推理测试，发现GLM结构并未显著提升性能，当前模型在结构推理上仍有明显短板。


<details>
  <summary>Details</summary>
Motivation: 当前GLM评估基准主要采用节点分类数据集，无法充分评估图与语言的多模态推理能力。作者发现仅用单一模态信息即可获得高性能，表明这些基准未真正考察图与语言的整合价值。

Method: 作者提出了CLEGR基准，包含不同复杂度水平的问题，需对图结构和文本语义进行联合推理。CLEGR通过合成图生成流程与配套问题实现多模态推理评测。随后，系统性评估了典型GLM架构。

Result: 即使仅用软提示LLM，性能也与结合完整GNN的GLM相当，质疑在LLM中嵌入图结构是否必要。此外，GLM在需结构推理的任务表现大幅下降，说明当前GLM在图推理能力上存在显著不足。

Conclusion: 当前基准未能有效推动多模态图-语言推理发展，CLEGR弥补了此评测空白，并揭示GLM架构本身在结构推理方面的不足，有助于推动学界探索更显式的图与语言推理模型。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [41] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 该文提出一种结合声学特征和生成式模型的新型命名实体纠错方式，能显著提升语音识别中实体的正确率，尤其适合处理转录单词与真实实体差异较大的情况。


<details>
  <summary>Details</summary>
Motivation: 端到端语音识别系统在转录领域特定的专有名词时，常常出现错误，严重影响后续任务效果。虽然已有许多轻量级命名实体纠错（NEC）模型，但这些模型在目标实体与错误转录单词差异较大时常常无效。

Method: 提出了一种新颖的NEC方法，利用语音声学特征检索候选实体，并采用生成式方法自动标注ASR中的实体错误，再将错误实体用正确实体替换。方法适用于单词形式差异较大的场景。

Result: 在开源与自建测试集上的实验结果显示，所提NEC方法能显著提升命名实体的识别准确率。

Conclusion: 通过结合语音声学特征与生成式纠错机制，有效解决了端到端语音识别系统在专有名词转录方面的困难，并提升了实体识别准确度，验证了方法的有效性。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [42] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 该论文提出了多语言多标签层次化模型HArch，显著优于提示式大模型，有效提升了隐含话语关系识别在多语言环境下的表现，并取得最新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 目前的隐含话语关系识别（IDRR）主要基于单一语言和单标签分类，未能充分考虑多语言及层次化话语意义之间的依赖。为提升多语言环境下IDRR模型的泛化与表现，需构建新的多标签及多语言模型。

Method: 提出首个用于多语言和多标签IDRR的层次化分类模型 HArch，在PDTB 3.0 框架下，利用话语意义的层次依赖，预测三个层次上的概率分布。分别采用 RoBERTa 和 XLM-RoBERTa 作为预训练编码器骨干，并与最新大模型 GPT-4o 及 Llama-4-Maverick 在少样本提示下进行对比。

Result: RoBERTa-HArch 在英语环境中表现最佳，XLM-RoBERTa-HArch 在多语言环境下表现最佳。微调后的模型在各种语言配置下的表现显著优于使用提示的大模型。提出的层次方法还在 DiscoGeM 1.0 语料库上刷新了 SOTA。

Conclusion: 专为任务微调的层次化多标签多语言模型在隐含话语关系识别任务中优于当前主流的提示式大模型，验证了层次依赖与微调在多语言IDRR中的有效性。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [43] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文分析了LLM生成文本过程中，因分词不一致导致隐写和水印系统鲁棒性下降的问题，并提出了两种有效的解决办法，实验证明可显著提升文本流畅度及水印检测率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的文本生成能力提升带来了更高质量的文本隐写和对防止恶意滥用的水印需求。尤其是，令Alice与Bob端的分词不一致(Tokenization Inconsistency, TI)成为隐写和水印系统中的薄弱环节，影响其鲁棒性。

Method: 对分词不一致(TI)影响隐写与水印的鲁棒性做系统性分析，发现关键问题在于不常用且临时出现的token。提出两种针对性解决方案：1) 面向隐写的逐步校验方法；2) 面向水印的后处理回滚方法。通过实验验证方案有效性。

Result: （1）在隐写场景中，相比以往间接消歧方法，直接消除分词不一致能提升文本流畅性、隐蔽性及抗检测能力；（2）在水印场景中，消除分词不一致后，水印的检测正确率和抗攻击能力均得以增强。

Conclusion: 针对LLM文本生成中分词不一致问题，提出了隐写和水印领域可行且高效的消除TI方法，并显著提高了鲁棒性和性能。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [44] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: rStar2-Agent-14B通过高效Agentic RL和创新方法，仅用有限算力就实现了顶尖数学推理表现，在相关任务上优于大模型，且具有效能与泛化优势。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在复杂数学推理和工具使用上仍有限，且需要大量算力和资源。研究旨在通过Agentic RL方法，用更少资源实现更强的推理与泛化能力。

Method: 提出并应用了三项关键创新：高效的RL基础设施和Python环境，GRPO-RoC（Resample-on-Correct）智能强化学习算法，以及逐步提升认知能力的高效训练方案。模型由非推理SFT起步，经过多阶段RL精炼能力。

Result: rStar2-Agent-14B在AIME24和AIME25等数学推理基准上，分别以80.6%和69.8%的pass@1分数取得SOTA，且显著优于DeepSeek-R1（671B）等大模型。同时还展现出科学推理和工具泛化能力。

Conclusion: rStar2-Agent-14B通过创新的Agentic RL方法和高效的训练手段，以较低的计算成本达到了前沿的数学推理能力，并在多个领域表现出良好的泛化能力。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [45] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: 本文提出基于语义三元组的DP-ST方法，在本地差分隐私下实现邻域感知的隐私文本生成，配合大语言模型后处理，有效提高了较低隐私参数下的文本连贯性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有很多自然语言处理领域中结合差分隐私（DP）的方法，目的是通过在DP保障下对文本进行变换，以保护隐私。但在本地DP下，将输入文本与任何其他可能文本变得不可区分是一项极具挑战性的任务，通常只能在很高的隐私参数ε下实现。

Method: 提出了一种新的DP-ST方法。该方法利用语义三元组进行邻域感知的本地DP文本生成。结合了分而治之的思想，并在DP邻域内实现更灵活的隐私保障。同时，还结合大语言模型（LLM）进行后处理，以提升文本的连贯性和可用性。

Result: 实验评估证明DP-ST方法有效，尤其是在将差分隐私理念限制在邻域内时，可以在较低的ε值下生成连贯且隐私性强的文本，兼顾隐私和实用性。

Conclusion: 连贯性对在合理ε水平下实现隐私和可用性的平衡至关重要。DP-ST方法有效突破了本地DP文本生成的ε瓶颈，为自然语言处理中的隐私保护提供了新方案。

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [46] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 通过微调通用LLM嵌入模型（如Stella等），可无需额外数据仍显著提升隐性仇恨言论识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论（IHS）由于其间接、隐晦表达偏见，如讽刺、暗语、细微暗示，极难被准确识别。传统方法依赖外部知识或上下文信息来提高检测准确度。本文旨在探究无需额外数据，仅通过微调基于大语言模型（LLM）的通用嵌入模型识别IHS的效果。

Method: 本文采用现有先进的基于LLM的嵌入模型，如Stella、Jasper、NV-Embed和E5，对其进行微调，仅用相关数据集训练，无需引入外部知识或情感、上下文信息。随后在多组隐性仇恨言论数据集上进行实验。

Result: 微调上述模型后，在数据集内测试提升了最多1.10个百分点，在跨数据集评估中提升了最高20.35个百分点（F1-macro分数），达到了当前最佳性能。

Conclusion: 即使不使用额外上下文或知识，只通过微调高级LLM嵌入模型，也能在隐性仇恨言论检测任务中取得优异效果，具有高效性和实用性。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [47] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: 论文提出自适应解码方法GUARD，通过融合全局与局部不确定性及token计数惩罚，有效平衡开放式文本生成的连贯性与多样性，大幅提升生成速度，表现优异。代码公开。


<details>
  <summary>Details</summary>
Motivation: 开放式文本生成需要在连贯性与多样性间取得平衡，现有对比搜索类解码策略虽能改善，但依赖超参数且计算成本高，实用性受限。

Method: 提出了一种自适应解码方法GUARD，通过“Glocal”不确定性驱动框架结合全局熵和局部熵偏差，整合长短期不确定性信号。引入基于token计数的惩罚项以降低计算开销。全局熵的设计能有效缓解不确定性剧烈波动，并有无偏和一致性理论保证。

Result: GUARD方法在文本多样性和连贯性上取得良好平衡，生成速度明显提升。在多维度文本质量对比测试中，GUARD的表现获得了人类和LLM评估者的高度认可。

Conclusion: GUARD是一种高效且效果良好的自适应文本解码方法，在开放式文本生成领域可有效兼顾多样性、连贯性及速度，具有显著实际价值。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [48] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 本研究对比分析了真实与LLM合成CBT对话的情绪动态，发现合成对话在情绪真实性、变化和表达上与真实对话显著不同，限制了其在心理健康应用领域的有效性，并发布了真实CBT数据集RealCBT供后续研究。


<details>
  <summary>Details</summary>
Motivation: 随着LLM合成的心理治疗对话在训练模型和补充数据中应用日增，亟需验证这些合成数据是否能真实反映真实心理咨询中的复杂情绪模式和互动，为后续相关研究和实际应用提供依据。

Method: 采用Utterance Emotion Dynamics框架，从效价、唤醒度和支配度等角度，细粒度分析真实与合成认知行为治疗（CBT）对话的情绪轨迹，并比较整体及不同说话者角色的表现，使用公开视频转录的真实对话和CACTUS数据集中LLM生成的合成对话。

Result: 合成对话在流畅性和结构上表现良好，但在核心情绪属性上与真实对话存在显著差异，如情绪变化较小、情绪丰富度低、情感反应和调节不够真实，真实和合成说话者之间的情绪轨迹相似性较低，尤其表现在来访者角色。

Conclusion: 目前大型语言模型生成的心理治疗对话与真实对话在情绪动态和表达方面有显著差异，尤其是在情感变化和真实反应上，显示出合成数据在心理健康领域应用中的重要局限性。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [49] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出了ROSI方法，仅需一次秩一权重修改即可增强大模型的安全拒绝能力，无需微调，提升安全性的同时不影响模型表现，且适用范围广，是提升LLM安全的简便有效新路径。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型的安全对齐方法通常涉及调整内部表示以拒绝有害请求，但这些机制容易被模型参数方向的删减绕过。作者希望能提出一种更稳健且高效的安全增强方法。

Method: 提出Rank-One Safety Injection (ROSI)方法，通过白盒安全方向注入，将模型激活永久性引导到拒绝有害请求的子空间。ROSI是对所有残差流写入矩阵进行一次秩一权重修改，无需微调，根据少量有害/无害指令配对即可计算所需方向。

Result: ROSI显著提升了安全拒绝率（由Llama Guard 3评估），并且在标准基准测试（如MMLU、HellaSwag和Arc）上保持了模型的效用。ROSI还可用于“未审查”模型的安全重新对齐，证明其作为最后一公里安全措施的有效性。

Conclusion: ROSI是一种廉价而有效的安全增强机制，可以与传统高资源消耗的微调方法互补，其针对性强且可解释性好，有助于提升大模型的安全性。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [50] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本研究首次探讨了认知扭曲检测在英语之外的新语种和不同文本风格下的表现，发现域适应是提升模型泛化能力的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 由于青少年心理健康问题日益突出，研究者希望通过自动化手段及早检测数字文本中的心理困扰迹象，特别是关注认知扭曲（一种加剧心理压力的非理性思维方式）的识别。

Method: 本文首次深入研究认知扭曲检测在跨语言（英语到荷兰语）和跨文体环境下的泛化能力，分析了荷兰青少年论坛帖子的表现，并比较了不同模型和域适应方法。

Result: 研究发现，语言和文体的变化会显著影响模型的性能，但域适应方法在提升模型的泛化能力方面表现最为突出。

Conclusion: 跨语言与跨文体的认知扭曲检测具有挑战性，但采用域适应技术能更好地提升模型的实用性。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [51] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 论文比较了多种主流机器学习和深度学习模型在多模态抑郁检测中的应用效果，揭示了各模型在不同数据模态下的优缺点，并为心理健康预测提出了新的多模态建模建议。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在提升利用多模态数据（音频、视频和文本）进行抑郁检测的效果，并响应首届Multimodal Personality-Aware Depression Detection Challenge。

Method: 探索并比较了XGBoost、基于transformer的架构以及大语言模型对不同模态特征的处理方式，综合运用了机器学习和深度学习方法。

Result: 不同类型模型在捕捉与抑郁相关的多模态信号时各有优劣，论文分析了这些模型在多模态心理健康预测中的表现。

Conclusion: 针对多模态抑郁检测，各模型在不同数据模态下有不同表现，建议采用更有效的多模态表示方法以提升心理健康检测能力。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [52] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 为解决事件时间关系抽取中少数类识别难及长距离依赖捕捉问题，作者提出GDLLM方法，通过GAT和软推理机制增强LLM性能，在两个公开数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 在事件时间关系抽取（ETRE）任务中，少量样本类别识别困难，且现有小型与大型语言模型在处理不均衡数据时存在局限。如SLMs预训练知识受限，而LLMs的手工提示或指令易引入噪声，影响长距离依赖的判断。

Method: 作者提出了一种全局距离感知（GDLLM）方法：结合基于图注意力网络（GAT）的距离感知图结构帮助LLMs捕捉事件间长距离依赖；同时，设计了基于软推理的时序特征学习范式，增强短距离关系识别，并将LLMs生成的概率信息融合到多头注意力机制中。

Result: 在TB-Dense和MATRES两个公开数据集上的实验表明，该方法有效提升了少数类关系的识别效果，综合表现达到最新SOTA水平。

Conclusion: GDLLM能够有效获取全局特征，提升模型对少数类（数据不平衡场景下的弱势类别）事件关系的识别能力及整体抽取表现。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [53] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了可扩展评测框架和新基准用于评估RAG系统在多源信息整合与长文本生成上的能力。实验发现生成效果极度依赖检索和推理，多源信息任务显著提升了系统挑战性。


<details>
  <summary>Details</summary>
Motivation: 目前检索增强(RAG)系统大多只评估在单一信息源或短/事实型回答下的效果，现实应用中常常需要整合多个信息源并生成长文摘要。该工作旨在填补针对多源信息综合的评估框架和基准缺失。

Method: 提出了一个可扩展的框架，用于构建挑战RAG系统多源信息整合与长文本生成能力的评测基准。基于该框架，构建了两个新基准MSRS-Story（叙事综合）和MSRS-Meet（会议摘要），分别针对不同任务类型，并在大规模语料下进行检索和生成实验。

Result: 实验显示，RAG系统的生成质量高度依赖于检索效果，不同任务下检索效果差异明显。即使在“检索全知”的设定下，多源综合依然困难，但专用的推理模型在信息整合阶段明显优于普通大模型。

Conclusion: 多源信息整合与生成是RAG系统面临的重要挑战，优质检索和推理能力对提升长文生成效果至关重要。文中基准和框架为后续研究提供了有力支撑。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [54] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 本论文系统评估了55种语言上的大语言模型量化方案，发现高资源语言和大模型在4-bit量化时翻译性能较好，但低资源和多样语言在2-bit量化时退化明显。GGUF算法表现最稳健，语言匹配的校准对于低比特有正面帮助，为量化条件下多语言LLM部署提供了参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）越来越大，对资源受限硬件的部署变得困难，量化技术是解决这一难题的关键。以往的研究主要关注高资源语言，针对多语言任务的量化影响却鲜有系统探索。

Method: 作者在55种语言上、使用5个参数规模不同（1.7B至70B）的LLM进行大规模后训练量化（PTQ）测试，比较了AWQ、BitsAndBytes、GGUF、AutoRound四种量化算法，并分析了量化精度、解码超参数、校准语言等因素的交互影响。

Result: 4-bit量化对于高资源语言和大模型通常能保持较好翻译质量，但低资源或类型多样的语言在2-bit量化下性能下降明显。不同量化算法和模型规模共同决定模型健壮性，其中GGUF算法即使在2-bit精度下也有最稳健表现。校准语言如果与目标语言匹配，在低比特量化下有辅助作用。

Conclusion: 不同量化策略、模型规模和校准方式对多语言机器翻译质量影响显著。如果在低资源场景下部署多语言LLM，需综合选择量化与校准方法以提升性能。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [55] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 该论文提出了可端到端评估语音大模型的SageLM，结合语义和声学评价，并采用合成偏好数据及理据监督方法，在评估准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前语音到语音（S2S）大语言模型在实现自然人机交互中扮演重要角色，但其评估面临基础性挑战，尤其是如何综合评价语音模型的语义与声学表现。

Method: 提出了SageLM，一个端到端的、多方面、可解释的语音大语言模型。SageLM同时考量语义和声学维度，通过理据监督提升可解释性和引导模型学习。设计了SpeechFeedback合成偏好数据集，并采用两阶段训练策略，解决语音偏好数据稀缺问题。

Result: SageLM在语义和声学维度的训练后，与人工评估者的一致率达到82.79%，对于级联方法和基于SLM的方法，分别提升了至少7.42%和26.20%。

Conclusion: SageLM能够全面、可解释地评估S2S大语言模型，在语音自动评价任务中显著优于传统方法。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [56] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 本文分析了当前大语言模型工具调用代理在多轮对话中的典型错误，提出了自动化输入重构框架IRMA，显著提升了任务表现，超越其他主流方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在推理和计划能力上的进展，使其有望在复杂环境中作为自主代理执行工具调用。然而，在多轮对话场景下（如τ-bench），现有代理在连贯推理、遵循领域规则以及长期信息提取方面存在明显不足。论文动机是分析这些弱点并探索改进方法。

Method: 本论文首先通过手动分析代理在多轮会话中的失败类型，总结常见错误。随后，作者尝试对工具调用代理的输入进行不同的重构实验。最终，提出了IRMA（Input-Reformulation Multi-Agent）框架，自动将用户查询与相关领域规则、工具建议进行扩充和重构，从而引导代理更高效地决策。

Result: 实验结果显示，IRMA在整体pass^5分数上分别比ReAct、Function Calling和Self-Reflection高出16.1%、12.7%和19.1%。IRMA方法在连续性和可靠性上显著优于其他方法。

Conclusion: IRMA框架通过自动化输入重构，有效提升了工具调用代理在多轮、动态对话环境中的推理连贯性和任务完成可靠性，优于当前主流方法。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [57] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 提出两阶段结构感知的示例选择策略，兼具效率、泛化和性能优势，在多个语义解析基准及不同主流LLM上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前ICL在结构化预测任务中，示例选择策略大多忽视了结构对齐，导致性能和泛化能力不足。因此亟需一种兼顾效率、泛用性和表现能力的新方法提升结构化任务的ICL效果。

Method: 首先，使用结构感知的监督方式微调基于BERT的检索模型，使其可以选取语义相关且结构对齐的范例。其后，通过一个插件模块增强检索器，在隐藏表示中强化对语法相关信息的捕捉。该插件模块对模型无关，可轻松集成进现有流程，且开销很小。

Result: 在四组涉及三类语义解析任务的基准测试中，方法在多个主流LLMs推理场景下均超越现有各种示例选择基线。

Conclusion: 提出的两阶段示例选择策略在结构化预测任务中显著提升了范例选择的效果，并在多个基准任务和不同的大型语言模型（LLM）上取得了优于现有方法的性能。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [58] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出了ProactiveEval统一评测框架，系统评估了22种LLM在主动对话能力上的表现，并指出不同模型在目标规划与会话引导方面具有不同优势，为后续模型优化提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM主动对话研究多聚焦于特定领域或任务，导致评测零散、对模型主动性能缺乏全面理解，因此提出统一框架系统性评估LLM主动会话能力。

Method: 提出了ProactiveEval框架，将主动对话任务分解为目标规划和对话引导两个子任务，并设计了一套统一的评测指标，可以自动生成多样化、具有挑战性的评测数据，在6个不同领域中开发了328个评测环境，进行多模型实验对比。

Result: 实验发现，不同LLM在主动对话不同子任务上表现有显著差异；推理能力对主动性表现有重要影响；ProactiveEval为未来模型开发和评测提供了参考。

Conclusion: 通过ProactiveEval框架对22种不同LLM的主动对话能力进行了系统评估，发现DeepSeek-R1在目标规划任务上表现突出，Claude-3.7-Sonnet在对话引导任务上表现更好，同时也探讨了推理能力对主动行为的影响。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [59] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 论文提出LETHE，通过内部知识稀释和外部证据扩充，有效消除LLMs后门攻击，显著优于主流防御方法，且成本低、稳健。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）尽管在多项NLP任务中表现卓越，但容易受到后门攻击。当前的后门防御方法存在覆盖面窄、仅限检测、领域有限，或无法应对更复杂攻击场景（如模型编辑、多个触发器及无触发器攻击）的不足。

Method: 提出一种新的防御方法LETHE，通过知识稀释（包含内部和外部机制）消除LLMs中的后门行为。内部通过轻量级数据集训练干净模型并与后门模型融合，稀释模型记忆中的后门影响。外部利用相关的、无害的证据扩充输入，分散模型对后门特征的关注。

Result: 在五种主流LLM，以及分类和生成任务环境下，LETHE在八种后门攻击上，对比八个现有防御基线实现显著优势。LETHE可将高级后门攻击成功率最多降低98%，且保持模型效用。该方法成本低、对自适应后门攻击具有鲁棒性。

Conclusion: LETHE作为一种新型后门防御方法，通过知识稀释和注意力分散机制，有效消除了LLMs中的后门行为，性能和成本均优于现有方法，并能应对复杂攻击情形。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [60] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文提出EASI-RAG方法，解决中小企业RAG部署难题，实验证明该方法支持快速部署、用户高采纳率和答案高准确，未来可拓展到更多场景。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统虽能缓解大模型幻觉及知识老化问题，但中小企业由于资源和NLP技术能力有限，落地实施仍存在障碍，因此需要一套适合工业SME的RAG实施方案。

Method: 提出了一套基于方法工程的结构化敏捷流程EASI-RAG，包含明确定义的角色、活动和技术，并通过实际环境检测实验室的案例研究进行了验证。

Result: EASI-RAG系统在无先验RAG经验的团队下，实现了一个月内部署、用户持续采用、动态迭代优化，并带来了较高的答案准确率和数据可靠性。后续可拓展至不同场景与与微调模型结合。

Conclusion: EASI-RAG可以有效帮助中小企业快速部署RAG系统，实现高用户采纳率，提高答案准确率和底层数据的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [61] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本文提出动态路由胶囊网络用于关系抽取，在主流数据集上优于当前方法，但在高噪声大数据集如Wikidata上遇到瓶颈。噪声和表征方式是关系抽取领域未来的重要挑战。


<details>
  <summary>Details</summary>
Motivation: 句子级关系抽取（RE）是自然语言处理中的重要任务。目前主流方法在某些数据集上表现不错，但对于如Wikidata这样更大规模、标注可能带噪声的数据集表现不佳。作者想找出现有方法的优缺点，并进一步提升模型性能。

Method: 作者提出在胶囊网络中引入动态路由机制来进行句子级关系抽取，并与现有主流方法进行对比实验。此外，还分析了模型在不同数据集，尤其是高噪声数据集上的表现，并引入脑科学中的“再表征”概念，对模型效果进行解释。

Result: 该方法在Tacred、Tacredrev、Retacred和Conll04这几个常用数据集上超越了现有方法，但在Wikidata大规模数据集上的效果不佳。进一步分析发现，Wikidata数据集的标签噪声是性能下降的重要原因。实验还发现该模型能更好地完成“再表征”，即能让相关项的表达更相似，从而提升关系对比的效果。

Conclusion: 动态路由胶囊网络能有效提升句子级关系抽取，但在有高噪声标签的数据集（如Wikidata）上依然存在挑战。噪声控制和更有效的再表征方法是后续需要研究的问题。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [62] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本研究提出利用神经-符号混合系统，结合LLM与符号求解器，自动解答复杂税务申报问题，大幅提升准确性与可审计性，并将实际成本压缩到比人工报税更低，展示其在可靠、公平自动化税务服务方面的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有税务自动化系统难以兼顾复杂推理、高准确性和可审计性。现代LLM在此任务中易出错，错误代价高昂，因此需要一种新方法提升自动化报税的准确性与可追溯性。

Method: 提出将大型语言模型（LLMs）与符号求解器结合，首先将文本税务规则翻译为形式化逻辑程序，并检索相关案例作为示例，进行税务计算和推理。方法在SARA挑战数据集上评估，并引入基于真实税务错误罚款的系统成本估算法。

Result: 结合上游规则翻译与智能检索案例可显著提升自动化报税系统的性能，将系统运维成本降至远低于真实世界的平均水平。同时验证了提出的成本估算方法，在代表性数据集上取得优良效果。

Conclusion: 神经-符号混合架构在实现高可靠性、可审计的自动化报税系统方面具有很大潜力，并在经济上可行，有望提升大众获取合规报税帮助的公平性。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [63] [Unclustered BWTs of any Length over Non-Binary Alphabets](https://arxiv.org/abs/2508.20879)
*Gabriele Fici,Estéban Gabory,Giuseppe Romana,Marinella Sciortino*

Main category: cs.DM

TL;DR: 该论文证明了在字母数≥3的情况下，任意长度都可构造BWT run数最大的项链，还给出了数量下界。二元情况仍然悬而未决。


<details>
  <summary>Details</summary>
Motivation: BWT在数据压缩和字符串处理领域广泛应用，作者关注BWT run聚集的最坏情况，以探索其结构极限及相关计数问题。

Method: 数学构造证明，讨论不同字母表的情况，并分析BWT run的最大化特性。

Result: 与二元字母表不同，多元字母表下可构造完全不聚集的BWT项链，并给出其数量的下界。二元情形仍为未解难题，与阿廷原根猜想相关。

Conclusion: 对于任意整数n > 0和字母表大小k ≥ 3，总存在一个长度为n的项链，使其BWT完全不聚集，即由n个run组成且无相邻相同符号。同时还对这类项链的数量给出了下界。

Abstract: We prove that for every integer $n > 0$ and for every alphabet $\Sigma_k$ of
size $k \geq 3$, there exists a necklace of length $n$ whose Burrows-Wheeler
Transform (BWT) is completely unclustered, i.e., it consists of exactly $n$
runs with no two consecutive equal symbols. These words represent the
worst-case behavior of the BWT for clustering, since the number of BWT runs is
maximized. We also establish a lower bound on their number. This contrasts with
the binary case, where the existence of infinitely many completely unclustered
BWTs is still an open problem, related to Artin's conjecture on primitive
roots.

</details>


### [64] [Enhancing Soft Happiness via Evolutionary Algorithms](https://arxiv.org/abs/2508.20934)
*Mohammad Hadi Shekarriza,Dhananjay Thiruvadya,Asef Nazari*

Main category: cs.DM

TL;DR: 本文针对NP难的soft happy colouring问题，提出并测试了遗传及记忆遗传算法，通过局部优化初始化有效提升性能，在社区检测和“开心顶点”数量方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 对于图着色问题，最大化满足一定比例邻居同色（即“开心”顶点）数的着色被称为soft happy colouring，该问题与社区检测密切相关，而且已知为NP难问题。本文旨在探索更优的启发式算法来求解这一问题，以提升顶点“开心度”并优化社区结构检测。

Method: 本文提出了基于进化思想的遗传算法（GA）和记忆遗传算法（Memetic Algorithm，MA）用于soft happy colouring，并在大量随机生成的部分着色图上进行实验比较。此外，还将本领域已有的局部优化方法如Local Maximal Colouring（LMC）和Local Search（LS）用于初始化种群，提升进化算法效果。

Result: 统计实验表明，当遗传算法与记忆遗传算法的初始种群由LMC这样的局部优化方法增强时，两者都能在社区结构检测上取得较高的平均准确率。记忆遗传算法能找到更多的“开心”顶点，且进化算法在找到完整解的数量上优于其他竞品方法。

Conclusion: 进化类算法（尤其在配合局部优化初始化时）是求解soft happy colouring及社区检测的有效途径，能得出更多完整解及更高“开心度”，表现优于其他现有启发式方法。

Abstract: For $0\leq \rho\leq 1$, a $\rho$-happy vertex $v$ in a coloured graph shares
colour with at least $\rho\mathrm{deg}(v)$ of its neighbours. Soft happy
colouring of a graph $G$ with $k$ colours extends a partial $k$-colouring to a
complete vertex $k$-colouring such that the number of $\rho$-happy vertices is
maximum among all such colouring extensions. The problem is known to be
NP-hard, and an optimal solution has a direct relation with the community
structure of the graph. In addition, some heuristics and local search
algorithms, such as {\sf Local Maximal Colouring} ({\sf LMC}) and {\sf Local
Search} ({\sf LS}), have already been introduced in the literature. In this
paper, we design Genetic and Memetic Algorithms for soft happy colouring and
test them for a large set of randomly generated partially coloured graphs.
Memetic Algorithms yield a higher number of $\rho$-happy vertices, but Genetic
Algorithms can perform well only when their initial populations are locally
improved by {\sf LMC} or {\sf LS}. Statistically significant results indicate
that both Genetic and Memetic Algorithms achieve high average accuracy in
community detection when their initial populations are enhanced using {\sf
LMC}. Moreover, among the competing methods, the evolutionary algorithms
identified the greatest number of complete solutions.

</details>


### [65] [Measuring Ransomware Lateral Movement Susceptibility via Privilege-Weighted Adjacency Matrix Exponentiation](https://arxiv.org/abs/2508.21005)
*Satyam Tyagi,Ganesh Murugesan*

Main category: cs.DM

TL;DR: 作者提出了用图论与概率建模组合的方法，量化网络资产间横向移动和潜在爆炸半径，有助于制定精细化隔离和分段安全策略，优先控制高风险服务以降低大规模被攻陷的概率。


<details>
  <summary>Details</summary>
Motivation: 勒索软件攻击的危害主要取决于攻击者在网络中横向移动和传播到更多资产的能力。现有的安全控制往往缺乏量化横向移动易感性的指标，因此需要一种系统化的方法来衡量和减少网络被攻破的风险。

Method: 作者提出了一种基于图论的方法：以资产为节点、服务为有向边，构建多重有向图，并通过为每种服务分配“可枢纽潜力”因子（pivot potential factor, π(s)），将横向移动建模为概率过程。通过迭代计算K跳妥协概率矩阵，量化网络内部妥协的传播过程，并据此设计相关指标。

Result: 实验表明，对交互式控制端口（如SSH与RDP）赋予更高π(s)时，其在网络中容易被用作横向移动的“跳板”。若对这些高风险服务的连通性进行裁剪，能显著降低横向移动易感性与爆炸半径相关指标。这一结果也与CISA、MITRE ATT&CK、NIST等安全最佳实践保持一致。

Conclusion: 该基于图论与概率建模的方法能够有效评估和量化网络中资产横向移动的风险与潜在影响，有助于企业进行分段防护和安全控制优先级排序，从而更好地防止勒索软件大范围传播。

Abstract: Ransomware impact hinges on how easily an intruder can move laterally and
spread to the maximum number of assets. We present a graph-theoretic method to
measure lateral-movement susceptibility and estimate blast radius. We build a
directed multigraph where vertices represent assets and edges represent
reachable services (e.g., RDP/SSH) between them. We model lateral movement as a
probabilistic process using a pivot potential factor $\pi(s)$ for each service.
This allows us to iteratively compute a $K$-hop compromise probability matrix
that captures how compromise propagates through the network. Metrics derived
from this model include: (1) Lateral-Movement Susceptibility (LMS$_K$): the
average probability of a successful lateral movement between any two assets
(0-1 scale); and (2) Blast-Radius Estimate (BRE$_K$): the expected percentage
of assets compromised in an average attack scenario. Interactive control (SSH
22, RDP 3389) gets higher $\pi(s)$ than app-only ports (MySQL 3306, MSSQL
1433), which seldom enable pivoting without an RCE. Across anonymized
enterprise snapshots, pruning high-$\pi(s)$ edges yields the largest
LMS$_K$/BRE$_K$ drop, aligning with CISA guidance, MITRE ATT\&CK (TA0008:
Lateral Movement), and NIST SP~800-207. The framework evaluates
(micro)segmentation and helps prioritize controls that reduce lateral movement
susceptibility and shrink blast radius.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [Formal equivalence between global optimization consistency and random search](https://arxiv.org/abs/2508.20671)
*Gaëtan Serré*

Main category: cs.FL

TL;DR: 作者利用定理证明器和概率论方法，正式证明了全局优化算法一致性对空间采样要求的充要性，并提出了可用于严格理论分析的算法模型。


<details>
  <summary>Details</summary>
Motivation: 本文旨在对全局优化算法的一致性进行形式化证明，特别关心在Lipschitz连续函数上的算法一致性。该问题理论意义重大，对于理解和校验各种优化算法的可靠性具有重要价值。

Method: 作者使用L$\exists$$\forall$N定理证明器和Mathlib库，并将全局优化算法形式化为初始概率测度加上一系列Markov核。通过Ionescu-Tulcea定理，构造了算法迭代序列上的概率测度，然后在这一框架下进行一致性证明。

Result: 证明了“只有采样整个搜索空间，随机迭代全局优化算法在Lipschitz连续函数上一致”，同时定义并形式化了足够通用且可用于正式证明的优化算法模型。

Conclusion: 对于Lipschitz连续函数，任何随机迭代式全局优化算法一致性的充要条件是能够覆盖整个搜索空间。该结论强调了广泛采样空间对算法长期表现的关键作用。

Abstract: We formalize a proof that any stochastic and iterative global optimization
algorithm is consistent over Lipschitz continuous functions if and only if it
samples the whole search space. To achieve this, we use the
L$\exists$$\forall$N theorem prover and the Mathlib library. The major
challenge of this formalization, apart from the technical aspects of the proof
itself, is to converge to a definition of a stochastic and iterative global
optimization algorithm that is both general enough to encompass all algorithms
of this type and specific enough to be used in a formal proof. We define such
an algorithm as a pair of an initial probability measure and a sequence of
Markov kernels that describe the distribution of the next point sampled by the
algorithm given the previous points and their evaluations. We then construct a
probability measure on finite and infinite sequences of iterations of the
algorithm using the Ionescu-Tulcea theorem.

</details>


### [67] [Evaluating Massively Parallel Algorithms for DFA Minimisation, Equivalence Checking and Inclusion Checking](https://arxiv.org/abs/2508.20735)
*Jan Heemstra,Jan Martens,Anton Wijs*

Main category: cs.FL

TL;DR: 本文针对DFA最小化与等价检查问题，系统分析并实现了多种并行算法在GPU上的性能表现，提出一种新算法在部分场景下效果更优，并展示了GPU平台在相关问题上的处理优势。


<details>
  <summary>Details</summary>
Motivation: 提升DFA最小化和等价性检查的计算效率，尤其是利用GPU并行能力来加速相关算法。

Method: 实现并评估了四种基于GPU的大规模并行DFA最小化算法；提出了一种结合并行分区优化与部分传递闭包的新算法，并将Hopcroft-Karp算法并行化以适应GPU架构。

Result: 理论最优的算法在实际GPU上资源消耗大，表现不理想；并行分区优化算法更适用于GPU，新提出的改进算法在某些测试上性能优于已有方法。语言等价性和包含问题可通过GPU并行模型高效处理。

Conclusion: 算法理论最优并不总是在实际GPU运行中效果最佳，另外提出的新算法在某些基准上表现更佳。

Abstract: We study parallel algorithms for the minimisation and equivalence checking of
Deterministic Finite Automata (DFAs). Regarding DFA minimisation, we implement
four different massively parallel algorithms on Graphics Processing
Units~(GPUs). Our results confirm the expectations that the algorithm with the
theoretically best time complexity is not practically suitable to run on GPUs
due to the large amount of resources needed. We empirically verify that
parallel partition refinement algorithms from the literature perform better in
practice, even though their time complexity is worse. Furthermore, we introduce
a novel algorithm based on partition refinement with an extra parallel partial
transitive closure step and show that on specific benchmarks it has better
run-time complexity and performs better in practice.
  In addition, we address checking the language equivalence and inclusion of
two DFAs. We consider the Hopcroft-Karp algorithm, and explain how a variant of
it can be parallelised for GPUs. We note that these problems can be encoded for
the GPU-accelerated model checker \GPUexplore, allowing the use its lockless
hash table and fine-grained parallel work distribution mechanism.

</details>
