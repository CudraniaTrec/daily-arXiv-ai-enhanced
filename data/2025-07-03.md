<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Advanced LPeg techniques: A dual case study approach](https://arxiv.org/abs/2507.01272)
*Zixuan Zhu*

Main category: cs.PL

TL;DR: 该论文通过优化语法结构和实现策略，大幅提升了基于LPeg的JSON解析器和Glob模式转换器的性能，无需修改底层库，实验结果显著优于现有主流工具，为文本解析领域贡献了实用的优化方法。


<details>
  <summary>Details</summary>
Motivation: 提升基于LPeg的解析器在实际任务（如JSON解析和Glob模式转换）中的性能表现，解决解析速度和内存使用优化难题。

Method: 通过两个案例研究：1) 对JSON解析器进行语法结构优化、替代捕获和表构造优化；2) 在Glob到LPeg模式转换工具中采用分段边界分离、Cox的扁平搜索策略和优化的大括号条件处理。所有优化均不需修改LPeg库本身。

Result: 优化后的JSON解析器处理速度高达125MB/s，性能超越dkjson，并在多数情况下比rxi_json表现优异。Glob转换器比Bun.Glob快14%-92%，比Minimatch快3-14倍。

Conclusion: 文中提出的优化技术能够显著提升基于LPeg的解析器的性能，并为文本处理提供了可行且有效的优化策略。

Abstract: This paper presents advanced optimization techniques for Lua Parsing
Expression Grammars (LPeg) through two complementary case studies: a
high-performance JSON parser and a sophisticated Glob-to-LPeg pattern
converter. We demonstrate how strategic grammar construction can dramatically
improve parsing performance without modifying the underlying LPeg library. For
the JSON parser, we implement substitution capture and table construction
optimization to reduce memory allocation overhead and improve object
processing. For the Glob converter, we introduce segment-boundary separation,
implement Cox's flattened search strategy, and develop optimized braced
condition handling to prevent exponential backtracking. Comprehensive
benchmarks demonstrate that our JSON parser achieves processing speeds up to
125 MB/s on complex documents, consistently outperforming dkjson and showing
competitive results against rxi_json across most test cases. Our Glob-to-LPeg
converter exhibits 14-92% better performance than Bun.Glob and runs 3-14 times
faster than Minimatch across diverse pattern matching scenarios. This research
provides practical optimization techniques for LPeg-based parsers, contributing
valuable strategies to the text processing ecosystem.

</details>


### [2] [Globality and Regions](https://arxiv.org/abs/2507.01664)
*Hector Gramaglia*

Main category: cs.PL

TL;DR: 本研究通过将抽象与区域抽象结合，给出了区域基础语言中全局变量的新刻画，连接了全局系统与线性系统，提升了函数式语言中命令式特性的表达与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决在区域基础语言中对全局变量的刻画问题，融合抽象与区域抽象以清晰表达命令式操作和内存安全的连接方式。

Method: 将抽象与区域抽象统一，对Tofte和Talping的region language建模，并分析在这种统一下全局变量的表现。

Result: 证明global语言中的全局变量概念可以通过对区域抽象与普通抽象的统一，从region语言自然导出。

Conclusion: 全局变量的概念和引入方式可以在区域基础语言中通过抽象的统一得到刻画，实现了对函数式语言中命令式操作、安全等的更明晰描述。

Abstract: We obtain a characterization of global variables by unifying abstraction with
region abstraction in a region-based language. More precisely, in a previous
work a language called global was presented, whose virtue is to provide a
conceptually clear way of introducing imperative operations in a functional
language. Memory safety is provided by the concept of linear protection, which
connects the global system to a linear one. In this paper we show that the
concept of global variable provided by the global language arises from the
Tofte and Talping's region language through the unification of abstraction and
region abstraction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice](https://arxiv.org/abs/2507.01065)
*Christiaan Verwijs,Evelien Acun-Roos,Daniel Russo*

Main category: cs.SE

TL;DR: 线上实践共同体心理安全感较线下更低，影响成员参与和风险承受。主要威胁为排他和敌对行为，建议通过明确规范和积极管理提升氛围。


<details>
  <summary>Details</summary>
Motivation: 随着混合、分布式和异步工作模式的流行，敏捷软件开发中对持续学习的需求日益增加，而实践共同体（CoP）被广泛作为支持社会性学习的一种方式。实际中，虚拟交流愈发普遍，但在这类环境下实现心理安全的机制并不明确。因此，作者旨在探究敏捷CoP中，尤其是在线交流环境下的心理安全状况及其影响因素。

Method: 作者采用混合方法研究。首先，通过对143名实践共同体成员的问卷调查收集数据，并结合定性主题分析来深入理解心理安全的威胁和相关互动模式。为增强结论可靠性，还进行了成员核查（member checking），邀请30位参与者对分析结果进行反馈验证。

Result: 研究发现，在线环境下的心理安全感显著低于面对面交流环境。心理安全较低会导致成员减少继续贡献的意愿，并倾向于规避人际风险。性别、社区资历和内容创作活跃度并未带来显著差异，但角色和年龄组存在差异，暗示有代际或职位相关效应。主题分析揭示了排他行为、消极互动和敌意作为主要心理安全威胁，并常被部落主义与社区特定动态强化。作者建议通过明确规范、结构化引导和积极管理等方式加以干预。

Conclusion: 本研究系统对比了不同交流模式下的心理安全水平，提出了提升在线/虚拟实践共同体包容性和影响力的实用建议，对于虚拟/混合工作环境的社区组织者具备参考价值。

Abstract: As hybrid, distributed, and asynchronous work models become more prevalent,
continuous learning in Agile Software Development (ASD) gains renewed
importance. Communities of Practice (CoPs) are increasingly adopted to support
social learning beyond formal education, often relying on virtual
communication. Psychological safety, a prerequisite for effective learning,
remains insufficiently understood in these settings. This mixed-methods study
investigates psychological safety within Agile CoPs through survey data from
143 participants. Results indicate that psychological safety is significantly
lower in online interactions compared to face-to-face settings. Moreover, low
psychological safety reduces participants' intent to continue contributing and
avoidance of interpersonal risk. No significant differences emerged based on
gender, community seniority, or content creation activity. However, differences
by role and age group suggest potential generational or role-related effects.
Thematic analysis revealed exclusionary behavior, negative interaction
patterns, and hostility as primary threats to psychological safety, often
reinforced by tribalism and specific community dynamics. Suggested
interventions include establishing explicit norms, structured facilitation, and
active moderation. The findings were validated through member checking with 30
participants. This study provides a comparative perspective on interaction
modalities and offers practical guidance for organizers seeking to cultivate
inclusive, high-impact CoPs and similarly structured virtual or hybrid work
environments.

</details>


### [4] [Bugs in the Shadows: Static Detection of Faulty Python Refactorings](https://arxiv.org/abs/2507.01103)
*Jonhnanthan Oliveira,Rohit Gheyi,Márcio Ribeiro,Alessandro Garcia*

Main category: cs.SE

TL;DR: 该论文提出静态分析方法检测 Python 重构中的类型错误，并在实际工具和项目中发现了若干重要缺陷，呼吁改进现有工具以保障软件可靠性。


<details>
  <summary>Details</summary>
Motivation: Python 作为一门流行的编程语言，由于其动态类型系统，给自动重构带来了显著挑战。重构过程中引入的类型错误会影响软件的可靠性和开发者生产力，因此了解这些错误的产生方式至关重要。

Method: 本文提出了一种静态分析技术，用于检测 Python 重构实现过程中引入的类型错误。作者将该技术应用于 Rope 重构工具，并在开源 Python 项目上进行了评估。

Result: 在 1,152 次重构尝试中，分析共发现了四类重构中的 29 个缺陷，其中一些还在 PyCharm 和 PyDev 等主流 IDE 中被发现。所有已报告的缺陷都提交给了对应开发团队，并有部分被采纳。

Conclusion: 当前 Python 重构工具在类型安全性方面仍存在不足，有必要进一步提升其稳健性，以保证自动化代码转换的正确性及软件维护的可靠性。

Abstract: Python is a widely adopted programming language, valued for its simplicity
and flexibility. However, its dynamic type system poses significant challenges
for automated refactoring - an essential practice in software evolution aimed
at improving internal code structure without changing external behavior.
Understanding how type errors are introduced during refactoring is crucial, as
such errors can compromise software reliability and reduce developer
productivity. In this work, we propose a static analysis technique to detect
type errors introduced by refactoring implementations for Python. We evaluated
our technique on Rope refactoring implementations, applying them to open-source
Python projects. Our analysis uncovered 29 bugs across four refactoring types
from a total of 1,152 refactoring attempts. Several of these issues were also
found in widely used IDEs such as PyCharm and PyDev. All reported bugs were
submitted to the respective developers, and some of them were acknowledged and
accepted. These results highlight the need to improve the robustness of current
Python refactoring tools to ensure the correctness of automated code
transformations and support reliable software maintenance.

</details>


### [5] [Context-Aware Code Wiring Recommendation with LLM-based Agent](https://arxiv.org/abs/2507.01315)
*Taiming Wang,Yanjie Jiang,Chunhao Dong,Yuxia Zhang,Hui Liu*

Main category: cs.SE

TL;DR: 该论文提出了WIRL，一个用大模型和检索技术辅助代码片段上下文变量替换的新方法，在真实数据上精度和召回远超现有方案，推动了IDE智能化辅助的发展。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，Copy-paste-modify（复制-粘贴-修改）是一种常见操作，但在将外部代码片段集成到本地代码库时，变量的上下文适配（code wiring）是一个关键且未被充分研究的问题。现有方法往往不能很好地利用上下文信息，导致在上下文依赖性强的场景下表现不佳。

Method: 提出WIRL，一个基于大语言模型（LLM）的代码变量适配智能体，把代码补全任务建模为检索增强生成（RAG）填空任务。WIRL结合了LLM、定制工具包和编排模块，能够识别未解决变量、检索上下文并进行合适替换。同时采用混合策略，对常见模式采取确定性规则，对于复杂情形用状态机引导探索。

Result: 在真实代码适配场景的高质量数据集上，WIRL的精确率达到91.7%，召回率达90.0%，分别比先进LLM提升22.6和13.7个百分点，比IntelliJ IDEA提升54.3和49.9个百分点。

Conclusion: WIRL有效提升了代码变量适配的自动化和准确性，尤其适用于复杂依赖和多变量未解决的场景，有望推动更智能、上下文感知的开发者辅助工具进入现代IDE。

Abstract: Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

</details>


### [6] [Combining Type Inference and Automated Unit Test Generation for Python](https://arxiv.org/abs/2507.01477)
*Lukas Krodinger,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 为解决动态类型语言（如Python）难以获取类型信息的问题，该文提出并实践了运行时类型追踪的方法，显著提升了自动化单元测试的代码覆盖率和类型推断质量。


<details>
  <summary>Details</summary>
Motivation: 自动化单元测试生成已在静态类型语言中应用成熟，但动态类型语言（如Python）因缺乏类型信息，阻碍了测试生成器的发展，特别是在选择参数和构造测试用例时。为了解决这一瓶颈，论文提出了新的思路。

Method: 提出了一种称为“类型追踪（type tracing）”的方法，通过在被测试代码频繁执行期间动态收集类型相关信息，逐步完善类型信息。并在Pynguin测试生成框架中实现，以观察参数在运行时的使用方式、记录返回值类型，并将这些信息应用于提升覆盖率。

Result: 引入类型追踪后，Pynguin框架实现了高达90%的分支覆盖率提升，提升了mutation得分，并获得了与先进类型推断工具相媲美的类型信息质量。

Conclusion: 通过类型追踪，解决了动态类型语言中自动化单元测试生成缺乏类型信息的关键难题，大大提升了测试生成的有效性和覆盖率。

Abstract: Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

</details>


### [7] [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](https://arxiv.org/abs/2507.01628)
*Zilong He,Pengfei Chen,Hongyu Zhang,Xiaoyun Li,Guangba Yu,Hongyang Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: DaiFu是一种为深度学习系统设计的轻量级原地崩溃恢复框架，通过代码改造实现了即时、动态环境修复，比现有方案快1372倍且几乎无性能损耗，适用于多种崩溃场景。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习系统广泛应用且越发流行，但其开发与执行涉及复杂的软件堆栈，导致崩溃难以避免且频繁发生，严重浪费计算资源并影响开发效率。现有如checkpoint-retry等恢复解决方案恢复速度慢、不适合细微编程和瞬时运行时错误后的敏捷恢复。因此亟需一种高效、轻量的崩溃恢复方法。

Method: 通过对目标深度学习系统进行轻量级代码转换，DaiFu能够原地拦截系统崩溃，并允许动态即时更新程序运行环境（如代码、配置和数据），实现敏捷地崩溃恢复。此外，作者构建了包含7种典型崩溃场景的基准测试集对系统进行全面评估。

Result: DaiFu将恢复时间降低到了现有技术的1/1372，且额外引入的系统开销非常小（小于0.4%），并能有效应对7类深度学习系统常见的崩溃场景。

Conclusion: DaiFu显著加快了深度学习系统的崩溃恢复速度，在实验中相较于现有技术提升了1372倍恢复速度，同时带来的性能开销极低（低于0.40%）。DaiFu适用于多种深度学习系统中的不同崩溃场景，展现了卓越的恢复能力。

Abstract: Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

</details>


### [8] [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](https://arxiv.org/abs/2507.01827)
*Haichuan Hu,Congqing He,Hao Zhang,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: 本文提出将蒙特卡洛树搜索与大语言模型结合用于自动程序修复，有效提升补丁搜索的全局性与效率。实验表明，APRMCTS 在修复数量、速度、成本上均超越传统方法，适合复杂缺陷场景。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLM）的自动程序修复（APR）方法，通常采用试错策略，在修复补丁的有效性和搜索效率上存在局限：一是易陷入局部最优导致补丁能力有限，二是冗余搜索造成效率低下。

Method: 提出了 APRMCTS 方法，将蒙特卡洛树搜索（MCTS）引入到 LLM 驱动的补丁搜索中，实现对所有探索补丁的全局评估，每轮选择最有前景的补丁进一步精修和生成，实现全局化、高效的补丁搜索。

Result: 在 Defects4J 的 835 个真实 bug 上实验，APRMCTS 与 GPT-3.5 集成后可修复 201 个 bug，超越所有 SOTA 方法。同时，APRMCTS 也能提升多个主流模型的修复能力，并在补丁生成数量远少于前人（仅16或32个补丁）下取得显著性能。时间和金钱成本较目前最佳方法分别降低至20%和50%以内。

Conclusion: APRMCTS 能有效避免陷入局部最优，在提升自动修复有效性和效率上具明显优势，特别在解决复杂程序缺陷时表现突出。

Abstract: Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Interpolation with Automated First-Order Reasoning](https://arxiv.org/abs/2507.01577)
*Christoph Wernhard*

Main category: cs.LO

TL;DR: 本文系统分析了通过自动化一阶逻辑证明实现Craig插值的方法，包括两阶段插值过程、具体证明技术及其扩展，还介绍了统一插值的二阶算法，推进了插值理论和实际应用的发展。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明和知识处理系统常依赖于插值（interpolation）原理，特别是Craig插值，对于提升一阶逻辑自动推理的能力具有重要意义。本文旨在系统探索通过自动推理技术有效实现插值方法。

Method: 文章主要讨论两阶段插值方法：首先基于给定证明计算本质上为命题化的ground插值子，随后提升为带量词的一阶公式。具体利用clausal tableaux和分辨率(resolution)两种证明方法获得ground插值。此外，探讨预处理、相等性编码等技术对插值过程的影响，并介绍通过二阶量词消除（second-order quantifier elimination）实现uniform interpolation，涉及DLS和SCAN等基本算法。

Result: 基于自动推理方法，可有效获得ground插值，并成功将其提升为一阶插值子。已验证clausal tableaux和resolution均适用。经特定限制的预处理技术和相等性编码不仅对插值推导有益，还带来了对Craig插值的增强变体。研究进一步发展了uniform interpolation（统一插值）的基本算法和实现示例。

Conclusion: 自动推理支持的一阶逻辑插值具有可操作性和实用性，相关技术（如clausal tableaux、resolution、相等性编码、预处理等）提供了灵活可靠的插值生成方式。二阶量词消除则拓展了插值方法的应用范畴。整体上推动了插值理论及其在自动知识处理系统中的实际应用。

Abstract: We consider interpolation from the viewpoint of fully automated theorem
proving in first-order logic as a general core technique for mechanized
knowledge processing. For Craig interpolation, our focus is on the two-stage
approach, where first an essentially propositional ground interpolant is
calculated that is then lifted to a quantified first-order formula. We discuss
two possibilities to obtain a ground interpolant from a proof, with clausal
tableaux, and with resolution. Established preprocessing techniques for
first-order proving can also be applied for Craig interpolation if they are
restricted in specific ways. Equality encodings from automated reasoning
justify strengthened variations of Craig interpolation. Also further
contributions to Craig interpolation emerged from automated reasoning. As an
approach to uniform interpolation we introduce second-order quantifier
elimination with examples and describe the basic algorithms DLS and SCAN.

</details>


### [10] [LeanLTL: A unifying framework for linear temporal logics in Lean](https://arxiv.org/abs/2507.01780)
*Eric Vin,Kyle A. Miller,Daniel J. Fremont*

Main category: cs.LO

TL;DR: LeanLTL是一个集成于Lean 4的时序逻辑框架，扩展了LTL的表达能力，并提升了自动化证明效率。


<details>
  <summary>Details</summary>
Motivation: 现有线性时序逻辑（LTL）的框架在表达和证明不同类型（如数值）的属性时存在局限性，且缺乏便于自动推理和整合现代证明助理（如Lean 4）的工具。

Method: 作者提出LeanLTL，这是一个可以集成于Lean 4证明助理，支持有限与无限时间轨迹推理的LTL统一框架。LeanLTL允许将传统LTL语法与任意Lean表达式结合，同时提供了自动化工具，支持用Lean现有战术高效推理和证明。

Result: LeanLTL框架能够嵌入标准LTL变体，并通过提供自动化工具提升了在Lean 4中对时序逻辑公式的推理效率。同时，通过示例展示了该库在各类系统推理中的实际应用价值。

Conclusion: LeanLTL实现了对线性时序逻辑的统一支持，不仅增强了LTL表达能力，也提升了与Lean 4整合下的证明流程效率，对系统属性形式化分析和应用具有重要意义。

Abstract: We propose LeanLTL, a unifying framework for linear temporal logics in Lean
4. LeanLTL supports reasoning about traces that represent either infinite or
finite linear time. The library allows traditional LTL syntax to be combined
with arbitrary Lean expressions, making it straightforward to define properties
involving numerical or other types. We prove that standard flavors of LTL can
be embedded in our framework. The library also provides automation for
reasoning about LeanLTL formulas in a way that facilitates using Lean's
existing tactics. Finally, we provide examples illustrating the utility of the
library in reasoning about systems that come from applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 论文提出MALIBU基准，系统评估多智能体LLM对社会偏见的加剧现象，发现现有偏见消减方法可能过度倾斜于边缘群体，呼吁更平衡公正的检测标准和评测手段。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在以人为本的互动中应用广泛，但设计不当会加剧大型语言模型（LLM）中的隐性偏见，影响公平性和多样化表达。因此，亟需评估和量化此类系统中隐性社会偏见的工具和方法。

Method: 提出了MALIBU，一个用于评估基于LLM的多智能体系统是否隐性强化社会偏见和刻板印象的基准。MALIBU通过情景任务让AI模型在指定背景下作答，并采用两阶段LLM多智能体裁决体系：第一阶段按特定人口身份标签评分，第二阶段比较不同身份标签作答并综合评分、择优。

Result: 研究量化了多智能体系统生成内容中的偏见，并发现偏见去除措施有时可能偏向边缘化群体而非真正中立。

Conclusion: 多智能体系统在消除偏见时容易失衡，存在对边缘化群体的额外偏向，需要更加细致的检测与公平策略，以及透明的评测基准。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [12] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 本文提出了一种通过计算事件重叠来评估自动生成新闻摘要内容的方法，在挪威语数据集上验证了其有效性，能够更好地衡量摘要中的关键信息传递。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成文本摘要的评估主要依赖于与人工摘要的重叠度或相似度分数，但这些方法并未真正关注新闻核心，即事件内容的传递。作者希望更好地衡量摘要对新闻事件信息的保留情况。

Method: 提出了一种基于事件重叠来评估自动生成新闻摘要的方法，通过比对生成摘要、人工摘要与原始新闻中的事件重叠度进行评价。实验使用带有丰富事件注释和专家编写摘要的挪威语数据集。

Result: 提出的方法能够为摘要所包含事件信息的衡量提供更深入的洞见，从事件视角更好地反映摘要质量。

Conclusion: 基于事件重叠的评估方法能够弥补传统摘要自动评估的不足，更加关注新闻摘要中关键信息（即事件）的传递与保留。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [13] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本研究通过数字人文方法分析瑞典百科全书《Nordisk familjebok》的两版内容，发现第二版在地理焦点上更关注全球，体现时代和大事件的影响。


<details>
  <summary>Details</summary>
Motivation: 分析《Nordisk familjebok》这部19-20世纪瑞典百科全书的词条在不同版本之间的内容变化，特别想探索词条收录、地理关注点等如何反映瑞典知识界的变迁及重大历史事件影响。

Method: 利用Project Runeberg的数字化版本，首先对原始文本进行重新分词并用语义句嵌入方法配对第一版和第二版之间的词条，然后用基于transformer的分类器提取地理词条，并将其关联到Wikidata，分析地理词条的变化趋势。

Result: 通过分析，发现第二版相较于第一版，百科全书的地理关注点从欧洲逐步向北美、非洲、亚洲、澳大利亚和北斯堪的纳维亚地区有所转移，这一变化与一战和新兴大国崛起等历史背景相印证。

Conclusion: 《Nordisk familjebok》的不同版本内容变化反映了瑞典及世界知识、地缘格局的转变。研究揭示百科全书作为知识载体，能够敏锐反映社会与历史变迁，相关代码与数据已开源。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [14] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 本文提出了一种融合xLSTM和MEGA机制的新方法，用于提升ABSA任务中的性能和效率。该方法结合双向mLSTM与新的门控融合机制MECGAF，能有效捕捉文本的短程及长程依赖关系，在多个公开数据集上优于当前主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA模型在性能和效率间难以平衡。深度学习模型全局建模能力弱、Transformer计算成本高、Mamba受CUDA依赖且局部建模能力弱。xLSTM近期在序列长依赖建模表现优异，但尚未在ABSA领域尝试。

Method: 方法为双向xLSTM结构，结合正向mLSTM与部分反向PF-mLSTM流，另外引入mLSTM基础的多头跨指数门控融合机制（MECGAF）以动态融合不同方向的上下文信息，优化短距离和全局依赖建模。

Result: 实验结果表明，所提出的MEGA框架在三组基准数据集上优于现有基线，无论是性能还是效率上都有突破。

Conclusion: 提出的xLSTM与MEGA框架在三组ABSA基准数据集上均超过现有先进方法，表现更佳，无论是在准确率还是效率上均有优势。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [15] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 提出一种去除文本嵌入中混杂因素（如来源、语言）的方法，极大提升了文本相似度计算和聚类效果，且模型整体性能未受负面影响。


<details>
  <summary>Details</summary>
Motivation: 基于嵌入的文本相似度度量常常受到文本来源、语言等无关属性的影响，这些“混杂因素”影响了文本聚合和比较的准确性，尤其是在需要整合不同语料来源文本的应用中。

Method: 提出一种去偏算法，在编码器生成的表示中去除已知混杂变量的信息，以减少无关属性对文本相似度的影响，同时保证计算开销较小。

Result: 在不同的嵌入变体和任务上，文本相似度和聚类指标均有显著提升，并且在每项评测中均观察到改进。在分布外测试集上的表现没有下降，说明去偏处理未削弱嵌入本身的效能。

Conclusion: 该去偏算法可有效减小文本嵌入中的混杂偏差，在保证效率的前提下提升了多项文本相似与聚类任务中的泛化表现。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [16] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 本文针对大语言模型在波兰这种非英语法域的法律检索与问答任务表现不足，提出了基于民法典与更可解释的检索机制的gAIus架构，在法律考试数据集上大幅提升主流模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在非英语及非中文法律领域给出的参考及答案有效性不足，需要更解释性强且用户友好的方案，尤其是在条文法体系下。

Method: 提出了以波兰民法典为基础的信息检索机制，并构建gAIus认知型大模型智能体架构。采用数据集评测并与现有嵌入式方法进行对比。

Result: 引入的机制使gpt-3.5-turbo-0125模型的表现提升了419%，优于gpt-4o，并将gpt-4o-mini得分从31%提升至86%。

Conclusion: 文中提出的gAIus架构极大提升了大语言模型在波兰法律领域的检索及答题能力，并展望了未来研究方向和应用前景。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [17] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4在眼科糖尿病视网膜病变和青光眼筛查中能通过文本提示模拟基础临床决策，但精度有限，尤其在复杂任务中表现不佳。加入患者元数据对其诊断性能无显著影响。目前不适合直接临床应用，但有望在教学或数据标注领域提供支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）如GPT-4在自然语言临床推理方面具有潜力，但在眼科领域的应用未被广泛探索。本研究旨在评估GPT-4通过结构化文本描述视网膜照片并模拟与糖尿病视网膜病变（DR）和青光眼筛查相关的临床决策的能力，尤其是考察添加真实或合成患者元数据的影响。

Method: 本研究为回顾性诊断验证研究，基于300张带注释的眼底照片。GPT-4分别根据仅含图像描述或结合患者元数据的结构化提示，完成ICDR严重程度评分、DR转诊建议和青光眼转诊所需的视盘杯盘比估算。用准确率、macro/weighted F1分数及Cohen's kappa进行性能评估，利用McNemar检验和变化率分析判断元数据影响。

Result: GPT-4对于ICDR分级表现为中等（准确率67.5%，macro F1 0.33，weighted F1 0.67，kappa 0.25），主要准确在正常病例。DR二元转诊任务表现更好（准确率82.3%，F1 0.54，kappa 0.44）。青光眼转诊所有设置表现较差（准确率约78%，F1<0.04，kappa<0.03）。元数据的加入对结果影响不显著（p>0.05），各条件下预测一致性较高。

Conclusion: GPT-4可通过结构化文本提示模拟基础的眼科决策，但对复杂任务精度有限。目前并不适用于临床，但可辅助教育、文档记录或图像标注等工作。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [18] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: 该论文提出一种新的RAG框架—CARE-RAG，可有效检测和处理证据间冲突，提高复杂检索环境下的生成可靠性，在各种含冲突或噪声场景下表现优于传统RAG组成。


<details>
  <summary>Details</summary>
Motivation: RAG在将大型语言模型与外部检索内容结合时可能引入知识冲突，内部不一致或检索内容噪声会影响生成结果的可靠性，因此需要提升RAG系统面对冲突和不一致时的表现。

Method: 提出CARE-RAG框架，包含参数感知证据的提取、检索证据的过滤与精炼、冲突驱动摘要、以及QA修复步骤，以实现多来源证据的可靠综合和答案修正。核心方法包括使用特定模型对证据进行冲突检测和摘要。

Result: 在修订过的QA数据集和包含检索数据的实验中，CARE-RAG在有噪声或冲突证据情况下，相较于已有的强RAG基线表现出持续更优的性能。

Conclusion: CARE-RAG通过冲突感知的证据处理和多层保证生成质量措施，显著提升了RAG系统面对信息冲突和不一致时的可靠性和可信度。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [19] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 本文提出了高质量web级数据存储CompactDS，提升RAG在推理任务上的准确率，同时效率和表现优于主流检索工具和复杂RAG系统，工具代码已开放。


<details>
  <summary>Details</summary>
Motivation: 现有RAG主要在简单任务（如事实型问答）中取得成果，而在复杂推理任务上的表现有限。作者认为问题在于缺乏一个能与预训练数据广度对齐的、可用的web级数据存储。

Method: 提出了CompactDS，一个高质量、多样化、web级的数据存储，能够在单节点实现高效检索。技术核心在于：大部分Web内容可过滤，保留紧凑高质量子集即可；并通过内存中的近似最近邻检索与磁盘上的精确检索结合以平衡速度和召回率。将CompactDS与一个minimal RAG流水线结合，并和主流检索系统和复杂RAG代理系统进行对比。

Result: 在MMLU, MMLU Pro, AGI Eval, GPQA和MATH等推理密集型基准测试中，无论模型规模如何（8B-70B），该方法都取得了持续的准确率提升，MMLU上提升10%，GPQA提升14%，MATH提升19%；单一数据源效果有限，数据来源多样性非常重要；自己的数据存储方案与Google等主流搜索引擎表现相当或更优，且优于复杂RAG系统。

Conclusion: 作者提出的CompactDS数据存储和RAG管道在推理密集型任务中显著优于现有方案，具有高效性、多样性和可复现性，并公开了全部工具以促进后续研究。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [20] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA通过对每层激活的旋转变换和Top-K筛选，实现了无需再训练的稳定激活稀疏化，大幅提升了大语言模型的推理速度和效率，且兼顾精度，优于现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 现有激活稀疏化方法在提升大语言模型推理效率时，要么需要慢且复杂的再训练，影响实际应用，要么依据激活值大小进行剪枝，导致稀疏性和推理加速效果不稳定。

Method: 提出LaRoSA方法，通过对每一层的激活进行正交旋转，再在旋转后的激活空间采用Top-K选择，实现稳定的模型级稀疏与推理加速，无需额外训练或传统的基于幅值的剪枝。

Result: LaRoSA对不同类型和规模的LLM均有效，几乎无性能损失，同时加速明显，如LLaMA2-7B在40%稀疏度下仅损失0.17困惑度，实现1.30倍加速，零样本任务准确率仅下降0.54%，优于TEAL和CATS。

Conclusion: LaRoSA是一种简单高效、便于落地的激活稀疏化方案，无需再训练且能保证持续的推理加速与模型精度，在实际LLM推理中具有很高应用价值。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [21] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文发现，针对物理复杂推理题，先进推理模型（如Deepseek-R1）不仅准确率极高，还擅长符号推导；即便对强大模型，少样本提示依然能提升效果。


<details>
  <summary>Details</summary>
Motivation: 物理推理对大型语言模型（LLM）而言一直是一个挑战，需要深刻的概念理解和熟练的问题解决能力。本研究旨在探索如何利用先进的指令微调推理模型提升物理问题的求解能力。

Method: 本文采用如Deepseek-R1等先进的指令微调推理模型，对SciBench基准中多样的高难度物理问题进行实验测试，系统评估模型的解题表现，并考察少样本提示（few-shot prompting）对模型推理能力的影响。

Result: 实验结果表明，推理模型不仅在复杂物理问题上取得了SOTA水平的准确率，还展示了以符号推导为核心的独特推理模式。同时，结合少样本提示，模型整体准确率进一步提升。

Conclusion: 先进的指令微调推理模型在高难度物理问题推理方面展现出极高的能力，符号推导推理值得关注。同时，少样本提示仍能为此类高性能模型带来可观增益，表明模型潜力尚未穷尽。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [22] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 首次提出并训练了纯逆向语言模型LEDOM，并通过逆向奖励方法提升算数推理等任务表现，展现了逆向建模的独特价值和潜力。


<details>
  <summary>Details</summary>
Motivation: 当前主流的语言模型（如GPT系列）都是基于传统的正向自回归方式进行建模，即预测下一个词。然而，逆向（reverse）语言建模尚未被充分探索，研究者希望探索逆向建模的方法，看是否能带来新的能力和应用。

Method: 提出并训练了全新的逆向语言模型（LEDOM），即从序列结尾向起始方向预测前一个词。使用了2B和7B参数的两个模型版本，在4350亿词的数据上自回归训练。同时，提出了利用LEDOM进行逆向奖励（Reverse Reward）的新应用方法：用LEDOM对传统正向语言模型生成的结果进行重排序，从而提升推理任务表现。

Result: 实验表明，LEDOM在数学推理等任务中，通过后验评估提升了文本生成的质量，表现出独特特征和广泛的应用潜力。

Conclusion: 逆向语言模型（LEDOM）不仅本身具备独特价值，还可以协同常规正向语言模型应用，“逆向奖励”策略可显著提升推理类任务的准确性。所有模型及数据将开源，促进相关后续研究。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [23] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 本文提出了SynPref-40M大规模偏好数据集及人机协同筛选流程，并基于该数据集训练了Skywork-Reward-V2奖励模型系列，在多个基准上达到SOTA，强调高质量数据与人机协同对于奖励模型性能的决定性影响。


<details>
  <summary>Details</summary>
Motivation: 现有的开源奖励模型（RM）在主流评价基准上表现不佳，难以捕捉复杂的人类偏好，且即便采用先进的训练方法，实际提升有限。作者认为，问题主要源自偏好数据集受限，如范围窄、合成标注多、质量控制薄弱。

Method: 提出了大规模偏好数据集SynPref-40M（包含4000万个偏好对），设计了一套人机协同的两阶段数据筛选流程：人工提供高质量标注，大模型依据人类指导自动过滤。基于精选的2600万个偏好对，训练了8个规模从0.6B到8B参数的Skywork-Reward-V2奖励模型。

Result: Skywork-Reward-V2在多种任务中表现全面，不仅对齐人类偏好，同时具有高客观正确性、安全性、风格抗干扰和可扩展性，并在7个主流奖励模型基准上取得SOTA表现。消融实验显示，提升来源于大规模且高质量的数据筛选。

Conclusion: Skywork-Reward-V2大幅推进了开源奖励模型的发展，证明了高质量大规模偏好数据及人机协同筛选在提升模型表现中的关键作用，也揭示了现有偏好数据集的巨大潜力。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [24] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于 Transformer 的医学文本信息抽取与多标签疾病预测模型。通过自注意力机制和语义对齐提升表示能力，实验表明其优于主流方法且具有良好泛化性，对实际医疗文本建模具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录文本具有非结构化和高维语义复杂性，现有方法难以同时高效处理信息抽取与多标签疾病预测。亟需统一的深度学习框架以提升模型在实际医疗场景下的应用能力和泛化性。

Method: 提出了一种基于多层自注意力机制的 Transformer 架构，进行临床文本的表示学习。采用了 context-aware 语义对齐机制提升模型表征能力，并通过 Sigmoid 多标签分类器实现多标签疾病预测。设计了包括基线对比、超参数分析、数据扰动和噪声注入等实验全面评估模型性能。

Result: 实验结果显示，所提方法在多项性能指标上超越代表性现有方法，并能在不同数据规模、干扰和深度条件下稳定泛化。该模型处理真实临床文本表现出显著优势。

Conclusion: 所提出的基于 Transformer 和注意力机制的深度学习方法在多标签疾病预测和信息抽取任务上，整体性能优于现有主流方法，并在不同数据规模和干扰条件下依然具有良好的泛化能力。该框架为实际医疗文本处理提供了高效且实用的算法基础。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [25] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大模型logit的检索式推理加速方法LogitSpec，无需辅助起草模型，可以在现有LLM系统中直接使用，实验证明了其明显的推理提速和高效draft tokens生成能力。


<details>
  <summary>Details</summary>
Motivation: 推断加速对于大语言模型（LLM）的实际应用至关重要。现有的Speculative Decoding(SD)方法通常依赖一个额外的小型起草模型（draft model），以降低推理延时，但起草模型本身的部署和计算负担依然存在。因此，近来的尝试集中在去除draft model，转向检索式起草来减少开销和应用难度。然而，传统的检索式起草因依赖匹配机制，难以找到高度相关且精准的draft tokens。

Method: 作者提出了一种新的检索式推理加速方法，称为LogitSpec。LogitSpec 利用大模型的最后一个token的logit，不仅预测下一个token，还推测下下个token。具体方法分为两步：（1）通过最后token的logit推测下下个token；（2）同时为下一个和下下个token检索相关的参考，从而拓展检索范围，提高找到合适draft tokens的成功率。整个方法无需额外训练，且可直接集成到现有LLM推理系统。

Result: 在多个文本生成基准测试上的大量实验证明，LogitSpec最多可实现2.61倍的推理速度提升，平均每步可接受签发token数达到3.28。

Conclusion: LogitSpec不依赖起草模型，通过扩展检索范围并利用大模型logit推理下下个token，实现了高效、灵活且易于集成的推理加速，是LLM推理加速的新进展。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [26] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 将直接偏好优化（DPO）方法用于LLM文本简化系统，并收集智力障碍者偏好反馈，可实现更个性化和包容性的文本简化AI系统。


<details>
  <summary>Details</summary>
Motivation: 自动文本简化（ATS）旨在帮助特定群体（如智力障碍者）更好地获取信息，但传统LLM简化系统缺乏个性化，无法充分满足目标群体的具体需求。

Method: 在标准LLM监督微调基础上，采用直接偏好优化（DPO）方法。通过收集智力障碍者对一对简化文本的偏好反馈，对模型进行再训练。同时，提出了个性化LLM-ATS系统开发管线，包括数据收集、模型选择、SFT和DPO微调及评估。

Result: 通过引入目标群体成员的反馈和偏好，训练得到更贴合其需求的个性化ATS模型。实验结果强调，让目标用户参与设计AI辅助工具，有助于实现更具包容性和人性化的AI系统。

Conclusion: 本研究推动了面向特定群体的AI系统个性化，显示了受益于目标用户参与的人类预期对齐机制，为更具包容性的AI辅助工具奠定了基础。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [27] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 作者提出了一种结合不确定性建模与微调LLM的新方法，实现了高效且准确的OOS意图检测，并在真实及标准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的任务型对话系统（TODS）在面对用户未见过或模糊查询时，识别超出范围（OOS）意图的能力较弱。提升系统对这些未知或模糊输入的鲁棒性成为关键挑战。

Method: 提出了一个新颖且简洁的模块化框架，将不确定性建模与微调的大型语言模型（LLM）结合。第一步，基于当前部署在真实对话系统中的意图分类器的输出进行不确定性估计。第二步，对高不确定性实例，调用已经微调的LLM做最终判决。

Result: 该方法能够高效且准确地识别OOS意图，在多个OOS检测基准上取得了最新的最佳结果，包括实际对话系统中收集的真实OOS数据。

Conclusion: 本文提出的方法在保证计算效率的同时，显著提升了OOS检测性能，有效结合了传统方法和LLM，为实际对话系统的鲁棒性提升提供了新路径。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [28] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 引入维基百科等外部信息反而会使多种LLM的立场检测能力显著下降，因为模型更容易被外部材料的立场和情感所误导；微调虽能部分缓解，但根本问题依然存在，结果与BERT相关研究显著相反。


<details>
  <summary>Details</summary>
Motivation: 前人研究表明引入外部信息（如维基百科摘录）能够提升立场检测任务的表现，但这些结果是否同样适用于大型语言模型（LLMs）尚未被深入研究。该论文意在系统评估外部信息对LLMs立场检测能力的影响。

Method: 作者在三个数据集上、针对12个目标、8种LLM，比较了有无维基百科或网页搜索外部信息时的立场检测表现。此外，还分析了连锁思考（Chain-of-Thought）提示和微调对结果的影响。

Result: 结果显示，在大多数情况下加入外部信息反而使表现下降（宏平均F1最多降低27.9%），而这种下降趋势即使在用chain-of-thought提示时依旧存在。微调可以缓解但不能完全解决该问题。

Conclusion: 外部信息会导致LLM过度对齐附加信息的立场和情感，而不是原文本的真实立场。这种“信息偏差”在LLM上比以往BERT类模型更为明显，提示在应用外部信息时必须谨慎。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [29] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 提出LUSTER框架，结合大语言模型和端到端强化学习，兼顾任务成功与情感响应，在复杂对话环境下实现了更高效、更具情感智能的任务型对话系统。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）提高了语言流畅性和上下文理解能力，但构建既高效又具备情感智能的任务型对话（ToD）系统仍然极具挑战。理想的ToD系统需同时优化任务完成度、情感识别与响应，以及信息的精准传递，并且应对对话环境中的噪音和歧义。

Method: 本文系统性探讨了ToD系统的架构、表示、优化及情感设计。作者设置了涵盖这些要素的对话系统，并在结合自然语言用户模拟器及不完美的自然语言理解模块的环境下进行评估。提出了LUSTER方法，即基于LLM的端到端强化学习系统，能够同时考虑短期（用户情感）和长期（任务成功）奖励。

Result: 实验结果显示，将大语言模型的能力与结构化奖励建模相结合，可以构建更具鲁棒性与情感响应能力的ToD系统。

Conclusion: LUSTER（结合LLM和结构化奖励的ToD系统）为打造兼具情感智能与任务效率的下一代对话系统提供了有效可行的路径。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [30] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 作者基于真实notebooks提出了更生态有效的CQA数据集，对主流模型的检验显示尚有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有图表问答(CQA)数据集存在缺乏真实性和生态有效性的问题。实际分析过程中，用户面对的是多视图、多样化的真实图表，而此前数据集多来源于人工合成，缺乏与真实推理流程的对应。

Method: 作者基于可视化notebooks，收集了真实的多视图图表和与之配套的、基于分析叙事的自然语言问题，构建了一个新的CQA数据集。同时，利用最新的大型多模态语言模型对数据集进行了基准测试。

Result: 在新数据集上，GPT-4.1仅取得了69.3%的准确率，表现出明显的性能瓶颈，说明该数据集比以往基准更具挑战性，更贴近真实任务。

Conclusion: 本文提出了一个更贴合实际分析场景的CQA数据集，能有效测试和推动现有方法在真实数据、复杂推理任务下的能力发展。当前的多模态大模型在该数据集上仍存在明显不足。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [31] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 本文对比了自然语言处理模型评测中的全局分数与成对比较方法，指出全局分数适合总体可靠排名，成对比较擅长发现潜力模型，但需更多数据。建议根据场景选用合适的评测策略，并开源了实验代码与数据。


<details>
  <summary>Details</summary>
Motivation: 随着指令微调神经语言模型能力的增强，自然语言处理领域的基准评测趋向于从传统的全局分数（如GLUE、BIG-bench）转向成对比较排行榜（如LMSYS Arena）。该研究动机在于分析并比较全局分数和成对比较两种评测方法的优劣，以便为模型评估策略的选择提供决策依据。

Method: 作者利用计算实验，在合成与真实数据集上，通过标准全局指标与广受欢迎的Bradley-Terry成对比较模型，系统地评估和对比两种模型评测方法的表现。

Result: 实验结果显示：全局分数在总体排名上更可靠，但对于偶有重大错误或低置信度的强模型可能低估其能力。成对比较方法在发掘全局评分较低但有潜力的强模型方面更有效，特别适用于质量度量难以明确定义的任务（如文本生成），但在出现较多平局时收敛更慢，需要更多的比较。

Conclusion: 全局分数适用于需要稳定、总体评价的场景；成对比较则更适合发掘潜力选手及难以量化的任务。实际应用中应根据需求权衡选用。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [32] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 该文系统评估了多种预训练模型在十种印尼本地语言上的迁移能力，发现多语模型表现受语言曝光度影响显著。MAD-X方法在提升少资源语言情感分析迁移能力上效果突出，无须目标语标注数据。


<details>
  <summary>Details</summary>
Motivation: 近年来，预训练语言模型在多语言任务中表现突出，但对资源稀缺的印尼本地语言的迁移能力尚未系统研究。因此，本研究旨在探讨这些模型在印尼本地语情感分析任务中的可迁移性，特别是在无监督和结构化迁移方案（如adapter）的实用性。

Method: 作者评估了四种不同类型的预训练模型在十种印尼本地语言上的情感分析转移性能，包括单语印尼BERT、多语mBERT、XLM-R及基于adapter的MAD-X方法。使用零样本和adapter迁移方式，将目标语言分为‘已见’、‘部分已见’和‘未见’三个组别，并分析了词片分割和词汇重叠对结果的影响。

Result: 多语模型在‘已见’语言上表现最佳，在‘部分已见’语言上表现中等，在‘未见’语言上表现较差。MAD-X方法能显著提升‘已见’和‘部分已见’语言的迁移效果，且无需目标语标注数据。词片分割和词汇重叠与模型表现仅弱相关，模型预训练阶段对目标语或相关语的曝光度才是迁移效果最重要的预测因素。

Conclusion: 预训练模型在低资源本地语言的情感分析迁移中表现存在明显分层，与模型在训练阶段对目标语或相关语的曝光度密切相关。MAD-X结构在无需标注数据条件下，显著提升了多数本地语言的迁移性能。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [33] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 作者提出了AdamMeme这一基于多代理、动态适应的评测框架，能更真实反映mLLMs在识别有害表情包时的能力与短板，弥补了现有静态评测的不足。


<details>
  <summary>Details</summary>
Motivation: 社交媒体时代多模态表情包大量涌现，现有多模态大语言模型（mLLMs）对表情包有害性理解的评测仍停留在基于准确率的静态数据集上，难以与表情包动态演化的实际情况相匹配。

Method: 提出了一种灵活的基于代理(agent-based)的评测框架——AdamMeme，采用多代理协作，动态地通过不断收集和融入具挑战性的表情包新样本，迭代评测mLLMs对表情包有害性推理的能力。

Result: AdamMeme框架能够系统性揭示不同mLLMs在识别表情包有害性方面的性能差异，深入细致地分析模型的具体薄弱环节。

Conclusion: AdamMeme能有效适应网络表情包的动态变化，为mLLMs在有害表情包理解评测中提供更全面和细粒度的分析。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [34] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本研究提出StereoBias数据集，发现同时训练偏见和刻板印象检测能显著提升偏见检测能力，有助于构建更公平的AI。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象会在内容审核和决策等敏感领域造成伤害，因此需要更好地检测和缓解这些问题。

Method: 提出了StereoBias数据集，涵盖宗教、性别、社会经济状况、种族、职业等五类，分别标注了偏见与刻板印象。比较了只用编码器的模型和用QLoRA微调的解码器模型，并设计联合训练同时检测偏见与刻板印象。此外，通过情感分析实验，验证了模型改进来源。

Result: 编码器模型表现较好，解码器模型也有竞争力。最重要的是，联合训练偏见与刻板印象检测任务，能显著提升偏见检测的效果，且这种提升不是因为多任务训练本身，而是因为偏见与刻板印象之间的联系。

Conclusion: 结合刻板印象检测，有助于提高对偏见的识别，有望促使AI系统更加公平有效。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [35] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 本研究评估了LLM在德国劳动合同条款合法性分类中的表现，发现精炼法律指导可明显提高无效条款识别能力，但LLM总体仍不及专业律师。研究贡献了扩展数据集和工具，并指出LLM辅助法律审查的机遇与不足。


<details>
  <summary>Details</summary>
Motivation: 法律工作由于文本密集、资源消耗大，面临独特的NLP挑战。目前数据驱动方法虽然推动了领域进步，但可解释性和可信度不足，限制了其在动态法律环境中的应用。为了解决这个问题，作者与法律专家合作，希望提升大语言模型（LLM）在德国劳动合同合法性评估的能力。

Method: 本研究扩展了现有数据集，探索使用LLM和in-context learning来评估德国劳动合同条款的合法性。实验设置包括三种法律环境：无法律背景、法律原文及判例全文、以及提炼的“审查指南”。对LLM进行不同环境下的分类能力对比，类别包括“合法”、“不公平”、“无效”。

Result: 实验结果显示，使用法律原文全文对性能有一定提升，而使用精炼的“审查指南”则显著提高了对无效条款的召回率和加权F1分数，最高可达80%。但即使用原文全文，LLM在条款合法性分类上的表现仍明显低于人类律师。

Conclusion: 本研究扩展并公开了包含“审查指南”和法律参考源的劳动合同条款数据集，同时发布了全部代码和日志。结果表明LLM有助于辅助律师进行合同合法性审查，但目前该方法仍有局限性。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [36] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 本文探讨了分词在语料库语言学中的关键作用，尤其关注表情符号、同形异义字符对分词准确性的影响，并提出了相应预处理方法，以提升分析的可靠性，对定量与定性研究均具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 分词差异会影响语言数据的呈现和分析结果的有效性，尤其是在面对新的数字文本特征（如表情符号、同形异义字符）时。

Method: 调查表情符号和同形异义字符带来的分词挑战，并提出预处理这些元素的方法，保障语料与原始数据的一致性。

Result: 研究提出了确保数字文本在语料库中准确再现的方法，强调在语料库研究的定量与定性分析中均要重视预处理和技术细节，以保证分析结果的可靠性和可重复性。

Conclusion: 该研究强调在语料库研究中，详细理解数字文本数据中语言和技术两个方面的重要性，以提升语料分析的准确性。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [37] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: 提出MuRating框架，将英文高质量数据选择经验迁移到17种语言，实现更优多语言数据筛选，提升了大模型预训练表现，尤其在知识密集型任务上进步明显。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于模型的数据选择方法主要面向英文，缺乏对多语言数据质量的有效评估和处理方法，因此需要一种能够利用高质量英文信号指导多语言数据质量评估的通用方法。

Method: 提出MuRating框架，通过整合多位英文“评分员”的成对比较结果，得到统一的文档质量分数，然后通过翻译将这些评价信号传递到17种目标语言，用于训练多语言评估模型，支持单语、跨语种和平行文本的评分。

Result: MuRating能够在英文和多语言语料上选出高质量的子集，用于预训练1.2B参数规模的LLaMA模型。与QuRater、AskLLM、DCLM等主流基线方法相比，无论在英文还是多语言评测上均有性能提升，尤其是在知识密集型任务上表现出显著优势。同时还对翻译保真、选择偏差及叙事材料的代表性不足进行了分析。

Conclusion: MuRating突破了基于模型的数据选择在多语言环境中的应用局限，通过迁移高质量英文信号，有效提升了多语言数据质量选择与评估能力，对多语言大模型的预训练效果有显著帮助，并为后续改进提出了分析与建议。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [38] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: Llama-3.3-70B-Instruct具备区分测试和部署的能力，这会影响AI安全和治理的评估可靠性。论文通过线性探测分析了模型的“评测感知”能力，提醒业界注意评测可信度，并考虑将模型内部信息用于更可靠的安全审计。


<details>
  <summary>Details</summary>
Motivation: 模型如果知道自己是在被测试或者部署阶段，这会影响我们对模型安全的评估、政策和管控的可信度。因此，有必要研究语言模型对评测与部署阶段的区分能力。

Method: 对Llama-3.3-70B-Instruct模型进行实验，利用线性探测器（linear probes）分析模型是否能区分测试和部署的提示词，并判断模型对当前安全评测的识别能力。

Result: 线性探测器能够区分真实世界的评测和部署提示，表明模型原本就在内部表示上区分这两者。此外，模型对现有安全评测也能被正确分类，说明这些评测在模型看来是“做作”的。

Conclusion: 必须确保评测的可信度，提高对模型欺骗性能力的认知，并建议利用模型内部信号辅助安全审计，尤其面向更擅长“评测感知”的未来模型。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [39] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文分析了视觉-语言多模态模型在面对矛盾信息时的处理偏向，揭示了模型内部结构决定了这种偏向性，并提出通过“路由头”提升模型可控性的机制，为多模态冲突信号的检测与调控提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型多模态集成需求增长，了解模型如何在输入存在矛盾时进行信息处理与表征重构，对于提升模型的健壮性及可控性至关重要。

Method: 本文针对视觉-语言模型输入互相矛盾的信息（如狗的图片配上“这是一只猫”的文字描述），要求模型根据特定指令报告某一模态的信息，并观察其行为及内部结构特征。主要通过行为观测、内部表征分析、注意力头与路由头识别及操作等实验手段研究。

Result: 不同的多模态模型在面对矛盾输入时处理方式不同，具有偏向性。通过对注意力头和“路由头”的分析和操作，可以提升模型按指令关注目标模态、改进其在不同数据集及模态下的表现。

Conclusion: 论文揭示了多模态模型在处理相互矛盾的信息时，通常会偏向某一输入模态（如图像或文本），且偏向性在模型内部的表示结构中有所体现。通过操作特定的注意力头，还能引导模型关注特定模态，甚至可以通过“路由头”提升模型根据指令选择正确模态作答的能力。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [40] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本研究通过分析MDACE数据集，深入评估了自动医学编码系统的可解释性，指出现有方法与真实证据有较高重合，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 自动医学编码对提高文档和账单处理效率有显著作用，但其透明性和可解释性对医学编码者和监管机构尤为重要。此前由于数据标注稀缺，相关方法评估仅限于短文本与二分类问题，MDACE数据集的出现为深入研究提供了可能。

Method: 深入分析了MDACE数据集，从应用角度对现有可解释性自动医学编码系统进行了合理性评估；提出了匹配度量标准，并对成功和失败案例进行了展示。

Result: 结果显示，真实证据与编码描述具有一定程度的一致性，最新方法生成证据与真实证据高度重叠。研究提出了评价指标，并对现有系统的优缺点进行了总结。

Conclusion: 研究发现，当前最先进的自动医学编码系统与真实证据有较大重合，但仍有进一步改进空间。研究给出了开发和评估可解释性医学编码系统的建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [41] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 本文系统比较了JSON、YAML和XML三种结构化格式在小型语言模型进行临床笔记属性-值抽取任务中的可解析性，发现JSON最优，并提出相关应用建议。


<details>
  <summary>Details</summary>
Motivation: 在使用小型语言模型从临床笔记中抽取属性-值进行结构化输出时，输出结果的可解析性问题影响模型实际应用。不同序列化格式在解析时效果不同，缺乏系统比较和实证分析。

Method: 对比三种常用结构化输出格式（JSON、YAML、XML）在提取临床笔记属性-值时的解析性，通过实际实验检验不同格式在不同情境下的解析表现和常见错误。

Result: JSON格式在不同测试中显示出最好的可解析性；通过针对性提示和使用更大模型，结构鲁棒性有所提升，但在处理长篇文档和某些类型笔记时表现下降。分析了各格式独有的错误模式。

Conclusion: 论文为在临床敏感场景下如何选择序列化格式和设计提示词，提升语言模型输出结构化内容的可解析性提供了具体建议。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [42] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 提出分析LLM输出与训练数据关系的新方法，发现大部分高概率输出来源未知，为理解模型行为提供量化方式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）应用广泛，需理解训练数据如何影响模型输出，以提升透明度、问责、隐私和公平性。

Method: 提出一种系统性方法，聚焦于分析模型生成的低困惑度序列（高概率文本片段），通过抽取此类序列并追溯其训练数据来源，探究训练数据与输出的关系。

Result: 发现大量高概率生成片段无法在训练语料中找到对应；对可匹配的片段，量化其在源文档中的分布，揭示逐字召回的范围与性质。

Conclusion: 本文为理解训练数据对大语言模型行为的影响提供了方法与量化依据，有助于提升模型的可解释性与审查能力。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [43] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL是一套面向多语言LLM（特别是印度本地语言）的综合评测框架，支持多项任务类别和先进硬件特性，填补了多语种评测工具空白，已开放源码，助推多语言AI发展。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估过于偏向英语，缺乏对印度等语言多样性地区的评测框架，亟需统一、易用、覆盖广泛的评估工具。

Method: 集成了35个以上基准（含10个印度本地数据集），涵盖推理、数学、工具使用、长上下文理解、阅读理解等多个类别。框架支持分布式推理、量化与多GPU，且对比现有方法有更广泛的覆盖和生产可用能力。

Result: EKA-EVAL提供了比现有工具更全面的评测能力，为多语言（尤其是印度语言）LLM的测评和对比提供了便利，且已开源发布。

Conclusion: EKA-EVAL是首个面向全球及印度本地LLM的端到端可扩展评测框架，极大降低了多语言基准测试的门槛，并计划未来扩展更多数据集，致力于建立健全多语种LLM评测生态。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [44] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: 该论文针对现有语言学习工具对多语种学习支持不佳的问题，提出了DIY-MKG系统，允许用户与LLM共同构建多语种词汇知识图谱并开展个性化学习。系统评估显示其多语种扩展与测验极为准确，提升用户体验和学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的语言学习工具，甚至是大型语言模型（LLMs）驱动的工具，常常无法很好地支持多语言学习者在多种语言词汇之间建立联系，定制性不足，且存在认知负荷过重的问题。该论文旨在解决这些痛点。

Method: 提出了DIY-MKG（自助式多语言知识图谱）开源系统。用户可通过该系统选择性地扩展个性化词汇知识图谱，LLM推荐相关词汇，并提供丰富注释功能及自适应复习模块（基于LLM动态生成个性化测验）。用户还能标记错误题目，为模型优化提供反馈循环。

Result: 系统中LLM相关组件的评估表明，多语言词汇扩展具备可靠性和公平性，测验生成准确度高，验证了DIY-MKG的鲁棒性。

Conclusion: DIY-MKG通过结合LLM和用户参与，实现了更加个性化、多语言支持和反馈驱动的语言学习体验，克服了现有工具的主要局限。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [45] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出MiCoTA方法，通过引入中等规模教师助理和中等长度推理链条，有效缩小小模型学习长链推理的鸿沟，实验显示该方法显著提升了SLMs的推理表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在需要长推理链条的任务上表现优异，但其庞大的规模与高算力需求限制了实际部署。而小语言模型（SLMs）因模型容量有限，难以学习长链式推理（CoT），存在“SLMs学习能力鸿沟”问题。

Method: 提出了一种新框架 MiCoTA（Mid-CoT Teacher Assistant Distillation），通过引入中等规模的教师助理模型和中等长度的推理链条，在蒸馏过程中逐步缩小容量和推理长度差距，提升SLMs对长链式推理的学习能力。

Result: 在AIME2024、AMC、Olympiad、MATH-500和GSM8K等基准测试中，蒸馏自大型教师的SLMs在未使用MiCoTA时表现不佳，使用MiCoTA后，Qwen2.5-7B-Instruct与Qwen2.5-3B-Instruct在平均分上分别提升3.47和3.93。分析结果表明，MiCoTA生成的数据更易被基础SLMs学习。

Conclusion: MiCoTA框架能有效提升小规模语言模型在长链式推理任务上的表现，为SLMs领域中长CoT数据蒸馏研究提供了新的方向。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [46] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种面向大语言模型的高层注意力头剪枝方法，结合自适应重缩机制，在多个主流模型和数据集上显著提升了剪枝后的性能，尤其适合生成任务。


<details>
  <summary>Details</summary>
Motivation: 传统的无训练结构化剪枝方法在修剪LLM时，通常使用启发式指标，未考虑注意力头在网络结构中的具体位置，从而影响剪枝效果。

Method: 提出了一种新颖的剪枝算法，有针对性地在模型高层剪除注意力头，并引入自适应重缩参数，用于在剪枝后校准token表示的幅值，抵消剪枝带来的影响。

Result: 在LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B、Gemma2-9B等多种LLM上，方法在生成和判别任务的27个数据集上进行了实验，结果显示该方法显著优于现有结构化剪枝方法，尤其在生成任务中表现突出。

Conclusion: 针对传统结构化剪枝忽视层次结构的不足，提出了高层注意力头剪枝与规模自适应重缩的联合方法，经大规模实验验证，方法在多任务和多模型上均有优越表现。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [47] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 该论文首次全面综述了AI辅助科学研究（AI4Research）的发展，梳理了主要任务、研究前沿和应用资源，为该领域后续创新指明方向。


<details>
  <summary>Details</summary>
Motivation: 最近AI、尤其是大语言模型（LLMs）在逻辑推理和实验编程等复杂领域表现卓越，推动了众多关于AI在科研创新过程中的应用探索。然而目前尚无对AI4Research领域的系统综述，这阻碍了该领域的整体认知和进一步发展。该论文旨在弥补该空白。

Method: 论文通过文献梳理，分类了AI4Research的五大主流任务，制定了系统性分类法；分析当前研究空白和未来研究方向，强调自动化实验的严谨性、可扩展性及社会影响；并整合和汇总了应用、数据和工具等丰富资源。

Result: 本文首次系统梳理了AI4Research领域，总结了相关任务、未来前沿以及众多跨学科应用资源，为进一步研究提供了基础和方向。

Conclusion: 本综述为AI4Research领域提供了统一的系统视角和丰富资源，有助于快速了解现状、发现挑战并促进创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [48] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 作者提出了GAPO方法，将LLM对齐建模为多目标优化问题，并用自适应梯度方案有效应对偏好冲突；实验和理论均证明GAPO优于主流方法，能更好满足用户多样化需求。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型(LLMs)需要对齐多样化的人类偏好，但当这些偏好相互冲突时，对齐变得极具挑战。现有方法难以同时满足所有多样性需求。

Method: 作者将人类价值对齐问题形式化为多目标优化问题，提出了一种新颖的fine-tuning范式——梯度自适应策略优化(GAPO)。该方法通过多梯度下降自适应调整每个目标的梯度，实现多目标间的平衡。此外，还提出了P-GAPO方法，进一步结合用户对不同目标的特定偏好，获得更贴合用户需求的Pareto最优解。

Result: 理论分析表明，GAPO可收敛到多目标下的Pareto最优。实验结果显示，在Mistral-7B模型上，GAPO在有益性(helpfulness)和无害性(harmlessness)两个指标上均超过当前先进方法。

Conclusion: GAPO及其变体P-GAPO能有效实现对冲突性人类偏好目标的平衡对齐，并在大模型微调中优于现有多目标对齐方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [49] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 作者提出系统性筛选高质量、难度更高的教师推理轨迹构建数据集，并验证这种方式比单纯扩增样本量或用现有数据集提升学生推理能力更有效。


<details>
  <summary>Details</summary>
Motivation: 之前的研究已经证明，通过有监督微调从大型教师模型中提炼推理过程，比学生模型单独用强化学习表现得更好，但目前尚缺乏系统性研究来判断：什么样的教师推理演示最能提升学生模型的推理能力。

Method: 作者从强大的教师模型出发，基于大规模问题池（来自NaturalReasoning）筛选出高质量推理轨迹，构建了“NaturalThoughts”数据集。对这些推理样本在提升学生模型推理能力方面进行了系统分析，考察数据量扩展及难例选择等因素对效率和泛化能力的影响。

Result: 发现随机扩增数据大小是一个强基线且可持续提升效果。进一步选取难度较大、需多样化推理策略的问题对推理知识迁移更高效。基于NaturalThoughts训练的模型，在STEM推理基准测试上显著优于现有的数据集（如OpenThoughts、LIMO等），适用于Llama、Qwen等模型。

Conclusion: 系统筛选高质量、覆盖多样化推理方式的教师推理样本，并优先难例，有助于更高效地提升学生模型的推理能力，优于传统随机采样或现有公开数据集。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [50] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 本文提出用实际决策结果（如金融交易收益）直接评估自然语言生成内容，发现丰富的分析性文本能提升人机协作决策效能，挑战了传统评价方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言生成（NLG）被应用于高风险领域，但主流的内在评价方法（如n-gram重叠与句子可然性）与实际决策效能相关性较弱。该论文旨在通过决策导向的方式重新审视NLG的效用。

Method: 作者提出了一种决策导向的生成文本评估框架，通过市场摘要文本（包括客观的晨间摘要和主观的收盘分析）作为案例，分别考察了仅依赖这些文本时，人类投资者与自主LLM投资代理的交易决策表现，并通过金融收益来衡量决策质量。

Result: 研究发现，无论人类还是LLM代理，仅依赖市场摘要都难以超越随机表现。而包含更丰富分析性内容的评论能够使人类-LLM协作明显优于单独的个体或代理。

Conclusion: 结果突显了评估生成文本时单纯依赖传统内在指标的局限性，强化了以促进人类和LLM协作性决策为核心的新评价范式的重要性。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [51] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 本文比较了Whisper与Wav2Vec-BERT模型在孟加拉语语音识别任务上的表现，结果显示Wav2Vec-BERT在准确率和效率上均胜一筹，适合低资源语言应用。


<details>
  <summary>Details</summary>
Motivation: 采用大规模多语种文本和语音数据训练的神经模型，在支持低资源语言方面显示出巨大潜力。研究旨在评估业界领先的ASR模型在孟加拉语（一种低资源语言）上的表现。

Method: 分别应用OpenAI的Whisper（Small & Large-V2）与Facebook的Wav2Vec-BERT，在Mozilla Common Voice-17 和 OpenSLR两大公开数据集上进行实验。通过系统性微调和超参数优化（如学习率、轮次、模型检查点选择），并以词错误率（WER）、字错误率（CER）、训练时间和计算效率为评估标准，比较模型性能。

Result: Wav2Vec-BERT在所有关键评估指标上均优于Whisper模型，不仅表现更优，还需要更少的计算资源。

Conclusion: 研究表明，Wav2Vec-BERT非常适合低资源语言下的语音识别任务，为相关系统建设提供了有价值的参考。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [52] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs虽能有效形成有说服力的对话，但并不真正理解对话的深层结构和语境，这反映出其在充当对话评估者时存在重要局限性。


<details>
  <summary>Details</summary>
Motivation: LLMs被广泛用于对话类敏感领域（如同行评审、心理健康），但其推理和理解能力存在争议，因此需要深入检验其对对话的理解能力。辩论作为复杂的人类互动形式，是检验LLMs对话理解的理想场景。

Method: 首先评估LLMs维持辩论的能力，然后测量其对话内容理解能力（对话结构和语用语境的理解）。对LLMs和人类之间的互动进行实证测试，包括辩论表现及其对他人的影响。同时还让LLMs自评其对对话深层结构的理解。

Result: LLMs能维护连贯、具有说服力的辩论，经常能够影响参与者和听众的信念。当人们怀疑对方是AI时，会更批判其论点。但LLMs对对话深层结构的理解表现不足，说明作为评估者它们有局限性。

Conclusion: LLMs虽然能进行有说服力的辩论，但它们未必真正理解对话的结构和语境。只要能维持对话并令人信服，理解语境和连贯性的重要性则是次要的。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [53] [Some remarks on the uncolored versions of the original CFI-graphs](https://arxiv.org/abs/2507.01459)
*Yijia Chen,Jörg Flum,Mingjun Liu*

Main category: cs.DM

TL;DR: 本文证明原始CFI-graphs去除颜色后即可满足大多数相关应用，无需引入体积较大的额外结构，并可利用一阶公式复原原有色彩分组。


<details>
  <summary>Details</summary>
Motivation: 有色CFI-graphs应用广泛，但去色操作需引入额外装置，导致图规模膨胀和参数变动，这可能影响相关组合界，对实际应用不理想。希望验证原始CFI-graphs在去色后仍可满足需求。

Method: 以数学证明为主，通过构造一阶逻辑公式，分析无色CFI-graphs是否可替代有色版本，并证明该结论。

Result: 原始CFI-graphs去色后无需引入额外结构即可满足应用需求，并给出一阶公式$(x,y)$在无色图中表达原色信息。

Conclusion: 已经证明只需忘记颜色信息，原始CFI-graphs仍能实现类似应用目的，且可用一阶公式在无色CFI-graphs间表达点原有颜色一致性。

Abstract: The CFI-graphs, named after Cai, F\"urer, and Immerman, are central to the
study of the graph isomorphism testing and of first-order logic with counting.
They are colored graphs, and the coloring plays a role in many of their
applications. As usual, it is not hard to remove the coloring by some extra
graph gadgets, but at the cost of blowing up the size of the graphs and
changing some parameters of them as well. This might lead to suboptimal
combinatorial bounds important to their applications. Since then for some
uncolored variants of the CFI-graphs it has been shown that they serve the same
purposes. We show that this already applies to the graphs obtained from the
original CFI-graphs by forgetting the colors. Moreover, we will see that there
is a first-order formula $\varphi(x,y)$ expressing in almost all uncolored
CFI-graphs that $x$ and $y$ have the same color in the corresponding colored
graphs.

</details>


### [54] [Scheduling on identical machines with conflicts to minimize the mean flow time](https://arxiv.org/abs/2507.01759)
*Nour ElHouda Tellache,Lydia Aoudia,Mourad Boudhar*

Main category: cs.DM

TL;DR: 本文研究多机带冲突约束的调度问题，证明其NP难性，提出数学模型和遗传算法并在算例中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 作者研究的是在相同机器上带有冲突约束的作业调度问题，即某些作业不能同时在不同机器上调度。该问题在实际中具有普遍性和应用价值，如资源共享受限的生产环境。

Method: 采用图论方法将冲突建模为简单无向图。针对该问题，作者提出了数学模型、下界计算方法以及基于遗传算法的求解方法。同时，通过大量基准实例进行实验评估。

Result: 结果显示：即使在只有两台机器及两种不同加工时间下，问题已为NP难；对于单位时间作业，三台机器时问题也为NP难。作者还发现了对于某些特殊冲突图类别，可以多项式时间求解。遗传算法和下界在算例测试中表现良好。

Conclusion: 本文系统研究了带有冲突约束的多机最小平均流动时间调度问题，揭示了问题的计算复杂性，提出了有效的数学模型和启发式方法，并通过实验验证了方法的有效性。

Abstract: This paper addresses the problem of scheduling jobs on identical machines
with conflict constraints, where certain jobs cannot be scheduled
simultaneously on different machines. We focus on the case where conflicts can
be represented by a simple undirected graph, and the objective is to minimize
the mean flow time. We show that the problem is NP-hard even on two machines
and two distinct processing times. For unit-time jobs, the problem becomes
NP-hard when the number of machines increases to three. We also identify
polynomial-time solvable cases for specific classes of conflict graphs. For the
general problem, we propose mathematical models, lower bounds, and a genetic
algorithm. We evaluate their performance through computational experiments on a
wide range of instances derived from well-known benchmark instances in the
literature.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [55] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Main category: cs.FL

TL;DR: 本文提出不可判定性是系统的结构性特征，任何参与不可判定系统的子系统也必然继承该不可判定性，颠覆了通过架构创新可以绕开经典计算极限的观点，对科学建模和知识获取有广泛影响。


<details>
  <summary>Details</summary>
Motivation: 传统上认为不可计算性是某些特定函数或问题的属性，但该文提出将其视为系统的结构性特征。作者希望更深入揭示不可判定性对自然和人工系统预测、建模及知识获取的普遍影响。

Method: 作者提出了因果嵌入（causal embedding）的概念，并证明了一个闭包原理：只要某个子系统在一个不可判定系统中起功能性作用，则该子系统也必然继承不可判定性。通过理论分析和推广，将不可判定问题放到动力系统的框架下讨论。

Result: 证明了不可判定性是系统内广泛存在的结构性限制，参与不可判定系统计算的所有子系统都会继承不可判定性。此外，作者的理论框架反驳了试图用“神谕机”或新架构绕过计算极限的观点，并扩展了数理逻辑和可计算性理论的经典结论。

Conclusion: 不可判定性应被视为系统性的结构属性，而非局部功能限制。这一视角揭示了计算与科学知识边界之间新的拓扑关系，对自然和人工系统的可预测性和建模能力设定了深刻限制。

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>
