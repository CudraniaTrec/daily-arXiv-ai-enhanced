<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 11]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Quantum circuits are just a phase](https://arxiv.org/abs/2507.11676)
*Chris Heunen,Louis Lemonnier,Christopher McNally,Alex Rice*

Main category: cs.PL

TL;DR: 本文提出一种具高层抽象的量子编程语言，提升了量子算法表达能力和程序结构化。该语言理论上具备普适性，能自然地描述多种核心量子算法，并有严苛语义界定及可用的编译器原型，推动量子编程迈向更高层级。


<details>
  <summary>Details</summary>
Motivation: 当前量子程序编写主要停留在近似汇编语言级别，缺乏高层抽象和结构化支持，严重影响可扩展性、清晰度和高阶推理。迫切需要一种支持更高抽象级别和表达能力的量子编程语言。

Method: 作者设计了一种基于全局相位与子空间选择（模式匹配）机制的最小量子编程语言，侧重于本征分解、共轭、受控酉操作等高层构建块，并给出该语言的范畴语义定义。通过实现编译器，从语言映射到量子电路，证明了其编译的语义正确性。

Result: （1）证明该语言描述的量子操作具有普适性，可导出通用基本门集；（2）多个重要量子算法（如Grover算法、哈密顿模拟、量子傅里叶变换等）能以自然简明的方式表述；（3）提供了严谨的范畴语义；（4）实现了原型编译器并验证了其语义正确性。

Conclusion: 本文提出了一种新型且简洁的量子编程语言，通过高阶抽象，大幅提高了量子算法表达的自然性与简洁性。该语言不仅理论上具备普适性，还通过编译器原型实现了实际可用性并保证其语义的正确性。

Abstract: Quantum programs today are written at a low level of abstraction - quantum
circuits akin to assembly languages - and even advanced quantum programming
languages essentially function as circuit description languages. This state of
affairs impedes scalability, clarity, and support for higher-level reasoning.
More abstract and expressive quantum programming constructs are needed.
  To this end, we introduce a novel yet simple quantum programming language for
generating unitaries from "just a phase"; we combine a (global) phase operation
that captures phase shifts with a quantum analogue of the "if let" construct
that captures subspace selection via pattern matching. This minimal language
lifts the focus from quantum gates to eigendecomposition, conjugation, and
controlled unitaries; common building blocks in quantum algorithm design.
  We demonstrate several aspects of the expressive power of our language in
several ways. Firstly, we establish that our representation is universal by
deriving a universal quantum gate set. Secondly, we show that important quantum
algorithms can be expressed naturally and concisely, including Grover's search
algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal
Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give
clean denotational semantics grounded in categorical quantum mechanics.
Finally, we implement a prototype compiler that efficiently translates terms of
our language to quantum circuits, and prove that it is sound with respect to
these semantics. Collectively, these contributions show that this construct
offers a principled and practical step toward more abstract and structured
quantum programming.

</details>


### [2] [Picat Through the Lens of Advent of Code](https://arxiv.org/abs/2507.11731)
*Neng-Fa Zhou,Cristian Grozea,Håkan Kjellerstrand,Oisín Mac Fhearaí*

Main category: cs.PL

TL;DR: 论文展示了Picat解决AoC 2024题目的能力，证明其约束求解和表格化使代码更加简洁高效，尤其适合逆向工程与路径查找类问题。


<details>
  <summary>Details</summary>
Motivation: Picat语言集成了多种编程范式，作者希望通过实例展现Picat在特定类型问题（如逆向工程、路径查找等）上的优势，尤其利用其约束求解、表格化等特性。

Method: 作者选取2024 Advent of Code的若干题目，使用Picat进行求解，并重点展示了其内建的SAT约束求解和tabling（表格化）机制如何优化问题实现。

Result: 结果表明，Picat能够用简洁的声明式代码高效地解决这些问题，相较于命令式语言实现同类问题需付出更多努力。

Conclusion: Picat的多范式集成及其特有的约束求解和表格化机制，使其在某些特定问题上表现出色，值得在类似问题中推广使用。

Abstract: Picat is a logic-based, multi-paradigm programming language that integrates
features from logic, functional, constraint, and imperative programming
paradigms. This paper presents solutions to several problems from the 2024
Advent of Code (AoC). While AoC problems are not designed for any specific
programming language, certain problem types, such as reverse engineering and
path-finding, are particularly well-suited to Picat due to its built-in
constraint solving, pattern matching, backtracking, and dynamic programming
with tabling. This paper demonstrates that Picat's features, especially its
SAT-based constraint solving and tabling, enable concise, declarative, and
highly efficient implementations of problems that would require significantly
more effort in imperative languages.

</details>


### [3] [Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers](https://arxiv.org/abs/2507.11827)
*Shaurya Gomber,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.PL

TL;DR: 本文提出了一种自动合成多面体数值域变换器的新算法，并借助梯度引导搜索策略提升了数值抽象解释器在多个域上的组合性和可调节精度。


<details>
  <summary>Details</summary>
Motivation: 现有的数值抽象解释器依赖于为每个域手工定制、与指令相关的变换器，缺乏处理跨域通用操作的通用算法，导致可扩展性差、难以精准组合推理并且对不同任务的适应性有限。

Method: 提出了一种通用的变换器合成算法，可以针对任何给定的多面体数值域和二次有界守卫操作符（QGO）的具体操作，自动构造参数化的、健全的抽象变换器族。提出了一种梯度引导的搜索策略（AGG），通过下游分析目标和运行时约束高效遍历变换器空间，并在USTAD框架中实现。

Result: 算法在Zones、Octagons和Polyhedra三种数值抽象域中均能成功构造出健全的变换器族。USTAD通过可组合推理和高效的梯度引导搜索，相较基线方法实现了灵活可调的显著精度提升。

Conclusion: 该方法为数值抽象解释器带来了更强的可扩展性、适应性和精度，通过通用变换器合成和AGG搜索，有效突破了人工定制与组合性分析中的瓶颈。

Abstract: Numerical abstract interpretation is a widely used framework for the static
analysis of numerical programs. However, existing numerical abstract
interpreters rely on hand-crafted, instruction-specific transformers tailored
to each domain, with no general algorithm for handling common operations across
domains. This limits extensibility, prevents precise compositional reasoning
over instruction sequences, and forces all downstream tasks to use the same
fixed transformer regardless of their precision, efficiency, or task-specific
requirements. To address these limitations, we propose a universal transformer
synthesis algorithm that constructs a parametric family of sound abstract
transformers for any given polyhedral numerical domain and a concrete operator
from the class of Quadratic-Bounded Guarded Operators (QGO), which includes
both individual instructions and structured sequences. Each instantiation in
this family is sound by construction, enabling downstream analyses to adapt the
transformer to their particular needs. The space of transformers is
differentiable but complex. To efficiently explore this space of transformers,
we introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided
search strategy that steers the search process based on downstream analysis
objectives and runtime constraints. We implement these ideas in the USTAD
framework and evaluate their effectiveness across three numerical abstract
domains: Zones, Octagons, and Polyhedra. Our results demonstrate that the
universal synthesis algorithm successfully constructs sound families of
transformers across domains, and that USTAD achieves significant, tunable
precision gains over baselines by leveraging compositional reasoning and
efficient gradient-guided traversal of the transformer space.

</details>


### [4] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: 本文总结了情境等价饱和的相关研究，着重介绍其在egglog中与关系等价饱和结合时遇到的主要挑战和应用前景。


<details>
  <summary>Details</summary>
Motivation: 将等价饱和技术和情境等价规则应用推广到支持关系表达的egglog，提升程序优化与重写能力。

Method: 对目前的情境等价饱和方法进行总结，分析其主要应用，讨论与关系模型结合时面临的关键问题。

Result: 总结了已有的情境等价饱和方法，阐明其主要应用领域，指出其与关系等价饱和结合的关键挑战。

Conclusion: 本文探讨了如何将情境等价饱和方法应用到egglog中的关系等价饱和，指出了主要挑战。

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: 本文针对量子软件架构中模式与策略选择的难题，基于文献和社区数据挖掘，提出决策模型并经实证验证有效，为实践者提供实际指引，推动量子软件架构设计方法的发展。


<details>
  <summary>Details</summary>
Motivation: 量子软件架构开发中存在巨大挑战，主要体现在如何在复杂的量子软件系统中选择和实施合适的架构模式与策略。目前缺乏有效的指引和最佳实践，限制了开发效率和系统质量。

Method: 本研究通过两步法：（1）利用数据挖掘（分析GitHub和Stack Exchange）与系统性文献综述，识别出与量子软件架构相关的模式、策略及其质量属性；（2）设计决策模型，并通过对16名量子软件从业者的半结构化访谈，对模型的熟悉度、可理解性、完整性和实用性进行了评估。

Result: 提出了涵盖六大设计领域（通信、分解、数据处理、容错、集成与优化、算法实现）的量子软件架构决策模型，经实践者评估证明能够有效帮助其选择合适的架构模式和策略，从而缓解量子软件架构设计的挑战。此外，数据集向社区开放，便于复现和拓展。

Conclusion: 该研究为量子软件架构师和开发者在复杂系统设计过程中选择合适的架构模式和策略提供了明确指引，提升了架构设计效率和系统质量，并为社区后续研究和实践提供了数据与知识基础。

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [6] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint是一种新颖的指令跟随框架，通过合成linter生成数据微调大模型，提升了代码质量分析中新或复杂代码习惯的检测与修复能力。其在PEP习惯检测与定位上取得优异成绩，显示出比传统静态方法和同类大模型更强的泛化与实用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）虽然在代码生成上很成功，但由于依赖静态训练数据，难以适应不断演进的代码最佳实践，因此在代码质量分析方面表现有限。

Method: 作者提出了一种新的instruction-following框架MetaLint，将代码质量分析转化为基于高级规范检测和修复有问题的语义代码片段或代码习惯（idioms）的任务。与传统方法使用静态、基于规则的数据训练不同，MetaLint基于合成的linter生成数据进行指令微调，以实现从简单到复杂的泛化能力，无需重新训练即可适应新颖或复杂的代码模式。

Result: MetaLint在检测未见过的PEP代码习惯时提升了泛化能力，在idiom检测上的F-score达到了70.37%，召回率为70.43%，均为所有模型中最高。在代码问题定位（localization）任务上，其成绩（26.73%）在4B参数规模下与更大的同类SOTA模型（如o3-mini）相当。

Conclusion: MetaLint方法有效提升了代码质量分析模型的泛化能力和适应能力，与更大的最新模型表现相当，有望成为面向未来的代码质量分析工具。

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [7] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: 研究通过自动化分析学生Web应用的REST API设计，发现代码质量普遍不达标，呼吁教育中加强API设计教学并引入自动检测工具。


<details>
  <summary>Details</summary>
Motivation: 本科计算机课程中，软件质量通常因时间有限和基础技能优先而被忽视，导致学生毕业后无法满足业界对代码质量的期望。作者希望通过量化学生代码质量，为教育改进和企业招聘提供依据。

Method: 分析第三年Web技术课程40个学生开发的全栈Web应用，利用自动静态分析工具，检查其REST API设计规则的遵循情况。

Result: 发现学生作品存在诸多基础API约定违规，如98%缺少连字符、88%复数使用错误、83%HTTP方法误用。

Conclusion: 当前教育中对API设计等软件质量内容重视不足，建议加强相关教学，并采用自动化工具以提升学生项目的代码质量。

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [8] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: 作者提出用大语言模型（LLM）自动生成极端情形测试用例，在网络协议实现和算法中成功发现新漏洞，方法简单高效，明显优于传统边界值分析。


<details>
  <summary>Details</summary>
Motivation: 物理学家通常通过考虑极端情况来检验理论。这一做法启发作者将极端测试自动化应用于网络软件。

Method: 方法包括两步：首先用大语言模型（LLM）生成输入约束（如DNS名称长度限制）；然后再用LLM生成违反这些约束的测试用例。

Result: 作者在HTTP、BGP和DNS实现上应用此方法，发现了新漏洞。方法也能推广至如最短路径算法等其他网络软件，并用于生成筛查极端输入的过滤代码。

Conclusion: LLM自动极端测试简化了网络软件的极端案例测试流程，优于传统边界值分析，并能自动发现实际漏洞。作者还提出了进一步用智能体AI实现更高度自动化。

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [9] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: 本文针对流程挖掘中的合规性检查分析，提出了一个系统性的任务分类法，以帮助明确可视化工具的目标和用途，并促进流程挖掘与可视分析领域的协同发展。


<details>
  <summary>Details</summary>
Motivation: 当前的合规性检查可视化工具种类繁多，但它们所服务的分析目的往往不明确，缺乏对这些目的的系统性理解，使得评估这些可视化工具的实用性变得困难。因此，需要对合规性检查分析任务有更深入的理解，以促进可视化工具的有效开发与评估。

Method: 本文提出了一个合规性检查任务分类法（task taxonomy），该分类法从任务目标、手段、约束类型、数据特性、数据对象与数据基数等多个维度，对合规性检查分析过程中可能出现的任务进行归类。该方法结合了流程挖掘与可视分析两个领域的概念，以便为学者指定可视化工具开发和任务需求。

Result: 提出的任务分类法有助于学者明确合规性检查可视化工具的具体应用目的，使研究人员能够更精确地指定相关分析任务，进而支持对可视化工具的科学评估和跨领域合作。

Conclusion: 该文构建了合规性检查分析任务的分类体系，为流程挖掘与可视分析领域专家之间的协作和可视化工具的针对性开发与评估提供理论基础与实践工具。

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [10] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: 本文提出了集成大语言模型和多反馈机制的LLAMA模糊测试框架，实现了对智能合约更高效的变异调度，实验中在漏洞检测和覆盖率方面均优于主流工具。


<details>
  <summary>Details</summary>
Motivation: 当前智能合约安全至关重要，模糊测试是主要手段之一，但现有模糊测试大多关注种子调度与生成，而对变异调度研究较少。论文旨在提升智能合约模糊测试中变异调度的效果。

Method: 提出一种基于大语言模型（LLMs）的多反馈智能合约模糊测试框架LLAMA。主要包括：（1）层次化提示策略，指导LLM生成有效初始种子，并进行预筛选；（2）多反馈优化机制，结合运行时覆盖率与依赖反馈，优化种子生成、选择与变异调度；（3）进化式模糊引擎，动态调整变异操作概率，并引入符号执行以避免测试陷入局部最优。

Result: LLAMA在测试中取得了领先的表现。实验显示，其指令覆盖率达到91%，分支覆盖率达到90%，共检测出148种已知漏洞中的132种，超越了现有最先进的模糊器。

Conclusion: LLAMA框架有效提升了智能合约模糊测试在覆盖率和漏洞检测方面的表现，具有较强的实际适用性和先进性。

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [11] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: 为了解决不同库版本下代码生成问题，作者构建了一个包含328个带有单元测试的Python代码补全任务的数据集。实验表明，当前主流代码生成模型在该任务上表现一般，GitChameleon为后续系统开发和评估提供了重要支持。


<details>
  <summary>Details</summary>
Motivation: 随着软件库快速演进，代码生成需要不断适应频繁的库版本更新，同时保持向后兼容性。这为现有的代码生成系统带来了很大挑战，尤其在针对特定库版本生成可执行代码方面，现有基准很少有基于执行结果的评估。

Method: 本文提出了GitChameleon数据集，精心整理了328个基于不同Python库版本的代码补全任务，并为每个任务配备可执行的单元测试。通过对现有主流大模型、基于大模型的Agent、代码助手及RAG系统，进行基于具体版本要求的代码生成和功能性执行评测。

Result: 主流的AI代码生成系统在该任务上的表现有限，最好的企业级模型成功率也仅在48-51%之间，反映出库版本带来的挑战性。

Conclusion: GitChameleon数据集通过强调执行性和库动态变化，为AI代码生成系统的适应性和鲁棒性发展提供了新基准和参考，有助于推动更通用、可靠的代码生成技术。

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [12] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: 提出利用LLM自动化实现SaaS静态页面的智能定价抽取工具，在多数据集上验证有效性，有助于提升定价管理效率与一致性，但仍需解决幻觉和复杂结构等问题。


<details>
  <summary>Details</summary>
Motivation: SaaS市场扩张带来了复杂多变的定价管理，手动更新既耗时又易出错，缺乏自动化工具阻碍了定价优化与扩展。

Method: 提出基于大语言模型（LLM）的自动化解决方案AI4Pricing2Yaml，结合网页爬取与LLM技术，自动将SaaS静态定价HTML转换为智能定价（iPricing）格式。系统包含信息抽取模块，用于提取定价组件、计划、功能、用量限制等。

Result: 在30个不同商用SaaS共150+定价方案数据集上验证，系统在各步骤成功提取所需元素，展现了有效性。但在幻觉、结构复杂和动态内容等方面仍面临挑战。

Conclusion: 智能自动化定价转换有助于提升SaaS定价管理效率与一致性，对提升定价模型可扩展性及适应市场动态具有重大意义。未来计划提升系统抽取能力和适应性。

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [13] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: 提出了一种结合设计思维、语言决策和A/B测试的网页易用性评价方法，并通过实证在大学Moodle平台实现，有效提升了用户参与和可用性评估的科学性。


<details>
  <summary>Details</summary>
Motivation: 近年来，提升用户界面（无论是移动应用还是网站）的用户满意度成为关注重点。人机交互的核心之一是网页易用性。当前A/B测试虽能比较两种设计的数据，但对于更复杂的测试，比如涉及不同设计与不同类型用户（真实和虚拟用户）时，在线工具支持有限，因此有必要探索新的网页易用性评估方法。

Method: 本文提出基于以用户为中心的方法（如设计思维和语言决策）的网页易用性评估方法，称为“基于语言决策的网页易用性评估”。该方法通过角色扮演让用户参与多种易用性测试，包括广泛认可的系统可用性量表（SUS），并结合A/B测试将其整合进决策支持系统。

Result: 该方法被应用于案例研究，实际招募用户对墨西哥瓜达拉哈拉大学的三个Moodle平台进行评估。

Conclusion: 用户参与、设计思维与语言决策相结合的方法可有效支持以用户为中心的网页易用性评估，并可通过A/B测试的决策支持系统实现。

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [14] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: 该论文提出MERA Code基准，用于评估俄语环境下代码生成大模型的实际开发能力，补足了传统自然语言任务评测的不足，并公开相关工具以促进模型发展与标准化评测。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）在软件工程领域的自动化能力有显著提升，但现有评测方法主要聚焦于自然语言任务，忽视了代码质量评估。现有基准测试过于关注高层推理能力，对于可执行代码与实际应用性能考虑不足，导致无法全面评估模型在生产环境中的能力与风险。

Method: 提出MERA Code，一款专门用于评估俄语代码生成大模型的新基准。该基准包含涉及8种编程语言的11项评测任务，设计了揭示模型实际编码能力的技能分类体系。基准工具包括开源代码库、兼容多种编程环境的评分体系、线上排行榜和提交系统，并对多个主流开源与API模型进行了评测分析。

Result: 通过MERA Code对开源及前沿API模型进行了评测，揭示了这些模型在俄语及非英语编码任务中的局限性。公开发布MERA Code，以推动研究社区规范评测流程，指导未来模型发展。

Conclusion: MERA Code填补了现有基准在代码质量与多语言评测方面的空白，为评估和推动代码生成大模型在非英语环境下的实际能力提供了标准化工具和方法。未来研究可基于此实现对模型能力的更全面认识和新特征发展。

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [15] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: 本文提出SWE-Perf，首个用于评估大语言模型在真实仓库代码性能优化任务的基准数据集，发现现有LLM与专家优化能力存在显著差距，为该领域研究提供了有力支撑。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）已经在代码生成和修复方面取得了优异表现，但其在真实软件仓库级别的代码性能优化能力尚未被深入研究。为填补这一空白，亟需构建相关基准来系统性评估LLMs在代码性能优化方面的表现。

Method: 本文提出了SWE-Perf，这是首个专门针对真实代码仓库的代码性能优化任务而设计的LLM基准。SWE-Perf包含140个经过精心筛选的实例，每个实例均来自于GitHub上受欢迎仓库的性能改进pull request。每个实例均包括相关代码、目标函数、性能相关测试、专家修补补丁和可执行环境。作者还通过综合评测代表性方法（如Agentless和OpenHands），对比了文件级和仓库级的提升能力。

Result: 实验结果显示，当前LLMs在代码性能优化上与专家级水平存在明显差距，表明这一领域仍有大量研究提升空间。

Conclusion: SWE-Perf基准为评估并推动LLMs用于真实软件仓库代码性能优化提供了关键工具，也揭示了当前自动优化技术需大力提升。

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Comment on Decidability of Quasi-Dense Modal Logics by Lyon and Ostropolski-Nalewaja](https://arxiv.org/abs/2507.11644)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 原论文声称解决准密模态逻辑判定性问题并给出复杂度上界，但其证明存在严重错误，因此该问题依然悬而未决。


<details>
  <summary>Details</summary>
Motivation: 之前的研究试图给出准密模态逻辑（quasi-dense modal logics）可判定性的问题的答案，并为其提供EXPSPACE上的上界。模态逻辑的判定性是理论计算机科学的核心难题。

Method: 重新分析已发表论文的证明过程，检查关键推理步骤，发现其假设存在根本性错误。

Result: 原论文的证明存在无法修正的严重缺陷，其中的核心错误是误以为两个一致集的并仍然一致。

Conclusion: 原研究对准密模态逻辑可判定性问题的解答是不成立的，该问题仍然未解决。

Abstract: In \cite{Lyon24} the question of the decidability of quasi-dense modal logics
is answered, and an upper bound in EXPSPACE is given. Unfortunately, authors'
intricate proof contains a major flaw that cannot be fixed, leaving the
question wide open. Once identified, this error roughly amounts to assuming
that the union of two consistent sets is consistent, which is of course wrong.

</details>


### [17] [Counting Answer Sets of Disjunctive Answer Set Programs](https://arxiv.org/abs/2507.11655)
*Mohimenul Kabir,Supratik Chakraborty,Kuldeep S Meel*

Main category: cs.LO

TL;DR: 本文提出了一种适用于析取逻辑程序的高效答案集计数框架SharpASP-SR，通过减法归约和新刻画法，结合枚举技术后显著提升了计数性能，达到业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 针对析取逻辑程序下答案集计数的高效工具依然不足，现有方法主要适用于普通逻辑程序，实际应用中如概率推理、网络可靠性分析等领域急需更强大工具。本文旨在推动析取逻辑程序答案集计数方法的发展。

Method: 提出SharpASP-SR框架，通过减法归约到投影命题模型计数，并引入了新的答案集刻画方法，使中间表示保持多项式规模，从而利用最新的投影模型计数技术。此外，作者将SharpASP-SR与枚举技术相结合，形成混合计数方法。

Result: SharpASP-SR在多种基准测试中大幅超越现有计数器，尤其在大规模答案集情况下表现突出。结合混合计数方案后，能在各种析取程序上实现最优性能。

Conclusion: SharpASP-SR以其更高的效率显著优于现有计数器，特别是在答案集数量较大的实例上，且结合枚举技术后实现了在不同类型析取逻辑程序上的最先进性能。

Abstract: Answer Set Programming (ASP) provides a powerful declarative paradigm for
knowledge representation and reasoning. Recently, counting answer sets has
emerged as an important computational problem with applications in
probabilistic reasoning, network reliability analysis, and other domains. This
has motivated significant research into designing efficient ASP counters. While
substantial progress has been made for normal logic programs, the development
of practical counters for disjunctive logic programs remains challenging.
  We present SharpASP-SR, a novel framework for counting answer sets of
disjunctive logic programs based on subtractive reduction to projected
propositional model counting. Our approach introduces an alternative
characterization of answer sets that enables efficient reduction while ensuring
that intermediate representations remain of polynomial size. This allows
SharpASP-SR to leverage recent advances in projected model counting technology.
Through extensive experimental evaluation on diverse benchmarks, we demonstrate
that SharpASP-SR significantly outperforms existing counters on instances with
large answer set counts. Building on these results, we develop a hybrid
counting approach that combines enumeration techniques with SharpASP-SR to
achieve state-of-the-art performance across the full spectrum of disjunctive
programs.

</details>


### [18] [Anthem 2.0: Automated Reasoning for Answer Set Programming](https://arxiv.org/abs/2507.11704)
*Jorge Fandinno,Christoph Glinzer,Zachary Hansen,Jan Heuer,Yuliya Lierler,Vladimir Lifschitz,Torsten Schaub,Tobias Stolzmann*

Main category: cs.LO

TL;DR: 本文介绍了Anthem 2.0，这是一款支持Clingo mini-gringo片段逻辑程序自动验证的工具，能够借助定理证明器进行性质分析和等价性验证，并指导用户高效解读分析结果。


<details>
  <summary>Details</summary>
Motivation: 作者希望开发一个能够辅助验证复杂逻辑程序的工具，特别针对Clingo的mini-gringo输入语言片段，这种需求源自于逻辑程序越发复杂、需求验证和正确性保障的场合。

Method: Anthem 2.0能将逻辑程序翻译成here-and-there逻辑的公式表达，并调用一阶定理证明器，对程序的紧致性、规范符合性及等价性进行自动化分析与验证。

Result: Anthem 2.0能够识别和验证mini-gringo逻辑程序中的性质，包括程序的规范遵循和等价性。系统还支持用户解读结果并有效提升程序分析的可信度和分析效率。

Conclusion: Anthem 2.0是一款功能强大的逻辑程序验证工具，能够扩展ASP的分析能力，辅助用户准确高效地进行程序验证和理解。

Abstract: Anthem 2.0 is a tool to aid in the verification of logic programs written in
an expressive fragment of Clingo's input language named mini-gringo, which
includes arithmetic operations and simple choice rules but not aggregates. It
can translate logic programs into formula representations in the logic of
here-and-there, and analyze properties of logic programs such as tightness.
Most importantly, Anthem 2.0 can support program verification by invoking
first-order theorem provers to confirm that a program adheres to a first-order
specification, or to establish strong and external equivalence of programs.
This paper serves as an overview of the system's capabilities. We demonstrate
how to use Anthem 2.0 effectively and interpret its results.

</details>


### [19] [Approximation Fixpoint Theory as a Unifying Framework for Fuzzy Logic Programming Semantics (Extended Version)](https://arxiv.org/abs/2507.11961)
*Pascal Kettmann,Jesse Heyninck,Hannes Strass*

Main category: cs.LO

TL;DR: 本论文将经典逻辑编程的稳定模型与良基语义，统一地重构到逼近不动点理论下，并将AFT成功推广到模糊逻辑领域，推动了模糊逻辑编程语义的发展和实际应用。


<details>
  <summary>Details</summary>
Motivation: 在处理不确定性问题时，模糊逻辑编程被广泛应用，但现有的经典逻辑语义如何扩展到模糊逻辑，一直存在理论和应用上的挑战。作者希望将经典的稳定模型语义和良基语义推广到多值逻辑（即模糊逻辑），并融合进更一般化的理论框架中，为模糊逻辑推理提供坚实的基础。

Method: 作者将稳定模型语义和良基语义重构到逼近不动点理论（AFT）这一通用理论框架下，这样不仅拓展了AFT的应用范围，也为已有AFT成果在模糊逻辑编程领域的应用奠定根基。同时，作者通过此框架对现有语义之间的关系进行了形式化分析，还推广了从经典逻辑到模糊逻辑的分层（stratification）概念，并提出了更精确的语义变体。

Result: 通过AFT框架，稳定模型语义和良基语义在模糊逻辑编程中得到了系统化重构。借助该框架，作者理清了模糊逻辑中已有语义之间的正式关系，实现了语义的分层推广，并提出了精度更高的语义版本。

Conclusion: 利用AFT框架能够统一并推广经典逻辑编程语义至模糊逻辑领域，丰富了模糊逻辑编程的理论基础和推理工具，并带来更强的理论可扩展性和灵活性。

Abstract: Fuzzy logic programming is an established approach for reasoning under
uncertainty. Several semantics from classical, two-valued logic programming
have been generalized to the case of fuzzy logic programs. In this paper, we
show that two of the most prominent classical semantics, namely the stable
model and the well-founded semantics, can be reconstructed within the general
framework of approximation fixpoint theory (AFT). This not only widens the
scope of AFT from two- to many-valued logics, but allows a wide range of
existing AFT results to be applied to fuzzy logic programming. As first
examples of such applications, we clarify the formal relationship between
existing semantics, generalize the notion of stratification from classical to
fuzzy logic programs, and devise "more precise" variants of the semantics.

</details>


### [20] [SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques](https://arxiv.org/abs/2507.12286)
*Anouk Oudshoorn,Magdalena Ortiz,Mantas Simkus*

Main category: cs.LO

TL;DR: 本文提出了一种能结合OWL本体与SHACL约束验证的新语义与方法，实现了验证任务的简化，并对其复杂性进行了细致分析。


<details>
  <summary>Details</summary>
Motivation: SHACL与OWL都是RDF管理的重要标准，但由于语义假设不同，二者的结合存在实际需求及挑战。本文动机在于弥合两者在语义和计算上的鸿沟，提出既可兼容本体又能高效验证数据完整性的解决方案。

Method: 作者基于核心通用模型来定义SHACL在本体（采用Horn-ALCHIQ）下的验证语义，提出了模型构造技术，并通过有限表示，实现了可将SHACL验证归约为标准验证的重写技术。随后，研究了SHACL验证在有本体时的计算复杂性。

Result: 获得了一种通用模型语义下的验证方法，相关技术能有效将带本体的SHACL验证简化为标准问题，并揭示即使简单本体也会使问题变为EXPTIME-complete，数据复杂度为PTIME-complete。

Conclusion: 文章提出了一种在本体存在下，对SHACL约束验证进行语义定义的新方法，并给出相应模型构建与重写技术，同时分析了其复杂性。

Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: 不同LLM对同一文学作品的评价表现出明显主观性和多样性，说明这些模型具有人类评论家的风格特征，而非单一的标准参考。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探究大型语言模型（LLMs）在文学评价中是否会表现出主观性和独特的审美偏好，是否能够像人类文学评论家一样展现出多样化的价值判断。

Method: 将十部日文科幻短篇小说翻译成英文，由六个最先进的LLM在七次独立测试中进行评估。通过主成分分析（PCA）、聚类技术和TF-IDF分析来检测评价一致性、模式和用词特征。该方法设计了单日多轮对照，减少外部偏差。

Result: 模型在对小说的评价上一致性差异显著（α值1.00到0.35），出现五种不同的评价模式。不同小说的评价方差高达4.5倍，不同模型的用词存在显著差异。

Conclusion: LLMs在文学评价上并非中立参考，而是表现出类似人类不同批评流派的主观评价特征，其隐含的价值体系与人类批评家相似。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [22] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 本文提出了涵盖多种地图类型与任务的MapIQ数据集，系统评估了多模态大语言模型在地图视觉问答任务中的表现及其鲁棒性，并对未来改进提出了方向。


<details>
  <summary>Details</summary>
Motivation: 地图相关视觉问答（Map-VQA）领域的研究主要聚焦于等值地图，覆盖的主题种类和视觉分析任务有限。为了填补这一空白，需要扩展研究到更多类型的地图和分析任务。

Method: 作者提出了MapIQ基准数据集，包括14,706个问答对，覆盖三种不同的地图类型和六大主题。此外，利用六种视觉分析任务对多种多模态大语言模型（MLLMs）进行了评测，并设计实验考察不同地图设计变化对模型鲁棒性的影响。

Result: 实验表明，不同MLLMs在不同地图类型和视觉任务下表现各异，且与人类基线有差距。地图设计元素的变化显著影响MLLMs的表现，结果揭示了这些模型对内在地理知识及地图设计的依赖性。

Conclusion: MapIQ数据集扩展了Map-VQA研究范围，有助于更全面评估和提升多模态大语言模型在地图视觉理解任务上的性能，同时为模型鲁棒性和地图设计优化等后续研究提供了依据。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [23] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: 本研究通过结合少样本和增量学习，利用多语言预训练模型在少量波斯语数据上实现了接近96%准确率的情感分析，显示了该方法在低资源语言中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 针对波斯语等低资源语言情感分析训练数据稀缺的问题，尝试借助高资源语言的知识迁移，提高波斯语情感分析的效果。

Method: 采用三种主流的多语言预训练模型（XLM-RoBERTa、mDeBERTa、DistilBERT），通过少样本学习和增量学习在多个来源的小规模波斯语数据集上进行微调。

Result: mDeBERTa和XLM-RoBERTa模型在波斯语情感分析任务中达到了96%的准确率，验证了方法的有效性。

Conclusion: 结合少样本学习和增量学习的方法，利用多语言预训练模型（尤其是mDeBERTa和XLM-RoBERTa），可以在有限的波斯语数据资源下实现高精度的情感分析。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [24] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: 本文提出了PgM多模态学习框架，通过分区器分别处理单模与配对模态特征，显著提升多模态任务表现，兼具迁移性与解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习通过结合多个模式的信息来提高学习效果，但每种模式的表示可分为只需单模训练即可学到的单模特征，以及通过跨模态交互才能学到的配对模态特征。针对如何高效区分和利用这两类特征，激发了本文的研究。

Method: 本文提出一种基于分区器指导的模态学习框架（PgM）。该方法由模态分区器、单模学习器、配对模态学习器和单-配对模态解码器组成。首先用分区器将学习到的模态表示分割为单模和配对模态特征，然后分别用专用学习器对两部分特征进行学习，最后通过解码器重建模态表示。

Result: 在四个多模态任务上进行了大量实验，结果表明PgM框架有效提升了多模态表示学习的性能，并且能够迁移应用于现有模型。还通过可视化展示了单模与配对模态特征在多种任务和模态下的分布，为其贡献提供了直观解释。

Conclusion: PgM方法能够彻底学习和分配单模及配对模态特征，并为不同下游任务调整特征分布，具有良好泛化性和迁移性。

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [25] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: 本文提出了一套面向复杂表格图像、多模态、可解释的问答系统ExpliCIT-QA，通过链式思维、自动代码生成等实现答案和推理过程的全透明展示，提升了表格问答系统在关键行业中的可信度与应用前景。


<details>
  <summary>Details</summary>
Motivation: 目前表格问答（TableVQA）系统多以黑盒方式端到端输出答案，缺乏合理的可解释性和可审计性，难以满足金融、医疗等敏感领域的应用需求。该文提出了面向可解释、可审计、处理复杂表格图像的多模态表格问答新系统。

Method: 提出ExpliCIT-QA系统，采取模块化设计，包含表格内容提取与转换、语言推理、代码生成、代码执行以及自然语言解释等环节。系统将表格图像中内容链式推理抽取出来，用自然语言解释步骤，并基于推理自动生成Python/Pandas脚本并运行，所有中间结果可透明展示与追溯。

Result: 在TableVQA-Bench基准测试集上，ExpliCIT-QA在可解释性和透明性方面优于现有基线，提升了在需要审计的敏感领域（如金融、医疗）中的实用性。

Conclusion: ExpliCIT-QA通过多模块方案协同，实现了多模态表格问答流程的步骤化解释及全链路可审计，为端到端表格视觉问答系统的可解释性提供了有力支持，有望推广至对结果可追溯性有高要求的实际领域。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [26] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: 本文提出结合静态语法分析与LLM的新方法CRABS，实现了不运行代码即可高精度提取Python数据科学笔记本中的信息流和依赖关系，在实际Kaggle笔记本实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 理解和分析Python数据科学、机器学习笔记本的操作流和信息流对评估、复用及迁移笔记本至新任务至关重要。然而，依赖重执行来探索笔记本因环境依赖难解等实际困难而不可行，而现有大语言模型（LLM）在无执行代码时存在理解偏差和上下文长度限制。

Method: 提出了一种基于LLM与有限句法分析结合的“Capture and Resolve Assisted Bounding Strategy (CRABS)”，利用抽象语法树（AST）进行浅层句法解析，捕捉单元格间I/O范围的估计，通过LLM对有歧义部分进行逐单元格零样本推理，最终推断出真实的单元格数据输入和输出。

Result: 在50个精选、标注的Kaggle笔记本和3454个实际单元格I/O上，CRABS方案对句法结构无法消解的1425个歧义点中，LLM正确解决了1397个（98%）。整体上，该方案在辨识单元格信息流和依赖时，平均F1得分分别达到98%和99%。

Conclusion: CRABS方法能高效准确地自动提取复杂Python笔记本中的信息流和执行依赖，显著提升了LLM处理此类任务的能力，不需实际运行代码即可支持自动化分析、复用与迁移。

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [27] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: 本论文提出通过将情感分数与transformer模型结合的方法，提升了新闻句子主观性检测的多语言表现，尤其在未见语言（如希腊语）上取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 新闻内容的主观性检测对于虚假新闻筛查和信息可信度评估非常重要，也面临多语言和跨语言的挑战。该赛题旨在推动跨语言场景下主观性检测技术的发展和评估。

Method: 采用基于transformer的分类器（如mDeBERTaV3-base、ModernBERT-base、Llama3.2-1B），在标准微调的基础上，将辅助模型得到的情感得分作为特征与句子表征结合进行训练。同时，为了解决类别不平衡问题，使用校准的决策阈值。

Result: 整合情感特征后，模型表现明显提升，特别是在主观性F1分数上。提出的框架在多语言环境下表现出良好的泛化能力，例如在希腊语测试集上获得Macro F1 0.51、排名第一。

Conclusion: 情感分数作为辅助特征整合到transformer主观性检测模型中，能显著提升其多语言及跨语言的主观性检测能力，模型在多个语言尤其是新语言上表现优异。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [28] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本文复现并整合了现有研究，发现LLM中的注意力头通过一般复制抑制而非选择性反事实抑制机制强化事实输出，且其行为在不同领域和模型规模下有差异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大型语言模型（LLM）面对事实与反事实信息冲突时，注意力头（attention heads）在处理这些竞争信息中的具体作用。动机还在于复现与整合近期三项使用机制可解释性方法的相关研究成果。

Method: 通过复现实验，分析了注意力头强度与模型输出事实比例之间的关系，评估了不同假设（如注意力头对子事实信息的抑制机制）并探索了注意力模式是否具有领域特异性。主要采用机制可解释性分析工具。

Result: 研究发现，促进事实输出的注意力头主要通过一般性的信息复制抑制（copy suppression）来实现，而不是有选择地抑制反事实信息。此外，注意力头的行为表现出领域相关性，模型规模越大，注意力头表现出更强的专业性和类别敏感性。

Conclusion: 大型语言模型中的注意力头主要通过抑制信息复制而增强事实输出，这种机制并非针对反事实的选择性抑制，且注意力头的行为会受到领域和模型规模的影响。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [29] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: 作者发布了涵盖英语及22种印度官方语言的大数据集，并基于主流机器学习与深度学习技术提供了强健的基线模型，两者均对提升多语言识别、尤其是印度多语场景下的NLP研究极具价值。


<details>
  <summary>Details</summary>
Motivation: 语言识别任务是NLP中的一个基础且关键的环节，特别是在处理嘈杂、短文本以及多种语言混合的环境下格外具有挑战性。印度多种语言在词汇和语音上有相似之处，但也有显著差别，并且许多语言共享同一种文字脚本，进一步增加了语言识别的难度。

Method: 本文公开了一个包含英语及全部22种印度官方语言、共23万句带语言标注的数据集，其中多数语言数据为新创。研究还基于最新的机器学习和深度学习方法开发并发布了稳健的基线模型。

Result: 所提出的数据集和基线模型表现与现有最先进的语言识别模型相当，可用于推动该领域的进一步研究。

Conclusion: 本文为印度语言的识别任务提供了大规模、高质量的数据集和强有力的基线模型，为相关NLP应用奠定了坚实基础。

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [30] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 本论文提出一种让自回归大模型可一次预测多个token的新方法，在保持输出质量的同时，将文本生成速度大幅提升，尤其在代码、数学和聊天任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型由于顺序生成一个token的固有限制，导致推理速度和并行性不足，特别是在生成后期内容已较为确定时。这一瓶颈影响了应用效率和体验。

Method: 作者提出一种新颖框架，结合多项技术来实现同时预测多个后续token：（1）引入masked-input方法，实现多个未来token的联合预测；（2）提出门控LoRA结构，在不影响原模型能力的基础上适配多token预测；（3）加入轻量可学习采样器，生成连贯的token序列；（4）设计多种辅助loss提升生成token一致性和准确性；（5）使用未来token指数扩展的猜测式生成方法。该方法通过对预训练模型有监督微调实现。

Result: 该方法能够带来显著推理速度提升，如代码与数学生成加速近5倍，聊天和知识问答任务加速2.5倍，且保持生成质量不下降。

Conclusion: 用多项创新机制实现了大模型的多token同时预测，极大提升了生成速度且无质量损失，推动了自回归语言模型实际应用效率。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [31] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: 该论文表明，不同行业的数据对PII识别模型迁移和融合有差异性，法律数据对传记有很好迁移效果，医学领域则较难迁移。低专业领域仅10%数据即可实现高质量识别，展示了样本高效利用的可能性。


<details>
  <summary>Details</summary>
Motivation: 提高自动化文本匿名化中个人身份信息（PII）识别的准确性，探索模型跨领域迁移、多领域数据融合和小样本学习对PII识别效果的影响。

Method: 利用医疗（I2B2）、法律（TAB）和传记（Wikipedia）等领域的带注释语料库，对模型进行了四个维度的评估：领域内表现、跨领域可迁移性、数据融合效果及小样本学习能力。

Result: 法律领域数据对传记文本的迁移效果良好，而医学领域表现出较强的抗迁移性。数据融合对不同领域的提升效果有差异。在专业性较低的领域，仅用10%的训练数据也能获得较高质量的PII识别效果。

Conclusion: 跨领域模型迁移、多领域数据融合和小样本学习能在一定条件下有效提升PII识别的泛化能力，但具体成效依赖于目标领域的特性与融合策略。高效、样本利用率高的方法在专业性较低的场景具有实际应用潜力。

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [32] [The Directed Disjoint Paths Problem with Congestion](https://arxiv.org/abs/2507.12096)
*Matthias Bentert,Dario Cavallaro,Amelie Heindl,Ken-ichi Kawarabayashi,Stephan Kreutzer,Johannes Schröder*

Main category: cs.DM

TL;DR: 本文针对著名的定向分离路径问题，证明了只要终端对足够多则无论拥塞常数多大均为NP完全，推翻了之前关于拥塞为2时的乐观猜想；但当拥塞为2且终端对为3时，可以高效多项式算法求解。


<details>
  <summary>Details</summary>
Motivation: 在理论计算机科学中，定向分离路径问题（Directed Disjoint Paths Problem, DDP）是研究网络路由、布线等多种场景中的基础难题。该问题的复杂度特征一直是图论和算法领域关注的热点，特别是在不同拥塞（congestion）和终端对（terminal pairs）数量下的难易程度。

Method: 作者通过复杂度归约证明了问题在一定条件下的NP完全性，同时针对特定参数（c=2, k=3）设计了多项式时间算法，从而对不同情景下问题的算法可解性进行了细致划分。

Result: （1）证明了在任意常数拥塞c≥1和终端对k≥3c-1时，定向分离路径问题为NP完全，否定了Giannopoulou等人关于常数k下拥塞为2时P可解的猜想；（2）而在未被该NP完全性覆盖的首个非平凡情形（c=2, k=3），证明了P可解，提供了多项式时间算法。

Conclusion: 本工作澄清了定向分离路径问题的复杂度分界：对于拥塞c≥1和足够多（k≥3c-1）的终端对问题为NP完全；而当c=2且k=3时，则可以多项式时间解决。此结果部分否定了先前文献的猜想，并厘清了多种情况下的复杂性。

Abstract: The classic result by Fortune, Hopcroft, and Wyllie [TCS~'80] states that the
directed disjoint paths problem is NP-complete even for two pairs of terminals.
Extending this well-known result, we show that the directed disjoint paths
problem is NP-complete for any constant congestion $c \geq 1$ and~$k \geq 3c-1$
pairs of terminals. This refutes a conjecture by Giannopoulou et al.
[SODA~'22], which says that the directed disjoint paths problem with congestion
two is polynomial-time solvable for any constant number $k$ of terminal pairs.
We then consider the cases that are not covered by this hardness result. The
first nontrivial case is $c=2$ and $k = 3$. Our second main result is to show
that this case is polynomial-time solvable.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [33] [Syntax Repair as Language Intersection](https://arxiv.org/abs/2507.11873)
*Breandan Considine*

Main category: cs.FL

TL;DR: 作者提出一种面向任意上下文无关语言的语法修复新方法，将修复问题转化为有限语言与目标语言的交集，理论上高效且在Python语法修复任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的语法错误修复技术在处理任意上下文无关语言时存在局限，且效率和正确性兼顾有难度。作者希望提升语法修复的适用性、效率和完备性。

Method: 将语法修复建模为语言交集问题：用一个有限语言描述所有在给定编辑距离内的语法修复方案。理论上关联Bar-Hillel构造和上下文无关语言可达性，形式化描述修复可在多对数并行时间内判定，并提出基于Brzozowski导数的枚举算法。

Result: 该方法及其实现经过评测，在Python语法修复基准上取得了业界领先的效果。

Conclusion: 提出的语法修复新方法不仅理论上高效可判定，而且在实际应用中超过了现有技术。

Abstract: We introduce a new technique for repairing syntax errors in arbitrary
context-free languages. This technique models syntax repair as a language
intersection problem by defining a finite language that provably generates
every syntactically valid repair within a given edit distance. Leveraging a
theoretical connection between the Bar-Hillel construction from formal language
theory and CFL reachability from program analysis, we show that repairability
in a finite number of typographic edits is polylogarithmic parallel time
decidable and provide an enumeration algorithm based on the Brzozowski
derivative. Finally, we evaluate this algorithm and its implementation,
demonstrating state-of-the-art results on a Python syntax repair benchmark.

</details>


### [34] [Hyper pattern matching](https://arxiv.org/abs/2507.12102)
*Masaki Waga,Étienne André*

Main category: cs.FL

TL;DR: 本文提出跨多个输入词的超模式匹配方法，支持检测更复杂的属性（如超属性），尽管问题本质上是NP完全的，但作者的工具HypPAu在多案例实验中展现出实际可用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的模式匹配主要检测单个词（word）中是否出现某个模式，但对于需要跨多个输入词或者同一词不同部分的属性（如超属性），传统模式匹配无法满足，需要更强的匹配能力。

Method: 作者将模式匹配推广为“超模式匹配”（hyper pattern matching），即在多个词的集合中查找模式。为此，提出了用非确定性异步有限自动机（NAA）作为形式表达，并给出一个朴素算法，同时设计了多种启发式策略以提升效率。

Result: 虽然作者证明该问题是NP完全的，但其实验实现HypPAu能够在长度、词的数量以及维度数可扩展的情况下处理多个案例，显示出该方法的实用性。

Conclusion: 超模式匹配方法能有效发现传统方法难以检测的性质（如鲁棒性、干扰/非干扰等），并在实际案例中表现出良好的扩展性和实用价值。

Abstract: In runtime verification, pattern matching, which searches for occurrences of
a specific pattern within a word, provides more information than a simple
violation detection of the monitored property, by locating concrete evidence of
the violation. However, witnessing violations of some properties, particularly
hyperproperties, requires evidence across multiple input words or different
parts of the same word, which goes beyond the scope of conventional pattern
matching. We propose here hyper pattern matching, a generalization of pattern
matching over a set of words. Properties of interest include robustness and
(non-)interference. As a formalism for patterns, we use nondeterministic
asynchronous finite automata (NAAs). We first provide a naive algorithm for
hyper pattern matching and then devise several heuristics for better
efficiency. Although we prove the NP-completeness of the problem, our
implementation HypPAu is able to address several case studies scalable in the
length, number of words (or logs) and number of dimensions, suggesting the
practical relevance of our approach.

</details>
