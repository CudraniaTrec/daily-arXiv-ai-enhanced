<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.DM](#cs.DM) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [One Weird Trick to Untie Landin's Knot](https://arxiv.org/abs/2507.21317)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 本研究指出导致非终止现象的根源在于不受限的环境类型量化而非高阶引用本身，表明可以无需复杂类型系统而安全地将高阶引用加入终止语言。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为，在终止语言中加入高阶引用后，会出现Landin's Knot模式，即通过引用函数可实现包括非终止在内的通用递归。本研究质疑了这种观点。

Method: 分析高阶引用对非终止现象的影响，通过将函数的环境显式化并使用不可约量化（impredicative quantification）隐藏环境类型，探讨Landin's Knot的本质。进而，通过闭包转换语言的构建与类型系统分析，考察量化限制下的系统行为。

Result: 发现Landin's Knot依赖的是对函数环境的不受限量化，而非简单的引用函数本身。理论上证明，通过限制对环境的量化，可以安全地将高阶引用加入终止语言而不会引入非终止现象，无需线性类型或禁止引用存储函数。

Conclusion: 本工作挑战了高阶引用必然引入非终止现象的传统观点，指出只要限制对环境的量化，高阶引用可安全用于原本终止的语言。这为简单类型系统下安全扩展语言特性提供了新思路。

Abstract: In this work, we explore Landin's Knot, which is understood as a pattern for
encoding general recursion, including non-termination, that is possible after
adding higher-order references to an otherwise terminating language. We observe
that this isn't always true -- higher-order references, by themselves, don't
lead to non-termination. The key insight is that Landin's Knot relies not
primarily on references storing functions, but on unrestricted quantification
over a function's environment. We show this through a closure converted
language, in which the function's environment is made explicit and hides the
type of the environment through impredicative quantification. Once references
are added, this impredicative quantification can be exploited to encode
recursion. We conjecture that by restricting the quantification over the
environment, higher-order references can be safely added to terminating
languages, without resorting to more complex type systems such as linearity,
and without restricting references from storing functions.

</details>


### [2] [Fixed-Point-Oriented Programming: A Concise and Elegant Paradigm](https://arxiv.org/abs/2507.21439)
*Yong Qi Foo,Brian Sze-Kai Cheong,Michael D. Adams*

Main category: cs.PL

TL;DR: 面向不动点编程（FPOP）通过高层抽象和声明式规范，显著简化了自指计算相关问题的实现，减少了代码复杂度，提高了开发效率和可维护性。


<details>
  <summary>Details</summary>
Motivation: 当前编程范式在实现自指计算（如图算法、静态分析、解析和分布式计算）时需要复杂且难以实现的工作队列算法，缺乏对固有不动点计算的直接支持，导致实现低效且容易出错。

Method: 提出了一种新的“面向不动点编程”（FPOP）范式，提供高级抽象，允许开发者以声明式规范编写程序，并通过结构化推理规则和用户导向优化实现高效执行。

Result: FPOP大幅简化了算法实现过程，提升了程序可维护性和快速原型开发的能力。例如，使用FPOP可以用两行可执行代码完成传统编程需十几行才能完成的图距离计算问题。

Conclusion: FPOP能够桥接理论和实际，实现简洁、可表达、易优化的问题描述，有助于推动进一步研究和推广。

Abstract: Fixed-Point-Oriented Programming (FPOP) is an emerging paradigm designed to
streamline the implementation of problems involving self-referential
computations. These include graph algorithms, static analysis, parsing, and
distributed computing-domains that traditionally require complex and
tricky-to-implement work-queue algorithms. Existing programming paradigms lack
direct support for these inherently fixed-point computations, leading to
inefficient and error-prone implementations.
  This white paper explores the potential of the FPOP paradigm, which offers a
high-level abstraction that enables concise and expressive problem
formulations. By leveraging structured inference rules and user-directed
optimizations, FPOP allows developers to write declarative specifications while
the compiler ensures efficient execution. It not only reduces implementation
complexity for programmers but also enhances adaptability, making it easier for
programmers to explore alternative solutions and optimizations without
modifying the core logic of their program.
  We demonstrate how FPOP simplifies algorithm implementation, improves
maintainability, and enables rapid prototyping by allowing problems to be
clearly and concisely expressed. For example, the graph distance problem can be
expressed in only two executable lines of code with FPOP, while it takes an
order of magnitude more code in other paradigms. By bridging the gap between
theoretical fixed-point formulations and practical implementations, we aim to
foster further research and adoption of this paradigm.

</details>


### [3] [Composable Effect Handling for Programming LLM-integrated Scripts](https://arxiv.org/abs/2507.22048)
*Di Wang*

Main category: cs.PL

TL;DR: 本文提出用可组合effect handling分离LLM脚本的流程逻辑与具体操作，实现了良好模块化且性能实现10倍加速，推广该编程范式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成脚本耦合性强，难以复用和扩展，且未充分利用并行化导致性能不足。希望在不损失模块化的前提下提升性能。

Method: 采用可组合的effect handling，将LLM调用、I/O和并发等影响性操作抽象为接口，并通过effect handlers进行具体实现和调度。

Result: 在Tree-of-Thoughts案例中，脚本性能提升达10倍，并且实现了脚本的高可模块化。

Conclusion: 通过将可组合的effect handling方法引入LLM脚本，可以实现更好的模块化，并在不牺牲性能优化机会的情况下显著提升运行速度。

Abstract: Implementing LLM-integrated scripts introduces challenges in modularity and
performance, as scripts are often coupled to specific LLM implementations and
fail to exploit parallelization opportunities. This paper proposes using
composable effect handling to separate workflow logic from effectful
operations, such as LLM calls, I/O, and concurrency, enabling modularity
without sacrificing the opportunity for performance optimization. By treating
these operations as abstract interfaces and discharging them via effect
handlers, this paper shows that scripts can achieve significant speedups (e.g.,
10$\times$ in a Tree-of-Thoughts case study) without compromising modularity.
This paper aims to promote composable effect handling as a programming style
for LLM scripting.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement](https://arxiv.org/abs/2507.21271)
*Zhaorui Yang,Yuxin Qiu,Haichao Zhu,Qian Zhang*

Main category: cs.SE

TL;DR: GRAphRef利用图结构和约束修复策略，有效生成高质量、结构合法的AI系统测试输入，在语义一致性和测试效率方面超越传统工具。


<details>
  <summary>Details</summary>
Motivation: 现代人工智能应用越来越多地处理高度结构化的数据（如3D网格和点云），但现有的fuzzing工具和输入生成器通常需要针对特定输入类型手工制作，且经常产生无效输入，导致效率低和泛化能力差。该研究动机是探索能否用统一的基于图的表示解决结构化领域中的测试输入生成和变异问题。

Method: 作者提出并开发了GRAphRef——一种基于图的测试输入生成框架。该方法将结构化输入映射为图，然后通过邻域相似性引导的变异以及基于约束的修复阶段，生成并修复无效输入。实验比较了GRAphRef与AFL、MeshAttack、Saffron及两个削弱版本的效果，主要评估结构有效性、语义保持和性能开销。

Result: 实验表明，GRAphRef在生成输入的结构有效性和语义保持方面优于其他方法。统计分析和延迟分解展示了其组件性能和整体优势。

Conclusion: 通过统一的基于图的表示和约束变异策略，GRAphRef能够高效地为结构化AI系统生成有效测试输入，为提升输入测试的通用性、效率和有效性提供了新方向。

Abstract: [Context] Modern AI applications increasingly process highly structured data,
such as 3D meshes and point clouds, where test input generation must preserve
both structural and semantic validity. However, existing fuzzing tools and
input generators are typically handcrafted for specific input types and often
generate invalid inputs that are subsequently discarded, leading to
inefficiency and poor generalizability. [Objective] This study investigates
whether test inputs for structured domains can be unified through a graph-based
representation, enabling general, reusable mutation strategies while enforcing
structural constraints. We will evaluate the effectiveness of this approach in
enhancing input validity and semantic preservation across eight AI systems.
[Method] We develop and evaluate GRAphRef, a graph-based test input generation
framework that supports constraint-based mutation and refinement. GRAphRef maps
structured inputs to graphs, applies neighbor-similarity-guided mutations, and
uses a constraint-refinement phase to repair invalid inputs. We will conduct a
confirmatory study across eight real-world mesh-processing AI systems,
comparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.
Evaluation metrics include structural validity, semantic preservation (via
prediction consistency), and performance overhead. Experimental data is derived
from ShapeNetCore mesh seeds and model outputs from systems like MeshCNN and
HodgeNet. Statistical analysis and component latency breakdowns will be used to
assess each hypothesis.

</details>


### [5] ["Maybe We Need Some More Examples:" Individual and Team Drivers of Developer GenAI Tool Use](https://arxiv.org/abs/2507.21280)
*Courtney Miller,Rudrajit Choudhuri,Mara Ulloa,Sankeerti Haniyur,Robert DeLine,Margaret-Anne Storey,Emerson Murphy-Hill,Christian Bird,Jenna L. Butler*

Main category: cs.SE

TL;DR: 开发者对生成式AI工具的采用存在显著差异，主要取决于看法、态度和应对挑战的方式。若仅追求生产力而忽视学习支持，反会适得其反，影响工具效果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在软件工程领域广泛可用，但开发者的使用率存在巨大差异，这种不均衡可能影响生产力目标、管理层预期和开发者角色的不确定性。

Method: 通过与27个团队的54名开发者进行配对访谈（每队一名高频用户和一名低频用户），分析他们使用生成式AI工具的不同之处。

Result: 开发者对工具的看法（协作者vs功能）、参与方式（实验性vs保守性）以及面对挑战时的应对策略（坚持适应vs迅速放弃）是使用差异的主要原因。

Conclusion: 组织普遍期望通过生成式AI工具快速提升生产力，但如果未能投入足够的学习支持，反而会形成“生产力压力悖论”，阻碍了这些工具应有的效益。

Abstract: Despite the widespread availability of generative AI tools in software
engineering, developer adoption remains uneven. This unevenness is problematic
because it hampers productivity efforts, frustrates management's expectations,
and creates uncertainty around the future roles of developers. Through paired
interviews with 54 developers across 27 teams -- one frequent and one
infrequent user per team -- we demonstrate that differences in usage result
primarily from how developers perceive the tool (as a collaborator vs.
feature), their engagement approach (experimental vs. conservative), and how
they respond when encountering challenges (with adaptive persistence vs. quick
abandonment). Our findings imply that widespread organizational expectations
for rapid productivity gains without sufficient investment in learning support
creates a "Productivity Pressure Paradox," undermining the very productivity
benefits that motivate adoption.

</details>


### [6] [Black-Box Bug-Amplification for Multithreaded Software](https://arxiv.org/abs/2507.21318)
*Yeshayahu Weiss,Gal Amram,Achiya Elyasaf,Eitan Farchi,Oded Margalit,Gera Weiss*

Main category: cs.SE

TL;DR: 利用回归模型引导测试输入，有效提高了并发系统中罕见bug的暴露概率，证明比随机采样高效，并提供了无须改动系统架构的实际测试框架。


<details>
  <summary>Details</summary>
Motivation: 并发系统中的bug往往难以复现，因为它们只在罕见条件下才会出现。测试人员经常遇到一些仅在特定输入下且概率极低时才会暴露的错误。本文旨在系统性地提高此类难以复现bug的显现概率。

Method: 将被测系统视为黑盒，通过多次试验运行训练预测模型，估算特定输入配置触发bug的概率。对比了几种基于模型的搜索技术与暴力随机采样基线。

Result: 在17个代表性并发bug数据库上评估，回归模型集成能在大多数场景下显著提高bug出现的概率，经常能比随机采样提升一个数量级。

Conclusion: 提出了一个将bug放大作为罕见事件回归问题的新颖方法，实证证明了模型引导搜索放大bug的有效性，并提出了一个实用、非侵入的测试框架，有助于曝光并发系统中的隐藏错误。

Abstract: Bugs, especially those in concurrent systems, are often hard to reproduce
because they manifest only under rare conditions. Testers frequently encounter
failures that occur only under specific inputs, even when occurring with low
probability. We propose an approach to systematically amplify the occurrence of
such elusive bugs. We treat the system under test as a black-box and use
repeated trial executions to train a predictive model that estimates the
probability of a given input configuration triggering a bug. We evaluate this
approach on a dataset of 17 representative concurrency bugs spanning diverse
categories. Several model-based search techniques are compared against a
brute-force random sampling baseline. Our results show that an ensemble of
regression models can significantly increase bug occurrence rates across nearly
all scenarios, often achieving an order-of-magnitude improvement over random
sampling. The contributions of this work include: (i) a novel formulation of
bug-amplification as a rare-event regression problem; (ii) an empirical
evaluation of multiple techniques for amplifying bug occurrence, demonstrating
the effectiveness of model-guided search; and (iii) a practical, non-invasive
testing framework that helps practitioners expose hidden concurrency faults
without altering the internal system architecture.

</details>


### [7] [Does Editing Improve Answer Quality on Stack Overflow? A Data-Driven Investigation](https://arxiv.org/abs/2507.21329)
*Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 本研究系统评价了Stack Overflow答案协作编辑的实际效果，发现虽然部分编辑提升了质量，但也有相当比例的编辑对代码相关性、可用性、安全和可读性造成负面影响，显示编辑结果的高度不确定性。


<details>
  <summary>Details</summary>
Motivation: Stack Overflow等技术问答平台的高质量答案对软件开发实践具有重要影响，而低质量答案会带来效率低下、bug、安全漏洞等问题。虽然协作编辑机制旨在提升内容质量，但以往研究未系统考察被接受的编辑是否真正提升了关键质量维度。

Method: 分析了94,994条至少有一次被接受编辑的Python相关答案，从语义相关性、代码可用性、代码复杂性、安全漏洞、代码优化和可读性六个维度评估编辑的实际影响。

Result: 编辑带来正负效应。例如，53.3%的编辑提升了问题与答案的匹配度，但38.1%则降低了相关性；9%的编辑修复了坏代码，而14.7%则使原本可用的代码损坏。32.3%的编辑增加了复杂性，20.5%引入了新的安全隐患。虽然51.0%进行了性能优化，但整体执行时间仍上升，49.7%降低了代码可读性。

Conclusion: 被接受的编辑结果并不一致，既有改善也有恶化，这提醒用户和版主在协作编辑时需注意其对可维护性、安全和效率的实际影响，并为未来编辑系统的改进提供方向。

Abstract: High-quality answers in technical Q&A platforms like Stack Overflow (SO) are
crucial as they directly influence software development practices. Poor-quality
answers can introduce inefficiencies, bugs, and security vulnerabilities, and
thus increase maintenance costs and technical debt in production software. To
improve content quality, SO allows collaborative editing, where users revise
answers to enhance clarity, correctness, and formatting. Several studies have
examined rejected edits and identified the causes of rejection. However, prior
research has not systematically assessed whether accepted edits enhance key
quality dimensions. While one study investigated the impact of edits on C/C++
vulnerabilities, broader quality aspects remain unexplored. In this study, we
analyze 94,994 Python-related answers that have at least one accepted edit to
determine whether edits improve (1) semantic relevance, (2) code usability, (3)
code complexity, (4) security vulnerabilities, (5) code optimization, and (6)
readability. Our findings show both positive and negative effects of edits.
While 53.3% of edits improve how well answers match questions, 38.1% make them
less relevant. Some previously broken code (9%) becomes executable, yet working
code (14.7%) turns non-parsable after edits. Many edits increase complexity
(32.3%), making code harder to maintain. Instead of fixing security issues,
20.5% of edits introduce additional issues. Even though 51.0% of edits optimize
performance, execution time still increases overall. Readability also suffers,
as 49.7% of edits make code harder to read. This study highlights the
inconsistencies in editing outcomes and provides insights into how edits impact
software maintainability, security, and efficiency that might caution users and
moderators and help future improvements in collaborative editing systems.

</details>


### [8] [MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration](https://arxiv.org/abs/2507.21382)
*Ruiyin Li,Yiran Zhang,Xiyu Zhou,Peng Liang,Weisong Sun,Jifeng Xuan,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出面向软件架构设计的多智能体自动化框架MAAD，实现了更优的架构设计与评估，并证实了GPT-4o等高级LLM的关键作用。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计阶段复杂且知识密集，需深厚领域知识和架构经验，但现有设计流程耗时、依赖专家、且敏捷开发下可选方案有限。尽管大语言模型（LLM）在软件工程任务中表现优秀，但在架构设计领域的探索还不充分，尤其面对复杂决策和多样知识需求。

Method: 提出自动化的MAAD（Multi-Agent Architecture Design）框架，利用多智能体系统（MAS）中的四个专职智能体（分析师、建模师、设计师、评估师）协作，解释需求、生成架构蓝图及质评报告，并通过案例研究与MetaGPT等最新方法对比测试，同时测试了不同LLM模型表现。

Result: MAAD能生成更全面的架构组件和有洞见、结构化的评估报告。实际架构师对11份需求实验证实其实用性。不同LLM中，GPT-4o在架构设计方面表现最佳。

Conclusion: MAAD框架通过多智能体结合多知识库和结构任务分工，实现架构设计的自动化和优化，提升了设计质量和实用性；选用合适的LLM进一步提升整体表现。

Abstract: Software architecture design is a critical, yet inherently complex and
knowledge-intensive phase of software development. It requires deep domain
expertise, development experience, architectural knowledge, careful trade-offs
among competing quality attributes, and the ability to adapt to evolving
requirements. Traditionally, this process is time-consuming and
labor-intensive, and relies heavily on architects, often resulting in limited
design alternatives, especially under the pressures of agile development. While
Large Language Model (LLM)-based agents have shown promising performance across
various SE tasks, their application to architecture design remains relatively
scarce and requires more exploration, particularly in light of diverse domain
knowledge and complex decision-making. To address the challenges, we proposed
MAAD (Multi-Agent Architecture Design), an automated framework that employs a
knowledge-driven Multi-Agent System (MAS) for architecture design. MAAD
orchestrates four specialized agents (i.e., Analyst, Modeler, Designer and
Evaluator) to collaboratively interpret requirements specifications and produce
architectural blueprints enriched with quality attributes-based evaluation
reports. We then evaluated MAAD through a case study and comparative
experiments against MetaGPT, a state-of-the-art MAS baseline. Our results show
that MAAD's superiority lies in generating comprehensive architectural
components and delivering insightful and structured architecture evaluation
reports. Feedback from industrial architects across 11 requirements
specifications further reinforces MAAD's practical usability. We finally
explored the performance of the MAAD framework with three LLMs (GPT-4o,
DeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance
in producing architecture design, emphasizing the importance of LLM selection
in MAS-driven architecture design.

</details>


### [9] [LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests](https://arxiv.org/abs/2507.21447)
*Zachariah Sollenberger,Rahul Patel,Saieda Ali Zada,Sunita Chandrasekaran*

Main category: cs.SE

TL;DR: 本文提出了生成式与判别式LLM结合的双LLM系统，通过实验证明其在编译器测试自动生成及验证方面具备可行性和有效性，对解决代码生成幻觉和可验证性问题提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在软件和测试开发中的应用不断增加，提升自动生成代码的可靠性和可验证性变得越来越重要，但现有工作缺乏全面且自动化的验证解决方案。作者关注于代码中的幻觉现象以及结果可解释性和信任性的问题。

Method: 提出了一种双LLM系统，包括生成式LLM和判别式LLM，并用以大规模自动生成和验证编译器测试。实验采用多种参数规模的LLMs，并利用十项详细指标进行评估。

Result: 结果表明，LLMs具备自动生成高质量编译器测试并实现自动验证的潜力。

Conclusion: 双LLM系统能在自动化测试生成和验证中有效缓解幻觉与信任等问题，LLMs未来有望大幅提升软件开发与测试的效率和质量。

Abstract: The usage of Large Language Models (LLMs) for software and test development
has continued to increase since LLMs were first introduced, but only recently
have the expectations of LLMs become more realistic. Verifying the correctness
of code generated by LLMs is key to improving their usefulness, but there have
been no comprehensive and fully autonomous solutions developed yet.
Hallucinations are a major concern when LLMs are applied blindly to problems
without taking the time and effort to verify their outputs, and an inability to
explain the logical reasoning of LLMs leads to issues with trusting their
results. To address these challenges while also aiming to effectively apply
LLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a
discriminative LLM) and experiments with the usage of LLMs for the generation
of a large volume of compiler tests. We experimented with a number of LLMs
possessing varying parameter counts and presented results using ten
carefully-chosen metrics that we describe in detail in our narrative. Through
our findings, it is evident that LLMs possess the promising potential to
generate quality compiler tests and verify them automatically.

</details>


### [10] [HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions](https://arxiv.org/abs/2507.21485)
*Jing Wang,Shang Liu,Yao Lu,Zhiyao Xie*

Main category: cs.SE

TL;DR: 本文针对HLS代码调试难题，提出了HLSDebugger方案，构建了大规模数据集，并实现了高效的bug定位和修复，性能显著优于现有LLM，推进了硬件自动化调试的发展。


<details>
  <summary>Details</summary>
Motivation: 高层次综合（HLS）能自动将高级描述转换为高效硬件实现，但HLS代码调试难度大、耗时高，尤其对新手或缺乏硬件知识的软件工程师更为困难。虽然大语言模型（LLM）自动化调试HLS代码前景广阔，但在应用中面临数据稀缺、硬件逻辑调试复杂、及多任务测试缺失三大难题。

Method: 提出HLSDebugger方案，构建并发布包含30万数据样本的大型HLS逻辑错误标注数据集。模型采用编码器-解码器结构，可以同时进行bug定位、类型预测及修复。

Result: HLSDebugger在bug定位和错误修复两方面均显著优于GPT-4等先进LLM，bug修复能力提升超过3倍。

Conclusion: HLSDebugger不仅提升了HLS代码自动化调试效果，也为自动化硬件设计调试方式做出了重要探索和进步。

Abstract: High-level synthesis (HLS) accelerates hardware design by enabling the
automatic translation of high-level descriptions into efficient hardware
implementations. However, debugging HLS code is a challenging and
labor-intensive task, especially for novice circuit designers or software
engineers without sufficient hardware domain knowledge. The recent emergence of
Large Language Models (LLMs) is promising in automating the HLS debugging
process. Despite the great potential, three key challenges persist when
applying LLMs to HLS logic debugging: 1) High-quality circuit data for training
LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in
hardware is inherently more complex than identifying software bugs with
existing golden test cases. 3) The absence of reliable test cases requires
multi-tasking solutions, performing both bug identification and correction.
complicates the multi-tasking required for effective HLS debugging. In this
work, we propose a customized solution named HLSDebugger to address the
challenges. HLSDebugger first generates and releases a large labeled dataset
with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts
an encoder-decoder structure, performing bug location identification, bug type
prediction, and bug correction with the same model. HLSDebugger significantly
outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x
in bug correction. It makes a substantial advancement in the exploration of
automated debugging of HLS code.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [Semantic Numeration Systems as Dynamical Systems](https://arxiv.org/abs/2507.21295)
*Alexander Yu. Chunikhin*

Main category: cs.LO

TL;DR: 该文提出将语义基数操作符和抽象对象建模为具有非线性控制的离散动力系统，并给出状态方程。配置矩阵整合了操作符、参数和网络拓扑，对系统建模起核心作用。


<details>
  <summary>Details</summary>
Motivation: 为理解语义计数系统理论的基础，并建立能够描述语义数量操作符作用下系统演化的数学模型。

Method: 介绍了关于语义计数系统（semantic numeration systems）的基本概念，提出将由基数语义操作符和相关抽象实体形成的基数抽象对象（CAO）视为具有非线性控制的线性离散动力系统，并推导了在理想可观测性假设下的状态方程（包括平稳与非平稳情况）。同时，构建了一个配置矩阵来综合表述操作符类型、参数和连接拓扑。

Result: 给出了CAO的状态方程，展示了配置矩阵在系统建模中的核心作用，说明该矩阵统一了操作符、参数和拓扑结构三方面的信息。

Conclusion: 本文为语义计数系统理论提供了系统建模的新视角，通过形式化状态方程和配置矩阵，有助于研究更复杂的语义符号系统动态。

Abstract: The foundational concepts of semantic numeration systems theory are briefly
outlined. The action of cardinal semantic operators unfolds over a set of
cardinal abstract entities belonging to the cardinal semantic multeity. The
cardinal abstract object (CAO) formed by them in a certain connectivity
topology is proposed to be considered as a linear discrete dynamical system
with nonlinear control. Under the assumption of ideal observability, the CAO
state equations are provided for both stationary and non-stationary cases. The
fundamental role of the configuration matrix, which combines information about
the types of cardinal semantic operators in the CAO, their parameters and
topology of connectivity, is demonstrated.

</details>


### [12] [A Tree-Shaped Tableau for Checking the Satisfiability of Signal Temporal Logic with Bounded Temporal Operators](https://arxiv.org/abs/2507.21598)
*Beatrice Melani,Ezio Bartocci,Michele Chiari*

Main category: cs.LO

TL;DR: 本文提出一种高效的新型表方法用于离散时间STL可满足性检测，实验显示在诸多测试中优于SMT和FOL等传统方法，且具有广泛应用价值。


<details>
  <summary>Details</summary>
Motivation: 在设计和验证智能物理系统（CPS）时，需要用信号时序逻辑（STL）来表达和检查系统的时序需求，尤其必须高效、自动地判定多条STL需求是否相互一致。解决这一需求的关键在于STL可满足性检测。

Method: 提出了一种新颖的基于树结构、一次遍历的表方法，用于离散时间、有界时序运算符下的STL可满足性检测。该方法利用公式中大时间区间带来的冗余来提升检测效率。

Result: 作者将所提出的表方法和当前文献中的SMT和一阶逻辑编码方法进行了基准测试，结果在很多案例中，新方法的性能优于现有的主流方法。

Conclusion: 此新表方法不仅适用于STL一致性检测，还可用于构造满足要求的信号示例、验证公式等价性或蕴含关系，以及进行MLTL的可满足性检测，在实际和工业应用上具有广阔前景。

Abstract: Signal Temporal Logic (STL) is a widely recognized formal specification
language to express rigorous temporal requirements on mixed analog signals
produced by cyber-physical systems (CPS). A relevant problem in CPS design is
how to efficiently and automatically check whether a set of STL requirements is
logically consistent. This problem reduces to solving the STL satisfiability
problem, which is decidable when we assume that our system operates in discrete
time steps dictated by an embedded system's clock.
  This paper introduces a novel tree-shaped, one-pass tableau method for
satisfiability checking of discrete-time STL with bounded temporal operators.
Originally designed to prove the consistency of a given set of STL
requirements, this method has a wide range of applications beyond consistency
checking. These include synthesizing example signals that satisfy the given
requirements, as well as verifying or refuting the equivalence and implications
of STL formulas.
  Our tableau exploits redundancy arising from large time intervals in STL
formulas to speed up satisfiability checking, and can also be employed to check
Mission-Time Linear Temporal Logic (MLTL) satisfiability. We compare our
tableau with Satisfiability Modulo Theories (SMT) and First-Order Logic
encodings from the literature on a benchmark suite, partly collected from the
literature, and partly provided by an industrial partner. Our experiments show
that, in many cases, our tableau outperforms state-of-the-art encodings.

</details>


### [13] [The Shape of $\mathcal{EL}$ Proofs: A Tale of Three Calculi (Extended Version)](https://arxiv.org/abs/2507.21851)
*Christian Alrabbaa,Stefan Borgwardt,Philipp Herrmann,Markus Krötzsch*

Main category: cs.LO

TL;DR: 论文比较了三种EL描述逻辑推理演算，通过将它们转换为存在性规则并用NEMO推理引擎执行，分析了其在DL证明解释性和复杂性方面的差异，为推理可解释性与工具选型提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 该论文关注描述逻辑（DL）本体中的蕴含推理解释，旨在分析和比较EL族中的多种基于结果的演算方法及其可解释性。针对DL推理中构造易于理解的证明具有重要意义。

Method: 作者研究了三种基于结果的EL族演算，将这些演算方法转换为带有分层否定的存在性规则。然后，通过规则引擎NEMO来执行这些规则，获取推理的执行轨迹，并将轨迹转回DL证明，最后按照多种复杂性相关指标进行比较。

Result: 通过实验对比三种推理演算在标准基准测试（基于OWL Reasoner Evaluation）下所产生的证明，评估了它们在不同复杂性方面的表现。

Conclusion: 三种演算方法产生了结构和复杂性各异的证明，这为后续选择更可解释或效率更高的证明方法提供了参考。此外，存在性规则和NEMO推理引擎的结合被证实可有效支持DL证明生成和分析。

Abstract: Consequence-based reasoning can be used to construct proofs that explain
entailments of description logic (DL) ontologies. In the literature, one can
find multiple consequence-based calculi for reasoning in the $\mathcal{EL}$
family of DLs, each of which gives rise to proofs of different shapes. Here, we
study three such calculi and the proofs they produce on a benchmark based on
the OWL Reasoner Evaluation. The calculi are implemented using a translation
into existential rules with stratified negation, which had already been
demonstrated to be effective for the calculus of the ELK reasoner. We then use
the rule engine NEMO to evaluate the rules and obtain traces of the rule
execution. After translating these traces back into DL proofs, we compare them
on several metrics that reflect different aspects of their complexity.

</details>


### [14] [Why not? Developing ABox Abduction beyond Repairs](https://arxiv.org/abs/2507.21955)
*Anselm Haak,Patrick Koopmann,Yasir Mahmood,Anni-Yasmin Turhan*

Main category: cs.LO

TL;DR: 本论文对含不一致数据的知识库下的溯因推理做了初步理论扩展，定义了修复语义下的溯因及最小性准则，并对存在性与验证问题进行了复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 在传统完美数据和经典语义下，失效推理的溯因(推理缺失结论的解释或假设)已被广泛研究，但对于含有错误数据导致不一致知识库的情况，关于溯因的研究较少。作者希望填补该领域的空白，提出适用于不一致数据情境下的溯因理论和方法。

Method: 作者定义了基于修复语义(repair semantics)的溯因新形式，提出了一系列最小性准则，用于引导溯因生成更‘有用’的假设。研究涵盖了不同修复语义，主要聚焦于描述逻辑DL-Lite和EL_bot中的溯因复杂度问题。

Result: 论文给出了在不同修复语义和两种主要描述逻辑(DL-Lite和EL_bot)下，带有最小性准则的溯因解的存在性判定和检验的初步复杂性结果。

Conclusion: 该研究扩展了溯因推理在知识库数据异常(不一致)下的理论范畴，并为修复语义和实际溯因分析提供了初步的复杂性分析，为进一步研究提供理论支持。

Abstract: Abduction is the task of computing a sufficient extension of a knowledge base
(KB) that entails a conclusion not entailed by the original KB. It serves to
compute explanations, or hypotheses, for such missing entailments. While this
task has been intensively investigated for perfect data and under classical
semantics, less is known about abduction when erroneous data results in
inconsistent KBs. In this paper we define a suitable notion of abduction under
repair semantics, and propose a set of minimality criteria that guides
abduction towards `useful' hypotheses. We provide initial complexity results on
deciding existence of and verifying abductive solutions with these criteria,
under different repair semantics and for the description logics DL-Lite and
EL_bot.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
*Kerem Keskin,Mümine Kaya Keleş*

Main category: cs.CL

TL;DR: 本研究比较了不同词嵌入和机器学习方法在土耳其语图书文本分类上的表现，结果显示TF-IDF和一热编码结合SVM、贝叶斯和逻辑回归效果最佳。


<details>
  <summary>Details</summary>
Motivation: 目前书籍网站上有大量图书摘要和类别信息，如何有效对这些文本进行分类，以及比较不同词嵌入方法和机器学习算法的分类表现，有助于提升自然语言处理应用特别是在土耳其语文本处理中的效果。

Method: 本研究应用了一热编码（One-Hot Encoding）、Word2Vec、TF-IDF三种常见的词嵌入方法，并结合机器学习算法（支持向量机、朴素贝叶斯、逻辑回归）对图书摘要和类别进行分类。同时展示并比较了不同文本预处理方法的组合和其对应结果。

Result: 实验结果显示，支持向量机、朴素贝叶斯和逻辑回归模型结合TF-IDF和一热编码方法在土耳其语文本分类任务中取得了最佳效果。

Conclusion: 对于以土耳其语为主的书籍文本摘要和类别分类任务，传统的特征提取方法（如TF-IDF和一热编码）配合经典机器学习模型（SVM、朴素贝叶斯、逻辑回归）仍具备较强的实用性与优越性。

Abstract: In this study, book summaries and categories taken from book sites were
classified using word embedding methods, natural language processing techniques
and machine learning algorithms. In addition, one hot encoding, Word2Vec and
Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are
frequently used word embedding methods were used in this study and their
success was compared. Additionally, the combination table of the pre-processing
methods used is shown and added to the table. Looking at the results, it was
observed that Support Vector Machine, Naive Bayes and Logistic Regression
Models and TF-IDF and One-Hot Encoder word embedding techniques gave more
successful results for Turkish texts.

</details>


### [16] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 论文提出以社会化对话为核心的AI学习方式“AI Social Gym”，研究不同教学策略对LLM知识获取的影响。结果显示，教师引导与学习者提问结合的对话策略，显著提升了模型获取复杂知识的能力，相较传统方法有明显优势，表明心理学教学理论应用于AI训练具有巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽然擅长处理离线大规模数据，但在在线高效获取和整合复杂知识方面存在局限。传统基于监督学习或强化学习的训练方法反馈稀疏，难以模拟人类社会化学习过程。因此，作者受到维果茨基社会文化理论启发，希望借助社会性学习范式提升AI学习效率。

Method: 作者提出了一个名为“AI Social Gym”的动态环境，让AI学习者与AI教师进行双人教学对话，采用结构化的社交对话作为知识获取的核心机制，并重点考察不同教学策略对本体知识获取的影响。

Result: 实验证明，特别是那些融合教师讲解与学习者主动提问的混合交互式对话策略，大幅提升了LLM获取和应用新知识的能力，优于单向教学法和直接提供结构化知识的方法。

Conclusion: 将教学法和心理学理念融入AI模型训练，能够显著提升训练后知识获取和应答质量，为优化大模型提供新的方法路径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [17] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
*David James Woo,Yangyang Yu,Kai Guo,Yilin Huang,April Ka Yeng Fung*

Main category: cs.CL

TL;DR: 本研究发现，香港中学生在编辑AI生成说明文时，编辑行为繁多但对写作质量提升作用有限，AI可辅助但不能替代写作核心能力。教学应重视体裁、写作过程与AI文本的批判性处理。


<details>
  <summary>Details</summary>
Motivation: 人工智能(AI)聊天机器人生成的文本在非英语母语(EFL)写作中的应用日益增多，但其对学生说明文写作过程和成品的影响尚未深入研究。该论文旨在探讨二语高中生在编辑AI生成文本时的行为与写作质量之间的关系。

Method: 研究对象为39名香港中学生，在写作工作坊中结合AI聊天机器人进行说明文写作。作者采用融合设计，分析了学生的屏幕录制和写作成品，方法包括质性编码、描述性统计、时序分析、人工评分和多元线性回归分析。数据样本包含260项编辑行为。

Result: 研究识别出两种主要编辑模式：一种是反复精修引言部分再继续写作，另一种是快速转向正文部分进行大量编辑。多元回归显示，AI生成文本数量对所有评分维度有正向预测，但大部分编辑变量对写作得分影响极小，显示出大量编辑并未带来明显的写作质量提升。

Conclusion: 学生对AI文本投入了大量编辑努力，但对写作质量提升有限，反映AI虽有辅助作用，却无法取代写作 core skills。建议关注体裁、过程教学并优化评估方式，促进学生对AI文本的批判性利用。

Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used
in English as a foreign language (EFL) writing contexts, yet its impact on
students' expository writing process and compositions remains understudied.
This research examines how EFL secondary students edit AI-generated text.
Exploring editing behaviors in their expository writing process and in
expository compositions, and their effect on human-rated scores for content,
organization, language, and overall quality. Participants were 39 Hong Kong
secondary students who wrote an expository composition with AI chatbots in a
workshop. A convergent design was employed to analyze their screen recordings
and compositions to examine students' editing behaviors and writing qualities.
Analytical methods included qualitative coding, descriptive statistics,
temporal sequence analysis, human-rated scoring, and multiple linear regression
analysis. We analyzed over 260 edits per dataset, and identified two editing
patterns: one where students refined introductory units repeatedly before
progressing, and another where they quickly shifted to extensive edits in body
units (e.g., topic and supporting sentences). MLR analyses revealed that the
number of AI-generated words positively predicted all score dimensions, while
most editing variables showed minimal impact. These results suggest a
disconnect between students' significant editing effort and improved
composition quality, indicating AI supports but does not replace writing
skills. The findings highlight the importance of genre-specific instruction and
process-focused writing before AI integration. Educators should also develop
assessments valuing both process and product to encourage critical engagement
with AI text.

</details>


### [18] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: 作者批判了现有语义基础问题的解决思路，认为“零语义承诺”无法实现，并提出应把关注点放在人工系统中意义行为的实现与功能解释上。


<details>
  <summary>Details</summary>
Motivation: Floridi和Taddeo提出了“零语义承诺”作为语义基础问题的解法前提，但作者认为这一条件存疑，因此希望重新审视该问题的定义和核心难点，并思考系统目标在其中的作用。

Method: 作者主要采用批判性理论分析的方法，评论并反驳现有方案，并结合对系统目标的重新思考与对计算本质的讨论，提出个人见解。

Result: 否定了“零语义承诺”条件的可实现性，经过对比不同方案，提出应关注于计算系统中意义的行为能力和功能的解释与再现。

Conclusion: 作者认为“语义承诺为零”条件无法实现，包括Floridi和Taddeo自己的方案也不能满足。作者提出唯一有意义的语义基础问题，是如何在人工计算代理中解释和复现语义的行为能力和功能。

Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for
solutions to the grounding problem, and a solution to it. I argue briefly that
their condition cannot be fulfilled, not even by their own solution. After a
look at Luc Steels' very different competing suggestion, I suggest that we need
to re-think what the problem is and what role the 'goals' in a system play in
formulating the problem. On the basis of a proper understanding of computing, I
come to the conclusion that the only sensible grounding problem is how we can
explain and re-produce the behavioral ability and function of meaning in
artificial computational agents

</details>


### [19] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
*Franck Bardol*

Main category: cs.CL

TL;DR: 这项研究发现GPT-4会基于问题的情感语气调整回复，倾向于中和负面语气，尤其在敏感话题上表现出更强的中性化和一致性，对AI偏差和对齐有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（如GPT-4）如何根据问题的情感语气调整回复尚未被系统性地研究，尤其是在引发争议或敏感话题上。理解模型在不同情感语气下的表现对于AI信任与对齐至关重要。

Method: 作者系统性地更改了156个涵盖争议和日常话题的提示（prompts）的情感语气，并分析了GPT-4对这些不同语气提示的回应。作者引入了“情感语气下限（tone floor）”概念，并用tone-valence转换矩阵以及1536维嵌入可视化方法对模型行为和语义漂移进行量化。

Result: GPT-4对负面语气的问题回复负面内容的概率比对中性问题低三倍，并表现出由情感语气导致的语义漂移现象。对于正义、政治等敏感话题，模型会主动抑制语气带来的变化，趋向中性化。引入的量化和可视化方法有助于揭示模型对情感语气的敏感性和潜在偏差。

Conclusion: GPT-4在面对带有负面语气的问题时，反而比面对中性问题时更少给出负面回应，呈现出一种“反弹”偏差现象，在涉及敏感话题时，这种情感语气的影响被进一步压制，这表明模型有“对齐覆盖（alignment override）”现象。

Abstract: Large Language Models like GPT-4 adjust their responses not only based on the
question asked, but also on how it is emotionally phrased. We systematically
vary the emotional tone of 156 prompts - spanning controversial and everyday
topics - and analyze how it affects model responses. Our findings show that
GPT-4 is three times less likely to respond negatively to a negatively framed
question than to a neutral one. This suggests a "rebound" bias where the model
overcorrects, often shifting toward neutrality or positivity. On sensitive
topics (e.g., justice or politics), this effect is even more pronounced:
tone-based variation is suppressed, suggesting an alignment override. We
introduce concepts like the "tone floor" - a lower bound in response negativity
- and use tone-valence transition matrices to quantify behavior. Visualizations
based on 1536-dimensional embeddings confirm semantic drift based on tone. Our
work highlights an underexplored class of biases driven by emotional framing in
prompts, with implications for AI alignment and trust. Code and data are
available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [20] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
*Aly M. Kassem,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 论文提出MNEME框架，通过稀疏模型对比，在无任务数据下自动检测LLM微调引发的副作用，精度高、无需定制启发，能有效帮助模型行为管控。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法只能检测大型语言模型（LLM）微调或“遗忘”后的性能变化，难以发现不可预测的副作用，如去除某一领域知识导致其它领域能力下降。因此，需要能自动检测此类副作用的方法。

Method: 提出MNEME框架：利用稀疏模型对比（model diffing），在与任务无关的数据上（如The Pile、LMSYS-Chat-1M）比较基础模型与微调后模型，无需获得微调数据，仅通过行为变化定位副作用。

Result: MNEME在五个LLM、三种场景下评估（知识遗忘、突现失调、良性微调），能以最高95%的准确率预测副作用，验证了其与既有基准的一致性。还发现对高激活样本再训练可部分逆转副作用。

Conclusion: MNEME框架利用稀疏探查与对比，能自动、规模化检测LLM微调带来的副作用，为理解和管理大模型行为提供有效工具。

Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt
to new tasks or eliminate undesirable behaviors. While existing evaluation
methods assess performance after such interventions, there remains no general
approach for detecting unintended side effects, such as unlearning biology
content degrading performance on chemistry tasks, particularly when these
effects are unpredictable or emergent. To address this issue, we introduce
MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight
framework for identifying these side effects using sparse model diffing. MNEME
compares base and fine-tuned models on task-agnostic data (for example, The
Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral
shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,
emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent
accuracy in predicting side effects, aligning with known benchmarks and
requiring no custom heuristics. Furthermore, we show that retraining on
high-activation samples can partially reverse these effects. Our results
demonstrate that sparse probing and diffing offer a scalable and automated lens
into fine-tuning-induced model changes, providing practical tools for
understanding and managing LLM behavior.

</details>


### [21] [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

TL;DR: 该文提出多业余对比解码（MACD），用多个‘小白’模型改进文本生成质量，并支持风格可控，实验效果显著优于现有方法，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 对比解码（CD）方法通过利用大、小语言模型之间的输出差异，可以提升文本生成的连贯性与流畅性。但现有做法只用单一“小白”模型，难以覆盖生成时可能出现的多种失败模式，如重复、幻觉、风格漂移等。

Method: 提出了多业余对比解码（MACD）方法，引入了多个“小白”模型作为业余者，以集成的方式更全面地识别和约束生成中的不良模式。MACD 结合了平均和共识惩罚机制，对不良生成信号进行处理，并扩展了合理性约束以适配多模型场景；此外，可通过特定风格或内容偏好的业余模型实现可控生成。

Result: 实验证明，MACD 在新闻、百科、叙事等多个领域的文本生成任务中，相较于传统解码方法和原始CD方法，在流畅性、连贯性、多样性和适应性等方面均取得更优效果，并且无需额外训练或微调。

Conclusion: MACD 通过联用多个业余模型，全面刻画文本生成中的多样错误模式，实现更好品质和可控的文本输出，为对比解码策略提供了实用且可扩展的进步。

Abstract: Contrastive Decoding (CD) has emerged as an effective inference-time strategy
for enhancing open-ended text generation by exploiting the divergence in output
probabilities between a large expert language model and a smaller amateur
model. Although CD improves coherence and fluency, its dependence on a single
amateur restricts its capacity to capture the diverse and multifaceted failure
modes of language generation, such as repetition, hallucination, and stylistic
drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a
generalization of the CD framework that employs an ensemble of amateur models
to more comprehensively characterize undesirable generation patterns. MACD
integrates contrastive signals through both averaging and consensus
penalization mechanisms and extends the plausibility constraint to operate
effectively in the multi-amateur setting. Furthermore, the framework enables
controllable generation by incorporating amateurs with targeted stylistic or
content biases. Experimental results across multiple domains, such as news,
encyclopedic, and narrative, demonstrate that MACD consistently surpasses
conventional decoding methods and the original CD approach in terms of fluency,
coherence, diversity, and adaptability, all without requiring additional
training or fine-tuning.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [22] [Pathwidth of 2-Layer $k$-Planar Graphs](https://arxiv.org/abs/2507.21864)
*Yuto Okada*

Main category: cs.DM

TL;DR: 本文证明了 2-layer $k$-planar 图的 pathwidth 上下界都为 $k+1$，构造了达到该 pathwidth 的图，从而完善了相关理论。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明，2-layer $k$-planar 图的 pathwidth 上界为 $k+1$，但关于下界并不紧，尚未有人证明是否可以达到此上界。作者旨在确定 2-layer $k$-planar 图的 pathwidth 上下界是否一致，从而完善理论。

Method: 作者构造了一类特殊的 2-layer $k$-planar 图，这些图满足每条边至多有 $k$ 个交叉，并且证明了这些图的 pathwidth 恰为 $k+1$。通过这种构造方法，作者实现了 tight bound。

Result: 作者成功构造出了 pathwidth 等于 $k+1$ 的 2-layer $k$-planar 图，证明了此前理论上界的紧致性，并将下界提升至 $k+1$，高于之前 $(k+3)/2$ 的下界。

Conclusion: 对于任意 $k \geq 0$，2-layer $k$-planar 图的 pathwidth 的 tight bound 为 $k+1$，之前的上界是 tight 的，改进了下界，理论结果得到了完善。

Abstract: A bipartite graph $G = (X \cup Y, E)$ is a 2-layer $k$-planar graph if it
admits a drawing on the plane such that the vertices in $X$ and $Y$ are placed
on two parallel lines respectively, edges are drawn as straight-line segments,
and every edge involves at most $k$ crossings. Angelini, Da Lozzo, F\"orster,
and Schneck [GD 2020; Comput. J., 2024] showed that every 2-layer $k$-planar
graph has pathwidth at most $k + 1$. In this paper, we show that this bound is
sharp by giving a 2-layer $k$-planar graph with pathwidth $k + 1$ for every $k
\geq 0$. This improves their lower bound of $(k+3)/2$.

</details>


### [23] [Perfect Graph Modification Problems: An Integer Programming Approach](https://arxiv.org/abs/2507.21987)
*Burak Nur Erdem,Tınaz Ekim,Zeki Caner Taşkın*

Main category: cs.DM

TL;DR: 作者基于整数规划，提出了针对三类NP难完美图修改问题的精确与启发式算法，创新地处理了奇圈与奇反圈约束，并通过实证有效性验证，填补了此类问题缺乏精确解法的空白。


<details>
  <summary>Details</summary>
Motivation: 图修改问题旨在通过对图进行少量修改，使其满足某种特性。尽管关于这些问题的NP完备性和多项式时间可解性的研究已较丰富，但针对NP困难情形的精确求解算法却鲜有探索。本文希望提出精确算法应对这些复杂场景。

Method: 本文针对三类完美图修改问题（最小完美编辑、最小完美补全和完美三明治问题）提出了基于整数规划的精确解法。主要方法包括利用强完美图定理，将奇圈与奇反圈转化为线性不等式，建立整数规划模型。为应对约束数量指数级增长，设计了基于剪枝平面（cutting plane）的算法，并通过研究随机图中奇圈与奇反圈的期望数量提升算法效率。此外，还提出了启发式算法用于生成较好的上界。

Result: 所提方法通过计算实验显示了良好的实证效果。剪枝平面算法和启发式策略均有效提升了相关问题的计算性能及解质量。

Conclusion: 本文为复杂的完美图修改问题提供了一套系统的整数规划与算法解决框架，不仅弥补了现有文献中精确算法匮乏的不足，也推动了该领域在理论及应用层面的进一步发展。

Abstract: Graph modification problems, which aim to find a small set of modifications
to a graph so that it satisfies a desired property, have been studied for
several special graph classes. The literature is rather rich in NP-completeness
results and polynomial time solvable cases. However, to the best of our
knowledge, only a few exact algorithms have been suggested to address NP-hard
cases. In this work, we propose exact solution methods based on integer
programming for three perfect graph modification problems: minimum perfect
editing, minimum perfect completion and the perfect sandwich problem. The
minimum perfect editing problem inquires the smallest number of edge additions
and deletions to make a graph perfect, while the completion problem allows only
edge additions. In the perfect sandwich problem, only a given subset of
non-edges can be changed to edges, and the problem asks whether a perfect graph
can be obtained in this way. The proposed methods are based on the Strong
Perfect Graph Theorem. We represent odd holes and odd antiholes as linear
inequalities, and formulate an integer programming model to solve minimum
perfect editing problem. To address the exponential number of constraints, we
propose a cutting plane algorithm which relies on finding odd holes and odd
antiholes. To enhance the practical efficiency of the cutting plane algorithm,
we address the expected number of odd holes and odd antiholes in random graphs.
In addition, we propose a heuristic algorithm to make a given graph perfect,
which is used to obtain improved upper bounds for the editing and the
completion problems. Finally, we demonstrate empirical effectiveness of the
proposed methods through computational experiments.

</details>
