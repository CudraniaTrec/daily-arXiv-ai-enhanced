<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
*Yifan Zong,Yuntian Deng,Pengyu Nie*

Main category: cs.PL

TL;DR: MoLE通过混合专家机制，以较低成本兼顾多语言代码生成效率与专门化，优于传统单/多模型微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在多编程语言任务上，单模型微调效率高但牺牲了语言专门化，多模型微调专门化强但成本高、参数冗余。如何兼顾效率与专门化成为挑战。

Method: 提出了MoLE（Mix-of-Language-Experts）新架构，由基础模型、共享LoRA模块及多个语言专属LoRA模块组成，采用联合优化微调方式，推理时根据目标编程语言自动选择对应的专属模块。

Result: MoLE比为每类语言分别训练LoRA更节省参数，同时在多语言代码任务上精度优于仅有单一微调LLM。

Conclusion: MoLE实现了效率与专门化之间的平衡，在多编程语言场景下具备更好的参数效率和表现。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
aiding developers with tasks like code comprehension, generation, and
translation. Supporting multilingual programming -- i.e., coding tasks across
multiple programming languages -- typically requires either (1) finetuning a
single LLM across all programming languages, which is cost-efficient but
sacrifices language-specific specialization and performance, or (2) finetuning
separate LLMs for each programming language, which allows for specialization
but is computationally expensive and storage-intensive due to the duplication
of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel
architecture that balances efficiency and specialization for multilingual
programming. MoLE is composed of a base model, a shared LoRA (low-rank
adaptation) module, and a collection of language-specific LoRA modules. These
modules are jointly optimized during the finetuning process, enabling effective
knowledge sharing and specialization across programming languages. During
inference, MoLE automatically routes to the language-specific LoRA module
corresponding to the programming language of the code token being generated.
Our experiments demonstrate that MoLE achieves greater parameter efficiency
compared to training separate language-specific LoRAs, while outperforming a
single shared LLM finetuned for all programming languages in terms of accuracy.

</details>


### [2] [The Autonomous Data Language -- Concepts, Design and Formal Verification](https://arxiv.org/abs/2506.19457)
*Tom T. P. Franken,Thomas Neele,Jan Friso Groote*

Main category: cs.PL

TL;DR: 本文提出了一种以自治数据为中心的全新并行编程范式，并实现了首个相关语言AuDaLa，兼具自然易用和形式化验证能力，为并行编程带来了创新途径。


<details>
  <summary>Details</summary>
Motivation: 目前的并行语言主要关注处理器和线程，导致数据和内存处理困难，使实现与原始算法脱节。作者提出新的并行编程范式以解决这一问题。

Method: 提出了data-autonomous（数据自治型）并行编程范式，用自治数据元素驱动计算，并首次推出AuDaLa语言，还给出了包括类型系统和操作语义的完整形式化描述。通过示例说明AuDaLa的编程方式和并利用其形式化特性实现了并行程序的形式化验证。

Result: 证实了AuDaLa语言自然易用，并适合进行并行程序的形式化验证，展示了与传统顺序及并行编程截然不同的编程风格。

Conclusion: 数据自治型编程范式及其实现的AuDaLa语言，能使并行程序设计更接近算法本意，同时便于形式化验证，提升了并行编程的表达力和可靠性。

Abstract: Nowadays, the main advances in computational power are due to parallelism.
However, most parallel languages have been designed with a focus on processors
and threads. This makes dealing with data and memory in programs hard, which
distances the implementation from its original algorithm. We propose a new
paradigm for parallel programming, the data-autonomous paradigm, where
computation is performed by autonomous data elements. Programs in this paradigm
are focused on making the data collaborate in a highly parallel fashion. We
furthermore present AuDaLa, the first data autonomous programming language, and
provide a full formalisation that includes a type system and operational
semantics. Programming in AuDaLa is very natural, as illustrated by examples,
albeit in a style very different from sequential and contemporary parallel
programming. Additionally, it lends itself for the formal verification of
parallel programs, which we demonstrate.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation](https://arxiv.org/abs/2506.19045)
*Ahmadreza Saboor Yaraghi,Golnaz Gharachorlu,Sakina Fatima,Lionel C. Briand,Ruiyuan Wan,Ruifeng Gao*

Main category: cs.SE

TL;DR: 提出无执行的LLM驱动系统测试代码故障定位方法，用一次失败日志推断相关代码并优先排序，无需SUT源码，具高准确率和效率，对工业级大规模复杂测试用例适用。


<details>
  <summary>Details</summary>
Motivation: 传统的故障定位方法需要多次执行程序，这在面临非确定性失败或高执行成本时不切实际。同时，现有基于大语言模型的无执行故障定位多专注于被测系统（SUT），而忽视测试代码本身的故障，但实际上很多故障由测试代码引发。因此，亟需一种不依赖执行、能定位测试代码故障的方法。

Method: 提出了一种完全静态、由大语言模型驱动的系统测试代码故障定位（TCFL）方法，无需执行测试用例。该方法利用一次失败执行日志，结合三种算法，估算测试的静态执行轨迹，筛选与失败相关的代码语句。将精简轨迹和错误信息一同输入LLM，进行故障位置排序。此方法不需SUT源码，适用大型测试脚本。

Result: 在一个未被LLM预训练工业数据集上评估，最优估算轨迹与真实轨迹F1得分约90%；精简大规模测试代码让LLM推理时间减少34%，无精度损失。块级TCFL取得最佳平衡，top-3命中率达81%。

Conclusion: 该方法实现了高效且可靠的系统测试代码静态故障定位，不需要执行测试或访问SUT源码，提升了大型复杂系统下的调试效率和实用性。

Abstract: Fault localization (FL) is a critical step in debugging which typically
relies on repeated executions to pinpoint faulty code regions. However,
repeated executions can be impractical in the presence of non-deterministic
failures or high execution costs. While recent efforts have leveraged Large
Language Models (LLMs) to aid execution-free FL, these have primarily focused
on identifying faults in the system under test (SUT) rather than in the often
complex system test code. However, the latter is also important as, in
practice, many failures are triggered by faulty test code. To overcome these
challenges, we introduce a fully static, LLM-driven approach for system test
code fault localization (TCFL) that does not require executing the test case.
Our method uses a single failure execution log to estimate the test's execution
trace through three novel algorithms that identify only code statements likely
involved in the failure. This pruned trace, combined with the error message, is
used to prompt the LLM to rank potential faulty locations. Our black-box,
system-level approach requires no access to the SUT source code and is
applicable to large test scripts that assess full system behavior. We evaluate
our technique at function, block, and line levels using an industrial dataset
of faulty test cases not previously used in pre-training LLMs. Results show
that our best estimated trace closely match actual traces, with an F1 score of
around 90%. Additionally, pruning the complex system test code reduces the
LLM's inference time by up to 34% without any loss in FL performance. Our
results further suggest that block-level TCFL offers a practical balance,
narrowing the search space while preserving useful context, achieving an 81%
hit rate at top-3 (Hit@3).

</details>


### [4] [Dataset of Yul Contracts to Support Solidity Compiler Research](https://arxiv.org/abs/2506.19153)
*Krzysztof Fonal*

Main category: cs.SE

TL;DR: 提出并发布了首个涵盖348,840个真实以太坊主网Yul智能合约实例的数据集YulCode，为底层合约研究和分析提供了开放标准和数据基础。


<details>
  <summary>Details</summary>
Motivation: 当前以太坊智能合约研究大量依赖Solidity源码，而面向底层EVM的开源Yul语言合约数据集极为稀缺，限制了底层代码分析、验证和优化等方向的深入研究。本文提出数据集以填补该领域空白。

Method: 通过收集并编译已在以太坊主网部署的Solidity源码，生成并整理对应的Yul低级代码实例，筛选后形成了包含348,840个Yul实例、135,013个独特合约的数据集。

Result: 成功构建了一个大规模、具代表性且公开可用的Yul智能合约数据集——YulCode，涵盖了主网真实智能合约的低级实现。

Conclusion: YulCode是首个聚焦Yul中间语言的公开数据集，为低级智能合约分析、机器学习、形式化验证、优化等多个研究和开发领域提供了重要支撑。该数据集弥补了智能合约研究领域的关键短板，并将推动低级可编程性与安全性工具的发展。

Abstract: The YulCode dataset presents a comprehensive collection of 348,840 Yul-based
smart contract instances, comprising approximately 135,013 unique contracts.
These contracts were generated through the compilation of Solidity source files
that have been deployed on the Ethereum mainnet, making the dataset directly
representative of real-world decentralized applications. YulCode provides a
rich foundation for a variety of research and development tasks, including but
not limited to machine learning applications, formal verification, optimization
analysis, and software engineering tool evaluation in the context of low-level
smart contract code. To the best of our knowledge at the time of writing,
YulCode is the first and only publicly available dataset that focuses
specifically on Yul, an intermediate language designed for the Ethereum Virtual
Machine (EVM). As such, it fills a critical gap in the current ecosystem of
smart contract datasets and opens new avenues for research and tooling aimed at
low-level contract analysis and generation.

</details>


### [5] [Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs](https://arxiv.org/abs/2506.19287)
*Yaoxuan Wu,Xiaojie Zhou,Ahmad Humayun,Muhammad Ali Gulzar,Miryung Kim*

Main category: cs.SE

TL;DR: PALM系统以全新方式结合符号执行与LLM生成测试用例，通过静态路径枚举+代码变体+断言+可视化前端，解决了传统方法的不足，提高了路径覆盖与测试理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的符号执行方法在测试用例生成方面表现出色，但受限于对目标代码及库函数的建模能力和底层约束求解器的能力，导致某些复杂路径无法分析。此外，虽然大型语言模型(LLM)可以生成多样的测试输入，但不能系统性地枚举程序路径，也难以覆盖边角路径。本文正是为了解决这两个方法的不足而提出新方案。

Method: PALM 系统结合符号路径枚举与LLM测试生成。具体来说，PALM静态地通过AST级分析枚举程序的可能路径，将每一条路径转换为带断言的可执行程序变体，用于指定目标路径，而不是传统地转化为SMT约束。这些变体便于LLM理解和生成相应测试。此外，PALM还提供了交互式前端，直观可视化路径覆盖情况以及对应的测试用例。

Result: 用户研究显示，PALM的前端能够显著帮助用户理解路径覆盖，并精确识别哪些路径被生成的测试用例覆盖。用户能通过路径分析和可视化更好地验证和理解测试用例行为。

Conclusion: PALM系统通过创新性地结合静态路径枚举与LLM辅助测试生成，在无需复杂SMT转换的前提下实现了有效路径覆盖和测试生成，并借助可视化交互界面提升了用户对覆盖性的理解和测试定位能力。

Abstract: Symbolic execution is a widely used technique for test generation, offering
systematic exploration of program paths through constraint solving. However, it
is fundamentally constrained by the capability to model the target code
including library functions in terms of symbolic constraint and the capability
of underlying constraint solvers. As a result, many paths involving complex
features remain unanalyzed or insufficiently modeled. Recent advances in large
language models (LLMs) have shown promise in generating diverse and valid test
inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths
and often fail to cover subtle corner cases. We observe that directly prompting
an LLM with the full program leads to missed coverage of interesting paths. In
this paper, we present PALM, a test generation system that combines symbolic
path enumeration with LLM-assisted test generation. PALM statically enumerates
possible paths through AST-level analysis and transforms each into an
executable variant with embedded assertions that specify the target path. This
avoids the need to translate path constraints into SMT formulae, by instead
constructing program variants that LLM can interpret. Importantly, PALM is the
first to provide an interactive frontend that visualizes path coverage
alongside generated tests, assembling tests based on the specific paths they
exercise. A user study with 12 participants demonstrates that PALM's frontend
helps users better understand path coverage and identify which paths are
actually exercised by PALM-generated tests, through verification and
visualization of their path profiles.

</details>


### [6] [What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance](https://arxiv.org/abs/2506.19425)
*Ang Jia,He Jiang,Zhilei Ren,Xiaochen Li,Ming Fan,Ting Liu*

Main category: cs.SE

TL;DR: 本文首次系统性分析不同编译设置下二进制函数调用图变化及其对分解方法的影响，发现现有方法难以适应跨编译器和优化多样性，提出的最优分解法为模块划分和评测提供新基准。


<details>
  <summary>Details</summary>
Motivation: 二进制重用检测过程中，二进制分解（将二进制文件分为模块）是关键环节。现有方法普遍假设已重用代码会保持相似的函数调用关系，依赖函数调用图（FCG），但作者发现使用不同编译设置（尤其是各种函数内联决策）会导致FCG差异很大，这对现有方法效果有重大影响。

Method: 本文首次系统性地实证分析了不同编译设置下FCG的变异，并研究这些变化对二进制分解方法的影响。作者构建涵盖17种编译器、6种优化选项和4种架构的多样化数据集，分析FCG的变化及其三种不同的映射方式。随后对现有分解方法在FCG变化背景下进行评测，最后提出一种用于识别最优分解的新方法，并用其与现有所有分解方法进行对比分析。

Result: 研究发现，FCG的规模随编译选项变化剧烈，但还可以通过三种映射方式建立联系。现有方法在面对这种跨编译器及优化多样性的情况下表现出很大挑战，存在覆盖率低或生成的社区相似性不稳定等问题。本工作提出的最优分解方法为评测现有方法提供了新基准。

Conclusion: 不同编译设置显著改变了二进制的函数调用图结构，现有模块分解方法难以应对跨编译器和多优化设置的场景。提出的最优分解方法提升了分解评测的科学性和准确性，为今后的二进制重用检测和分解算法研究提供了新思路。

Abstract: Binary decomposition, which decomposes binary files into modules, plays a
critical role in binary reuse detection. Existing binary decomposition works
either apply anchor-based methods by extending anchor functions to generate
modules, or apply clustering-based methods by using clustering algorithms to
group binary functions, which all rely on that reused code shares similar
function call relationships. However, we find that function call graphs (FCGs)
vary a lot when using different compilation settings, especially with diverse
function inlining decisions.
  In this work, we conduct the first systematic empirical study on the variance
of FCGs compiled by various compilation settings and explore its effect on
binary decomposition methods. We first construct a dataset compiled by 17
compilers, using 6 optimizations to 4 architectures and analyze the changes and
mappings of the FCGs. We find that the size of FCGs changes dramatically, while
the FCGs are still linked by three different kinds of mappings. Then we
evaluate the existing works under the FCG variance, and results show that
existing works are facing great challenges when conducting cross-compiler
evaluation with diverse optimization settings. Finally, we propose a method to
identify the optimal decomposition and compare the existing decomposition works
with the optimal decomposition. Existing works either suffer from low coverage
or cannot generate stable community similarities.

</details>


### [7] [LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code](https://arxiv.org/abs/2506.19481)
*Shahbaz Siddeeq,Muhammad Waseem,Zeeshan Rasheed,Md Mahade Hasan,Jussi Rasku,Mika Saari,Henri Terho,Kalle Makela,Kai-Kristian Kemell,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文提出并验证了一种基于LLM的多智能体自动重构系统，可有效提升Haskell等函数式代码的质量、性能和可维护性，为自动化重构提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 代码重构是软件开发和维护中的常规活动，但这一过程仍然高度依赖人工分析与操作，容易出错且耗时；如何自动化、智能化地进行重构，尤其是针对函数式语言（如Haskell），意义重大。

Method: 提出了一种基于大型语言模型（LLM）的多智能体系统，每个智能体承担不同职责（如代码分析、重构执行、验证和调试），实现Haskell代码自动重构，并在多个开源Haskell代码库上进行了实验评估。

Result: 实验表明，该系统平均降低了代码复杂度11.03%，整体代码质量提升22.46%，性能效率提高13.27%，内存分配优化最高可达14.57%。

Conclusion: 基于LLM的多智能体系统能够有效管理针对函数式编程范式的重构任务，提升代码可维护性、自动化开发流程，有望成为函数式编程语言自动重构的有效工具。

Abstract: Refactoring is a constant activity in software development and maintenance.
Scale and maintain software systems are based on code refactoring. However,
this process is still labor intensive, as it requires programmers to analyze
the codebases in detail to avoid introducing new defects. In this research, we
put forward a large language model (LLM)-based multi-agent system to automate
the refactoring process on Haskell code. The objective of this research is to
evaluate the effect of LLM-based agents in performing structured and
semantically accurate refactoring on Haskell code. Our proposed multi-agent
system based on specialized agents with distinct roles, including code
analysis, refactoring execution, verification, and debugging. To test the
effectiveness and practical applicability of the multi-agent system, we
conducted evaluations using different open-source Haskell codebases. The
results of the experiments carried out showed that the proposed LLM-based
multi-agent system could average 11.03% decreased complexity in code, an
improvement of 22.46% in overall code quality, and increase performance
efficiency by an average of 13.27%. Furthermore, memory allocation was
optimized by up to 14.57%. These results highlight the ability of LLM-based
multi-agent in managing refactoring tasks targeted toward functional
programming paradigms. Our findings hint that LLM-based multi-agent systems
integration into the refactoring of functional programming languages can
enhance maintainability and support automated development workflows.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [FO-Query Enumeration over SLP-Compressed Structures of Bounded Degree](https://arxiv.org/abs/2506.19421)
*Markus Lohrey,Sebastian Maneth,Markus L. Schmid*

Main category: cs.LO

TL;DR: 本文将有界度结构第一阶查询结果的高效枚举扩展到SLP压缩表示，提出算法在常数延迟、SLP线性预处理下完成结果集枚举。


<details>
  <summary>Details</summary>
Motivation: 以往研究已知，对于有界度的关系结构，第一阶查询可以在线性预处理及常数延迟下枚举结果。然而，实际数据往往经过高度压缩，本工作旨在扩展该理论，研究结构以SLP形式压缩存储时的高效枚举方法。

Method: 作者设计并分析了一种新算法，该算法基于对SLP（straight-line program）高效展开与遍历，并结合第一阶逻辑查询的特殊结构，保证了在结构压缩存储时的高效枚举性能。

Result: 提出了针对满足apex条件的SLP编码有界度结构的第一阶查询结果常数延迟枚举方法，并证明其预处理时间为SLP大小的线性级别。

Conclusion: 本文提出了一种算法，能够在结构由满足apex条件的SLP编码时，对有界度关系结构的第一阶查询结果集进行常数延迟枚举，并且预处理时间与SLP大小线性相关。

Abstract: Enumerating the result set of a first-order query over a relational structure
of bounded degree can be done with linear preprocessing and constant delay. In
this work, we extend this result towards the compressed perspective where the
structure is given in a potentially highly compressed form by a straight-line
program (SLP). Our main result is an algorithm that enumerates the result set
of a first-order query over a structure of bounded degree that is represented
by an SLP satisfying the so-called apex condition. For a fixed formula, the
enumeration algorithm has constant delay and needs a preprocessing time that is
linear in the size of the SLP.

</details>


### [9] [Time-Sensitive Importance Splitting](https://arxiv.org/abs/2506.19568)
*Gabriel Dengler,Carlos E. Budde,Laura Carnevali,Arnd Hartmanns*

Main category: cs.LO

TL;DR: 该论文提出了一种面向时间敏感的评价函数，提升了非马尔可夫模型中稀有事件仿真的效率，并通过实验验证了其在可靠性工程中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前稀有事件仿真的非马尔可夫模型方法，在需要提前了解系统定时行为时受到实际或理论上的限制。作者希望突破这些限制。

Method: 将重要性分裂技术与对时间敏感的评价函数相结合。具体做法是从目标状态进行反向可达性搜索，利用活动计时器的上下界信息来引导路径生成，目标是更高效地仿真稀有事件的发生。方法在Modest Toolset中针对输入/输出随机自动机实现了原型。

Result: 初步实验表明，该方法在可靠性工程样例中，对稀有事件概率的估算显示出潜力。

Conclusion: 利用时间敏感的评价函数扩展重要性分裂方法，有助于提升非马尔可夫模型稀有事件仿真的有效性。方法具有实际应用前景。

Abstract: State-of-the-art methods for rare event simulation of non-Markovian models
face practical or theoretical limits if observing the event of interest
requires prior knowledge or information on the timed behavior of the system. In
this paper, we attack both limits by extending importance splitting with a
time-sensitive importance function. To this end, we perform backwards
reachability search from the target states, considering information about the
lower and upper bounds of the active timers in order to steer the generation of
paths towards the rare event. We have developed a prototype implementation of
the approach for input/output stochastic automata within the Modest Toolset.
Preliminary experiments show the potential of the approach in estimating rare
event probabilities for an example from reliability engineering.

</details>


### [10] [Homomorphism Indistinguishability and Game Comonads for Restricted Conjunction and Requantification](https://arxiv.org/abs/2506.19746)
*Georg Schindling*

Main category: cs.LO

TL;DR: 本文在图的同态不可区分性理论中，将有限变量计数逻辑和受限递量化逻辑框架下的等价关系与可复用性分解、博弈、范畴理论三者统一连接，对Lovász型定理进行推广，并通过新颖的路径/树分解工具和范畴等价，揭示逻辑与组合结构的深层联系。


<details>
  <summary>Details</summary>
Motivation: 当前图同态不可区分性为研究图结构的等价关系提供了组合框架，尤其在有限模型理论中的计数逻辑具有特殊意义。探索如何将这一视角扩展到更受限的逻辑，如受限递量化的逻辑，需进一步揭示逻辑资源分层与图结构之间的关系，并对已有理论进行推广。

Method: 作者将Lovász型定理推广到受限合取或有界量词深度的逻辑，通过引入可复用性的路径与树分解、发展基于追逃博弈的刻画方法，以及构造包涵受限可复用性沟子游戏的新伴随函子体系，实现逻辑、组合及范畴三重视角的统一。

Result: 证明了带可复用性约束的有界路径宽度与树宽度图类在同态不可区分性下构成封闭类，并建立了该类图分解的具体逻辑、组合及范畴刻画。同时，通过新构造的伴随函子，实现了逻辑游戏与分解的范畴等价性。

Conclusion: 本文拓展了图同态不可区分性的理论框架，统一了逻辑、组合与范畴视角，推动了有界逻辑资源与图结构之间深层关系的理解。对受限递量化逻辑以及可复用性分解的研究有重要推动作用，对有限模型理论和图结构理论均具参考价值。

Abstract: The notion of homomorphism indistinguishability offers a combinatorial
framework for characterizing equivalence relations of graphs, in particular
equivalences in counting logics within finite model theory. That is, for
certain graph classes, two structures agree on all homomorphism counts from the
class if and only if they satisfy the same sentences in a corresponding logic.
This perspective often reveals connections between the combinatorial properties
of graph classes and the syntactic structure of logical fragments. In this
work, we extend this perspective to logics with restricted requantification,
refining the stratification of logical resources in finite-variable counting
logics. Specifically, we generalize Lov\'asz-type theorems for these logics
with either restricted conjunction or bounded quantifier-rank and present new
combinatorial proofs of existing results. To this end, we introduce novel path
and tree decompositions that incorporate the concept of reusability and develop
characterizations based on pursuit-evasion games. Leveraging this framework, we
establish that classes of bounded pathwidth and treewidth with reusability
constraints are homomorphism distinguishing closed. Finally, we develop a
comonadic perspective on requantification by constructing new comonads that
encapsulate restricted-reusability pebble games. We show a tight correspondence
between their coalgebras and path/tree decompositions, yielding categorical
characterizations of reusability in graph decompositions. This unifies logical,
combinatorial, and categorical perspectives on the notion of reusability.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)
*Hexiang Gu,Qifan Yu,Saihui Hou,Zhiqin Fang,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出大规模高质量的 MemeMind 数据集和创新检测框架 MemeGuard，推进多模态有害迷因检测领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前有害迷因识别受限于数据集规模、小样本和缺乏可解释性，影响模型进一步提升，因此亟需系统性、具多样性和高解释性的数据支持及更强建模方法。

Method: 构建了包含中英文、链式推理标注的大规模多样化 MemeMind 数据集，并设计集成多模态信息与推理过程建模的新方法 MemeGuard，结合在新数据集上的广泛实验。

Result: MemeMind 数据集具备规模大、科学标准高、多样性与可解释性强（含推理链标注）；新方法 MemeGuard 在相关任务上优于当前最先进方法。

Conclusion: 提出并验证了新的数据集 MemeMind 和检测框架 MemeGuard，显著提升了有害网络迷因识别的效果。

Abstract: The rapid development of social media has intensified the spread of harmful
content. Harmful memes, which integrate both images and text, pose significant
challenges for automated detection due to their implicit semantics and complex
multimodal interactions. Although existing research has made progress in
detection accuracy and interpretability, the lack of a systematic, large-scale,
diverse, and highly explainable dataset continues to hinder further advancement
in this field. To address this gap, we introduce MemeMind, a novel dataset
featuring scientifically rigorous standards, large scale, diversity, bilingual
support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.
MemeMind fills critical gaps in current datasets by offering comprehensive
labeling and explicit reasoning traces, thereby providing a solid foundation
for enhancing harmful meme detection. In addition, we propose an innovative
detection framework, MemeGuard, which effectively integrates multimodal
information with reasoning process modeling, significantly improving models'
ability to understand and identify harmful memes. Extensive experiments
conducted on the MemeMind dataset demonstrate that MemeGuard consistently
outperforms existing state-of-the-art methods in harmful meme detection tasks.

</details>


### [12] [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: 大模型在STEM领域往往混淆记忆与推理真实能力，对自知能力评估不一致，尤其科学医学类最严重，需改进训练方法以提升AI可信任度。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLMs）常被诟病：一方面其表现出强大的能力，另一方面又容易落入简单记忆而非真正理解、推理的问题。以往研究往往将记忆化和自知之明的缺陷视为两个独立问题，忽视了它们之间的内在联系。本研究动机在于揭示两者交织后对模型可信性的实际影响。

Method: 作者提出了一个新的分析框架，通过特定实验区分大模型在STEM领域（科学、技术、工程、数学）的表现，是来源于真实推理还是仅凭记忆。他们主要关注模型在面对自验证、符合逻辑扰动任务时的表现，并评估其自知能力（self-knowledge）及其波动性。

Result: 分析揭示，LLMs在遇到与训练集中类似的问题时，往往因对记忆化内容的高信心误判自身推理能力。对自验证、逻辑一致但有扰动的任务，模型在可行性评估（即是否能解决问题的自我评估）上有超过45%的表现不一致，尤其在科学及医学领域更明显。

Conclusion: 工作显示LLMs在自知能力上明显波动，暴露了现有模型结构和训练方法的不完善。为提升可解释性和可信度，未来需研发能平衡模型知识感知、提升自知一致性的技术。

Abstract: When artificial intelligence mistakes memorization for intelligence, it
creates a dangerous mirage of reasoning. Existing studies treat memorization
and self-knowledge deficits in LLMs as separate issues and do not recognize an
intertwining link that degrades the trustworthiness of LLM responses. In our
study, we utilize a novel framework to ascertain if LLMs genuinely learn
reasoning patterns from training data or merely memorize them to assume
competence across problems of similar complexity focused on STEM domains. Our
analysis shows a noteworthy problem in generalization: LLMs draw confidence
from memorized solutions to infer a higher self-knowledge about their reasoning
ability, which manifests as an over 45% inconsistency in feasibility
assessments when faced with self-validated, logically coherent task
perturbations. This effect is most pronounced in science and medicine domains,
which tend to have maximal standardized jargon and problems, further confirming
our approach. Significant wavering within the self-knowledge of LLMs also shows
flaws in current architectures and training patterns, highlighting the need for
techniques that ensure a balanced, consistent stance on models' perceptions of
their own knowledge for maximum AI explainability and trustworthiness. Our code
and results are available publicly at
https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.

</details>


### [13] [Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations](https://arxiv.org/abs/2506.19004)
*Brian Siyuan Zheng,Alisa Liu,Orevaoghene Ahia,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文发现大语言模型对从未见过的分词方法有极强鲁棒性，一些特别分词方式还能提升特定任务表现，且提升主要得益于instruction-tuning阶段，说明模型对分词器依赖性低于过往认知。


<details>
  <summary>Details</summary>
Motivation: 现有分词器采用确定性算法，但同一字符串在词表下存在大量可能的非规范分词编码。研究旨在探究大模型面对训练未见的非规范分词时的鲁棒性与机制。

Method: 在20个基准测试集上，将语言模型暴露于完全随机和字符级的非规范分词编码，并对比原始与非规范分词下的模型表现。进一步分析在特定任务中不同分词方案的性能差异，并追溯鲁棒性来源。

Result: instruction-tuned模型在非规范分词下最高可保留93.4%原始表现，字符级分词下为90.8%；更强模型通常更鲁棒，且非规范分词在部分场景明显提升任务效果（如代码理解提升14%，大数运算提升33%）。鲁棒性主要来源于instruction-tuning阶段。

Conclusion: 研究发现，现代大模型对训练时从未见过的非规范分词仍具较高的鲁棒性，且这种鲁棒性主要来源于instruction-tuning阶段。此外，在某些任务上，采用特别的非规范分词方式可显著提升表现。

Abstract: Modern tokenizers employ deterministic algorithms to map text into a single
"canonical" token sequence, yet the same string can be encoded as many
non-canonical tokenizations using the tokenizer vocabulary. In this work, we
investigate the robustness of LMs to text encoded with non-canonical
tokenizations entirely unseen during training. Surprisingly, when evaluated
across 20 benchmarks, we find that instruction-tuned models retain up to 93.4%
of their original performance when given a randomly sampled tokenization, and
90.8% with character-level tokenization. We see that overall stronger models
tend to be more robust, and robustness diminishes as the tokenization departs
farther from the canonical form. Motivated by these results, we then identify
settings where non-canonical tokenization schemes can *improve* performance,
finding that character-level segmentation improves string manipulation and code
understanding tasks by up to +14%, and right-aligned digit grouping enhances
large-number arithmetic by +33%. Finally, we investigate the source of this
robustness, finding that it arises in the instruction-tuning phase. We show
that while both base and post-trained models grasp the semantics of
non-canonical tokenizations (perceiving them as containing misspellings), base
models try to mimic the imagined mistakes and degenerate into nonsensical
output, while post-trained models are committed to fluent responses. Overall,
our findings suggest that models are less tied to their tokenizer than
previously believed, and demonstrate the promise of intervening on tokenization
at inference time to boost performance.

</details>


### [14] [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
*Weijie Xu,Yiwen Wang,Chi Xue,Xiangkun Hu,Xi Fang,Guimin Dong,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 本文提出了FiSCo框架，可在claim级别检测LLM输出长文本中的细微偏见，实验显示其在群体公平性检测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）生成的回答常带有固有偏见，影响其在现实世界场景中的可靠性。现有评估方法往往忽视了长文本回答中的偏见以及LLMs输出的内在变异性。

Method: 提出一种名为FiSCo（Fine-grained Semantic Computation）的新型统计框架。该方法将模型输出分解为语义上区分明确的claim，基于蕴含检验进行claim级别的意义一致性评估，并通过统计假设检验比较群体内和群体间的相似性，以检测微妙的偏见。方法还形式化了分组反事实公平性的新定义。

Result: FiSCo被应用于合成和人工注释的多个数据集（涵盖性别、种族和年龄），实验表明，相比多种评价指标，FiSCo能更可靠地检测细微偏见，同时减少LLM生成随机性的影响。

Conclusion: FiSCo能够在长文本输出和不同人群间，细致、稳健地发现大语言模型回答中的深层偏见，为LLMs公平性评估提供了有力工具。

Abstract: Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.

</details>


### [15] [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
*Omer Luxembourg,Haim Permuter,Eliya Nachmani*

Main category: cs.CL

TL;DR: DUS通过扩张分组和并行去掩码机制，使非自回归扩散语言模型推理大幅加速且提升质量，无须重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散语言模型（MDLM）在非自回归文本生成上表现良好，但主流采样器采用的启发式方法（如置信度或熵分数）在并行去掩码时无法很好地处理多个token间的依赖关系，导致推理速度难以超过传统自回归模型。提升效率且不损失生成质量成为迫切需求。

Method: 提出了一种仅在推理阶段使用、无须额外训练的 Dilated-scheduled Unmasking Strategy（DUS）。该方法依托一阶马尔可夫假设，将序列位置按照扩张（dilation）方式分组，每组为非相邻token，实现可以并行、独立地去掩码。DUS每轮最小化联合熵，相比半自回归（semi-AR）block法，极大减少了去噪器调用次数（O(log B) vs. O(B)）。

Result: 在math（GSM8K）和代码补全（Humaneval, MBPP）等重非序生成任务中，DUS相比并行置信度规划方法取得了更好的分数，且无需更改底层去噪模块。效率和生成质量双提升。

Conclusion: DUS是一种轻量、高效、无需模型改动的推理增强方案，为MDLMs带来高质量与推理效率的兼得，提升了其实际应用价值。

Abstract: Masked diffusion language models (MDLM) have shown strong promise for
non-autoregressive text generation, yet existing samplers act as implicit
planners, selecting tokens to unmask via denoiser confidence or entropy scores.
Such heuristics falter under parallel unmasking - they ignore pairwise
interactions between tokens and cannot account for dependencies when unmasking
multiple positions at once, limiting their inference time to traditional
auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking
Strategy (DUS), an inference-only, planner-model-free method that requires no
additional training. DUS leverages a first-order Markov assumption to partition
sequence positions into dilation-based groups of non-adjacent tokens, enabling
independent, parallel unmasking steps that respect local context that minimizes
the joint entropy of each iteration step. Unlike semi-AR block approaches
(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces
the number of denoiser calls to O(log B) per generation block - yielding
substantial speedup over the O(B) run time of state-of-the-art diffusion
models, where B is the block size in the semi-AR inference process. In
experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -
domains suited to non-ordinal generation - DUS improves scores over parallel
confidence-based planner, without modifying the underlying denoiser. DUS offers
a lightweight, budget-aware approach to efficient, high-quality text
generation, paving the way to unlock the true capabilities of MDLMs.

</details>


### [16] [NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching](https://arxiv.org/abs/2506.19058)
*Mike Zhang,Rob van der Goot*

Main category: cs.CL

TL;DR: 本文提出并比较多种NLP方法用于多语言岗位匹配和基于职位名的技能预测，发现大规模多语言模型和prompt方法在岗位匹配上效果最好，分类模型在技能预测上更优，最终在国际任务中取得较好排名。


<details>
  <summary>Details</summary>
Motivation: 工作岗位名称匹配和岗位技能对齐在就业市场中对自动候选人匹配、职业路径预测和就业市场分析等下游应用非常重要。这个研究的动机是提升这些任务的效果，尤其在多语言环境下的职位名匹配与职位相关技能预测。

Method: 比较了三种方法：微调的分类模型、微调的对比学习模型以及基于prompt的模型。此外，还利用了额外的多语言职位名称和描述数据作为增强。

Result: 在多语言岗位名称匹配（Task A）上，基于prompt的方法表现最佳，测试集平均MAP为0.492。在基于职位名称的技能预测（Task B）上，微调分类模型表现最好，MAP为0.290。使用了来自ESCO的多语言数据增强方法。大规模多语言模型在两项任务上均表现优异。排名方面，Task A在20支队伍中排第5，Task B在14支队伍中排第3。

Conclusion: 多语言大模型和合适的方法可有效提升职位名匹配和技能预测的效果。通过方法比较确定了在不同任务下表现最佳的技术，综合表现优异，在国际比赛中取得前列成绩。

Abstract: Matching job titles is a highly relevant task in the computational job market
domain, as it improves e.g., automatic candidate matching, career path
prediction, and job market analysis. Furthermore, aligning job titles to job
skills can be considered an extension to this task, with similar relevance for
the same downstream tasks. In this report, we outline NLPnorth's submission to
TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title
Matching, and Job Title-Based Skill Prediction. For both tasks we compare
(fine-tuned) classification-based, (fine-tuned) contrastive-based, and
prompting methods. We observe that for Task A, our prompting approach performs
best with an average of 0.492 mean average precision (MAP) on test data,
averaged over English, Spanish, and German. For Task B, we obtain an MAP of
0.290 on test data with our fine-tuned classification-based approach.
Additionally, we made use of extra data by pulling all the language-specific
titles and corresponding \emph{descriptions} from ESCO for each job and skill.
Overall, we find that the largest multilingual language models perform best for
both tasks. Per the provisional results and only counting the unique teams, the
ranking on Task A is 5$^{\text{th}}$/20 and for Task B 3$^{\text{rd}}$/14.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [17] [Local Search Improvements for Soft Happy Colouring](https://arxiv.org/abs/2506.19284)
*Mohammad Hadi Shekarriz,Dhananjay Thiruvady,Asef Nazari,Wilfried Imrich*

Main category: cs.DM

TL;DR: 论文研究了soft happy colouring问题，提出三种局部搜索算法，并对比它们的性能，证明了较大ρ值更有利于社区检测但更难求解。提出的线性时间算法十分高效且易取得优良结果。


<details>
  <summary>Details</summary>
Motivation: 当前关于happy colouring问题已经被证明为NP-hard，并且该问题和图的社区结构有直接关联，特别是在特定的happiness阈值和社区结构下，社区的染色能够使所有顶点都成为ρ-happy。然而，不同参数、算法和结构下相关结论及最优策略尚不完全清楚，需要进一步研究。

Method: 文中提出并分析了三种针对soft happy colouring的局部搜索算法。同时，将它们与现有的算法进行了性能对比。还理论上探讨了较大ρ值下完全ρ-happy染色的不可能性，以及不同ρ值对社区检测准确率的影响。

Result: 主要结果有：1. 证明了当0≤ρ1<ρ2≤1时，完全ρ2-happy染色能比完全ρ1-happy染色在社区检测中更准确；2. 当ρ超过某一阈值后，几乎不可能构造出色类规模接近的完全ρ-happy染色；3. 提出的线性时间局部搜索算法不仅速度非常快，还能够显著提升ρ-happy顶点数量，表现优异。

Conclusion: 通过不同参数下的理论分析和算法比较，验证了提出的线性时间局部搜索算法在soft happy colouring问题中的高效性与可靠性。同时，阐明了不同happiness参数对社区检测及可解性的影响。

Abstract: For $0\leq \rho\leq 1$ and a coloured graph $G$, a vertex $v$ is $\rho$-happy
if at least $\rho \deg(v)$ of its neighbours have the same colour as $v$. Soft
happy colouring of a partially coloured graph $G$ is the problem of finding a
vertex colouring $\sigma$ that preserves the precolouring and has the maximum
number of $\rho$-happy vertices. It is already known that this problem is
NP-hard and directly relates to the community structure of the graphs; under a
certain condition on the proportion of happiness $\rho$ and for graphs with
community structures, the induced colouring by communities can make all the
vertices $\rho$-happy. We show that when $0\leq \rho_1<\rho_2\leq 1$, a
complete $\rho_2$-happy colouring has a higher accuracy of community detection
than a complete $\rho_1$-happy colouring. Moreover, when $\rho$ is greater than
a threshold, it is unlikely for an algorithm to find a complete $\rho$-happy
colouring with colour classes of almost equal sizes. Three local search
algorithms for soft happy colouring are proposed, and their performances are
compared with one another and other known algorithms. Among them, the
linear-time local search is shown to be not only very fast, but also a reliable
algorithm that can dramatically improve the number of $\rho$-happy vertices.

</details>


### [18] [Paired Disjunctive Domination Number of Middle Graphs](https://arxiv.org/abs/2506.19529)
*Hande Tuncel Golpek,Zeliha Kartal Yildiz,Aysun Aytac*

Main category: cs.DM

TL;DR: 该论文系统研究了多类中间图的配对析取支配数，给出具体计算、一般性上下界及某些运算下的精确表达，推进了对图支配参数的理论理解。


<details>
  <summary>Details</summary>
Motivation: 图的支配（domination）概念在理解结构性质和网络理论应用中具有核心作用。该文献关注中间图（middle graphs）中的配对析取支配数（paired disjunctive domination number），以揭示其在图结构变换后的组合性质。

Method: 首先，分析路径图、圈图、轮图、完全图、完全二分图、星图、友谊图和双星图的中间图的配对析取支配数，然后建立一般任意图（尤其是树）中间图此参数的上下界，并研究了中间图的 join 运算下该参数的确切值。

Result: 给出了多类特殊图的中间图的配对析取支配数，获得了任意图的中间图此参数的上线界，特别对树类图给出强调，且精确确定了 join 运算下的配对析取支配数。

Conclusion: 本文的结果丰富了对图结构变换下支配类参数的理解，为其组合行为提供了新见解，有助于拓展相关图论领域的理论和应用。

Abstract: The concept of domination in graphs plays a central role in understanding
structural properties and applications in network theory. In this study, we
focus on the paired disjunctive domination number in the context of middle
graphs, a transformation that captures both adjacency and incidence relations
of the original graph. We begin by investigating this parameter for middle
graphs of several special graph classes, including path graphs, cycle graphs,
wheel graphs, complete graphs, complete bipartite graphs, star graphs,
friendship graphs, and double star graphs. We then present general results by
establishing lower and upper bounds for the paired disjunctive domination
number in middle graphs of arbitrary graphs, with particular emphasis on trees.
Additionally, we determine the exact value of the parameter for middle graphs
obtained through the join operation. These findings contribute to the broader
understanding of domination-type parameters in transformed graph structures and
offer new insights into their combinatorial behavior.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [19] [In-Memory Sorting-Searching with Cayley Tree](https://arxiv.org/abs/2506.19379)
*Subrata Paul,Sukanta Das,Biplab K Sikdar*

Main category: cs.FL

TL;DR: 论文提出了一种基于Cayley树的内存内计算架构，通过软/硬件结合显著降低CPU负担和提升搜索、最大值、排序等操作的效率，FPGA实验证明该方案优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CPU在处理数据密集型计算时工作负载较高，存在数据要频繁移动到CPU处理的效率瓶颈。为了解决CPU瓶颈，提升数据密集型任务的处理效率，本文提出一种“内存内计算”（IMC）平台。

Method: 该研究提出一种基于Cayley树结构的内存内计算模型，每个带有附加逻辑的内存单元视为微小处理单元，形成树的节点。通过该结构实现内存内的数据搜索、最大（最小）值计算和排序，并设计了两种硬件实现方式：一种基于传统内存架构，另一种基于新定义的内存架构。同时在FPGA上进行了验证。

Result: 基于IMC的内存内搜索和最大（最小）值计算的最坏时间复杂度为O(log n)，与列表中元素顺序无关；内存内排序的最坏时间复杂度为O(n log n)。论文提出的两类硬件平台在FPGA上的实现效果优于现有设计。

Conclusion: IMC模型有效减少了CPU的参与，通过在内存端进行并行处理，提升数据密集型任务的性能。其灵活的硬件实现保证了架构的实际可用性，并优于当下主流方案。

Abstract: This work proposes a computing model to reduce the workload of CPU. It relies
on the data intensive computation in memory, where the data reside, and
effectively realizes an in-memory computing (IMC) platform. Each memory word,
with additional logic, acts as a tiny processing element which forms the node
of a Cayley tree. The Cayley tree in turn defines the framework for solving the
data intensive computational problems. It finds the solutions for in-memory
searching, computing the max (min) in-memory and in-memory sorting while
reducing the involvement of CPU. The worst case time complexities of the IMC
based solutions for in-memory searching and computing max (min) in-memory are
$\mathcal{O}\log{n}$. Such solutions are independent of the order of elements
in the list. The worst case time complexity of in-memory sorting, on the other
hand, is $\mathcal{O}(n\log{n})$. Two types of hardware implementations of the
IMC platform are proposed. One is based on the existing/conventional memory
architecture, and the other one is on a newly defined memory architecture. The
solutions are further implemented in FPGA platform to prove the effectiveness
of the IMC architecture while comparing with the state-of-the art designs.

</details>
