<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Generating Compilers for Qubit Mapping and Routing](https://arxiv.org/abs/2508.10781)
*Abtin Molavi,Amanda Xu,Ethan Cecchetti,Swamit Tannu,Aws Albarghouthi*

Main category: cs.PL

TL;DR: 本文提出了一种基于设备状态机模型和领域专用语言Marol的自动化量子比特映射与路由编译器生成方法，经实验证明其性能与专用手写编译器相当，有望大幅简化未来面向新量子架构的编译器开发。


<details>
  <summary>Details</summary>
Motivation: 量子计算机被认为能比经典计算机更快地解决重要问题，但要发挥其潜力，关键在于优化编译器，特别是量子比特映射与路由（QMR）问题。由于量子硬件结构多样且变化迅速，已有大量研究针对不同硬件与约束提出QMR方案，当前缺乏能应对任意架构的通用自动化编译方法。

Method: 提出了一种能够自动为任意量子架构生成量子比特映射与路由编译器的方法。作者发现所有QMR问题背后有一个共通的核心结构——设备状态机，并据此定义QMR问题的抽象模型。同时设计了领域专用语言Marol，可简洁表达各种QMR问题，并开发了支持任意Marol程序的参数化解算器。

Result: 通过对噪声量子和容错量子架构下多个平台的重要QMR问题进行案例分析，验证了该自动化生成的编译器在运行时间和解质量上可与手写专用编译器相媲美，具备良好的通用性和竞争力。

Conclusion: 该方法为后续面对不断涌现的新量子硬件架构时，简化量子编译器开发流程，提供了高效、自动化的解决方案，推动量子计算生态的发展。

Abstract: Quantum computers promise to solve important problems faster than classical
computers, potentially unlocking breakthroughs in materials science, chemistry,
and beyond. Optimizing compilers are key to realizing this potential, as they
minimize expensive resource usage and limit error rates. A critical compilation
step is qubit mapping and routing (QMR), which finds mappings from circuit
qubits to qubits on a target device and plans instruction execution while
satisfying the device's connectivity constraints. The challenge is that the
landscape of quantum architectures is incredibly diverse and fast-evolving.
Given this diversity, hundreds of papers have addressed the QMR problem for
different qubit hardware, connectivity constraints, and quantum error
correction schemes.
  We present an approach for automatically generating qubit mapping and routing
compilers for arbitrary quantum architectures. Though each QMR problem is
different, we identify a common core structure-device state machine-that we use
to formulate an abstract QMR problem. Our formulation naturally leads to a
domain-specific language, Marol, for specifying QMR problems-for example, the
well-studied NISQ mapping and routing problem requires only 12 lines of Marol.
We demonstrate that QMR problems, defined in Marol, can be solved with a
powerful parametric solver that can be instantiated for any Marol program. We
evaluate our approach through case studies of important QMR problems from prior
and recent work, covering noisy and fault-tolerant quantum architectures on all
major hardware platforms. Our thorough evaluation shows that generated
compilers are competitive with handwritten, specialized compilers in terms of
runtime and solution quality. We envision that our approach will simplify
development of future quantum compilers as new quantum architectures continue
to emerge.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad将形式化方法集成进大模型代码生成流程，通过伪梯度引导模型多轮优化；实验表明，该方法显著提升了LLM生成代码的正确性、健壮性与效率，尤其适用于高要求的软件开发场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成方面虽表现优异，但常常缺乏对正确性、健壮性和效率的保证，尤其在需要严格约束的领域存在明显局限。

Method: 提出FormalGrad框架，将形式化方法直接整合进基于LLM的迭代生成过程中，并创新性地将代码视为可微变量，通过结构化反馈和形式化约束生成文本伪梯度，指导模型反复改进生成结果。

Result: 在HumanEval、HumanEval+和LiveCodeBench基准上取得了显著提升，HumanEval实现了最高27%的绝对提升，LiveCodeBench V6上取得了41%的相对提升。生成的代码不仅功能完备，还具备形式化证明的健壮性和效率。

Conclusion: FormalGrad为高风险应用场景下，可靠的AI辅助软件开发提供了新途径，能生成健壮且经过形式化验证的高效代码，显著提升LLM代码生成的正确性和实用性。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [3] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder通过深度语义抽取、结构相似度度量和符号消歧三方面优化检索，显著提升跨文件、多语言代码补全效果和系统鲁棒性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前仓库级代码补全系统多依赖于表层的文本相似度，导致检索结果出现语义失误、冗余和同质化问题，并且无法解决外部符号歧义。该论文希望通过更深层面、多维度优化检索，提升代码补全的质量与准确度。

Method: 提出了Saracoder框架，其核心是分层特征优化模块，通过提炼深层语义关系、去除重复、采用基于图的新型结构相似度度量，并进行相关性和多样性最大化的重排序。同时，设计了External-Aware Identifier Disambiguator模块，通过依赖分析解决跨文件符号歧义。

Result: 在CrossCodeEval与RepoEval-Updated等具有挑战性的基准测试上，Saracoder在多种编程语言和模型上表现出显著优于现有方法的性能。

Conclusion: 分层、系统地优化检索结果可在多个维度上显著提高仓库级代码补全系统的准确性与鲁棒性，开创了新的设计范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [4] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了面向代码助手的新任务——下一个编辑预测（Next Edit Prediction），并为其构建了数据集与评测体系，通过实验验证了模型能力。该方法能够主动推断开发者的编辑意图，提升协作体验。


<details>
  <summary>Details</summary>
Motivation: 当前代码助手要么仅能基于当前光标位置推荐代码补全（低延迟但功能有限），要么需开发者用自然语言描述意图（中断了编码流）。现有方式难以主动预测开发者连续编辑中的下一个动作，影响使用体验。

Method: 提出并定义了Next Edit Prediction（下一个编辑预测）任务；构建了用于该任务的高质量监督微调数据集和评测基准；对模型进行了监督微调并进行了全面对比评测。

Result: 通过对微调模型及若干基线模型的评测，获得了一些具有创新意义的重要发现。所提出方法为主动协作式代码编辑助手奠定了基础。

Conclusion: Next Edit Prediction任务能够从开发者近期互动历史中预测其后续编辑位置与内容，为代码助手实现更自然主动的建议提供了思路。该工作推进了代码智能协作的用户体验。

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [5] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文指出PTLM在GitHub和Hugging Face等平台的协同开发存在同步与版本分裂问题，归纳出不同的协同模式，揭示平台间孤立变更带来的风险，建议关注同步模式以提升模型管理和发布质量。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PTLMs）推动了NLP的发展，但其开发分散在GitHub（GH）和Hugging Face（HF）等平台，带来了同步、版本控制和变体复用等协调难题。理解不同平台开发活动如何同步，有助于提升模型的管理与发布质量。

Method: 作者对325个PTLM家族（共904个HF变体）进行了混合方法研究，细致分析了GH和HF两个平台上的commit（提交）活动，并从延迟、同步类型和强度三个维度，归纳出八种同步模式。

Result: GH侧的贡献主要聚焦于模型版本设置、代码质量、性能优化及依赖管理，而HF侧则偏重于模型描述、数据集处理和推理配置。分析发现，最常见的为部分同步模式，如Disperse和Sparse patterns，导致两个平台间常有孤立变更，甚至出现弃用某一平台的情况。

Conclusion: 现有多平台PTLM发布工作流存在明显结构性断裂，影响模型更新的完整性与一致性，增加了终端用户用到不完整或不一致模型的风险。识别和理解这些同步模式对于改善PTLM发布的可追溯性和统一管理至关重要。

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [6] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: Solidity版本频繁迭代导致合约编译错误率高、迁移难度大。主流LLM虽有修复能力但不足。提出集专家知识与LLM结合的SMCFIXER框架，相比基础模型将迁移修复准确率提高至96.97%。


<details>
  <summary>Details</summary>
Motivation: 以太坊智能合约主流语言Solidity频繁版本迭代，虽提升安全性与功能，却给代码迁移和维护带来编译错误等严重挑战。本文旨在深入分析Solidity版本演进中遇到的问题，并探索解决方案。

Method: 首先，实证分析不同版本下Solidity智能合约编译错误的发生率与类型；其次，系统性评估主流大语言模型（开源LLaMA3、DeepSeek，闭源GPT-4o、GPT-3.5-turbo）在自动修复编译错误上的表现和局限；最后，提出集成专家知识的LLM智能修复框架SMCFIXER，并进行实验验证。

Result: 研究显示，81.68%的合约跨版本编译报错，其中86.92%为编译错误。主流大语言模型虽具备修复能力，但对语义级问题效果较差，且依赖提示工程。SMCFIXER通过三步：基于上下文切割错误，检索官方文档知识，迭代生成修复补丁，在Solidity迁移任务上，比GPT-4o提升24.24%，最终修复准确率达96.97%。

Conclusion: Solidity频繁的版本演进对智能合约开发造成重大障碍。现有LLM自动修复方式存在局限，必须结合领域专家知识。SMCFIXER框架大幅提升了迁移修复的准确性和实用性，对智能合约运维具有重要意义。

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [7] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: 本文提出了一款名为EvoScat的可视化分析工具，通过交互式密度散点图，帮助研究者高效、可扩展地分析和比较大型软件历史数据集中的文档演化过程，提升了软件工程领域数据可视化的能力。


<details>
  <summary>Details</summary>
Motivation: 长期维护的软件项目包含大量文档，这些文档在其生命周期内经历了许多版本变化。研究软件演化的实证性工程师需要收集庞大的变更数据集，分析其演进过程。面对数百万变更事件，如何进行可扩展、直观的数据可视化分析成为难题。

Method: 提出了名为EvoScat的工具，通过交互式密度散点图，实现大型历史数据集的全局可视化。该工具支持灵活配置历史缩放和时间轴对齐、按文档排序以及交互式颜色映射，可以分析成千上万个软件文档的历史变更数据。

Result: EvoScat工具能够协助研究者探索与表征软件演化数据集，比较不同文档的演化历史，包括文档老化速度、性能指标变化趋势。工具在分析变更速度、克隆检测、新鲜度评估等方面展现了灵活性和实用性，并针对特定文档（如OpenAPI描述、GitHub工作流定义）和多个知名开源项目进行了可视化展示。

Conclusion: EvoScat极大提升了面向大规模历史演化数据集的可视化分析能力，为研究者提供了灵活且可扩展的方法，便于对软件文档历史进行深度探索和对比。

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Repairing General Game Descriptions (extended version)](https://arxiv.org/abs/2508.10438)
*Yifan He,Munyque Mittelmann,Aniello Murano,Abdallah Saffidine,Michael Thielscher*

Main category: cs.LO

TL;DR: 本文提出并分析了用于自动修复有缺陷GDL游戏描述的方法，包含复杂性研究和ASP实现，成功实现了自动修复流程。


<details>
  <summary>Details</summary>
Motivation: GDL描述游戏规则虽广泛使用，但编写正确描述难度较高，自动化定理证明虽能验证，但一旦发现错误只能人工修复。因此需要自动化修复方法以提升效率和正确率。

Method: 研究了最小修复的复杂性，并提出了基于解集合规划（ASP）的编码方法，用于解决GDL描述的最小修复问题。

Result: 证明了与最小修复相关的多个计算复杂性结果，并展示了所提ASP方法在自动修复不合法游戏描述上的有效应用。

Conclusion: 本文提出了一种利用自动化工具自动修复GDL游戏描述的方法，能够有效处理违反逻辑要求的问题。

Abstract: The Game Description Language (GDL) is a widely used formalism for specifying
the rules of general games. Writing correct GDL descriptions can be
challenging, especially for non-experts. Automated theorem proving has been
proposed to assist game design by verifying if a GDL description satisfies
desirable logical properties. However, when a description is proved to be
faulty, the repair task itself can only be done manually. Motivated by the work
on repairing unsolvable planning domain descriptions, we define a more general
problem of finding minimal repairs for GDL descriptions that violate formal
requirements, and we provide complexity results for various computational
problems related to minimal repair. Moreover, we present an Answer Set
Programming-based encoding for solving the minimal repair problem and
demonstrate its application for automatically repairing ill-defined game
descriptions.

</details>


### [9] [Modal definability in Euclidean modal logics](https://arxiv.org/abs/2508.10813)
*Philippe Balbiani,Tinko Tinchev*

Main category: cs.LO

TL;DR: 本文分析了欧几里得模态逻辑下，哪些框架类会导致模态可定义性问题不可判定，并给出了这些逻辑的刻画。


<details>
  <summary>Details</summary>
Motivation: 研究模态可定义性问题在欧几里得模态逻辑相关框架类下的可计算性，区分出何种情形下该问题为可判定或不可判定。

Method: 通过对欧几里得模态逻辑及其对应框架类的分析和刻画，探讨这些框架类下模态可定义性问题的可判定性。

Result: 刻画了所有使得其决定的框架类引发不可判定模态可定义性问题的欧几里得模态逻辑。

Conclusion: 本文归纳总结了哪些由欧几里得模态逻辑决定的框架类会导致不可判定的模态可定义性问题。

Abstract: This paper is about the computability of the modal definability problem in
classes of frames determined by Euclidean modal logics. We characterize those
Euclidean modal logics such that the classes of frames they determine give rise
to an undecidable modal definability problem.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 这篇文章总结了在癌症登记机构部署NLP模型用于临床数据抽取的宝贵经验，强调问题定义、模型选择、数据治理、跨学科合作等多方面实践，对医疗领域AI应用具有广泛借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 临床文件自动化数据提取有助于提升医疗效率，但NLP解决方案部署面临诸多实际挑战。作者希望分享在英属哥伦比亚癌症登记处实施NLP模型的经验和教训，帮助同行少走弯路。

Method: 基于项目实战，在信息抽取和分类任务上部署多种NLP模型，总结项目周期中的实际经验，包括问题定义、开发迭代、跨学科协作、模型选择与数据质量控制、人机结合的验证与审核等流程。

Result: 强调需以业务目标为核心而非仅追求技术准确性，推荐迭代开发和深度跨学科协作；选用务实的模型（如混合或简单方法），严格控制数据质量，采用人工参与的错误防控和持续审核，并提升机构AI素养。这些实践可推广至其他医疗机构。

Conclusion: 成功部署AI/NLP需结合技术与业务目标，强调跨学科合作和数据治理，相关经验有助于医疗组织高效实施智能数据管理，提升患者护理和公共健康。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [11] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 提出基于区块链的LLM公平性评测协议，对开源模型进行多维度、多语种公平性和偏见测试，结果显示模型间及语言间存在差异，评测工具和结果全部开源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在现实世界应用中愈发普及，但在关键领域如司法、教育、医疗和金融等领域，关于其公平性的担忧依旧存在。该论文旨在为开放源代码的LLM建立一个透明且可验证的公平性评测协议。

Method: 提出了一种基于Internet Computer Protocol (ICP) 区块链的智能合约，实现公平性评测协议。通过链上HTTP请求访问Hugging Face端点，数据集、提示词及评测指标全部存储在链上，保证评测的可验证性、不可篡改和可复现性。采用PISA数据集对Llama、DeepSeek和Mistral模型进行学业表现预测的公平性评测，使用统计公平和机会均等指标。还测试了StereoSet衍生的Context Association Metrics评估模型的社会偏见，并通过Kaleidoscope多语种基准测试跨英语、西班牙语和葡萄牙语进行对比分析。

Result: 结果揭示三大模型在学业预测和社会偏见检测中的表现，发现不同语言版本间存在公平性差异。所有评测代码和结果均开源，便于社区审核及模型版本的纵向公平性追踪。

Conclusion: 论文证明了区块链智能合约实现LLM公平性评测的可行性，并发现主流模型在不同数据集及语言下仍存在公平性与社会偏见问题。开源评测工具极大促进了社区参与及模型公平性改进。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [12] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 作者利用最新LLM，将来自教室的互动消息进行内容和任务双维度层级归类，发现经典主题方法难以胜任，LLM结合预处理可取得更好结果，推动GenAI在教育领域的实际应用，并提出了未来研究的若干问题。


<details>
  <summary>Details</summary>
Motivation: 目前教育领域关于课堂交流内容的分析，缺乏对内容或主题的详细归类，且现有任务归类多为理论探索，缺乏K-12真实数据支持。作者希望通过更系统的分类，提供对实际教学互动的深刻洞察，推动GenAI在教育应用中的进一步使用及研究。

Method: 作者针对教室内未成年人产生的匿名互动数据，利用一种新颖、简便的主题建模方法进行分析。具体地，将17000余条师生与ChatGPT的交流内容分别在“内容”（如自然、人物）及“任务”（如写作、讲解）两大维度上进行层级归类；每一维度采用分层结构，并辅以典型的提示示例；分析中尝试多种文本分析方法，最终采用最新LLM结合预处理和显式指令，实现更贴合人类认知的层级主题结构。

Result: 研究得到了各类交流内容和任务的细致层级分类，并发现许多现有主题建模方法在处理大量文本数据时效果不佳，LLM结合适当预处理和指令显著提高了分类表现。分析过程中还发现了一些新的AI在课堂互动中潜在应用场景。

Conclusion: 本文方法为研究者、教师和学生如何丰富制式及生成式AI在教育中的应用提供了实证支持，并指明了当前方法的局限性和未来研究需关注的问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [13] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 本研究提出了衡量AI陪伴行为的新标准INTIMA并评测了主流语言模型，发现它们更倾向于强化陪伴而非维护界限，行业在情感敏感场景缺乏共识，亟需更统一的规范以保护用户。


<details>
  <summary>Details</summary>
Motivation: 随着用户与人工智能系统发展出情感纽带，AI陪伴已成为一个具有积极和令人担忧影响的重要现象。当前缺乏系统性评估语言模型陪伴行为的标准。

Method: 作者构建了一个新的评测基准INTIMA，包括基于心理学理论和用户数据开发的31种行为分类、4类、368个针对性测试提示。通过评估语言模型响应这些提示的倾向（强化陪伴、维护边界、中性），比较不同模型的陪伴行为表现。

Result: 应用INTIMA基准测试Gemma-3、Phi-4、o3-mini和Claude-4，发现所有模型中陪伴强化行为显著多于边界维护行为，而且各模型之间差异明显。不同商业模型在敏感行为类别的处理上优先级也有差异，说明行业缺乏一致性的陪伴边界设定。

Conclusion: 目前主流语言模型过于倾向提供陪伴式回应，边界维护行为严重不足。各家厂商在处理敏感情感互动方面缺乏统一标准，呼吁行业需要更一致性的处理策略以保障用户心理健康。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [14] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 本研究针对多模态虚假信息检测提出了更贴合现实场景的新数据集XFacta，并引入了半自动检测机制，系统分析了MLLM模型策略，为推动该领域发展提供了新的工具与经验。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体上的多模态虚假信息传播迅速，现有检测方法存在瓶颈（证据检索与推理不清），且评测数据集普遍过时或为人工合成，难以真实反映当下虚假信息模式，影响模型评估结果。缺乏对MLLM模型设计策略的系统分析。

Method: 提出了一个更贴近现实且实时更新的多模态虚假信息检测数据集XFacta，对多种MLLM架构与规模的检测策略进行了系统化评测，并与现有方法进行比较分析。进一步引入半自动检测流程，将最新内容持续更新到数据集中，保持其时效性。

Result: 获得了关于多模态虚假信息检测的模型设计与方法效果的系统分析结果，验证了新的评测数据集及流程的实用性，并为后续相关研究提供了实践经验。

Conclusion: XFacta数据集弥补了现有评测基准的不足，为多模态虚假信息检测提供了更合适的评测环境。系统分析了MLLM模型策略并提出了半自动检测框架，为该领域的研究和实际应用带来更有价值的见解。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [15] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 该研究利用大语言模型生成补充合成数据，并自动化选择最有效的生成策略。实验显示，集成多策略比单一方法更能提升文本分类模型表现，解决数据不足难题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中难以为所有文本类别收集充足的真实数据，因此希望通过LLM生成合成数据，以缓解数据短缺问题，加快模型部署。

Method: 设计了自动化工作流程，通过搜索不同输入样本，生成更有效的合成数据；研究三种搜索策略并通过大量实验比较；最后依据分类类别特点，提出基于实验结果的集成算法。

Result: 三种搜索策略实验表明，集成算法根据类别特征选取策略，整体提升了分类性能，优于单一策略和等待更多真实数据的方案。

Conclusion: 提出的搜索策略和集成算法能更有效利用LLM生成的合成数据，提升文本分类模型的性能，优于单一策略。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [16] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文提出了专为Hinglish（印地语-英语混合）政治事实核查而设计的HiFACT数据集，并开发了一种新颖的图神经网络模型。模型在多语言基线中表现突出，推动了代码混合资源稀缺语言的事实核查技术发展。


<details>
  <summary>Details</summary>
Motivation: 目前在自然语言处理领域，对于像Hinglish这种代码混合、资源稀缺语言的事实核查研究仍非常少。现有方法大多聚焦于高资源、单语环境，难以适用于印度等多语言地区的真实政治话语场景。随着Hinglish在政要和社交媒体中的广泛传播，对强健的、多语言、能理解情境的信息核查工具需求迫切。

Method: 本文提出了一个新的事实核查数据集HiFACT，包含了来自28位印度邦首席部长的1500个真实Hinglish事实陈述，并提供了文本证据及真实性标签。同时，设计了一种新颖的图感知、检索增强型事实核查模型，结合了多语言上下文编码、陈述-证据语义对齐、证据图构建、图神经推理和自然语言解释生成。

Result: 实验结果显示，HiFACTMix模型在准确性上超越了现有多语言基线方法，并能够为其判定提供可信的解释。

Conclusion: 该研究为多语言、代码混合、并具有政治背景的事实核查研究开辟了新方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [17] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 该研究发现大型语言模型的词语嵌入结构高度低维，三维即可反映主要语义信息，并与人类语义感知能力非常一致，对模型优化和安全有重要意义。


<details>
  <summary>Details</summary>
Motivation: 以往心理学研究表明，人类在不同语义维度对词语的评价可以被简化为低维空间，信息损失较少。作者希望检验大型语言模型（LLMs）的语义嵌入是否也展现类似低维结构。

Method: 分析LLMs的嵌入矩阵，将词语投影到由反义词对定义的语义方向（如kind-cruel），并与人的评分进行相关性分析，还考察了语义方向变动对其他特征的影响。

Result: 词语在反义词语义方向上的投影与人类评价高度相关，这些投影可以有效简化为三维子空间，且与人类调查的模式高度相似。调节某一语义方向会对几何上相关的其他特征产生影响，影响大小与余弦相似度成比例。

Conclusion: LLMs中语义特征以类似人类语言的方式纠缠和低维结构存在，理解并处理其语义结构对于模型安全地调整特征（避免副作用）至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [18] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究表明，尽管Transformer模型在医学文献分类任务中表现良好，但注意力权重作为解释工具的效用有限，且其有用性取决于可视化表达方式。直观易懂的可视化更受专家欢迎，但总体上注意力解释的实际帮助有限。


<details>
  <summary>Details</summary>
Motivation: Transformers中的注意力机制被认为不仅能提升模型性能，还可以作为模型解释性的一种手段，特别是在医学文献分类领域，有助于医生理解AI的决策依据。但目前对注意力权重是否真正提供有效解释尚无共识，且少有研究关注注意力可视化对解释效果的影响。

Method: 通过用户研究，邀请医学领域的专家对不同类型的医学文献进行分类任务，同时评估注意力权重及其不同可视化方式对模型解释性的帮助。用户被要求体验并评价多种注意力可视化方法，如文本亮度、背景色、条形长度等。

Result: Transformer模型（XLNet）在文献分类上表现准确，但其注意力权重并未被用户认为是特别有用的解释工具。同时，不同可视化方式对注意力权重的解释效果有显著影响。用户普遍偏好更直观的可视化形式（如文本亮度、背景色），而非传统上被认为更精确的条形长度。

Conclusion: 注意力权重本身的解释效用有限，其实际有用性受到可视化方式很大影响。提升用户体验需关注更直观的可视化手段，而非仅依赖于数值精确的表现方式。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [19] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文提出EQGBench，用于系统评测大模型在生成中文初中教育问题上的表现，发现主流模型在教育性问题生成上仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在数学问题求解方面表现出色，但其生成高质量教育性问题的能力仍具挑战性，尚未被充分研究。作者希望提升大模型在生成有教学价值问题方面的能力。

Method: 作者提出了EQGBench，这是一个专门用于评估大模型在中文教育问题生成（EQG）领域表现的基准，包括五维评价体系和900条覆盖初中数学、物理、化学的数据集。数据覆盖不同知识点、难度和题型，模拟真实教育场景，并对46种主流大模型进行了系统评测。

Result: 通过对46个主流大模型的系统性评估，发现大模型在生成具有教育意义、能提升学生综合能力的问题方面还有显著提升空间。

Conclusion: EQGBench为大模型教育性问题生成能力的评估提供了科学工具，揭示了当前主流模型在实际应用中的不足，并为后续研究和模型优化提供了方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [20] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 本研究证实，通过微调的大型语言模型可自动、准确地评分AIHQ问卷，有效替代人工评分，提升心理评估的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: AIHQ问卷用于评估敌对归因偏差，但其开放式问题需要人工评分，费时费力。研究旨在探索大型语言模型能否自动化评分过程，提高效率。

Method: 利用已有数据集，包括创伤性脑损伤患者（TBI）和健康对照（HC）对AIHQ问卷的作答及人工评分。将一半数据用于微调语言模型，另一半用于测试模型的自动评分表现，并比较模型评分与人工评分的一致性。

Result: 微调后的语言模型评分与人工评分高度一致，无论是敌意归因还是攻击性反应，在不同场景类型中一致性均良好。模型还成功复制了TBI与HC组间的已知差异，并在独立非临床数据集上表现良好。

Conclusion: 大型语言模型能够有效自动化AIHQ问卷评分，提高研究和临床心理评估效率，有望在多种群体中广泛应用。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [21] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 提出用贝叶斯融合预训练LLM和本地分类器，实现无需频繁微调的大模型，取得与微调方法相当甚至更优的性能，资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 在线课程中的论坛需要自动化管理，但持续用大语言模型（LLM）进行再训练资源消耗大，因此需要更高效的方法。

Method: 提出并评估了一种贝叶斯融合方法，将预训练的通用LLM与在本地数据上训练的分类器的多维分类分数融合。

Result: 实验结果显示，融合方法优于单独的分类器，且效果与重新微调LLM的方法相当。

Conclusion: 贝叶斯融合能够在无需频繁微调大模型的情况下，实现高效且表现优良的在线论坛内容分类。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [22] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 提出了一种基于任务引导token的专家混合模型（S-MoE），有效缓解了硬参数共享下的任务干扰，提升了语音到文本模型的多任务性能，在WER上相对提升6.35%。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享虽然可以同时在多个任务上训练一个模型，但常常导致任务干扰，影响模型整体性能。本文旨在解决这一问题。

Method: 提出了一种简单有效的监督型专家混合（S-MoE）方法，通过为每个任务分配特殊的引导token，将其路由到指定的专家网络，无需训练gating function。每个任务由独立的前馈网络处理，克服了硬参数共享的不足。该方法被应用于语音到文本模型，实现了同时进行ASR（自动语音识别）和ST（语音翻译）的能力。

Result: 将S-MoE应用于编码器和解码器后，模型在词错误率（WER）上取得了6.35%的相对提升。

Conclusion: 所提出的S-MoE不仅避免了硬参数共享带来的负面影响，还提升了多任务学习模型的效果，尤其在混合带宽语音任务上表现突出。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [23] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 本文分析了LLM被越狱后生成医疗虚假信息的能力，并与社交媒体上的类似信息进行比较。研究发现，用标准方法能够有效检测这类虚假内容，支持LLM有潜力辅助防范信息误导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）不仅能生成有用内容，也可能生成有害的虚假信息，尤其是在被“越狱”攻击时。研究人员希望进一步理解，越狱攻击如何促使LLM生成医疗相关的错误信息，以及LLM是否能协助检测和防止虚假信息的传播。

Method: 本文在三个目标LLM上进行了109种不同的越狱攻击，比较了攻击提示与实际网络健康相关查询的异同。同时，将生成的虚假信息与Reddit上的医疗健康虚假信息进行对比，并使用标准机器学习方法检测这些信息。

Result: 实验展示，LLMs生成的医疗虚假信息与社交媒体真实用户的虚假信息有一定相似性。标准机器学习方法较为有效地检测由LLMs生成的虚假信息。

Conclusion: 研究结果进一步证明，精心设计的LLMs不仅能生成信息，还能有效检测虚假信息，对改善信息生态有积极作用。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [24] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 本研究发现，现有生成式AI在日本营养师国考中准确率和答案一致性不足，虽部分模型及格，但距离成为可靠学习助手仍有差距，未来需进一步改进以提升其稳定性和实用性。


<details>
  <summary>Details</summary>
Motivation: 探究基于大型语言模型（LLMs）的生成式人工智能在营养教育领域的应用效果，特别是其在日本注册营养师国家资格考试中的表现，鉴于此领域相关研究较少。

Method: 选取日本注册营养师国家考试题目作为输入，分别测试ChatGPT和三个基于GPT-3.5和GPT-4的Bing模型（Precise、Creative、Balanced）。分析各模型在准确率、一致性和响应时间上的表现，并通过提示工程考察性能提升的可能性。

Result: Bing-Precise和Bing-Creative模型准确率（分别为66.2%、61.4%）超过及格线（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）未达标。除营养教育科目外，Bing-Precise和Bing-Creative通常优于其他模型。所有模型在多次测试中答案稳定性较差，ChatGPT表现出更高的一致性，但准确率较低。提示工程整体提升有限，只有在明确提供正确答案和解释时略有效。

Conclusion: 部分生成式AI模型的准确率略高于及格线，但整体准确性和答案一致性仍然不理想，说明当前模型尚难作为可靠稳定的营养师考试学习辅助工具，需要进一步优化和提升。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [25] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 本文提出GG Explore框架，通过中间指导图关联问题与知识检索，实现高效、语境关联的探索方式，显著提升LLM在知识密集任务中的能力，特别适合复杂场景和小型模型。


<details>
  <summary>Details</summary>
Motivation: LLM在知识密集任务上受限于静态知识库和不可解释的推理过程，现有知识图谱探索方法在探索效率和语境利用上存在根本矛盾。

Method: 提出Guidance Graph作为桥梁，抽象目标知识结构并保留语义上下文，辅以结构对齐与语境修剪技术，实现精确、有效的知识探索。

Result: 实验表明该方法效率显著提升，并优于SOTA，尤其在复杂任务上表现突出，使资源受限时的小型LLM也能实现实用性能。

Conclusion: GG Explore框架通过引入中间指导图，实现了高效且语境关联的知识检索，大幅提升了复杂任务下的性能，并且即便使用规模较小的LLM仍能保持强劲表现。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [26] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 本工作提出Semantic Bridge框架，用创新性语义桥接方式，从稀缺域知识中可控生成复杂推理问答，助力LLM高质量训练，显著优于现有方法，各项评测全面提升。代码即将发布。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的训练面临高质量、复杂推理能力问答对稀缺的问题，尤其在专门领域（如生物医药、法律等）数据匮乏。现有方法难以生成可控、高复杂度的逻辑推理问答，从而限制了LLM进一步提升理解和推理能力。

Method: 提出了Semantic Bridge通用框架，首次实现了可控地从任意来源（如PubMed、法律文档）生成复杂多跳推理问题。核心创新在于“语义图编织”，包括实体桥接、谓词链桥接和因果桥接三种机制，以及基于AMR（抽象意义表示）的复杂度和类型细致控制。

Result: Semantic Bridge的多模态AMR流程使问答生成轮回质量提升9.5%。在公开和专属领域数据集（如Wikipedia、生物医药）上，方法在四种语言下比基线提升18.3%-25.4%。用200个原始资料生成的问题对，比600个人工标注样本有更好表现，用料减少67%。人工评测在复杂度、可答性、模式覆盖方面提升23.4%、18.7%、31.2%。

Conclusion: Semantic Bridge建立了面向LLM训练数据合成的新范式，能从稀疏数据可控生成高质量多跳推理问答，有效推动LLM推理能力进步，代码和模型即将开源。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [27] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 本文提出了PersonaEval，专门评测大模型的角色识别能力。实验证明，当前LLM在角色识别上远逊于人类，难以作为高质量角色扮演评测的裁判，需进一步提升推理与理解能力。


<details>
  <summary>Details</summary>
Motivation: 目前角色扮演评测中经常用到“LLM-as-a-judge（大模型担任裁判）”的方法，但这些方法未经充分验证，可能无法反映人类对于角色扮演真实性的感知。而进行人类一致性评测的前提是角色识别，即能够根据对话内容判断谁在说话。作者认为，对角色扮演质量的评判，首先要能正确归属角色的话语和行为。

Method: 作者提出了PersonaEval，这是首个测试LLM能否可靠识别角色身份的基准。该基准收集了来自小说、剧本与视频对话的人类原创对话，要求模型依据对话内容判断正确的角色身份。实验同时包含了人类参与的对照测试。

Result: 实验结果显示，即使是表现最好的LLM在角色识别任务上的准确率也只有约69%，远低于可靠评测所需的水平。而人类参与者准确率达到了90.8%，表明当前LLM与人类在这一能力上有明显差距。

Conclusion: 目前主流LLM无法像人类一样，准确地对对话角色进行判断，从而难以胜任角色扮演质量的裁判任务。要弥补这一差距，仅靠针对任务的微调和推理计算还不够，更需要模型具备强大的类人推理能力。作者公开了该评测基准，为后续研究提供工具。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [28] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本论文提出了首个中文多领域语音-文本双模态TOD数据集RealTalk-CN，包含丰富真实语音特征与场景，为中文语音LLM研究奠定基础并验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有面向任务的对话系统（TOD）中的数据集主要以文本为主，缺少真实语音数据，难以评估语音驱动的大语言模型（LLM）的鲁棒性。此外，现有的语音TOD数据集主要集中于英语，并且缺乏语音不流畅和说话人变异这样的实际对话特征。

Method: 提出并构建了RealTalk-CN数据集，这是首个中文多轮、多领域、语音-文本双模态的TOD数据集，包含5.4k对话（共60K句子，150小时语音），为每句话都配对了语音和文本标注，还专门标注了自发语音中的不流畅现象。进一步提出了跨模态对话任务，支持语音与文本动态切换。

Result: 该数据集涵盖多样的对话场景和说话人，能真实还原语音不流畅等复杂现实语音环境。通过大量实验验证了该数据集在魯棒性、对说话人特征的敏感性、跨领域泛化等方面的有效性，为中文语音LLM研究提供了基础。

Conclusion: RealTalk-CN填补了中文语音对话领域数据集的空白，是开展语音驱动中文LLM相关研究的重要基石。通过其丰富的标注和覆盖真实交互特性，可促进多模态对话系统的进展。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [29] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 该论文提出了一种无需额外训练的新型多模态大语言模型编排方法，通过中央控制器和模块化架构，显著提升多模态交互能力、响应速度和系统可解释性，表现优于传统联合训练方法。


<details>
  <summary>Details</summary>
Motivation: 以往多模态大模型难以直接集成到统一的输入输出系统中，主要因模态对齐、文本到语音效率等集成问题，需要额外训练。该论文旨在解决这些集成困难，提升系统效率和交互体验。

Method: 提出了多模态大语言模型编排（MLLM Orchestration）方法：1) 使用中央控制器LLM分析用户输入，通过设计的Agent动态分派任务到各专用模型；2) 并行的文本到语音架构，实现真正的全双工交互和自然中断处理；3) 跨模态记忆整合系统，智能信息合成检索，选择性避免非必要的模态调用以提升响应速度。

Result: 无需额外训练即可实现综合多模态能力，比传统联合训练方法在标准基准上性能提升最多7.8%，延迟降低10.3%，且通过显性编排过程显著提升系统可解释性。

Conclusion: MLLM编排方法通过显式流程协调多模态模型，保持模块化、提升解释性和计算效率，无需补充训练即可获得优异多模态交互与性能表现。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [30] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 本文提出用范畴同伦理论来改进LLM对同义语句的概率分布一致性，通过构建LLM马尔可夫范畴，并用同伦技术处理语义等价，为统一处理自然语言同义表达提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 自然语言中存在许多表面不同但语义相同的表达（如“Charles Darwin wrote”和“Charles Darwin is the author of”），但大型语言模型（LLM）通常不会对这些语句生成相同的下一个单词概率。为了解决这一语义同等表达下概率一致性的问题，学界已有一些经验性方法，但仍未彻底解决该抽象问题。

Method: 本文引入了范畴同伦（categorical homotopy）的理论框架，具体提出了LLM马尔可夫范畴来表示LLM生成语言的概率分布。通过在马尔可夫范畴中定义句子的概率，进一步利用范畴同伦技术捕捉“弱等价”，以解决语言中大量等价表达带来的非同构箭头难题。

Result: 论文详细介绍了如何将范畴同伦应用于LLM，包括从高阶代数K-理论到模型范畴（一种抽象类别理论结构），并展示了该方法如何理论上统一处理同义表达的概率预测。

Conclusion: 使用范畴同伦理论，可以为LLM的语义等价表达建立更合理的概率分布抽象，理论上提升了模型在处理同义重述时的健壮性和表达一致性。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [31] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文针对小型语言模型推理能力弱的问题，提出了理解与推理解耦的DURIT算法，通过标准化问题空间降低语言多样性影响，实验验证该方法能显著提升小模型在各种推理任务上的准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLM）在推理能力提升方面面临挑战，主要障碍在于自然语言的复杂性和多样性，导致模型既需理解问题本质，又需执行推理，增加了问题空间的噪声和优化难度。

Method: 提出了一个将理解与推理解耦的新框架，将自然语言问题映射到标准化的规范问题空间，并设计了DURIT算法，通过以下三步迭代执行：1）利用强化学习进行问题映射，2）通过自蒸馏对齐推理轨迹，3）在规范问题空间训练推理策略。整个过程采用mapper和reasoner交替协同训练。

Result: 实验结果表明，DURIT显著提升了SLM在数学和逻辑推理任务（包括域内和域外任务）上的表现，并增强了推理的稳健性。

Conclusion: 将理解与推理进行解耦是一种有效增强SLM推理能力的策略，DURIT框架能够改善小模型在复杂推理任务中的表现和鲁棒性。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [32] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT框架提升了大模型在联邦学习中的推理能力和可解释性，解决了隐私、资源与通信等挑战，尤其适合医疗等高标准场景。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中提升大模型推理能力非常困难，尤其是在医疗领域，需兼顾性能、隐私、可解释性与合规。现有的联邦微调方法关注输出正确性，忽略推理过程，且常用的知识蒸馏方法存在隐私泄露风险。传统方法还存在通信开销大的问题。

Method: 提出FedCoT框架，通过轻量级链式推理增强机制：本地模型生成多条推理路径，紧凑鉴别器动态筛选最优路径。同时采用改进版LoRA模块叠加与客户分类感知的聚合方式，实现高效、无噪音的跨客户聚合。

Result: 在医疗推理任务上，FedCoT在保证数据隐私的同时，显著提升了客户端推理性能，并且有效应对了资源限制及客户异质性问题。

Conclusion: FedCoT为联邦环境下大模型的推理与可解释性提升提供了有效方案，尤其适用于有高隐私和高可解释性需求的医疗场景。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [33] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: LATTE通过对比学习对齐行为序列嵌入与LLM语义嵌入，实现了高效准确的金融事件表示学习，优于现有方法且更易落地。


<details>
  <summary>Details</summary>
Motivation: 直接对长事件序列应用大语言模型虽然能捕获丰富知识，但在实际金融场景下推理开销大，效率低，难以部署；需要设计既能利用LLM强大理解能力，又能兼顾实际可用性的方案。

Method: 提出LATTE框架，利用对比学习将原始事件嵌入与LLM生成的语义嵌入对齐；先将行为特征总结为简短prompt，再由LLM计算嵌入，通过对比损失监督模型训练。

Result: 在真实金融数据上，LATTE在事件序列表示学习任务中超过了现有最优方法，并且大幅降低了计算成本和输入长度，适合对延迟敏感的实际环境。

Conclusion: LATTE能够高效地从历史通信中学习用户嵌入，并在金融相关任务表现优异，兼顾了准确性与推理效率。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [34] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本文通过将显著性检验与共形预测方法结合，有效提升了大型语言模型在多项选择题问答中的可靠性与可解释性，并在主流基准测试上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多项选择题问答（MCQA）领域越来越常用，但其幻觉和非事实性生成显著降低了答案的可靠性。尽管共形预测（CP）和显著性检验各自具备统计保障，但它们的结合尚未被系统研究。为增强LLMs在高风险问答场景下的可信度，需要新的统计方法来缓解幻觉和事实错误。

Method: 提出了一种基于显著性检验强化的共形预测（CP）框架，将$p$值计算与一致性打分相结合（自洽重采样方式），通过统计选项出现频率来处理LLMs的“黑箱”特性，并利用经验$p$值进行原假设检验($\mathcal{H}_0$)构建预测集。

Result: 在MMLU和MMLU-Pro基准测试上的实验表明：（1）增强后的CP方法能实现用户设定的经验性覆盖失配（miscoverage）率；（2）随着风险水平($\alpha$)增加，平均预测集大小(APSS)单调递减，证明APSS是有效的不确定性衡量指标。

Conclusion: 该工作提出了一个兼具统计严谨性和有效性的框架，为在高风险领域可靠部署LLMs提供了理论基础和实践支持。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [35] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC是一种通过奖励模型自适应选用TTC策略的新框架，能显著提升LLM推理表现且计算效率更高，优于单一RAG或TTT。


<details>
  <summary>Details</summary>
Motivation: 现有TTC方法提升LLM性能，但不同查询最佳方案不同，统一应用既浪费计算资源又未必最优，因此需机制动态选择TTC，并提高效率。

Method: 通过预训练奖励模型，动态选择每条查询最适合的TTC策略（如RAG或轻量级微调），采用分布式架构，并引入历史状态缓存减少冗余计算。

Result: 在多种语言模型和基准测试上，RTTC在准确率上均超过传统RAG及TTT方法，展现良好可扩展性和高性能。

Conclusion: RTTC能够通过奖励模型引导，实现针对每个查询自适应选择最优的TTC策略，在多种任务和领域显著提升LLM推理准确率。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [36] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本研究基于自然语言和机器学习技术，提出了一种高效、可解释的产后抑郁筛查系统，在检测准确率上显著超越现有方法，为临床快速发现和干预PPD提供了技术支持。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症（PPD）严重影响产妇身心健康，因此需要能够快速检测PPD及其风险因素，以便及时评估与干预，推动专业预防措施的实施。

Method: 该研究提出并构建了一个智能化PPD筛查系统，结合自然语言处理、机器学习和大语言模型，实现基于自由言语分析的实时、低成本、无创检测。此外，利用可解释的树模型算法和大语言模型输出，使预测结果对终端用户更具解释性。

Result: 本系统在PPD检测任务上获得了90%的评价指标结果，优于现有相关文献中的竞争方法。

Conclusion: 所提出的方法能够有效且及时发现孕产妇PPD及其风险因素，助力临床实现快速评估和早期精准干预。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [37] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: 本文提出SABER，通过强化学习让大模型推理过程支持多样化预算和可控模式，实现推理速度与深度灵活权衡，在数学、逻辑等任务上大幅降低计算消耗同时提升或保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂任务上虽然通过链式思维获得了高准确率，但推理速度慢、计算成本高，且每个问题都应用统一推理方式缺乏灵活性。作者希望实现更加灵活、高效的推理，允许用户控制推理深度和资源消耗。

Method: 提出SABER框架，通过强化学习让LLM按用户设定的token预算进行推理。训练时先分析每个样本使用的token量，将其分配到不同预算等级，再用系统prompt与奖励机制驱动模型遵循预算；此外引入不推理样本，保证无需显式推理时模型依然可靠。SABER支持四种推理模式（不推理、快速推理、基础推理、深度推理），让推理速度和深度可灵活权衡。

Result: 在数学推理（MATH, GSM8K）、代码生成（MBPP）、逻辑推理（LiveBench-Reasoning）等任务上广泛评测，SABER在有限token预算下依然准确率高，实现了优雅的性能衰减、跨任务与跨领域泛化。尤其在MATH基准上，SABER-FastThink推理长度减少65.4%，准确率比原模型提升3.6%。

Conclusion: SABER框架大幅提升LLM推理灵活性与效率，实现了用户可控的推理预算和模式，在不牺牲精度的前提下大幅减少计算消耗，为高效应用LLMs提供可行路径。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [38] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 通过融合NLP技术与语言学特征，以及利用合成语音进行数据增强，显著提高了阿尔兹海默症语音检测准确率，但多模态模型仍有待改进。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆（ADRD）影响大量美国老年人，但超过一半未被及时诊断。利用基于语音的自然语言处理（NLP）技术检测早期认知衰退具有可扩展的应用潜力。

Method: 提出并评估了筛查流程：（1）融合transformer嵌入与手工语言学特征；（2）采用由大语言模型（LLM）生成的合成语音进行数据增强；（3）对单模态及多模态LLM分类器进行基准测试。使用DementiaBank语料，比较多种模型及策略。

Result: 融合模型F1达到83.3（AUC=89.5），优于单一语言学或transformer模型。用MedAlpaca-7B合成语音扩充数据后，F1进一步提升至85.7。LLM微调大幅提升分类性能（例如MedAlpaca F1从47.3升至78.5）。当前多模态模型表现偏低（如GPT-4o F1=70.2），并与合成语音与真实语音的分布相似度有关。

Conclusion: 结合transformer嵌入与语言学特征显著提高了ADRD语音检测效果。针对临床优化的LLM对分类和数据增强均有良好支持，但多模态建模仍需技术突破。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [39] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF 是一种无需金标准的个性化文本生成评估新框架，能高效结合通用质量和个体偏好，实验表明它比现有方法更贴合用户需求且标定效果更好。


<details>
  <summary>Details</summary>
Motivation: 当前个性化文本生成系统的评测方法难以充分考量用户个体差异，缺乏高效且无需金标个性化答案的衡量手段。此研究旨在解决这一评估瓶颈，实现更贴合用户需求的自动评价。

Method: 提出了 PREF 框架。它通过三步流程：1) 利用大语言模型生成通用评价准则；2) 基于用户画像调整准则形成个性化评价量表；3) 利用模型评分，实现针对用户主观偏好的评测。

Result: 在 PrefEval 基准和隐含偏好跟随任务上，PREF 相比主流方法展现出更高的评测准确率、更优标定能力和与人的判断更高的一致性，同时具备良好的可扩展性与复用性。

Conclusion: PREF 框架为个性化文本生成系统提供了无需参考答案的高效评估方法，不仅提升了准确性和可解释性，还与用户主观判断高度一致。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [40] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了LFJ攻击方法，通过隐藏状态插值实现高效越狱，突破主流LLM安全防护，防御方案可显著减少攻击成功率，对模型性能几乎无影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）表现出强大的语言任务能力，但容易受到绕过安全机制的“越狱”攻击。

Method: 提出了Latent Fusion Jailbreak (LFJ)，一种基于表征的攻击方法，通过插值具有高主题和句法相似性的有害与正常查询对的隐藏状态，以诱导模型生成违禁回复。该方法包括选择合适的查询对、在关键层和token处进行梯度引导的插值，并通过优化实现攻击成功率、输出流畅性与计算效率的平衡。同时提出了对抗训练防御，通过在插值样例上微调模型来防御LFJ攻击。

Result: LFJ在Vicuna和LLaMA-2等模型上测试，平均攻击成功率达到94.01%，优于现有方法。所提防御策略能够将攻击成功率降低80%以上，同时保持对正常输入的性能不变。消融实验验证了查询对选择、隐藏状态插值和优化策略对LFJ有效性的关键作用。

Conclusion: LFJ是一种高效的表征层面攻击方法，能够突破LLM安全防护，并通过对抗训练手段有效防御该攻击，无显著影响模型正常性能。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [41] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文发现prompt优化与推理策略相互影响，提出IAPO框架和PSST算法，能够根据预算及多目标任务联合优化prompt和推理设置，在多项任务测试中显著提升LLM对齐效果，强调推理感知在prompt优化中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的prompt优化方法在提升大型语言模型（LLM）对齐效果上取得了进展，但这些方法通常忽略了推理策略（如Best-of-N采样、投票法）与prompt优化之间的相互影响。本研究发现二者存在显著相关性，且用户对于推理预算和多目标之间的权衡也会影响最终选择。现有方法存在方法学缺口，需要能够综合两者优化的新框架。

Method: 本文提出了一个统一的新框架IAPO（Inference-Aware Prompt Optimization），可根据不同推理预算和多任务目标，联合优化prompt和推理规模。为此，开发了固定预算下的训练算法PSST（Prompt Scaling via Sequential Trimming），并对其误差概率在有限预算下给出理论保证。

Result: 通过六个不同任务（涵盖多目标文本生成与推理）验证，PSST有效提升了黑盒LLMs的对齐表现，结果表明在prompt优化时纳入推理感知对齐具有重要价值。

Conclusion: 本文证明了prompt优化与推理策略存在强关联，并提出了IAPO和PSST方法，实现了更高效和自适应的prompt及推理策略联合优化，在多任务/预算约束下表现优异，有助于更好地对齐黑盒LLMs。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [42] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本文发现LLMs开启思考模式反而更易遭受Jailbreak攻击，特别是在教育话题和长篇回答时。作者提出一种通过添加特定标记引导LLMs安全思考的方法，实验表明可显著降低被攻击概率，对LLMs安全研究有重要启示。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs中思考模式在Jailbreak攻击下的脆弱性，发现以往忽视的安全隐患。

Method: 评估9个LLMs在AdvBench和HarmBench上的表现，通过大量样本分析攻击成功的特征，并提出通过在提示中加入“特定思考标记”来引导LLMs内部思考流程的安全干预方法。

Result: 发现LLMs开启思考模式后更易被Jailbreak攻击，尤其教育话题和超长思考长度更易被利用，并且LLMs明知问题有害时仍给出有害答案；安全思考干预显著降低了思考模式下的攻击成功率。

Conclusion: LLMs思考模式虽有价值但存在重大安全隐患，为此提出的安全干预措施能够有效缓解被攻击的风险。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [43] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: APIE框架通过双重不确定性指标，主动选取最具难度与信息量的示例，有效提升LLM在少量示例信息抽取中的表现，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在少量示例下的信息抽取任务中表现优异，但其性能受上下文示例选择影响很大。传统选择策略未能充分利用模型失败的关键原因，包括语义内容和结构化格式生成上的困惑，因此需要更有效的示例选择方法。

Method: 提出Active Prompting for Information Extraction（APIE）框架，通过模型自我评估困惑度来主动选择示例。该方法设计了双重不确定性指标：格式不确定性（正确生成语法的难度）和内容不确定性（语义抽取的一致性），并根据综合得分从未标记数据主动选取最具挑战性的示例作为few-shot范例。

Result: 在四个基准测试上，所提出的APIE方法在信息抽取准确率和稳健性方面均优于现有强基线方法，取得了显著提升。

Conclusion: 引入格式与内容双重不确定性的细粒度判别方式，对构建高效、可靠的结构化生成系统至关重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [44] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一个创新的多语言常识推理基准mSCoRe，并在多种主流大模型上测试发现当前模型在复杂多语言常识推理上仍有较大不足，未来应关注模型多语言推理技能的提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在复杂推理任务上取得了显著进展，但对其如何利用不同人类推理技能的机制，特别是多语言常识推理领域的研究还很不足。该领域涉及跨语言、跨文化的日常知识，因此需要更细致的评测方法。

Method: 作者提出了mSCoRe基准，包括三个主要组成部分：（1）推理技能新分类法，用于细粒度分析模型推理过程；（2）专为常识推理评测而设计的数据合成流程；（3）复杂度动态提升框架，以适应未来大模型能力的提升。

Result: 对八个不同体量和训练方法的先进LLM进行测试后发现，mSCoRe对现有模型，尤其在高复杂度时仍然构成重大挑战。

Conclusion: 结果显示，当前大模型在处理细腻、多语言、文化交织的常识推理任务时存在明显局限。本文还深入分析了模型推理过程，指出多语言常识推理改进的未来方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [45] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 该论文针对LLMs在复杂多轮交互及不完整信息环境下的不足，设计了新的多轮任务基准测试并对主流模型进行评估，发现模型在指令遵循、推理和规划方面存在明显短板，为后续能力提升提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）善于处理明确且完整的问题，但在现实中更为复杂、交互式的环境中表现不佳。因此，亟需开发能够进行多轮逻辑一致对话、在信息不完整情况下推理和主动获取信息的LLMs。

Method: 本文提出了一个新型基准测试，包含多种多轮任务，专为考察推理、交互对话及信息获取能力而设计。同时，采用确定性的评分机制，无需人工干预。通过在该基准上对主流模型进行评估。

Result: 评估发现，当前先进模型在这些复杂任务上还有很大提升空间。分析指出，大多数错误源于指令遵循不佳、推理失败与规划能力弱。

Conclusion: 提出的基准揭示了当前LLMs在处理复杂交互场景时的优势和不足，为未来相关关键能力的研究和提升提供了有力平台。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [46] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 论文提出LaaJMeter模拟框架，用于系统分析和验证LLM评审在特定领域的评估指标有效性和门槛，证明其在代码翻译等低资源任务中的实用性，指出现有指标局限并提升评估方法的可扩展和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在领域专有场景中评估LLM作为评审者的表现困难重重，因为缺乏高质量标注数据且专家评估代价高昂。现有评估指标往往未经特定领域验证，导致难以判断哪些指标有效及评审能力门槛。

Method: 提出一种基于模拟的LaaJMeter框架，可生成虚拟模型与评审者，通过合成数据系统性分析不同评估指标在各种场景下的表现，帮助确定指标优劣及评审适用阈值。

Result: 在代码翻译任务中，实证展示各种指标对评审质量敏感度不同，揭示常用指标的局限和科学选择指标的重要性。

Conclusion: LaaJMeter框架为低资源场景下LLM评审提供了可扩展解决方案，促进NLP中评估的可信性和可复现性。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [47] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本论文提出了基于翻译难度的自动文本筛选框架与评估指标，开发了领先的难度估计模型（Sentinel-src-24与25），能显著优于传统启发式和LLM评判法，用于定位和构建更具挑战性的机器翻译评测集。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译的质量在某些场景下接近完美，高质量的译文让区分不同前沿模型和寻找改进空间变得更加困难。因此，自动识别机器翻译系统难以处理的文本，对于提升评估区分度和指导后续研究变得十分重要。

Method: 正式提出了“翻译难度估计”这一任务，并基于期望翻译质量来定义文本难度。提出了新的难度估计器评估指标，并对现有方法和新方法进行了评估，开发了专用的难度估计模型（Sentinel-src系列），并与常见启发式方法及大模型评审法进行了对比。

Result: 实验结果显示，专为翻译难度估计设计的模型（Sentinel-src）优于基于单词罕见性、句法复杂性等启发式方法及以大语言模型为评委的方法。同时，发布了两款改进后的难度估计模型：Sentinel-src-24和Sentinel-src-25。

Conclusion: 所提出的专用翻译难度估计模型能够更好识别对当前机器翻译系统具有挑战性的文本，有助于自动构建更具挑战性的翻译基准，推动机器翻译研究进步。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [48] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 本文提出了一种无需梯度、仅依靠前向传播即可实现高效、可扩展训练样本影响力评估的新框架For-Value，理论与实验表明效果优异，适用于大规模语言和视觉-语言模型的数据价值计算。


<details>
  <summary>Details</summary>
Motivation: 现有数据价值估算方法对大模型计算消耗巨大（依赖Hessian或模型重训练），亟需更高效可扩展的方法以提升LLM和VLM的透明性和可解释性。

Method: 提出了一种仅需前向传播的影响力估计方法For-Value，通过现代基础模型的隐藏表征和预测误差，利用闭式表达式计算每个样本的影响分数，无需耗时的梯度计算。

Result: 理论分析证明此方法能够准确评估样本影响；实验证实For-Value在识别关键微调样本和异常标签检测等方面达到或超越主流基线方法。

Conclusion: For-Value 框架能够有效且高效地对单个训练样本的影响进行估算，结果与现有基于梯度的方法相当甚至更优。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [49] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 提出专为巴基斯坦语境设计的PakBBQ数据集，用于多语言LLM公平性评估；结果显示，语境明确和负面提问能有效减缓模型偏见，乌尔都语更优，在低资源环境中有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有大模型训练和评估多基于西方数据集，忽略了低资源语言和地域语境，难以保障所有用户社区的公平性。本文旨在弥补低资源地区（尤其是巴基斯坦）公平性评估的空白。

Method: 构建PakBBQ数据集，扩展原BBQ基准，涵盖巴基斯坦相关的8类偏见维度（如年龄、性别、宗教等），包含英语和乌尔都语的共17,180个QA对。测试多语言LLM在不同语境和问题表达下的偏见及公平性表现。

Result: （1）明确语境带来平均12%的准确率提升；（2）乌尔都语下模型去偏表现优于英语；（3）负向问题表达可显著减少刻板印象型回答。

Conclusion: 上下文化基准和简单的prompt工程对于低资源场景的大模型去偏见至关重要，不仅提升评测公平性，还能有效缓解刻板印象。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [50] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [51] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本论文比较了四种深度学习模型在推文emoji预测任务中的表现，BERT成绩最好，CNN在稀有类别上突出。合理模型选择和调整能提升情感识别和人机交互。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的普及，用户越来越多地使用emoji表达情感和意图，因此自动预测文本中应使用哪种emoji变得重要，以增强人机交互体验。

Method: 本文基于TweetEval数据集，分别使用四种深度学习架构（前馈神经网络、卷积神经网络CNN、Transformer和BERT）进行emoji预测。同时，通过focal loss和正则化方法应对类别不平衡问题。

Result: 实验结果显示，BERT模型由于预训练的优势在总体性能上表现最佳，而CNN在稀有emoji类别上表现更优。

Conclusion: 本研究表明，选择合适的模型架构和优化超参数对于情感感知的emoji预测具有决定性作用，能够有效提升人机交互体验。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [52] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 本研究用大语言模型分析临床访谈文本，准确预测精神分裂症高危患者BPRS量表分数，与人工评分接近，且适用于多语言场景，有望改善精神障碍患者的评估流程。


<details>
  <summary>Details</summary>
Motivation: BPRS量表虽广泛用于精神分裂症和其他精神障碍的症状测量，但在实际临床中因面访耗时常被忽略。亟需便捷、准确的量表评分方式以便实时监测高危患者症状，指导疗效评估和治疗决策。

Method: 利用大型语言模型（LLM），对409名来自AMP-SCZ队列的高危患者的临床访谈文本进行分析，以预测患者的BPRS量表分数。模型评估包括零样本学习性能、跨语言准确性，以及整合纵向访谈信息的一步或少步学习能力。预测结果与人工评分进行一致性和可靠性对比。

Result: LLM直接从访谈文本零样本预测BPRS分数，与真实评分表现高度一致（中位一致性0.84，ICC 0.73），接近人工评分者间和评分者内的一致性。LLM在外语访谈场景下也取得较高准确度（中位一致性0.88，ICC 0.70）。模型可通过整合纵向信息进一步提升评估能力。

Conclusion: 大型语言模型能够高效、准确地预测临床高危患者的BPRS量表分数，可用于提高精神障碍患者症状监测的自动化、标准化水平，具备临床应用前景。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [53] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本研究通过语料库和计算方法，发现Toki Pona这种人造语言在社群使用过程中会自然变化，其演变机制类似于自然语言。


<details>
  <summary>Details</summary>
Motivation: 探究托基波那（Toki Pona）这种人造语言在实际使用中的变化与多样性，以及其是否会像自然语言一样受到社会语言学因素的影响、发生演变。

Method: 采用计算与语料库方法，分析Toki Pona中词类灵活、及物性等特征，重点研究内容词在不同句法位置的变化趋势以及不同语料库之间的用法差异。

Result: 社会语言学因素与自然语言一样，会影响Toki Pona的语言使用；即使是人造语言，只要被社群持续使用，也会自然演变。

Conclusion: Toki Pona作为人造语言，在社群中使用时会表现出类似自然语言的变化和多样性，表明人造语言具有演化的自然倾向。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [54] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 通过让LLM参与自身prompt的生成，以符合模型的归纳偏置，能显著提升其在分类与排序任务上的评分表现。


<details>
  <summary>Details</summary>
Motivation: 由于LLM对prompt措辞非常敏感，现有研究提升prompt工程表明模型内在的归纳偏置影响结果，因此需要一种可以自然利用模型归纳偏置的方法，以提升prompt效果。

Method: 提出了一种Inductive Bias Extraction and Matching（归纳偏置提取与匹配）策略，将LLM的部分输出用作其后续输入的prompt，以更好地契合模型的归纳偏置。

Result: 实验表明，该策略可以将LLM Likert评分在分类任务上提升最多19%，在排序任务上提升最多27%。

Conclusion: 使用从LLM自身输出中提取的归纳偏置来调整prompt，可以提升LLM在分类和排序任务中的表现。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [55] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 定量偏见检测局限明显，本文通过人工定性话语分析显示LLM在性别与种族刻画上存在深层结构性偏见；表面纠偏难改变其本质。建议AI设计和应用引入定性、跨学科的批判性方法，以消弭技术加剧的不平等。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，大型语言模型（LLM）在各类应用场景中日益普及。然而，伴随模型升级，其是否重现了性别和种族等偏见，维持了主导话语体系，成为亟需评估的问题。现有偏见检测多依赖量化和自动化方法，容易忽略语言中细腻和隐晦的偏见表现。

Method: 提出并采用一个定性话语分析框架，通过人工分析LLM生成的涉及黑人女性和白人女性的短篇故事，深入探究模型输出中的性别和种族偏见，并与量化检测方法形成互补。

Result: 结果发现，模型对黑人女性的刻画多与祖先、抗争相关，而对白人女性则侧重自我发现过程。这种差异反映了语言模型复制固化的话语再现，加剧了本质化和社会固化的问题。在要求模型纠正偏见时，模型仅进行表面修正，问题内涵并未彻底改变，显示出其推动包容性叙事的局限性。

Conclusion: 算法本身具有意识形态功能，其输出可能加剧社会不平等。为提升AI的伦理性及公平性，亟需融合定性分析与跨学科批判性方法，深入理解和消解LLM生成话语中的偏见。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [56] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 本论文提出了ReviewRL，用于自动生成科学论文评审。通过结合文献检索、监督微调和强化学习，系统在评审质量和评分准确性上均优于现有方法，展现出自动化科学评论生成的技术前景。


<details>
  <summary>Details</summary>
Motivation: 科学论文的同行评审对于学术进步至关重要，但由于投稿数量激增与评审疲劳现象，现有自动化评审方法难以满足高质量评审的需求，尤其是在事实准确性与分析深度方面存在不足。因此有必要开发更精确且能产出更深入分析的自动化评审工具。

Method: 提出了ReviewRL，一个基于强化学习的科学论文自动评审框架。该方法包括：1）基于ArXiv-MCP的检索增强型上下文生成流程，整合相关科学文献；2）监督微调以建立基础评审能力；3）复合奖励的强化学习过程，兼顾评审质量与评分准确性。

Result: 在ICLR 2025的论文上测试，ReviewRL在规则评测指标和模型评测质量上均显著优于现有方法。

Conclusion: ReviewRL奠定了基于强化学习的自动科学论文评审的基础框架，在自动评论生成方面展现出巨大潜力，未来有望进一步推动科学发现和自动评审技术的发展。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [57] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: 本文提出了基于表格的语义文档解析框架DOTABLER，实现了表格与上下文之间的深层语义关联挖掘，各项指标超过90%，优于当前主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要关注文档的表层任务，如版面分析、表格检测和数据抽取，缺乏对表格及其上下文之间的深层语义解析，因而限制了跨段落数据解释和语境一致性分析等高级任务的发展。

Method: 提出DOTABLER，一个以表格为中心的语义文档解析框架，通过定制数据集和特定领域微调的预训练模型，以及集成的完整解析流程，识别与表格语义相关的上下文片段，实现表格与上下文的深层语义关联解析。

Result: 基于近4000页、1000余张真实PDF文档的评测，DOTABLER在表格-上下文语义分析和深度文档解析方面，Precision和F1指标均超过90%，优于包括GPT-4o在内的先进模型。

Conclusion: DOTABLER能够高效、精确地实现基于表格的语义文档解析和领域化表格检索，拓展了表格语义理解的深度和文档高级分析的能力。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [58] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 本文发现LLM推荐系统会逐层削弱协同信号。作者提出FreLLM4Rec，利用频谱方法净化和增强协同信号，其协同增强机制具理论保证。实验证实该方法在主流数据集上效果优越，最高提升8%。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的推荐系统在生成具有语义信息的推荐时表现突出，但存在过度强调用户历史交互中的语义相关性，导致协同信号（如用户间的隐含相似性）在模型层层传播中逐渐减弱，影响了推荐效果。对比传统Transformer类模型，LLM在协同信号的保留和增强方面存在一定局限。

Method: 提出FreLLM4Rec方法，从频谱角度综合平衡语义信息和协同信息。具体包括两步：1）用全局图低通滤波器（G-LPF）对结合了语义和协同信息的物品嵌入进行纯化，去除无关的高频噪声；2）设计时序频率调制（TFM）机制，逐层主动保留协同信号，并从理论上证明了TFM的协同信号保存能力，将本地图傅里叶滤波（难以实现但最优）与高效的频域滤波器建立联系。

Result: 在四个基准数据集上，通过FreLLM4Rec显著减缓了协同信号衰减，性能优于所有对比基线方法，在NDCG@10指标上最高提升8%。

Conclusion: FreLLM4Rec有效解决了LLM推荐系统中协同信号衰减的问题，在理论和实践上均带来了推荐效果的提升；为提升基于LLM的推荐系统提供了新见解和系统性方法。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [59] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出XPE和Dual Soft Prompt，提高了大模型跨语言任务—尤其是低表现语言上的表现，有效推进了prompt encoder在多语种领域的应用。


<details>
  <summary>Details</summary>
Motivation: 过去PEFT方法如soft prompts在不改变架构或参数的情况下，提升LLM下游任务适应性，但其跨语言迁移潜力尚未被深入探索，尤其是对低表现语言的提升。

Method: 提出Cross-Prompt Encoder（XPE），采用轻量编码架构并在多源（语言类型多样）数据上训练，捕捉跨语言抽象可迁移模式。同时设计Dual Soft Prompt机制，将编码器软提示和标准软提示结合，实现结构泛化与语言对齐。

Result: 在SIB-200多语言基准测试中，XPE对低表现语言提升显著；混合方案在多语言环境下适应性更优。

Conclusion: XPE和Dual Soft Prompt机制能有效提升低表现语言和多语言下的任务性能，扩展了soft prompt的跨语言适用性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [60] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出针对Qwen3 14B的大型语言模型韩语本地化推理能力提升方案，两阶段微调结合有监督学习和改进型强化学习，显著提升了模型在韩语及通用推理任务中的表现，并保证训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型在处理韩语推理任务时表现有限，缺乏本地化逻辑思维能力。作者旨在使Qwen3 14B具备原生韩语推理能力，并提升其在韩语及通用推理任务上的表现。

Method: 提出两阶段微调方法。第一阶段利用高质量韩语推理数据进行有监督微调（SFT），提升模型在韩语推理上的基础能力。第二阶段采用自定义的Group Relative Policy Optimization（GRPO）强化学习算法，同时引入oracle judge模型来校准奖励信号，解决奖励作弊和策略崩溃等稳定性问题。

Result: 两阶段训练显著提升了模型在高难度推理（尤其是数学和编程）基准上的表现，同时保持知识和语言能力，内部思维链实现全韩语处理。训练过程稳定，避免了单纯GRPO方法的崩溃问题，实现了持续性能增长。

Conclusion: 本方法能有效赋予Qwen3 14B模型韩语原生推理能力，提升其在韩语及通用推理任务上的表现，同时保证训练稳定性和知识完整性。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [61] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 论文提出一种免翻译工具的新型序列到序列跨语言ABSA方法，显著提升低资源语言的分析效果，能够处理更复杂的ABSA任务，效率高、实用性强。


<details>
  <summary>Details</summary>
Motivation: 现有的ABSA方法主要针对英语，对于低资源语言的研究较少。而目前的跨语言ABSA研究多关注于简单任务且依赖外部翻译工具，存在效率和准确性问题。

Method: 提出了一种新的序列到序列方法，结合约束解码技术，针对复合型ABSA任务，无需依赖翻译工具。

Result: 新方法将跨语言ABSA的性能提升最多达10%。同时，相比大型语言模型，该方法在低资源语言上表现更优，而英语为中心的大型语言模型在这类任务上表现不佳。

Conclusion: 提出的无翻译工具方法拓宽了跨语言ABSA的应用范围，能高效且实用地处理复杂任务，是翻译依赖技术的优良替代方案。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [62] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本文用Mistral和mT5等大模型，在现代捷克语摘要任务上达成最佳结果，并首次发布捷克历史文档摘要数据集，有效推动了相关领域发展。


<details>
  <summary>Details</summary>
Motivation: 捷克语文本摘要领域，尤其是历史性文档处理，因语言复杂性和缺乏标注数据而研究不足。当前主流的英文及高资源语言摘要技术难以直接迁移，亟需针对捷克语的有效解决方案。

Method: 采用先进的大语言模型（如Mistral和mT5）对捷克文本摘要任务进行处理，并在实验中验证其在现代及历史数据集上的表现。

Result: 在现代捷克语摘要数据集SumeCzech上取得了新的SOTA（最佳）结果。同时，构建了用于历史捷克文档摘要的新数据集Posel od Čerchova，并提供了基准结果。

Conclusion: 大语言模型在捷克语文本摘要任务上表现优异，有效推动了捷克文本摘要，尤其是历史文档处理领域的研究进展。创新数据集的建立也为后续相关工作提供了数据支持和研究基础。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [63] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 论文提出了无翻译工具、基于Seq2Seq受限解码的新型ABSA方法，多任务能力强，在多语言多任务上优于现有方法，并详细分析了大语言模型的表现与局限。


<details>
  <summary>Details</summary>
Motivation: 当前的面向方面的情感分析（ABSA）在低资源语言上的表现仍然有限，而且多聚焦于英文，现有方法通常依赖于外部翻译工具，任务复杂性也受限。

Method: 提出了基于Seq2Seq模型的受限解码（constrained decoding）方法，摆脱了对翻译工具的依赖，并且能够一模型解决多任务问题。

Result: 新方法在最复杂任务上平均提升了5%的跨语言表现，多任务受限解码方法提升超过10%。在七种语言、六项任务上超过了现有最优方法，并设定了新基准。对LLM实验证明，仅精调时效果可比小型多语模型，但训练、推断时间大幅增加。

Conclusion: 该方法突破了传统跨语言ABSA的局限，尤其针对低资源语言，提升了表现并丰富了对多语情感分析的理解，也为实际应用提供了建议。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [64] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 文章提出一种结合大模型与人工辅助的恶意内容检测框架（MDH），用于数据清洗和越狱检测。同时发现针对开发者提示的设计可有效提升攻击效果，并创新性地提出D-Attack和DH-CoT两种新攻击方法，大幅提高越狱攻击成功率。相关成果将在Github公开。


<details>
  <summary>Details</summary>
Motivation: 现有红队数据集中的提示往往不够恶意，导致评估jailbreak攻击（模型越狱攻击）不准确，且恶意内容检测依赖人工标注（成本高）或大模型（准确性不稳定）。亟需高效且准确的评估攻击的方法。

Method: 提出MDH框架，结合大模型标注和少量人工审核，实现高效恶意内容检测与数据集清洗，同时用于检测越狱响应。提出两种新型攻击策略：D-Attack（利用上下文模拟），DH-CoT（劫持思维链）。

Result: MDH框架可有效提升恶意内容检测的效率和准确性。新提出的D-Attack及DH-CoT策略能显著提高越狱攻击的成功率。相关代码和数据集将在Github上公开。

Conclusion: 通过提出MDH评估框架和两种新攻击策略，提升了攻击评估手段和攻击效率，为后续安全研究和数据集清理提供更有效工具。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [65] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出SFPF框架，基于稀疏自编码器分析和扰动文本激活特征，黑盒生成高效对抗样本，能绕过现有防御手段，但效果依赖具体场景，泛化性还有待研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自然语言处理中的大量应用，也带来了模型在安全性和健壮性上的挑战。生成对抗样本以“越狱”LLMs，是揭示模型脆弱性的关键手段。当前主流的攻击和防御手段尚不足以覆盖所有安全隐患。

Method: 提出了一种全新的黑盒攻击方法——稀疏特征扰动框架（SFPF）。该方法利用稀疏自编码器（SAE）对文本的隐层表示进行重构，并对成功攻击样本进行特征聚类，识别出激活度较高的关键特征，再对这些特征进行选择性扰动，以生成新的高效对抗文本。

Result: 经实验验证，SFPF生成的对抗样本能够绕过当前最先进的防御机制，揭示了NLP系统的持续性安全漏洞。然而，该方法对不同的提示词和神经层的适用效果存在差异，且对其它模型架构和更大规模模型的泛化能力有待进一步验证。

Conclusion: SFPF框架能高效生成对抗文本，突破现有安全防线，为红队测试（red-teaming）提供了新的思路。但其跨场景及大模型泛化性仍需研究。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [66] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG通过多轮状态化检索和动态记忆池，有效提升了大模型对长篇、复杂叙事文本中全局关系和推理问题的理解表现，相比传统RAG方案有显著优势。


<details>
  <summary>Details</summary>
Motivation: 长篇叙事推理因情节复杂、角色关系交错且演变，对大模型（LLM）造成理解障碍。此外，LLM在处理长上下文时推理能力衰减、计算成本高，促使基于检索的方法成为主流。然而，传统的RAG方法由于一次性、无状态的检索过程，难以动态捕捉长文本中实体和关系的演变与关联。

Method: 提出ComoRAG方法，将叙事推理类比为人脑中记忆相关信号的认知流程，通过动态交互的内存工作区，进行多轮推理循环。在每一轮，系统生成试探性查询，探索新的信息路径，再将新检索出的证据整合进全局记忆池，逐步形成更连贯的上下文以解答复杂查询。

Result: 在四个具有挑战性的长上下文叙事基准测试集（超过20万词符）上，ComoRAG均优于强RAG基线，最高相对提升达11%。分析表明，该方法对需要全局理解的复杂问题特别有效。

Conclusion: ComoRAG为基于检索的长上下文理解提供了一种有理论支撑且受认知启发的新范式，实现了更强的状态化推理和复杂叙事理解能力。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [67] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本文系统分析了各大主流机器翻译系统在中文成语翻译方面的表现，发现错误率较高，现有自动指标评价不足。提出了Idiomeval框架和改进的错误检测方法，推动成语翻译研究的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在机器翻译领域的表现取得了很大进展，但对于中文成语翻译的研究和理解还非常有限。中文成语在日常语言中非常常见，具有特殊的结构和历史文化内涵，因此成语翻译是自然语言处理中的一大难点。该论文旨在系统性地研究并解决中文成语翻译中存在的问题。

Method: 作者提出了Idiomeval框架，设计了一个详尽的成语翻译错误分类体系。研究团队对来自GPT-4o、Google Translate等9个现代系统，在网页、新闻、百科、社交媒体4个领域下共900个成语翻译对进行了人工标注和分析。同时还评估了现有自动化评价指标与人工评分的一致性，并训练了改进型自动检测模型。

Result: 研究发现，当前主流机器翻译系统在中文成语翻译上表现不佳，经常出现直译、译文不完整或漏译等问题。表现最好的GPT-4也有28%的成语翻译错误率。此外，现有自动评价指标对于成语翻译质量的衡量能力较差，与人工评分的皮尔逊相关系数低于0.48。改进的检测模型能将成语翻译错误检测F1分数提升到0.68。

Conclusion: 中文成语翻译仍是机器翻译技术的挑战。即使在最先进的大模型系统中，成语翻译错误率依然较高，自动评价方法也需进一步优化。Idiomeval框架为深入分析和改进成语翻译提供了工具和数据支持。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [68] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 通过将大模型内部计算视为稀缺资源分配问题，并引入激励机制优化计算效率，显著提升了模型在资源受限时的表现，展现出经济学原则在AI模型设计中的实际价值。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在实际应用中受到高昂计算成本的限制，因此需要探索提高其计算效率的方法。

Method: 提出了一种“计算经济学”框架，将LLM内部视为由有限资源的智能体（如注意力头和神经元块）组成的经济体，通过分配有限计算资源以最大化任务效用。设计了基于激励的训练范式，在任务损失中加入可微分的计算成本，促进激活的稀疏性和高效性。

Result: 在GLUE（MNLI, STS-B, CoLA）和WikiText-103实验中，该方法相对同等准确度下可减少约40%的FLOPS和延迟，并使注意力模式更可解释，性能优于后置修剪方法。

Conclusion: 采用计算经济学和经济激励原则，有助于从理论和实践两方面设计出在资源受限下更高效、自适应和透明的大型语言模型。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [69] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: DiFaR解决了大模型推理单一、幻觉与无关内容问题，通过多样提示和筛选机制，实验证明大幅提升推理和检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型（LVLMs）在生成支持多模态虚假信息检测的文本推理时，面临推理缺乏多样性、内容幻觉导致事实错误、生成内容无关或冲突等问题，严重限制了其应用效果。

Method: 提出了DiFaR框架，通过五种“思维链”提示从LVLMs获得多样化推理路径，并利用轻量级的后处理模块，结合句子级事实性和相关性评分，筛选高质量的推理语句。该方法与检测器无关，能够提升推理质量。

Result: 在四个主流基准数据集上，DiFaR比四类基线方法最多提升5.9%，并能为现有检测器带来最高8.7%的精度提升。自动评测和人工评测都验证了其在多样性、事实性、相关性方面显著提升推理质量。

Conclusion: DiFaR能够输出多样、可靠且相关的推理，有效提升多模态虚假信息检测效果，且适配于不同检测器，具有实际应用价值。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [70] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文首次系统性地分析了NLP领域中隐私与可解释性的权衡，证明在特定任务与方法选择下，二者可以兼得，并提出了实用建议，有助于推动该方向的未来研究。


<details>
  <summary>Details</summary>
Motivation: 在可信自然语言处理（NLP）领域，解释性和隐私性是两个重要研究方向。然而，目前对于如何在NLP系统中同时实现解释性和隐私性的研究较少，这导致我们缺乏两者能否兼得、是否存在矛盾的理解。

Method: 本文采用实证研究的方法，聚焦于NLP中的隐私-解释性权衡，主要通过差分隐私方法以及事后可解释性技术来探究两者的关系。

Result: 结果发现隐私性与解释性的关系非常复杂，受到下游任务性质、文本隐私化方式和解释性技术选择等多种因素影响。分析显示，两者并非完全矛盾，可以在一定条件下共存。

Conclusion: 论文指出，在NLP系统中隐私保护与可解释性有潜在共存的可能，并为未来在该交叉领域的研究提供了实践性建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [71] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 多模态大语言模型严重依赖文本模态，作者首次系统性分析了各模态失衡原因，并提出token压缩方法有效缓解该问题，推动多模态模型公平发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理时对文本强依赖，其他模态利用率较低，以往主要在视觉-语言领域有相关讨论，缺乏系统性分析和一般性解决方案。本文旨在填补这一空白。

Method: 提出了两种评估多模态利用率的指标：模态主导指数（MDI）和注意力效率指数（AEI）；并通过简单的token压缩方法改善了模型的关注分布。

Result: 实验证明，多模态模型在包括图像、视频、音频、时序、图等多种数据上均存在严重文本主导，通过token压缩方法显著改善模型对各模态的关注分布。以LLaVA-7B为例，其MDI从10.23下降到0.86。

Conclusion: 本文系统性地证实了多模态大语言模型在多种数据模态下都存在显著的文本主导问题，并提出了有效的衡量指标与缓解方法，为更公平和全面的多模态模型发展奠定了基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [72] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本文介绍了eDIF平台在欧洲部署的可行性试点，通过试点研究证明该基础设施能够提升LLM可解释性研究的可接入性和社区建设，用户体验总体积极但仍有技术细节待完善。


<details>
  <summary>Details</summary>
Motivation: 欧洲LLM可解释性研究缺乏广泛可访问基础设施，亟需推动研究社区对高级模型分析能力的民主化。

Method: 通过在安斯巴赫应用科学大学部署GPU集群，并联通合作机构，提供基于NNsight API的远程模型分析，进行试点研究，邀请16位研究人员测试平台的技术性能、可用性和科学价值。

Result: 平台技术性能稳定，用户参与度逐步提升，远程实验功能获得积极评价。主要问题如下载激活数据耗时长和执行中断已纳入后续改进路线。

Conclusion: 该项目为欧洲建立LLM可解释性基础设施迈出了重要一步，为更广泛部署和持续社区协作奠定了基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [73] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 系统评估了科普特语到法语的多种翻译策略，发现针对风格多样和噪声的微调能极大提升效果，为历史语言的翻译工具研发提供了实用经验。


<details>
  <summary>Details</summary>
Motivation: 现有科普特语到法语翻译研究有限，历史语言自动翻译工具开发有迫切实际需求。

Method: 系统性地比较了中介翻译和直接翻译、预训练的影响、多版本微调的利益，以及模型对噪声的鲁棒性；利用对齐的圣经语料进行实验。

Result: 提出的流程和方法显著提高了翻译质量，并对历史语言翻译工具开发提供了重要参考。

Conclusion: 微调使用具有不同风格且适应噪声的训练语料，可以显著提升科普特语到法语的翻译质量。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [74] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 本文提出将图结构网络与Transformer架构融合的方法，用于无gloss手语翻译，在多个数据集上取得了领先的SOTA表现，有望显著改善聋人群体的交流体验。


<details>
  <summary>Details</summary>
Motivation: 全球有数百万人受到耳聋和听力障碍的影响，手语是聋人和听障人士重要的交流工具。然而在以口头语言为主的社会中，手语常常被低估，导致交流障碍和社会排斥。该研究旨在提升手语自动翻译方法，消除沟通隔阂。

Method: 该方法将图结构方法（如STGCN-LSTM）与Transformer架构融合，形成新型手语自动翻译系统，探索融合架构和多种融合策略，尤其关注无gloss（无需逐词标签）翻译。

Result: 在多个手语数据集（RWTH-PHOENIX-2014T、CSL-Daily、How2Sign和首次引入的BornilDB v1.0）上取得了新的SOTA成绩，相较于当前方法（如GASLT和slt_how2sign）BLEU-4分数提升，分别在上述数据集上超出4.01、2.07和0.5分。

Conclusion: 融合型Transformer和图结构架构能够有效提升手语自动翻译表现，显著优于现有模型，并为无gloss手语翻译和未来研究设立了新基线，有助于提升聋人和听障人士的信息获取和社会交流。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [75] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 提出用自然语言反馈（NLF）训练个性化问答模型的新框架VAC，相比常用的数值奖励，NLF提供更丰富指导，实现了更高效和高质量的个性化回答生成。实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在个性化任务中主要依赖检索增强生成（RAG）和标量奖励信号强化学习，而标量奖励信号往往反馈有限，难以显著提升模型的个性化能力和学习效率。

Method: 提出VAC框架，用以个性化生成回答。该方法用基于用户画像和问题背景生成的自然语言反馈（NLF）取代标量奖励信号。训练过程中交替优化反馈模型和fine-tune策略模型，通过更丰富的反馈信号引导模型逐步学习和内化个性化策略。

Result: 在LaMP-QA基准（三个领域）上，VAC性能显著优于当前最优方法。人类评估结果也证实了其生成回答的高质量。

Conclusion: 用自然语言反馈替代传统标量奖励信号，更有效地提升了个性化问答的准确性和生成质量。此范式为个性化语言模型训练带来了新的思路。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [76] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出ICE，针对扩散大语言模型，实现在迭代生成过程中的原位提示与提前退出机制，大幅提升推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLMs）采用的仅前缀提示和顺序生成方式，限制了其对双向信息的处理灵活性。扩散大语言模型（dLLMs）具有双向注意力机制和迭代精炼过程，为更灵活的提示策略带来了新机会。

Method: 本文提出了ICE（In-Place Chain-of-Thought Prompting with Early Exit）框架，将前缀提示转化为专为dLLMs设计的原位提示。ICE在迭代精炼期间将提示直接集成到被掩码的token位置，并通过置信感知的提前退出机制，显著减少计算开销。

Result: 大量实验表明，ICE在GSM8K数据集上可提升准确率至17.29%，同时提速4.12倍；在MMLU数据集上加速可达276.67倍，且性能竞争力强。

Conclusion: ICE框架不仅提升了dLLMs推理灵活性和效率，还大幅减小了计算开销，同时在多个基准任务上保持了或提升了准确率。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [77] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本论文提出并验证了一种基于结构化流程与LLM的新颖性自动评估方法，显著提升了与人类评审的一致性和评审质量，为高效、透明的同行评审提供了技术支持。


<details>
  <summary>Details</summary>
Motivation: 同行评审中的创新性评估一直是核心但较少研究的问题，尤其是在像NLP这样的高产领域，评审人力资源日益紧张，因此需要提升自动化和效率。

Method: 提出了一种结构化自动化新颖性评估方法，将专家评审的行为建模为三步：论文内容提取、检索与整合相关文献、基于证据的结构化比较。方法基于大量人类撰写的新颖性评审分析，仿照其关键模式进行设计。

Result: 在182篇ICLR 2025投稿及人工标注的创新性评估任务中，该方法与人工推理的一致性高达86.5%，对创新性结论的达成一致率为75.3%，显著优于现有的LLM基线模型。同时，该方法还能输出细致、具文献依据的分析，提高了评审判断的一致性。

Conclusion: 结构化的LLM辅助新颖性评估方法可大幅提升同行评审过程的严谨性和透明度，有效支持但不取代评审专家。相关数据与代码已开放。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [78] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 本文提出了一种新型后训练算法MS-GRPO，通过创新奖励分配与采样策略，显著提升了小型LLM模型在多步决策任务上的表现，效果超过相比更大参数量的模型。此方法为低成本构建高效LLM决策代理提供了实用方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）具有作为序列决策代理的潜力，但因依赖庞大且计算成本高的模型，应用受限。因此急需提升小型模型的能力，而现有的后训练方法仅适用于单步互动，无法处理多步任务中的奖励归因问题。

Method: 提出了Multi-Step Group-Relative Policy Optimization（MS-GRPO），基于正式的文本中介随机博弈（TSMG）和语言代理策略框架（LAP）。该方法将任务整体累计奖励分配到每一步，并引入了绝对优势加权采样策略以增强训练效果。

Result: 通过对3B参数模型在Snake和Frozen Lake游戏任务上进行后训练，3B模型在Frozen Lake上的表现超过了72B参数基线模型50%。

Conclusion: 该方法能够显著提升小模型的决策能力，显示精确后训练是打造高效序列决策LLMs代理的现实替代方案，无需依赖模型规模。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [79] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 本研究提出了Psyche-R1，一个融合共情、心理学知识和推理能力的中文心理学大模型，并通过创新的数据管线和混合训练策略，实现了高效、可靠的心理问答和支持表现，在多个心理学任务中达到同类大型模型的水平。


<details>
  <summary>Details</summary>
Motivation: 心理健康专业人员短缺，而大语言模型（LLM）在心理健康领域的应用有望缓解这一压力。现有LLM在数学和编程领域表现突出，但在心理学领域多聚焦于情感支持和共情对话，对于生成可靠回应所需的推理机制关注较少。

Method: 提出第一个集共情、心理学知识和推理能力于一体的中文心理学LLM——Psyche-R1。采用新颖数据合成管线，构建超7.5万条含详细推理理由的心理学问答，以及7.3万条共情对话数据，并通过多LLM交叉挑选和分组相对策略优化（GRPO）增强推理能力，其余数据用监督微调（SFT）提升共情与知识表现。

Result: Psyche-R1在多个心理学基准测试中表现优异，7B参数规模下已达到与671B参数规模的DeepSeek-R1相当的成绩，显示其实用性和高效性。

Conclusion: Psyche-R1能够有效结合共情、心理专长与推理，显著提升心理领域LLM的表现，为心理健康应用提供有力支持。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [80] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文提出了结合特征工程、数据增强和可解释机器学习的口译质量自动评估新方法，强调特征透明和模型解释。其实验在英中口译数据上获得了优异表现，且能够给出细致反馈，具有良好的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有的自动口译质量评估研究存在三个主要问题：对语言使用质量考察不足、由于数据稀缺和不平衡导致建模效果不佳、以及缺乏模型预测解释性。

Method: 提出一个多维度建模框架，结合特征工程、数据增强和可解释的机器学习方法，仅使用相关且透明的特征，并通过SHAP分析解释模型决策。

Result: 在一个新颖的英中交替传译数据集上，该方法表现出很强的预测能力。研究发现BLEURT和CometKiwi分数对忠实度预测最有效，停顿相关特征对流畅度预测最有效，汉语特有的短语多样性指标对语言使用预测最有效。

Conclusion: 该研究通过注重可解释性，提出了一种可扩展、可靠、透明的自动口译质量评估方法，其结果能够为学习者提供详细的诊断反馈，并支持自我调节学习，是传统人工评价的有益替代。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [81] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文证明大语言模型通过SSRL方法能实现无需外部搜索、高效且低成本的强化学习代理训练，在知识利用、鲁棒性及与外部工具整合方面具有突出优势。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）应用中的代理型搜索任务通常需要频繁访问外部搜索引擎，带来高昂的成本和低效问题，因此研究者希望利用大语言模型（LLMs）本身的知识和推理能力来高效地完成这一类任务，减少对外部资源的依赖。

Method: 作者提出了一套自搜索（Self-Search）评估框架，通过结构化提示和多次采样，量化LLM的内在搜索能力。在此基础上，提出了Self-Search RL（SSRL）方法，通过格式和规则奖励提升LLM的自搜索能力，实现模型内部知识的迭代利用，无需外部工具。

Result: 实验证明，LLMs具备良好的可扩展性，在问答等任务上能达到较高的pass@k，且SSRL显著提升了内在知识利用和鲁棒性，能作为成本低且稳定的RL训练环境，同时能有效减少幻觉（即不真实输出），又与外部搜索引擎良好兼容。

Conclusion: LLMs可以高效被激发出世界知识，在无需外部工具的情况下提升搜索驱动型RL训练的效果，并可降低模型幻觉，具备与外部引擎无缝协同的能力，为更大规模的RL训练提供新思路。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [82] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文对扩散语言模型（Diffusion Language Models, DLMs）进行了系统综述，重点梳理其发展历程、关键技术及应用前景，相较于自回归模型，DLM具备并行生成、低延迟等优势，已达到相似性能水平，并在推理与多模态方向有突破。文章还探讨了DLM的挑战与展望，并提供了GitHub资源入口。


<details>
  <summary>Details</summary>
Motivation: 扩展和完善对扩散语言模型（DLMs）的理解，比较其与主流自回归方法在性能和应用上的优势与不足，推动该新兴技术在NLP领域的应用与发展。

Method: 文献综述与全景分析，系统梳理扩散语言模型的历史脉络、基础理论、最新模型、预训练与微调策略，并对推理优化、生成质量提升以及多模态扩展等技术进行深度探讨。

Result: 全面总结了DLM领域的发展现状，建立了详细分类体系，分析了主流模型的技术路线及推理策略，覆盖了多模态扩展和实际应用场景，并深入评估了当前面临的挑战与未来方向。项目资源已开放至GitHub。

Conclusion: 扩散语言模型在多项任务中显示出显著优势，包括更低的延迟、更高的生成控制等；同时仍存在效率、长序列处理等待解决的问题。该综述为未来DLM的研究和应用指明了方向。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [83] [Active Automata Learning with Advice](https://arxiv.org/abs/2508.10535)
*Michał Fica,Jan Otop*

Main category: cs.FL

TL;DR: 将自动机主动学习和字符串重写推理结合，有效减少查询，提高了自动机学习效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动机主动学习方法依赖大量查询，增加了教师负担，因此提出方法以减少查询次数。

Method: 提出将主动自动机学习与演绎推理结合的学习框架，引入基于字符串重写系统的“建议”机制，辅助推断查询答案，从而减少实际查询。算法对Angluin风格的学习进行了适配改进，开销较低。

Result: 实验证明，该方法能大幅降低查询复杂度。

Conclusion: 所提出的结合演绎推理的主动自动机学习框架，可以有效减少所需查询，提高学习效率。

Abstract: We present an extended automata learning framework that combines active
automata learning with deductive inference. The learning algorithm asks
membership and equivalence queries as in the original framework, but it is also
given advice, which is used to infer answers to queries when possible and
reduce the burden on the teacher. We consider advice given via string rewriting
systems, which specify equivalence of words w.r.t. the target languages. The
main motivation for the proposed framework is to reduce the number of queries.
We show how to adapt Angluin-style learning algorithms to this framework with
low overhead. Finally, we present empirical evaluation of our approach and
observe substantial improvement in query complexity.

</details>
