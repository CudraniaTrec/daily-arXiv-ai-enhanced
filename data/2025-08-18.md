<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.CL](#cs.CL) [Total: 38]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Generic Reduction-Based Interpreters (Extended Version)](https://arxiv.org/abs/2508.11297)
*Casper Bach*

Main category: cs.PL

TL;DR: 本文提出利用泛型编程技术，减少归约解释器实现所需的样板代码，提高开发效率。


<details>
  <summary>Details</summary>
Motivation: 在实现基于归约的解释器时，虽然遵循系统化的步骤，但需要编写大量样板代码，增加了工程师的工作负担。

Method: 本文应用了泛型编程中的一些成熟技术，旨在减少归约解释器实现过程中的样板代码。

Result: 利用泛型编程技术，可以有效地精简归约解释器中需要实现的重复性代码，提升开发效率。

Conclusion: 通过将泛型编程技术引入归约解释器的实现过程，能够显著减少样板代码，为解释器开发带来便利。

Abstract: Reduction-based interpreters are traditionally defined in terms of a one-step
reduction function which systematically decomposes a term into a potential
redex and context, contracts the redex, and recomposes it to construct the new
term to be further reduced. While implementing such interpreters follows a
systematic recipe, they often require interpreter engineers to write a
substantial amount of code -- much of it boilerplate. In this paper, we apply
well-known techniques from generic programming to reduce boilerplate code in
reduction-based interpreters.

</details>


### [2] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: 本文提出一种在Futhark语言中实现高效GPU数据并行哈希映射的方法，在某些场景下优于传统方案，但由于底层和编程模型限制，不及cuCollections库，作者探讨了未来扩展函数式数组语言的方向。


<details>
  <summary>Details</summary>
Motivation: 目前在函数式数组语言中实现灵活、多态和抽象的哈希映射接口存在困难，尤其是动态键尺寸和数据并行性表达受限。亟需探索新方法以提升此类语言在GPU应用中的高效性和实用性。

Method: 采用Fredman等人的两级哈希映射构造方法进行函数式表达，并通过flattening实现数据并行。针对键的动态大小问题，通过为每个哈希映射关联任意上下文来解决。具体算法以Futhark语言实现，并在GPU上进行性能测试和基准对比。

Result: 开发的新哈希映射方案在简单基准测试中明显优于传统树/搜索方法。但在与cuCollections库比较时，构建和查找速度均落后，差距主要由编译器底层代码生成和数据并行语义限制造成。

Conclusion: 本文开发的数据并行哈希映射方案在功能上优于传统树/搜索方法，但与cuCollections库相比，在构建和查找性能上仍有差距，主要由于底层代码生成和数据并行编程模型的限制。作者建议未来可通过扩展函数式数组语言来改善这些不足。

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: 本研究分析了GitHub PR流程中GPT辅助的效果，发现可将整体解决时间减少逾60%，在审查和等待阶段分别节省显著时间。GPT主要用于优化代码和修复Bug，为团队提升效率和协作方式提供参考。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）在软件开发领域提升了生产力和团队合作效率，但其在代码审查流程中各阶段具体的影响尚未被充分研究。该研究旨在探索GPT在GitHub拉取请求（PR）工作流中是否能缩短解决时间、优化各阶段表现并协助开发者。

Method: 研究者收集了来自9,254个GitHub项目的25,473个PR，采用关键词检测、正则过滤与人工验证相结合的方法识别GPT辅助的PR，标注准确率达到95%。通过多元线性回归和Mann-Whitney U检验，分析GPT辅助与未辅助PR在整体和各阶段的解决效率差异。

Result: GPT辅助的PR中位解决时间显著降低（9小时，对比未辅助的23小时），减少超过60%。在代码审查环节减少33%时间，在等待接受阶段减少87%。GPT主要被用于代码优化（60%）、修复缺陷（26%）和文档更新（12%）。

Conclusion: 早期引入GPT能显著提升PR流程效率，为软件团队优化工作流和促进协作提供了可操作建议。

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [4] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: 本文探讨了如何获取和分析微服务系统的时序网络，但由于目前能获得的数据规模有限，时序网络分析的能力受到制约，有待进一步突破大规模时序网络采集与分析方法。


<details>
  <summary>Details</summary>
Motivation: 随着微服务架构的普及，理解微服务之间的依赖关系和动态变化对于保障系统性能和稳定性至关重要。传统网络分析方法难以捕捉微服务系统随时间演变的结构特征，因此有必要引入时序网络分析来填补这一空白。

Method: 利用时序网络分析的方法，对微服务系统的服务依赖关系随时间的变化进行建模和研究，主要通过系统版本迭代和部署追踪方式获取网络数据。

Result: 成功获得包含42个微服务在7个时间点的最完整时序网络数据，但由于时序网络的规模受限，可应用的分析方法有所限制，无法进行更深入全面的动态演化分析。

Conclusion: 微服务系统的时序网络分析面临数据获取和网络规模限制等挑战，当前获得的数据量和维度难以发挥时序网络方法的全部潜力，未来需改进数据收集方式以实现更有效的分析。

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


### [5] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 本文发现并验证：代码扩散模型在最后阶段的去噪非常适合修复残缺代码。通过重启扩散过程或生成修复数据，可以显著提升代码自动修复效率，在Python、Excel、PowerShell等领域效果良好。


<details>
  <summary>Details</summary>
Motivation: 代码生成任务通常面临“最后一公里修复”问题，即自动生成的代码往往存在最后一些小错误或不完整性，需要主动修复。已有的代码扩散模型在迭代去噪过程中，临近收敛阶段的生成与最后修复动作非常相似，因此本文研究如何利用扩散模型这一特性来应对最后一公里修复问题。

Method: 提出了利用预训练代码扩散模型用于最后一公里修复的两种方案：1）对损坏的代码加噪声后，重新运行扩散过程进行修复；2）在扩散过程中采样中间态代码（作为输入）和最终态代码（作为输出），自动生成训练数据，以支持更高效的最后一公里修复训练。

Result: 在三个领域（Python、Excel、PowerShell）进行实验。验证了基于扩散模型实现的最后一公里修复的有效性，并分析了方法属性。

Conclusion: 扩散模型不仅能生成代码，也能胜任最后一公里自动修复任务；同时扩散过程本身可用作数据生成器，为相关修复任务提供高质量训练样本，拓展了扩散模型在代码智能处理上的应用价值。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [6] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: 本文是对AI agentic编程领域的全面综述，涵盖其定义、技术方法、评测指标与主要挑战，并指明了未来研究的关键问题和发展路径，适合相关领域研究者参考。


<details>
  <summary>Details</summary>
Motivation: AI agentic programming正在快速发展，而当前对其范围、技术基础和研究挑战尚缺乏系统梳理。研究者亟需对该领域进行全面回顾和总结，以指引后续创新和发展。

Method: 本文通过文献综述和系统化分类，构建了AI agentic编程的行为与系统架构分类法，评估了规划、记忆管理、工具集成、执行监控等核心技术。分析了现有基准和评测方法，并归纳了面临的关键技术与应用挑战，结合最新发展趋势，提出了未来研究方向。

Result: 本文系统梳理了AI agentic编程领域的技术基础、核心方法和评价体系，指出了处理长上下文、任务间持久记忆、安全与意图对齐、人机协作等关键挑战，并提出提升系统可靠性、适应性和透明度的机遇。为后续智能可靠的AI编程代理研究奠定基础。

Conclusion: AI agentic编程正推动软件开发流程变革，虽然技术进展迅速，但仍面临诸多挑战。本文通过系统综述，明确了发展现状和难题，为该领域未来研究提供了方向和基础。

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [7] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文提出一种结合用户评论、提示工程与多重监控机制的移动应用性能问题自动复现系统RevPerf，并在实验中取得70%复现成功率，有效提升了性能问题的自动检测和定位能力。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能很大程度上影响用户体验，而开发环境下性能问题难以检测且诊断困难。现有复现方法受限于信息不全与自动化程度低，因此亟需更智能和高效的性能问题复现方案。

Method: RevPerf首先从Google Play中获取相关应用评论，通过prompt engineering技术丰富评论细节，随后利用执行智能体自动生成并执行复现性能问题的操作流程，并结合多种检测机制（监控Android日志、GUI变化及系统资源利用率）进行问题识别。

Result: 提出的RevPerf工具在经过手动验证的数据集上，实现了70%的性能问题复现成功率。

Conclusion: RevPerf系统能有效提升基于用户评论的移动应用性能问题复现的自动化与准确性，在实验中取得了70%的复现成功率。

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [8] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: 本文提出PTMPicker方法，通过结构化模板统一表达模型与用户需求，用embedding和prompt评价模型匹配度，远超现有关键词检索方法，实验证明其在大规模数据集上能有效帮助用户高效选取兼顾多种约束的PTM。


<details>
  <summary>Details</summary>
Motivation: 现有PTM模型搜寻方法主要依赖关键词搜索，难以充分捕捉用户实际需求，尤其在涉及偏见消除、硬件要求或许可合规等非功能性约束时，难以找到合适的模型。作者希望解决这一限制。

Method: PTMPicker方法：首先定义PTM常用核心属性结构化模板；然后将候选模型和用户需求统一表达为结构化形式。针对功能性属性计算embedding相似度，针对特殊约束（如许可、硬件）用精心设计的prompt进行评估。共收集了543,949个Hugging Face预训练模型，并抽取其描述转为结构化格式。合成15,207个模型搜索请求进行实验。

Result: 在精心构建的PTM数据集及合成搜索请求上，PTMPicker能帮助用户高效选取模型，85%的样本请求能在Top-10候选中成功找到合适PTM。

Conclusion: PTMPicker显著提升预训练模型的检索效果，尤其能更好满足非直接功能性限制，改善用户选型体验。

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [9] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 本文提出首个可进化的LLM过度拒绝检测框架ORFuzz，能够系统发现主流语言模型因安全设置过度拒绝正常请求的问题，并提供高质量基准ORFuzzSet，极大提升测试效率和覆盖，有助于促进LLM安全性和可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在安全措施上过于保守，容易错误拒绝正常请求，影响可靠性和可用性。现有测试方法不足以有效检测这一问题，存在基准不完善和测试生成能力有限等缺点。作者希望提出更系统有效的检测方案。

Method: 作者研发了ORFuzz进化测试框架，包含三部分：1）根据安全类别选择种子，实现全面测试覆盖；2）利用推理型LLM优化变异器，生成有效测试用例；3）开发与人类感知一致的判别模型OR-Judge，对毒性和拒绝进行准确评判。

Result: ORFuzz在检测LLMs过度拒绝方面，生成多样化的验证实例，平均检测率（6.98%）是主流基线方法的两倍，能更有效发现模型漏洞。同时，基于ORFuzz产出的ORFuzzSet新基准数据集（1855条用例），在10种LLM上平均过度拒绝率达63.56%，优于现有数据集。

Conclusion: ORFuzz与ORFuzzSet为LLM过度拒绝问题带来了系统、高效的自动化检测工具和公开数据集，有助于推动更可靠可信的LLM系统开发。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [10] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 论文评估了主流大型语言模型在汽车领域代码生成中的幻觉现象，发现只有在提供极为详细上下文时，部分模型才能成功生成可用代码，提示在关键行业应用中亟需解决幻觉和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成方面表现出较大潜力，但因其幻觉问题（如生成看似合理但实际错误或无法验证的代码）而限制了实际应用，尤其是在如汽车等高安全领域，探索这些幻觉现象的具体表现和成因至关重要。

Method: 针对代码生成中的幻觉问题，论文以汽车行业为例，设计案例研究，评估了多种代码LLM（包括GPT-4.1、Codex和GPT-4o）在三种不同提示复杂度下（最简单的一行提示、含VSS上下文的提示、再加代码框架的复杂提示）的表现。

Result: 实验发现，在最丰富的上下文提示下，仅GPT-4.1和GPT-4o能给出正确解决方案，其余方式和模型均未能产出可用结果。所有模型普遍出现语法错误、引用无效和API知识冲突等问题。

Conclusion: 当前主流代码LLM在没有足够上下文支持下，容易产生幻觉，生成大量不可靠代码，特别是在高安全要求的汽车软件开发领域，必须研究更有效的防控方法来确保代码安全可靠。

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [11] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文系统整理了日志代码缺陷模式，构建了真实缺陷数据集，并发现合理利用知识能有效提升大型语言模型检测日志缺陷的准确率。


<details>
  <summary>Details</summary>
Motivation: 日志代码对系统调试、性能分析和监控非常重要，但日志代码缺陷会影响日志的有效性。已有研究主要关注于有限来源和较少种类的缺陷模式，缺乏系统性和全面性分析。同时，大型语言模型（LLMs）在代码相关任务上表现优异，但其在日志代码缺陷检测中的能力尚未被充分探索。

Method: 作者提出了一套详尽的日志代码缺陷分类法，涵盖7大缺陷模式和14个具体情景。同时，构建了一个包含164个真实且由开发者验证的日志缺陷数据集。基于多种提词策略和上下文信息，提出自动化框架评估LLMs在日志代码缺陷检测和推理中的能力。

Result: 实验证明，LLMs单靠源码信息较难准确检测和推理日志代码缺陷，但融入更丰富的知识（如缺陷模式具体情景）后，可提升10.9%的检测准确率。

Conclusion: 本文为日志代码缺陷检测提供了系统性基准，为实际开发者避免常见日志缺陷提供了具体指导，也为提升基于LLM的缺陷检测建立了基础。

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [12] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: LLM自动代码翻译虽提高了正确性，但运行效率常被忽视。作者通过TRACY基准系统性评测主流模型，发现即使代码正确最好的模型在效率上也有明显短板，呼吁未来关注正确性和效率的双重提升。


<details>
  <summary>Details</summary>
Motivation: 当前的自动代码翻译通过大型语言模型（LLMs）在代码正确性上取得了显著进步，但对代码运行效率的关注不足。因此，为补足这一短板，作者提出对翻译后代码的执行效率进行系统、标准化评测。

Method: 作者提出了TRACY，这是第一个用于评估LLM翻译代码执行效率的综合基准。TRACY 采用两阶段LLM驱动流程：首先生成压力测试以放大不同模型间的性能差异，然后进行任务筛选以提取最能区分效率的任务。最终数据集涵盖C++、Java、Python三种语言，共1,011个翻译任务，每个任务均含多个高强度测试和大量人工验证的参考答案。

Result: 通过对26种主流LLM的测试发现，即便是正确性最好的模型（如Claude-4-think），在考虑运行效率后整体表现也仅列第八，被多款较小的开源模型超越。算法缺陷与资源处理不当会导致中位时间慢5.6倍、内存占用高12倍。

Conclusion: 未来LLM代码翻译任务必须兼顾正确性和效率双重指标，单一优化正确性不足以满足实际需求。

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Characterizing NC1 with Typed Monoids](https://arxiv.org/abs/2508.11019)
*Anuj Dawar,Aidan T. Evans*

Main category: cs.LO

TL;DR: 本文通过逻辑扩展与单体理论，将NC1复杂度类用一阶逻辑方法进行了刻画，证明了高维单体量词可简单化为一元量词并回答了相关公开问题，为复杂度与代数结构的研究提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 以往研究已利用型单体（typed monoid）将TC0复杂度类与抽象代数自动机理论关联，但如何用类似方法刻画更高复杂度类（如NC1）仍不明确。作者旨在通过扩展相应的逻辑与单体理论方法，为NC1提供新的特征化。

Method: 首先，作者用一阶逻辑的扩展（只使用对正规语言的单一元量词）来表达NC1，并通过证明在字符串解释环境下，高维有限单体乘法量词可被单一元量词替代，实现表达能力的“坍塌”。这一过程结合了更一般的解释结果并参考了Bojańczyk等人（2019）的理论框架。

Result: 作者成功证明，NC1可以用一阶逻辑扩展只含正规语言的一元量词来表达，并建立了高维量词到一元量词的坍塌结果。这不仅回答了Lautemann等人（2001）提出的问题，也将解释坍塌扩展到更广泛的理论情形。

Conclusion: 本研究将抽象代数和逻辑方法进一步推进至超越TC0的复杂度类，对NC1进行了新的逻辑和单体特征化，同时建立了量词坍塌结果，为理解复杂度类与代数结构之间的关系提供了新视角。

Abstract: Krebs et al. (2007) gave a characterization of the complexity class TC0 as
the class of languages recognized by a certain class of typed monoids. The
notion of typed monoid was introduced to extend methods of algebraic automata
theory to infinite monoids and hence characterize classes beyond the regular
languages. We advance this line of work beyond TC0 by giving a characterization
of NC1. This is obtained by first showing that NC1 can be defined as the
languages expressible in an extension of first-order logic using only unary
quantifiers over regular languages. The expressibility result is a consequence
of a general result showing that finite monoid multiplication quantifiers of
higher dimension can be replaced with unary quantifiers in the context of
interpretations over strings, which also answers a question of Lautemann et al.
(2001). We establish this collapse result for a much more general class of
interpretations using results on interpretations due to Boja\'nczyk et al.
(2019), which may be of independent interest.

</details>


### [14] [Automating the Derivation of Unification Algorithms: A Case Study in Deductive Program Synthesis](https://arxiv.org/abs/2508.11136)
*Richard Waldinger*

Main category: cs.LO

TL;DR: 本论文实现了统一算法的定理证明自动化合成，采用三参数环境方法，可自动统一符号表达式且更易于程序合成，相较传统方法更高效可靠。


<details>
  <summary>Details</summary>
Motivation: 统一算法是编程合成领域的重点，但至今没有实现完全自动化推导。这项工作的动机是将计算机编程转化为定理证明任务，通过自动定理证明器根据声明式规格自动生成正确的程序。

Method: 本文在领域理论公理系统下，通过自动化一般化Manna和Waldinger（1981）手工证明的方法，推导出新的统一算法。该算法以环境替换作为额外参数，递归记录替换，自动证明其最一般幂等统一性的存在性，并在不能统一时输出失败指示。

Result: 新算法可自动将两个符号表达式和环境替换统一，输出最一般幂等统一的替换或者失败信息。三参数（带环境）的算法不仅更高效，也推测比传统二参数算法更易于自动合成。

Conclusion: 通过自动化定理证明过程，作者实现了三参数环境统一算法的自动合成，提高了效率，也为统一算法自动化合成提供了可行路径。

Abstract: The unification algorithm has long been a target for program synthesis
research, but a fully automatic derivation remains a research goal. In
deductive program synthesis, computer programming is phrased as a task in
theorem proving; a declarative specification is expressed in logical form and
presented to an automatic theorem prover, and a program meeting the
specification is extracted from the proof. The correctness of the program is
supported by the proof, which also provides an explanation of how the program
works. The proof is conducted in an appropriate axiomatic subject-domain
theory, which defines the concepts in the specification and the constructs in
the target programming language and provides the background knowledge necessary
to connect them.
  For the unification proof, we generalize and automate the manual proof
presented in Manna and Waldinger [1981]. The new program unifies two given
symbolic expressions (s-expressions) relative to a given "environment"
substitution. The proof establishes the existence of an output substitution
that is a most-general idempotent unifier of the given expressions and is an
"extension" of the environment substitution. If no such substitution exists and
the expressions are not unifiable, the program is to produce a failure
indicator.
  Initially the environment substitution is the empty substitution, which makes
no replacements at all; during execution of recursive calls, the environment
substitution records the replacements that have been found so far. Our own
unification algorithm employs an environment, and such algorithms appear in the
literature [e.g., Luger and Stubblefield, 1997]. We suspect, in addition to
being more efficient, the three-argument algorithm with an environment is
easier to synthesize automatically than the two-argument version from the
Manna-Waldinger paper.

</details>


### [15] [Encoding and Reasoning About Arrays in Set Theory](https://arxiv.org/abs/2508.11447)
*Maximiliano Cristiá,Gianfranco Rossi*

Main category: cs.LO

TL;DR: 本文将数组编码为集合中的函数，提出了支持数组的集合论片段及决策过程，并集成到{log}工具中，实现了集合、函数及数组的统一推理。


<details>
  <summary>Details</summary>
Motivation: 传统上对数组的推理和处理往往与集合和函数的推理分离，缺乏统一方法，且现有的决策过程不支持数组，限制了工具的应用范围，因此需要一种将数组与集合推理统一的方法。

Method: 首先将数组编码为集合中的函数（即有序对集合），再定义集合论的一个片段用于描述程序中的数组规范。最后通过决策过程实现对该理论片段的可判定性，并将其集成到{log}工具中。

Result: 成功定义并实现了对应的集合论片段及其决策过程，数组推理得以无缝集成到{log}工具中，扩展了工具的能力。

Conclusion: 本文提出了一种将数组编码为函数，并进一步编码为有序对集合的方法，进而将数组推理转化为集合推理。该方法已集成到{log}工具中，通过新的决策过程实现对数组的支持，使得用户可统一地处理集合、函数和数组。

Abstract: We encode arrays as functions which, in turn, are encoded as sets of ordered
pairs. The set cardinality of each of these functions coincides with the length
of the array it is representing. Then we define a fragment of set theory that
is used to give the specifications of a non-trivial class of programs with
arrays. In this way, array reasoning becomes set reasoning. Furthermore, a
decision procedure for this fragment is also provided and implemented as part
of the {log} (read 'setlog') tool. {log} is a constraint logic programming
language and satisfiability solver where sets and binary relations are
first-class citizens. The tool already implements a few decision procedures for
different fragments of set theory. In this way, arrays are seamlessly
integrated into {log} thus allowing users to reason about sets, functions and
arrays all in the same language and with the same solver. The decision
procedure presented in this paper is an extension of decision procedures
defined in earlier works not supporting arrays.

</details>


### [16] [Interpolation in Classical Propositional Logic](https://arxiv.org/abs/2508.11449)
*Patrick Koopmann,Christoph Wernhard,Frank Wolter*

Main category: cs.LO

TL;DR: 本文系统介绍了命题逻辑中的插值及相关理论，提出并比较了四种计算插值的方法，探讨了插值复杂度与电路复杂性的关系，对逻辑分解与自动推理具有参考价值。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在介绍并分析经典命题逻辑中的Craig插值及相关概念，如一致插值、Beth可定义性和理论分解，解决逻辑表达式分解与推理中的插值问题。

Method: 论文提出了四种计算插值的方法：通过量词消去、从析取范式公式出发、以及从析取或表格证伪中抽取插值。

Result: 系统展示了每种方法的计算过程，并讨论了插值公式的复杂度与电路复杂性的联系。

Conclusion: 插值及相关概念对理解与分解逻辑理论、自动推理和复杂性分析具有重要意义。此外，插值公式的规模与计算复杂度密切相关。

Abstract: We introduce Craig interpolation and related notions such as uniform
interpolation, Beth definability, and theory decomposition in classical
propositional logic. We present four approaches to computing interpolants: via
quantifier elimination, from formulas in disjunctive normal form, and by
extraction from resolution or tableau refutations. We close with a discussion
of the size of interpolants and links to circuit complexity.

</details>


### [17] [Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations](https://arxiv.org/abs/2508.11515)
*Qipeng Kuang,Václav Kůla,Ondřej Kuželka,Yuanhong Wang,Yuyi Wang*

Main category: cs.LO

TL;DR: 本文分析带多个关系扩展的二元一阶模型计数问题，首次揭示：某些扩展使问题变为#P1难，但在特定条件下仍可高效求解。


<details>
  <summary>Details</summary>
Motivation: 现有研究只关注单一关系的扩展，缺乏对多关系扩展下复杂性边界的理解。本文旨在填补该空白，将分析扩展到两个关系。

Method: 在WFOMC问题中系统分析二元片段扩展至多个关系时的复杂性，分别证明某些情况为#P1难，并为特定关系组合设计多项式时间算法。

Result: 发现：对于FO^2片段，添加两个线性序或两个无环关系会使WFOMC变为#P1难；而C^2片段加上一个线性序及两个继任关系可以多项式时间内计算WFOMC。

Conclusion: WFOMC在两个关系上扩展的二元片段有复杂性边界：加入两个线性序或两个无环关系会导致#P1难，而在某些继任关系的扩展下仍可多项式时间计算。

Abstract: The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the
weighted sum of models of a given first-order logic sentence over a given
domain. The boundary between fragments for which WFOMC can be computed in
polynomial time relative to the domain size lies between the two-variable
fragment ($\text{FO}^2$) and the three-variable fragment ($\text{FO}^3$). It is
known that WFOMC for \FOthree{} is $\mathsf{\#P_1}$-hard while polynomial-time
algorithms exist for computing WFOMC for $\text{FO}^2$ and $\text{C}^2$,
possibly extended by certain axioms such as the linear order axiom, the
acyclicity axiom, and the connectedness axiom. All existing research has
concentrated on extending the fragment with axioms on a single distinguished
relation, leaving a gap in understanding the complexity boundary of axioms on
multiple relations. In this study, we explore the extension of the two-variable
fragment by axioms on two relations, presenting both negative and positive
results. We show that WFOMC for $\text{FO}^2$ with two linear order relations
and $\text{FO}^2$ with two acyclic relations are $\mathsf{\#P_1}$-hard.
Conversely, we provide an algorithm in time polynomial in the domain size for
WFOMC of $\text{C}^2$ with a linear order relation, its successor relation and
another successor relation.

</details>


### [18] [Robust Topology and the Hausdorff-Smyth Monad on Metric Spaces over Continuous Quantales](https://arxiv.org/abs/2508.11623)
*Francesco Dagnino,Amin Farjudian Eugenio Moggi*

Main category: cs.LO

TL;DR: 本文通过定义具有连续量纲的量纲值度量空间范畴和相关的Hausdorff-Smyth幺半范畴，实现了鲁棒拓扑与度量空间拓扑的统一。该框架为计算与物理系统中的不精确性和鲁棒性建模提供了理论基础，并证明每一拓扑都能由相应的量纲值度量诱导而来。


<details>
  <summary>Details</summary>
Motivation: 现有的拓扑与度量空间理论在处理鲁棒性和参数微小扰动时存在局限，特别是在需要定量分析计算与物理系统中的不精确性和鲁棒性时。该文旨在建立更一般的理论框架，以增强对这些现象的建模和分析能力。

Method: 该文定义了一个以前序富集范畴为基础的量纲值度量空间与一致连续映射（$	extsf{Met}$），其核心要求是量纲为连续量纲。在此框架下，提出了广义的开球拓扑和子集幂集上的鲁棒拓扑，并利用前序富集幺半范畴（$	extsf{P}_S$，Hausdorff-Smyth幺半范畴）描述鲁棒拓扑与开球拓扑之间的对应关系。

Result: 建立了$	extsf{Met}$范畴及其上的$	extsf{P}_S$幺半范畴，证明了对每一个拓扑都有相应的量纲值度量存在，从而统一了鲁棒拓扑和广义开球拓扑的理论基础。

Conclusion: 该理论框架为定量分析计算与物理系统中的不精确性和鲁棒性提供了基础，可应用于更广泛的场景，推动鲁棒性和不精确建模的理论发展。

Abstract: We define a (preorder-enriched) category $\mathsf{Met}$ of quantale-valued
metric spaces and uniformly continuous maps, with the essential requirement
that the quantales are continuous. For each object $(X,d,Q)$ in this category,
where $X$ is the carrier set, $Q$ is a continuous quantale, and $d: X \times X
\to Q$ is the metric, we consider a topology $\tau_d$ on $X$, which generalizes
the open ball topology, and a topology $\tau_{d,R}$ on the powerset
$\mathsf{P}(X)$, called the robust topology, which captures robustness with
respect to small perturbations of parameters. We define a (preorder-enriched)
monad $\mathsf{P}_S$ on $\mathsf{Met}$, called the Hausdorff-Smyth monad, which
captures the robust topology, in the sense that the open ball topology of the
object $\mathsf{P}_S(X,d,Q)$ coincides with the robust topology $\tau_{d,R}$
for the object $(X,d,Q)$. We prove that every topology arises from a
quantale-valued metric. As such, our framework provides a foundation for
quantitative reasoning about imprecision and robustness in a wide range of
computational and physical systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: 本文提出A2HCoder，通过分层架构和大语言模型自动化算法到硬件描述代码转换，实现高效可靠部署，在5G通信实际场景验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 无线通信对超低延迟和低功耗的需求，导致算法与硬件实现之间存在显著鸿沟，现有方法需要大量专业知识和手工开发，难以高效完成转化。

Method: 提出A2HCoder——一种由大语言模型（LLM）驱动的分层算法到硬件描述代码自动生成框架，通过横向模块化分解和纵向细粒度逐步翻译，并结合外部工具链进行调试与综合。

Result: A2HCoder在5G无线通信实际部署场景中验证了其实用价值，显著提升了自动化转化效率和可靠性，同时减轻了代码幻觉问题。

Conclusion: A2HCoder能有效实现高效、可靠的算法到硬件的自动转化，并在实际5G无线通信系统中展示了其部署效率和实用性。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [20] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 作者提出基于多维用户特征的PersonaTwin框架，显著提升了大模型进行个性化数字孪生和用户行为模拟的准确性与公平性，在现实医疗数据集上有较强实验验证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）具备模拟用户和预测人类行为的潜力，但它们往往难以捕捉个体用户多维度的细微差别。为提高个性化模拟的准确性与公平性，作者尝试构建新的方法。

Method: 提出PersonaTwin框架，通过多层次提示调整，整合用户的人口统计、行为及心理数据，实现对用户的适应性数字孪生建模。在包含8500余名医疗场景用户的数据集上，与标准LLM输出进行系统性对比，结合文本相似性指标和人口公正性评估，全面评测生成内容的准确性及无偏性。

Result: 实验发现，PersonaTwin生成的用户模拟在仿真保真度上接近oracle。基于PersonaTwin训练的下游模型在预测和公平性指标方面，与直接用真实用户数据训练的模型表现相似，适用于不同的主流大模型（如GPT-4o、Llama）。

Conclusion: PersonaTwin框架能显著提升LLM在用户个性化建模上的真实感和情感细腻程度，有助于高质量的个体数字孪生和行为分析。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [21] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 本文发布了两个高效、开源、能力强大的推理大模型gpt-oss-120b和gpt-oss-20b，在多种任务测试中表现突出，全部资源均开放，推动领域进一步研究与应用。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在准确性、推理成本、可扩展性等方面均有待突破，现有闭源模型缺少足够的开放性，限制了研究与应用。作者旨在通过构建高效、能力全面且开源的推理模型促进领域发展。

Method: 采用高效的专家混合（MoE）Transformer架构，通过大规模蒸馏与强化学习训练。在模型端引入了渲染聊天格式，优化了模型的指令跟随和角色分工能力。

Result: 两个模型在数学、编程和安全等基准测试上都取得了强劲表现。同时支持深度检索浏览、python工具调用、开发者函数集成等高级智能能力。所有权重、推理代码、工具环境和分词器均已以Apache 2.0协议开源。

Conclusion: 提出了gpt-oss-120b和gpt-oss-20b两个开源推理模型，在准确率和推理成本之间取得新突破，并达到了先进的测试表现。所有模型与工具均已开源，支持广泛使用和后续研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [22] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 该研究提出了一种自动提取公司风险因素的新闻分析框架，发现微调后的模型比大语言模型零/少样本表现更优，展示了从新闻中提取风险因素的广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 公司风险识别对投资者和金融市场健康至关重要，但如何从新闻中自动提取公司风险因素尚未被系统研究。

Method: 构建了一个计算框架，设计了包含供应链、法规、竞争等七个方面的新风险因素提取schema，对744篇新闻进行了采样和人工标注，并基于这些数据集测试了多种机器学习模型，包括零样本和少样本的大型语言模型（如LLaMA-2），以及经过微调的预训练语言模型。

Result: 零样本和少样本的大型语言模型在风险因素识别任务上的表现一般到较弱，微调后的预训练模型在大部分风险因素上表现更好。利用表现较好的模型分析了超27万篇彭博新闻，证明新闻风险因素挖掘能为公司和行业运营提供丰富洞见。

Conclusion: 虽然大语言模型在NLP任务上取得了突破，但在公司风险识别方面，微调后的模型效果更优。自动化挖掘新闻中的公司风险因素具有重要应用价值。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [23] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: 该论文提出利用大语言模型自动将复杂的知识图谱挖掘规则转化为易懂的自然语言解释，并在多个数据集和评价维度上证实了方法能有效提升规则可用性和解释质量。作者还开放了所有数据和代码资源。


<details>
  <summary>Details</summary>
Motivation: 知识图谱中的规则挖掘有助于提升知识图谱能力，但挖掘出的逻辑规则通常难以被人类理解，主要因为规则复杂且标签命名方式各异，导致可用性降低。

Method: 提出Rule2Text框架，利用大语言模型（LLMs）生成逻辑规则的自然语言解释。研究在多个数据集（如Freebase变体和ogbl-biokg）上测试不同的LLM及丰富的提示策略，并由人工和“LLM评判员”对生成解释的正确性和清晰度进行评估。此外构建黄金数据集并通过微调进一步提升模型表现，加入类型推断模块以支持没有显式类型信息的KG。

Result: 微调后的开源Zephyr模型在解释质量上有显著提升，尤其是在特定领域数据集上表现突出；类型推断模块有效扩展了系统的适用范围。

Conclusion: Rule2Text框架显著提升了知识图谱规则的可解释性与可用性，代码与数据公开，有利于未来相关研究和应用。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [24] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文针对掩码扩散语言模型(MDM)，提出利用预训练嵌入模型作为软值验证器的推断时间扩展方法，可显著提升文本生成质量，并在风格迁移等任务上超越自回归语言模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在文本生成中普遍表现优良，但进一步提升生成质量的方法需探索，尤其是在推断阶段的指导机制和与自回归模型的对比。

Method: 提出了一种在MDM去噪过程中利用外部验证器引导生成的推断时间扩展方法，并采用软值验证器结合预训练嵌入模型来优化生成。

Result: 在文本风格迁移等任务上，MDM取得了优于自回归语言模型的性能，并且通过软值验证器实现了显著的生成质量提升。

Conclusion: 验证器辅助的推断时间扩展方法能显著提升MDM的生成质量，使其成为一种优于自回归模型的文本生成框架。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [25] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 当前AI安全评估忽视未成年群体，作者提出并实测了专为该群体设计的安全评测试题，揭示多个主流大模型对儿童与青少年用户有严重安全隐患，并给出优化建议。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全评估大多关注成年人，忽视了针对未成年用户（尤其是不同年龄段的儿童和青少年）的特殊心理和发展风险，紧迫需要更适用的安全框架。

Method: 开发SproutBench评测工具，覆盖不同儿童和青少年发展阶段的认知、情感和社会风险，并用其对47种不同LLM进行了系统性安全评测，分析安全性、风险防范与交互性等维度的相关关系。

Result: 许多主流LLM存在明显针对未成年人的安全漏洞，安全与风险防范性之间高度相关，交互性越高的模型其针对未成年人的适龄性反而越低。提出了改进AI设计和部署以更好保护未成年人的具体建议。

Conclusion: 提出了SproutBench，一个包含1283条发展阶段相关对抗性提示的新评测套件，并据此揭示现有大语言模型在面向未成年人时显著的安全隐患。所发现问题为推进儿童友好型AI安全设计与部署提供了实际建议。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [26] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 本文在可控环境下系统分析了LLMs跨语言知识迁移的关键机制，通过小型合成实验发现事实表征统一性影响模型跨语迁移能力，提出数据及分词调控方法，并用新指标和可视化进行评估，揭示了未来提升LLMs多语言性能的新途径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在跨语言知识迁移方面存在挑战：当被用一种语言提问时，会对训练期间以另一种语言表达的事实产生幻觉。理解该现象的原因和机制对于提升LLM性能具有重要意义。

Method: 在可控环境下，作者从零开始训练小型Transformer模型，使用合成多语言数据集，分析模型在学习过程中对同一事实在不同语言下形成统一或分离表示的阶段，探讨其对跨语言迁移效果的影响。同时，设计并应用了一系列数据分布操控、分词策略，以及度量和可视化方法。

Result: 发现模型形成事实的统一表示对于跨语言迁移至关重要，统一程度受到事实与训练数据语言之间的互信息及语言可提取难易度影响。作者提出可通过调整数据分布和分词方式来控制跨语言迁移程度，并用新指标和可视化方法展现调控结果。

Conclusion: 通过可控设置研究，多语言模型知识迁移机制得以明确；相关发现为改进LLM跨语言能力提供新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [27] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 作者提出一个专门基准测试语言模型在遭遇环境挫折时的规划与适应能力。结果发现，无论开源还是商业模型，在环境变化下执行后备方案的能力普遍不足。该工作揭示了生成式模型在现实世界复杂任务中的重要挑战和未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型应用于越来越复杂的现实世界问题，人们期望语言模型在大规模搜索空间内进行规划。但如果这些规划由于外部不可控因素而失败，模型能否有效寻找替代方案尚不明朗。为此作者提出专门的基准来研究语言模型适应环境反馈与外部挫折的能力。

Method: 作者设计了一个基于代理的规划基准测试。每个规划问题通过组合函数调用解决，代理从4000多个函数中搜索合适的函数，并依据函数输出或错误信息获得环境反馈。基准测试有意引入外部失败（如函数突然不可用），但保证任务始终可解，以评估代理在受挫时调整规划能力。

Result: 结果显示，当前语言模型在面对环境反馈、执行替代方案和做后备规划时表现不佳。尽管最新模型通常能识别出正确的函数，但它们很难对环境的变化做出灵活调整，即便搜索空间被限制也如此。同时，作者也系统分析了开源和商用模型在搜索空间大小、模型规模扩展等不同条件下的表现。

Conclusion: 语言模型在多变环境中进行备选方案规划方面存在明显短板。当前生成式模型要提升在实际复杂任务中的鲁棒性，适应和规划能力是亟待突破的重要方向。作者也据此提出了未来改进和研究的若干方向。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [28] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 本文提出了一种可自动生成数据集、细粒度分析极化相关偏见的大语言模型评估框架，能细致反映模型在敏感话题上的不同表现，对俄乌冲突案例显示多数模型偏向乌克兰。该方法通用、可复用，并能补充现有偏见评估技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理敏感话题如政治、性别、民族或国家刻板印象时，常常表现出偏见。尽管在偏见检测与缓解方面取得了进展，仍有部分挑战未被充分探索。本文旨在提出一种更通用、可复用的细粒度偏见评估框架。

Method: 提出了一个可复用、细粒度且主题无关的框架，结合极化敏感情感指标和合成生成的冲突相关平衡数据集（设定语义类别），以评估 LLMs 的极化偏见。以俄乌战争为案例，构建合成数据集并测试多种开源和闭源大模型（如 Llama-3, Mistral, GPT-4, Claude 3.5, Gemini 1.0）。

Result: 整体偏见分数显示模型普遍对乌克兰有更积极的情感倾向，同时框架可以细致分析语义类别间的巨大差异，揭示不同模型在细节上的行为差异。对提示词修改具有适应性，进一步显现偏向预设语言或公民身份修改的现象。

Conclusion: 该框架支持自动化数据集生成和细粒度偏见评估，能够应用于多种极化场景和主题，并与其他偏见评估方法互补。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [29] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 通过大语言模型将数字词典嵌入AMR语义图，并合流式简化，分析得出对符号锚定问题的新见解。


<details>
  <summary>Details</summary>
Motivation: AMR作为一种语义表示方法，但如何将实际的数字词典嵌入AMR图并结合大语言模型进行处理，进而探讨与符号锚定问题的关系，还未有系统的研究。

Method: 利用最新预训练的大语言模型将真实数字词典信息嵌入到AMR有向无环图中，然后通过保持电路空间不变的方式对图进行合流简化。

Result: 成功将数字词典嵌入AMR digraph中，并对简化后的图的性质进行了分析，探讨了其与符号锚定问题的关联。

Conclusion: 本文展示了数字词典信息与AMR图有效融合的新路径，并提出了一种简化及分析方法，有助于更深入理解符号锚定问题。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [30] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 该论文提出RAMP框架，实现了基于LLM的营销受众策划任务，通过工具调用、结果验证以及长期记忆，准确率和召回率提升显著，为实际部署可靠的AI系统提供了实证支持。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽已能通过工具规划与交互执行复杂任务，但其在实际应用中的可靠性尚缺少系统性研究。该论文针对这一不足，在营销领域任务——受众策划上进行研究。

Method: 提出了一个名为RAMP的多智能体框架。该框架通过迭代规划、调用工具、结果验证与改进建议来提升任务完成质量。同时，为模型配备了长期记忆存储，即客户特定事实和历史查询的知识库。

Result: 在88个评估查询上的准确率提升了28个百分点。在较难的查询集上，通过多轮验证与反思，召回率提升约20个百分点，同时显著提升了用户满意度。

Conclusion: RAMP框架结合了LLM的规划和长期记忆机制，通过反复验证与反思，提高了行业实际环境下的系统可靠性与效果。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [31] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个高难度复杂问答基准，显示现有LLM在处理真实信息检索任务时表现有限，并为模型推理能力提升提供了新测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）标准很少包含既真实又费时的信息检索类问题，缺乏对复杂自然问题的测试。为填补这一空白，作者提出了新基准。

Method: 开发了MoNaCo基准，包括1,315个自然且复杂的问题，并设计了详细的分解标注流程，以人工方式大规模收集和回答这些问题。

Result: 最先进的LLMs在MoNaCo基准上最高只达到61.2% F1，主要受限于召回率低和生成幻觉。

Conclusion: MoNaCo揭示了现有模型在复杂真实问题上的不足，强调了开发更强推理能力模型的需求，并为未来进展的衡量提供了有效资源。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [32] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: 该文提出了MobQA数据集，用于系统评估大语言模型理解人类移动轨迹语义的能力。实验表明，模型在事实检索上表现强，但在语义推理与解释上仍有限，MobQA为相关模型的改进提供了基准。


<details>
  <summary>Details</summary>
Motivation: 现有模型虽然能很好地预测人类移动轨迹，但对这些轨迹背后原因与语义的理解能力尚不明确。因此需要一种衡量大语言模型在人类移动数据语义理解能力的新基准。

Method: 提出了MobQA数据集，通过自然语言问答的方式，系统评估大语言模型在解读人类GPS轨迹过程中空间、时间和语义推理的能力。MobQA包含5800个高质量问答对，覆盖事实检索、多项选择推理与自由表述解释三种题型。

Result: 主流大语言模型在事实检索任务上表现良好，但在语义推理和解释类问答方面表现出明显不足，且轨迹长度对模型有效性有较大影响。

Conclusion: MobQA揭示了目前大语言模型对人类移动轨迹的语义理解能力存在成就和明显限制，为该领域后续研究和算法改进提供了重要参考。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [33] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 首次建立了Tulu混合语社交媒体攻击性语言识别数据集，并系统评估多种模型算法，发现BiGRU注意力模型效果最好，Transformer类模型效果一般。研究为低资源代码混合语言的NLP任务奠定了基础。


<details>
  <summary>Details</summary>
Motivation: Tulu是一种南印度的低资源达罗毗荼语，尽管其数字使用不断增长，但计算资源极为有限，特别是在社交媒体内容中存在的攻击性语言识别方面。

Method: 针对混合语的Tulu社交媒体评论，建立并标注首个攻击性语言识别基准数据集，包括3845条评论，分为四类，并使用多种深度学习模型（如GRU, LSTM, CNN及注意力机制模型和transformer架构）进行评估。

Result: BiGRU自注意力模型表现最佳，准确率达82%，宏F1得分为0.81；而多语言预训练的transformer模型在低资源和混合语环境中表现较差。

Conclusion: 该研究为Tulu及其他低资源混合语的NLP任务（如攻击性语言识别）建立了基础数据集和评价标准，推动了相关领域研究。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [34] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 本研究创新性地提出了为每个学生量身定制选择题干扰项的框架，通过重构个体误区，有效提升了诊断学生错误思维的能力，并在实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统基于大模型生成的选择题干扰项多为群体共性，无法诊断和暴露学生的个性化认知误区，影响测评有效性。为此提出针对个人误解定制干扰项的任务。

Method: 提出一个无需训练的两阶段框架。第一步，基于学生过往答题记录，利用蒙特卡洛树搜索（MCTS）追溯其错误答案的推理路径，构建“学生特定的误区原型”；第二步，用该原型引导对新问题的推理模拟，生成符合其惯常错误的个性化干扰项。

Result: 在140名学生的实验中，该方法生成的个性化干扰项在合理性和个性化方面表现最优，并能适应群体级别应用，展现了鲁棒性和通用性。

Conclusion: 本文提出了个性化干扰项生成新任务，并用无训练两阶段方法有效实现，显著提升了干扰项的诊断能力、个性化及适应性。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [35] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 作者提出了一种结合推测采样、模型压缩和蒸馏的双尺度创新方法，大幅提升了语音多语言翻译模型的推理速度，同时保证翻译质量，尤其适合本地部署。


<details>
  <summary>Details</summary>
Motivation: 近年来语音转文本翻译领域发展迅速，然而多语言模型参数量大，影响推理效率，尤其在本地部署时更为突出。因此亟需提高推理效率又能保持性能的方法。

Method: 作者提出了创新性的"寄生双尺度方法"，结合了增强型推测采样方法、模型压缩和知识蒸馏等技术。基于Whisper Medium模型，进一步开发出适用于多语言语音翻译的whisperM2M模型，并集成了KVSPN模块。

Result: 在六种主流语言上，集成KVSPN模块实现了40%的推理速度提升，同时BLEU分数无损失。结合蒸馏方法后，总体推理速度提升了2.6倍，并且性能优于原始Whisper Medium模型。

Conclusion: 提出的方法能够在显著提升推理效率的同时，保持甚至超越原有翻译性能，适合多语言语音翻译模型的本地部署。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [36] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: 提出E-CaTCH多模态事件级虚假信息检测框架，融合多种表征与时序建模，在多个数据集上全面超越现有方法，适用于多种社交媒体虚假信息检测场景。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的多模态虚假信息检测极具挑战性，主要由于不同模态之间的不一致性、时序模式变化以及类别极度失衡。现有方法多独立处理帖子，难以捕捉横跨时间和模态的事件级结构。

Method: 提出E-CaTCH框架，将帖子基于文本相似性与时间临近性聚类成伪事件，并分别处理。使用BERT与ResNet提取文本和视觉特征，通过自注意力和双向跨模态注意力进行对齐。引入软门控机制融合表示。事件被分割为时间窗口，引入趋势感知LSTM与语义变化和动量信号建模叙事发展。采用事件级分类，并融合自适应类别加权、时序一致性正则化和难例挖掘，最终对所有事件聚合损失。

Result: 在Fakeddit, IND及COVID-19 MISINFOGRAPH等数据集上，E-CaTCH均优于最先进基线方法，并在跨数据集测试中展现出鲁棒性、泛化性及实际适用性。

Conclusion: E-CaTCH是一套可解释、可扩展且鲁棒的多模态虚假信息检测框架，有效解决事件级结构建模与类别失衡问题，提升了跨场景的虚假信息识别能力。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [37] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 本文提出HGRAG方法，通过超图融合细粒度结构与粗粒度语义，强化多跳问答所需知识整合，实验显示性能与效率均领先现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法多侧重粗粒度语义相似性而忽略分散知识间的结构关联，GraphRAG虽利用KG建模结构关系但过度依赖细粒度结构，未能充分结合文本语义；多跳问答要求更高效地整合分散知识，促使提出融合结构与语义的新方法。

Method: 通过构建实体超图，将细粒度实体作为节点、粗粒度篇章作为超边，利用超图扩散机制结合实体相似性和篇章相似性进行检索，并加入检索增强模块优化结果。

Result: 在主流基准数据集上，HGRAG显著优于当前最先进方法，在问答表现和检索效率方面均有突破，检索速度提升达6倍。

Conclusion: HGRAG方法能够在跨粒度融合结构和语义信息方面表现优越，有效提升了多跳问答性能，并显著提高了检索效率。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [38] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本研究分析LLMs解决低资源语言奥赛谜题的表现，发现模型在形态复杂性高的题目上存在明显不足，且更倾向于解决含英语常见特征的题目。分词预处理能提升解决能力，反映对低资源语言更智能分词和建模的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在低资源语言及复杂形态结构下的语言推理能力，揭示其局限性并寻求提升方法。

Method: 分析了629道涵盖41种低资源语言的语言学奥赛谜题，用语言学特征进行标注，从不同维度评估和分析LLMs的答题表现。

Result: LLMs在形态复杂的谜题上表现较差，通过词素分解能改善模型解决谜题的能力，展现了低资源语言建模和推理的挑战与改进方向。

Conclusion: LLMs在复杂形态结构的语言学谜题上表现较差，但在涉及英语常见语言特征的谜题上表现较好。将词语分解为词素有助于提升模型表现，提示需要更智能的分词器。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [39] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 该研究提出LETToT框架，无须标注，仅借助专家推理结构，高效评估旅游领域LLMs，验证了方法有效性和模型推理能力，对传统评估具有重要补充作用。


<details>
  <summary>Details</summary>
Motivation: 在旅游等特定领域评估大语言模型（LLMs）面临高昂的标注成本和生成偏差（如幻觉）问题。

Method: 提出了一种无需标注数据的评估框架LETToT（基于专家树式推理结构），通过与通用质量维度和专家反馈对齐，迭代优化推理组件。

Result: 专家树式推理结构的优化为LLMs带来4.99-14.15%的相对质量提升。模型规模扩大在专业领域依旧有效，但推理增强的小模型可缩小差距。理由架构在中小规模模型下显著提升准确性和简明性。

Conclusion: 建立了可扩展、无标注的新范式，实现了特定领域LLM评估，为传统标注基准提供了有力替代。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [40] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 本论文建立了法语毒性评论数据集，通过LLM和人工混合标注方法高效构建，创新性地提出了一种提升模型忠实性的微调策略，小模表现优于部分大模，经验证达成当前最优结果，并具备良好多语言推广能力。


<details>
  <summary>Details</summary>
Motivation: 针对法语环境下有毒内容检测的滞后现状（主要因缺乏大规模本地文化相关数据集），作者希望提升法语毒性检测的有效性与准确性。

Method: 提出TOXIFRENCH数据集（53,622条法语在线评论），通过高置信度的LLM预标注+人工验证，降低人工标注需求至10%；同时，比较不同规模的语言模型表现，并创新性地引入动态加权损失的CoT微调方法，增强模型决策的忠实性。

Result: 发现“小模型”在鲁棒性和泛化能力上优于部分大模型；采用新微调方法后，其4B参数模型F1分数较基线提升13%，优于GPT-40及Gemini-2.5，在跨语言基准测试中也表现出色。

Conclusion: 提出的TOXIFRENCH数据集与CoT微调策略不仅显著提升法语毒性检测性能，还具备多语言适应潜力，适合推广到其他安全相关分类任务。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [41] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 本文系统比较了8种主流LLM在应答心理健康问题时的情感表现，指出模型差异明显，且心理健康问题类型影响大，人口学特征影响有限，强调模型选择在实际应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前抑郁、焦虑和压力等心理健康问题普遍存在，越来越多的人开始借助大型语言模型（LLM）获取心理健康方面的信息。然而，不同LLM对类似问题的情感表达和回复风格可能差异显著，直接影响用户体验和心理状态。因此，研究不同LLM在应对相关问题时的情感输出，对于心理健康应用具有重要意义。

Method: 本研究选取8种LLM（如Claude Sonnet、Copilot、Gemini Pro、GPT-4o等），针对6类用户画像（基线、女性、男性、青少年、老年和大学生），提出20个关于抑郁、焦虑和压力的实际问题，收集共2,880个回复。然后，利用先进的情感与情绪分析工具对所有回复进行评分与分析，并通过统计方法考察模型、心理健康状况和人口特征对情感输出的影响。

Result: 结果发现，所有LLM输出整体以乐观、恐惧和悲伤三种情绪主导，且中性情感分值高。感激、喜悦和信任处于中等水平，而愤怒、厌恶和爱较少表达。模型之间情感表现差异显著，例如Mixtral更易表达负面情绪（如不满、悲伤），Llama则更为乐观和愉快。心理健康问题类型对情绪输出影响很大，如焦虑相关问题激发恐惧分值最高，抑郁问题激发悲伤和负面情感最高，压力问题则引发更多乐观、信任和喜悦。不同人口画像对情感表现的影响很小。

Conclusion: 不同类型LLM在应答心理健康相关问题时存在明显且独特的情感风格，模型选择对用户体验和效果具有关键影响。心理健康问题类型对情感输出影响远大于用户画像。因此，在心理健康应用场景下需谨慎选择LLM以确保合适的情感回应。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [42] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 论文提出SafeConstellations方法，通过追踪并引导嵌入空间轨迹，有效减少大模型对实际无害任务的过度拒绝，提升了模型实用性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在安全机制驱动下，常常会对一些看似可能有害但实际无害的指令过度拒绝。这种现象影响了实际应用的效用，尤其是在重复依赖常见提示模板或特定任务（如情感分析、翻译等）的场景下。

Method: 通过全面评估，分析大模型在面对经过改写的指令时的拒绝行为，并在机制层面分析模型的嵌入空间，发现模型在拒绝和非拒绝时呈现出稳定的“星座”式特征轨迹。提出SafeConstellations方法，通过在推理时跟踪任务特定的轨迹模式，引导模型呈现非拒绝的响应路径，仅在易于过度拒绝的任务上进行干预，保持模型整体行为。

Result: 该方法能显著减少大模型的过度拒绝现象，最高可降低73%，同时对总体实用性影响极小。

Conclusion: SafeConstellations方法为缓解大模型过度拒绝问题提供了系统性、有效的新思路，既保障安全性，又提升了实际应用效用。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [43] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: 提出了针对自动综述生成的新型多维评价基准SGSimEval，通过融合LLM与量化指标、引入人类偏好，更科学地评价ASG系统。实验证实ASG的大纲生成已高度接近人类，内容与参考部分仍需提升，SGSimEval评估结果高度贴近真实人类判断。


<details>
  <summary>Details</summary>
Motivation: 当前自动综述生成(ASG)工作因大型语言模型(LLMs)及其检索增强生成(RAG)、多智能体系统(MASs)发展而受到关注，但现有评价方法存在诸多局限，如偏见指标、缺乏人类偏好考量且过度依赖LLMs判别。亟需科学、全面的新型评价体系。

Method: 提出了一套综合基准SGSimEval，通过融合大纲、内容及参考文献多维度评价，将LLM评分与量化指标相结合，实现多层次综合评估。SGSimEval还创新性地引入人类偏好指标，强调系统固有质量与与人类偏好的相似性。

Result: 广泛实验证明，现有ASG系统在大纲生成方面已具备与人类相当甚至优越水平，但内容和参考文献生成部分依然有较大提升空间。SGSimEval评价结果与真实人类评估有高度一致性。

Conclusion: SGSimEval为自动综述生成领域提供了更全面、可靠的评价新范式，促进该领域技术更科学的发展。未来ASG系统应重点提升内容与参考部分的生成质量。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [44] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文实证对比了GSQ和GPTQ两种量化技术在三种主流小型LLM模型上的表现，涵盖信息检索、问答和数学推理任务。结果显示，4-bit量化方案能明显提升推理效率并在精度损失可控范围内，有效提升实际部署的可行性。作者还分析了不同量化方法和模型大小的优缺点，为用户和后续研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 量化技术可以有效降低大模型的存储与计算成本，但在不同任务上对模型性能的影响尚需深入评估。本文旨在通过实证研究为实际应用中模型压缩与性能权衡提供参考。

Method: 将4-bit Group Scaling Quantization（GSQ）和Generative Pretrained Transformer Quantization（GPTQ）分别应用于LLaMA 1B、Qwen 0.5B、PHI 1.5B三种模型，并在MS MARCO（信息检索）、BoolQ（布尔问答）、GSM8K（数学推理）三类任务上测试其准确率、推理延迟和吞吐量。

Result: 低比特量化（GSQ和GPTQ）在保证模型性能基本不变的情况下，显著提升了模型的效率（包括推理速度和输出吞吐量），但不同量化方法与模型大小之间存在权衡。

Conclusion: 通过对多种低比特量化方法及模型进行全面评测，本文为真实场景下量化模型的选择与部署提供了决策依据，并为后续相关研究建立了基准。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [45] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 本文提出用傅里叶变换分析文本token概率序列，创新性地将检测问题转化为信号能量判别，人类写作频谱能量显著高于LLM生成文本。基于此，设计的检测器在准确率及速度上均超越现有方案，展示了信号处理方法的强大潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）生成高质量文本日益普遍，亟需高效、可靠的自动检测方法。现有无需训练的方法多依赖表层统计特征，忽略了文本生成过程中的根本信号属性。

Method: 将文本检测问题转化为信号处理问题，利用离散傅里叶变换（DFT）和短时傅里叶变换（STFT）分析token对数概率序列的频谱特性，并构建基于全局DFT总能量特征的检测器SpecDetect，同时提出包含采样差异机制的加强版SpecDetect++。

Result: 实验证明，采用频域信号能量特征的检测方法不仅优于目前最先进模型，并且速度提升近一倍。SpecDetect及其增强版具备高效性和可解释性。

Conclusion: 将传统信号处理技术应用于LLM文本检测，为AI写作判别带来高效、直观的新方案。频域能量特征在区分人类与LLM生成文本方面非常有效。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [46] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 该研究利用Llama 3.1大语言模型从学生提交的语言学习作业中自动提取反馈指标，结果表明LLM生成的指标与人工评分有很高相关性，为未来自动生成可解释、透明反馈提供了有力基础。


<details>
  <summary>Details</summary>
Motivation: 自动化反馈有助于提升学生学习进度和教师教学效能，但生成高质量反馈需要先从学生提交的作业中提取相关指标。教师常用反馈标准网格，但提取指标过程费时。该研究旨在探索使用大语言模型（LLM）自动提取语言学习课程中学生提交作业的反馈指标的可行性。

Method: 使用Llama 3.1大型语言模型，从学生语言学习作业中自动提取反馈指标，并与人工评分对这些指标的契合度进行对比分析。重点考察LLM生成指标与人工评定在多种反馈标准上的一致性。

Result: 结果显示，LLM自动提取的指标与人工评分的相关性显著且较强，即使存在一些未预料的指标与标准组合，也能够保持较高一致性。

Conclusion: 用LLM自动从学生提交中提取反馈指标在技术上可行，并能显著提升反馈效率及透明度。此方法可为自动生成可解释、透明的形成性反馈奠定基础。未来研究可进一步将自动提取的指标用于高质量反馈生成。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [47] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 首次系统对比5种提示鲁棒性增强方法，全面评测主流大模型在52项任务上的表现，并分析前沿模型的鲁棒性，结果为实际应用中提升模型稳定性提供实用参考。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）对提示词措辞和格式的细微、非语义变化高度敏感，影响了其实际应用的稳定性和可靠性。为了解决这个问题，有必要系统评估提升模型提示鲁棒性的多种方法。

Method: 系统性评估了5种用于提升大语言模型提示鲁棒性的方法，在统一的实验框架下进行。评测覆盖Llama、Qwen 和 Gemma 等8个模型，任务来自Natural Instructions 数据集共52项，比较了微调和上下文学习两大范式下鲁棒性方法，并对分布偏移的多种情况进行了泛化测试。此外，还扩展到了GPT-4.1和DeepSeek V3前沿模型，用于测试它们对格式扰动的鲁棒性。

Result: 对提升提示鲁棒性的5种方法在不同模型和任务上的效果进行了系统比较，分析结果揭示了各种方法在面对格式扰动和分布偏移时的相对有效性，并提供了有价值的实证参考。

Conclusion: 不同的鲁棒性提升方法在大语言模型间表现各异，研究为实际应用中提升LLM稳定性和可靠性提供了有指导性的建议。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [48] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 作者开发了一种适合本地部署的轻量级RAG系统，通过特定领域微调和推理增强，显著提升了小模型在专用任务中的表现，且全部代码开放，易于其他领域复用。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG（检索增强生成）系统通常依赖于大规模模型和外部API，这导致难以在资源有限或对隐私有高要求的环境中部署高效且安全的生成系统。作者希望开发一种既高效又易于本地部署的RAG方案。

Method: 作者提出一个新型系统，将稠密检索器与微调过的Qwen2.5-Instruct模型结合，采用了合成查询生成和利用先进模型（如DeepSeek-R1）推理轨迹的技术，基于特定领域语料库（NHS的A-Z疾病页面）进行训练。还研究文档压缩、合成数据设计和引入推理能力细致微调对模型表现的影响。

Result: 通过针对特定领域的细致微调，模型在回答准确性和一致性方面显著超过传统无推理或通用小模型，接近当前先进（frontier）模型的表现，同时仍适合本地部署。

Conclusion: 该方法证明，基于改进的检索增强和推理能力的小型模型可以实现接近前沿性能，并能应用于资源有限和高隐私要求的场景，相关实现细节已全部公开，支持跨领域复现和推广。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [49] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 作者提出了一种结合梯度优化与正则化的新型解释生成方法，能为神经网络分类结果生成高质量的可提取解释，适用于文本和图像等多种任务，推动了可解释性与推理片段抽取方法的融合。


<details>
  <summary>Details</summary>
Motivation: 近年来神经网络模型在自然语言处理和计算机视觉领域取得了快速发展，但这些黑盒模型的预测不可解释性问题日益突出，亟需有效的解释方法。

Method: 提出了一种基于梯度优化和新的正则化策略的掩蔽方法，可以提取神经网络模型预测的可解释性片段，不需单独训练解释模型，仅依赖已有分类器。正则化策略确保解释具备充分性、全面性和紧凑性这三大特性。

Result: 所提出方法不仅能在自然语言处理任务中获得高质量解释，还成功应用于图像分类任务，显示该解释框架对不同输入类型均适用。

Conclusion: 本方法有效提升了神经网络模型解释性的同时，验证了无需专门训练解释模型即可实现高质量解释，拓宽了模型可解释性研究的应用范围。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [50] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 作者提出了一种端到端的Transformer训练方法，用一个模型同时实现分类与rationale评分，提升了效率和稳定性，并在无需显式监督的情况下与人类标注达到了最佳一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的rationalized transformer模型训练不稳定，且需要三个模型协同工作，流程复杂，且难以与人工标注保持一致。

Method: 提出一种端到端可微分的训练范式，让单一模型同时进行分类和对输入token进行评分，实现三项任务的统一，简化训练流程。同时改进rationale参数化和正则化方法，以支持类别别的rationale生成。

Result: 该方法能稳定训练，提升训练效率，并显著改善结果与人工标注的一致性，达到了当前最优水平，无需显式监督。

Conclusion: 提出的方法有效解决了模型训练不稳定及与人类标注对齐难的问题，且简化了整个模型结构和流程。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [51] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 本文提出用价值观问卷对大语言模型进行微调，以可靠改变模型的价值体系。实验证明，模型不仅在问卷回答上有所改变，且在实际情境和任务中表现出更强的目标价值一致性。这种简单方法能高效调控模型价值观。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练中隐含地编码了人类价值观，但想要改变其价值偏好通常需要大量的数据。作者想探究是否可以通过简单地训练模型回答价值观调查问卷，可靠地改变模型的价值体系。

Method: 作者首先通过让开源LLMs对20种不同人类价值的描述进行打分，构建了模型的价值画像作为基线。随后，采用微调技术使模型在价值观调查问卷上的回答发生变化，并分为两步评估：一是在问卷的内部、未见过的问题上，二是在外部领域（如情境判断和文字冒险游戏）中的表现。为此，他们构建了基于Reddit的情境道德判断数据集，并用来评估模型行为的变化。

Result: 微调后的模型不仅在问卷问题（域内）上的回答发生变化，在外部情境测试中也出现了明显的行为变化，表现出对目标价值的更一致的对齐（即价值观整合）。

Conclusion: 通过微调模型在价值观调查问卷上的回答，可以可靠地改变其对人类价值的偏好，且对下游任务的价值观呈现出明显的调控能力。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [52] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: 针对现有LLM生成幽默时缺乏语境感知的问题，本文提出了模块化流程HumorPlanSearch，通过计划搜索、文化推理、知识图谱、新颖性筛选与人工修订，显著提升了幽默内容的上下文适应性与质量。实验显示幽默生成评分提升明显，方法更贴合人类文化和语境需求。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）自动生成幽默内容常常显得平庸、重复或不合时宜，原因在于幽默很大程度依赖于听者的文化背景、心态和具体情境。

Method: 提出了HumorPlanSearch模块化流程，明确模型幽默生成过程中的语境处理，包括：（1）计划搜索多样、主题定制的幽默策略；（2）幽默链式思考模板（HuCoT）实现文化与风格推理；（3）知识图谱用于检索和调整历史高效幽默策略；（4）通过语义嵌入进行新颖性筛选；（5）基于人工评审的迭代修订循环。

Result: 在9个主题、13位人类评审的实验中，完整流程（结合知识图谱与修订）平均幽默生成评分（HGS）较强基线提升了15.4%（p<0.05）。

Conclusion: HumorPlanSearch在每个环节强调语境，从策略规划到多元信号评价，有效推动AI幽默生成向更连贯、适应性强、具有文化敏感性的方向发展。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [53] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 大型语言模型常将反性别歧视言论误判为有害，有压制挑战性别歧视声音的风险。建议内容审核设计更精细、引入人工复核，并加强模型对反性别歧视言论的识别能力。


<details>
  <summary>Details</summary>
Motivation: 自动化内容审核系统（越来越多基于大型语言模型）在抵制性别歧视的反性别歧视言论和本身性别歧视言论之间难以区分，存在抑制反性别歧视声音的风险。研究动机是分析此种系统在复杂政治事件中，尤其是女性议员相关事件中，对不同类型政治言论（性别歧视、反性别歧视、中立）识别的准确性及其社会影响。

Method: 以英国2022年女性议员热门政治事件中的政治推文为数据，分类为性别歧视、反性别歧视和中立三类，使用五种大型语言模型进行分类测试和分析。

Result: 模型常将反性别歧视的言论错误分类为有害言论，尤其是在政治事件高发期，易将表达“伤害”与“反抗”的修辞混淆。这种误判会压制挑战性别歧视的声音，特别对边缘群体影响更甚。

Conclusion: 内容审核不能简单区分有害/无害，还应在敏感事件中引入人工审核，将反性别歧视言论纳入训练数据。研究结合女性主义理论、事件分析与模型评测，凸显数字政治空间中维护反抗性别歧视声音挑战。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [54] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出CoDiEmb，通过任务专属优化和创新模型融合，有效解决信息检索与语义相似性联合训练中的性能损失，全面提升统一文本嵌入的下游效果。


<details>
  <summary>Details</summary>
Motivation: 联合学习统一文本嵌入以同时提升在信息检索（IR）和语义文本相似性（STS）等异构任务上的表现，但常规做法存在负迁移问题，即多任务联合训练常导致性能折衷。作者认为亟需系统性地隔离不同任务的学习信号，以缓解这种冲突。

Method: 提出CoDiEmb框架，通过三项创新实现IR和STS的协同但独立优化：(1) 针对任务设计特定目标函数，并用动态采样器创建单任务批次及平衡各任务更新，防止梯度干扰。IR任务用多正例和难负例的对比损失，并有跨设备采样；STS任务采用直接优化相关性和排名一致性的有序目标。(2) 使用delta-guided模型融合策略，通过分析参数与预训练初始化的偏差，为各检查点分配细致合并权重，比传统模型融合更有效。(3) 一步、易实现且收敛稳定的训练流程。

Result: 在三类基础编码器和15个标准IR/STS基准上进行大量实验，验证了CoDiEmb框架能有效减轻任务互相影响带来的性能牺牲，并提升嵌入空间的几何属性。

Conclusion: CoDiEmb不仅减少了多任务联合训练中的负迁移效应，还实实在在提升了文本嵌入在多下游任务上的表现及性质。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [55] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 通过在情感分析提示中补充结构化参考信息（如JSON），能显著提升小型LLM的表现，无需微调即胜过传统方法，适合在资源有限场景部署。


<details>
  <summary>Details</summary>
Motivation: 传统的情感分析多数仅依赖于评论文本，忽略了营销学理论所强调的参考信息（如预期与实际体验差异等），而这些信息对消费者评估有显著影响。因此，研究动机为探究在情感分析中补充这些信息能否提升LLMs的效果，特别是在资源有限场景中。

Method: 设计实验，将补充信息以自然语言（NL）和结构化JSON格式分别添加到提示词中，通过轻量级3B参数大语言模型，比较不同提示格式在Yelp两个类别（餐饮和夜生活）上的表现，无需微调。

Result: JSON格式补充信息的提示显著优于所有基线，Macro-F1分别提高1.6%和4%，RMSE分别降低16%和9.1%；实验证明结构化提示的信息利用助力模型真实理解上下文，而非简单标签推断。该方法适合资源受限的边缘设备部署。

Conclusion: 结构化（如JSON）提示能让小型LLM实现媲美大模型的性能，提供具有实际价值、低资源消耗的情感分析新方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [56] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 本论文系统分析了LLMs在物种主义偏见上的表现。研究发现，LLMs能够识别物种主义，但往往不加以谴责，且在动物与人类利益冲突时多优先人类。模型的道德权衡更关注认知能力，而非物种本身，并在不同动物类别的合理化上有所区分。作者呼吁将非人类动物纳入AI公平性和伦理对齐考量，以减少AI系统中的物种主义。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，其伦理倾向成为重要研究议题。此前的AI公平性与歧视研究主要关注人类群体，而对于物种主义（基于物种的歧视）及模型对非人类动物的价值判断尚缺乏系统性探讨。论文旨在填补这一研究空白，分析LLMs是否存在物种主义偏见及其对动物道德地位的态度。

Method: 论文采用三种实证方法：（1）构建具有1,003项的SpeciesismBench基准，系统评估LLMs对物种主义陈述的识别与道德评价；（2）采用心理学测量工具，比较模型与人类参与者的选择与表达；（3）设计文本生成任务，检测模型对物种主义理由的扩充与抗拒表现。

Result: LLMs对于物种主义陈述有较强检测能力，但很少对其进行谴责，常认为物种主义态度可接受。在心理测量中，LLMs表现出比人类稍低的显式物种主义，但在人-动物权衡任务中更倾向于优先拯救人类。模型在物种能力认知上表现为：若认知能力相等，物种偏好消失；若描述动物能力更高，则倾向优先考虑动物。在开放文本生成中，LLMs更常为农场动物遭受伤害进行合理化，却拒绝对非农场动物合理化。

Conclusion: LLMs综合体现了进步与主流的人类观点，但在动物利用等问题上，依然重现社会根深蒂固的文化规范。作者建议将AI公平性与对齐框架充分扩展，纳入非人类道德受体，以减少物种主义偏见，避免其在AI及所影响社会中的进一步加深。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>
