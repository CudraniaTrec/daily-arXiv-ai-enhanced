<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 33]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 论文提出B-bound约束域，实现对复杂递归函数的高度非线性数值不变量的自动化推断，同时借助域抽象和凸性特性，提升了抽象解释在程序分析等领域的效率和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了推导递归定义函数（如算子不动点或泛函方程解）的闭式界限，这对于程序分析（如复杂性分析、循环加速、声明式语言分析）和混合系统具有广泛应用。传统数值抽象域难以推断高度非线性数值不变量，因此需要新的理论和工具。

Method: 作者提出了一种基于约束的抽象域B-bound（边界绑定）域，用一组预选的边界函数的结合来抽象数值函数。这种方法可推断难以通过传统数值抽象域发现的高度非线性不变量。同时，提出了域抽象化方法——一种函子，可将任意值域映射提升到函数域的伽罗瓦连接，支持将符号函数抽象为数值函数，实现维度约简。相关的转移函数在简单的算子语言基础上构建，涵盖序列、多元、分段及非离散域函数。

Result: B-bound抽象域能够自动、高效地推断高度非线性的数值不变量。约束空间中揭示的凸性特性可简化甚至自动化转移函数的设计。域抽象化技术则支持方程求解的维度约简和更加普适的抽象能力。

Conclusion: 本文提出的新型基于约束的B-bound抽象域及其相关方法在高阶抽象解释、复杂递归函数约束推理等方面表现出强大能力，为程序分析和混合系统中函数推断问题提供了高效、自动化的解决路径。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [2] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI是一种支持交互式笔记本（如Jupyter）跨语言功能调用和对象操作的自动化FFI框架，简化了多语言协作开发，实现了更强大的OOP和递归/异步调用支持。


<details>
  <summary>Details</summary>
Motivation: 现有的外部函数接口（FFI）难以满足诸如Jupyter笔记本等现代交互式环境中的动态开发需求，主要问题包括手动配置繁琐、模板代码多、递归调用和面向对象编程（OOP）支持不足，严重影响多语言开发效率。

Method: 提出Kernel-FFI框架，采用源代码级转换自动重写跨语言调用，无需手动绑定或模板代码；通过新颖的辅助通道通信机制解决Jupyter内核阻塞问题，支持递归与异步外部调用，同时实现跨语言对象引用和自动资源管理以增强OOP支持。

Result: Kernel-FFI实现了在交互式笔记本中无缝的跨语言函数调用和对象操作，显著简化了开发流程，提升了多语言环境下的工作效率。工具将开源提供。

Conclusion: Kernel-FFI为现代交互式开发环境带来了透明且高效的语言互操作解决方案，解决了以往FFI在Jupyter等笔记本环境中的主要痛点，特别是在OOP和递归/异步调用方面。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: 本文提出了一种自动化评测LLM生成智能合约代码的框架，发现现有LLM在可靠性方面存在不足，建议未来结合评测框架优化LLM在实际开发中应用。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代码生成主要依赖人工检查或编译通过，缺乏对代码正确执行的系统性评估，尤其在智能合约生成领域，传统方法局限明显，因此亟需更自动化、全面的评测框架。

Method: 设计并实现了一个自动化评测框架，基于大规模业务流程模型数据集，对不同型号和规模的LLM进行实验，重点考察LLM在流程执行特性（如流程流转、资源分配、基于数据的条件）上的表现。

Result: 实验发现，现有LLM在智能合约开发所需的可靠性上表现不足，未能达到实际应用标准。

Conclusion: 提出应在现有代码生成工具中探索负责任地集成LLM方法，结合该评测框架以提升输出可靠性，为未来相关研究指明了方向。

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [4] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: 该文提出了FlowETL系统，可根据示例自动规划和执行ETL转换任务，在多种类型的数据集上展现了较强的自动化和泛化能力，极大简化了传统ETL流程中的人工设计过程。


<details>
  <summary>Details</summary>
Motivation: 当前ETL（提取、转换、加载）流程对于数据仓库和数据分析至关重要，但现有ETL方案大多依赖于人工设计和实现特定场景下的数据转换，难以自动化和泛化处理各种类型的数据。

Method: 提出了FlowETL，这是一种基于示例的自主ETL管道架构，能够根据用户定义的目标数据集自动标准化和准备输入数据。该架构包括规划引擎（根据输入-输出样本自动生成转换计划）、ETL工作单元（自动施加转换），并在整个流程中提供监控与日志。

Result: FlowETL在14个包含不同领域、文件结构和文件大小的数据集上表现出了良好的泛化能力，自动实现了数据转换和标准化。

Conclusion: FlowETL能够实现ETL流程的高效自动化，显著减少人工干预，对于提升数据预处理效率和灵活性具有重要意义。

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [5] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: 本文提出以vibe modeling整合AI（LLM）和MDE优点，加速并提升复杂软件系统开发，展望了其机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统需求增长和复杂性提升，现有开发方法和工具难以满足需求。新型用户界面、智能组件和可持续性等问题也带来新的挑战。尽管模型驱动工程（MDE）提升了软件开发的质量和生产力，但模型本身变得越来越复杂。同时，依靠大型语言模型（LLMs）进行自然语言到代码自动生成的“vibe coding”流行起来，却存在代码脆弱、安全、可维护性差等问题。

Method: 本文提出了一种名为“vibe modeling”的新范式，它将人工智能（如LLMs）的优势与传统模型驱动工程（MDE）相结合，实现复杂系统的高可靠性、高效开发。文章概述了vibe modeling的核心概念与方法。

Result: 本文展示了vibe modeling融合AI与MDE的可能性，指出该方法有助于提升开发效率与系统可靠性，同时也提出了其面临的新机遇与挑战。

Conclusion: vibe modeling作为一种新的建模方式，有望加速复杂系统的开发并提升其可靠性，为软件建模的未来提供新方向，但还需要进一步研究和探讨其具体实现与挑战。

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [6] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: GitHub Marketplace中约65%新CI工具重复已有功能，影响创新与生态效率。作者分析了工具冗余的时机与模式，为开发者和维护者提供优化建议，并开放数据供进一步研究。


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace工具数量每年快速增长，但大量新增工具实际为现有功能的重复，造成资源浪费和市场混乱。作者希望分析和量化这种冗余现象，尤其聚焦在最大细分领域——持续集成（CI）工具。

Method: 将6,983个CI Action与3,869个提供者关联，挖掘版本历史，利用图模型标注每项功能首发时间、跟踪采纳过程，并对冗余工具进行聚类分析。

Result: 约65%新增CI Action为现有功能的再实现，且通常在6个月内发生。少数首发工具成为后续广泛分叉和扩展的基础。

Conclusion: 本研究提供了CI工具市场创新与重复的深度分析，结果可指导开发者选择最佳发布时间、发现市场空白，并帮助维护者减少冗余。公开数据集有助于后续软件生态系统创新与竞争的纵向研究。

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [7] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge通过自动化方法生成并调试IoT集成代码，在准确率和效率上全面超越人类专家和商用代码大模型，大幅简化物联网设备集成流程。


<details>
  <summary>Details</summary>
Motivation: 随着多模态物联网（IoT）系统的发展，集成新型IoT设备成为普遍需求，而手动编程集成代码耗时费力且需要专业知识，成为IoT平台发展的主要瓶颈。

Method: 提出AutoBridge系统，采用分而治之策略，先通过检索设备特定知识生成设备控制逻辑，再结合平台特定知识合成平台兼容的集成代码。为保证正确性，设计了包含虚拟IoT设备自动调试器和实时硬件人机交互调试器的多阶段调试流程，其中后者仅需用户二元反馈（是/否）进行验证。

Result: 在34种IoT设备和2个开源IoT平台上测试，AutoBridge在无人参与情况下平均成功率为93.87%、功能覆盖率为94.87%；结合用户极少量二元反馈后，功能覆盖率提升至100%。用户实验显示，AutoBridge的代码准确率比专家程序员高50%-80%，即便专家可用商业代码大模型辅助。

Conclusion: AutoBridge大幅提高了IoT集成代码的生成效率与准确性，实现了自动化水平显著超越人工与现有工具，并有效降低了集成新设备的门槛。

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [8] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: 本文针对自主业务流程带来的信任与监管等问题，提出并系统化了可解释的ABPs（XABPs）框架，为未来业务流程管理提供了研究和实践路径。


<details>
  <summary>Details</summary>
Motivation: 自主业务流程（ABPs）利用AI/ML自动执行复杂流程，具备提升效率、降低错误和成本、提升响应速度等潜力。但这类系统带来了如信任度下降、调试困难、问责机制弱化、偏差风险及合规挑战等问题。

Method: 文章提出了“可解释的自主业务流程（XABPs）”的系统性方法，涵盖可解释性模式的定义、结构化，并识别了相关的业务流程管理（BPM）领域的核心研究难题。

Result: 系统阐述了构建XABPs的多种形式、可解释性的结构以及未来应关注的主要研究挑战。

Conclusion: 推行XABPs不仅有助于解决AI驱动流程中的信任、问责和合规等风险问题，也为BPM研究指明了新方向。

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [9] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate利用多代理辩论和多视角推理，显著提升了代码问题的定位与修复效果，在主流基准上刷新了性能记录。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大模型的自动化代码问题解决进展很大，但主流的方法依赖于单一智能体的独立探索，常常陷入局部最优，难以识别涉及代码库不同部分的复杂问题模式。

Method: 提出了一种称为SWE-Debate的多智能体竞争辩论框架。该框架通过遍历代码依赖图，生成多条错误传播路径作为定位建议，并让不同推理视角的专门智能体进行三回合辩论，最终达成统一的修复方案，再由基于蒙特卡洛树搜索（MCTS）的代码修改智能体生成补丁。

Result: 在SWE-bench基准测试集上，SWE-Debate在开源智能体框架中取得了新的最优性能，并显著优于现有基线方法。

Conclusion: 通过引入竞争与多样化推理机制，SWE-Debate能够更加有效地定位和修复复杂代码问题，推动了自动化软件工程任务的进步。

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [10] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: 本文提出了一种自动化系统，通过结合多种评测技术，实现了对COBOL到Java代码迁移的高效自动评估，促进了代码现代化进程。


<details>
  <summary>Details</summary>
Motivation: 当前COBOL到Java的代码自动迁移过程中，使用大模型（LLM）进行翻译已成趋势，但如何全面、客观、自动地评估翻译质量仍存在巨大挑战。例如，大模型的决策过程不透明，评估标准复杂，依赖人工审查效率低。

Method: 提出了一套自动化评估系统，结合分析型检测工具与“LLM作为评判者”（LaaJ）技术，对基于IBM watsonx Code Assistant for Z (WCA4Z)的COBOL到Java代码翻译效果进行多维度、可扩展的自动化评估，并集成入持续集成流程。

Result: 系统支持大规模基准测试，减少人工评审工作，并能为开发者和项目管理者提供有用的质量报告，帮助持续迭代形成高质量现代化代码库。

Conclusion: 通过集成分析工具和LaaJ，提高了代码迁移的自动评测覆盖度和效率，助力现代化项目高效推进。

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [11] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp通过沉淀和复用以往修复经验，使软件工程类智能体突破了传统单次记忆的局限，在开源测试集上取得最优成绩，为AI自动修复带来持续性学习和知识迁移的新思路。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）软件问题解决智能体，虽然在多智能体协作和算法探索方面表现卓越，但每次修复任务都从头开始，无法积累和复用以往的经验，导致重复无效尝试和错过迁移成功经验的机会。

Method: 提出SWE-Exp，一种将智能体以往修复轨迹中的可用经验进行提炼和沉淀的方法。该方法通过多元经验库，系统采集并存储成功及失败的修复尝试，并在高层次理解到具体代码修改多级抽取可复用的知识，实现持续跨任务学习。

Result: 在SWE-bench-Verified测试集（基于开源智能体框架）上，SWE-Exp达到了41.6%的Pass@1，创下最优表现。

Conclusion: SWE-Exp开启了让自动化软件工程智能体系统性积累和利用修复知识的新范式，由盲目试错转向战略性、经验驱动的问题解决。

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [12] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: 本文提出集成多智能体的Trae Agent方法，补足LLM在库级问题解决和大集成空间搜索的能力短板。实验显示其在SWE-bench数据集和多LLM下均超越现有技术，提升显著，并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的软件问题解决方法在处理大规模集成推理空间和缺乏库级理解方面存在局限性，影响了整体表现。本文试图突破这些限制。

Method: 提出了一种名为Trae Agent的基于智能体的集成推理方法。通过模块化的生成、修剪和选择智能体，应对大集成推理空间和库级理解两大挑战。将目标建模为最优解搜索问题，并在SWE-bench基准上同四种先进集成推理方法做对比实验。

Result: Trae Agent在所有对比方法中表现最佳，Pass@1平均提升10.22%。在SWE-bench Verified榜单上取得第一，Pass@1得分为75.20%。

Conclusion: Trae Agent以模块化多智能体方法，显著增强了LLM在库级问题解决的能力，突破了以往集成推理的关键瓶颈，并作为开源项目为社区提供支持。

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [13] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: Kieker框架新增了对Python的支持，结合静态和动态分析，可为Python应用提供深入的结构洞察。


<details>
  <summary>Details</summary>
Motivation: 原本Kieker观测框架主要支持Java，但由于Python近年来极度流行，因此为Python应用提供观测与结构性分析工具具有重要意义。

Method: 结合静态分析和动态分析方式，对Python应用构建完整的系统结构视图。

Result: 开发并实现了支持Python分析的Kieker管道，可用于系统结构性洞见。

Conclusion: 通过为Kieker观测框架增加Python支持，扩展了其应用范围，让用户能够针对Python应用灵活设计定制化观测流程。

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [14] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本文基于丰富的GitLab MRs数据，首次系统化分析代码审查（CR）所需的代码修改量，发现近三成MR需大幅调整。提出的机器学习模型可高效预测CR工作量，揭示文本、复杂度、经验等关键影响因子，对实际软件开发管理具有参考价值。


<details>
  <summary>Details</summary>
Motivation: 虽然代码审查（CR）对于保证新代码的高质量集成至关重要，但CR过程常常耗费大量人力，包括代码调整和回应审查意见。以往研究虽然分析了CR延迟和迭代次数，但鲜有基于代码修改量量化审查工作量，尤其是在GitLab Merge Requests（MRs）场景下。因此本研究旨在填补该领域的研究空白。

Method: 作者以四个GitLab项目中超过23600个Merge Requests为数据集，将CR工作量定义为提交后被修改的代码行数。通过统计分析MR调整情况，并训练解释性强的机器学习模型，结合文本特征、代码复杂度、开发经验、审阅历史、分支等多维度特征预测CR工作量。采用AUC作为衡量模型性能的标准。

Result: 高达71%的MR提交后需调整，其中28%涉及200行以上代码更改。CR工作量与审查时长或参与人数并无相关性。机器学习模型预测表现优异（AUC 0.84-0.88），表明代码复杂性、开发者经验和文本特征最为关键，同时项目历史特征也对最新CR工作量有影响。

Conclusion: 通过多维度特征建模，能够有效预测和解释CR阶段所需工作量，为代码集成管理提供理论支持。机器学习方法能够辅助识别高工作量MR，对资源分配和流程优化具有实际意义。

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Explanations for Unrealizability of Infinite-State Safety Shields](https://arxiv.org/abs/2507.23603)
*Andoni Rodriguez,Irfansha Shaik,Davide Corsi,Roy Fox,Cesar Sanchez*

Main category: cs.LO

TL;DR: 本文提出了一种用时序公式展开来解释强化学习安全屏蔽不可实现性的技术，能有效为规范不一致造成的问题给出直观说明，增强了屏蔽方法在复杂连续环境中的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习旨在优化策略的同时保证系统安全。现有的屏蔽技术存在无法实现的问题，通常源于规范自身不一致（如矛盾），这一问题直接阻碍了屏蔽方法在实际复杂环境中的应用。

Method: 提出了一种基于时序公式展开的技术，用于生成简单的无条件和有条件解释，帮助解释屏蔽无法实现的具体原因。文中展示了该方法的不同变体及其适用性。

Result: 所提出的方法能够有效地为不可能实现屏蔽的情形提供明晰的解释，从而弥补了以往屏蔽技术在面对规范不一致时的分析空白。

Conclusion: 通过时序公式展开获取解释，有助于开发者快速定位和理解屏蔽物理不可实现的根本原因，提升了安全强化学习在实际连续环境中的适用性和可靠性。

Abstract: Safe Reinforcement Learning focuses on developing optimal policies while
ensuring safety. A popular method to address such task is shielding, in which a
correct-by-construction safety component is synthesized from logical
specifications. Recently, shield synthesis has been extended to infinite-state
domains, such as continuous environments. This makes shielding more applicable
to realistic scenarios. However, often shields might be unrealizable because
the specification is inconsistent (e.g., contradictory). In order to address
this gap, we present a method to obtain simple unconditional and conditional
explanations that witness unrealizability, which goes by temporal formula
unrolling. In this paper, we show different variants of the technique and its
applicability.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
*Sergio Di Meglio,Aniello Somma,Luigi Libero Lucio Starace,Fabio Scippacercola,Giancarlo Sperlì,Sergio Di Martino*

Main category: cs.CL

TL;DR: 本研究比较两种大语言模型在物业预订平台上的实际表现，发现更高级模型能提升数据描述质量，但需付出更高算力和成本，提示应用时需合理权衡。


<details>
  <summary>Details</summary>
Motivation: 在线物业预订平台依赖外部数据源，这些源常常信息不全或不一致，导致用户体验变差和市场损失。

Method: 将两种主流大语言模型（Mistral 7B 和 Mixtral 8x7B）集成在CALEIDOHOTELS物业预订平台，对它们生成描述的能力及减少幻觉表现进行评估，并比较其资源消耗。

Result: Mixtral 8x7B在描述完整性（99.6%对93%）、精确性（98.8%对96%）和幻觉率（1.2%对4%）等方面优于Mistral 7B，同时内容更简洁，但计算资源消耗显著更高（50GB VRAM、$1.61/小时 vs 5GB、$0.16/小时）。

Conclusion: LLM能提升住宿数据描述的一致性与可靠性，但需要在模型质量与资源效率之间权衡，为生产环境部署提供了实际指导。

Abstract: Online property booking platforms are widely used and rely heavily on
consistent, up-to-date information about accommodation facilities, often
sourced from third-party providers. However, these external data sources are
frequently affected by incomplete or inconsistent details, which can frustrate
users and result in a loss of market. In response to these challenges, we
present an industrial case study involving the integration of Large Language
Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by
FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,
fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.
Both models were assessed based on their ability to generate consistent and
homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B
outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision
(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet
more concise content (249 vs. 277 words on average). However, this came at a
significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB
and $0.16/hour for Mistral 7B. Our findings provide practical insights into the
trade-offs between model quality and resource efficiency, offering guidance for
deploying LLMs in production environments and demonstrating their effectiveness
in enhancing the consistency and reliability of accommodation data.

</details>


### [17] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
*Jinzhi Wang,Qingke Peng,Haozhou Li,Zeyuan Zeng,Qinfeng Song,Kaixuan Yang,Jiangbo Zhang,Yaoying Wang,Ruimeng Li,Biyi Zhou*

Main category: cs.CL

TL;DR: 提出了电力营销领域首个LLM评测基准ElectriQ，结合领域知识增强后，小型LLM能超过主流大模型，为智能电力客服发展提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 现有的电力营销客户服务系统（如中国95598热线）存在响应慢、流程僵化、处理专业领域问题准确率低等问题。同时，当前的大型语言模型虽然拥有良好的通用能力，但缺乏电力领域专业知识和共情能力，难以满足复杂客户服务的需求。

Method: 提出了ElectriQ，这是首个面向电力营销场景的大型语言模型评测基准。ElectriQ包含覆盖六大服务类别的对话数据集，并设计了四个评价指标：专业性、通用性、可读性、易用性。还融入了领域特定的知识库，并提出了一种知识增强方法以提升模型表现。随后对13个主流LLM进行了实验证明成效。

Result: 经过微调和知识增强后，小型模型（如LLama3-8B）在专业性和用户友好性上甚至能超过GPT-4o。ElectriQ为后续发展更契合电力营销需求的专用LLM提供了数据和评测基础。

Conclusion: ElectriQ基准填补了电力营销领域LLM评测的空白，通过专业数据集和指标推动模型针对性提升，对智能客户服务领域具有重要推动作用。

Abstract: Electric power marketing customer service plays a critical role in addressing
inquiries, complaints, and service requests. However, current systems, such as
China's 95598 hotline, often struggle with slow response times, inflexible
procedures, and limited accuracy in domain-specific tasks. While large language
models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,
they lack the domain expertise and empathy required in this field. To bridge
this gap, we introduce ElectriQ, the first benchmark designed to evaluate and
enhance LLMs in electric power marketing scenarios. ElectriQ consists of a
dialogue dataset covering six key service categories and introduces four
evaluation metrics: professionalism, popularity, readability, and
user-friendliness. We further incorporate a domain-specific knowledge base and
propose a knowledge augmentation method to boost model performance. Experiments
on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and
augmented, can surpass GPT-4o in terms of professionalism and
user-friendliness. ElectriQ establishes a comprehensive foundation for
developing LLMs tailored to the needs of power marketing services.

</details>


### [18] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
*Navid Yazdanjue,Morteza Rakhshaninejad,Hossein Yazdanjouei,Mohammad Sadegh Khorshidi,Mikko S. Niemela,Fang Chen,Amir H. Gandomi*

Main category: cs.CL

TL;DR: 本文提出采用多层次分类框架及多特征融合，利用增强版ModernBERT与加权集成半监督学习，有效提升在深网等多平台非法商品识别上的准确性和鲁棒性，其表现超越诸多主流基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着非法市场转向更加隐蔽的互联网部分（深网、暗网等），以及Telegram、Reddit、Pastebin等平台，检测和分类此类非法内容变得更具挑战性。原因在于标注数据有限、非法语言不断演变、数据结构异质性高。

Method: 提出一种层次化分类框架，将经过预训练和领域微调的ModernBERT语言模型与半监督集成学习（XGBoost、Random Forest、SVM通过基于熵的加权投票）结合。首先识别销售相关文档，再细分为毒品、武器或凭证买卖。同时结合人工设计特征（如结构特征、嵌入式比特币地址、邮箱、IP和元数据）增强模型表现。

Result: 在三组数据集（包括自建多源语料库、DUTA、CoDA）上进行实验，所提模型在准确率（0.96489）、F1 分数（0.93467）、TMCC（0.95388）等多项指标上优于BERT、ModernBERT、DarkBERT、ALBERT、Longformer、BigBird等基线模型。

Conclusion: 结合先进语言模型领域微调与多特征集成学习，能在非法市场内容识别与分类上获得卓越且稳健的效果，尤其适应数据稀缺和复杂环境。模型泛化能力强，适用于实际环境下的非法内容侦测。

Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the
internet, including the deep and dark web, as well as platforms such as
Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of
illicit goods including drugs, weapons, and stolen credentials. Detecting and
categorizing such content remains challenging due to limited labeled data, the
evolving nature of illicit language, and the structural heterogeneity of online
sources. This paper presents a hierarchical classification framework that
combines fine-tuned language models with a semi-supervised ensemble learning
strategy to detect and classify illicit marketplace content across diverse
platforms. We extract semantic representations using ModernBERT, a transformer
model for long documents, finetuned on domain-specific data from deep and dark
web pages, Telegram channels, Subreddits, and Pastebin pastes to capture
specialized jargon and ambiguous linguistic patterns. In addition, we
incorporate manually engineered features such as document structure, embedded
patterns including Bitcoin addresses, emails, and IPs, and metadata, which
complement language model embeddings. The classification pipeline operates in
two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random
Forest, and SVM with entropy-based weighted voting to detect sales-related
documents. The second stage further classifies these into drug, weapon, or
credential sales. Experiments on three datasets, including our multi-source
corpus, DUTA, and CoDA, show that our model outperforms several baselines,
including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The
model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of
0.95388, demonstrating strong generalization, robustness under limited
supervision, and effectiveness in real-world illicit content detection.

</details>


### [19] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
*Jinyu Liu,Xiaoying Song,Diana Zhang,Jason Thomale,Daqing He,Lingzi Hong*

Main category: cs.CL

TL;DR: 本论文提出用机器学习和大语言模型结合的方法改进图书主题词自动分配，在准确率和词汇一致性上优于单一LLM方法。


<details>
  <summary>Details</summary>
Motivation: 传统的多标签机器学习模型虽然用于主题分析，但在处理新的、未知的案例时效果不佳。大型语言模型虽有潜力，但在实际应用中容易产生过多标签和幻觉（即生成不准确的信息），亟需更有效和可靠的方案进行文献主题分析。

Method: 提出了一种将嵌入式机器学习模型与大型语言模型（LLM）结合的混合框架。具体包括：1）用ML模型预测最优的LCSH标签数量来指导LLM的生成过程；2）对LLM输出的主题词进行后期编辑，使其与实际LCSH术语一致，从而减少幻觉。

Result: 实验证明，利用ML模型的初步预测来指导LLM生成，以及后期的术语校正，能够更好地控制LLM输出，并使其与标准词汇更为一致。生成的主题词更精准、规范。

Conclusion: 混合了嵌入式ML模型和LLM的主题分析框架，能有效提升主题词预测的准确性和规范性，减少模型幻觉，适用于为图书分配合适的LCSH主题。

Abstract: Providing subject access to information resources is an essential function of
any library management system. Large language models (LLMs) have been widely
used in classification and summarization tasks, but their capability to perform
subject analysis is underexplored. Multi-label classification with traditional
machine learning (ML) models has been used for subject analysis but struggles
with unseen cases. LLMs offer an alternative but often over-generate and
hallucinate. Therefore, we propose a hybrid framework that integrates
embedding-based ML models with LLMs. This approach uses ML models to (1)
predict the optimal number of LCSH labels to guide LLM predictions and (2)
post-edit the predicted terms with actual LCSH terms to mitigate
hallucinations. We experimented with LLMs and the hybrid framework to predict
the subject terms of books using the Library of Congress Subject Headings
(LCSH). Experiment results show that providing initial predictions to guide LLM
generations and imposing post-edits result in more controlled and
vocabulary-aligned outputs.

</details>


### [20] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
*Victor Eiti Yamamoto,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的知识图谱集成方法，重点解决现实异质知识图谱的上下文集成难题，在公开竞赛和多类测试中获得了高准确率，并发布了新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有关于本体和实体匹配的研究多集中于模式(schema)与身份(identity)匹配，对于语境(context)匹配研究较少；而实际知识图谱在信息源、规模和密度等方面高度异质，因此当前实体匹配方法在复杂多变的背景下将面临挑战。作者旨在弥补当前研究在复杂上下文集成能力上的不足。

Method: 提出了一种结合标签匹配与三元组匹配的知识图谱集成方法，其中采用字符串处理、模糊匹配和向量相似度技术对实体与谓词标签对齐，并识别表达可比信息的三元组映射，从而提高实体匹配准确性。

Result: 实验结果显示，该方法在OAEI竞赛中以及与有监督方法对比下表现具有竞争力，并在各类测试用例上取得高精准度；此外，作者还构建了一个新数据集用于评估三元组匹配步骤，提升评测全面性。

Conclusion: 作者提出的方法在OAEI竞赛及与有监督方法的对比中表现出色，在多样化的测试场景下取得了高准确率。

Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.

</details>


### [21] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
*Esmail Gumaan*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型的幻觉问题，理论上界定风险并集成多策略检测与缓解手段，提出评估流程，为未来研究与应用提供基础。


<details>
  <summary>Details</summary>
Motivation: LLM常出现生成内容与输入或事实不一致的幻觉现象，亟需系统的理论分析与实际解决方案来提升模型可靠性。

Method: 采用了理论分析（如PAC-Bayes、Rademacher复杂度）界定幻觉风险，综述了多种幻觉检测和缓解技术，并提出了统一的工作流及评估方法。

Result: 区分了内在与外在幻觉，定义了幻觉风险并给出了理论界，通过调研和集成检测与缓解方案，提出了评测流程与实践指南。

Conclusion: 本文为大语言模型（LLM）中的幻觉现象提供了理论基础和实际操作建议，从定义、风险界定到检测与缓解，并提出了可评估的实验流程，为未来解决幻觉问题指明了方向。

Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of
content that is not faithful to the input or the real-world facts. This paper
provides a rigorous treatment of hallucination in LLMs, including formal
definitions and theoretical analyses. We distinguish between intrinsic and
extrinsic hallucinations, and define a \textit{hallucination risk} for models.
We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes
and Rademacher complexity). We then survey detection strategies for
hallucinations, such as token-level uncertainty estimation, confidence
calibration, and attention alignment checks. On the mitigation side, we discuss
approaches including retrieval-augmented generation, hallucination-aware
fine-tuning, logit calibration, and the incorporation of fact-verification
modules. We propose a unified detection and mitigation workflow, illustrated
with a diagram, to integrate these strategies. Finally, we outline evaluation
protocols for hallucination, recommending datasets, metrics, and experimental
setups to quantify and reduce hallucinations. Our work lays a theoretical
foundation and practical guidelines for addressing the crucial challenge of
hallucination in LLMs.

</details>


### [22] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
*Kwun Hang Lau,Ruiyuan Zhang,Weijie Shi,Xiaofang Zhou,Xiaojun Cheng*

Main category: cs.CL

TL;DR: 本文提出在RAG中引入时序逻辑的新框架，通过拆解查询和特化检索，有效覆盖时间跨度，实验证明在复杂长期问题上精度提升显著，同时公开了数据和代码。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法只重视语义相关性，难以处理需追踪实体和现象随时间演变的长期查询，因此需通过引入时序逻辑解决证据检索的时间一致性问题。

Method: 方法包括将查询拆分为主题核心和时间窗口，并通过特化的检索器结合语义与时序相关性，确保检索到覆盖整个时间段、话题相关的证据集合。

Result: 在新提出的金融新闻基准集ADQAB上，提出的方法精确率比传统RAG提升了13%—27%，展现出优越的纵向分析能力。

Conclusion: 本文提出了在RAG系统中注入时间逻辑的新框架，显著提升了其处理横跨时间的复杂查询的能力，在权威基准测试中大幅优于传统方法。

Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static,
factual knowledge into Large Language Models (LLMs), it exhibits a critical
deficit in handling longitudinal queries that require tracking entities and
phenomena across time. This blind spot arises because conventional,
semantically-driven retrieval methods are not equipped to gather evidence that
is both topically relevant and temporally coherent for a specified duration. We
address this challenge by proposing a new framework that fundamentally
redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by
disentangling a user's query into its core subject and its temporal window. It
then employs a specialized retriever that calibrates semantic matching against
temporal relevance, ensuring the collection of a contiguous evidence set that
spans the entire queried period. To enable rigorous evaluation of this
capability, we also introduce the Analytical Diachronic Question Answering
Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus
of real and synthetic financial news. Empirical results on ADQAB show that our
approach yields substantial gains in answer accuracy, surpassing standard RAG
implementations by 13% to 27%. This work provides a validated pathway toward
RAG systems capable of performing the nuanced, evolutionary analysis required
for complex, real-world questions. The dataset and code for this study are
publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [23] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
*Daniel Son,Sanjana Rathore,Andrew Rufail,Adrian Simon,Daniel Zhang,Soham Dave,Cole Blondin,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 无论参数规模，Gemma-2大语言模型在中间层都能生成类似的内部语义特征，进一步证明特征通用性有助于跨模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 探究不同规模（2B和9B参数量）Gemma-2语言模型在内部特征上的通用性，以及模型规模差异是否影响其内部语义概念的趋同。此类研究有助于推动跨模型可解释性的研究基础。

Method: 采用稀疏自动编码器（Sparse Autoencoder, SAE）对模型残差流激活进行字典学习，提取单语义特征。通过激活相关性对齐不同模型之间的特征，并运用SVCCA和RSA等方法对比匹配特征空间的相似性。此外，还将分析从单token扩展到多token子空间。

Result: 发现中间层特征重叠度最高，表现出较强的一致性，而早期层和后期层的特征相似性较低。初步实验还发现，相似语义的多token子空间与模型的相互作用方式也很相似。

Conclusion: 大语言模型在不同规模下会提取出大致相似、可解释的特征，这进一步支持了特征通用性的观点，为不同模型间的可解释性研究提供理论基础。

Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B
and Gemma-2-9B), asking whether models with a four-fold difference in scale
still converge on comparable internal concepts. Using the Sparse Autoencoder
(SAE) dictionary-learning pipeline, we utilize SAEs on each model's
residual-stream activations, align the resulting monosemantic features via
activation correlation, and compare the matched feature spaces with SVCCA and
RSA. Middle layers yield the strongest overlap, while early and late layers
show far less similarity. Preliminary experiments extend the analysis from
single tokens to multi-token subspaces, showing that semantically similar
subspaces interact similarly with language models. These results strengthen the
case that large language models carve the world into broadly similar,
interpretable features despite size differences, reinforcing universality as a
foundation for cross-model interpretability.

</details>


### [24] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 该研究基于2万余项临床试验注册数据，利用预训练语言模型和滑动窗口法，开发出可较好预测试验中严重不良事件的新模型，有助于提前评估试验风险、优化设计。


<details>
  <summary>Details</summary>
Motivation: 临床试验如果能够准确预测安全结果，可以避免试验中途终止，也能减少参与者暴露于不必要风险。因此，作者旨在利用现有注册信息，仅基于试验注册前资料预测严重不良事件（SAE）的发生情况。

Method: 作者分析了ClinicalTrials.gov上22,107项两组平行对照干预型临床试验。开发了两个预测模型：一是分类模型预测实验组的SAE发生率是否高于对照组（用AUC评价）；二是回归模型预测对照组SAE发生比例（用RMSE评价）。方法采用转移学习，利用预训练语言模型（如ClinicalT5、BioBERT）提取特征，并结合下游预测模型。为处理试验文本较长超出模型输入长度，提出滑动窗口嵌入提取法。

Result: 最佳模型（ClinicalT5+Transformer+MLP）在预测哪个试验组SAE比例更高时AUC为77.6%；预测对照组SAE发生比例时，RMSE为18.6%。滑动窗口法在所有模型中都优于未用该法的模型，平均使AUC提升2.00%，RMSE下降1.58%。

Conclusion: 利用注册前的临床试验信息和公开结果，结合预训练语言模型，可有效预测试验中的严重不良事件。这有助于优化临床试验设计，提升安全性和效率。ClinicalTrials.gov上的总结性结果数据尚未被充分利用。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [25] [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)
*Jindong Li,Yali Fu,Jiahong Liu,Linxiao Cao,Wei Ji,Menglin Yang,Irwin King,Ming-Hsuan Yang*

Main category: cs.CL

TL;DR: 本文系统梳理了大语言模型离散化（矢量量化）方法，归纳8种代表性变体，总结现有挑战并展望未来发展，为多模态LLM系统研究提供核心参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的快速发展，需要有效的方法将连续的多模态数据转化为适合语言处理的离散表示。离散化（如矢量量化，VQ）不仅计算高效，也契合LLM架构，但当前尚无系统综述分析VQ方法在LLM中的应用。

Method: 本论文对用于LLM的离散化方法进行了首次结构化分类和分析。归纳总结了8种具有代表性的VQ变体，系统分析其算法原理、训练机制及与LLM集成的挑战。此外，从传统应用、单模态LLM、多模态LLM的角度梳理相关研究，并分析量化策略对对齐、推理与生成性能的影响。

Result: 论文归纳了VQ面临的主要挑战，包括码本崩塌、梯度估计不稳定、特定模态编码约束等。探讨了动态/任务自适应量化、统一离散框架、生物启发式码本学习等未来方向。通过系统梳理，搭建了VQ与现代LLM之间的桥梁，为多模态系统高效与泛化发展提供基础参考。

Conclusion: 本综述首次系统梳理了用于大语言模型的离散化与矢量量化方法，为相关研究和应用提供了结构化知识，并指明了未来发展与挑战方向，是该领域重要的参考文献。

Abstract: The rapid advancement of large language models (LLMs) has intensified the
need for effective mechanisms to transform continuous multimodal data into
discrete representations suitable for language-based processing. Discrete
tokenization, with vector quantization (VQ) as a central approach, offers both
computational efficiency and compatibility with LLM architectures. Despite its
growing importance, there is a lack of a comprehensive survey that
systematically examines VQ techniques in the context of LLM-based systems. This
work fills this gap by presenting the first structured taxonomy and analysis of
discrete tokenization methods designed for LLMs. We categorize 8 representative
VQ variants that span classical and modern paradigms and analyze their
algorithmic principles, training dynamics, and integration challenges with LLM
pipelines. Beyond algorithm-level investigation, we discuss existing research
in terms of classical applications without LLMs, LLM-based single-modality
systems, and LLM-based multimodal systems, highlighting how quantization
strategies influence alignment, reasoning, and generation performance. In
addition, we identify key challenges including codebook collapse, unstable
gradient estimation, and modality-specific encoding constraints. Finally, we
discuss emerging research directions such as dynamic and task-adaptive
quantization, unified tokenization frameworks, and biologically inspired
codebook learning. This survey bridges the gap between traditional vector
quantization and modern LLM applications, serving as a foundational reference
for the development of efficient and generalizable multimodal systems. A
continuously updated version is available at:
https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

</details>


### [26] [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)
*Lee Harris*

Main category: cs.CL

TL;DR: 本文提出了LMC算法，在多模型级联下，大幅提升了抽取医疗文本信息的速度和准确性，同时显著减少了模型幻觉，为知识抽取领域带来新思路。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽能抓住文本复杂关系，但成本高且容易生成虚假信息（幻觉），导致资源浪费。为提升信息提取的准确性和效率，提出新的算法应对这些问题。

Method: 提出并实现了Language Model Chain（LMC）算法。LMC通过将模型的候选答案做过滤，将潜在错误交由更强但更慢的大模型处理，分阶段、多级联动地提高整体准确率，应用于抽取医疗文档中的患者出生日期。

Result: 采用LMC算法后，多模型级联相较单一语言模型，预测速度和准确率显著提升，同时大幅减少了幻觉的发生率。

Conclusion: LMC算法能够有效提升语言模型在知识抽取中的准确率和效率，并减少幻觉现象，值得在未来进一步深入研究和应用。

Abstract: Language models can capture complex relationships in given text, but these
are notorious for being costly and for producing information that does not
exist (i.e., hallucinations). Furthermore, the resources invested into
producing this information would be wasted if it were incorrect. We address
these issues by proposing, implementing, and applying the Language Model Chain
(LMC) algorithm. In this, a language model's response to a given prompt about
given text is only correct if it exists in the collection of possible (i.e.,
candidate) answers, and text corresponding to incorrect responses is fed into a
more predictive (but slower) language model. This process is repeated for a
collection of language models, or until all predictions about the text are
correct. We used the LMC algorithm to extract patient dates of birth from
medical documents, and combining a collection of language models in a
multi-stage cascade significantly increased prediction speed and accuracy over
individual language models, while greatly reducing the number of corresponding
hallucinations. We believe that the novel LMC algorithm significantly
contributes to the knowledge extraction field, and that this should be explored
much further in the future.

</details>


### [27] [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)
*Mateusz Kmak,Kamil Chmurzyński,Kamil Matejuk,Paweł Kotzbach,Jan Kocoń*

Main category: cs.CL

TL;DR: 社交媒体情绪对股票预测作用有限，评论量与热度更具参考价值，传统情绪分析模型难以把握社交讨论的复杂性。


<details>
  <summary>Details</summary>
Motivation: 近期散户在社交媒体上的活跃，尤其是2021年GameStop事件，引发了对线上情绪影响股票价格的关注。本文研究社交媒体讨论中的情绪能否有效预测股市动态。

Method: 聚焦Reddit的r/wallstreetbets，以GME和AMC为研究对象，应用两种现有码文本情绪分析方法，并引入用ChatGPT标注并微调的RoBERTa模型，以更好适配社交媒体中的非正式语言和表情。通过相关性和因果性指标评估这些模型的预测能力。

Result: 研究发现，社交媒体情绪与股票价格的相关性较弱，而评论量与Google搜索热度等简单指标有更强的预测作用。

Conclusion: 社交媒体情绪分析对股价预测作用有限，表明散户行为复杂，传统情绪分析难以捕捉影响市场的网络讨论细节。

Abstract: The surge of retail investor activity on social media, exemplified by the
2021 GameStop short squeeze, raised questions about the influence of online
sentiment on stock prices. This paper explores whether sentiment derived from
social media discussions can meaningfully predict stock market movements. We
focus on Reddit's r/wallstreetbets and analyze sentiment related to two
companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's
role, we employ two existing text-based sentiment analysis methods and
introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model
designed to better interpret the informal language and emojis prevalent in
social media discussions. We use correlation and causality metrics to determine
these models' predictive power. Surprisingly, our findings suggest that social
media sentiment has only a weak correlation with stock prices. At the same
time, simpler metrics, such as the volume of comments and Google search trends,
exhibit stronger predictive signals. These results highlight the complexity of
retail investor behavior and suggest that traditional sentiment analysis may
not fully capture the nuances of market-moving online discussions.

</details>


### [28] [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)
*Aman Gupta,Yingying Zhuang,Zhou Yu,Ziji Zhang,Anurag Beniwal*

Main category: cs.CL

TL;DR: 本文系统评估了多语言RAG系统中提示翻译策略对分类任务的影响。实验表明优化提示设计可明显提升低资源语言的任务表现，提倡更广泛的多语言资源共享和提示优化。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在多语言能力上取得了进步，但其在不同语言和任务中的表现仍存在较大差异。在多语言检索增强生成（RAG）系统中，知识库常为高资源语言（如英语），导致检索信息常与上下文语言不一致。现有常见做法为预翻译单语提示或直接采用跨语言提示，然而这些策略的实际影响尚不明确。

Method: 系统性评估了在RAG增强的多语言系统中，不同提示翻译策略对分类任务的影响。通过实验对比优化不同的提示策略。

Result: 实验显示，优化的提示策略能显著提升跨语言知识共享，并改善下游分类任务性能。

Conclusion: 研究建议在非英语，尤其是低资源语言情境下，进一步推广多语言资源共享和跨语言提示优化。

Abstract: Despite advances in the multilingual capabilities of Large Language Models
(LLMs), their performance varies substantially across different languages and
tasks. In multilingual retrieval-augmented generation (RAG)-based systems,
knowledge bases (KB) are often shared from high-resource languages (such as
English) to low-resource ones, resulting in retrieved information from the KB
being in a different language than the rest of the context. In such scenarios,
two common practices are pre-translation to create a mono-lingual prompt and
cross-lingual prompting for direct inference. However, the impact of these
choices remains unclear. In this paper, we systematically evaluate the impact
of different prompt translation strategies for classification tasks with
RAG-enhanced LLMs in multilingual systems. Experimental results show that an
optimized prompting strategy can significantly improve knowledge sharing across
languages, therefore improve the performance on the downstream classification
task. The findings advocate for a broader utilization of multilingual resource
sharing and cross-lingual prompt optimization for non-English languages,
especially the low-resource ones.

</details>


### [29] [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)
*Brittney Exline,Melanie Duffin,Brittany Harbison,Chrissa da Gomez,David Joyner*

Main category: cs.CL

TL;DR: 以英语为母语与否会在一定程度上影响美国在线CS课程学生的同伴互评体验。非母语学生写评论更积极，但收到的评价较不积极；母语学生对收到的反馈评价更低。语言背景影响存在，但复杂且幅度有限。


<details>
  <summary>Details</summary>
Motivation: 美国高校研究生CS项目中，国际学生比例逐渐升高，且许多国际学生通过在线课程学习，这些课程常用同伴互评促进学习。然而，课程常用英语授课，许多学生并非以英语为母语。研究希望探讨母语与否如何影响学生在在线同伴互评中的体验。

Method: 该研究采用Twitter-roBERTa模型分析了500名学生的同伴互评（包括写出与收到的）情感倾向，并结合学生语言背景（母语/非母语）探讨情感得分与互评评分的关系。控制了性别和年龄变量，分析其交互作用。

Result: 结果显示：1）以英语为母语的学生对收到的反馈评价较低；2）非母语学生写出的反馈情感更积极，但他们收到的反馈情感分反而较低；3）控制性别与年龄后，语言背景对同伴互评体验有统计学的显著复杂作用，但影响幅度适中。

Conclusion: 语言背景对在线计算机课程中同伴互评体验有一定但复杂的影响，需充分关注国际学生的语言和文化差异。

Abstract: Graduate-level CS programs in the U.S. increasingly enroll international
students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.
students. Many of these students take online courses, where peer feedback is
used to engage students and improve pedagogy in a scalable manner. Since these
courses are conducted in English, many students study in a language other than
their first. This paper examines how native versus non-native English speaker
status affects three metrics of peer feedback experience in online U.S.-based
computing courses. Using the Twitter-roBERTa-based model, we analyze the
sentiment of peer reviews written by and to a random sample of 500 students. We
then relate sentiment scores and peer feedback ratings to students' language
background. Results show that native English speakers rate feedback less
favorably, while non-native speakers write more positively but receive less
positive sentiment in return. When controlling for sex and age, significant
interactions emerge, suggesting that language background plays a modest but
complex role in shaping peer feedback experiences.

</details>


### [30] [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)
*Haoran Sun,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了层次化记忆结构H-MEM，提升了LLM代理的长期记忆与推理性能，在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长期记忆对于大型语言模型代理（LLM Agents）的推理能力至关重要。目前用于记忆存储与检索的方法在结构化和高效检索方面存在不足，限制了LLM代理的决策和上下文连贯性。

Method: 提出一种分层记忆（Hierarchical Memory, H-MEM）架构，将记忆按照语义抽象程度进行多层次组织和更新。每个记忆向量嵌入了带有位置信息的索引，用以指向下一层相关子记忆。在推理阶段，通过基于索引的路由机制进行逐层高效检索，避免了繁琐的全量相似度计算。

Result: 在LoCoMo数据集的五个任务场景下进行测试，该方法在长期对话任务中始终优于五种主流对比方法。

Conclusion: H-MEM架构能有效提升LLM代理的长期记忆能力，使得多轮推理和对话任务中的决策更加高效和连贯。

Abstract: Long-term memory is one of the key factors influencing the reasoning
capabilities of Large Language Model Agents (LLM Agents). Incorporating a
memory mechanism that effectively integrates past interactions can
significantly enhance decision-making and contextual coherence of LLM Agents.
While recent works have made progress in memory storage and retrieval, such as
encoding memory into dense vectors for similarity-based search or organizing
knowledge in the form of graph, these approaches often fall short in structured
memory organization and efficient retrieval. To address these limitations, we
propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that
organizes and updates memory in a multi-level fashion based on the degree of
semantic abstraction. Each memory vector is embedded with a positional index
encoding pointing to its semantically related sub-memories in the next layer.
During the reasoning phase, an index-based routing mechanism enables efficient,
layer-by-layer retrieval without performing exhaustive similarity computations.
We evaluate our method on five task settings from the LoCoMo dataset.
Experimental results show that our approach consistently outperforms five
baseline methods, demonstrating its effectiveness in long-term dialogue
scenarios.

</details>


### [31] [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)
*Nilesh,Atul Gupta,Avinash C Panday*

Main category: cs.CL

TL;DR: 本文提出了一种捕捉全局实体位置信息的新型输入编码方法，提升了文档级关系抽取的准确率，并具备良好的理论和实践价值。


<details>
  <summary>Details</summary>
Motivation: 在文档级关系抽取任务中，实体可能在文档中多次出现，且其关系会随上下文变化。以往方法仅关注实体出现的句子，难以捕捉全局上下文，从而影响关系预测的准确性。为了解决这一问题，需要有效建模整个文档范围内的实体关系和推理。

Method: 提出了一种新的输入嵌入方法，用于捕捉实体在整个文档中的位置，而不仅仅是它们出现的具体片段。通过将实体视为独立片段进行编码，实现全球关系建模和多句推理。

Result: 在DocRED、Re-DocRED和REBEL等三个基准数据集上实验，结果显示该方法能有效提升文档级实体关系预测的准确性。

Conclusion: 该方法在理论上推动了全球上下文建模与多句推理技术的发展，在实际应用上能大幅提升需要全面实体级分析和可解释性的自然语言处理任务表现。

Abstract: In document-level relation extraction, entities may appear multiple times in
a document, and their relationships can shift from one context to another.
Accurate prediction of the relationship between two entities across an entire
document requires building a global context spanning all relevant sentences.
Previous approaches have focused only on the sentences where entities are
mentioned, which fails to capture the complete document context necessary for
accurate relation extraction. Therefore, this paper introduces a novel input
embedding approach to capture the positions of mentioned entities throughout
the document rather than focusing solely on the span where they appear. The
proposed input encoding approach leverages global relationships and
multi-sentence reasoning by representing entities as standalone segments,
independent of their positions within the document. The performance of the
proposed method has been tested on three benchmark relation extraction
datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results
demonstrated that the proposed method accurately predicts relationships between
entities in a document-level setting. The proposed research also has
theoretical and practical implications. Theoretically, it advances global
context modeling and multi-sentence reasoning in document-level relation
extraction. Practically, it enhances relationship detection, enabling improved
performance in real-world NLP applications requiring comprehensive entity-level
insights and interpretability.

</details>


### [32] [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)
*Zhehao Tan,Yihan Jiao,Dan Yang,Lei Liu,Jie Feng,Duolin Sun,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出一种细粒度的RAG基准评测体系，发现主流大模型在文档利用方面仍有明显不足，为RAG系统的进步提供了新工具和思路。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG（检索增强生成）系统提升了大语言模型的表现，但现有评测体系更关注整体效果，缺乏对LLM本身能力的细致分析，尤其是在文档利用上。

Method: 提出了Placeholder-RAG-Benchmark，这是一套多层次、细粒度的基准，聚焦多级过滤能力、组合能力和参考推理，并通过创新的Placeholder方法将模型固有知识与外部知识贡献解耦。

Result: 实验表明，当前主流LLMs在RAG系统中的生成能力存在局限，尤其在错误恢复和内容忠实性方面表现不足。基准为开发更可靠高效的RAG系统提供可复现的评测框架。

Conclusion: Placeholder-RAG-Benchmark可以更细致地评估LLM在RAG系统中的文档利用能力，促进行业内对更强RAG系统的探索与改进。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge, where the LLM's ability to generate responses
based on the combination of a given query and retrieved documents is crucial.
However, most benchmarks focus on overall RAG system performance, rarely
assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects
such as noise robustness, but lack a systematic and granular evaluation
framework on document utilization. To this end, we introduce
\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,
emphasizing the following progressive dimensions: (1) multi-level filtering
abilities, (2) combination abilities, and (3) reference reasoning. To provide a
more nuanced understanding of LLMs' roles in RAG systems, we formulate an
innovative placeholder-based approach to decouple the contributions of the
LLM's parametric knowledge and the external knowledge. Experiments demonstrate
the limitations of representative LLMs in the RAG system's generation
capabilities, particularly in error resilience and context faithfulness. Our
benchmark provides a reproducible framework for developing more reliable and
efficient RAG systems. Our code is available in
https://github.com/Alipay-Med/PRGB.

</details>


### [33] [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
*Xi Chen,Aske Plaat,Niki van Stein*

Main category: cs.CL

TL;DR: 本研究首次用因果分析手段，揭示CoT不仅提升大模型多步推理表现且能带来更清晰的内部表征，验证了其促进模型结构化推理和解释性的作用。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）在提升大语言模型（LLM）处理多步骤任务准确性上表现突出，但目前尚不清楚模型生成的“思维链”是否真实反映其内部推理过程。作者试图通过特征层面的因果分析研究CoT的忠实性。

Method: 将稀疏自动编码器与激活修补（activation patching）技术结合，从Pythia-70M和Pythia-2.8B模型中提取单语义特征。在GSM8K数学题目上分别采用有CoT和无CoT提示词进行输入，并通过交换特征激活，检测CoT推理特征对无CoT流程的影响，以分析神经元层面上思维链产生的效果。此外，引入patch-curves和随机特征修补对照实验。

Result: 将CoT推理特征注入无CoT流程可显著提升Pythia-2.8B模型的答案对数概率（但对70M模型无显著作用），显示出模型规模上的明显门槛。大模型在CoT下表现出更高的激活稀疏度及更好的特征可解释性，表明其内部分工更加模块化（如答案置信度从1.2提升至4.3）。信息不仅在最优修补中有效，而是广泛分布于多个特征。

Conclusion: CoT能在高容量LLM中诱发更具可解释性和模块化的内部结构，验证了思维链作为结构化提示方法的有效性。

Abstract: Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on
multi-step tasks, yet whether the generated "thoughts" reflect the true
internal reasoning process is unresolved. We present the first feature-level
causal study of CoT faithfulness. Combining sparse autoencoders with activation
patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B
while they tackle GSM8K math problems under CoT and plain (noCoT) prompting.
Swapping a small set of CoT-reasoning features into a noCoT run raises answer
log-probabilities significantly in the 2.8B model, but has no reliable effect
in 70M, revealing a clear scale threshold. CoT also leads to significantly
higher activation sparsity and feature interpretability scores in the larger
model, signalling more modular internal computation. For example, the model's
confidence in generating correct answers improves from 1.2 to 4.3. We introduce
patch-curves and random-feature patching baselines, showing that useful CoT
information is not only present in the top-K patches but widely distributed.
Overall, our results indicate that CoT can induce more interpretable internal
structures in high-capacity LLMs, validating its role as a structured prompting
method.

</details>


### [34] [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)
*Xiaoyu Pan,Yang Bai,Ke Zou,Yang Zhou,Jun Zhou,Huazhu Fu,Yih-Chung Tham,Yong Liu*

Main category: cs.CL

TL;DR: 本文针对医学大语言模型在眼科应用中的幻觉问题，提出了新的评测基准和多智能体三阶段缓解框架，极大提高了模型的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型（MLLMs）在眼科诊断中作用重要，但在有限的专业知识、视觉定位与推理能力不足及多模态数据稀缺的限制下，准确性受幻觉影响，现有基准也难以有效评测和减少幻觉。

Method: 提出 EH-Benchmark，这是一个用于评估 MLLMs 幻觉的新型眼科基准。作者将幻觉分为视觉理解和逻辑组合两大类，并细分为多个子类。同时，提出以智能体为中心的三阶段多智能体框架，包括知识检索、案例分析及结果验证。

Result: 实验结果显示，该多智能体框架有效降低了两类幻觉，提升了诊断的准确性、可解释性和可靠性。

Conclusion: EH-Benchmark 能系统地评估并缓解医学大语言模型在眼科中的幻觉问题，其三阶段智能体框架显著提升了模型的表现。

Abstract: Medical Large Language Models (MLLMs) play a crucial role in ophthalmic
diagnosis, holding significant potential to address vision-threatening
diseases. However, their accuracy is constrained by hallucinations stemming
from limited ophthalmic knowledge, insufficient visual localization and
reasoning capabilities, and a scarcity of multimodal ophthalmic data, which
collectively impede precise lesion detection and disease diagnosis.
Furthermore, existing medical benchmarks fail to effectively evaluate various
types of hallucinations or provide actionable solutions to mitigate them. To
address the above challenges, we introduce EH-Benchmark, a novel ophthalmology
benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'
hallucinations based on specific tasks and error types into two primary
classes: Visual Understanding and Logical Composition, each comprising multiple
subclasses. Given that MLLMs predominantly rely on language-based reasoning
rather than visual processing, we propose an agent-centric, three-phase
framework, including the Knowledge-Level Retrieval stage, the Task-Level Case
Studies stage, and the Result-Level Validation stage. Experimental results show
that our multi-agent framework significantly mitigates both types of
hallucinations, enhancing accuracy, interpretability, and reliability. Our
project is available at https://github.com/ppxy1/EH-Benchmark.

</details>


### [35] [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)
*Shalini Jangra,Suparna De,Nishanth Sastry,Saeed Fadaei*

Main category: cs.CL

TL;DR: 该论文提出利用大语言模型合成标注个人信息披露的新数据集，解决了缺乏公开PII标注数据难题，并有效保证数据安全性、难溯源性、与真实数据的高度相似，有助社交媒体隐私风险研究的发展。


<details>
  <summary>Details</summary>
Motivation: 社交平台如Reddit中大量涉及个人信息披露的帖子，但相关风险研究受限于缺乏可公开的带标签数据集。为推进该领域可复现性研究，需要安全可分享的合成人工数据。

Method: 提出了一种新方法，利用大型语言模型（如Llama2-7B、Llama3-8B和zephyr-7b-beta）按顺序提示，合成与原Reddit数据相似、可公开的PII标签文本数据。构建19类易受威胁人群的PII分类法。

Result: 生成并公开了基于三种LLM合成的多文本PII标签数据集，并通过三项标准评估：（1）模型在合成数据和原始数据训练的效果等价性；（2）合成数据无法溯源真实用户；（3）人类难以区分原始与合成数据。

Conclusion: 本研究提供了能安全公开、结构化合成且与真实数据高度等价的PII数据集，有助于推动社交媒体PII风险研究的可复现发展。

Abstract: Social platforms such as Reddit have a network of communities of shared
interests, with a prevalence of posts and comments from which one can infer
users' Personal Information Identifiers (PIIs). While such self-disclosures can
lead to rewarding social interactions, they pose privacy risks and the threat
of online harms. Research into the identification and retrieval of such risky
self-disclosures of PIIs is hampered by the lack of open-source labeled
datasets. To foster reproducible research into PII-revealing text detection, we
develop a novel methodology to create synthetic equivalents of PII-revealing
data that can be safely shared. Our contributions include creating a taxonomy
of 19 PII-revealing categories for vulnerable populations and the creation and
release of a synthetic PII-labeled multi-text span dataset generated from 3
text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and
zephyr-7b-beta, with sequential instruction prompting to resemble the original
Reddit posts. The utility of our methodology to generate this synthetic dataset
is evaluated with three metrics: First, we require reproducibility equivalence,
i.e., results from training a model on the synthetic data should be comparable
to those obtained by training the same models on the original posts. Second, we
require that the synthetic data be unlinkable to the original users, through
common mechanisms such as Google Search. Third, we wish to ensure that the
synthetic data be indistinguishable from the original, i.e., trained humans
should not be able to tell them apart. We release our dataset and code at
https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster
reproducible research into PII privacy risks in online social media.

</details>


### [36] [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
*Shuyu Guo,Zhaochun Ren*

Main category: cs.CL

TL;DR: 本文提出了一种自适应上下文压缩（ACC-RAG）框架，根据查询复杂度动态调整压缩率，大幅提升RAG推理速度且不损失准确率，优于传统固定压缩方案。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）方法虽然能为大语言模型提供外部知识，但因需要处理大量冗长的检索内容而导致推理成本很高。现有上下文压缩方法采用固定压缩率，无法兼顾简单查询和复杂查询的差异，容易过度或不足压缩。

Method: 本文提出自适应上下文压缩的RAG（ACC-RAG）框架，根据输入复杂度动态调整压缩率。ACC-RAG包括分层压缩器（实现多粒度嵌入）和上下文选择器，实现类似人类浏览的最小充分信息保留。

Result: 在Wikipedia及五个问答数据集上，ACC-RAG相较于固定压缩率方法表现更优，并且在推理速度上可达到标准RAG的4倍以上，同时保持或提升准确率。

Conclusion: ACC-RAG能够在不牺牲模型准确率的前提下，大幅提升推理效率，优于现有固定压缩方法，为检索增强生成任务带来更灵活高效的方案。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external knowledge but incurs significant inference costs due to lengthy
retrieved contexts. While context compression mitigates this issue, existing
methods apply fixed compression rates, over-compressing simple queries or
under-compressing complex ones. We propose Adaptive Context Compression for RAG
(ACC-RAG), a framework that dynamically adjusts compression rates based on
input complexity, optimizing inference efficiency without sacrificing accuracy.
ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with
a context selector to retain minimal sufficient information, akin to human
skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms
fixed-rate methods and matches/unlocks over 4 times faster inference versus
standard RAG while maintaining or improving accuracy.

</details>


### [37] [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)
*Baptiste Lefort,Eric Benhamou,Beatrice Guez,Jean-Jacques Ohana,Ethan Setrouk,Alban Etienne*

Main category: cs.CL

TL;DR: 本文提出一种利用轻量级LLM与层次化DRL结合的投资组合优化方法，有效融合市场指标和新闻情感，实现年化26%的高收益，并开源复现，优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 在投资组合优化领域，如何有效融合金融新闻舆情与传统市场指标、提升收益并增强模型稳定性一直是重要挑战。当前的方法在利用多模态数据和提升模型可扩展性方面仍有不足。

Method: 提出一种新颖的三级分层架构，将轻量级大型语言模型(LLM)与深度强化学习(DRL)相结合。底层RL智能体处理融合的市场与舆情数据，元智能体对决策进行聚合，超级智能体最终融合市场与情感分析结果，为投资组合提供优化建议。

Result: 在2018-2024年数据上测试（训练数据为2000-2017），框架实现了年化26%收益率和1.2的夏普比率，均优于等权和标普500基线。

Conclusion: 该分层架构通过跨模态融合与层次化RL结构提升了投资组合优化的稳定性与收益表现。同时，具有良好的可扩展性和可复现性，对金融人工智能领域具有实际应用价值。

Abstract: This paper presents a novel hierarchical framework for portfolio
optimization, integrating lightweight Large Language Models (LLMs) with Deep
Reinforcement Learning (DRL) to combine sentiment signals from financial news
with traditional market indicators. Our three-tier architecture employs base RL
agents to process hybrid data, meta-agents to aggregate their decisions, and a
super-agent to merge decisions based on market data and sentiment analysis.
Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework
achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming
equal-weighted and S&P 500 benchmarks. Key contributions include scalable
cross-modal integration, a hierarchical RL structure for enhanced stability,
and open-source reproducibility.

</details>


### [38] [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)
*Anthony C Davis,Burhan Sadiq,Tianmin Shu,Chien-Ming Huang*

Main category: cs.CL

TL;DR: 本文综述了通过结合外部符号系统提升视觉-语言模型理解和推理能力的最新技术与研究进展，强调神经-符号系统在可解释性和适应性等方面的优势。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型虽然表现优异，但存在可解释性差、难以集成新知识、资源消耗大、逻辑推理能力有限等问题。研究动机在于通过引入外部符号系统，提升模型在推理、记忆和解释方面的能力。

Method: 采用系统性文献综述（systematic literature review）的方法，梳理分析了视觉-语言理解与外部符号信息系统交互的主要技术路径和方式。

Result: 归纳并分类了将视觉-语言模型与外部符号信息系统结合以提升其理解能力的各种技术手段，并总结了当前领域的研究进展。

Conclusion: 将视觉-语言模型与外部符号信息系统结合，能够显著提升模型的可解释性、推理能力与对新知识的适应性，是视觉-语言理解领域的重要发展方向。

Abstract: Recent advances in visual-language machine learning models have demonstrated
exceptional ability to use natural language and understand visual scenes by
training on large, unstructured datasets. However, this training paradigm
cannot produce interpretable explanations for its outputs, requires retraining
to integrate new information, is highly resource-intensive, and struggles with
certain forms of logical reasoning. One promising solution involves integrating
neural networks with external symbolic information systems, forming neural
symbolic systems that can enhance reasoning and memory abilities. These neural
symbolic systems provide more interpretable explanations to their outputs and
the capacity to assimilate new information without extensive retraining.
Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural
component, augmented by external systems, offers a pragmatic approach to
realizing the benefits of neural-symbolic integration. This systematic
literature review aims to categorize techniques through which visual-language
understanding can be improved by interacting with external symbolic information
systems.

</details>


### [39] [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)
*Jingwei Zhao,Yuhua Wen,Qifei Li,Minchi Hu,Yingying Zhou,Jingyao Xue,Junyang Wu,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CL

TL;DR: 本文是一篇关于意图识别领域深度学习和多模态方法的综述，总结了技术进展、数据集、应用、挑战和未来方向，重点关注Transformer等新模型带来的突破及其对自然人机交互的推动作用。


<details>
  <summary>Details</summary>
Motivation: 随着人机自然交互需求的不断增长，传统仅依赖文本的意图识别已无法满足实际应用，促使研究向多模态和深度学习方法演进。

Method: 本文综述了意图识别领域中，深度学习方法的发展，包括从单一模态到多模态的演变，以及相关数据集、方法、应用和当前面临的挑战。

Result: 总结了当前深度学习和多模态意图识别领域的主要进展，对Transformer等新模型带来的突破进行了介绍，并归纳了面向未来研究的主要方向。

Conclusion: 多模态深度学习方法有效推动了意图识别的发展，但仍有诸多挑战，需要持续探索新型模型与跨模态信息的融合。

Abstract: Intent recognition aims to identify users' underlying intentions,
traditionally focusing on text in natural language processing. With growing
demands for natural human-computer interaction, the field has evolved through
deep learning and multimodal approaches, incorporating data from audio, vision,
and physiological signals. Recently, the introduction of Transformer-based
models has led to notable breakthroughs in this domain. This article surveys
deep learning methods for intent recognition, covering the shift from unimodal
to multimodal techniques, relevant datasets, methodologies, applications, and
current challenges. It provides researchers with insights into the latest
developments in multimodal intent recognition (MIR) and directions for future
research.

</details>


### [40] [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
*Kathleen Mealey,Jonathan A. Karr Jr.,Priscila Saboia Moreira,Paul R. Brenner,Charles F. Vardeman II*

Main category: cs.CL

TL;DR: 本论文针对航空行业设备维护应用，系统评估了16种NLP工具及LLM的知识抽取能力，发现它们在安全受控环境下受限明显，尚不能广泛应用于关键行业。作者开源数据集并提出了增强可信度的建议。


<details>
  <summary>Details</summary>
Motivation: 组织数据仓库需要平衡数据机密性和数据集成的要求，并且NLP工具在特定领域（如运维领域）知识结构下表现有限。因此，亟需研究如何高效、安全地从数据中提取知识以获得运营智能。

Method: 论文对知识图谱构建过程进行了分解，重点分析了命名实体识别、共指消解、实体链接和关系抽取四个功能环节，并对16种NLP工具及大语言模型（LLM）的零样本表现进行了系统评估。数据集以美国联邦航空局公开设备故障/维护需求数据为基础，且所有评估均保证数据在受控环境内处理，不外发至第三方。

Result: 实验结果显示，当前NLP和LLM工具在受控环境下的零样本性能存在显著局限性，特别是在需要高度可信的航空工业智能应用中，它们的技术成熟度（TRL）尚不足以广泛应用。论文还开源了经整理的数据集以促进进一步基线测试与评估。

Conclusion: 现有的NLP及LLM工具在航空等关键行业的受控环境下，一方面因隐私和安全需求限制，另一方面受技术能力制约，难以满足信任和集成要求。作者建议加强工具的可信性，并提供了开放数据集以支持后续改进和评估。

Abstract: Deriving operational intelligence from organizational data repositories is a
key challenge due to the dichotomy of data confidentiality vs data integration
objectives, as well as the limitations of Natural Language Processing (NLP)
tools relative to the specific knowledge structure of domains such as
operations and maintenance. In this work, we discuss Knowledge Graph
construction and break down the Knowledge Extraction process into its Named
Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation
Extraction functional components. We then evaluate sixteen NLP tools in concert
with or in comparison to the rapidly advancing capabilities of Large Language
Models (LLMs). We focus on the operational and maintenance intelligence use
case for trusted applications in the aircraft industry. A baseline dataset is
derived from a rich public domain US Federal Aviation Administration dataset
focused on equipment failures or maintenance requirements. We assess the
zero-shot performance of NLP and LLM tools that can be operated within a
controlled, confidential environment (no data is sent to third parties). Based
on our observation of significant performance limitations, we discuss the
challenges related to trusted NLP and LLM tools as well as their Technical
Readiness Level for wider use in mission-critical industries such as aviation.
We conclude with recommendations to enhance trust and provide our open-source
curated dataset to support further baseline testing and evaluation.

</details>


### [41] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 对五种主流LLM在金融10-K文本摘要任务中展开系统性评测，发现GPT表现最佳，不同模型间差异明显，且受业态和时间影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在金融自然语言处理领域表现出色，但不同主流LLM之间的系统比较仍然相对缺乏。随着LLM在金融分析中的应用日益广泛，系统性评估对实际应用具有重要指导意义。

Method: 选取GPT、Claude、Perplexity、Gemini和DeepSeek五种主流LLM，基于'Magnificent Seven'科技公司的10-K财报，设计金融领域特定的prompt，从人工标注、自动词汇—语义指标（如ROUGE、余弦相似度、Jaccard）和模型行为诊断（prompt层面的方差、不同模型输出相似性）三个方面，综合评估模型性能。

Result: GPT在连贯性、语义对齐以及上下文相关性方面表现最佳，其次是Claude和Perplexity。Gemini和DeepSeek表现波动更大、一致性较差。此外，模型输出的相似性和稳定性因公司和时间有所变化，对prompt写作方式和原始材料敏感。

Conclusion: 在金融NLP场景下，GPT表现最优，Claude和Perplexity次之。不同LLM的表现受具体任务、数据和prompt设计影响显著，说明模型选择需结合实际需求和场景特性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


### [42] [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)
*Jinkun Zhao,Yuanshuai Wang,Xingjian Zhang,Ruibo Chen,Xingchuang Liao,Junle Wang,Lei Huang,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于多专家协作的大语言模型+检索增强机制的新AIOps框架，在多个AIOps任务上全面超越了传统及现有方法，实验指标提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前AIOps领域内，单一模型通常受限于特定领域知识，只能完成特定任务（如日志解析、根因分析），而多模型协作方法在提升性能上已有前人研究（集成学习、LLM等），该文希望解决AIOps中单模型适应性差、组合模型效率不足等问题。

Method: 提出了一个专家协作架构（CoE-Ops），整合了通用型大型语言模型作为任务分类器，并引入了检索增强生成机制，以增强该框架在处理高、低层次问题（如代码、构建、测试与故障分析、异常检测等）时的能力，并在AIOps领域内进行实现及实验。

Result: 在DevOps-EVAL数据集上的大量实验表明，CoE-Ops在高层次AIOps任务的路由精度比现有专家协作方法提高了72%，在DevOps问题解决上比单一AIOps模型提升了8%准确率，且在准确率上较大规模的混合专家模型（MoE）提升了14%。

Conclusion: 提出的CoE-Ops框架能够高效整合多种模型，通过LLM分类器与检索增强机制，有效提升了AIOps任务的适应性与精度，显著优于现有单模型及混合专家方法。

Abstract: With the rapid evolution of artificial intelligence, AIOps has emerged as a
prominent paradigm in DevOps. Lots of work has been proposed to improve the
performance of different AIOps phases. However, constrained by domain-specific
knowledge, a single model can only handle the operation requirement of a
specific task,such as log parser,root cause analysis. Meanwhile, combining
multiple models can achieve more efficient results, which have been proved in
both previous ensemble learning and the recent LLM training domain. Inspired by
these works,to address the similar challenges in AIOPS, this paper first
proposes a collaboration-of-expert framework(CoE-Ops) incorporating a
general-purpose large language model task classifier. A retrieval-augmented
generation mechanism is introduced to improve the framework's capability in
handling both Question-Answering tasks with high-level(Code,build,Test,etc.)
and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed
method is implemented in the AIOps domain, and extensive experiments are
conducted on the DevOps-EVAL dataset. Experimental results demonstrate that
CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps
tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement
over single AIOps models in DevOps problem resolution, and outperforms
larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

</details>


### [43] [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)
*Sumit Soman,H. G. Ranjani,Sujoy Roychowdhury,Venkata Dharma Surya Narayana Sastry,Akshat Jain,Pranav Gangrade,Ayaaz Khan*

Main category: cs.CL

TL;DR: 本文提出对技术文档中的流程图先用视觉大语言模型提取图结构，再结合文本RAG用于高效问答。实验证明，该方法提升了问答准确率，且推理速度快、成本低。


<details>
  <summary>Details</summary>
Motivation: 当前技术文档中的问答系统（QA）常常遇到需要参考流程图等图片信息的问题，而传统的基于文本的RAG（检索增强生成）方法难以有效处理这类问题。

Method: 用视觉大语言模型（VLM）获取流程图的图结构表示，并将这些信息融合进文本RAG系统。此外，设计了端到端流程，包括技术文档处理、图片类型分类、图结构构建和与文本嵌入流程结合，实现高效检索。

Result: 在基于专有电信产品文档创建的数据集上，微调后的VLM模型获取的图结构表示与真实答案的编辑距离较低，表明其对流程图图像的表示稳健。基于这些结构进行QA表现良好，且检索性能优异，同时推理过程中减少了对VLM的依赖，降低了部署成本。

Conclusion: 将流程图等图像的结构信息与文本检索系统结合，提升了电信领域QA系统对技术文档图像信息的处理能力，并可有效降低推理成本。

Abstract: Question-Answering (QA) from technical documents often involves questions
whose answers are present in figures, such as flowcharts or flow diagrams.
Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such
questions. We leverage graph representations of flowcharts obtained from Visual
large Language Models (VLMs) and incorporate them in a text-based RAG system to
show that this approach can enable image retrieval for QA in the telecom
domain. We present the end-to-end approach from processing technical documents,
classifying image types, building graph representations, and incorporating them
with the text embedding pipeline for efficient retrieval. We benchmark the same
on a QA dataset created based on proprietary telecom product information
documents. Results show that the graph representations obtained using a
fine-tuned VLM model have lower edit distance with respect to the ground truth,
which illustrate the robustness of these representations for flowchart images.
Further, the approach for QA using these representations gives good retrieval
performance using text-based embedding models, including a telecom-domain
adapted one. Our approach also alleviates the need for a VLM in inference,
which is an important cost benefit for deployed QA systems.

</details>


### [44] [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
*Bastien Le Guellec,Kokou Adambounou,Lisa C Adams,Thibault Agripnidis,Sung Soo Ahn,Radhia Ait Chalal,Tugba Akinci D Antonoli,Philippe Amouyel,Henrik Andersson,Raphael Bentegeac,Claudio Benzoni,Antonino Andrea Blandino,Felix Busch,Elif Can,Riccardo Cau,Armando Ugo Cavallo,Christelle Chavihot,Erwin Chiquete,Renato Cuocolo,Eugen Divjak,Gordana Ivanac,Barbara Dziadkowiec Macek,Armel Elogne,Salvatore Claudio Fanni,Carlos Ferrarotti,Claudia Fossataro,Federica Fossataro,Katarzyna Fulek,Michal Fulek,Pawel Gac,Martyna Gachowska,Ignacio Garcia Juarez,Marco Gatti,Natalia Gorelik,Alexia Maria Goulianou,Aghiles Hamroun,Nicolas Herinirina,Krzysztof Kraik,Dominik Krupka,Quentin Holay,Felipe Kitamura,Michail E Klontzas,Anna Kompanowska,Rafal Kompanowski,Alexandre Lefevre,Tristan Lemke,Maximilian Lindholz,Lukas Muller,Piotr Macek,Marcus Makowski,Luigi Mannacio,Aymen Meddeb,Antonio Natale,Beatrice Nguema Edzang,Adriana Ojeda,Yae Won Park,Federica Piccione,Andrea Ponsiglione,Malgorzata Poreba,Rafal Poreba,Philipp Prucker,Jean Pierre Pruvo,Rosa Alba Pugliesi,Feno Hasina Rabemanorintsoa,Vasileios Rafailidis,Katarzyna Resler,Jan Rotkegel,Luca Saba,Ezann Siebert,Arnaldo Stanzione,Ali Fuat Tekin,Liz Toapanta Yanchapaxi,Matthaios Triantafyllou,Ekaterini Tsaoulia,Evangelia Vassalou,Federica Vernuccio,Johan Wasselius,Weilang Wang,Szymon Urban,Adrian Wlodarczak,Szymon Wlodarczak,Andrzej Wysocki,Lina Xu,Tomasz Zatonski,Shuhang Zhang,Sebastian Ziegelmayer,Gregory Kuchcinski,Keno K Bressem*

Main category: cs.CL

TL;DR: 本研究开发并开放了全球最大、多语种的虚构影像报告数据集（PARROT），包含大样本、多元元数据与英文对照，实现NLP模型跨语种和临床场景测试。数据集消除了隐私限制，有力推动医学NLP发展。


<details>
  <summary>Details</summary>
Motivation: 由于现有的医学影像报告自然语言处理（NLP）研究受限于数据集语言单一、样本量不足以及涉及隐私保护等问题，迫切需要构建一个多语言、开放共享、且隐私友好的大型影像报告数据集，以促进跨语种NLP在医学领域的发展。

Method: 该研究通过邀请来自21个国家共76位放射科医生，按照各自惯用报告方式撰写虚构的影像学报告，涵盖13种语言。每份报告配有相关元数据（解剖部位、检查类型、临床背景等），非英文报告均附有英文翻译，并统一分配ICD-10编码。另开展了由154名参与者参与的人-机报告辨别实验，以评估AI生成报告与真人报告的可辨识性。

Result: 最终收集到2,658份报告，涵盖CT、MRI、X线、超声等多种影像方式，以及多个主要解剖部位（如胸部、腹部、头部、骨盆）。在辨别实验中，参与者整体辨别正确率为53.9%；放射科医生组表现最佳，正确率为56.9%。

Conclusion: PARROT数据集是目前规模最大、样本多语种、完全开放的虚构放射学报告集，可广泛促进跨语言、跨地域、跨临床场景的医学NLP相关应用开发和验证，无需担忧隐私问题。

Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal
Annotated Radiology Reports for Open Testing), a large, multicentric,
open-access dataset of fictional radiology reports spanning multiple languages
for testing natural language processing applications in radiology. Materials
and Methods: From May to September 2024, radiologists were invited to
contribute fictional radiology reports following their standard reporting
practices. Contributors provided at least 20 reports with associated metadata
including anatomical region, imaging modality, clinical context, and for
non-English reports, English translations. All reports were assigned ICD-10
codes. A human vs. AI report differentiation study was conducted with 154
participants (radiologists, healthcare professionals, and non-healthcare
professionals) assessing whether reports were human-authored or AI-generated.
Results: The dataset comprises 2,658 radiology reports from 76 authors across
21 countries and 13 languages. Reports cover multiple imaging modalities (CT:
36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical
regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)
being most prevalent. In the differentiation study, participants achieved 53.9%
accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated
reports, with radiologists performing significantly better (56.9%, 95% CI:
53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the
largest open multilingual radiology report dataset, enabling development and
validation of natural language processing applications across linguistic,
geographic, and clinical boundaries without privacy constraints.

</details>


### [45] [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
*Rui Jiao,Yue Zhang,Jinku Li*

Main category: cs.CL

TL;DR: RELIANCE框架显著提升了大模型推理过程中每一步的事实准确率，并提出激活层可解释性分析，有望为未来提升LLMs事实健壮性提供技术路线。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLMs）在推理过程中，尽管最终答案正确，但中间推理步骤中经常存在事实性错误。尤其在医疗、法律及科研等高风险领域，这种带有自信的错误推理对用户决策构成严重危险。因此，需要提升LLMs在推理全过程中的事实准确性。

Method: 提出RELIANCE框架，包含三个核心组成部分：1）利用反事实增强数据训练的事实核查分类器，在推理链中检测细微的事实不一致性；2）使用群体相对策略优化（GRPO）强化学习，通过多维奖励同时提升事实性、连贯性和结构正确性；3）机制可解释性模块，分析事实性提升在模型激活层面的体现。

Result: 在十个最先进的大模型上评估后发现，即使是Claude-3.7和GPT-o1等领先模型，其推理事实准确率仅为81.93%和82.57%。RELIANCE可使事实健壮性提升最高达49.90%，且在Math-500、AIME-2024和GPQA等高难度基准上表现提升或持平。激活层分析还为未来通过激活层优化提升事实健壮性提供支持。

Conclusion: RELIANCE有效解决了LLMs在中间推理步骤中的事实性错误问题，大幅增强了模型推理的事实健壮性，并为后续通过激活层优化进一步提升事实性奠定了基础。

Abstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy
for Confidence Enhancement), a novel framework addressing a critical
vulnerability in Large Language Models (LLMs): the prevalence of factual
inaccuracies within intermediate reasoning steps despite correct final answers.
This phenomenon poses substantial risks in high-stakes domains including
healthcare, legal analysis, and scientific research, where erroneous yet
confidently presented reasoning can mislead users into dangerous decisions. Our
framework integrates three core components: (1) a specialized fact-checking
classifier trained on counterfactually augmented data to detect subtle factual
inconsistencies within reasoning chains; (2) a Group Relative Policy
Optimization (GRPO) reinforcement learning approach that balances factuality,
coherence, and structural correctness through multi-dimensional rewards; and
(3) a mechanistic interpretability module examining how factuality improvements
manifest in model activations during reasoning processes. Extensive evaluation
across ten state-of-the-art models reveals concerning patterns: even leading
models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of
only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual
robustness (up to 49.90% improvement) while maintaining or improving
performance on challenging benchmarks including Math-500, AIME-2024, and GPQA.
Furthermore, our activation-level analysis provides actionable insights into
how factual enhancements reshape reasoning trajectories within model
architectures, establishing foundations for future training methodologies that
explicitly target factual robustness through activation-guided optimization.

</details>


### [46] [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)
*Paul Minchella,Loïc Verlingue,Stéphane Chrétien,Rémi Vaucher,Guillaume Metzler*

Main category: cs.CL

TL;DR: SigBERT结合BERT和粗路径signature技术，将文本时序特征引入生存分析，在真实肿瘤数据集上取得了较好表现，提升了基于文本的风险预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的生存分析方法难以有效处理电子健康记录（EHR）中的复杂文本数据，特别是在顺序性方面。如何利用大量的临床文本信息提升生存分析模型的表现是一个亟待解决的问题。

Method: 提出了SigBERT框架：先用BERT抽取时间戳医疗报告的词嵌入，并平均生成句子嵌入，再通过粗路径理论的signature提取方法得到每个患者的几何特征，最后将这些特征输入到LASSO惩罚的Cox模型中用以风险评分。

Result: 在Léon Bérard Center的真实肿瘤学数据集上测试，SigBERT模型在独立测试队列上取得了C-index 0.75（标准差0.014）。

Conclusion: SigBERT能够有效集成顺序医疗文本信息，增强风险估计能力，对基于叙述的生存分析方法提供了新进展。

Abstract: Electronic medical reports (EHR) contain a vast amount of information that
can be leveraged for machine learning applications in healthcare. However,
existing survival analysis methods often struggle to effectively handle the
complexity of textual data, particularly in its sequential form. Here, we
propose SigBERT, an innovative temporal survival analysis framework designed to
efficiently process a large number of clinical reports per patient. SigBERT
processes timestamped medical reports by extracting and averaging word
embeddings into sentence embeddings. To capture temporal dynamics from the time
series of sentence embedding coordinates, we apply signature extraction from
rough path theory to derive geometric features for each patient, which
significantly enhance survival model performance by capturing complex temporal
dynamics. These features are then integrated into a LASSO-penalized Cox model
to estimate patient-specific risk scores. The model was trained and evaluated
on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a
C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT
integrates sequential medical data to enhance risk estimation, advancing
narrative-based survival analysis.

</details>


### [47] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

TL;DR: 本文提出结合自然语言处理与自适应抽样的新验证流程，大幅提升了基于代码算法健康结果识别的验证效率，在保持精度的前提下减少了大量人工工作，有助于数据库研究的可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 基于代码的算法在健康研究中广泛使用，但其准确性往往需要通过手动审查电子健康记录来验证，这一过程费时费力。提升验证效率对于数据库研究的可信度具有重要意义。

Method: 提出并应用了两种方法：（1）利用自然语言处理（NLP）减少人工审查每份病例的时间；（2）多波次自适应抽样，制定精确度达标的提前终止标准，从而控制所需审查的病例数量。

Result: NLP辅助审查将单份病历的审查时间减少40%，多波次抽样方案可减少77%的病例审查量，同时对测量精度影响有限。

Conclusion: 该流程可高效促进基于代码的算法常规化验证，有助于提升数据库研究结果的可靠性。

Abstract: Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [48] [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)
*Naomi Omeonga wa Kayembe*

Main category: cs.CL

TL;DR: 本文将任意性从传统的负面视角转为中性乃至核心机制，提出其在语言、法律、社会互动和AI可解释性中，都是保护权威和维持系统运作的结构特征。


<details>
  <summary>Details</summary>
Motivation: 批判传统理论将任意性等同于不公正、支配或规范缺陷，作者旨在重新界定任意性在各类人类系统中的功能，并用以解释法律和社会互动中的结构性不透明。

Method: 作者基于索绪尔的符号任意性理论，将其从语言领域扩展到法律、社会系统，并结合香农的信息熵模型，构建了一个将“动机-可证实性-可争议性”链条与系统控制关联起来的形式化模型。

Result: 论文提出了“动机-可证实性-可争议性”链条，解释了当链条断裂时，任意性掩盖了系统运作的内部动因，从而使权力规避问责。此外，这一框架被用于解释AI系统的可解释性问题，为相关领域提供了分析新思路。

Conclusion: 论文提出，任意性本质上是一种结构性机制，是人类系统和互动的基本功能，而非仅仅是一种规范缺陷或支配的表现。

Abstract: This article redefines arbitrariness not as a normative flaw or a symptom of
domination, but as a foundational functional mechanism structuring human
systems and interactions. Diverging from critical traditions that conflate
arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a
property enabling systems - linguistic, legal, or social - to operate
effectively while withholding their internal rationale. Building on Ferdinand
de Saussure's concept of l'arbitraire du signe, the analysis extends this
principle beyond language to demonstrate its cross-domain applicability,
particularly in law and social dynamics. The paper introduces the "Motivation
-> Constatability -> Contestability" chain, arguing that motivation functions
as a crucial interface rendering an act's logic vulnerable to intersubjective
contestation. When this chain is broken through mechanisms like
"immotivization" or "Conflict Lateralization" (exemplified by "the blur of the
wolf drowned in the fish"), acts produce binding effects without exposing their
rationale, thus precluding justiciability. This structural opacity, while
appearing illogical, is a deliberate design protecting authority from
accountability. Drawing on Shannon's entropy model, the paper formalizes
arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern
theory of arbitrariness as a neutral operator central to control as well as
care, an overlooked dimension of interpersonal relations. While primarily
developed through human social systems, this framework also illuminates a new
pathway for analyzing explainability in advanced artificial intelligence
systems.

</details>
