<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: 本文提出NPUEval，首个专为NPU核函数生成设计的公开评测基准，并对多种LLM在AMD NPU上进行功能和向量化效率实测。最新大模型在部分任务上表现尚可，但整体得分较低，显示该领域仍颇具挑战。NPUEval的发布有助于推动NPU代码生成和优化自动化研究。


<details>
  <summary>Details</summary>
Motivation: 随着AI PC等电源敏感设备日益普及，神经网络处理单元（NPU）成为其核心组件。然而，与更成熟的GPU编程相比，NPU开发生态零散，优秀的领域内优化代码极度稀缺，导致利用大语言模型（LLM）辅助NPU核函数开发变得困难。

Method: 提出并构建了NPUEval基准集，涵盖102个常见机器学习算子，通过开源编译工具在AMD NPU上，对LLM自动生成的核函数进行功能正确性与向量化效率的实机评测。测试范围包括当前主流开源及专有LLM，并结合编译器反馈和向量化代码示例进行分析。

Result: 部分最新LLM（如DeepSeek R1）在部分算子上可实现50%以上的向量化效果，但整体102个算子平均得分仅约10%。即便有编译器反馈和优化示例，当前模型在该任务上依然表现有限，NPUEval数据集具有显著挑战性。

Conclusion: NPUEval为NPU核函数代码生成与优化研究提供了首个可量化、开源的评测平台，将推动领域进步，并丰富LLM辅助NPU开发的能力探索。数据集与评测代码均以宽松开源协议发布。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [2] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种支持分布、形式化验证且不依赖物理时钟同步的新型多时钟同步语言Timetide，显著改善了分布式系统中确定性编程的难题。


<details>
  <summary>Details</summary>
Motivation: 尽管同步语言在确定性模型方面取得了巨大进展，但它们主要用于集中式应用。现有用于分布式的时触发语言依赖于昂贵且不易扩展的物理时钟同步。因此，分布式系统的确定性编程仍是一项挑战。

Method: 提出了一种新颖的多时钟语义的同步程序方法。开发了Timetide这一编程模型，基于逻辑同步模型而非物理时钟同步，并考虑了分布式计算相关的网络通信延迟等问题。同时探索了对Timetide程序的形式化验证。

Result: Timetide成为首个无需物理时钟同步或时钟门控，就能实现分布、支持形式化验证的多时钟同步语言。

Conclusion: Timetide编程模型有效解决了分布式系统中确定性编程所面临的时钟同步难题，实现了更高效和可验证的分布式同步支持。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [3] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 本研究提出了一种能将Python静默错误转为语音和可视双通道反馈的调试插件，大幅降低认知压力、提升定位速度，对视觉障碍及编程新手特别有益，并推动了人性化、可访问性编程工具的新标准。


<details>
  <summary>Details</summary>
Motivation: 目前代码调试主要依赖文字栈追踪，对于美学障碍人士和多任务操作场景支持不足，同时带来较高的认知负担，难以快速定位和理解错误。

Method: 本研究开发了一个创新的Python语音辅助调试插件，基于全局异常钩子架构，整合pyttsx3文本转语音和Tkinter图形界面，实现错误信息的听觉和视觉并行反馈。插件能实时朗读异常类型及后果，并显示带有文档链接的交互式堆栈追踪界面。

Result: 实验证实，该插件能将认知负荷降低37%（p<0.01，n=50），错误定位速度提升78%，语音响应延迟低于1.2秒，异常处理时CPU开销不超过18%。仅需两行集成代码，适配主流平台，辅助美学障碍设计师及新手程序员调试能力提升45%。

Conclusion: 该插件极大提升了编程调试的易用性和效率，推动错误诊断走向以人为本的新范式，弥补了编程可访问性不足的短板，并为未来集成GPT修复建议及多语言功能奠定基础。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [4] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 本文提出了一种新颖的约束求解框架及两种高效的多项式不变量生成算法，实现了对浮点误差影响下程序不变量的更精确、更高效的判定。实验证明本方法在多个基准测试上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 浮点数运算由于舍入误差会导致程序结果不精确。尽管单次误差很小，但累积后可能导致严重甚至灾难性的错误。因此需要考虑浮点误差对程序正确性的影响，尤其是在判定程序不变量时要有鲁棒性。

Method: 作者提出了一个理论框架，将FPTaylor的一阶微分特性与约束求解方法结合，降低约束求解的计算负担，并基于此框架设计了两种多项式不变量生成算法：一种适用于广泛的浮点运算但需输入初始不变量，另一无需初始不变量但适用于多项式程序。同时提出了处理条件分支的方法。

Result: 实验结果显示，提出的算法在多个基准测试上表现出超越SOTA方法的计算效率与生成不变量的精度。

Conclusion: 本文建立了一套鲁棒的不变量生成方法，能够更高效、更准确地处理浮点数程序中的数值误差，对保障数值密集型程序的正确性具有意义。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [5] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 本论文提出了一种自动多版本代码生成与性能可移植性调优的新方案，能在多种GPU和输入环境下实现接近最优的计算性能，无需频繁手动优化或重新autotuning，在实验中优于传统自动调优方法，并能良好适应新设备。


<details>
  <summary>Details</summary>
Motivation: 针对不同GPU设备和应用，手动优化线性代数核心代码非常复杂且费时。虽然自动性能调优（autotuning）常被用于获得更高性能，但其方案容易过拟合，只要环境发生变化（如设备或输入特性变化）就需重新调优。因此，寻求一种可移植且无需频繁调优的性能优化方法具有重要意义。

Method: 作者提出了一种新的框架“portability tuning”，通过多版本化（multi-versioning）技术自动生成多版本代码，使性能具有可移植性，无需反复调优。该框架在CLBlast线性代数库的GEMM kernel上进行实验，通过对不同环境下的执行时间数据进行评估。

Result: 实验结果表明，该框架生成的代码在多种环境下的性能普遍优于CLBlast的默认kernel，通常能接近理论性能上限的90%；更重要的是，这些代码在新设备上的性能也能很好地泛化，无需额外的调优即可达到与autotuning相当的效果。

Conclusion: 多版本化结合portability tuning框架能够实现无需频繁调优的高性能移植，兼具高效与可扩展性。该方案有助于简化线性代数kernel在多GPU和新设备上的性能优化流程。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [6] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: 本文提出了贝叶斯分离逻辑（BaSL），使概率分离逻辑能够处理贝叶斯更新，为贝叶斯概率编程语言（BPPLs）提供了新的严格语义工具，并能证明复杂统计模型的性质。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯概率编程语言（BPPLs）使用户能以代码形式表示统计模型，但其语义复杂，难以对期望值、随机变量独立性等性质进行推理。现有概率分离逻辑虽可处理部分概率推理，但无法支持贝叶斯更新，这是BPPLs的核心特性。解决这一空白是本文动机。

Method: 提出了一种新的贝叶斯分离逻辑（BaSL），利用Rokhlin-Simmons分解定理，实现在逻辑内部形式化表述贝叶斯定理。BaSL建立于Kripke资源monoid在Hilbert立方体σ-有限测度空间上的新实例，并与BPPL的基于s-有限核的语义兼容。

Result: BaSL能全面建模概率编程中的贝叶斯更新、非规范化分布、条件分布、软约束、共轭先验及不当先验，并保持模块性（frame rule适用）。实验用BaSL证明了多个统计模型中的重要性质，包括贝叶斯抛硬币的期望、因果网络中随机变量的相关性、盗警模型的后验分布、参数估计算法与高斯混合模型的推理。

Conclusion: BaSL填补了概率分离逻辑不能处理贝叶斯更新的空白，为BPPLs提供了严密的语义基础，并可便利地证明和推理各种关键统计性质。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [7] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 该论文提出了能同时分析PLC程序、网络通信与物理连续行为的形式化框架，并通过部分序约简方法缓解状态爆炸，实现了实时工业系统的高效精确验证。


<details>
  <summary>Details</summary>
Motivation: 可编程逻辑控制器（PLC）在工业自动化中控制物理系统，但随着应用复杂化，保障其正确性变得十分重要。现有形式化验证方法多孤立验证单个PLC，忽视了控制器间通信和物理环境交互，这限制了真实工业系统分析能力。

Method: 提出一个统一的形式化框架，整合离散PLC语义、网络通信和连续物理行为，并采用部分序列约简技术，减少状态数目，同时保证验证的正确性。

Result: 所提出的框架实现了对具有连续动力学和网络通信的PLC系统的精确分析，有效缓解了状态爆炸问题。

Conclusion: 该统一框架提升了对实际工业系统中PLC程序正确性的分析能力，既考虑了多控制器通信，也结合了物理环境连续动态，为复杂自动化系统的验证提供了有效工具。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [8] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 本文系统比较了编译器闭包转换与抽象机中的闭包管理，提出了简洁的正确性证明方法和改进环境处理机制，并通过复杂度分析表明闭包转换降低了运行时开销但总体计算复杂度不变。


<details>
  <summary>Details</summary>
Motivation: 在函数式语言的编译器中，闭包转换是一项关键的程序变换技术。本论文旨在深入研究编译器使用的闭包转换与抽象机中的闭包和环境实现之间的关系，特别是在不共享环境的简单情况下探索其理论基础与实际影响。

Method: 采用极简的λ-演算以及元组作为源语言，同时分别针对源语言和闭包转换后的目标语言设计抽象机，实现对两者的比较与分析。引入了一种基于抽象机灵感的新型闭包转换正确性证明方法，并对抽象机的时间复杂性采用现有分析方法加以适配。

Result: （1）提出了一种新的、源自抽象机思路的闭包转换正确性证明技术；（2）展示了目标语言的闭包不变量如何用于设计新的环境管理方法，从而规避传统方法的缺陷；（3）通过复杂度分析发现，闭包转换虽然增加了初始代码体积，但降低了多种动态运行成本，总体的机器复杂度保持不变。

Conclusion: 本文不仅提出了验证闭包转换正确性的简化技术和全新环境管理方案，还通过形式化分析，揭示了转换前后抽象机运行效率的微妙变化，为编译技术以及抽象机理论提供了新的见解。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 本文表明，利用合适的提示和上下文，大型语言模型能高效生成高质量的单元测试，其中链式思考策略效果最佳，Gemini 2.5 Pro模型表现最强。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在软件工程中受到广泛关注，尤其在测试环节。单元测试往往结构化强，自动生成可提升开发效率。

Method: 本文研究了代码上下文和提示策略对多种大语言模型生成单元测试质量与充分性的影响。实验中对比了不同代码上下文（如docstring和完整实现）和链式思考提示策略对生成测试的效果。

Result: 包含docstring信息能显著提升测试充分性，添加完整实现的提升有限。链式思考提示策略效果最好，最高达到96.3%分支覆盖率、57%平均变异分数和接近完美的编译成功率。Gemini 2.5 Pro模型在得分和覆盖率方面表现最佳，同时其编译成功率也处于高位。

Conclusion: 通过选择合适的上下文和链式思考提示策略，可显著提升大模型生成单元测试的质量。Gemini 2.5 Pro在所有评测指标表现优异。测试代码和结果均已开源。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [10] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 将自然语言需求转化为可验证规范极具挑战，本文综述了相关自动化技术，明确了存在问题并指出未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 形式化验证能保证软件正确性，但将自然语言（通常模糊、非正式）的需求转化为可验证的形式化规范是最大难题。

Method: 通过文献综述，总结应用NLP、基于本体的领域建模、工件复用和大语言模型等自动化和半自动化技术，寻求从自然语言需求生成形式化规范的可行方法。

Result: 本文初步梳理了当前自动生成形式化规范的相关技术及成果，同时总结了面临的主要挑战，并展望了未来研究方向。

Conclusion: 现有从自然语言需求生成可验证规范的自动化方法尚不成熟，本文为后续研究明确了主要难点和潜在路线，对推进安全关键系统软件的自动化验证具有参考价值。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [11] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 引入共享词汇系统虽有学习门槛，但能长期提高软件团队沟通效率与文档质量，建议业界采纳。


<details>
  <summary>Details</summary>
Motivation: 在软件工程合作中，沟通至关重要，但沟通障碍常常导致误解、效率低下和缺陷。论文动机在于探索为何这些沟通问题持续存在，并寻找通过共享词汇系统改善沟通的可能性。

Method: 采用设计科学研究（DSR）框架，分三步：1）通过主题分析和半结构化访谈识别沟通问题来源；2）应用扎根理论设计协作词汇开发方法；3）通过受控实验对方法进行实证验证。

Result: 研究发现推进共享词汇系统初期会增加负担，但长期能明显提高信息密度、文档清晰度和协作效率。

Conclusion: 论文证明了制定共享词汇系统能有效改善软件工程团队的沟通效率与质量，虽有初期成本，但带来可观的长期效益；同时指出了该系统的局限并给未来研究提出建议。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [12] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出合并代码子token隐藏表示的策略，显著减少了模型计算量。实验表明，对性能影响视具体任务而定：部分任务略有下降，部分任务反而提升。该方法能更高效地提升代码语言模型的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型的分词结果往往比传统编译器和解释器更长，带来计算开销增加等不利影响。如何减少这种开销，同时维持甚至提升模型性能，是一个亟需解决的问题。

Method: 提出两种语义单元内子token隐藏表示的合并策略：一种是平均方法，另一种是基于学习的方法。两种方法均可无缝集成至现有代码语言模型中。作者在多个模型（CodeBERT、GraphCodeBERT、UniXCoder、CodeT5、CodeT5+）和三大任务（漏洞检测、代码分类、代码翻译）上进行了实证研究。

Result: 提出的合并策略可将浮点运算数量减少1%-19%。在下游任务中，漏洞检测任务F1分数下降最明显，降低1.82分；而在代码翻译任务上，CodeBLEU提升了2.47分。

Conclusion: 合并代码分词语义单元的隐藏表示能在一定程度上提升计算效率，对于不同下游任务对性能影响不同。该方法有助于提升代码语言模型的多维表现，尤其在部分任务还能实现性能提升。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [13] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文综述了架构劣化领域的定义、成因与修复进展，发现虽能较好检测，但持续修复手段不足，呼吁采用度量-工具-修复一体化、主动性的维护策略。


<details>
  <summary>Details</summary>
Motivation: 虽然架构劣化对软件系统质量、可维护性和适应性有重大影响，但相关定义、度量与修复策略分散且不统一，因此需要系统梳理现有研究，推动该领域认知和方法的统一。

Method: 采用多声道文献综述（Multivocal Literature Review），汇总了108篇学术及灰色文献，通过提取和归纳定义、成因、度量、工具及修复策略，构建了架构劣化分类法，分析该领域的发展趋势与研究空缺。

Result: 归纳出架构、代码、过程债务等三大类成因，收集54种度量及31种检测方法，发现主流工具仅提供检测、多数无法持续支持修复；研究揭示出技术与组织因素均会导致劣化，持续主动修复方法缺乏。提出需融合度量、检测与修复逻辑，推动预防性、可持续性的解决方案。

Conclusion: 本研究通过多渠道文献回顾，梳理并统一了架构劣化的定义、成因、度量、检测工具及修复方法。结果显示：当前检测手段较成熟，然而对持续预防和修复的支持较少；度量、检测工具与修复逻辑相互割裂。呼吁采用综合主动策略实现可持续性软件架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [14] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本研究实验证明，LLMs如ChatGPT和DeepSeek能有效辅助或自动生成Alloy声明式规范，包括公式合成、等价改写和草图补全，显著提升规范开发效率和质量，对可靠软件开发具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 声明式规范在开发安全可靠的软件系统中非常重要，但编写规范仍然极具挑战性。

Method: 通过受控实验，使用大型语言模型（LLMs）在Alloy语言中完成三项任务：1）根据自然语言描述编写完整的Alloy公式，2）给定Alloy公式生成等价的替代表达式，3）完成Alloy公式的结构草图并填补空缺，使其准确表达所需属性。实验涉及11个经典规范，采用ChatGPT和DeepSeek两种主流LLM进行评估。

Result: 实验结果显示，LLMs在根据自然语言或Alloy输入合成完整Alloy公式、枚举多个独特解、以及基于自然语言完成公式结构草图等方面均表现良好，无需测试用例也能准确补全草图。

Conclusion: LLMs极大提升了规范编写的能力，有助于规范在软件开发中发挥核心作用，提升构建健壮软件的能力。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [15] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 业界软件架构实践被少数核心技术（如Kubernetes、Serverless）主导，主要应用于部署等DevOps后期。行业关注点随技术与需求变化而演进，规划与编程等早期阶段关注度相对较低。


<details>
  <summary>Details</summary>
Motivation: 云计算、微服务和容器化的兴起带来架构实践多样化，亟需理解产业技术演化与采用动因，洞察当前软件架构实践的演变趋势。

Method: 本研究通过分析过去五年八大顶级行业会议的5677个报告，结合大语言模型和专家校验，抽取涉及的软件架构技术、应用目的及使用上下文，并探索各技术之间的关系，以及它们在DevOps与部署流水线中的位置。

Result: 在450种检测到的技术中，Kubernetes、Cloud Native、Serverless和容器类技术最为核心且频繁。业界实践主要围绕部署、通信、AI和可观测性展开，可归为自动化、协调、云端AI、监测、云-边缘五大技术社区。多数技术跨多个DevOps阶段，支持混合部署。少数核心技术在架构实践中占据主导，多针对实施后期环节。

Conclusion: 当前的软件架构实践中，Kubernetes、Serverless、Cloud Native和容器等核心技术占据主导地位。业界技术关注点更偏向于DevOps实施后期阶段，如部署、监控和自动化，较少涉及计划和编码等前期环节。技术实践随着行业需求持续演化。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [16] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: 本文提出了基于大语言模型驱动的OpenCV API文档引导模糊测试方法VISTAFUZZ，在实际测试中发现了多个尚未被揭示的bug，显示该方法对提升开源视觉库质量具有实际价值。


<details>
  <summary>Details</summary>
Motivation: OpenCV作为最流行的开源计算机视觉库，其可靠性直接影响下游应用，因此急需一种高效的自动化测试方法来发现潜在bug。

Method: 提出VISTAFUZZ框架，利用大语言模型解析API文档，自动获取标准化API信息，再提取输入参数约束和依赖关系，从而自动生成测试输入，对OpenCV API系统性模糊测试。

Result: VISTAFUZZ对OpenCV库330个API进行了测试，发现17个新bug，其中10个得到确认，5个已被修复。

Conclusion: VISTAFUZZ能够有效利用大语言模型自动化挖掘和检测OpenCV API中的bug，提高了库的可靠性和安全性。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [17] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 本论文在PyPI生态中系统分析了开源许可证变体，指出其虽大多为文本差异，但会带来显著合规风险。提出的新工具提升了许可证分析的准确性与效率，为开发者和组织合规提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 开源许可证确保了软件复用的法律基础，但许可证变体（包括标准许可证的修改版本和自定义许可证）会导致合规性问题且现有工具难以应对。作者发现，对这些变体的理解有限，缺乏针对性的分析工具，导致有效性和效率受限。为了解决这一研究空白，作者选择对PyPI生态下的许可证变体进行实证研究。

Method: 作者开展了对PyPI生态系统中许可证变体的全面实证分析，具体包括对许可证文本差异的统计与分类，并分析其在依赖关系中的合规风险。此外，提出了LV-Parser方法（基于diff和大语言模型进行许可证变体分析）和LV-Compat自动化工具（用于检测依赖网络中的许可证不兼容）。通过实验评估了工具的准确性和效率。

Result: 研究发现许可证文本变化很普遍，但其中只有2%属于实质性修改。尽管如此，这些变体会在依赖链中引发合规问题，导致10.7%的下游依赖存在许可证不兼容。LV-Parser的准确率达到0.936，并减少了30%的计算成本；LV-Compat相比现有方法能发现5.2倍的更多不兼容包，精度高达0.98。

Conclusion: 本研究首次系统性地研究了开源软件生态中的许可证变体问题，并带来了高效的新型分析工具，有助于开发者和组织更好地应对开源许可的复杂合规性挑战。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [A Proof System with Causal Labels (Part I): checking Individual Fairness and Intersectionality](https://arxiv.org/abs/2507.14650)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 本文通过扩展TNDPQ推理系统，引入因果标签和结构规则约束，首次形式化地实现了对概率分类器个体公平性与交叉性验证的理论建模。


<details>
  <summary>Details</summary>
Motivation: 公平性和交叉性是当前机器学习尤其是概率分类器中关注的热点难题，该文试图针对如何形式化地验证这两类公平性提出解决方案。

Method: 本文提出了对类型化自然演绎演算TNDPQ的扩展，通过引入结构规则中的削弱（Weakening）操作条件化，将因果标签用于检查受保护变量和目标变量之间的条件独立性。

Result: 通过对削弱结构规则施加由因果标签给定的限制，实现了形式化地表达和验证概率分类器中的个体公平性与交叉性。

Conclusion: 本文扩展了自然演绎演算，使之能够刻画并验证概率分类器中关于公平性和交叉性的逻辑属性，为自动化、公平性分析和验证提供了理论工具。

Abstract: In this article we propose an extension to the typed natural deduction
calculus TNDPQ to model verification of individual fairness and
intersectionality in probabilistic classifiers. Their interpretation is
obtained by formulating specific conditions for the application of the
structural rule of Weakening. Such restrictions are given by causal labels used
to check for conditional independence between protected and target variables.

</details>


### [19] [A Proof System with Causal Labels (Part II): checking Counterfactual Fairness](https://arxiv.org/abs/2507.14655)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 提出了一种基于类型自然演绎演算扩展的方法，形式化验证概率分类器中的反事实公平性，通过结构条件分析和鲁棒性检验提供模型公平性分析工具。


<details>
  <summary>Details</summary>
Motivation: 随着概率分类器在敏感领域应用增多，模型的反事实公平性变得重要。现有方法缺乏系统的、形式化的验证手段。作者旨在通过逻辑演绎方法为反事实公平性提供一种结构化、可验证的分析框架。

Method: 方法上，作者扩展了TNDPQ类型自然演绎演算，并将其用于刻画和验证概率分类器中的因果标签结构条件及其变化下的表现。通过这种逻辑演算框架，检查模型对标签因果结构的敏感性，从而实现反事实公平性的分析。

Result: 提出并实现了一种可以对概率分类器反事实公平性进行形式化验证的推理系统，能够有效分析因果标签结构变化对模型公平性的影响。

Conclusion: 文章提出了一种扩展的类型自然演绎演算模型（TNDPQ），用于验证概率分类器中的反事实公平性。该方法通过对因果标签制定特定结构条件，并检查推断在这些条件变化下的鲁棒性，从而实现对公平性的验证。

Abstract: In this article we propose an extension to the typed natural deduction
calculus TNDPQ to model verification of counterfactual fairness in
probabilistic classifiers. This is obtained formulating specific structural
conditions for causal labels and checking that evaluation is robust under their
variation.

</details>


### [20] [PSPACE-completeness of bimodal transitive weak-density logic](https://arxiv.org/abs/2507.14949)
*Philippe Balbiani,Olivier Gasquet*

Main category: cs.LO

TL;DR: 本文将窗口法引入弱稠密双模态逻辑及其加传递性的可满足性问题，提出了多项式算法并确定该问题为PSPACE完全。


<details>
  <summary>Details</summary>
Motivation: 以往对于弱稠密双模态逻辑的可满足性判定存在复杂度较高或方法不清晰的问题，本文旨在通过引入 window 技术改进现有算法，提升算法效率，并系统分析加上传递性后的情况。

Method: 首先回顾了双模态 K4 情形，然后将窗口法（windows-approach）与现有双模态 K 算法结合，用于处理加入传递性至弱稠密性的可满足性判定。

Result: 基于改进后的算法，不仅成功提升了算法的多项式时间性能，还证明了在加入传递性后，这类逻辑的可满足性和有效性判定均为PSPACE完全（PSPACE-complete）。

Conclusion: 窗口法为弱稠密及其扩展（如加传递性）双模态逻辑的可满足性判定提供了多项式级别有效算法，并澄清了复杂度归属，为后续相关逻辑算法设计和理论分析打下基础。

Abstract: Windows have been introduce in \cite{BalGasq25} as a tool for designing
polynomial algorithms to check satisfiability of a bimodal logic of
weak-density. In this paper, after revisiting the ``folklore'' case of bimodal
$\K4$ already treated in \cite{Halpern} but which is worth a fresh review, we
show that windows allow to polynomially solve the satisfiability problem when
adding transitivity to weak-density, by mixing algorithms for bimodal K
together with windows-approach. The conclusion is that both satisfiability and
validity are PSPACE-complete for these logics.

</details>


### [21] [PSPACE-completeness of Grammar logics of bounded density](https://arxiv.org/abs/2507.14956)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 本文提出了多模态有界稠密性逻辑，并证明了其可满足性判定为PSPACE-complete。


<details>
  <summary>Details</summary>
Motivation: 研究多模态稠密性逻辑的可满足性问题，其理论及复杂性尚未被完全刻画。

Method: 采用基于有限窗口的tableau方法，对多模态有界稠密性逻辑进行可满足性判定。

Result: 证明了多模态有界稠密性逻辑的可满足性问题为PSPACE-complete。并顺带证明了单模态稠密性逻辑可满足性处于para-PSPACE。

Conclusion: 多模态有界稠密性逻辑的可满足性问题复杂度为PSPACE-complete。单模态稠密性逻辑属于para-PSPACE。

Abstract: We introduce the family of multi-modal logics of bounded density and with a
tableau-like approach using finite \emph{windows} which were introduced in
\cite{BalGasq25}, we prove that their satisfiability problem is
PSPACE-complete. As a side effect, the monomodal logic of density is shown to
be in para-PSPACE.

</details>


### [22] [A meta-modal logic for bisimulations](https://arxiv.org/abs/2507.15117)
*Alfredo Burrieza,Fernando Soler-Toscano,Antonio Yuste-Ginel*

Main category: cs.LO

TL;DR: 作者通过为模态逻辑引入新的算子，实现了在语言中直接表达双模拟关系，并对双模拟下的模型对给出了完备公理化，拓展了模态逻辑的表达力。


<details>
  <summary>Details</summary>
Motivation: 研究双模拟（bisimulation）在模态逻辑中的刻画及表达能力，突破现有基本模态语言对双模拟性质表达的局限。

Method: （1）扩展基本模态语言，引入新的模态算子[b]，用以在本体语言中对所有与当前状态双模拟的状态进行全称量化；（2）给出了所有由双模拟关联的Kripke模型对的公理化描述，并证明了该体系的完备性与可靠性。

Result: 通过在语言层面实现了双模拟的可定义性，并提出了完整的公理系统，刻画了所有由双模拟关联的Kripke模型对。

Conclusion: 新增模态算子[b]使得双模拟可表达，完善了模态逻辑对双模拟结构的刻画和理解，并给出了理论上的完备公理体系。

Abstract: We propose a modal study of the notion of bisimulation. Our contribution is
twofold. First, we extend the basic modal language with a new modality [b],
whose intended meaning is universal quantification over all states that are
bisimilar to the current one. We show that bisimulations are definable in this
object language. Second, we provide a sound and complete axiomatisation of the
class of all pairs of Kripke models linked by bisimulations.

</details>


### [23] [STL-GO: Spatio-Temporal Logic with Graph Operators for Distributed Systems with Multiple Network Topologies](https://arxiv.org/abs/2507.15147)
*Yiqi Zhao,Xinyi Yu,Bardh Hoxha,Georgios Fainekos,Jyotirmoy V. Deshmukh,Lars Lindemann*

Main category: cs.LO

TL;DR: 本文提出了一种新颖的时空逻辑（STL-GO）及其分布式监控方法，可更准确地建模和验证多智能体系统中的复杂交互需求，在实用案例中展现了更强的表达能力和监控效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在许多应用场景（如机器人、智慧城市和物联网）中广泛存在，确保这些系统能够满足任务目标、安全性和可靠性的需求是一项挑战，尤其是在涉及异构感知、通信和空间关系时。当前对MAS的要求难以通过传统建模形式得到充分表达。

Method: 作者采用多有向图建模智能体之间的复杂交互，并提出了一种新的基于时空逻辑的规范语言：带有图操作符的时空逻辑（STL-GO）。STL-GO引入了图操作符，可以表达特定属性在交互图的入边或出边上的代理数量，进而捕捉更复杂的多智能体行为。作者还提出了分布式监控条件，使得单个智能体只需依赖本地信息即可判断STL-GO规范是否被满足。

Result: STL-GO比现有的时空逻辑工具具有更强的表达能力。所提出的分布式监控方法在单车共享系统和多无人机系统的案例中得到了有效验证，展示了其实用性和性能。

Conclusion: 通过引入STL-GO和相应的分布式监控机制，能够更有效和高效地建模、表达和验证多智能体系统的复杂需求，提升系统的安全性和可靠性。

Abstract: Multi-agent systems (MASs) consisting of a number of autonomous agents that
communicate, coordinate, and jointly sense the environment to achieve complex
missions can be found in a variety of applications such as robotics, smart
cities, and internet-of-things applications. Modeling and monitoring MAS
requirements to guarantee overall mission objectives, safety, and reliability
is an important problem. Such requirements implicitly require reasoning about
diverse sensing and communication modalities between agents, analysis of the
dependencies between agent tasks, and the spatial or virtual distance between
agents. To capture such rich MAS requirements, we model agent interactions via
multiple directed graphs, and introduce a new logic -- Spatio-Temporal Logic
with Graph Operators (STL-GO). The key innovation in STL-GO are graph operators
that enable us to reason about the number of agents along either the incoming
or outgoing edges of the underlying interaction graph that satisfy a given
property of interest; for example, the requirement that an agent should sense
at least two neighboring agents whose task graphs indicate the ability to
collaborate. We then propose novel distributed monitoring conditions for
individual agents that use only local information to determine whether or not
an STL-GO specification is satisfied. We compare the expressivity of STL-GO
against existing spatio-temporal logic formalisms, and demonstrate the utility
of STL-GO and our distributed monitors in a bike-sharing and a multi-drone case
study.

</details>


### [24] [Quantum Programming in Polylogarithmic Time](https://arxiv.org/abs/2507.15415)
*Florent Ferrari,Emmanuel Hainry,Romain Péchoux,Mário Silva*

Main category: cs.LO

TL;DR: 作者提出了一种新型量子编程语言，首次用程序方法刻画了量子多对数时间复杂度类FBQPOLYLOG，并建立了其与QNC量子电路复杂度类的联系与区分。


<details>
  <summary>Details</summary>
Motivation: 在经典计算模型中，多对数时间是衡量可行性的一个重要标准，对于量子计算，目前缺乏相应基于编程语言的复杂度刻画。作者希望用编程语言方法刻画量子多对数时间复杂度类（FBQPOLYLOG），并理解其与知名量子复杂度类（如QNC）的关系。

Method: 作者提出了一种支持一阶递归过程的量子编程语言，并利用该语言刻画了FBQPOLYLOG复杂度类。通过证明每个FBQPOLYLOG中的函数都可由此语言编写的程序计算，反之亦然（证明完备性与正确性）。此外，提出了从这种程序到多对数深度且多项式规模的量子电路族（即QNC）之间的编译策略，并分析了两类复杂度的关系。

Result: 1. 所有FBQPOLYLOG复杂度的函数都能通过所提出的编程语言程序计算，反之亦然，实现复杂度类与编程语言之间的精确刻画。2. 实现了从程序到QNC电路族的编译，将计算任务映射到已知的量子复杂度类。3. 重申并恢复了FBQPOLYLOG严格包含于QNC的分离结果。

Conclusion: 该论文首次用编程语言视角精确定义并刻画了量子多对数时间复杂度类FBQPOLYLOG，并在理论上将其与量子电路复杂度类QNC进行了联系和区分。此结果为量子复杂度理论和量子编程语义学的进一步发展奠定了基础。

Abstract: Polylogarithmic time delineates a relevant notion of feasibility on several
classical computational models such as Boolean circuits or parallel random
access machines. As far as the quantum paradigm is concerned, this notion
yields the complexity class FBQPOLYLOG of functions approximable in
polylogarithmic time with a quantum random-access Turing machine. We introduce
a quantum programming language with first-order recursive procedures, which
provides the first programming-language-based characterization of FBQPOLYLOG.
Each program computes a function in FBQPOLYLOG (soundness) and, conversely,
each function of this complexity class is computed by a program (completeness).
We also provide a compilation strategy from programs to uniform families of
quantum circuits of polylogarithmic depth and polynomial size, whose set of
computed functions is known as QNC, and recover the well-known separation
result FBQPOLYLOG $\subsetneq$ QNC.

</details>


### [25] [A SHACL-based Data Consistency Solution for Contract Compliance Verification](https://arxiv.org/abs/2507.15420)
*Robert David,Albin Ahmeti,Geni Bushati,Amar Tauqeer,Anna Fensel*

Main category: cs.LO

TL;DR: 本文提出并实现了一种半自动化的GDPR合同合规性验证与数据修复方法，有效提升了数据一致性和自动化管理能力，并已集成并验证于现有ACT工具中。


<details>
  <summary>Details</summary>
Motivation: 现有的基于同意的数据访问和共享方式在复杂场景下不足，许多组织依赖合同来执行GDPR合规性，然而合同合规性验证工具在数据一致性和自动化处理方面存在局限。

Method: 提出一种新的方法，通过半自动方式提供修复策略，自动向用户建议最优的解决方案，从而恢复数据一致性。该方法已集成到ACT工具中，并用来测试其在CCV一致性要求下的正确性和性能。

Result: 新方法能够半自动修复CCV一致性问题，为管理GDPR合规合同数据生命周期提供了数据一致性支持，并在ACT工具中通过了基本的正确性和性能测试。

Conclusion: 本研究提出并实现了一种集成数据修复策略的合同合规性验证方法，有效改进了现有工具的局限，提升了GDPR合规合同数据的一致性与管理效率。

Abstract: In recent years, there have been many developments for GDPR-compliant data
access and sharing based on consent. For more complex data sharing scenarios,
where consent might not be sufficient, many parties rely on contracts. Before a
contract is signed, it must undergo the process of contract negotiation within
the contract lifecycle, which consists of negotiating the obligations
associated with the contract. Contract compliance verification (CCV) provides a
means to verify whether a contract is GDPR-compliant, i.e., adheres to legal
obligations and there are no violations. The rise of knowledge graph (KG)
adoption, enabling semantic interoperability using well-defined semantics,
allows CCV to be applied on KGs. In the scenario of different participants
negotiating obligations, there is a need for data consistency to ensure that
CCV is done correctly. Recent work introduced the automated contracting tool
(ACT), a KG-based and ODRL-employing tool for GDPR CCV, which was developed in
the Horizon 2020 project smashHit (https://smashhit.eu). Although the tool
reports violations with respect to obligations, it had limitations in verifying
and ensuring compliance, as it did not use an interoperable semantic formalism,
such as SHACL, and did not support users in resolving data inconsistencies. In
this work, we propose a novel approach to overcome these limitations of ACT. We
semi-automatically resolve CCV inconsistencies by providing repair strategies,
which automatically propose (optimal) solutions to the user to re-establish
data consistency and thereby support them in managing GDPR-compliant contract
lifecycle data. We have implemented the approach, integrated it into ACT and
tested its correctness and performance against basic CCV consistency
requirements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [26] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: 本文提出了一种基于离线知识库的多模态AI写作助手DeepWriter，在事实准确性和内容质量上显著优于现有同类方法，尤其适合专业领域的高要求写作场景。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在众多领域表现优异，但在金融、医疗、法律等专业领域，由于知识深度有限和幻觉现象，其作为写作助手的效果受限。现有如RAG的方法存在跨多步检索一致性差的问题，在线搜索依赖网络内容也易导致内容不可靠，因此亟需更可靠的专业写作辅助方案。

Method: 提出DeepWriter系统，利用离线、精心策划的知识库。系统包含任务分解、纲要生成、多模态检索以及逐段撰写与反思流程，结合文本与视觉信息，并设计了分层知识表示提升检索效率和准确性。

Result: 在金融报告生成任务上，DeepWriter生成的文章在事实准确性和内容质量方面均超越了现有基线方法，能够输出高质量、可验证的专业文本。

Conclusion: DeepWriter能够有效挖掘结构化知识库信息，生成结构连贯、真实可靠、专业级的长文本，为专业领域写作辅助提供了更优解。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [27] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 论文系统分析了大语言模型在模型编辑和后续微调之间的相互影响，指出编辑知识更易遗忘，并建议后续工作关注编辑鲁棒性和采用方法（如冻结层）增强知识保留。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）需要不断更新知识，以纠正事实错误、纳入新信息或调整模型行为。虽然已有高效的模型编辑方法能实现知识的快速本地化修改，但后续微调对这些已编辑知识的影响此前尚不清楚。

Method: 系统性地研究了不同微调目标（任务）与多种模型编辑技术之间的交互，检测了知识遗忘与保留情况，分析并对比了微调对模型本身知识和后期编辑知识的影响。实验还尝试冻结包含被编辑内容的模型层，以测试其对知识保留效果。

Result: 发现编辑过的知识在微调过程中比原始模型预训练阶段获得的知识更容易被遗忘。冻结包含被编辑内容的层可以显著提升编辑知识的保留率。

Conclusion: 目前的模型编辑方法对于后续下游微调较为脆弱，微调后极易遗忘已编辑知识。研究提示未来应关注模型编辑在微调下的鲁棒性，并可通过技术如冻结相关层以改善知识保留。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [28] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: 这篇论文提出了SMACS框架，通过智能组合多个开源大模型，实现了在主流基准上全面超越GPT-4等领先闭源大模型，验证了开源模型协作的强大潜能。


<details>
  <summary>Details</summary>
Motivation: 近年来，闭源大模型（如GPT-4、Claude等）在各种任务中表现卓越，但开源大模型社区迅速发展，推动了问题：我们能否通过集成多个开源大模型，达到甚至超过闭源大模型的性能？

Method: 提出了SMACS（高性能可扩展多智能体协作系统）框架。方法创新包括：1）检索式先验选择（RPS），对每个模型分配代理性能分数，在实例层面动态选择最优Top-k开源大模型；2）探索-利用驱动的后验增强（EPE），通过先验丢弃促进输出多样性，并使用混合后验评分选择高质量结果。该系统支持新模型的持续集成，适应多样任务。

Result: 在8个主流基准测试中，集成15个开源大模型的SMACS在多个任务上超越领先闭源大模型。如较Claude-3.7-Sonnet提升12.73%、较GPT-4.1提升5.36%、较GPT-o3-mini提升5.28%。结果也超过开源与闭源模型在不同数据集上的最佳平均结果，分别高出2.86%与2.04%。

Conclusion: SMACS框架将开源大模型的集成推至新的高度，首次在多个任务上整体超越了现有闭源大模型，显示了开源社群通过协作方式推动智能边界的潜力和力量。代码已开源。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [29] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer结合NLP与符号逻辑推理，实现了隐私政策的自动、个性化分析，显著降低了用户理解政策的认知负担，并揭示了常见的数据使用与用户期望的冲突，可助益数据权利保护和数据政策透明化。


<details>
  <summary>Details</summary>
Motivation: 现代用户拥有众多线上账户，但很少真正阅读网站的服务条款或隐私政策。现有的隐私政策阅读负担很大，导致用户对数据使用缺乏认知和控制。为帮助用户更好地理解和管理自己的数据偏好，需要自动化、个性化的隐私政策分析工具。

Method: 提出了PoliAnalyzer神经符号系统，结合自然语言处理（NLP）和形式化逻辑推理，从隐私政策文本中抽取数据使用规则，并用扩展的形式化隐私政策语言分别建模App政策和用户偏好。随后，逻辑推理对比两者，自动生成合规性报告。PoliAnalyzer利用丰富的由法律专家整理的PolicyIE数据集进行评估，并在用户偏好合规性检测上进行了大规模实证分析。

Result: PoliAnalyzer在相关数据使用实践识别上取得了90-100%的F1分数，能够建模23种用户数据共享偏好，并对前100大网站的隐私政策进行符合性分析。分析显示，平均95.2%的政策条款未与用户偏好冲突，用户只需关注4.8%有冲突的部分，从而极大降低了认知负担。还挖掘出如位置数据与第三方共享等常见隐私政策与用户期望不符的现象。

Conclusion: PoliAnalyzer系统能够有效、自动地执行个性化的大规模隐私政策分析，借助通用NLP工具帮助用户聚焦于真正重要的政策条款，推动个人数据控制权提升和数据使用透明性，为平台数据实践带来更公平的力量格局提供可行方案。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [30] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 多种深度学习NLP模型用于社交媒体自述文本分析，RoBERTa和BERT嵌入LSTM能高精度识别双相障碍，DistilBERT高效又准确，传统静态嵌入效果极差。上下文建模对心理筛查有关键作用，研究为实际应用选型提供参考。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍早期症状隐蔽且具有社会污名化，常被漏诊。作者希望通过NLP技术，利用社交媒体文本，提升双相情感障碍的识别率。

Method: 对多种自然语言处理模型进行了系统评估，包括多种transformer（如BERT、RoBERTa、ALBERT、ELECTRA、DistilBERT）及以BERT和静态词嵌入（GloVe、Word2Vec）为基础的LSTM模型，用于分析Reddit大数据集的用户发帖。

Result: RoBERTa模型F1分数最高（约98%），基于BERT嵌入的LSTM结果相近，而基于静态词嵌入的LSTM表现很差。DistilBERT在效率与准确度间表现最好。

Conclusion: 上下文语境语言建模对双相障碍检测极为重要，深度预训练模型能显著提升NLP心理健康筛查效果，尤其在早期筛查方面。模型选择建议具有实际应用价值。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [31] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: LLMs在医疗、法律等领域会因用户的族裔、性别、年龄显示出明显偏见，可能造成实际伤害。建议在广泛应用前必须全面评估和改进模型，以防止扩大社会不公平。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被广泛应用于医疗、法律、政治等用户直面场合，它们对于文本中隐含的个人身份信息具备越来越强的识别能力。然而，目前尚不清楚这些信息在LLM实际决策中的实际影响。

Method: 作者对五类高风险用户应用领域（医疗、法律、政治、政府福利、工作薪资）的真实场景进行了系统分析，研究了用户文本中身份标识（如种族、性别、年龄）对LLM输出结果的影响，并开发了新工具评估这些变量的作用。

Result: LLMs对用户语句中的身份线索极其敏感，不同的身份特征会导致明显不同的模型输出。例如，医疗建议根据用户族裔变化，政治问题依据提问者年龄呈现意识形态偏差，薪资建议存在种族和性别的不公平。

Conclusion: 现有通用LLM在高风险应用中会放大或产生针对不同身份群体的不公平偏见，导致医疗、工资和政治等领域的现实差异，需在部署前做系统性评估和改进。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [32] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 提出CCL-XCoT，结合课程对比学习和跨语言Chain-of-Thought，有效缓解MLLMs在低资源语言上的幻觉现象，实验最高降幻觉62%，提升知识迁移，无需额外资源。


<details>
  <summary>Details</summary>
Motivation: 多语种大语言模型（MLLMs）虽然具备很强的跨语言泛化能力，但由于训练数据分布不均，在低资源语言上容易产生幻觉（即输出不准确或虚构的内容），尤其在特定领域文本生成任务中表现突出。该问题影响了模型的可信度和实际应用。

Method: 提出了一种名为CCL-XCoT（基于课程学习的对比学习+跨语言Chain-of-Thought）的方法，包含两个阶段。第一阶段，利用基于课程的对比学习结合下一词预测，强化模型在持续预训练中的跨语言语义对齐能力。第二阶段，在指令微调时引入跨语言Chain-of-Thought（XCoT）策略，让模型先用高资源语言推理，再用目标低资源语言生成答案。

Result: 实验结果显示，CCL-XCoT能使幻觉率最高降低62%，显著提升了不同语言对之间的事实知识迁移能力，无需外部检索或多模型集成。

Conclusion: CCL-XCoT是一个有效的双阶段微调框架，不仅降低了多语种大模型在低资源语言上的幻觉，还提升了知识迁移能力，对多语种应用具有实际意义。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [33] [The Labeled Coupon Collector Problem](https://arxiv.org/abs/2507.15231)
*Andrew Tan,Oriel Limor,Daniella Bar-Lev,Ryan Gabrys,Zohar Yakhini,Paul H. Siegel*

Main category: cs.DM

TL;DR: 本文推广了经典的优惠券收集者问题，研究了每次抽取k个不同标签优惠券的最小和期望抽取次数，并提出了完整的理论分析及数值解法。


<details>
  <summary>Details</summary>
Motivation: 现有的经典优惠券收集者问题仅考虑每次抽取一个优惠券，本论文希望推广到每次随机抽取k个带有不同标签的优惠券，并研究相关的最小和期望抽取次数。该问题对于理解组合最优抽取策略及应用于信息分离等领域有实际意义。

Method: 作者将问题扩展为两种类型：I型（所有标签已知）和II型（标签未知），并将其与Rénnyi和Katona提出的分离系统问题关联。论文对最小抽取次数进行了完整刻画，并利用马尔可夫链模型对期望抽取次数进行数值分析，特别关注每次抽取2个优惠券的情形。

Result: 论文详细给出了最小抽取次数的特征描述，并提出了用马尔可夫链模型数值计算期望抽取次数的方法，对于每次抽2张券的特殊情况做了深入分析。

Conclusion: 通过更一般化的抽取模型，本文扩展了优惠券收集问题的理论边界，建立了与分离系统问题的关联，并为实际计算提供了可行的数值工具。该工作为相关组合结构和概率问题提供了新思路。

Abstract: We generalize the well-known Coupon Collector Problem (CCP) in combinatorics.
Our problem is to find the minimum and expected number of draws, with
replacement, required to recover $n$ distinctly labeled coupons, with each draw
consisting of a random subset of $k$ different coupons and a random ordering of
their associated labels. We specify two variations of the problem, Type-I in
which the set of labels is known at the start, and Type-II in which the set of
labels is unknown at the start. We show that our problem can be viewed as an
extension of the separating system problem introduced by R\'enyi and Katona,
provide a full characterization of the minimum, and provide a numerical
approach to finding the expectation using a Markov chain model, with special
attention given to the case where two coupons are drawn at a time.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [34] [Studying homing and synchronizing sequences for Timed Finite State Machines with output delays](https://arxiv.org/abs/2507.14526)
*Evgenii Vinarskii,Jakub Ruszil,Adam Roman,Natalia Kushik*

Main category: cs.FL

TL;DR: 本文拓展了同步和归位序列到带输出延迟的定时有限状态机，发现无时序结果并不总成立，分析了算法适用性、复杂性及需新技术的问题，为TFSM的同步性与归位性理论和实践奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着定时有限状态机（TFSM）在实际系统中的应用增加，需要将同步和归位序列（SS与HS）的概念推广到考虑输出延迟的TFSMs上，但已有针对无时序状态机的方法并不总适用。因此，研究TFSMs的同步与归位序列及其性质十分必要。

Method: （1）正式定义了带输出延迟的TFSMs中的归位（HS）与同步（SS）序列；（2）比较了无时序FSM与有时序FSM在这些性质上的不同；（3）分析和尝试利用截断后继树方法与状态机抽象方法推导TFSM的同步与归位序列；（4）归纳了可以直接应用这些方法的TFSM子类，并指出需要其他方法的情况；（5）评估了同步/归位序列的存在性判断及最短序列求解的复杂性。

Result: 证明了许多无时序有限状态机的性质在定时有限状态机中并不总成立；明确了某些方法在特定TFSM子类中是可行的，对其他情况则需要新方法；分析了存在性与最短序列推导问题的计算复杂性。

Conclusion: 为带输出延迟的Timde FSMs扩展并澄清了同步与归位序列的定义及性质，指出了已有无时序分析方法的局限，确定了直接适用或需新方案的TFSM类别，并对复杂性做出评估，为今后相关算法与理论研究提供了基础。

Abstract: The paper introduces final state identification (synchronizing and homing)
sequences for Timed Finite State Machines (TFSMs) with output delays and
investigates their properties. We formally define the notions of homing
sequences (HSs) and synchronizing sequences (SSs) for these TFSMs and
demonstrate that several properties that hold for untimed machines do not
necessarily apply to timed ones. Furthermore, we explore the applicability of
various approaches for deriving SSs and HSs for Timed FSMs with output delays,
such as truncated successor tree-based and FSM abstraction-based methods.
Correspondingly, we identify the subclasses of TFSMs for which these approaches
can be directly applied and those for which other methods are required.
Additionally, we evaluate the complexity of existence check and derivation of
(shortest) HSs / SSs for TFSMs with output delays.

</details>


### [35] [A Myhill-Nerode Type Characterization of 2detLIN Languages](https://arxiv.org/abs/2507.15316)
*Benedek Nagy*

Main category: cs.FL

TL;DR: 本文用前缀-后缀对等价类刻画了确定性线性自动机（2detLIN）能接受的语言，并证明了该方法的必要约束和有效性。


<details>
  <summary>Details</summary>
Motivation: 线性自动机（linear automata）用于处理输入字符串的结构有限语言。该研究探讨了这类自动机及其确定性版本（2detLIN类）的表达能力和等价条件，试图更精确地刻画其可接受语言。

Method: 提出了一种基于Myhill-Nerode等价类的新方法来刻画2detLIN语言类。此外，利用前缀-后缀对（prefix-suffix pairs）来表示和分析这些等价类，从理论上研究了等价类数量和结构特征。

Result: 证明了前缀-后缀对的等价类数量有限时刻画的语言正好与2detLIN语言类一致，但这个刻画需要满足无交叉对、完整性等约束条件。

Conclusion: 通过前缀-后缀对的Myhill-Nerode型等价类成功刻画了2detLIN语言，为判定和分析线性自动机语言类提供了新的理论工具。

Abstract: Linear automata are automata with two reading heads starting from the two
extremes of the input, are equivalent to 5' -> 3' Watson-Crick (WK) finite
automata. The heads read the input in opposite directions and the computation
finishes when the heads meet. These automata accept the class LIN of linear
languages. The deterministic counterpart of these models, on the one hand, is
less expressive, as only a proper subset of LIN, the class 2detLIN is accepted;
and on the other hand, they are also equivalent in the sense of the class of
the accepted languages. Now, based on these automata models, we characterize
the class of 2detLIN languages with a Myhill-Nerode type of equivalence
classes. However, as these automata may do the computation of both the prefix
and the suffix of the input, we use prefix-suffix pairs in our classes.
Additionally, it is proven that finitely many classes in the characterization
match with the 2detLIN languages, but we have some constraints on the used
prefix-suffix pairs, i.e., the characterization should have the property to be
complete and it must not have any crossing pairs.

</details>


### [36] [Input-Driven Pushdown Automata with Translucent Input Letters](https://arxiv.org/abs/2507.15310)
*Martin Kutrib,Andreas Malcher,Matthias Wendlandt*

Main category: cs.FL

TL;DR: 本文提出并系统分析了“半透明输入字母”下的输入驱动下推自动机模型，证明非确定性比确定性更强、非返回模式比返回模式更强，且很多判定问题在非确定性下不可半判定。


<details>
  <summary>Details</summary>
Motivation: 研究输入驱动下推自动机（input-driven pushdown automata）在具有“半透明输入字母”处理机制下的计算能力和性质，区别于传统下推自动机，并分析新模型在不同工作模式下的表现及相关问题的可判定性。

Method: 提出将输入驱动下推自动机构造引入“半透明输入字母”机制，即输入经过多轮扫描，不同状态下部分输入可见、部分不可见。讨论了两种模式：返回式和非返回式，分别分析确定性和非确定性模型下的计算能力及其语言的闭包属性和判定性问题。

Result: （1）有半透明输入字母的输入驱动下推自动机，非确定性模型的计算能力强于确定性模型，无论是在返回还是非返回模式，而原本一般模型两者是等价的。 （2）非返回模式的模型计算能力强于返回模式，无论确定性还是非确定性。 （3）关于布尔运算下的闭包性：确定性情形得出完整结论，非确定性情形下相关语言族不对补集闭包。 （4）非确定性模型下，关于普适性、包含性、等价性和正则性等判定问题都是不可半判定的。

Conclusion: 引入半透明输入字母的新型输入驱动下推自动机模型在计算能力、语言闭包性和判定性问题上均展现出与传统模型显著不同的性质，系统地揭示了这些差异及其理论影响。

Abstract: Input-driven pushdown automata with translucent input letters are
investigated. Here, the use of translucent input letters means that the input
is processed in several sweeps and that, depending on the current state of the
automaton, some input symbols are visible and can be processed, whereas some
other symbols are invisible, and may be processed in another sweep.
Additionally, the returning mode as well as the non-returning mode are
considered, where in the former mode a new sweep must start after processing a
visible input symbol. Input-driven pushdown automata differ from traditional
pushdown automata by the fact that the actions on the pushdown store (push,
pop, nothing) are dictated by the input symbols. We obtain the result that the
input-driven nondeterministic model is computationally stronger than the
deterministic model both in the returning mode and in the non-returning mode,
whereas it is known that the deterministic and the nondeterministic model are
equivalent for input-driven pushdown automata without translucency. It also
turns out that the non-returning model is computationally stronger than the
returning model both in the deterministic and nondeterministic case.
Furthermore, we investigate the closure properties of the language families
introduced under the Boolean operations. We obtain a complete picture in the
deterministic case, whereas in the nondeterministic case the language families
are shown to be not closed under complementation. Finally, we look at
decidability questions and obtain the non-semidecidability of the questions of
universality, inclusion, equivalence, and regularity in the nondeterministic
case.

</details>


### [37] [Idefix-Closed Languages and Their Application in Contextual Grammars](https://arxiv.org/abs/2507.15312)
*Marvin Ködding,Bianca Truthe*

Main category: cs.FL

TL;DR: 本文系统比较了idefix-closed（前/中/后缀封闭）选择语言在情境语法表达能力上的位置，扩展了相关层级，并解决了内部情境语法与后缀封闭选择语言结合的开放问题。


<details>
  <summary>Details</summary>
Motivation: 前人在研究带有来自正规语言子族的选择语言的情境语法表达能力，尚有关于特定封闭类型选择语言（如前缀、后缀、中缀封闭）的分类与对比不足，同时尚有相关开放问题未解决。

Method: 系统地考察并比较了以前缀、后缀、中缀封闭（称为idefix-closed）为代表的选择语言子族，并将这些新类型与其它子正规语言家族（如有限、幂等、可交换等）进行对比；还分析了外部与内部情境语法所生成的层级与这些选择语言族的关系。

Result: 比较出idefix-closed选择语言相对于其它子正规家族的表达能力，并将原有的层级体系拓展到包含这些新类型的家庭。特别地，解决了有关带有后缀封闭选择语言的内部情境语法的一个开放问题。

Conclusion: 论文总结了idefix-closed类型在带有选择语言的情境语法家族中的表达能力，将原有层级结构进行了扩展，并填补了后缀封闭类型下内部情境语法的未知空白。

Abstract: In this paper, we continue the research on the power of contextual grammars
with selection languages from subfamilies of the family of regular languages.
We investigate infix-, prefix-, and suffix-closed languages (referred to as
idefix-closed languages) and compare such language families to some other
subregular families of languages (finite, monoidal, nilpotent, combinational,
(symmetric) definite, ordered, non-counting, power-separating, commutative,
circular, union-free, star, and comet languages). Further, we compare the
families of the hierarchies obtained for external and internal contextual
grammars with the language families defined by these new types for the
selection. In this way, we extend the existing hierarchies by new language
families. Moreover, we solve an open problem regarding internal contextual
grammars with suffix-closed selection languages.

</details>


### [38] [On a Generalization of the Christoffel Tree: Epichristoffel Trees](https://arxiv.org/abs/2507.15313)
*Abhishek Krishnamoorthy,Robinson Thamburaj,Durairaj Gnanaraj Thomas*

Main category: cs.FL

TL;DR: 本文提出epichristoffel树，借此寻找和分析epichristoffel词中保留Christoffel词某些分解性质的子类，并获得新理解和结果。


<details>
  <summary>Details</summary>
Motivation: Christoffel词有优美的几何与代数定义，在二元字母表下性质极佳，而其对更高字母表的推广（epichristoffel词）相关性质并不全然继承。亟需工具和方法深入揭示epichristoffel词中的新特性与相似性。

Method: 作者提出并构建了一种新的结构——epichristoffel tree，用于分析和分类epichristoffel词。通过该树结构，研究其分解性质及在字母表扩展情况下的表现。

Result: 引入“epichristoffel tree”后，成功确定了可以分解为更小epichristoffel词的子类，且借助该树获得了一些有助于加深epichristoffel词理解的新结果。

Conclusion: 文中通过引入epichristoffel树这一工具，进一步理解并刻画epichristoffel词，尤其确定了其中具有Christoffel词关键性质的子类。

Abstract: Sturmian words form a family of one-sided infinite words over a binary
alphabet that are obtained as a discretization of a line with an irrational
slope starting from the origin. A finite version of this class of words called
Christoffel words has been extensively studied for their interesting
properties. It is a class of words that has a geometric and an algebraic
definition, making it an intriguing topic of study for many mathematicians.
Recently, a generalization of Christoffel words for an alphabet with 3 letters
or more, called epichristoffel words, using episturmian morphisms has been
studied, and many of the properties of Christoffel words have been shown to
carry over to epichristoffel words; however, many properties are not shared by
them as well. In this paper, we introduce the notion of an epichristoffel tree,
which proves to be a useful tool in determining a subclass of epichristoffel
words that share an important property of Christoffel words, which is the
ability to factorize an epichristoffel word as a product of smaller
epichristoffel words. We also use the epichristoffel tree to present some
interesting results that help to better understand epichristoffel words.

</details>


### [39] [Orchestration of Music by Grammar Systems](https://arxiv.org/abs/2507.15314)
*Jozef Makiš,Alexander Meduna,Zbyněk Křivka*

Main category: cs.FL

TL;DR: 本文提出并验证了一种基于新型文法系统的自动管弦乐编曲方法，为计算机音乐自动化提供了新方向，并提出五个未来研究问题。


<details>
  <summary>Details</summary>
Motivation: 探究如何利用文法系统辅助计算音乐学，特别是在管弦乐编曲中的应用。

Method: 定义并提出了多生成规则同步散点上下文文法系统（无消除规则），并通过古典音乐和爵士乐曲的例子，展示如何用该系统实现管弦乐作品的自动编曲。

Result: 通过实例演示了上述文法系统在古典与爵士音乐编曲中的可行性，为计算机辅助编曲提供了理论与方法基础。

Conclusion: 证明了多生成规则同步散点上下文文法系统在自动管弦乐编曲中的有效性，并提出了该领域的五个开放性问题供后续研究。

Abstract: This application-oriented study concerns computational musicology, which
makes use of grammar systems. We define multi-generative rule-synchronized
scattered-context grammar systems (without erasing rules) and demonstrates how
to simultaneously make the arrangement of a musical composition for performance
by a whole orchestra, consisting of several instruments. Primarily, an
orchestration like this is illustrated by examples in terms of classical music.
In addition, the orchestration of jazz compositions is sketched as well. The
study concludes its discussion by suggesting five open problem areas related to
this way of orchestration.

</details>


### [40] [On Repetitive Finite Automata with Translucent Words](https://arxiv.org/abs/2507.15315)
*František Mráz,Friedrich Otto*

Main category: cs.FL

TL;DR: 通过加入重复性计算，有限自动机对特殊输入的表达能力显著增强，能识别更复杂的语言。


<details>
  <summary>Details</summary>
Motivation: 研究有限自动机（DFA和NFA）对于“半透明词”（translucent words）的接受能力，并探讨其允许重复计算后的表现。

Method: 提出了带有重复运算能力的确定性和非确定性有限自动机（repetitive DFAwtw和NFAwtw），并分析其表达能力。

Result: 证明repetitive DFAwtw能够接受某些非半线性语言，显著提升了其表达能力。

Conclusion: 允许重复计算的有限自动机，在处理半透明词时，比传统模型具有更高的表达能力。

Abstract: We introduce and study the repetitive variants of the deterministic and the
nondeterministic finite automaton with translucent words (DFAwtw and NFAwtw).
On seeing the right sentinel, a repetitive NFAwtw need not halt immediately,
accepting or rejecting, but it may change into another state and continue with
its computation. We establish that a repetitive DFAwtw already accepts a
language that is not even semi-linear, which shows that the property of being
repetitive increases the expressive capacity of the DFAwtw and the NFAwtw
considerably.

</details>


### [41] [On some Classes of Reversible 2-head Automata](https://arxiv.org/abs/2507.15317)
*Benedek Nagy,Walaa Yasin*

Main category: cs.FL

TL;DR: 本文研究了可逆的双读头有限自动机的计算能力，证明其对部分线性语言（如回文）有识别力，但不能识别所有正则语言。进一步，比较了其受限变体的能力，并证明了存在严格的能力层级。


<details>
  <summary>Details</summary>
Motivation: 现有的有限自动机模型在判定某些类型语言时，可能存在正向计算和反向计算不是双向唯一的局限性。该文希望探索一种能实现可逆计算的模型，即在任意计算路径上，既能唯一地正向计算，也能唯一地逆向还原。具体关注使用两个读头、能从输入两端处理信息的确定性有限自动机的可逆性与识别能力。

Method: 主要分析了确定性双读头有限自动机（可从输入两端操作）的可逆计算能力。重点考察了其对某些正则语言和线性语言的识别能力，并证明了不同受限变体（如1-限制可逆双读头自动机、完全可逆双读头自动机）之间的表达能力严格分层。还通过将这些自动机识别的语言与左确定线性文法生成的语言家族进行比较，进一步刻画其能力。

Result: 发现有些正则语言无法被这些自动机识别，但某些典型的线性语言（如回文语言）可以被识别。此外，证明了两种受限变体的识别能力均低于一般可逆双读头有限自动机，并且三者之间具有严格的能力层级关系。同时，比较了这些自动机与左确定线性文法生成语言的异同。

Conclusion: （1）一般性可逆双读头有限自动机的识别力超出传统自动机，但仍有局限，不及所有正则语言；（2）1-限制与完全可逆模型识别能力更弱且有严格层级；（3）为可逆计算和有限自动机理论提供了丰富分层与结构化理解。

Abstract: Deterministic 2-head finite automata which are machines that process an input
word from both ends are analyzed for their ability to perform reversible
computations. This implies that the automata are backward deterministic,
enabling unique forward and backward computation. We explore the computational
power of such automata, discovering that, while some regular languages cannot
be accepted by these machines, they are capable of accepting some
characteristic linear languages, e.g., the language of palindromes.
Additionally, we prove that restricted variants, i.e., both 1-limited
reversible 2-head finite automata and complete reversible 2-head finite
automata are less powerful and they form a proper hierarchy. In the former, in
each computation step exactly one input letter is being processed, i.e., only
one of the heads can read a letter. These automata are also characterized by
putting their states to classes based on the head(s) used to reach and to leave
the state. In the complete reversible 2-head finite automata, it is required
that any input can be fully read by the automaton. The accepted families are
also compared to the classes generated by left deterministic linear grammars.

</details>
