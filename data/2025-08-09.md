<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.CL](#cs.CL) [Total: 41]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 本论文提出首个能保证微服务系统在混合版本（新旧并存）下更新一致性的算法，并建立了相应理论与形式化框架。研究强调语义感知对一致性维护至关重要，避免了传统方法的高资源消耗或一致性风险。


<details>
  <summary>Details</summary>
Motivation: 微服务架构在在线服务中广泛使用，但在不中断服务的情况下进行功能修改很困难。最大挑战在于避免“混合模式”下的系统一致性问题，即新旧版本的服务同时运行并通过数据存储交互，可能导致数据不一致。传统方法虽然能避免混合模式，但通常代价高昂，如资源翻倍或降低吞吐量；而“滚动更新”则可能导致严重的一致性故障。

Method: 该论文提出了首个能保证混合模式更新一致性的算法。算法基于服务操作的语义特性，例如可交换性。作者提出了一个框架，用于形式化服务原子性更新的需求，并构建了理论基础，从而推导出新算法并证明其正确性。

Result: 通过理论分析及算法设计，作者证明了任何忽略语义的混合模式更新方法都无法避免不一致问题。新提出的算法通过语义感知，能够保证混合模式下的更新一致性，实现原子性效果。

Conclusion: 论文解决了微服务架构服务在线升级时一致性维护的难题，提出了基于语义的混合模式更新一致性保障算法。理论和算法均证明了其有效性，同时阐明了语义感知在服务一致性中的关键作用。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本研究评估了GPT-4o mini在ML项目文件级日志生成中的能力。尽管约六成位置与人类一致，但过度生成严重，且难以遵循项目规范，反映目前仍难直接应用于实际开发中。


<details>
  <summary>Details</summary>
Motivation: 日志在软件开发中具有重要作用，能够帮助开发者监控系统行为和辅助调试。然而，现有工作主要关注于函数级的日志生成，文件级日志生成尤其在机器学习(ML)应用领域尚未被充分研究。鉴于大语言模型（LLMs）能够生成自然语言和代码，研究者希望评估它们在文件级日志生成方面的能力。

Method: 研究者选取171个含有日志语句的ML项目，涉及4,073个Python文件，手动去除原有日志，然后使用GPT-4o mini提示生成新的日志语句，对比分析其插入位置、日志级别、涉及变量和文本质量，并对部分结果样本进行人工分析，归纳常见模式与挑战。

Result: 结果显示，GPT-4o mini在63.91%的情况下能将日志插入与人类相同的位置，但存在高达82.66%的过度日志记录(overlogging)率。人工分析还发现模型容易在函数开头或结尾过度记录日志，难以处理大型代码块内部的日志，且常与项目日志规范不匹配。

Conclusion: 尽管LLM在文件级日志生成上展现出一定潜力，但实用化仍面临过度记录、代码块理解不足及与项目规范对齐性差等挑战。需要进一步提高模型在日志生成的准确性和契合度，以便实际应用。

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [3] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 提出自动流程，从游戏bug报告视频中自动筛选最能代表问题的一帧，极大降低人工审核成本，准确率高，助力游戏开发和质量保障。


<details>
  <summary>Details</summary>
Motivation: 游戏开发过程中，快速发布新版本和补丁会产生大量包含视频的bug报告，人工观看和处理这些视频非常耗时且难以扩展。针对这一问题，亟需自动化方法来加快bug验证和筛选流程。

Method: 提出了一套自动化处理流程，首先用FFmpeg提取视频关键帧，大幅减少需要处理的帧数；然后利用视觉语言模型（GPT-4o）根据bug描述，对关键帧进行评分和排序，选出最能代表问题的一帧。整个系统基于真实游戏（FPS）视频和JIRA bug报告进行评估。

Result: 该系统在98.79%的情况下能够捕捉到bug场景，关键帧仅占原视频的1.90%。在不同bug类型中表现较好，整体F1分数为0.79，准确率为0.89；在Lighting & Shadow类bug中F1高达0.94，Physics & Collision为0.86，UI & HUD为0.83，Animation & VFX最低为0.51。

Conclusion: 本方法能显著减少人工视频审核工作量，加快bug筛选和回归检查，为QA团队和开发者带来实质效率提升，具有较高的实际价值。

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [4] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 本文用场景和社会技术分析方法探讨了生成式AI对开源社区的冲击，梳理了四大关键领域的风险与机遇，并建议社区主动塑造未来生态。


<details>
  <summary>Details</summary>
Motivation: 面对生成式AI快速改变软件开发方式，现有开源社区缺乏有效应对框架，可能被其复杂性和模糊性所困扰，威胁到协同精神。作者希望通过理论探索帮助社区主动塑造未来。

Method: 采用场景驱动的概念性探索，运用社会技术分析框架，尤其参考麦克卢汉的Tetrad，分析生成式AI对开源软件发展的影响。

Result: 揭示了在软件实践、文档、社区参与和治理四个领域中，GenAI带来的风险与机遇，为社区和研究者提供前瞻性建议。

Conclusion: 通过采用基于麦克卢汉四分法的社会技术框架，开源社区能够主动应对生成式人工智能带来的冲击，增强社区韧性。

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [5] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 本文首次系统分析了注意力神经网络中的故障，提出了七个全新故障类别，并总结出四条可有效诊断相关故障的启发式方法，填补了现有分类体系的空白。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是现代神经网络架构的重要部分，但其独特的故障类型在现有深度学习故障分类体系中尚未被充分涵盖，这导致开发者在诊断过程中缺乏针对性的指导。

Method: 系统性地收集了555个来自实际项目的注意力神经网络(ABNNs)故障，涵盖了十个框架和多个社区平台（如GitHub、Hugging Face等），并对这些故障进行系统分析，提出了新的故障分类法和诊断启发式方法。

Result: 提出了七个专门针对注意力机制的全新故障类别，发现超过一半的ABNN故障来自注意力特有的机制。同时，识别出四个可解释33%的注意力机制独有故障的诊断启发式规则，首次为注意力模型提供系统的诊断指导。

Conclusion: 注意力机制引入了许多现有深度学习分类体系未曾涵盖的独特故障。本文不仅提出了适用于ABNN的全新故障分类体系，还提供了首批系统性诊断启发式建议，有助于开发者更有效诊断和修复相关模型故障。

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [6] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: 大型语言模型（LLMs）在OOP领域的应用尚缺系统研究，作者从多角色视角分析其赋能编程流程的关键环节，并提出优化逻辑推理与代码编写的思路，为提升OOP开发体验提供了理论与实践指导。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能，尤其是大型语言模型（LLMs）研究蓬勃发展，在各领域应用广泛，但其与面向对象编程（OOP）的结合尚未得到充分研究，尤其在提高OOP学习与代码编写以及AI工具评价方面的理解有限。

Method: 作者从OOP任务中不同利益相关者（程序员、新手、资深开发者）的视角出发，系统梳理了LLMs在OOP编程流程中可能产生重要作用的关键节点，并提出增强代码逻辑推理和编写体验的方法。

Result: 本文识别出OOP编程流程中LLMs可赋能的重要环节，建议了利用LLMs提升逻辑推理和代码编写效率的若干策略。

Conclusion: 本文为LLMs与OOP结合提供了新视角，为后续相关研究与实际应用奠定了基础，也为提升编程体验提出了具体建议。

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [7] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: 该论文分析了大型软件系统（OpenStack）中的变更依赖管理问题，发现当前依赖识别滞后且耗时高。提出基于机器学习的半自动化方案，显著提升依赖预判与识别效果，为开发流程优化提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，变更之间的依赖关系愈发关键。合理管理这些依赖有助于避免构建失败和功能部署不完整，尤其是在跨团队、跨组件协作中挑战更大。当前，依赖关系常常滞后被识别，开发者耗费大量时间寻找相关依赖，需要更高效方式主动识别。

Method: 对OpenStack系统依赖管理进行实证研究，统计分析依赖识别时机及耗时。提出并实现半自动化方案，结合两个机器学习模型：第一个模型预测变更间存在依赖的概率，第二个模型确定具体依赖的变更对。

Result: 研究发现，过去十年OpenStack中有大量变更互相依赖，51.08%的依赖仅在代码审查阶段被识别，开发者平均耗时57.12小时寻找依赖。所提模型效果显著，AUC分别为79.33%和91.89%，Brier分数分别为0.11和0.014。第二个模型在top-k召回率表现优秀，但top-k精度仍有提升空间。

Conclusion: 变更之间的依赖识别需提前介入，依赖管理效率亟需提升。文中提出的半自动化机器学习方法能明显改善依赖识别流程，提高CI/CD等关键环节的稳定性和效率。后续可进一步优化模型精度和泛化能力。

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [8] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: LadyBug 是一个开源的GitHub机器人，结合bug文本与UI操作轨迹实现高效Android错误定位，比传统文本检索方法更准，全流程可用。


<details>
  <summary>Details</summary>
Motivation: 现有的错误定位方法主要依赖文本检索，面对Android App等高度依赖UI交互的软件，其有效性受限，缺乏利用UI信息提升定位准确度的工具。

Method: 提出并实现了LadyBug——一个GitHub上的自动错误定位机器人。LadyBug结合了issue的文本信息和开发者通过设备或模拟器上传的复现UI交互轨迹，用于检索项目中最有可能包含错误的文件列表，并通过RedWing自动测试基准集进行评估。

Result: LadyBug在准确性上明显优于传统的基于文本检索的基线方法，结合UI信息显著提升了错误定位精度，且工具为开源可用。

Conclusion: LadyBug通过结合bug描述和UI信息，有效提升了Android应用错误定位的准确率，表现优于纯文本方法，并具备实用性和开放性。该工具可作为开发者定位bug的重要辅助。

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [9] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文针对RL驱动的LLM代码生成中忽视推理质量和奖励欺骗问题，提出结合推理质量评估的新基准和奖励模型，并用P-GRPO新方法提升推理对结果的正向作用，最终超越传统方法并媲美GPT-4-Turbo。


<details>
  <summary>Details</summary>
Motivation: 当前基于RL的大语言模型代码生成侧重于结果（如通过测试用例），忽视了中间推理过程的质量，而且直接对推理过程监督存在奖励欺骗问题，即模型可能会钻奖励规则的空子而非真正提升最终效果。

Method: 1）开发LCB-RB基准，包含高、低质量推理过程的偏好配对，用于推理评价。2）提出OD-based奖励训练方法，通过系统性优化和降级推理过程，沿事实准确性、逻辑严密性和连贯性等维度生成高质量偏好配对，训练出SOTA奖励模型。3）提出P-GRPO RL算法，仅对成功输出中的优质推理过程给予奖励，从而减少奖励欺骗并提升推理质量与结果对齐。

Result: OD-based训练的奖励模型在LCB-RB上取得SOTA效果，且对其它基准测试有良好泛化性。采用P-GRPO的模型在多项代码生成任务上表现优异，较仅基于结果的RL提升4.5%，与GPT-4-Turbo性能可比。同时方法对数学任务也具备良好泛化。

Conclusion: 提出了一套能有效结合推理过程质量的RL方法，通过新基准、奖励建模和P-GRPO算法，显著提升了代码及数学生成任务的性能，缓解了奖励欺骗问题，相关资源已开放。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [10] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: 本文提出将LLM与确定性方法结合，通过MetaConfigurator工具让非专家也可用自然语言轻松实现JSON Schema和数据集成，并在化学数据场景下证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 许多领域仍缺乏标准化的数据模型，创建这些模型对非专家而言是重大障碍。模型驱动工程中，模型（如 schemas）对于研究数据至关重要。作者旨在降低结构化数据建模和集成的门槛。

Method: 提出了一种混合方法，将大语言模型（LLMs）与确定性技术结合，使用户能够通过自然语言输入创建、修改 JSON Schema，并进行模式映射。该方法集成进了开源工具 MetaConfigurator，既支持可视化编辑、验证，也通过 LLM 处理 JSON、CSV、XML、YAML 的模式映射，并用确定性规则保证可扩展性与可靠性。

Result: 通过案例（化学领域）验证了方法的适用性，实现了自然语言交互下的结构化数据建模与集成，切实降低了非专家用户的使用门槛。

Conclusion: 将大语言模型与确定性技术结合，通过自然语言大幅简化了数据模式创建与集成流程，使非专业用户也能高效参与结构化数据建模，推动了数据标准化及可用性。

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [11] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: 论文提出了SX-Bench新基准，能细致评估大模型复杂多函数代码的理解与推理能力。实验证明，现有大模型在该基准下表现有较大提升空间，SX-Bench成为先进代码智能模型新一代重要评测工具。


<details>
  <summary>Details</summary>
Motivation: 目前主流的代码评测基准如HumanEval和MBPP主要关注功能正确性，CRUXEVAL等推理基准局限于单函数、低复杂度场景，导致先进大模型分数趋于饱和，难以区分模型间细微差异。亟需针对更复杂代码推理能力的评测工具。

Method: 提出了STEPWISE-CODEX-Bench (SX-Bench)，包含多函数间协作（如链式调用、嵌套循环）任务。以“计算步”作为细粒度执行评估单元，要求模型预测推理任务总步数，从而检验模型对动态执行过程的深度理解，并联合程序合成、符号执行及LLM验证自动生成高质量数据集。

Result: 评估了20余个主流大模型（含14个增强推理能力的模型），发现SX-Bench有极强区分力：即使是最先进模型OpenAI-O3，在高难推理任务上的准确率仅为78.37%，显著低于以往基准的饱和分数，揭示了模型在复杂细粒度推理上的瓶颈。

Conclusion: SX-Bench由功能验证拓展至多函数动态推理评估，为代码智能模型的深入评测提供了关键工具，显著提升了评测复杂度与区分能力。

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [12] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph是一种利用小型语言模型，自动优化软件源代码、文档和管道的框架，实验证明其可大幅提升安全性、现代化迁移效率与文档即时性，同时显著降低计算资源消耗，为下一代软件系统的自我演化提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 传统软件系统在自我演化和现代化过程中面临诸多挑战，如代码维护、管道更新、文档及时性，以及安全漏洞修复等问题，且使用大型语言模型成本高，效率低。

Method: 提出EvoGraph框架，通过将软件系统的各种工件表示为有类型的有向图，利用小型语言模型（SLMs）驱动的学习型变异操作，并用多目标适应度选择最优“幸存者”，实现代码、管道、文档及工单的自动演化和优化。

Result: 在三个基准测试中，EvoGraph能够修复83%的已知安全漏洞，实现COBOL到Java的93%功能等价转换，并将文档新鲜度维持在2分钟以内。同时，实验显示系统延迟降低40%，特性上线时间缩短7倍。进一步利用语言专属的SLMs对多种旧代码库进行现代化，语义等价性达82-96%，计算成本相比大模型减少90%。

Conclusion: EvoGraph能够有效解决遗留系统现代化过程中的契约隐含、性能保持和集成演化等问题，为实现可持续、自适应且可控的软件系统（即Software 3.0）提供了实践路径。

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [13] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: 本文提出了一个基于物联网的业务流程可持续性分析与改进的模型和方法，不仅关注环境，还涵盖多个可持续性维度，并通过旅游和医疗领域案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）通过实时数据收集和自动化，正在改变企业业务流程，展现出提升可持续性的巨大潜力。现有的商业流程管理（BPM）中的可持续性研究主要集中于环境问题，但要实现全面、长远的影响，需要系统性地涵盖环境之外的可持续性维度。

Method: 本文提出了一个概念模型和一套结构化方法，用于分析物联网提升企业业务流程可持续性的潜力。该模型正式地描述了关键的可持续性概念，将BPM与物联网关联起来，并突出物联网设备如何支持和贡献于可持续性。所提方法指导系统分析当前流程、发现改进机会，并实施面向可持续性的物联网增强业务流程。通过旅游业示例和医疗行业案例展示流程。

Result: 所提出的概念模型和方法可以有效帮助企业系统性地识别和实现不仅限于环境维度的可持续性改进，案例分析表明该方法在不同领域均有实际应用价值。

Conclusion: 系统性地利用物联网提升企业业务流程的多维可持续性是可行且有成效的，所提出的模型和方法为实际操作提供了理论和实践支持。

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 本文提出一种结合音频基础模型和语言模型的对话转录后处理技术，能自动为说话人添加年龄、性别、情绪等属性标签。方法无需特定任务微调，速度快且易于模块化。在多项说话人属性识别任务上取得了良好结果，证明了冻结的语言模型也能直接做说话人向量的比较。


<details>
  <summary>Details</summary>
Motivation: 当前对话转录常用LLM进行语法和可读性优化，但很少探讨自动丰富说话人属性标签（如年龄、性别、情感）的可行性。为提升对话文本的实用性和丰富性，亟需一种无需复杂训练的新方法。

Method: 将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型结合，通过轻量级连接器桥接音频与语言表征，实现对说话人属性的推断。部分属性是整体性的，部分是随时间变化的，不需对模型进行专门微调。

Result: 该方法在说话人属性分析上取得了有竞争力的表现，并且在不涉及微调的情况下，冻结的LLAMA模型能直接比较x-vector，实现8.8%的Equal Error Rate。

Conclusion: 该方法能够在不需要特定任务微调的情况下，通过结合冻结的音频基础模型和语言模型，对对话转录进行说话人属性标注，并在说话人识别任务中取得了有竞争力的表现。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [15] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 提出了针对多语种分词不平等问题的Parity-aware BPE算法，通过优化最劣语种的压缩，实现分词公平；大幅改善低资源语言的分词表现且对整体性能无负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有的分词算法（如BPE）由于以频率为目标，导致主流语种分词效率高，而低资源语言分词往往较长、不合理。进而加剧了不同语言用户之间的算力和经济不平等。

Method: 提出Parity-aware BPE算法，在每次合并步骤优先最大化当前分词最差语种的压缩增益，从而在全局压缩率略有牺牲的情况下，实现跨语种分词平等。

Result: 实验表明，Parity-aware BPE使各语种分词长度更加均衡，且对整体压缩率影响极小，并不会显著降低下游任务的模型表现。

Conclusion: Parity-aware BPE可以在不影响主流性能的前提下，实现不同语种分词的公平性。

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [16] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: 将音高重音检测与半监督ASR联合建模，可以大幅提升语音识别和重音检测效果，展现了韵律信息在语音模型中的重要性。


<details>
  <summary>Details</summary>
Motivation: 在半监督语音识别模型中，强调保留或重新学习韵律特征（如音高重音）可以提升整体性能，因此考虑引入重音检测以辅助ASR任务。

Method: 提出了一个联合语音识别（ASR）和音高重音检测的模型，将半监督语音表示与重音检测模块结合。

Result: 重音检测F1分数提升41%，ASR在有限资源微调下，WER降低28.3%。

Conclusion: 联合ASR和音高重音检测能显著提升语音识别和重音检测表现。

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [17] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 本文通过大规模实证发现，大语言模型无法通过现有方法实现行为一致性，且关键应用场景下以人格概念制订的安全对齐策略不可靠。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全部署时需要表现出一致的行为模式，但其“人格特质”表现尚未被充分理解。为确保在实际应用中能够预测和管理模型行为，有必要系统性研究其行为一致性。

Method: 提出了PERSIST评估框架，利用传统和新开发的适用于LLM的人格测量工具（如BFI-44、SD3），在25+开源模型（参数范围1B-671B）上进行超过50万条响应的测试。系统性地改变问题顺序、措辞、角色设定和推理模式，检测模型行为的一致性。

Result: 发现：即使是参数高达400B+的大模型也存在显著的响应变异性；仅仅改变提示顺序就会造成高达20%的测量偏移；一些预期能稳定行为的干预（如链式思维、详细人设、对话历史）反而会加剧不稳定性；适用于LLM的人格测量工具与人类版本同样不稳定，说明不稳定性根源于模型架构。

Conclusion: 目前的LLM在多种规模和缓解策略下都难以实现真正的行为一致性。对于需要可预测行为的安全关键应用，这种不稳定性表明以“人格”为基础的对齐策略存在根本性的不足。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [18] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: 论文提出RCR-Router，能够针对多智能体大语言模型的协同推理任务，动态、角色感知地选择上下文内存，显著减少令牌消耗并提升答案质量；新提出的评价指标更全面衡量LLM输出表现，在主流数据集上验证了系统优越性。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型（LLM）在复杂推理和协作决策任务中具有强大潜力，但现有的协调方案普遍依赖静态或全上下文的路由方式，导致令牌使用过多、内存暴露冗余、以及跨多轮交互时适应性有限。

Method: 提出RCR-Router，一个模块化并具备角色感知的上下文路由框架，可动态为每个智能体选择与其角色和任务阶段相关的语义内存子集，同时严格控制令牌预算。框架采用轻量化的评分策略引导内存选择，并将智能体输出递归集成到共享内存，实现上下文的逐步优化。此外，论文还提出了一个新评分指标Answer Quality Score，用于衡量LLM生成的解释质量，超越传统问答准确性。

Result: 在三个多跳问答数据集测试（HotPotQA、MuSiQue、2WikiMultihop）中，RCR-Router能减少最高30%的令牌使用，同时保持或提升答案质量。

Conclusion: 结构化的内存路由和输出感知的评价方式对于提升多智能体LLM系统的可扩展性和协作效率至关重要。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [19] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: 本文提出衡量LLM语言歧视的基准，显示模糊语言被系统性低估，并为AI公平性研究提供新工具。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）在各类自动化任务中表现优异，但它们在处理具有细微语言差异的内容（如社会、性别或地区语言标记）时可能存在偏见。这种偏见影响模型在公平性和社会责任领域的应用，因此需要一个系统性方法来揭示并量化这些隐性歧视问题。

Method: 本文设计了一个详尽的基准，包括100组经过验证的问题和回答，用以模拟采访场景，通过对比不同语言风格（如模糊、明确等）但内容等同的回答在LLM中的评分变化，隔离并检测语言标记下的模型偏见。同时在多维语言现象下验证方法有效性，并能精准测量模型歧视。

Result: 研究发现，在内容质量相同的前提下，模糊（hedging）语言被LLM系统性地给予更低评分，平均低25.6%。该基准能够有效识别不同模型的语言偏见，为自动化评判系统的公平性分析提供了工具。

Conclusion: 本文建立了可用于检测与测量AI系统语言歧视的基准框架，为自动化决策中的公平性评价奠定了基础。该方法为理解和解决LLM中的隐性语言歧视提供了新途径，具有广泛应用前景。

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [20] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文针对动作识别评估中的语义歧义问题，提出了动词意义聚类的评估框架。实验显示该方法更贴近人类判断，能更全面评价模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前视觉动作识别系统的评估存在挑战，主要因为动词语义和图像解释存在歧义，导致单一标准答案方式无法全面反映模型表现。

Method: 提出了一种视觉-语言聚类框架，将同义或视角不同的动词聚为动词意义簇，用于更健壮的模型评估。

Result: 在imSitu数据集分析发现，每张图片平均对应2.8个意义簇，反映了多样化的图像解读视角。多种识别模型在此新方法下评估，并与传统方法做了对比。

Conclusion: 基于聚类的评估方法更符合人类判断，比传统单一答案方式能更细致地评估模型性能。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [21] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 提出多阶段大语言模型用于提升自杀相关社会健康因子提取的准确性和可解释性，并通过对比实验和用户调研证明了方法的有效性与实用性，有望支持早期预测和预防自杀风险。


<details>
  <summary>Details</summary>
Motivation: 理解自杀事件背后的社会健康决定因素（SDoH）对于早期干预和预防至关重要。然而，以数据为驱动的方法面临如因子分布长尾、分析自杀事件前关键压力源以及模型可解释性有限等挑战。

Method: 提出了一种多阶段的大语言模型框架，用于从非结构化文本中增强对SDoH因子的提取。将该方法与主流语言模型（BioBERT、GPT-3.5-turbo）及推理模型（DeepSeek-R1）进行对比，并通过自动化和用户调研评估模型的解释性对标注效率和准确性的提升。

Result: 多阶段框架在总体SDoH提取及细粒度相关语境检索任务中表现更优。微调的小型任务特定模型能以更低推理成本取得同等或更好性能。多阶段设计增强了提取能力，同时提供解释过程提升了模型可解释性。

Conclusion: 该方法提升了从非结构化文本中提取自杀相关SDoH因子的准确性和透明度，有助于早期识别高风险个体和制定更有效的预防策略。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [22] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 作者提出先结构化分割对话、再分步抽取情感四元组的新方法，通过结构熵最小化算法划分子对话，大幅提升抽取效果并减少计算成本，达到业内领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有的对话型方面情感四元组抽取（DiaASQ）方法通常在整个对话范围内学习词关系，假设情感元素均匀分布，但实际上对话往往包含多个语义独立的子对话，导致全局学习时引入噪声。作者希望解决这一噪声问题，提升抽取准确性。

Method: 本方法将对话划分为语义独立的子对话，并采用结构熵最小化算法进行分割，以最大程度区分相关与无关话语。之后，提出两步抽取框架：首先在话语级进行单个情感元素抽取，然后在子对话级进行四元组匹配。

Result: 实验证明，该方法在DiaASQ任务上取得了当前最优性能，同时计算成本显著降低。

Conclusion: 结构化分割和分步抽取有效提升了对话情感四元组抽取的准确率和效率，为DiaASQ任务提供了高效低耗的新思路。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [23] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 通过简单微调，解码器型LLM（如LLaMA 3.2）已能实现语义解析性能逼近复杂SOTA AMR系统，为AMR解析带来更易用、有效的新方法。


<details>
  <summary>Details</summary>
Motivation: AMR解析是一种将句子意义以图形形式表达的语义分析方法。此前，多数优秀的AMR解析器需复杂设计。随着大语言模型（LLMs）兴起，探究是否可通过对解码器型LLM进行简单微调以实现高效AMR解析，具有重要意义。本文动机是检验该新思路的可行性及性能极限。

Method: 对四种不同结构的解码器型LLM（Phi 3.5、Gemma 2、LLaMA 3.2、DeepSeek R1 LLaMA Distilled）进行微调，使用LDC2020T02 Gold AMR3.0测试集，评估各模型AMR解析效果，并与当前复杂SOTA方法进行对比分析。重点关注语义性能和结构有效性。

Result: 微调后的解码器型LLM在AMR解析上取得与复杂SOTA系统相近的性能，尤其是LLaMA 3.2微调后SMATCH F1为0.804，与APT + Silver（IBM）持平，接近Graphene Smatch（MBSE）的0.854。LLaMA 3.2在语义表现上领先，Phi 3.5在结构有效性上表现最佳。

Conclusion: 单纯的微调解码器型LLM已可实现与现有复杂SOTA AMR解析器相媲美的解析效果，尤其在语义表现与结构有效性等维度展现出鲜明优势。这为AMR解析提供了更简洁、高效的新路径。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [24] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 本文提出通过少量结构优化和增强表示对齐，实现比复杂多头/多适配器结构更好的大模型多任务微调效果，Align-LoRA方法显著优于现有基线，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）需要通过高效的参数微调（PEFT）适应多任务与多领域场景。现有多任务学习（MTL）方法倾向于采用多个适配器或多头结构，以获得任务特定的知识，但其结构复杂并未必高效。本文旨在探究更简单的结构是否能取得更好或同等的性能，并挑战现有多组件范式。

Method: 本文首先对比了多适配器/多头结构与高相似度多头的简化架构，发现后者表现更优。此外，作者测试了提高单一LoRA适配器rank的方法进行多任务微调，证明了其竞争力。基于观察，提出了一种Align-LoRA方法，在共享适配器空间中，采用显式的表示对齐损失，以强化跨任务的共享表征学习。

Result: 实验证明，简化的高相似度多头结构和高rank的单适配器LoRA在多任务场景下都能取得很好的效果。Align-LoRA进一步提升了表现，显著优于所有基线方法，表明提升任务间表示对齐性可增强多任务泛化能力。

Conclusion: 多任务微调无需复杂多组件结构，通过增强任务间共享表征（如Align-LoRA）可获得更优性能。本文为LLMs多任务适应带来了更简单高效的新范式。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [25] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: 本文提出了一个统一的多模态事实核查框架MultiCheck，结合文本和图像编码、跨模态融合与对比学习方法，在公开数据集上显著优于基线，为应对复杂场景下的虚假信息检测提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态虚假信息（文本结合图像）增长，传统依赖文本的事实核查系统面临巨大挑战，因此需要能同时处理文本和视觉信息的多模态事实核查框架。

Method: 采用专门的文本和图像编码器，并结合跨模态融合模块进行细粒度特征交互。通过对比学习目标，使声明与证据在共享潜空间中语义对齐，最终用分类头判断真假。

Result: 在Factify 2数据集上，MultiCheck框架实现了加权F1得分0.84，显著超越基线方法，验证了跨模态推理的有效性。

Conclusion: 提出的MultiCheck框架能够有效对多模态（文本与图像）信息进行事实核查，表现优于现有基线方法，具有实际应用潜力。

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [26] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出BEE-RAG框架，通过熵平衡机制优化RAG长上下文处理，并用新推理和微调方法进一步提升性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，检索增强生成（RAG）被广泛用于弥补LLMs知识局限。然而，RAG面临长文本检索导致注意力稀释和熵增长无控制的问题，影响了其性能。

Method: 提出了BEE-RAG框架，通过熵不变性原则平衡上下文熵，从而优化检索文档对注意力分配的影响。此外引入了多重要性估计零样本推理策略，以及参数高效的自适应微调机制，完善不同设置下的平衡因子选择。

Result: 大量实验表明，BEE-RAG框架在多个RAG任务中表现出更优的适应性和效果，有效缓解了熵增长和注意力稀释带来的性能损失。

Conclusion: BEE-RAG通过熵平衡方法，提升了RAG在长上下文环境下的性能和鲁棒性，具有实际应用价值。

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [27] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: 本文发现大语言模型倾向于关注信息序列的开头和结尾，提出根据此注意力分布重排输入（AttnRank），在多种任务和模型上有效提升性能，且无需训练和模型改动。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在输入信息的上下文位置上表现高度敏感。理解这种位置偏差机制，有助于提升模型的推理与信息过滤能力。之前尚未系统地揭示模型内部对不同位置的信息处理差异。

Method: 通过大量实验证明，大语言模型在序列化信息（如检索文档或few-shot示例）时，对序列开头和结尾分配较高注意力，而中间部分关注度较低（称为attention basin）。据此，提出了Attention-Driven Reranking（AttnRank）：先用少量校准数据估算模型固有位置注意力偏好，再将最重要信息重排至高注意力位置，实现无训练、免修改参数、可直接接入的排序方法。

Result: AttnRank在多跳问答和few-shot in-context learning等任务中，于10种不同架构和规模的大模型上均取得了显著性能提升，无需修改或微调模型参数。

Conclusion: 模型对序列中不同位置存在系统性注意力分配偏差。只需通过智能重排输入信息，即可显著提升模型表现，而且此方法通用、轻量、无需额外训练。

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [28] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 本文提出PrinciplismQA基准，用3648道高质量题目，系统评测大语言模型的医学伦理能力。结果揭示主流模型在伦理原理理解和实际应用上仍存短板，尤其是善行原则。微调有助提升表现，但更深入伦理对齐十分关键。该基准为医疗AI伦理改进提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在医疗领域应用越来越广泛，但对其伦理推理能力的系统评估却被现有基准严重忽视。本文提出弥补相关评估空白，以促进更安全、负责任的医疗AI发展。

Method: 作者构建了PrinciplismQA基准，含3648道题，系统性评估模型对医学伦理核心原则的一致性。数据集包括权威教材甄选的选择题以及医学伦理案例文献改编的主观题，均经医学专家验证。同时对不同模型进行实验对比，涵盖未调优模型及专业领域微调后模型。

Result: 实验发现大部分大语言模型在伦理知识与实际应用之间有明显差距，尤其在善行原则（Beneficence）方面表现不佳，往往过度强调其它原则。闭源前沿模型表现最好，医学领域微调可提升模型伦理能力，但本质改进仍需更好对齐伦理知识。

Conclusion: PrinciplismQA为识别LLM医学伦理薄弱点、引导更平衡和负责任的医疗AI赋能提供了可扩展评估框架。进一步进步需强化模型对医学伦理原则的理解与动态应用能力。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [29] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: 本文针对问答系统中的幻觉文本检测任务（SemEval-2025 Task 3），提出基于上下文集成、few-shot提示和模型微调的方法，在多语言赛道上获得优异成绩，强调了上下文和模型微调在缓解幻觉中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在自然语言生成方面取得了巨大进步，但仍然容易生成错误或误导性内容（即“幻觉”），这对问答系统的实际应用构成挑战，因此有必要研究检测幻觉文本的有效方法。

Method: 本研究采用了有外部上下文和无外部上下文两类方法，包括：使用LLM进行少样本提示（few-shot prompting）、基于token级别的分类方法，以及在合成数据上微调LLM。

Result: 提出的方法在西班牙语赛道获得了榜首，在英语和德语赛道也取得了有竞争力的成绩，证明了方案在不同语言上的有效性。

Conclusion: 集成相关上下文能够显著缓解LLM产生幻觉问题，微调模型和提示工程也具有较大潜力，可以提升问答系统生成文本的可靠性。

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [30] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 本文提出了适用于资源受限环境的多模态情感推理与分类轻量模型MulCoT-RD，实验验证性能优良且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感分析(MSA)依赖参数庞大的多模态大语言模型实现情感分类，难以适应资源受限场景。论文关注如何在轻量级模型架构下，实现多模态情感推理生成和分类的联合任务(JMSRC)。

Method: 提出了一种多模态链式推理蒸馏模型MulCoT-RD，采用“教师—助手—学生”的蒸馏范式。具体过程为：首先利用高性能MLLM生成推理数据集，并训练中等规模助手模型，随后通过多任务学习机制联合训练轻量级学生模型，实现高效的多模态情感推理生成与分类。

Result: 在四个数据集上实验表明，只有3B参数的MulCoT-RD在JMSRC任务上取得了强劲表现，具备良好的泛化能力和更强的可解释性。

Conclusion: MulCoT-RD能够在资源有限环境下以轻量级模型高效地完成联合推理和分类任务，为多模态情感分析开辟了低资源部署新路径。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [31] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出以识别和保留功能网络为核心的大型语言模型结构化剪枝方法，有效解决了传统方法剪枝后模型性能下降的问题。实验验证该方法能高效剪枝，同时保持模型关键功能。


<details>
  <summary>Details</summary>
Motivation: 目前用于压缩大型语言模型（LLMs）的结构化剪枝方法，主要通过评估结构单元的重要性，剪除不重要的部分。但是这些方法大多忽略了神经元之间的相互作用和协作，导致模型整体功能结构受损，从而影响剪枝效果。因此，亟需一种能够保留模型宏观功能结构的剪枝方法。

Method: 受到人脑神经网络分功能区的启发，作者提出将LLM视为一个数字大脑，通过解构模型中的功能网络（类似于脑神经影像学中的分区方法），识别并保留这些功能网络中的关键神经元，然后再进行剪枝。具体操作包括：将LLM分解为多个功能网络，基于功能保持关键神经元，并据此实施剪枝。

Result: 实验结果表明，该方法能够成功识别出LLMs中的功能网络及其关键神经元，实现高效且性能友好的模型剪枝。剪枝后的模型在保持功能完整性的同时，显著提升了推理速度和减小了GPU显存消耗。

Conclusion: 基于功能网络保留的结构化剪枝方法能够缓解现有方法对模型功能结构的破坏，有效提升了大模型的压缩效果，为深度学习模型剪枝领域提供了新的思路。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [32] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
*Sijie Wang,Quanjiang Guo,Kai Zhao,Yawei Zhang,Xin Li,Xiang Li,Siqi Li,Rui She,Shangshu Yu,Wee Peng Tay*

Main category: cs.CL

TL;DR: 当前代码LLM训练受限于稀缺的人类指令，CodeBoost提出仅用代码片段、结合多种训练与奖励机制，有效提升了模型表现，为大规模自动化代码建模提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型（LLM）后训练高度依赖人工标注的高质量代码指令，但这类数据难以获取且难以扩展，而代码片段广泛可得，数据供需严重不平衡，限制了模型训练效果。

Method: 提出CodeBoost后训练框架，无需人工标注指令，仅基于纯代码片段进行训练。关键方法包括：（1）最大团体择优筛选形成多样代表性的训练语料；（2）双向预测任务实现正向和反向的训练目标；（3）错误感知预测机制，融合正确与错误输出的学习信号；（4）异构增强技术，丰富语义多样性；（5）异构奖励机制，根据多种奖励（格式正确性、执行反馈等）指导模型训练。

Result: CodeBoost在多个代码大模型及基准任务上均带来显著性能提升，验证了其可扩展性和训练效果。

Conclusion: CodeBoost框架无需人工指令，通过系统设计优化模型训流程，实现了兼具高效与可扩展性的代码大模型提升路径。

Abstract: Code large language models (LLMs) have become indispensable tools for
building efficient and automated coding pipelines. Existing models are
typically post-trained using reinforcement learning (RL) from general-purpose
LLMs using "human instruction-final answer" pairs, where the instructions are
usually from manual annotations. However, collecting high-quality coding
instructions is both labor-intensive and difficult to scale. On the other hand,
code snippets are abundantly available from various sources. This imbalance
presents a major bottleneck in instruction-based post-training. We propose
CodeBoost, a post-training framework that enhances code LLMs purely from code
snippets, without relying on human-annotated instructions. CodeBoost introduces
the following key components: (1) maximum-clique curation, which selects a
representative and diverse training corpus from code; (2) bi-directional
prediction, which enables the model to learn from both forward and backward
prediction objectives; (3) error-aware prediction, which incorporates learning
signals from both correct and incorrect outputs; (4) heterogeneous
augmentation, which diversifies the training distribution to enrich code
semantics; and (5) heterogeneous rewarding, which guides model learning through
multiple reward types including format correctness and execution feedback from
both successes and failures. Extensive experiments across several code LLMs and
benchmarks verify that CodeBoost consistently improves performance,
demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [33] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
*Dongxu Zhang,Ning Yang,Jihua Zhu,Jinnan Yang,Miao Xin,Baoliang Tian*

Main category: cs.CL

TL;DR: 通过系统性实验，发现推理链后期错误更能影响LLM的最终输出结果。提出ASCoT纠错机制，优先找出并修正高风险后期推理步骤，大幅提升模型推理准确率。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为推理链早期错误影响最大，但该观点缺乏系统验证，且推理链可靠性仍是瓶颈，因此作者探究推理链不同阶段错误的影响，并提出针对性改进方法。

Method: 提出了ASCoT（Adaptive Self-Correction Chain-of-Thought）方法，包括自适应验证管理器（AVM）和多角度自我纠错引擎（MSCE），利用位置影响分数函数识别和优先纠正推理链中的高风险步骤。

Result: 实验证明ASCoT在GSM8K和MATH等基准上准确率显著优于现有方法，包括标准CoT。

Conclusion: 论文提出了错误在推理链后期影响更大的新观点，并通过提出ASCoT方法显著提升了LLM的推理可靠性。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning
capabilities of Large Language Models (LLMs), yet the reliability of these
reasoning chains remains a critical challenge. A widely held "cascading
failure" hypothesis suggests that errors are most detrimental when they occur
early in the reasoning process. This paper challenges that assumption through
systematic error-injection experiments, revealing a counter-intuitive
phenomenon we term "Late-Stage Fragility": errors introduced in the later
stages of a CoT chain are significantly more likely to corrupt the final answer
than identical errors made at the beginning. To address this specific
vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought
(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive
Verification Manager (AVM) operates first, followed by the Multi-Perspective
Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score
function I(k) that assigns different weights based on the position within the
reasoning chains, addressing the Late-Stage Fragility issue by identifying and
prioritizing high-risk, late-stage steps. Once these critical steps are
identified, the MSCE applies robust, dual-path correction specifically to the
failure parts. Extensive experiments on benchmarks such as GSM8K and MATH
demonstrate that ASCoT achieves outstanding accuracy, outperforming strong
baselines, including standard CoT. Our work underscores the importance of
diagnosing specific failure modes in LLM reasoning and advocates for a shift
from uniform verification strategies to adaptive, vulnerability-aware
correction mechanisms.

</details>


### [34] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
*Sukannya Purkayastha,Nils Dycke,Anne Lauscher,Iryna Gurevych*

Main category: cs.CL

TL;DR: 元审稿决策过程复杂，作者提出基于 LLM 合成高质量对话数据，并训练专用对话代理，结果表明其在元审稿任务上优于通用模型，并能提升实际审稿效率。


<details>
  <summary>Details</summary>
Motivation: 以往将元审稿仅仅视为对评审报告的总结，但实际上它也涉及复杂的决策过程，需要权衡评审意见并将其置于更广泛的学术背景下。元审稿人可以通过对话代理获得辅助，但受限于训练数据的稀缺性，难以有效开发高质量对话助手。作者因此希望解决数据不足问题，从而训练更有效的对话助手辅助元审稿人，提高审稿效率和质量。

Method: 首先，提出利用大型语言模型（LLMs）和自我精炼策略生成面向专业领域的高质量合成对话数据。其次，基于该合成数据训练专门用于元审稿的对话代理。最后，将训练好的对话代理应用在真实的元审稿场景中进行效果评估，与现成的 LLM 助手进行对比验证。

Result: 本研究生成的合成数据质量较高，为元审稿助理的训练提供了宝贵资源。基于这些数据训练的对话代理在元审稿任务上优于现有的 LLM 助手。实际应用表明，这些代理能够有效提升元审稿的效率和效果。

Conclusion: 通过生成高质量合成数据并专门训练对话代理，能够显著提升元审稿环节的决策效率和审稿质量，对推动自动化审稿流程具有重要意义。

Abstract: Meta-reviewing is a pivotal stage in the peer-review process, serving as the
final step in determining whether a paper is recommended for acceptance. Prior
research on meta-reviewing has treated this as a summarization problem over
review reports. However, complementary to this perspective, meta-reviewing is a
decision-making process that requires weighing reviewer arguments and placing
them within a broader context. Prior research has demonstrated that
decision-makers can be effectively assisted in such scenarios via dialogue
agents. In line with this framing, we explore the practical challenges for
realizing dialog agents that can effectively assist meta-reviewers. Concretely,
we first address the issue of data scarcity for training dialogue agents by
generating synthetic data using Large Language Models (LLMs) based on a
self-refinement strategy to improve the relevance of these dialogues to expert
domains. Our experiments demonstrate that this method produces higher-quality
synthetic data and can serve as a valuable resource towards training
meta-reviewing assistants. Subsequently, we utilize this data to train dialogue
agents tailored for meta-reviewing and find that these agents outperform
\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our
agents in real-world meta-reviewing scenarios and confirm their effectiveness
in enhancing the efficiency of meta-reviewing.\footnote{Code and Data:
https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [35] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
*Nikita Dragunov,Temurbek Rahmatullaev,Elizaveta Goncharova,Andrey Kuznetsov,Anton Razzhigaev*

Main category: cs.CL

TL;DR: 本文提出SONAR-LLM，通过结合SONAR嵌入空间和token级交叉熵监督，优化生成模型训练方式，取得了优异结果并公开资源。


<details>
  <summary>Details</summary>
Motivation: 旨在保留LCM模型的句子级嵌入优势的同时，消除其扩散采样器并恢复基于似然的训练优势。

Method: 提出SONAR-LLM，一种仅用解码器的transformer，在SONAR连续嵌入空间进行推理，并使用冻结的SONAR解码器以token级交叉熵进行监督，实现语义抽象和似然训练信号的结合。

Result: SONAR-LLM在39M至1.3B参数规模区间，生成质量表现竞争力，完成模型扩展趋势、消融实验及基准测试，并公布相关代码与模型权重。

Conclusion: SONAR-LLM在生成质量方面与其它模型具有竞争力，并公开了完整的训练代码和所有预训练检查点以促进复现与后续研究。

Abstract: The recently proposed Large Concept Model (LCM) generates text by predicting
a sequence of sentence-level embeddings and training with either mean-squared
error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer
that "thinks" in the same continuous SONAR embedding space, yet is supervised
through token-level cross-entropy propagated via the frozen SONAR decoder. This
hybrid objective retains the semantic abstraction of LCM while eliminating its
diffusion sampler and restoring a likelihood-based training signal. Across
model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive
generation quality. We report scaling trends, ablations, benchmark results, and
release the complete training code and all pretrained checkpoints to foster
reproducibility and future research.

</details>


### [36] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: 提出CGRS方法，有效减少大型推理语言模型中的冗余反思与token用量，实现推理高效化且无损准确率，适用于不同架构与规模模型，提升实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型（LRLMs）常使用复杂的链式思维与反思行为（如出现“Wait”、“Alternatively”等触发词），来提升推理性能。但这种反思有时会导致“过度思考”问题：产生冗余推理步骤，浪费计算资源，提升推理成本，并降低实用性。

Method: 提出了Certainty-Guided Reflection Suppression (CGRS)方法，当模型对当前答案置信度高时，动态抑制反思触发词的生成，从而减少冗余的反思循环，同时保持推理准确性。CGRS可直接集成到现有自回归生成流程，且对模型架构无要求，无需重新训练。

Result: 在四项推理测试集（AIME24、AMC23、MATH500、GPQA-D）上，CGRS平均减少18.5%到41.9%的token使用量，且准确率没有下降。与当前最佳基线相比，CGRS在减少长度和保留性能之间取得了最佳平衡，且无论模型规模或架构皆有效。

Conclusion: CGRS可以高效缓解LRLMs中的过度思考问题，降低推理成本，并广泛适用于多种模型和任务，具有较高实际应用价值。

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [37] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: Hololens 2 手语虚拟人即使增强了个性化调整，用户体验与手语理解性仍无明显提升，主要因缺乏核心手语特征与交互设计不佳。关注动画质量与参与式设计更有助于提升系统实用性。


<details>
  <summary>Details</summary>
Motivation: 目前的虚拟现实设备（如Microsoft Hololens 2）中的手语虚拟人对手语使用者的支持有限，特别是在可调整性和个性化方面。作者希望探索增加可调整特性的影响，以提升用户体验和系统可接受性。

Method: 采用对比实验法，让专家级德国语言（DGS）手语用户在具体场景下体验可调整和不可调整的手语虚拟人，并分析关键因素对可理解性、用户体验（UX）与接受度的影响。

Result: 尽管用户偏好可调整设置，但在用户体验与手语理解性方面未见显著提升。由于缺乏手语关键元素（如口型、面部表情）和实现问题（如手型不清晰、反馈和菜单位置不理想），系统整体理解性和实用性较低。调节型虚拟人甚至导致更高压力及挫折感，且交互手势不够直观。个性化概念虽然受到认可，但其接受度高度依赖于系统可用性和动画质量。

Conclusion: 单靠个性化设置难以解决手语虚拟人的核心问题——系统本身默认需达较高可理解性。作者建议提升面部表情、口型动画，优化交互界面，并采用参与式设计以更好满足手语用户需求。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


### [38] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
*Samy Ateia,Udo Kruschwitz*

Main category: cs.CL

TL;DR: 本研究探索了通过让大语言模型自动生成、评估和修正搜索输出的自反馈机制，在生物医学专业搜索任务中的应用，并比较了推理型与非推理型模型的表现，初步发现自反馈对某些模型的性能有提升，但效用因模型和任务而异，对今后如何结合机器反馈与专家人工反馈提供了参考。


<details>
  <summary>Details</summary>
Motivation: 目前基于代理的RAG和“深度研究”系统试图让大语言模型实现自主搜索并迭代优化结果，但在专业领域搜索（如生物医学）中面临用户参与度降低和专业需求难以契合等挑战，因此需要开发更适合专家使用、并提高透明度的搜索系统。

Method: 采用自反馈机制，让大语言模型自动生成并评估自身输出，随后进行自我修正，主要用于查询扩展和多种回答类型（是/否、事实、列表、理想）。对是否能提升性能以及推理模型在生成有用反馈方面的能力进行了分析。

Result: 自反馈策略在部分模型和任务下有助于提升性能，但不同模型之间表现存在差异。

Conclusion: 初步结果显示，不同的自反馈策略在各模型和任务中的表现差异明显。

Abstract: Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim
to enable autonomous search processes where Large Language Models (LLMs)
iteratively refine outputs. However, applying these systems to domain-specific
professional search, such as biomedical research, presents challenges, as
automated systems may reduce user involvement and misalign with expert
information needs. Professional search tasks often demand high levels of user
expertise and transparency. The BioASQ CLEF 2025 challenge, using
expert-formulated questions, can serve as a platform to study these issues. We
explored the performance of current reasoning and nonreasoning LLMs like
Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our
methodology was a self-feedback mechanism where LLMs generated, evaluated, and
then refined their outputs for query expansion and for multiple answer types
(yes/no, factoid, list, ideal). We investigated whether this iterative
self-correction improves performance and if reasoning models are more capable
of generating useful feedback. Preliminary results indicate varied performance
for the self-feedback strategy across models and tasks. This work offers
insights into LLM self-correction and informs future work on comparing the
effectiveness of LLM-generated feedback with direct human expert input in these
search systems.

</details>


### [39] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
*Eleftherios Avramidis,Vera Czehmann,Fabian Deckert,Lorenz Hufe,Aljoscha Lipski,Yuni Amaloa Quintero Villalobos,Tae Kwon Rhee,Mengqian Shi,Lennart Stölting,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本论文构建了12种手语及对应主流口语字幕的并行视频语料库，弥补了多语种、尤其是拉美手语语料的空白，将有力推进手语相关研究与技术发展。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、多语言手语与口语平行语料库，特别是拉美地区手语数据资源的稀缺，阻碍了手语研究和相关技术的发展。

Method: 从多种在线渠道（主要为新闻节目、政府和教育频道的广播素材）收集多种手语视频，并配以对应国家主要口语的字幕。数据准备过程包括数据收集、联系内容创作者及获取使用许可、爬取、剪辑等环节。

Result: 收集并处理了12种手语的平行语料，总计4,381个视频文件，时长超过1,300小时，含有130万条字幕、1400万词元。特别收录了8种拉美手语的首个稳定平行语料库，德国语言手语语料库规模提升十倍。

Conclusion: 该语料库极大丰富了手语与口语平行资源，尤其填补了拉美手语等小语种空白，为后续手语识别、翻译和语言学研究提供了宝贵数据支持。

Abstract: We present a collection of parallel corpora of 12 sign languages in video
format, together with subtitles in the dominant spoken languages of the
corresponding countries. The entire collection includes more than 1,300 hours
in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.
Most notably, it includes the first consistent parallel corpora for 8 Latin
American sign languages, whereas the size of the German Sign Language corpora
is ten times the size of the previously available corpora. The collection was
created by collecting and processing videos of multiple sign languages from
various online sources, mainly broadcast material of news shows, governmental
bodies and educational channels. The preparation involved several stages,
including data collection, informing the content creators and seeking usage
approvals, scraping, and cropping. The paper provides statistics on the
collection and an overview of the methods used to collect the data.

</details>


### [40] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 本文提出MyCulture基准，用于系统评估大语言模型对马来西亚文化的理解，采用开放式多选、马来语表达，发现现有模型表现出显著文化与语言偏见，强调多元文化基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型由于训练数据主要来源于英语和中文等高资源语言，导致存在明显的文化偏见，难以准确评估和代表如马来语等低资源语言及文化背景。

Method: 提出了MyCulture基准，专门针对马来西亚文化的六大领域（艺术、服饰、习俗、娱乐、美食和宗教），采用马来语，并设计了无固定选项的开放式多选题来减少猜测和格式偏见。同时，比较结构化与非结构化输出、分析多语言提示下的语言偏见，并进行理论分析。

Result: 在一系列区域性和国际大语言模型上实验发现，各模型对马来西亚文化的理解存在显著差异，表现出明显的结构性和语言性偏见。

Conclusion: 当前大语言模型在多元文化理解上有欠缺，亟需文化嵌入和多语言包容的基准用于训练和评测。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [41] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
*Ming Zhang,Yujiong Shen,Jingyi Deng,Yuhui Wang,Yue Zhang,Junzhe Wang,Shichun Liu,Shihan Dou,Huayu Sha,Qiyuan Peng,Changhao Jiang,Jingqi Tong,Yilong Wu,Zhihao Zhang,Mingqi Wu,Zhiheng Xi,Mingxu Chai,Tao Liang,Zhihui Fei,Zhen Wang,Mingyang Wan,Guojun Ma,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMEval-3通过动态采样和防作弊机制，为LLM提供更真实、公正的能力评估，揭示了静态基准的局限与模型的实际短板，有助于建立更可信赖的评测标准。


<details>
  <summary>Details</summary>
Motivation: 静态基准测试易受数据污染和排行榜过拟合影响，难以真实反映LLM能力，因此亟需一个动态、抗污染且公正的新评测框架。

Method: LLMEval-3系统基于22万道研究生水平题库，每次评测动态采样未见过的测试集，采用自动化流程防控数据污染、作弊，并利用经过校准的LLM评判体系（与人类专家有90%一致率）和相对排名机制，进行模型评价。

Result: 历时20个月对约50个主流模型的研究表明，LLMEval-3在知识记忆评测上揭示了性能天花板，并成功发现了静态基准检测不到的数据污染风险。此框架在排名稳定性与一致性上表现出色，强有力地验证了动态评测范式的有效性。

Conclusion: LLMEval-3提供了一种更可靠的LLM动态评测方法，能够突破静态基准测试的局限，有效反映模型实际能力，有助于推动更可信的评测标准建立。

Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is
vulnerable to data contamination and leaderboard overfitting, critical issues
that obscure true model capabilities. To address this, we introduce LLMEval-3,
a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary
bank of 220k graduate-level questions, from which it dynamically samples unseen
test sets for each evaluation run. Its automated pipeline ensures integrity via
contamination-resistant data curation, a novel anti-cheating architecture, and
a calibrated LLM-as-a-judge process achieving 90% agreement with human experts,
complemented by a relative ranking system for fair comparison. An 20-month
longitudinal study of nearly 50 leading models reveals a performance ceiling on
knowledge memorization and exposes data contamination vulnerabilities
undetectable by static benchmarks. The framework demonstrates exceptional
robustness in ranking stability and consistency, providing strong empirical
validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and
credible methodology for assessing the true capabilities of LLMs beyond
leaderboard scores, promoting the development of more trustworthy evaluation
standards.

</details>


### [42] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
*Chenzhuo Zhao,Xinda Wang,Yue Huang,Junting Lu,Ziqian Liu*

Main category: cs.CL

TL;DR: 提出TASE基准评测LLM的标记级和结构理解能力，30多款LLM测试结果均低于人类水平，揭示细粒度推理和跨语种泛化能力瓶颈，并为相关研究提供公开资源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽在语义级任务上表现优异，但在需要精细控制和精确理解的应用场景下，标记级和结构性推理能力不足。缺乏标准化评测工具来诊断这类基础能力的短板，故需构建专门基准。

Method: 本文构建了一个综合性基准TASE，覆盖标记感知和结构化理解两大类共10项任务（如字符计数、标记对齐、句法结构解析、长度约束满足），支持中文、英文和韩文，包括35,927个评测实例和可扩展合成数据生成管道，并对30余种主流LLM进行评测和定制训练。

Result: 所有主流LLM在标记级任务中的表现与人类相比差距明显，暴露了在低层面理解及跨语言泛化能力上的固有弱点。定制训练虽有提升，但与人类水平仍有差距。TASE公开了数据和代码便于后续研究和迭代。

Conclusion: 人类在细粒度的标记级推理任务上表现远超当前主流LLMs（大型语言模型），显示出LLMs在精准控制和结构化理解方面仍有明显不足。TASE基准为未来改进低层次语言理解和跨语言泛化能力提供了新视角。

Abstract: While large language models (LLMs) have demonstrated remarkable performance
on high-level semantic tasks, they often struggle with fine-grained,
token-level understanding and structural reasoning--capabilities that are
essential for applications requiring precision and control. We introduce TASE,
a comprehensive benchmark designed to evaluate LLMs' ability to perceive and
reason about token-level information across languages. TASE covers 10 tasks
under two core categories: token awareness and structural understanding,
spanning Chinese, English, and Korean, with a 35,927-instance evaluation set
and a scalable synthetic data generation pipeline for training. Tasks include
character counting, token alignment, syntactic structure parsing, and length
constraint satisfaction. We evaluate over 30 leading commercial and open-source
LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a
custom Qwen2.5-14B model using the GRPO training method. Results show that
human performance significantly outpaces current LLMs, revealing persistent
weaknesses in token-level reasoning. TASE sheds light on these limitations and
provides a new diagnostic lens for future improvements in low-level language
understanding and cross-lingual generalization. Our code and dataset are
publicly available at https://github.com/cyzcz/Tase .

</details>


### [43] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
*Li-Chun Lu,Miri Liu,Pin-Chun Lu,Yufei Tian,Shao-Hua Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文系统分析了主流的创造力评估方法，发现它们在不同领域中一致性有限，并存在各自的缺陷，强调需要更贴近人类判断的新型评价标准。


<details>
  <summary>Details</summary>
Motivation: 现有的创造力评估指标在不同领域中的一致性和有效性未明确，需要系统分析它们的优缺点。

Method: 系统性地检验、分析和比较了代表性的创造力测量方法（创造力指数、困惑度、句法模板、LLM-as-a-Judge），涵盖了创意写作、非常规问题解决和研究创意等多种领域。

Result: 这些指标表现出一致性有限，各自反映不同的创造力维度。具体发现包括创造力指数仅注重词汇多样性，困惑度受模型置信度影响，句法模板无法捕捉概念创造力，LLM-as-a-Judge存在不稳定性和偏见。

Conclusion: 目前常用的创造力评估方法存在显著局限性，呼吁研发更健壮、通用且与人类判断更一致的评价框架。

Abstract: We systematically examine, analyze, and compare representative creativity
measures--creativity index, perplexity, syntactic templates, and
LLM-as-a-Judge--across diverse creative domains, including creative writing,
unconventional problem-solving, and research ideation. Our analyses reveal that
these metrics exhibit limited consistency, capturing different dimensions of
creativity. We highlight key limitations, including the creativity index's
focus on lexical diversity, perplexity's sensitivity to model confidence, and
syntactic templates' inability to capture conceptual creativity. Additionally,
LLM-as-a-Judge shows instability and bias. Our findings underscore the need for
more robust, generalizable evaluation frameworks that better align with human
judgments of creativity.

</details>


### [44] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出以逻辑分解和依赖推理为核心的LAG方法，有效提升LLM在复杂知识任务中的推理能力与稳定性，减少幻觉，优于传统RAG。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在面对需要专业知识的问题时，常常产生幻觉，现有的检索增强生成（RAG）方法虽能缓解，但在复杂推理场景下效果有限。

Method: 提出了Logic-Augmented Generation (LAG)，通过系统性的问题分解和依赖感知的推理，先将复杂问题分解为逻辑依赖的原子子问题，顺序求解并利用前面答案指导后续检索，遇到无法解决的子问题时终止推理以防错误传播，最终综合所有子问题答案产生可信结果。

Result: 在四个基准数据集上的实验表明，LAG显著提升了推理稳健性，减少了幻觉，与人类认知过程更加一致。

Conclusion: LAG为复杂知识增强任务提供了一种更为合理和鲁棒的解决方案，是现有RAG系统的重要替代。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [45] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 本论文通过“20问题”游戏，发现主流LLMs在推理不同地域与文化实体时存在显著差异，这些偏见在传统检测中不易发现。新方法与数据集为LLM隐性偏见分析提供新工具。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）经过大量调整以减轻显性偏见，但其训练数据中的隐性偏见依然存在。目前主流偏见检测通常用人工编写问题进行，这容易触发模型防护措施，难以揭示深层隐性偏见。因此，论文提出采用模型主动提问的方式，探索其推理过程中呈现的潜在地理与文化偏见。

Method: 作者设计了一个基于“20 个问题”猜谜游戏的多轮任务，并开发了新数据集 Geo20Q+，涵盖来自不同地区的知名人物和文化标志物（如食物、地标、动物）。他们在 7 种语言以及两种游戏模式下，系统对主流 LLMs 进行了评测，并分析了实体推理的地理表现差异。

Result: 实验结果表明，LLMs 在推断“全球北方”与“全球西方”地区实体时表现明显优于“全球南方”与“全球东方”，而且 Wikipedia 热度或训练集中频率只能部分解释这种差距。此外，语言种类对这些表现差距影响甚微。

Conclusion: 通过模型主动推理的自由任务，揭示了LLM推理过程中的地理和文化偏见，这类隐性偏见通常在传统人工提示检测下难以发现。Geo20Q+ 数据集和代码已开源，有助于未来相关研究。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [46] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
*Santosh T. Y. S. S,Youssef Tarek Elkhayat,Oana Ichim,Pranav Shetty,Dongsheng Wang,Zhiqiang Ma,Armineh Nourbakhsh,Xiaomo Liu*

Main category: cs.CL

TL;DR: 本文提出CoCoLex解码方法，通过置信度引导模型更多地直接复制上下文内容，显著提升了法律文本长文本生成的忠实性和效果，优于现有解码策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然能够处理复杂、长文本，但在法律领域的应用受限于其生成虚假、不准确或幻觉性内容的倾向。特别是当前的增强检索生成方法虽能引入外部知识作为生成依据，但无法保证模型真正利用和忠实于提供的内容。

Method: 提出了一种新的解码策略，称为“基于置信度引导的上下文复制解码”（CoCoLex），该方法通过结合复制上下文内容的分布和模型自身的词汇分布（动态插值），并根据模型置信度决定直接复制的比例，从而提升生成文本对上下文的忠实度。

Result: 实验证明，CoCoLex在五个法律领域基准数据集上的表现优于现有的上下文感知解码方法，尤其是在长文本生成任务中具有显著优势。

Conclusion: CoCoLex通过置信度引导提升了生成内容对上下文的忠实性，在法律文本生成等需要高度可信输出的场景中具有重要实用价值。

Abstract: Due to their ability to process long and complex contexts, LLMs can offer key
benefits to the Legal domain, but their adoption has been hindered by their
tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While
Retrieval-Augmented Generation offers a promising solution by grounding
generations in external knowledge, it offers no guarantee that the provided
context will be effectively integrated. To address this, context-aware decoding
strategies have been proposed to amplify the influence of relevant context, but
they usually do not explicitly enforce faithfulness to the context. In this
work, we introduce Confidence-guided Copy-based Decoding for Legal Text
Generation (CoCoLex)-a decoding strategy that dynamically interpolates the
model produced vocabulary distribution with a distribution derived based on
copying from the context. CoCoLex encourages direct copying based on the
model's confidence, ensuring greater fidelity to the source. Experimental
results on five legal benchmarks demonstrate that CoCoLex outperforms existing
context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [47] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 本文提出了基于采样频率的不确定性量化方法，适用于黑盒大模型的多项选择题场景，实验显示能更可靠地区分预测对错，并具备理论覆盖率保障，提升了模型在高风险领域的可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多项选择题（MCQA）中表现优异，但其易幻觉、过度自信等不可靠特性限制了它们在高风险领域的应用。需要解决其预测不确定性问题，增强其在实际场景中的可信度。

Method: 提出了一种基于频率的不确定性量化方法，适用于黑盒模型。该方法结合符合性预测（conformal prediction），对每个输入进行多次独立采样，统计出现频率最高的输出样本，并以此作为计算预测熵（PE）的参考。该框架无需模型内部logit信息，属于分布无关、模型无关的设定。

Result: 在六个LLMs和四个数据集上实验，结果表明基于频率的预测熵在识别正确与错误预测（AUROC指标）上优于传统的基于logit的PE。此外，该方法可有效控制实际误覆盖率，满足用户指定的风险水平，验证了采样频率可替代logit概率用于黑盒推理的不确定性衡量。

Conclusion: 该方法为MCQA任务提供了分布无关、模型无关且具备理论覆盖率保障的不确定性量化框架，提高了LLMs在实际高风险领域的可信度及应用价值。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [48] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
*Franziska Weeber,Tanise Ceron,Sebastian Padó*

Main category: cs.CL

TL;DR: 作者使用五种西方语言评估多语言大模型在政治观点上的表现，发现未对齐模型跨语言的观点差异极小，用英语数据对其政治立场进行偏好对齐后，模型在各语言中的观点趋于一致，表明MLLMs的观点在语言间高度可迁移，难以实现针对特定语境的精细对齐。


<details>
  <summary>Details</summary>
Motivation: 公共舆论调查显示，不同社会文化背景下的政治观点存在跨文化差异。然而，目前尚不清楚这些差异是否会在多语言大语言模型（MLLMs）中表现为跨语言的观点差异。作者希望探究MLLMs在政治观点表达上是否存在跨语言一致性或分离性。

Method: 研究团队选用五种西方语言，使用多种规模的MLLMs，通过让模型对政治声明表达（不）同意来评估其政治观点。进一步，他们仅用英语政治偏好对模型进行左或右偏对齐，然后评估模型在所有语言下的观点变化。

Result: 未对齐（unaligned）的MLLMs在政治观点上的跨语言差异很少。对齐（alignment）过程会使模型在所有五种语言上的政治观点几乎一致地发生变化。

Conclusion: MLLMs在西方语言环境下，政治观点能够跨语言迁移，说明实现模型明确的社交、语言及政治对齐面临着挑战。

Abstract: Public opinion surveys show cross-cultural differences in political opinions
between socio-cultural contexts. However, there is no clear evidence whether
these differences translate to cross-lingual differences in multilingual large
language models (MLLMs). We analyze whether opinions transfer between languages
or whether there are separate opinions for each language in MLLMs of various
sizes across five Western languages. We evaluate MLLMs' opinions by prompting
them to report their (dis)agreement with political statements from voting
advice applications. To better understand the interaction between languages in
the models, we evaluate them both before and after aligning them with more left
or right views using direct preference optimization and English alignment data
only. Our findings reveal that unaligned models show only very few significant
cross-lingual differences in the political opinions they reflect. The political
alignment shifts opinions almost uniformly across all five languages. We
conclude that in Western language contexts, political opinions transfer between
languages, demonstrating the challenges in achieving explicit socio-linguistic,
cultural, and political alignment of MLLMs.

</details>


### [49] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
*Shaoxiong Zhan,Yanlin Lai,Ziyu Lu,Dahua Lin,Ziqing Yang,Fei Tang*

Main category: cs.CL

TL;DR: MathSmith是一种新型数学问题合成框架，通过自动、独立、高难度地生成数学题显著提升了大模型的推理能力，在多个基准测试中优于现有方法，并能进行针对性训练。


<details>
  <summary>Details</summary>
Motivation: 现有高质量高难度数学数据稀缺，合成方法依赖人工模板，缺乏多样性和可扩展性，限制了大语言模型（LLM）在数学推理领域的提升。

Method: MathSmith框架采用从PlanetMath随机抽取概念-解释对，设计9种难度提升策略作为软约束，引入强化学习联合优化结构有效性、推理复杂性和答案一致性，通过自回归生成推理链长度反映问题难度，并支持根据模型弱点生成专项训练数据。

Result: 在5个数学基准测试（包含简单到高难度）上，MathSmith在短链和长链推理场景均超越现有主流方法，并显示出良好的泛化、扩展和迁移能力。

Conclusion: MathSmith通过从头生成高难度数学问题，有效提升了大模型推理能力，其生成问题的数据独立性、结构和认知复杂性均优于现有方法，并能针对概念弱点生成变体，表现出良好的扩展性和泛化能力。

Abstract: Large language models have achieved substantial progress in mathematical
reasoning, yet their advancement is limited by the scarcity of high-quality,
high-difficulty training data. Existing synthesis methods largely rely on
transforming human-written templates, limiting both diversity and scalability.
We propose MathSmith, a novel framework for synthesizing challenging
mathematical problems to enhance LLM reasoning. Rather than modifying existing
problems, MathSmith constructs new ones from scratch by randomly sampling
concept-explanation pairs from PlanetMath, ensuring data independence and
avoiding contamination. To increase difficulty, we design nine predefined
strategies as soft constraints during rationales. We further adopts
reinforcement learning to jointly optimize structural validity, reasoning
complexity, and answer consistency. The length of the reasoning trace generated
under autoregressive prompting is used to reflect cognitive complexity,
encouraging the creation of more demanding problems aligned with
long-chain-of-thought reasoning. Experiments across five benchmarks,
categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,
OlympiadBench), show that MathSmith consistently outperforms existing baselines
under both short and long CoT settings. Additionally, a weakness-focused
variant generation module enables targeted improvement on specific concepts.
Overall, MathSmith exhibits strong scalability, generalization, and
transferability, highlighting the promise of high-difficulty synthetic data in
advancing LLM reasoning capabilities.

</details>


### [50] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 论文提出了协同优化策略与奖励模型（Cooper）框架，有效缓解奖励攻击，提高了大模型强化学习推理能力，实验提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型强化学习主流奖励体系存在缺陷：规则奖励鲁棒性差，模型奖励易受攻击。为解决这些问题，需设计能兼具鲁棒性与抗攻击的奖励方法。

Method: 提出了Cooper（协同优化策略模型与奖励模型）强化学习框架，融合规则与模型奖励，动态采样积极/消极样本对优化奖励模型，并引入混合标注和参考答案机制，训练了VerifyRM奖励模型。通过在Qwen2.5-1.5B-Instruct等模型上进行实验，对比各奖励体系与训练策略效果。

Result: Cooper框架和VerifyRM奖励模型在VerifyBench等任务上精度优于同类模型。在Qwen2.5-1.5B-Instruct模型上的平均准确率提升了0.54%，进一步验证了动态优化奖励模型在防御奖励攻击和提升强化学习表现方面的有效性。

Conclusion: 动态更新奖励模型能有效缓解大语言模型强化学习中的奖励攻击风险，并提升模型推理性能。提出的Cooper框架和VerifyRM奖励模型为更好地融合奖励模型与强化学习提供了参考。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [51] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: OmniEAR框架系统评估了大语言模型在具身任务中的推理能力，发现当前模型在物理约束和多智能体协作方面表现不佳，显著区别于以往抽象推理，提出了新的研究挑战和评测标准。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在抽象推理上表现优异，但在具身智能体推理领域的能力尚未得到充分探索。作者希望建立一个系统化基准，全面评估语言模型在物理交互、工具使用和多智能体协作中的推理能力。

Method: 提出OmniEAR框架，采用文本环境表示，涵盖家居和工业场景下1500个任务，要求智能体动态获取工具和协作策略，并评估模型在显式、隐式任务指令下的推理表现。通过情景设置和任务约束，对模型在物理属性、空间关系及多智能体协调方面进行系统性测试。

Result: 在显式指令下，模型表现良好（成功率85%-96%），但在需要工具推理（56%-85%）、隐式协作（63%-85%）时明显下降。复合任务失败率超过50%。完整的环境信息反而降低了协作性能。微调显著提升单智能体任务（0.6%到76.3%），但对多智能体任务帮助有限（提升仅1.5%到5.5%）。

Conclusion: 具身推理对大语言模型提出了与抽象推理截然不同的挑战，模型尚无法应对复杂的物理约束和多智能体协作需求，OmniEAR为相关领域提供了严格的评测标准。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [52] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
*Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas Oğuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau Yih*

Main category: cs.CL

TL;DR: 提出综合事实性、细节、相关性的新奖励函数，用于在线RL提升R-LLMs长文本推理的事实准确性和细节性，实验证实大幅降低幻觉且不损失有用性。


<details>
  <summary>Details</summary>
Motivation: R-LLMs在复杂推理任务上表现出色，但在长文本事实性任务中相比非推理模型更易产生幻觉（错误信息），当前缺乏高可靠性的事实性验证方法，阻碍RL方法在该领域的应用。

Method: 分析直接使用自动事实性评分作为在线RL奖励会导致奖励作弊问题，如回答变简单或不相关。提出一种新型奖励函数，综合考虑事实准确性、细节丰富度和相关性，并将其用于在线RL训练事实性推理模型。

Result: 所提模型在六个长文本事实性基准上实验，平均降低幻觉率23.1个百分点，提升回答细节丰富度23%，且没有损害整体回答有用性。

Conclusion: 通过新设计的奖励函数和在线RL方法，可以显著提升R-LLMs在长文本事实推理中的表现，实现更高的事实准确性、细节丰富度，而不降低回答有用性。

Abstract: Reasoning Large Language Models (R-LLMs) have significantly advanced complex
reasoning tasks but often struggle with factuality, generating substantially
more hallucinations than their non-reasoning counterparts on long-form
factuality benchmarks. However, extending online Reinforcement Learning (RL), a
key component in recent R-LLM advancements, to the long-form factuality setting
poses several unique challenges due to the lack of reliable verification
methods. Previous work has utilized automatic factuality evaluation frameworks
such as FActScore to curate preference data in the offline RL setting, yet we
find that directly leveraging such methods as the reward in online RL leads to
reward hacking in multiple ways, such as producing less detailed or relevant
responses. We propose a novel reward function that simultaneously considers the
factual precision, response detail level, and answer relevance, and applies
online RL to learn high quality factual reasoning. Evaluated on six long-form
factuality benchmarks, our factual reasoning model achieves an average
reduction of 23.1 percentage points in hallucination rate, a 23% increase in
answer detail level, and no degradation in the overall response helpfulness.

</details>


### [53] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文利用线性探针分析LLM在多轮自然对话中的说服过程，揭示探针可高效、准确识别说服成功、策略等关键要素，表现优于传统prompt方法，为研究模型复杂行为提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）已逐步展现出说服人类的能力，但我们对其说服动态的认知仍有限。此前研究利用线性探针分析模型表征，研究了诸如用户情感、政治观点等技能。受此启发，本文希望深入探究模型在自然多轮对话中说服动力的具体表现。

Method: 借鉴认知科学观点，训练线性探针以分析说服成功、被说服者性格、和说服策略三大说服相关要素。通过这些轻量工具，可以高效地从对话中提取关键说服指标。

Result: 探针能在样本级和数据集级捕捉说服的各个方面，例如识别具体对话中被说服的时刻，以及数据集整体说服成功的节点。探针不仅推理速度快，在揭示说服策略上甚至优于昂贵的prompt方法。

Conclusion: 线性探针是一种高效且灵活的手段，可用于分析复杂的交互行为如说服、欺骗及操纵，尤其在需要多轮对话和大规模数据集时较prompt方法更为可行。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [54] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: H-NET++是一种新型字节级模型，通过分层、自适应的片段学习，有效提升了形态丰富语言（如波斯语）的建模质量和计算效率，显著优于传统分词方案，并无需人工分词标注。


<details>
  <summary>Details</summary>
Motivation: 字节级语言模型可以消除对易碎分词器的依赖，但在形态丰富语言（如波斯语）中，单词往往跨越多个字节，造成计算挑战。为此，现有模型在这些语言上的表现有限，需要更有效的分词方案。

Method: 提出H-NET++模型，一种分层动态切分的语言建模方法。其创新包括：(1)采用轻量级Transformer上下文混合模块实现跨片段注意力；(2)引入双层潜变量超先验模型，增强文档级连贯性；(3)针对正字法特殊符号如波斯语ZWNJ定制处理机制；(4)利用分阶段序列长度的课程式训练。

Result: 在1.4B波斯语语料上，H-NET++模型相较基于BPE的GPT-2-fa实现了更低的压缩比（BPB下降0.159，压缩率提升12%）、ParsGLUE榜提升5.4个百分点、对ZWNJ字符破坏鲁棒性提升53%，并在金标准词形边界检测上达到73.8% F1。模型自动学习得到的分块与波斯语形态分界高度一致，无需人工监督。

Conclusion: H-NET++证明了分层动态分块机制能在无需分词器的前提下，有效适应形态丰富语言，兼具计算效率和建模效果。

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [55] [Aircraft routing: periodicity and complexity](https://arxiv.org/abs/2508.05532)
*Frédéric Meunier,Axel Parmentier,Nour ElHouda Tellache*

Main category: cs.DM

TL;DR: 本文系统分析了飞机调度问题在周期性与非周期性实例下的解决方案和计算复杂性：当维护周期不超过四天时，能强制获得周期性解，同时也首次证明了非周期版本的NP难性。


<details>
  <summary>Details</summary>
Motivation: 飞机调度问题涉及为飞机分配航班，同时确保飞机能定期前往维修基地。这一问题管理和优化航空资源至关重要，但关于周期性任务分配与维护要求的关联以及计算复杂性仍有诸多未解之处。本文旨在探讨周期性实例与周期性解之间的关系，以及提升对非周期情况下该问题计算复杂性的认知。

Method: 作者首先对周期性实例与周期性解的关系进行理论分析，特别是讨论定期维护要求下能否存在满足强周期性的解；其次，通过复杂性理论方法，证明了非周期版本飞机调度问题的NP难性，同时也证明了某个特殊情形可在多项式时间内解决。

Result: 证明了当飞机须每隔最多四天进行一次维护时，总能存在强周期性的解。进一步，首次在非周期实例下证明了该调度问题的NP难性，并给出了某一特殊但自然子问题的多项式可解性。

Conclusion: 周期性调度与解决方案并非总是等价，但在特定维护频率下可强制存在周期性解。飞机调度问题在非周期版本下也具有NP难性质，丰富了理论基础并为实际航空调度提供了参考。

Abstract: The aircraft routing problem is one of the most studied problems of
operations research applied to aircraft management. It involves assigning
flights to aircraft while ensuring regular visits to maintenance bases. This
paper examines two aspects of the problem.
  First, we explore the relationship between periodic instances, where flights
are the same every day, and periodic solutions. The literature has implicitly
assumed-without discussion-that periodic instances necessitate periodic
solutions, and even periodic solutions in a stronger form, where every two
airplanes perform either the exact same cyclic sequence of flights, or
completely disjoint cyclic sequences. However, enforcing such periodicity may
eliminate feasible solutions. We prove that, when regular maintenance is
required at most every four days, there always exist periodic solutions of this
form.
  Second, we consider the computational hardness of the problem. Even if many
papers in this area refer to the NP-hardness of the aircraft routing problem,
such a result is only available in the literature for periodic instances. We
establish its NP-hardness for a non-periodic version. Polynomiality of a
special but natural case is also proven.

</details>
