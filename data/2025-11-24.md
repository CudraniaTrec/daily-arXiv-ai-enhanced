<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Large language models for automated PRISMA 2020 adherence checking](https://arxiv.org/abs/2511.16707)
*Yuki Kataoka,Ryuhei So,Masahiro Banno,Yasushi Tsujimoto,Tomohiro Takayama,Yosuke Yamagishi,Takahiro Tsuge,Norio Yamamoto,Chiaki Suda,Toshi A. Furukawa*

Main category: cs.SE

TL;DR: 本论文以版权合规基准测试数据为基础，对多种LLM模型PRISMA 2020指南遵循性进行评估，发现结构化清单输入能大幅提升评估准确性，自动化审核有助于加速评审，但最终仍需人工核查。


<details>
  <summary>Details</summary>
Motivation: 在同行评审过程中，评估系统综述报告是否遵循PRISMA 2020指南是一项繁重的任务。此前缺乏可共享的基准数据以助力自动化评估。

Method: 构建了一个版权合规的、包含108篇“创作共用”许可的系统综述文献基准数据集，并在五种输入格式下，评估了十个大型语言模型（LLM）。在开发群体中，分别测试了结构化PRISMA 2020清单（如Markdown、JSON、XML、纯文本）和仅有手稿输入的表现。随后，挑选出高敏感性的Qwen3-Max进行全量数据集进一步评估。

Result: 结构化清单（比手稿输入）极大提高了LLM对PRISMA准则的评估准确率（约79%，vs仅手稿45%），不同结构化格式间无显著差异。各模型准确率在70.6%-82.8%之间，存在敏感性-特异性权衡。在验证集上结果得以复现。Qwen3-Max模型在全数据集上达到了95.1%敏感性和49.3%特异性。

Conclusion: 向LLM提供结构化PRISMA清单能大幅提升自动评估系统综述质量指南的能力，但在编辑决策前依然需人工专家的核查。

Abstract: Evaluating adherence to PRISMA 2020 guideline remains a burden in the peer review process. To address the lack of shareable benchmarks, we constructed a copyright-aware benchmark of 108 Creative Commons-licensed systematic reviews and evaluated ten large language models (LLMs) across five input formats. In a development cohort, supplying structured PRISMA 2020 checklists (Markdown, JSON, XML, or plain text) yielded 78.7-79.7% accuracy versus 45.21% for manuscript-only input (p less than 0.0001), with no differences between structured formats (p>0.9). Across models, accuracy ranged from 70.6-82.8% with distinct sensitivity-specificity trade-offs, replicated in an independent validation cohort. We then selected Qwen3-Max (a high-sensitivity open-weight model) and extended evaluation to the full dataset (n=120), achieving 95.1% sensitivity and 49.3% specificity. Structured checklist provision substantially improves LLM-based PRISMA assessment, though human expert verification remains essential before editorial decisions.

</details>


### [2] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: LLMs生成的代码漏洞多，现有工具检测效率和准确率有限。CodeX-Verify多智能体系统大幅提升检测准确率和速度，无需执行测试，具有实际应用可行性，多漏洞叠加风险超预期。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）生成的代码存在大量漏洞和缺陷，现有检测工具存在检测准确率不高和误报率较高的问题，因此亟需更有效与高效的代码漏洞检测方法。

Method: 提出了CodeX-Verify，一个多智能体系统，由四个专门的智能体检测不同类型的代码漏洞。作者通过数学证明和实验，论证多智能体组合比单一智能体更能发现多样的漏洞，并测试不同的智能体组合以优化检测准确性。

Result: CodeX-Verify系统能检测76.1%的漏洞，与当前最佳方法持平，但无需测试执行且效率更高。多智能体组合能显著提升检测准确率，最佳双智能体组合达到79.3%，系统在实际环境下能在200ms内完成一次检测，适合生产应用。同时，作者发现代码同时存在多种漏洞时，风险呈指数级上升。

Conclusion: 多智能体结合检测不同类型的漏洞能大幅提升代码安全检测的准确性与效率，且在生产环境下具备实用性。多种漏洞同时存在会极大提高系统安全风险。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [3] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 本研究发现自动化程序修复生成的代码仍存在明显测试过拟合问题，对提升实际修复质量提出了挑战。


<details>
  <summary>Details</summary>
Motivation: 长期以来，自动化程序修复（APR）容易出现修复后的代码在已知测试集通过，但在隐藏测试集失败的情况，导致测试过拟合。随着大语言模型的兴起，有必要重新评估这一问题是否依然存在。

Method: 作者采用SWE-bench仓库级任务进行实验，系统性测试并比较自动化修复代码在已知测试和隐藏测试上的表现。

Result: 实验结果表明，即使在当前技术环境下，自动化程序修复依然存在测试过拟合问题，修复方案在隐藏测试集上的通过率显著下降。

Conclusion: 测试过拟合依旧是现代自动化程序修复领域需要重点关注的问题。现有工具在隐藏测试集的泛化能力不足，需进一步改进。

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>
