<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 本文发现现有大模型解码策略难以满足RTL代码生成任务，于是提出了DecoRTL作为新的推理解码方法，通过结合自一致性采样和语法感知温度调整，显著提升了代码的有效性和多样性，不增加任何微调开销。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型（LLM）在生成寄存器传输级（RTL）代码时，使用的解码策略主要是为自然语言设计的，往往不能满足RTL代码对结构和语义的严格要求，导致生成无效、重复或错误的代码。因此，改进解码策略以提升RTL代码生成的有效性和多样性十分必要。

Method: 提出了DecoRTL，这是一种新的运行时解码策略，具备语法感知和对比性。它包括两个核心组件：一是自一致性采样（self-consistency sampling），通过生成多个候选并基于token级的一致性进行重排序，以保证正确性并保持多样性；二是语法感知温度自适应（syntax-aware temperature adaptation），根据token的语法和功能类别动态调整采样温度，确保语法关键token低温度，探索性token高温度。该方法只需在推理时运行，无需额外微调模型。

Result: 在VerilogEval基准上，通过对多个开源LLM的评测显示，DecoRTL显著提升了生成代码的语法有效性、功能正确性和输出多样性，同时在执行性能上几乎没有开销。

Conclusion: DecoRTL针对RTL代码生成场景，提出了一种无需模型微调的高效推理解码策略，能够在保持模型原有性能的基础上，提升生成代码的结构和功能表现，具有重要应用前景。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 技术面试准备难度大，普通计算机课程无法有效支持，候选人普遍觉得压力大、准备不足。建议课程改革以更好地帮助求职者进行面试准备。


<details>
  <summary>Details</summary>
Motivation: 技术面试对软件工程求职者至关重要，但其复杂性难以在传统计算机课程中准备和应对，了解候选人如何准备技术面试及不同准备方法和教育的作用。

Method: 向131名正在准备技术面试的候选人分发调查问卷，分析他们的准备方式及教育对准备的影响。

Result: 结果显示，候选人很少在真实场景下训练，课程对面试准备支持较差，导致面临较大压力和准备不足。

Conclusion: 候选人在技术面试中的准备通常缺乏真实环境的训练，计算机课程对面试准备支持有限，导致候选人感到压力和准备不足。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [3] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 本文提出用大语言模型将自然语言查询自动转换为结构化代码查询，在多个测试中均显著优于传统方法，显著降低了开发者使用门槛，提升了代码结构化搜索的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的代码结构化搜索工具要求开发者使用难以掌握的领域特定语言（DSL）编写查询，这增加了学习门槛，限制了结构化搜索的普及和应用。本文旨在降低这一门槛，使开发者可以用自然语言进行结构化代码搜索，提升易用性。

Method: 本文提出一种新的通用方法，将大语言模型（LLM）的自然语言理解能力与结构化代码搜索工具结合，实现自然语言到结构化查询的自动转换。具体实现分别针对Semgrep和GQL两种DSL。作者还构建了包含400个查询、覆盖10个Java项目的新基准测试集，并通过实验评估了所提出方法。

Result: 实验结果表明，本文方法在将自然语言查询转换为DSL查询进行结构化代码搜索时，表现出较高的精度（precision）和召回率（recall），具体范围为55%到70%。相比以往基于语义代码搜索及LLM检索的基线方法，在F1分数上分别高出57%和14%。

Conclusion: 将自然语言与结构化代码搜索有效结合可大幅提升查询的易用性与效果，并且本文新构建的基准和方法具有高精度、良好性能，以及在实际开发场景中较强的实用价值。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [4] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本文探索了以未发布前源代码可测量的内部软件度量作为预测移动应用受欢迎程度的依据。在回归表现不佳的情况下，将任务转为二分类后，多层感知机模型F1分数达0.72，证明内部代码度量可作为流行度的重要参考。


<details>
  <summary>Details</summary>
Motivation: 在移动应用发布前预测其受欢迎程度可以帮助开发者在竞争激烈的市场中获得战略优势，但这一预测一直具有挑战性。本文动机在于探索发布前仅凭源代码即可评估应用的受欢迎程度。

Method: 作者收集了446个开源安卓应用（来自F-Droid）的数据，提取了系统级、类级和方法级的代码度量指标、代码异味和应用元数据，还从Google Play获取了用户评论、下载量和权限使用等信息。针对不同特征集合，应用了回归与二分类模型：仅用Size基线、专家挑选的特征组合，以及通过特征选择算法得出的Voting集。

Result: 回归模型因数据分布偏斜表现不佳（$R^2$低），但问题转化为二分类（热门/不热门）后，效果明显提升。最佳模型（多层感知机+Voting特征集）F1分数达0.72。

Conclusion: 内部代码度量虽然解释力有限，但能作为评估移动应用受欢迎度的有用指标，挑战了之前认为内在指标无法预测软件质量的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [5] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 本研究比较了主观压力量表和生理压力指标发现：缩短时间限制未能有效诱发压力，仅部分生理数据有反应，提示未来需完善压力诱发方法。


<details>
  <summary>Details</summary>
Motivation: 传统上，人类福祉、压力等因素的研究多依赖自我报告量表，但这些工具即便经过严格验证也存在偏差。因此，学界越来越关注结合更客观的生理测量方法与主观量表来评估压力。

Method: 本研究开展实验，先让参与者填写预测调查问卷，然后穿戴生理传感器完成两项编程任务，并在每项任务结束后填写简短的后测问卷，最终进行简短访谈。

Result: 实验结果多样：心理量表未发现压力迹象，访谈中参与者感受既有无压力也有时间压力，仅在生理指标上（EDA phasic peaks）观测到显著差异。

Conclusion: 本研究使用的施加压力方法（缩短时间限制）诱发压力不充分，对压力、心理量表和生理数据结合使用的研究设计提出方法论建议。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [6] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 本文分析了多平台开发者沟通数据及14种情感分析工具表现，提出特征驱动的工具推荐方案。实验发现模型需因数据特性择优，变换器模型表现突出，但场景适应性依旧关键。


<details>
  <summary>Details</summary>
Motivation: 软件开发过程中，团队成员主要通过文本沟通，因此情感分析对于理解团队动态和构建可信的AI分析具有重要意义。然而，不同平台的数据集因沟通风格和内容差异，现有情感分析工具表现不一。

Method: 本文分析了来自五个平台的10个开发者沟通数据集的语言与统计特征，并对14个情感分析工具进行了性能评估。随后提出了一种映射方法和问卷，可基于新数据集的特征推荐适合的情感分析工具。

Result: 结果表明，不同平台的数据集在语言与统计特征上有显著差异，这些特征可用于改进工具选择。变换器（transformer）模型，比如SetFit和RoBERTa，一贯表现优异，但工具的有效性仍随具体场景而变。

Conclusion: 通过利用数据集特性进行工具推荐，有助于研究者和从业人员选择可信赖的情感分析工具，同时也强调了随着沟通语境变化需持续评估的必要性。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [7] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 本文为 COBOL 代码自动生成解释提出了基于多智能体 LLM 的方法，在函数、文件和项目层面上均显著提升了解释的质量和准确性，为维护人员理解和维护老旧 COBOL 系统提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: COBOL 作为一种历史悠久的业务编程语言，广泛应用于金融、商业和政府领域。由于开发人员日益减少、代码复杂且缺乏文档，新开发人员难以理解和维护现有 COBOL 系统。原有利用大语言模型（LLM）对代码功能进行解释的研究在处理 COBOL 时因其特有结构和语法常超出模型的 token 限制而面临困难。

Method: 本文提出了一种多智能体方法，利用两个基于 LLM 的智能体协同工作，对 COBOL 代码的函数、文件及整体项目进行解释。这两个智能体通过整合代码库的上下文信息到解释提示中，共同生成更为精准和全面的代码说明，并在 14 个开源 COBOL 项目上进行了评估。

Result: 在函数级代码解释上，所提方法较基线分别提升了 METEOR、chrF、SentenceBERT 得分 12.67%、18.59%、0.62%；在文件级别，对超出 token 窗口的长/短文件的解释在目的性、功能性和清晰度上分别领先基线 4.21%、10.72%、14.68%；在项目级别，能够生成涵盖 82% 所选项目功能和目的的说明。

Conclusion: 提出的多智能体 LLM 方案有效提升了对 COBOL 代码的多层级自动化解释能力，显著优于现有基线方法，尤其是在处理大规模、缺乏文档的 COBOL 工程时具有实用价值。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [8] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出了RTED，一种结合类型约束分析和反射验证的Python类型错误自动检测工具，在检测能力和精度上显著优于现有方法，并在实际项目中发现了新错误。


<details>
  <summary>Details</summary>
Motivation: Python语言因类型错误而出现运行时故障，给软件可靠性和开发效率带来挑战。已有静态分析工具误报率高，自动测试方法又难以针对类型错误生成有效用例，因此亟需一种精确检测Python类型错误的新方法。

Method: 提出了一种新型类型感知测试生成技术RTED。该方法结合了逐步类型约束分析和反射式验证，有效引导测试生成过程，压制误报，提高类型错误检测的准确性。

Result: 在两个公开基准集（BugsInPy和TypeBugs）上，RTED相比四个最新技术多检测出22-29个类型错误，并且误报更少，精度提升173.9%-245.9%。此外，RTED还在六个真实开源项目中发现了12个未知类型错误。

Conclusion: RTED显著提升了Python类型错误的自动检测能力，无论是在检测数量还是误报率上均优于现有方法，具备很强的实用价值。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [9] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 本文针对VFL推理软件执行正确性问题，提出了VeFIA审计框架，通过TEE和随机采样在无隐私泄露和延迟的情况下，实现异常检测，并获得极高的检测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的垂直联邦学习（VFL）方案中，虽然能够实现跨机构AI协作且保护数据隐私，但缺乏对数据方推理软件执行正确性的审计机制。这意味着一旦数据方的推理软件存在异常，任务方难以及时发现并纠正，从而可能影响模型推理的可信和准确。

Method: 本文提出VeFIA框架，结合受信任执行环境（TEE）和协调器，允许任务方在保护数据隐私和不引入额外推理延迟的前提下，对数据方的推理结果进行审计。核心方法为随机采样推理结果验证，并通过TEE确保计算结果的正确性。

Result: VeFIA框架能保证当异常推理超过5.4%时，任务方能以99.99%的概率检测到推理软件的异常执行，并且不会带来额外的在线推理延迟。实验结果还显示该方法在异常检测方面达到100%的正预测值、负预测值和真正率。

Conclusion: 本文提出了首个用于VFL场景下推理软件执行正确性审计的框架VeFIA，并证明了其高效、隐私保护、无延时开销和极高异常检测准确率。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [10] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 本文提出的Meta-Fair方法，通过变形测试和LLM驱动自动生成与评价，实现了高效、自动化地发现和度量大语言模型（LLM）中的偏见。实验结果显示该方法精度高、适用性强，并通过配套开源工具显著简化了测试流程。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统公平性（即无不公正偏见）至关重要，但大型语言模型（LLMs）公平性评估仍存在难度。目前方法主要依赖人工评估、固定模板、启发式规则和策划数据集，难以扩展，耗费资源。因此，有必要开发更加自动化、通用且易扩展的测试方法。

Method: 提出了一种新的自动化LLM公平性测试方法Meta-Fair。方法核心包括两个方面：（1）采用变形测试（Metamorphic Testing），通过设计变形关系（MRs）来控制输入提示的变化，据以检测输出结果的偏见变化；（2）充分利用LLMs自身能力，自动生成测试样例并对输出进行分类判定。该方法配套开发了三款开源工具，实现测试用例的生成、执行、评估全流程自动化。

Result: 通过对12个预训练LLM、14种MR、5种偏见维度、7900个自动生成测试用例的大规模实验，结果表明Meta-Fair能有效发现LLMs中的偏见行为，平均精度达92%，检测到29%的执行中存在偏见。同时发现LLM本身作为评估者表现出高度一致性和可靠性，最佳模型F1-score可达0.79。虽然LLM输出非确定性带来一定影响，但通过精心设计MR可以缓解。

Conclusion: Meta-Fair能够大幅提升LLM公平性测试的自动化程度，兼具准确性和可扩展性，为LLM测试自动化指明了有前景的发展方向，尽管在更广泛应用上仍需进一步优化。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [11] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的自动化需求访谈机器人LLMREI，并验证其在减少人工错误、准确提取需求及适应性提问方面与人类分析师表现相近，对提升需求获取效率有积极作用。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈是收集系统需求的重要环节，但高度依赖有经验的分析师，导致资源消耗大、易受人为偏见和误解影响。近年来大模型的发展为自动化该过程提供了新机遇。

Method: 提出了LLMREI对话机器人，通过零样本提示（zero-shot prompting）和逐步提示（least-to-most prompting）两种策略优化其访谈表现，并在33次模拟访谈中评估其性能。微调（fine-tuning）方法因初步效果不佳被放弃。

Result: LLMREI在常见问询错误方面与人工访谈者表现相近，能够提取出较大比例的需求内容，并能根据访谈上下文生成高度相关的问题。

Conclusion: LLMREI在需求获取自动化领域具备良好前景，尤其适用于大规模多干系人访谈，有望降低人工成本与误差，提升需求获取的可扩展性。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [12] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 本文提出新的方法和框架，将人机团队协作和伦理合规系统性地融入自适应CPS，解决了反馈闭环集成和人本价值保障的挑战。


<details>
  <summary>Details</summary>
Motivation: 目前自适应的网络物理系统（CPS）在实现人与机器之间的无缝、高效协作（HMT）方面存在挑战，尤其是在如何将人类融入到CPS的反馈闭环以及保障隐私和人类价值观方面。

Method: 1）开发将人机协作原则融入CPS自适应反馈闭环的创新方法与流程；2）构建从需求工程开始，贯穿系统整个生命周期的伦理与人类价值观集成、验证和验证框架。

Result: 提出了新的HMT集成方法，并建立了面向CPS全生命周期的人类价值观和伦理集成框架，为CPS系统更加有效且符合人类价值标准地支持HMT提供了技术基础。

Conclusion: 本研究为解决人类和机器在CPS合作中的反馈集成和伦理问题提供了系统化方法，有助于实现更高效、可靠和人本的自适应CPS。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [13] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: RSE与SER之间术语存在差异，本文提出系统术语映射方法并发现共通与互补点，为未来协作奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程师（RSEs）与软件工程研究者（SERs）即使面对相似概念，也采用不同术语，导致沟通困难。因此需要理解这两者在基本概念上的异同。

Method: 系统性地对SER社区的软件工程基本概念进行解释，并调查RSE社区如何理解这些概念，梳理共同点、知识鸿沟及可适配领域。采用了术语映射的方法。

Result: 初步发现表明两者在一些概念存在对齐，也发现了互补学习和合作的机会。建立起一种可被大家共创与验证的术语映射基础框架。

Conclusion: RSE与SER之间存在术语认知差异，但通过系统性术语映射方法，可促进相互理解和未来合作。搭建的术语映射方法有望通过众包方式持续完善。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [14] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: 提出RLHGNN，结合异构图建模与强化学习，自适应流程结构，预测准确率高且推理快，适合实时业务流程监控。


<details>
  <summary>Details</summary>
Motivation: 流程预测对于服务型架构、分布式企业系统和云原生平台至关重要，但现有基于序列方法无法捕捉并行与条件依赖，基于图的方法又模型单一，难以适应流程复杂度差异。

Method: 提出RLHGNN框架，将事件日志转化为包含3类边的异构流程图，并通过强化学习自动选择最优图结构，再用异构图卷积结合关系特定的聚合策略进行预测。

Result: 在6个真实数据集上，方法在准确率上优于主流方法，且单次推理延迟约为1毫秒，可达实时监控需求。

Conclusion: RLHGNN能够自适应建模顺序及非顺序关系，并用于实时业务流程监控，实用性强。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [15] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 用可持续性标记辅助识别云架构论坛中的可持续性内容，效果优于传统定义法，能更准且易用地支持架构决策。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的兴起，软件系统的可持续性日益重要，但在业界交流中如何识别相关可持续性内容缺乏明确指引，造成讨论和决策的困难。

Method: 提出“可持续性标记”（sustainability flags）用于在线Q&A论坛的实践性讨论中进行指示，并基于云服务商的可持续性最佳实践进行主题分析，随后在受控实验中评估其识别效果。

Result: 使用标记后，被归类为可持续性相关的帖子数量减少，分类信心有所提升，表现明显进步。相比只用可持续性的定义，标记法更实用且易于理解。

Conclusion: 可持续性标记提升了云架构讨论中可持续性识别的准确性和用户体验，有助于促进行业相关决策。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [16] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文提出一种将法律文本自动转化为结构化Python代码表示的新方法，无需大量人工标注，能有效捕获结构与语义信息及其关系。在美国州法律实验中，该方法表现出较高的准确率和通用性，有助于中小企业合规需求的自动化支持。


<details>
  <summary>Details</summary>
Motivation: 软件系统需要遵循法律法规，尤其对于缺乏法律专家的小型组织和初创企业，实现合规非常耗费资源。从法规中提取元数据以获得软件的法律需求是合规的重要一步，但由于法律文本冗长且复杂，这一过程极为繁琐。现有的自动化方法存在局限，如未处理元数据属性间的关系，以及依赖人工标注或启发式方法，泛化性差。

Method: 提出了一种基于文本蕴涵和上下文学习的方法，自动生成可用Python代码表示的法律文本标准化表达。该表达基于手工设计的Python类结构，作为领域特定元模型，既捕获结构和语义元数据，也反映元数据之间的关系，减少了对大规模人工标注数据的需求。

Result: 在13个美国州的数据泄露通知法案上评估该方法，生成的表示通过了约89.4%的测试用例，精确率为82.2，召回率为88.7。

Conclusion: 文中方法能够更自动化且推广性强地将复杂法律文本转换为结构化、执行性强的表示，减轻了依赖人工标注和提升针对新法规的适用性。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [17] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 用GPT-4o辅助访谈提问，在得到适当指引后，自动生成的追问在质量上优于人类，显示出大语言模型在面向实时需求获取场景的巨大应用前景。


<details>
  <summary>Details</summary>
Motivation: 访谈在软件需求获取过程中非常常见，但受访谈者专业知识、信息超载等问题影响，提出有效追问的问题具有挑战性。大语言模型近期在文本生成、总结等任务上表现优异，研究者希望探索其辅助访谈的可能性。

Method: 本文采用了两轮对比实验：第一轮将GPT-4o生成的追问与人类编写的追问进行比对，并且尽量减少指引；第二轮则在追问生成过程中参照面试过程中常见错误类型，对LLM的生成进行引导和优化。两轮实验评估了问题的清晰度、相关性和信息量。

Result: 实验发现，在没有引导的情况下，LLM自动生成的问题与人类生成的问题在清晰度、相关性和信息量方面并无差异。而当利用面试常见错误类型指导LLM生成追问时，LLM生成的问题表现超过了人类编写的问题。

Conclusion: LLM，尤其是在得到面试错误类型指导时，可实时帮助访谈者提出更优质的需求获取追问问题，将有效提升访谈的质量和效率。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: 本文在 DHOL 框架下，通过将 refinement 与 quotient 类型作为子类型特殊案例优雅集成，实现了自动定理证明系统对这些类型的良好支持，并保持了系统的理论完备性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 当前的自动定理证明工具通常缺乏对 refinement 和 quotient 类型的支持，主要因为它们要求不可判定的类型系统，而大多数系统以可判定性为前提。论文动机是利用最近提出的 DHOL（依赖类型的高阶逻辑），为这两种类型提供便捷且自动化支持，并提升实用性。

Method: 将 refinement 类型和 quotient 类型作为子类型(subtyping)的特殊情形集成进 DHOL，其中相关的包含和投影映射被设计为恒等映射，从而避免复杂的表示转换。论文还提出了该扩展语言的语法、语义以及向 HOL 的翻译方法，并给出了完备性与可靠性证明。

Result: 成功构建并实现了支持 refinement 和 quotient 类型的 DHOL 扩展，实现了优雅且简单的编码，引入新类型无需复杂的系统修改；理论上证明了扩展的可靠性和完备性。

Conclusion: 基于 DHOL 可支持 refinement 和 quotient 类型的自动定理证明，为实用和自动化的高阶逻辑证明工具提供了新的思路和方法。该方法实现简单，理论基础坚实，有望推动实际应用中的类型系统设计。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


### [19] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: 论文提出了一种新颖的字级逻辑精简与等价性检测方法SMT-Sweep，基于SMT理论，实验显著优于现有工具，是首次将sweeping技术扩展到SMT硬件验证的工作，且实现开源。


<details>
  <summary>Details</summary>
Motivation: 随着硬件验证中越来越多采用word-level（字级）构造，如位向量操作、算术运算和数组，而传统的SAT sweeping技术只适用于bit-level（位级）逻辑，缺乏在字级层面的高效等价性检查和冗余精简方法。

Method: 本论文提出了SMT-Sweep方法，将SAT sweeping从位级扩展到字级，基于SMT（Satisfiability Modulo Theories）理论。该方法结合了随机和基于约束的字级模拟，通过支持丰富的位向量操作和数组语义，检测等价性和精简逻辑。框架包含了符号表达式下的字级仿真，并超越了单纯的布尔逻辑运算。

Result: 实验结果显示，SMT-Sweep方法相比最新的位级SAT sweeping和字级单体SMT求解，在速度上分别平均提升了44倍和69倍。

Conclusion: SMT-Sweep首次将sweeping技术应用于基于SMT的硬件验证领域，极大提升了字级层面等价性检查和逻辑精简的效率，并且该实现已开源。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [20] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 本文提出一种将含有连续可导实值函数与其区间属性的可满足性判定问题，转化为等价的实数基本代数判定问题的方法，并证明了这种转换过程的可满足性保持性。


<details>
  <summary>Details</summary>
Motivation: 原有的关于无量词语言的可满足性测试在处理带有一阶导数连续的实值单变量函数时存在局限，因此需要丰富相应的理论工具来应对Tarski基本代数的函数扩展片段。

Method: 论文提出将包含函数变量与微分算子的公式预处理为等价的、仅包含实数变量的无量词基本代数公式，并借助Tarski决策方法检测其可满足性。具体做法是将函数变量用一组实变量取代，消解公式中的所有函数相关内容。

Result: 所提出的处理方法将原含有函数变量、导数与区间属性的判定问题转化为一个纯实数代数问题，并表明该方式能保存可满足性。核心证明在于构造一族可插值的$C^1$函数，使得只要目标公式可满足，原公式也可找到匹配模型。

Conclusion: 本文推动了函数属性可判性理论的发展，将特定扩展的实函数代数片段的可满足性问题降低为实数代数的可判性，理论上增强了利用Tarski决策方法分析更丰富公式的能力。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [21] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 本文将经典CK条件逻辑迁移到直觉主义框架，建立了新的证明系统，提出了包含might算子的扩展逻辑CCK，并给出了相应的模型和公理化，为构造性条件推理理论提供了新工具和方向。


<details>
  <summary>Details</summary>
Motivation: 直觉主义条件逻辑希望为条件推理提供一种构造性的分析方式。在这个框架下，would 和 might 两种条件算子不再可以互相定义，需对其进行独立而详细的研究。

Method: 本文采用基于选择函数的Chellas条件逻辑（CK），将其置于直觉主义模态逻辑的框架下，得出具备直觉主义与构造性质的逻辑变体。然后为IntCK开发嵌套演算系统，为CCKbox开发序列演算系统，在此基础上进一步扩展并引入新的模型和公理化方法。

Result: 作者构建了IntCK和CCKbox的证明体系，并提出了带might算子的保守扩展逻辑CCK。针对CCK及其扩展，给出了模型理论和公理化方案。

Conclusion: 本文为直觉主义条件逻辑提供了系统的语法与语义基础，丰富了该领域的理论工具，为后续研究复杂条件推理及其构造化分析提供了坚实的理论支撑。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了首个内容丰富、类别多样、覆盖五大任务的中文多任务偏见评测基准McBE，填补了现有数据集的不足，并对多种大语言模型进行了系统偏见评测，揭示了它们在中文任务中的偏见程度和类型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各种NLP任务中的广泛应用，其内在的偏见问题逐渐暴露。现有的大多数偏见评估数据集主要针对英语和北美文化，且类别有限，不适用于其他文化背景，尤其是中文和中华文化相关的数据集极为稀缺。此外，现有数据集通常仅支持单一评估任务，无法多角度全面评估模型的偏见。

Method: 作者提出了一个多任务中文偏见评测基准（McBE），包括4077个偏见评测实例，涵盖12个单一偏见类别、82个子类别，并引入5种评测任务，以实现广泛的类别覆盖、内容多样性和全面性。同时，作者对多个流行的大语言模型进行了偏见测试和系统性评测。

Result: 评测结果显示，所有被评估的大语言模型都表现出不同程度的偏见。

Conclusion: 作者对评测结果进行了深入分析，提供了关于LLMs偏见的新见解，丰富了中文和中华文化语境下LLMs偏见评估的研究基础，并为未来相关研究提供了工具和思路。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [23] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本研究首次系统评估了推理型和非推理型LLMs在各类对话摘要任务中的表现，发现逐步推理方法并不能有效提升对话摘要质量，甚至带来冗余和事实错误。强调对现实应用需开发新的模型和评价方法。


<details>
  <summary>Details</summary>
Motivation: 对话摘要在客户服务、会议分析和对话式人工智能中有着重要应用，但目前长链思维（Long Chain-of-Thought, CoT）推理等逐步推理架构在摘要场景中的表现未被系统评估，尤其在需要兼顾抽象与简洁的多样对话中。

Method: 提出针对对话摘要三大主要范式（通用、角色导向、查询导向）对最先进推理型LLMs（如OpenAI-o1、DeepSeek-R1）与非推理LLMs进行了首次全面系统评测，覆盖多语言、多领域和不同摘要长度，结合SAMSum、DialogSum、CSDS、QMSum等权威数据集与多维评估协议，包括LLM自动评价指标和人工评价标准。

Result: 实验证明，逐步推理并未如在其它推理任务中那样显著提升对话摘要质量。推理型LLM经常出现冗长、事实不一致和不够精炼的问题，甚至整体表现逊于非推理模型。更细致的情景分析揭示了推理何时以及为何会妨碍复杂对话场景下的摘要性能。

Conclusion: 当前推理型LLMs在对话摘要领域具有局限性，针对真实世界对话摘要任务亟需更有针对性的建模和评测新策略。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [24] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 本文研究了循环Transformer模型中是否能涌现可解释的潜在链式思维结构。实验发现，这类结构非常有限，且可解释性受层级和解码影响较大，增加循环次数也无法显著提升推理表现。直接外化推理依然优于仅靠循环结构内嵌链式思维。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型中，链式思维（CoT）虽然提升了模型的复杂推理与规划能力，但由于其步骤以自然语言外化，导致推理更易解释但效率偏低。作者希望探究，通过循环结构将推理过程内嵌于隐空间（即潜在CoT），能否兼顾可解释性与效率。

Method: 作者使用Huginn-3.5B（一个推理时重复利用层、不增加参数量的深度循环Transformer架构）模型，通过Logit Lens和Coda Lens等探针技术，对模型在算术任务的内部表示进行分析，追踪最终和中间结果token的秩变化。

Result: 分析显示，在Huginn-3.5B模型中，能够被解释的潜在链式思维结构仅有有限证据。并且，探针结果在不同循环块之间存在很大不一致性，隐藏状态的可解释性极度依赖于具体层索引和解码方法。此外，即使增加循环深度，也仅带来边际性能提升，远低于显式外化推理步骤的模型。

Conclusion: 当前深度循环Transformer模型在算术推理任务上，仅能显现极有限的可解释潜在链式思维结构。探针可解释性受模型结构显著影响，增加循环深度不能有效替代外化推理。未来工作需探索更有效的结构以提升隐空间推理能力及可解释性。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [25] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 该论文提出GDC Cohort Copilot，一款基于自然语言的GDC队列筛选工具，并展示其开源大模型在队列生成方面超过GPT-4o的表现。


<details>
  <summary>Details</summary>
Motivation: GDC用户可以通过图形化界面创建复杂的癌症基因组队列，但因字段众多，新用户难以准确找到所需的队列条件。相比之下，自然语言描述会更容易表达所需的队列特征。

Method: 开发了GDC Cohort Copilot工具，允许用户通过自然语言输入队列描述，工具自动生成对应GDC队列筛选条件，并支持交互式进一步编辑。作者还针对不同大模型方案进行了开发和评估。

Result: GDC Cohort Copilot能根据自然语言生成队列条件，生成结果可直接导入GDC继续分析。作者的本地开源大模型（GDC Cohort LLM）在队列生成表现上优于GPT-4o。

Conclusion: GDC Cohort Copilot显著简化了GDC队列筛选过程，提高了新用户的易用性和效率，并提供了优于主流商用大模型性能的本地开源模型。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [26] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 本文提出MemAgent，通过分段读取和创新内存覆盖策略，实现对超长文本任务的高效处理与外推，性能损失极小，准确率达95%以上。


<details>
  <summary>Details</summary>
Motivation: 处理超长文本在效率和性能上的挑战，目前的方法虽然有提升，但在无限长文本情境下依然会出现性能退化。任务动机是设计能够线性复杂度处理无限长文档且外推性能不下降的模型。

Method: 提出了一种端到端优化的MemAgent代理工作流。MemAgent将长文本按段读取，并采用覆盖(overwrite)策略动态更新内存。同时，扩展了DAPO算法，用于独立上下文多对话生成的训练。

Result: MemAgent在长文本处理能力上表现优异，能将8K上下文外推出3.2M文本的QA任务，性能损失小于5%；在512K RULER测试中准确率超过95%。

Conclusion: MemAgent通过端到端学习与创新的内存更新机制，实现了对无限长文本的高效处理和优异的外推能力，显著缓解了长文本处理中的性能退化问题。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [27] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 本文提出了基于LoRA的DoMIX方法，有效降低了领域增量自适应预训练资源消耗，提升了对领域顺序的鲁棒性，并能为不同任务输出专属模型，可广泛应用于大模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有的持续领域自适应预训练（continual DAP）方法在训练过程中计算资源需求高、对数据顺序敏感、且通常只能生成一个泛化模型，不符合为每个任务定制模型的核心诉求。

Method: 提出了DoMIX方法，利用PEFT中的代表技术——LoRA模块，实现参数高效的并行领域自适应预训练。该方法更加高效、对领域顺序不敏感，并能够根据子任务需求生成专属模型。

Result: DoMIX显著降低了算力和显存消耗，对增量域顺序鲁棒，并能根据具体任务生成定制的预训练模型。此外，该方法对标准大模型微调（LLM fine-tuning）同样适用。

Conclusion: DoMIX有效解决了持续领域自适应预训练高资源消耗、顺序敏感及单一输出等问题，为领域适应和模型微调提供了更高效和灵活的解决方案。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [28] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本论文提出一种集成多模态大模型和检索策略的新系统，在科学视觉问答挑战赛中取得第三名，F1分数达85.12，代码已开放。


<details>
  <summary>Details</summary>
Motivation: 科学领域的视觉问答任务存在多模态信息理解和回答生成的挑战，需要更高效、准确的方法来提升模型的表现。

Method: 提出了一套集成两种多模态大语言模型以及多种少样本示例检索策略的系统。针对不同的图表和问题类型，选择相应的模型和少样本设定，同时基于模型的置信度来筛选答案。

Result: 在SciVQA 2025的盲测数据上，系统在七支队伍中排名第三，平均F1分数为85.12（基于ROUGE-1、ROUGE-L和BERTS评测）。

Conclusion: 所提出的多模型系统能有效提升科学视觉问答任务的准确性，并具备较强的综合表现。系统代码已公开，有助于社区复现和后续研究。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [29] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 作者提出了一种将参数化量子电路用于Transformer中前馈网络模块的方法，显著减少参数量并在标准数据集上超越经典BERT，尤其在小样本学习下表现更优。研究证明，合理设计的PQC可成为FFN的高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 近年来参数化量子电路（PQCs）被用于提升神经网络结构的表达能力。在Transformer结构中，前馈网络（FFN）占比最大的参数量。以往大多将PQCs用于自注意力模块，本研究则专注于FFN模块，旨在探索替代FFN以实现参数高效化及模型表达能力提升。

Method: 提出QFFN-BERT，将轻量版BERT中的FFN模块替换为PQC层。系统研究了PQC深度、可表达性和可训练性之间的权衡，设计中加入残差连接，采用$R_Y$和$R_Z$旋转门，并用交替纠缠方式，实现稳定训练和高表达能力。所有实验都在经典模拟器上进行。

Result: 在SST-2和DBpedia数据集上，QFFN-BERT配置优化后，分类准确率可达或略超基线（最高102.0%），FFN相关参数减少99%以上。在小样本学习场景下，模型表现更具优势，显示出较强的数据效率。消融实验表明，不优化的PQC无法收敛，证明PQC需与深度学习基本设计原则协同设计。

Conclusion: PQC可作为经典FFN的高表达力和高参数效率替代方案，前提是需结合经典深度学习设计，合理调整量子电路结构。QFFN-BERT在主流任务上表现优越，尤其适合参数受限和小样本训练环境。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [30] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 通过参数化数据选择，仅用少量高质量数据即显著提升代码大模型性能并节省算力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型主要通过增加数据量提升性能，忽视了数据质量，导致训练效率降低。

Method: 提出利用参数化模型进行代码数据选择的方法，优化以保证选中子集的数据分布一致性和多样性，从而获得高质量训练数据。

Result: 实验表明，仅使用1万条样本，我们的方法在HumanEval和MBPP上分别比9.2万全量样本提升2.4%和2.3%，且优于其他采样方法。

Conclusion: 该方法能够在显著降低计算成本的同时有效提升模型性能。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [31] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究评估了Akan语ASR模型在多领域的泛化能力，发现模型对训练领域依赖强，不同架构在错误类型上有权衡，强调需针对低资源语言开发更适应不同场景的ASR技术。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别（ASR）研究主要基于同源数据集评估模型性能，鲜有关注其在不同语音场景下的泛化能力。

Method: 本研究以Akan语为例，基于Whisper、Wav2Vec2等Transformer架构，构建七种ASR模型，利用四个包含不同领域的Akan语音语料库，对比其在词错误率和字错误率上的表现。

Result: 实验显示，各模型在其训练领域内表现最佳，在不匹配场景下准确率显著下降。其中Whisper模型生成更流畅但有误导性的转录，Wav2Vec2在陌生输入下产生较难解读但明显的错误。

Conclusion: ASR模型在域外泛化能力有限，且架构间在错误表现上存在可读性与透明性的权衡。针对低资源语言，亟需开展领域自适应、多语种训练等方面的研究。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [32] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 该研究提出并实证了一套低资源语言障碍语音数据收集流程，发布了首个阿坎语障碍语音开源数据集和工具，初步微调ASR模型后对特殊语音识别性能有所提升，为包容性语音识别奠定基础。


<details>
  <summary>Details</summary>
Motivation: 受语言障碍影响者在语音识别技术中的需求常常被忽视，尤其是在资源匮乏的语言环境下，如加纳广泛使用的阿坎语，现有的语音识别系统对这类特殊语音的识别效果较差。该研究希望通过收集和开放相应语音数据，推动包容性的ASR（自动语音识别）技术发展。

Method: 提出了一套社区驱动的数据收集最佳实践（即“cookbook”），并为低资源语言（如阿坎语）中传播受损语音构建数据集。随后，公开分享了数据集、cookbook以及相应工具，并对现有开源ASR模型进行了微调，使其更好地识别阿坎语的障碍语音样本。

Result: 成功建立并公开了全球首个阿坎语障碍语音开源数据集、社区数据收集手册与工具，初步微调的开放源ASR模型在阿坎障碍语音识别上有一定改进。

Conclusion: 社区驱动、公开透明的障碍语音数据收集和ASR模型开发是推动低资源语种包容性AI应用的有效途径，相关成果为其他语种和社群提供了可复制模板。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [33] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 作者构建了一个1200份印度保释判决的数据集，并做了详细标注，是推动印度法律NLP研究的重要资源。


<details>
  <summary>Details</summary>
Motivation: 印度等地区的法律NLP发展受限，主要由于缺乏结构化数据集。

Method: 构建了IndianBailJudgments-1200数据集，收集了1200份印度法院关于保释的判决，标注了20多种属性（如保释结果、罪名、犯罪类型、法律推理等）。采用经prompt优化的GPT-4o进行自动标注，并进行一致性验证。

Result: 形成了首个专注于印度保释判例、公开可用的数据集，可用于预测结果、摘要、公平性分析等多种NLP法律任务。

Conclusion: IndianBailJudgments-1200数据集为印度法律NLP任务提供了重要基础，有望推进该领域的发展。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [34] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 针对现有开源LLM在高不确定性信息查找任务中能力不足的问题，作者提出了WebSailor后训练方法，设计多种机制提升推理和信息探索能力。结果表明，该方法能让开源LLM在此类任务上达到专有系统水平，极大缩小了能力差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在面对极其复杂的信息查找任务时，因受限于人类认知模式，在面对高度不确定性信息时表现不佳。专有系统如DeepResearch能取得超越人类水平的成绩，说明专有系统具备某种开源模型缺乏的重要推理能力。本文希望用新方法让开源LLM具备此能力。

Method: 提出了WebSailor，一种包含后训练的完整方法流程。主要包括：1）通过结构化采样和信息模糊生成高不确定性新任务；2）引入RFT冷启动机制；3）采用高效的agent型强化学习算法Duplicating Sampling Policy Optimization (DUPO)。通过这一套流程提升模型在不确定环境中的信息探索和推理能力。

Result: 实验显示，采用WebSailor的模型在复杂的信息查找任务上，显著超越了所有开源智能体，并达到了与专有系统相当的性能。

Conclusion: WebSailor后训练方法能有效提升开源LLM在高不确定性复杂任务中的推理能力，使其能力接近甚至匹配商业专有代理系统。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [35] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文指出传统主动学习与标注体系忽视了人类标签变异（HLV），作者提出应区分标签变异的有用信号与噪声，并提出一套纳入HLV的主动学习新框架，为更贴合真实世界的标注和模型训练奠定基础。


<details>
  <summary>Details</summary>
Motivation: 在监督学习中，高质量有标注数据的获取十分困难，且标签变化（LV）现象普遍存在。传统注释框架通常假设“唯一正确标签”，忽略了人类标注变异（HLV）的信息价值。此外，主动学习（AL）在优化有限标注预算时，通常也依赖于一些不现实的简化假设。

Method: 作者梳理并分析了现有在主动学习和标签变异领域对于标签类型与真实性基本假设的处理方式，提出需要将观测到的标签变异分解为信号（例：HLV）和噪声（例：注释错误）。进而，作者提出了一套将HLV整合进主动学习全流程（包括样本选择、注释员选择和标签表示）的概念性框架，并讨论了大语言模型（LLM）作为注释员的应用。

Result: 系统回顾并对比了主动学习与（人类）标签变异领域对标签真实性问题的处理，揭示了现有方法未能充分利用HLV，并首次系统性提出了HLV感知主动学习的理论基础和构建框架。

Conclusion: 充分考虑和整合人类标注变异的主动学习框架，更能反映现实世界注释的复杂性，有助于提升有标注数据的利用效率与机器学习模型的实际应用能力。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [36] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF是一种后训练对齐框架，通过将基线分解为多视角成分，实现对LLM输出的去偏和对齐，效果良好且无需精细化微调。


<details>
  <summary>Details</summary>
Motivation: 随着对简便偏见缓解方法需求的增长，需要一种在不影响部署的LLM基础上自动对齐和减少偏见的新方法。

Method: MPF基于SAGED流程，将基线（如HR专业人士的情感分布）分解成可解释的视角组件，通过采样和加权生成响应，使输出与细致且类人的基线对齐。

Result: 实验证明，MPF能有效减少LLM输出与理想或特定基线之间的差异（如降低KL散度和校准误差），并能泛化至未见过的问题。

Conclusion: MPF提供了一种可扩展且可解释的方法，用于大型语言模型的对齐和去偏，兼容已部署的LLMs，无需复杂的提示工程或微调。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [37] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文提出了一个新数据集与分析框架，可量化和解释多种语言元素中的性别及上下文偏见，并在多数据集上验证了其有效性，揭示了职业之外也存在广泛的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的性别偏见研究多集中于职业词汇，但尚未深入探讨动作动词、名词等多样语境下的性别偏见，对性别偏见的解释性和跨语种泛化能力也有待提升。

Method: 作者提出了一个包含多种词类（如动作动词、名词和职业名称）的新数据集GenderLexicon，并建立了一个可以评估上下文偏见及相关性别偏见的分析框架。该框架通过分数方式量化偏见，并提升了结果的解释性。此外，在五个不同数据集（包含日语数据）上进行了实验验证。

Result: 实验表明，该方法不仅能够量化和解释性别偏见，还发现性别偏见并不局限于职业词汇，在其它语言元素中也存在性别倾向。方法在多数据集上均取得有效性验证。

Conclusion: 本文提出的数据集和方法能够较好地检测、解释多类语言元素中的性别和上下文偏见，有助于拓展性别偏见研究的广度和深度。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [38] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了一种专注于AI领域论文局限性识别的基准LimitGen，结合文献检索加强LLM系统此方面的能力，并证实其可为论文评议提供有用补充。


<details>
  <summary>Details</summary>
Motivation: 随着论文数量的激增，同行评议变得更具挑战性。虽然大语言模型（LLMs）在科学任务中表现突出，但它们在协助同行评议、特别是识别论文局限性方面的潜力尚未被充分研究。

Method: 作者提出了一种限制类型的系统分类法，并据此创建了LimitGen基准，包括合成数据集（LimitGen-Syn）和人工标注数据集（LimitGen-Human），用于评估LLMs识别论文局限性的能力。此外，引入文献检索来增强LLMs基于已有科学发现识别局限性的能力。

Result: 搭配文献检索的LLM系统在生成论文局限性、提供具体且有建设性的反馈方面性能提升。LimitGen基准为后续LLM评测提供了新的工具。

Conclusion: 增强型LLM系统能够在支持早期反馈、补充人类同行评议中更好地发挥作用，推动了LLM在科学研究评审中的应用探索。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [39] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究发现，两元音在F1×F2空间至少需间隔14-51 mels才会被说话者准确区分模仿，为人类语音系统的设计和音位分布提供了新理论依据。


<details>
  <summary>Details</summary>
Motivation: 人类元音发音涉及复杂且协调的动作，这些动作被认为部分受控于感知的听觉空间，但这种控制的精细度未知。本文旨在探究在听觉空间中多大的元音差异才能被说话者准确区分并模仿。

Method: 使用元音模仿实验（vowel mimicry paradigm）来测量两个不同前元音在听觉空间中的最小可产生差异（Just Producible Difference, JPD），并对两组英语母语者进行实验。

Result: JPD在F1×F2空间中被估算为14至51 mels之间。

Conclusion: 本研究首次测量了英语前元音产生的最小可识别差异，为元音系统的结构设置了理论下界，对语音产生的情节理论和元音音位系统的排列规律提供了心理声学的解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [40] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大语言模型自我纠错存在明显盲点，主要受训练数据影响，通过恰当提示能极大改善。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）非常强大，但在自动回归生成中经常出错且难以及时自我纠正，阻碍了其在可信领域的应用。研究动机在于系统性揭示和量化LLM在自身输出自我纠错方面的盲点。

Method: 提出了Self-Correction Bench，通过人工注入不同复杂度的错误，系统评估14种主流LLM在自我纠错上的表现，并结合训练数据和模型机制深入分析盲点成因。同时，尝试简单提示词激活自我纠错能力。

Result: 14个模型自我纠错盲点率高达平均64.5%；训练数据中主要是无错误演示，缺乏错误及其纠正的案例，RL训练模型因有反馈能表现更好。简单提示“Wait”能显著激活纠错能力，将盲点率降低89.3%。

Conclusion: 现有LLM普遍存在自我纠错盲点，主要原因在于训练数据和范式的局限。通过提示工程等方法可大幅提升其自我纠错能力，未来模型训练范式有待优化以提升其可靠性与可信度。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [41] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 推理能力虽提升了大语言模型的复杂推理表现，却可能让其更易受到社会偏见攻击，尤其是在Chain-of-Thought（CoT）提示时。未来推理机制设计需强化对偏见安全的考量。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）提示和微调推理路径等机制提升了语言模型进行多步推理的能力，但这些推理能力对模型抗社会偏见的鲁棒性影响尚不明确。因此，研究者希望系统性评估推理能力对模型公平性和安全性（抵抗偏见引发）的影响。

Method: 利用CLEAR-Bias基准和LLM-as-a-judge自动化安全评分方法，对多种最先进推理语言模型（RLMs）在多元社会文化维度下进行系统评测，并通过jailbreak技术测试其安全防护机制的强度。

Result: 结果显示，不论是通过CoT提示还是微调方式拥有显式推理能力的模型，通常比普通模型更容易受到偏见引发攻击。其中，推理能力模型对讲故事、虚构角色等情境重构攻击尤其敏感。因此，推理机制并不必然提升模型鲁棒性，反而可能增加刻板印象的风险。推理微调模型略优于仅用CoT提示的模型。

Conclusion: 推理能力并不必然增强模型对偏见的鲁棒性，甚至可能提供新的偏见攻击路径。因此，推理设计需更注重偏见风险评估和防护措施。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [42] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 该论文提出了一个包含多样解题路径的新数据集MathV-DP，并通过创新性方法提升了多模态大模型在数学推理的准确性与多样性，显著优于以往方法，强调了多视角和多样性的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM在数学推理时仅基于一对一图文配对和单一解答，忽视了多种合理的推理视角和内在反思，无法全面提升推理能力。

Method: 提出MathV-DP数据集，包含每个图像-问题对的多种解题路径；在Qwen-VL基础上，采用有监督学习微调，并通过分组相对策略优化（GRPO）进行强化学习，结合正确性判别与多样性奖励函数。

Result: Qwen-VL-DP在MathVista's minitest和Math-V基准测试中，在准确性和生成多样性方面均显著优于现有主流多模态LLM。

Conclusion: Qwen-VL-DP模型显著提升了多模态LLM在数学推理中的准确性和生成多样性，验证了引入多样化解题视角与反思性推理的重要性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [43] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 作者提出了动态路由框架SynapseRoute，根据问题复杂度智能选择高推理或低成本模式，有效提升了准确率并大幅降低了时间与成本，适合实际部署和节能需求，并通过新指标AIT多维度衡量了效果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在实际应用中需要权衡性能与成本，高推理模式虽然强大但代价高昂，然而有相当部分问题可以通过低推理模式准确解决，因此需要一种智能分配模式以优化效率和体验。

Method: 通过对医学数据集的实证分析，设计机器学习动态路由系统，将输入问题分配给高推理或低推理（非推理）模式，并进行相关实验对比分析。

Result: SynapseRoute在准确率（0.8390 vs 0.8272）上优于单独高推理模式，推理时间和Token消耗分别降低36.8%和39.66%；提出了AIT指数以综合评估方案表现。

Conclusion: 提出并验证了一种基于动态路由的LLM推理模式选择方法（SynapseRoute），能够在保持高准确率的同时显著降低推理时间和成本，并提出新的综合评价指标（AIT）。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [44] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 本文提出用于评测模型精准指令遵循与泛化能力的新基准IFBench，并通过可验证奖励进行强化学习，实验证明显著提升了模型对新型约束的遵守和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流大模型在面对带输出约束的指令时缺乏泛化能力，尤其是在未见过的约束上，且现有评测基准覆盖有限，难以真实反映模型精准理解和遵循指令的能力。

Method: 提出并使用了IFBench这个包含58种多样且有挑战性的验证性输出约束的新基准，还设计了约束验证模块，并通过可验证奖励的强化学习（RLVR）提升了模型的一般化能力。

Result: 实验发现：大多数现有模型在现有基准集上过拟合，难以泛化到新约束；引入RLVR机制可显著提升泛化能力。公开了IFBench基准集、29个新的人工注释训练约束与验证函数、RLVR训练提示词和相关代码。

Conclusion: 通过在训练中结合可验证的奖励（RLVR）并设计约束验证模块，模型在精准指令遵循方面有明显提升，但面对完全新颖的输出约束仍存在挑战。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [45] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本论文指出，通过简单的用户反馈机制，攻击者可以持续性地影响语言模型，植入虚假知识、安全漏洞和虚假信息。这对依赖用户偏好微调的模型安全性提出重要警示。


<details>
  <summary>Details</summary>
Motivation: 探索利用用户反馈调整语言模型时可能带来的安全隐患，尤其是攻击者是否能通过有限的反馈方式大规模、持续性地影响模型行为。

Method: 攻击者通过精心设计的提示词诱导模型输出“投毒”或正常响应，并对“投毒”响应进行点赞，或对正常响应点踩，使得偏好的反馈信号在后续的模型偏好微调环节中被利用。随后，评估模型在未受控情况下输出“投毒”内容的概率提升。

Result: 证明了单个用户仅通过有限的提示与反馈机制，可以实现：（1）向模型注入原本不存在的事实信息；（2）改变模型代码生成方式，引入潜在安全漏洞；（3）注入虚假财经新闻。即高度受限用户反馈信号可以精准操控大模型行为。

Conclusion: 该研究揭示了基于用户反馈微调语言模型时存在的显著安全风险：有限而细致化的用户偏好数据也可被滥用，成为模型行为高度可控的攻击载体，拓展了以往关于数据投毒与提示注入攻击的研究。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [46] [On Obtaining New MUBs by Finding Points on Complete Intersection Varieties over $\mathbb{R}$](https://arxiv.org/abs/2507.02492)
*Arindam Banerjee,Kanoy Kumar Das,Ajeet Kumar,Rakesh Kumar,Subhamoy Maitra*

Main category: cs.DM

TL;DR: 本文通过代数簇理论和矩阵理论研究MUBs，提出了MUBs扩展性的判据，并建立了MUBs与最大交换正交正规矩阵集合之间的双射关系，从而推进了量子信息数学理论的发展。


<details>
  <summary>Details</summary>
Motivation: 互相不相干基（MUBs）在量子物理中具有重要作用，且其结构背后有丰富的数学背景。本文希望推动MUBs理论发展，解决如何扩展MUBs集合的问题。

Method: 通过研究一个特定仿射代数簇的实点，给出对$C^n$中MUBs集合可扩展性的等价判据。该代数簇来自于决定MUBs系统可扩展性的关系。此外，作者考察了最大交换正规正交矩阵类与MUBs之间的对应关系。

Result: 提出了等价判据用于判断$C^n$中MUBs可扩展性；证明了该仿射代数簇的部分为完备交域；且给出了MUBs与$	hcal M_n(bC)$中最大交换正规正交矩阵基的双射关系，从而关联了MUBs存在性与最大交换基的存在性。

Conclusion: 本文为MUBs的扩展性问题提供了新的等价判据和数学工具，深化了MUBs理论与矩阵理论的联系，有助于进一步理解量子系统的数学结构。

Abstract: Mutually Unbiased Bases (MUBs) are closely connected with quantum physics,
and the structure has a rich mathematical background. We provide equivalent
criteria for extending a set of MUBs for $C^n$ by studying real points of a
certain affine algebraic variety. This variety comes from the relations that
determine the extendability of a system of MUBs. Finally, we show that some
part of this variety gives rise to complete intersection domains. Further, we
show that there is a one-to-one correspondence between MUBs and the maximal
commuting classes (bases) of orthogonal normal matrices in $\mathcal
M_n({\mathbb{C}})$. It means that for $m$ MUBs in $C^n$, there are $m$
commuting classes, each consisting of $n$ commuting orthogonal normal matrices
and the existence of maximal commuting basis for $\mathcal M_n({\mathbb{C}})$
ensures the complete set of MUBs in $\mathcal M_n({\mathbb{C}})$.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [47] [Engineering an LTLf Synthesis Tool](https://arxiv.org/abs/2507.02491)
*Alexandre Duret-Lutz,Shufang Zhu,Nir Piterman,Giuseppe de Giacomo,Moshe Y Vardi*

Main category: cs.FL

TL;DR: 作者提出了一种通过MTBDD优化表示的直接LTLf到DFA合成方法，使得合成器在基准测试中超越现有工具，实现了更高效的反应式合成。


<details>
  <summary>Details</summary>
Motivation: LTLf反应式合成旨在构建一个根据输入历史决定输出的转换器，确保任意输入序列能在某一前缀上满足LTLf规范。当前工具在效率和性能上仍有提升空间。

Method: 提出了一种从LTLf直接转化为DFA的新方法，DFA通过共享节点的二元决策图数组(MTBDD)表示，并直接将其作为可达性博弈模型，在构建过程中动态求解。

Result: 基于此方法实现的LTLf合成器在作者的基准测试集上优于现有工具。

Conclusion: 该工作提供了一种更高效的LTLf反应式合成方法，并通过实验验证其优越性能。

Abstract: The problem of LTLf reactive synthesis is to build a transducer, whose output
is based on a history of inputs, such that, for every infinite sequence of
inputs, the conjoint evolution of the inputs and outputs has a prefix that
satisfies a given LTLf specification. We describe the implementation of an LTLf
synthesizer that outperforms existing tools on our benchmark suite. This is
based on a new, direct translation from LTLf to a DFA represented as an array
of Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based
representation can be interpreted directly as a reachability game that is
solved on-the-fly during its construction.

</details>
