<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 12]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 本文提出PerfOrch多阶段编排框架，基于异构LLM性能动态分配编程任务，在不需微调的前提下大幅提升代码生成正确率与执行效率，框架易扩展，为生产级AI自动编程提供可行路径。


<details>
  <summary>Details</summary>
Motivation: 当前单一模型的自动代码生成方法未能充分利用不同大语言模型在各种编程语言、算法领域和开发阶段上的异质计算优势。

Method: 提出了一种多阶段、基于性能引导的编排框架PerfOrch，通过动态分配任务给最适合的LLM，在结构化的生成-修复-优化流程中自动完成代码生成；该方法以对17个主流LLM在5种编程语言上的系统性实验为基础，通过功能正确性和运行时性能指标指导模型选择，并采用分阶段验证与回滚机制。

Result: PerfOrch在HumanEval-X和EffiBench-X基准上分别达到了96.22%和91.37%的正确率，显著优于单一强基线如GPT-4o（分别为78.66%和49.11%）；此外，对58.76%的问题实现了显著的执行时间优化，五种语言的中位加速比为17.67%至27.66%。框架具备模块化和可扩展性，可便捷集成新LLM。

Conclusion: 利用多模型优势，实现代码生成性能与正确性的双重提升，提出了适应生成式AI快速发展且具备生产级能力的自动化软件工程新范式。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 本文系统分析了 GitHub 上 'wontfix' 标签的使用现状和背后原因，指出其虽利于资源管理，但也有潜在的负面社区影响，并为项目管理者提供改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管 'wontfix' 标签在 GitHub 上被广泛使用，但其对项目管理和开源社区动态的具体影响尚不明确。本研究旨在深入了解该标签的使用现状及背后原因。

Method: 采用混合方法研究，结合量化分析（统计 'wontfix' 标签的使用频率）和定性分析（通过开放编码与主题分析梳理标签原因）。数据来源于3,132个 GitHub 最受欢迎的开源仓库。

Result: 约30%的 GitHub 项目会对部分问题应用 'wontfix' 标签，尤其常在用户提交的 bug 报告和功能请求上出现，共归纳出8个常见的 'wontfix' 理由，涵盖用户及维护者视角。

Conclusion: 'wontfix' 标签是 GitHub 项目资源管理和贡献者引导的重要工具，但其同时可能打击社区参与热情，降低管理透明度。理解其应用原因有助于开源项目高效协作和决策。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC框架通过模拟多样化玩家特质，让自动化测试代理在视频游戏中表现出不同策略，从而提高测试质量和发现更多问题，比现有技术效果更强。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏非常复杂，传统自动化测试方法难以应对，全面测试又对保证游戏质量至关重要。现有的自动化游戏代理虽采用强化学习、模仿学习或大语言模型，但常常忽略不同玩家的多样化策略，导致测试结果单一，难以覆盖更多边缘情况。

Method: 提出了MIMIC框架，将多样化人格特质融入游戏代理，使其能够在类似情境中采用不同的游戏策略，通过模仿不同玩家风格提升测试覆盖率和交互丰富性。

Result: MIMIC在多个游戏测试中实现了更高的测试覆盖率和更多样化的游戏交互，尤其在Minecraft中完成任务的成功率更高且解决方案更丰富，优于当前先进代理。

Conclusion: 将玩家多样化的人格特质融入自动化测试代理能显著提升测试覆盖率和发现更多边缘情况，为视频游戏质量保障带来新突破。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: 本文提出并实现了结合区块链与许可证自动化管理的FOSS-chain平台，用于解决开源软件许可证兼容与合规难题。初步评估显示平台实际可行，并具备推向实际场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）广泛应用于各类项目，但涉及许多不同的许可协议，这些协议在被二次创作、修改和分发时，存在不兼容等复杂合规性问题，容易引发法律纠纷。如何简单、透明、可靠地管理和合规使用开源软件，是亟需解决的问题。

Method: 本文提出并开发了一种将区块链技术与许可证管理结合的新方法。具体实现为FOSS-chain平台，通过区块链不可篡改的特性，自动化管理和记录软件许可合规过程，支持14种主流开源协议，并进行了初步的小规模用户研究评估。

Result: FOSS-chain平台原型在初步小规模用户研究中表现良好，显示出在真实软件系统中应用和推广的潜力。

Conclusion: 结合区块链与开源软件许可证自动化管理，有助于提升许可证合规性和操作透明度，可有效应对开源许可证兼容性及合规问题。实验结果支持该平台具备进一步应用与发展的前景。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: 本文提出并实现了ARENA工具插件，简化安卓应用硬件能耗测量流程，实现测量、统计和可视化一体化，方便开发者和研究者在IDE中高效开展能耗分析。


<details>
  <summary>Details</summary>
Motivation: 现有安卓应用能耗硬件测量流程繁琐，难以适配和复现，且缺乏易用的开源工具，限制了能耗优化研究与实践。

Method: 开发了ARENA工具作为IntelliJ和Android Studio的插件，允许用户连接硬件测量设备、执行能耗测试场景、自动采集和处理数据并输出统计分析与可视化结果。

Result: ARENA实现了能耗数据的自动采集、去噪、统计聚合与可视化，支持开发者在开发阶段对不同版本或不同应用进行能耗对比测试，提高测量效率和结果可靠性。

Conclusion: ARENA工具极大简化了基于硬件的安卓应用能耗测量流程，使开发者和研究者能够在IDE中方便、可靠地完成测量、分析和可视化。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本论文针对自动程序修复的速度瓶颈问题，提出了NARRepair——首个定制的非自回归代码生成模型，有效兼顾修复速度与质量，实验表现超过现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复（APR）多依赖自回归（AR）方式逐token生成补丁，参数量大会导致修复延迟，制约实际应用。急需提升修复速度，解决大规模程序修复时的时间瓶颈。

Method: 提出NARRepair，这是首个为APR定制的非自回归（NAR）代码生成模型。其主要创新在于三方面：1）修复动作预测器用于缓解过度修复；2）token间依赖提取器用于增强信息联系；3）双阶段解码器提升上下文感知能力。

Result: 在三组主流APR数据集上的实验表明：1）NARRepair在有限时间内性能优于其他APR技术；2）在GPU环境下，NARRepair修复速度较AR方法快1.4-6.4倍。速度与修复准确率均达业界领先水平。

Conclusion: NARRepair显著提升了自动程序修复的速度和准确率，在大规模与高时效要求场景下具有重要应用价值。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: 该论文提出RefFilter工具，结合自动重构检测，有效减少协同开发中语义干扰检测的误报率。在多数据集实验中将误报率降至32%，虽略有召回损失，但整体精度有大幅提升，显示出在现代软件合并流程中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 在协同软件开发中检测语义干扰非常困难。目前一些轻量级静态分析技术提高了效率，但依然存在高误报率，主要原因在于无法区分保持行为不变的代码重构与真正影响行为的更改。

Method: 提出了RefFilter工具，通过自动检测重构操作，并将行为保持的重构从干扰检测结果中剔除，在现有静态分析框架基础上提升精确度。

Result: 在一组99个场景和1,087个新汇总的合并场景数据集上实验显示，RefFilter能够将误报率降低约32%，虽然略微提升了漏报（假阴性），但精度提升远高于召回率的损失。

Conclusion: 引入重构感知的语义干扰检测显著提升了协同开发中的合并支持，其带来的高精度使得这种方法在实际开发流程中具备实用性和有效性。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 提出CLAST技术，优化单元测试示例语义清晰度，显著提升LLM驱动的测试生成效果，实验证实优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在利用in-context learning（ICL）生成单元测试时表现出色，但生成质量很依赖上下文示例的结构和语义清晰度。低质量示例会导致效果下降。

Method: 提出CLAST技术，通过程序分析和LLM重写，将复杂测试拆解为逻辑更清晰、语义更明确的单元测试，从而提高作为上下文示例的质量。

Result: CLAST在4个开源和3个工业项目上的实验证明，其能显著优于现有的UTgen方法，在保持测试有效性的同时提高语义清晰度。与UTgen相比，CLAST完全保留原始测试有效性，而UTgen分别使编译成功率、通过率、测试覆盖率、变异分数平均降低12.90%、35.82%、4.65%、5.07%。用户调查有超过85.33%参与者更喜欢CLAST改进的测试。将CLAST改进的测试作为示例有助于提升ICL测试生成方法效果，具体生成测试的编译成功率、通过率和覆盖率分别平均提升25.97%、28.22%、45.99%。

Conclusion: CLAST能够系统提升单元测试语义清晰度和上下文示例质量，极大增强基于LLM的单元测试生成性能，对软件测试实践和未来研究具有重要意义。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 文章提出利用模型驱动工程和声明式建模语言，系统性推导优化问题变化后的重新优化规范，并通过助教分配案例，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 已有的优化问题在解决后，随着环境因素变化，解决方案通常需要适应，这一问题称为重新优化，广泛存在于铁路乘务重新调度、护士排班调整和航班恢复等领域。

Method: 本文提出利用模型驱动工程（MDE），尤其是声明式建模语言和模型变换的方法，系统化地从原始优化问题推导出重新优化问题的规范。针对组合型重新优化问题，提供问题变化类型及相应规范推导策略，并用基于GIPS工具的初步实现，对助教分配资源调度问题进行验证。

Result: 初步证明了模型驱动工程方法在重新优化问题规范推导上的可行性，并通过助教分配案例，展示了系统按变动需求生成新的优化问题的能力。

Conclusion: 声明式建模和模型驱动方法为优化问题的重新优化提供了系统化和高效的规范推导途径。初步实现证明了其有效性，有望应用于多种实际资源分配场景。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 本文介绍了一个新的ESE专栏，用于探讨经验软件工程研究过程中的方法论和元问题，促进社区对难点和不足的讨论，推动该领域的持续进步。


<details>
  <summary>Details</summary>
Motivation: 尽管经验软件工程（ESE）已取得显著进展，但在可复现性、外部有效性、评审主观性、及将学术成果转化为工业实践等方面仍面临挑战。此外，许多ESE领域细节缺乏明确记录，阻碍新成员快速融入和学习。

Method: 创建ACM SIGSOFT SEN的新专栏（SEN-ESE），以对ESE领域相关的元问题进行定期讨论。专栏内容将通过专家访谈、焦点小组、问卷调查和立场文章等多种形式阐述，促进经验、反思和最佳实践的交流。

Result: 新专栏提供一个开放平台，使ESE社区成员能够就研究方法、统计手段、跨学科出版、行业转化等问题深入交流，并鼓励社区提出意见和建议，以不断完善专栏内容。

Conclusion: 该专栏旨在激发社区持续关注和讨论ESE研究中的隐性或被忽视主题，推动该领域方法和沟通方式的改进，并增强对新成员的指导和社区互动。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 本文提出一种融合视频与音频特征的多模态系统，利用先进的特征提取与融合结构，有效提升公共交通欺诈检测的准确性和实时性，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有公共交通系统中的欺诈和逃票检测大多依赖单一模态（如视频或音频），准确率和召回率有限，且早期融合方法难以有效捕捉视觉和音频之间的复杂交互。

Method: 提出了多模态欺诈与逃票检测系统，结合ViViT模型进行视频特征提取、AST模型进行音频分析，并采用Tensor Fusion Network（TFN）结构，通过2阶笛卡尔积显式建模单模态及双模态交互，实现视觉行为和音频线索的信息融合。

Result: 在自制数据集上，系统检测准确率达到89.5%，精准率87.2%，召回率84.0%，明显优于早期融合基线（召回率仅75%）。消融实验表明融合方法比传统串联提升7%的F1分数和8.8%的召回。

Conclusion: 所提出的系统能在实时下高效检测公共交通欺诈行为，提升运营合规与乘客安全，降低收入损失。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: 论文针对代码数据集缺乏可验证质量问题，提出了SIEVE框架，用置信卡片实现机器可读、统计可验证的数据质量认证方案，期待降低质量保证成本，提高数据集可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的公共代码数据集在代码智能体和经验软件工程领域应用广泛，但缺乏可验证的质量保障。现有的静态“数据集卡片”虽能提供一定信息，但不可审计且不具备统计保证，导致团队各自搭建孤立、临时的数据清洗流程，增加了碎片化和成本。

Method: 提出了SIEVE这一社区驱动框架，将对数据集各属性的检查转化为Confidence Cards（置信卡片），即机器可读、可验证、随时可用且具备统计边界的证书。

Result: 提出了一项研究计划，旨在完善SIEVE，使其能用随时可验证的置信卡片替代传统的叙述性卡片。这样有望降低代码数据集的质量保证成本，并提升对数据集的信任度。

Conclusion: 通过社区驱动和随时可验证的置信卡片框架，可有效解决当前代码数据集质量保障不足的问题，推动领域的信任和成本降低。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Sequent Calculi for Data-Aware Modal Logics](https://arxiv.org/abs/2510.01868)
*Carlos Areces,Valentin Cassano,Danae Dutto,Raul Fervari*

Main category: cs.LO

TL;DR: 该文首次为捕捉XPath导航和数据比较的逻辑HxpathD建立了完整、可逆且支持cut消除的Gentzen归结演算，为数据感知模态逻辑的证明理论研究开辟新途径，可助力于对图状数据查询语言的更深层逻辑分析。


<details>
  <summary>Details</summary>
Motivation: 以往关于HxpathD逻辑的研究主要集中在模型理论层面，而对其证明理论基础关注较少。因此，作者期望从证明理论角度，为HxpathD逻辑及类似的数据感知模态系统提供系统化工具，便于对半结构化查询语言进行更深入的逻辑分析。

Method: 作者从证明理论角度出发，设计并证明了HxpathD逻辑的Gentzen风格归结演算系统的正确性和完备性，同时展示了所有规则均具备可逆性，并实现了cut消除。

Result: 成功设计了一套针对HxpathD数据感知模态逻辑的完整、可逆的Gentzen风格归结系统，并证明了cut消除。这为该类逻辑的进一步理论拓展和应用打下了基础。

Conclusion: 本文提出了一套完整且可逆的Gentzen风格归结演算，用于处理数据感知模态逻辑HxpathD，并证明了该系统具备消解剪枝的特性。这为数据感知模态逻辑的证明理论基础提供了重要贡献。

Abstract: Data-aware modal logics offer a powerful formalism for reasoning about
semi-structured queries in languages such as DataGL, XPath, and GQL. In brief,
these logics can be viewed as modal systems capable of expressing both
reachability statements and data-aware properties, such as value comparisons.
One particularly expressive logic in this landscape is HXpathD, a hybrid modal
logic that captures not only the navigational core of XPath but also data
comparisons, node labels (keys), and key-based navigation operators. While
previous work on HXpathD has primarily focused on its model-theoretic
properties, in this paper we approach HXpathD from a proof-theoretic
perspective. Concretely, we present a sound and complete Gentzen-style sequent
calculus for HXpathD. Moreover, we show all rules in this calculus are
invertible, and that it enjoys cut elimination. Our work contributes to the
proof-theoretic foundations of data-aware modal logics, and enables a deeper
logical analysis of query languages over graph-structured data. Moreover, our
results lay the groundwork for extending proof-theoretic techniques to a
broader class of modal systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 该论文通过设计新的概念学习任务，发现大型语言模型存在隐含的‘向上单调性’偏差，并证明上下文概念学习实验在揭示模型隐性偏差方面比直接提示效果更好。


<details>
  <summary>Details</summary>
Motivation: 设计一种新型的数据集和实验方法，以揭示大型语言模型内部隐含的认知偏差，尤其是概念学习任务中的表现。

Method: 使用带有上下文的概念学习实验，通过语言模型对量化词的推理进行分析，并与直接提示法进行对比。

Result: 通过实验发现，语言模型在处理量化词时倾向于“向上单调性”，而在不含概念学习成分的直接提示测试中这种偏差不明显。

Conclusion: 上下文概念学习是一种有效手段，可揭示大型语言模型中隐藏的认知偏差。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [15] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: 本文提出，低资源语言的AI应用应突破对静态语料依赖，转向人机互动、基于不确定性驱动的动态语言发现与学习，促进社群参与与语言多样性保护，倡导人本导向、协同合作的AI发展模式。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的自然语言处理（NLP）受到缺乏文本语料库、标准正字法和可扩展标注流程的根本性限制。虽然大型语言模型的最新进展提升了跨语言能力，但对弱势群体依然不可及，其依赖于大规模预收集数据和中心化基础设施。

Method: 提出一种基于联合人机不确定性的框架，将AI模型的知识不确定性与人类说话者的犹豫和信心信号结合，用以引导互动、查询选择和知识记忆，强调开放式、交互式的人机协作语言发现过程。

Result: 该文主张将语言技术研究的重点从静态数据收集转向基于互动、以不确定性驱动的动态发现，提出人机协作、共同适应的学习模式，促进弱资源语言的发现与保护，倡导重塑人机知识合作方式，尊重并赋能相关社群。

Conclusion: 未来的语言技术，尤其是面向低资源和未充分记录语言，应走向交互型、合作式模型构建，通过人机协作动态学习，替代以往以收集数据为核心的方式，从而保护和发现世界语言多样性，推动以人为本的AI进一步发展。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [16] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 气候披露同质化严重，企业承诺与具体目标脱节，大型企业披露较多但不够细致。大语言模型有助于文本评估，监管应加强承诺与实际转型的衔接。


<details>
  <summary>Details</summary>
Motivation: 面对气候变化导致的信息披露透明化诉求与披露内容象征化、仿效化的矛盾，本文旨在提出并实证检验一种多维度评估企业披露成熟度的新方法。

Method: 采用经气候传播微调的大语言模型，对828家美国上市公司的可持续发展与年报文本进行分析。通过四类分类器（情感、承诺、具体性、目标雄心）提取文本指标，再与企业属性做关联分析。

Result: 1）风险导向叙述与显式承诺一致，但与量化目标（如净零承诺）无关；2）大市值和高排放企业更积极披露承诺和行动，但与量化目标仍不符；3）披露风格高度同质，表明模仿行为盛行，降低了披露分辨力与决策价值。

Conclusion: 本文指出，尽管企业披露了大量气候相关承诺与行动，披露内容存在跟风和象征性表达，导致报告彼此相似，缺乏决策价值。大型和高排放企业披露更积极，但与量化目标存在脱节。需要更强有力的监管以连接承诺与可验证的转型计划。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [17] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 本文比较了三款面向兽医学的LLM总结工具在病例总结任务上的性能，以Hachiko表现最佳，并表明LLM-as-a-judge方法在该领域中的评测标准稳定且可扩展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在临床领域应用广泛，但其在兽医学领域的性能尚未得到充分探索。鉴于兽医领域专业需求，亟需评估和优化相关LLM工具。

Method: 作者选取了三个市面上的兽医专用LLM总结工具（Hachiko、产品2与产品3），在标准化的兽医肿瘤学病例数据集上进行比较，并采用由LLM主导的打分框架，从五个方面对生成的病例总结进行评分：事实准确性、完整性、时间顺序、临床相关性及组织结构。同时通过三次独立评估检验评分框架的稳定性。

Result: 产品1整体表现最佳，平均得分中值为4.61，显著高于产品2（2.55）和产品3（2.45）。在事实准确性与时间顺序上获得满分。评分框架的重现性高，三次独立评估的平均分标准差较低。

Conclusion: 兽医专用LLM工具在临床病例总结任务中表现突出，特别是Hachiko。采用LLM作为评审者的方法在兽医学临床NLP评测上具有良好的可扩展性和可重复性，对推动相关工具的落地有重要价值。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [18] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 本文提出了基于小型语言模型和Web实时证据的自动事实核查系统ClaimCheck，其模块化设计与优化提示使准确率超过大模型，且算力低、可解释性强并且已公开Demo。


<details>
  <summary>Details</summary>
Motivation: 现有的事实核查系统大多依赖庞大的、闭源的大型语言模型以及静态知识库，导致系统透明性差、计算需求高且难以解释。作者旨在开发一种依赖小型语言模型且具有高准确性和可解释性的自动事实核查系统。

Method: 提出了ClaimCheck系统，通过模仿人工事实核查流程，实现了透明且逐步的事实核查管道，包括：Web搜索查询规划、基于Web的证据检索与摘要、证据综合与再检索、以及最终评价。系统各模块均针对小型LLM进行优化，采用模组化设计和精细的提示策略。

Result: ClaimCheck使用远小于现有主流模型（如Qwen3-4B，而非LLaMA3.1 70B或GPT-4o），在AVeriTeC数据集上取得了76.4%的准确率，超越了更大模型的表现。消融实验表明，通过模块化设计和合理提示，小模型也能实现高效事实核查。并且系统已开放Demo，促进可访问性与透明性。

Conclusion: ClaimCheck充分表明小型开源语言模型在结构化、可解释的事实核查任务上（结合Web实时证据和模块化流程）能够达到甚至超越大型模型的效果。优化的小模块与提示设计是关键，系统具备高准确率、低算力需求和高度透明性。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [19] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 本文发现现有的数学奥林匹克基准可能高估了LLMs的数学能力。作者提出EEFSUVA新题集，结果显示LLMs在该题集上的表现明显不如传统题集，建议未来使用更广泛的数据集来评价和引导模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的数学能力评估以IMO等公开比赛为主，可能存在训练数据污染和类型单一，迫切需要更全面、真实的评估方式。

Method: 分析现有基准数据来源，提出并实验新的区域性奥林匹克题集EEFSUVA，比较LLMs在不同题集上的表现。

Result: LLMs在EEFSUVA上的表现明显下降，说明其真实数学推理能力受限，现有基准不足以充分测试模型能力。

Conclusion: 现有的数学基准可能高估了大语言模型（LLMs）的数学推理能力，全面评估需要更广泛的数据集。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [20] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 当前大语言模型对系统指令的优先级把控薄弱，对社会地位/共识线索反而异常敏感；机制分析揭示这两类冲突在模型内部有不同表征，社会线索反而能放大神经模型的整体顺从性。因此，未来需要专门对齐模型的层级感知能力，避免社会性误导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被设计为优先遵循系统指令并覆盖用户输入，但已经被发现模型经常违背这一原则，对社会性提示反而过度顺从。因此，要理解模型为何层级服从性脆弱，需要结合机制解释和行为分析。

Method: 1. 行为分析结合大规模数据集。
2. 使用线性探针(linear probing)分析冲突决策信号的编码时序与空间；
3. 使用Direct Logit Attribution方法检测模型内部对冲突信号的检测能力及一致性；
4. 通过steering实验整合上述发现，研究社会性提示向量对模型顺从行为的影响。

Result: 1. 系统与用户指令、以及社会线索之间的冲突在模型内部形成不同的子空间。
2. 对于系统-用户冲突模型虽能早期识别，但仅在社会性线索下表现出一致的顺从。
3. steering实验中，社会性提示不仅未削弱指令顺从，甚至增强了不区分角色的服从行为。
4. 这些发现解释了模型“系统服从性脆弱”的根本原因。

Conclusion: 本文认为大语言模型对系统指令的服从性脆弱，而对社会性线索（如权威、共识）高度敏感，说明当前模型缺乏层级感知能力（hierarchy-sensitive），需要更轻量化、敏感于层级结构的对齐方法。

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [21] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: 用LLM自动生成和标注数据训练高效小模型，不依赖人工标注，重排序效果优秀且低成本。


<details>
  <summary>Details</summary>
Motivation: 大模型在文档重排序上表现优异，但运行成本高；小模型推理快但依赖昂贵的人工标注数据。研究动机是能否不依赖人工标注，也能训练高效的小模型用于文档重排序。

Method: 提出了一种新颖的流水线，利用LLM从领域语料生成合成查询，并用LLM分类器自动标注正负样本，随后用这些合成数据通过局域对比损失(LCE)对小型模型进行对比学习微调。

Result: 在MedQuAD数据集上的实验结果表明，该方法显著提升了领域内表现，并在领域外任务上也有良好的泛化能力。

Conclusion: 通过利用LLM进行数据生成和监督而不是直接推理，既降低了计算成本，又保持了出色的重排序能力。此方案为无需人工标注下的高效文档重排序提供了有效途径。

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [22] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 该论文利用PHATE流形分析等多种方法，系统研究了中文字符嵌入的几何结构，发现字符的语义丰富性与其嵌入空间的几何复杂度相关，为语言学理论提供了新的计算支持，并建立了新的几何分析框架。


<details>
  <summary>Details</summary>
Motivation: 了解中文字符中的语义如何通过几何方式在嵌入空间中表现，以验证并扩展传统语言理论。

Method: 采用PHATE流形分析对中文字符嵌入进行系统研究，通过七种嵌入模型和八种降维方法交叉验证，并分析1000多个字符跨越12个语义领域及123个短语的结构。

Result: 发现内容词在嵌入空间中呈现簇状模式，功能词呈分支模式；语义丰富的字符显示出复杂几何结构，而结构部首则高度聚集。同时，短语的网络分析揭示了语义从基本字符向更复杂表达的系统扩展。

Conclusion: 几何复杂性与字符的语义内容相关，该研究提供了支持传统语言理论的计算证据，并提出了用于语义组织几何分析的新框架。

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [23] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 本文提出通过贝叶斯不确定性建模及风险感知优化机制，显著提升了大语言模型在高风险应用中的摘要鲁棒性和可靠性，保障了核心信息表达与信任度。


<details>
  <summary>Details</summary>
Motivation: 在信息过载和高风险决策情境下，传统自动摘要方法缺乏对不确定性和风险感知的考量，存在过度自信和关键信息遗漏的问题，难以满足高可靠性应用需求。

Method: 提出了一种融合不确定性量化与风险感知机制的大语言模型自动摘要框架。该方法在生成摘要时引入贝叶斯推断建模参数不确定性，通过预测分布熵衡量生成内容的不确定性，并联合熵正则化与风险感知损失优化，模型还引入了风险打分与调控模块。

Result: 所提方法在多组对比实验与敏感性分析中展现出优良的鲁棒性和摘要可靠性，同时保证摘要的流畅性与语义完整。

Conclusion: 文中所提出的体系系统性提升了高风险场景下自动摘要的可信度，并具备良好的可扩展性与实际应用价值。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [24] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 该文提出了一种系统解析大语言模型基准表现的分析方法，真实揭示基准测试背后的能力结构和模型短板，有助于提升评价和模型解释的精准性。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型能力的评价方式（如通过标准基准测试得分）可能无法全面反映模型的真实能力，因为这些得分常常掩盖了任务所需的多种技能，并且缺少系统化的验证手段来确保基准测试名副其实。

Method: 提出了一种称为Benchmark Profiling的诊断框架，通过融合基于梯度的重要性评分和有针对性的参数剪除，将基准测试中的表现分解为十种认知能力，量化每种能力对完成任务的影响，提出了能力影响分数（AIS）。

Result: 对三种指令微调大模型在十种常用基准上的剖析，得出四个主要发现：大多数基准涉及多项能力、同类标签数据集依赖不同能力组合、代码生成基准更需要多项能力提升且领域微调收益有限、无关能力可能负面影响表现。

Conclusion: Benchmark Profiling为模型评估、基准测试审计及可解释性提供了透明工具，并揭示了基准得分提升未必带来用户认可能力提升的原因。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [25] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: 本文提出了首个系统性数字框架，用计算社会科学手段分析和保护特勒古格律诗，通过创新算法、专家协作和文化融入，实现了高精度自动化识别，并为数字人文领域的文化传承树立了新范例。


<details>
  <summary>Details</summary>
Motivation: 特勒古诗歌（Chandassu）作为一种重要的格律诗传统，代表了几个世纪的集体文化智慧。随着这一文化遗产濒临消失，亟需借助现代数字技术保护、传承和分析。

Method: 作者提出了一个计算社会科学方法，包括社群协作数据集（4651首已注释诗歌）、专家验证的语言模式和结合文化背景的算法设计。框架包含AksharamTokenizer（韵律敏感分词）、LaghuvuGuruvu Generator（轻重音节分类）、PadyaBhedam Checker（自动化格律辨识），并用传统文学标准进行算法评估。

Result: 所提算法在Chandassu Score上达到91.73%的准确率，评估结果能够反映传统文学标准，展示了其有效性。

Conclusion: 计算社会科学方法不仅能保护濒危文化知识，还促进了文学遗产的新型集体智能。这一方法为社区驱动的文化保护和数字人文、社会计算系统的相关研究提供了参考。

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [26] [Computing Phylogenetic Diversity](https://arxiv.org/abs/2510.01849)
*Jannik Schestag*

Main category: cs.DM

TL;DR: 论文将Max-PD问题推广到更贴合现实的保护情境，分析多种问题的参数化复杂度，提出针对部分问题的高效算法，说明部分问题仍具有很高计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 在物种保护规划中，资源有限，需要优先选择能最大化生态系统发育多样性的物种组成。标准Max-PD问题及其推广更贴近生物过程和保护实际，但计算难度提升，亟须理论分析和算法改进。

Method: 论文采用参数化复杂度理论，针对不同问题设计了多种算法（如色码算法），并分析了问题的FPT、W[1]-hard和NP-hard性质。

Result: 1. 广义诺亚方舟问题（GNAP）对物种数量是W[1]-hard，但对成本及存活概率数量为XP；其特殊情形unit-cost-NAP是NP-hard。2. 时间敏感PD最大化（Time-PD）问题，提出的色码算法证明其是对多样性阈值和可接受损失FPT。3. 含依赖关系的PD优化（PDD）问题，在参数解大小加树高度时是FPT。其他推广问题也作了讨论。

Conclusion: 该论文分析了多种最大化系统发育多样性（Max-PD）问题的推广，在参数化复杂度框架下讨论了它们的算法可行性和计算难度。

Abstract: Phylogenetic Diversity(PD)is a well-regarded measure of the overall
biodiversity of a set of present-day species(taxa)that indicates its ecological
significance.In the Maximize Phylogenetic Diversity(Max-PD)problem one is asked
to find a small set of taxa in a phylogenetic tree for which this measure is
maximized.Max-PD is particularly relevant in conservation planning,where
limited resources necessitate prioritizing certain taxa to minimize
biodiversity loss.Although Max-PD can be solved in polynomial time
[Steel,SB,2005;Pardi&Goldman,PLoS,2005],its generalizations-which aim to model
biological processes and other aspects in conservation planning with greater
accuracy-often exhibit NP-hardness,making them computationally challenging.This
thesis explores a selection of these generalized problems within the framework
of parameterized complexity. In Generalized Noah's Ark Problem(GNAP),each taxon
only survives at a certain survival probability,which can be increased by
investing more money in the taxon.We show that GNAP is W[1]-hard with respect
to the number of taxa but is XP with respect to the number of different costs
and different survival probabilities. Additionally,we show that unit-cost-NAP,a
special case of GNAP,is NP-hard. In Time Sensitive Maximization of Phylogenetic
Diversity(Time-PD),different extinction times of taxa are considered after
which they can no longer be saved.For Time-PD,we present color-coding
algorithms that prove that Time-PD is fixed-parameter tractable(FPT)with
respect to the threshold of diversity and the acceptable loss of diversity. In
Optimizing PD with Dependencies(PDD),each saved taxon must be a source in the
ecological system or a predator of another saved species.These dependencies are
given in a food-web.We show that PDD is FPT when parameterized with the size of
the solution plus the height of the phylogenetic tree. Further,we consider
pa...

</details>


### [27] [Computability of extender sets in multidimensional subshifts: asymptotic growths, dynamical constraints](https://arxiv.org/abs/2401.07549)
*Antonin Callard,Léo Paviet Salomon,Pascal Vanier*

Main category: cs.DM

TL;DR: 本文研究了高维子移位空间（$oldsymbol{	ext{Z}^d}$）中的extender set的计算属性，重点分析了其增长速度（extender entropy与维数）。作者证明了sofic与effective子移位在这些参数上等价，无法用extender entropy区分两类空间，并讨论了相关的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 背景在于对于一维情形，sofic子移位的sofic性等价于extender set数量有限。之前有人猜测，extender set数量可能帮助区分高维情况下的sofic与effective子移位。本文旨在探索此分离的可能性及其计算属性。

Method: 分析了多维子移位空间的extender set的计算特性，采用了extender entropy和extender entropy 维数来量化extender set的增长，并进一步研究了这些量在不同动力学与组合约束下的计算复杂性。

Result: 结果表明，extender set的若干增长型参数（extender entropy与其维数）不能用于区分高维sofic与effective子移位，两者在这些参数上完全一致。同时分析了相关的计算复杂性。

Conclusion: 作者证明了在多维子移位空间中，sofic子移位和effective子移位具有完全相同的每个可能的extender entropy（即所有$oldsymbol{	ext{[0,+∞)}}$中的$oldsymbol{	ext{Π}_3}$-可计算实数）以及extender entropy维数。

Abstract: Subshifts are sets of colorings of $\mathbb{Z}^d$ defined by families of
forbidden patterns. In a given subshift, the extender set of a finite pattern
is the set of all its admissible completions. Since soficity of $\mathbb{Z}$
subshifts is equivalent to having a finite number of extender sets, it had been
conjectured that the number of extender sets could provide a way to separate
the classes of sofic and effective subshifts in higher dimensions. We
investigate some computational characterizations of extender sets in
multidimensional subshifts, and in particular their growth, in terms of
extender entropies (arXiv:1711.07515) and extender entropy dimensions. We prove
here that sofic and effective subshifts have the same possible extender
entropies (exactly the $\Pi_3$-computable real numbers of $[0,+\infty)$) and
extender entropy dimensions, and investigate the computational complexity of
these growth-type quantities under various dynamical and combinatorial
constraints.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [28] [MightyPPL: Verification of MITL with Past and Pnueli Modalities](https://arxiv.org/abs/2510.01490)
*Hsi-Ming Ho,Shankara Narayanan Krishna,Khushraj Madnani,Rupak Majumdar,Paritosh Pandya*

Main category: cs.FL

TL;DR: MightyPPL是一款新型工具，可将更强表达力的时序逻辑公式高效转换为时序自动机，并提供主流模型检测兼容性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有MITL验证方法仅支持有限片段或不完整验证，无法完全满足复杂时序系统的需求。

Method: 提出了MightyPPL工具，能将包含Past和Pnueli模态的MITL公式翻译为时序自动机。设计了符号化转换和对称性规约等性能优化技术。可生成对应的自动机，并与多种主流验证后端兼容。

Result: MightyPPL支持对有限与无限字上的更强表达力规范逻辑进行可满足性与模型检测，实验显示其在多种案例和配置下均有良好表现，状态空间优化效果显著。

Conclusion: MightyPPL极大扩展了可验证逻辑的表达能力，并通过多种优化提升了模型检验的效率和适应性。

Abstract: Metric Interval Temporal Logic (MITL) is a popular formalism for specifying
properties of reactive systems with timing constraints. Existing approaches to
using MITL in verification tasks, however, have notable drawbacks: they either
support only limited fragments of the logic or allow for only incomplete
verification. This paper introduces MightyPPL, a new tool for translating
formulae in Metric Interval Temporal Logic with Past and Pnueli modalities
(MITPPL) over the pointwise semantics into timed automata. MightyPPL enables
satisfiability and model checking of a much more expressive specification logic
over both finite and infinite words and incorporates a number of performance
optimisations, including a novel symbolic encoding of transitions and a
symmetry reduction technique that leads to an exponential improvement in the
number of reachable discrete states. For a given MITPPL formula, MightyPPL can
generate either a network of timed automata or a single timed automaton that is
language-equivalent and compatible with multiple verification back-ends,
including Uppaal, TChecker, and LTSmin, which supports multi-core model
checking. We evaluate the performance of the toolchain across various case
studies and configuration options.

</details>
