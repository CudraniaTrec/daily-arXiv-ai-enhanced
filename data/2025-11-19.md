<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.LO](#cs.LO) [Total: 7]
- [cs.CL](#cs.CL) [Total: 47]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: 该论文提出了针对易变测试修复的FlakyGuard方法，通过智能筛选代码上下文提升大语言模型修复效率，在工业仓库中的表现明显优于现有方法，并获得开发者一致好评。


<details>
  <summary>Details</summary>
Motivation: 现有的修复易变测试的自动化方法在工业环境下效果不佳，主要原因在于向大语言模型（LLM）提供上下文信息的方式存在问题，过少会缺少关键代码，过多则造成信息冗余。

Method: 提出了FlakyGuard方法，通过将代码视为图结构，并采用选择性图探索技术，只提取与修复相关的最关键上下文供LLM使用。

Result: 在真实工业仓库中的易变测试上评估，FlakyGuard修复了47.6%的可复现易变测试，其中51.8%的修复获得开发者认可。修复成功率较现有方法提高至少22%。开发者调查显示，所有人都认为FlakyGuard的根因解释有帮助。

Conclusion: FlakyGuard高效地修复了工业环境中的易变测试，明显优于此前方法，并能提供有用的根因解释。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [2] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 语言模型生成的代码常常与人类有风格差异。通过对比指令型、示例型和两者结合的提示，发现结合型提示能在初始和代码增强阶段都实现最好的风格控制效果。提示设计时需兼顾初始有效性和扩展约束力，单一方式难以兼顾两者。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然能生成功能正确的代码，但往往过于冗长，包含大量文档和防御性模式，这种风格与人类编程习惯有很大差异。为此，研究者希望通过不同的提示方式来实现代码风格控制。本文关注：在代码功能提升时，模型生成的风格限制是否依然有效。

Method: 设置了四种系统提示条件，采用配对的两轮协议：第一轮让模型生成中等难度Python任务的解；第二轮让模型在保持用户任务不变的前提下，根据『一般改进指令』对代码进行修订。比较了指令型、示例型及二者结合的提示方式，对160组代码样本的初始输出和功能增强后的风格控制效果进行了分析。

Result: 三种提示方式各有不同。结合型提示初始压缩最强，扩展时风格约束也最严格；指令型提示初始作用明显，扩展控制适中；示例型提示仅有微弱初始作用，扩展时几乎无约束。

Conclusion: 初始提示的风格控制力与代码增强时的风格保持能力是不同的两方面。结合使用指令和示例型提示可以实现最稳定的代码风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [3] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 本研究通过访谈揭示英国计算机系学生在编程项目中战略性和伦理性使用 ChatGPT的实际做法。他们将 ChatGPT 作为辅助工具但限制其贡献，多数主动评估其输出并拒绝未署名引用，强调隐私和技能等风险，仅少数能批判性分析 AI 生成代码。研究建议教师提供明确指导以促进学生负责任地使用此类工具。


<details>
  <summary>Details</summary>
Motivation: ChatGPT 在计算机科学领域中逐渐普及，虽有效支持编程任务，但也带来了学术诚信和过度依赖等问题。现有研究多用问卷调查，缺乏对学生实际策略和伦理认知的深入探讨。因此，该研究旨在补充定性角度，分析学生如何在项目实践中战略性和伦理性使用 ChatGPT。

Method: 采用半结构化访谈，面向英国某高校计算机科学专业学生，聚焦两大问题：学生如何战略性、伦理性地报告在项目中使用 ChatGPT？以及他们如何理解和看待 AI 在学术与职业背景中的伦理问题。

Result: 研究发现学生在使用 ChatGPT 时，学习模式已由传统“独立思考-手工编码-迭代调试”向“AI辅助构思-交互式编程-协同优化”转变。多数学生将 ChatGPT 贡献限定在 30% 左右，主要用于深入理解与辅助创意，而保留核心决策和创意任务给自己；学生会主动评估 AI 产出以防止过度依赖。但仅少数人会彻底分析 AI 生成的代码，体现批判性参与度降低。此外，学生拒绝不署名引用，关注隐私和技能退化等风险，并呼吁教师制定明确的使用规范。

Conclusion: 学生在实际项目中展现了对 ChatGPT 的审慎与自律态度，但在批判性分析方面仍有短板。研究揭示了 AI 参与学习的模式转变，并提醒教育者需制定更明确的指导方针，以保障工具的负责任和有效使用。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [4] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: 本文提出LoCoBench-Agent框架，从多轮交互、工具使用、与长上下文范围系统评估软件工程场景下LLM代理。实测显示部分模型具有强长上下文能力，理解与效率存在权衡，且工具策略影响表现。LoCoBench-Agent填补业界评测空白，对未来自主代码开发具有重要推动作用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）逐渐具备自主复杂软件开发能力，现有评测仅考察单次长上下文代码理解，无法覆盖多轮交互、工具使用和适应性推理等真实开发需求。实际工程场景下，需更全面评估LLM代理的整体表现。

Method: 提出LoCoBench-Agent评测框架，将原有的8,000长上下文场景扩展为多回合交互环境，提供8种专业化工具支持（文件操作、搜索、代码分析等），设置上下文长度从10K到1M tokens，并采用9个维度指标（理解与效率相关）系统评估代理模型的多轮交互、工具使用效率、错误恢复与架构一致性。

Result: 系统评测主流大模型代理，得出三大结论：1）代理表现出出色的长上下文鲁棒性；2）存在理解与效率的负相关权衡——探索越充分，理解越深，但效率下降；3）不同模型之间交互效率差异显著，高效模型多采用策略性的工具使用。

Conclusion: LoCoBench-Agent作为首个针对长期上下文的软件工程LLM代理评测基准，为精准测量代理能力、发现性能短板、推动自主软件开发奠定坚实基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [5] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文分析了代码检索模型在代码库演进下如何保持新鲜度，比较了全量重训、上下文注入和增量微调三类方法。实验表明，增量微调最适用于多数变动场景，结合遗忘量化和重命名敏感评估，使模型既能适应新变更又记住旧知识。


<details>
  <summary>Details</summary>
Motivation: 随着代码库的不断演进，传统模型在旧数据上训练后很快就会因代码结构、API或行为的变化而失效，尤其在代码库结构频繁变动时，如何让代码检索模型保持“新鲜度”成为亟需解决的问题。

Method: 作者将“新鲜度”建模为代码库快照之间的域漂移，并对比了三种维持模型新鲜度的方法：A. 全量重训（Full Refresh）；B. 上下文注入（In-Context Learning），即在推理时注入最近的变更（git diff或简明英文摘要）；C. 增量微调（Inc-FT），在新旧数据混合、受控比例下增量训练。还设计了重命名敏感的评估协议和遗忘探针量化模型对已删除路径的“遗漏”现象。

Result: 在Flask、SQLAlchemy、Pandas和Poetry等开源项目上，采用“新旧数据混合”的增量微调方法在新旧混合测试集上取得最优平衡；而在无法重新训练的条件下，上下文注入（利用英文摘要）可最迅速适应新代码；若对新代码高准确度有极致需求，完全重训仍有最强表现。Git diff 驱动的增量微调在重命名/删除频繁时明显优于全文件微调，而全文件方法则在功能变化频繁环境下更好。

Conclusion: 结合任务需求选择不同的模型更新策略十分必要，增量微调在大部分实际场景下表现最好，但完全重训和上下文注入也各自有用武之地。量化遗忘和准确评估新旧代码的能力，对于代码智能系统的长期维护极为关键。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [6] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 本文提出了基于大语言模型和规则增强的日志数据自动清洗框架LogPurge，大幅提升了异常检测样本集的质量和模型表现，减少了人工标注和清洗成本。


<details>
  <summary>Details</summary>
Motivation: 日志异常检测依赖于高质量的无异常日志数据用于训练深度学习模型，但现有自动数据清洗方法未充分利用日志的特性与语义，且人工标注成本高昂。

Method: 提出了LogPurge框架，结合大语言模型（LLM）和系统规则，采用两阶段过滤算法：第一阶段使用LLM与增强规则去除聚集的异常模式，第二阶段用分治策略将剩余污染区域分解为小子问题，并对每个子问题复用第一阶段流程。

Result: 在两个公开数据集和一个工业数据集上，LogPurge平均去除了98.74%的异常，同时保留了82.39%的正常样本。在F-1分数上，相较最新无监督方法，公开数据集提升了35.7%和84.11%，工业数据集提升149.72%。

Conclusion: LogPurge能够高效低成本地自动筛选清洗出高质量的正常日志序列，可显著提高日志异常检测模型的训练效果。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [7] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 定制的Scrum敏捷框架显著提升航空航天安全关键软件开发效率和质量，达成DO-178C合规，并显示出行业推广价值。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统日趋复杂，开发过程需同时兼顾敏捷性与严格的安全、认证要求。传统开发流程难以满足高安全性与灵活迭代的双重需求，促使研究者探索适用于安全关键领域的敏捷开发方法。

Method: 设计并实证验证了一套基于Scrum的敏捷框架，专门针对符合DO-178C标准的航空航天安全关键软件。通过调整Scrum角色、工件和事件，加入多学科产品所有权、双重验收标准、独立测试/文档团队以及认证联络机制。采用实证对比，两类项目分别运用定制的敏捷流程与传统瀑布模型进行评估。

Result: 采用敏捷流程的项目比传统瀑布模型项目表现更优：每个需求总人力减少76%、缺陷检测速度提升75%、缺陷解决速度提升78%、缺陷密度降低超50%，且完全符合DO-178C最高安全等级认证要求。

Conclusion: 敏捷实践与合规监管在航空航天软件开发领域可以高效协同，通过严格流程定制和主动与认证方沟通，两者可兼得。后续可通过自动化及CI/CD进一步优化，但需要关注迭代开发带来的V&V工作量增加等挑战。建议未来扩展至更广泛行业和领域进行验证。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [8] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本论文提出将运行时验证与大语言模型深度结合，通过RV提供安全守护、由LLM辅助规范获取和不确定性处理，实现更可信的自主系统，并讨论了相关挑战、未来方向及认证问题。


<details>
  <summary>Details</summary>
Motivation: 自主系统在引入学习组件和开放环境后，安全与可信保障变得更困难；现有形式化方法依赖完整模型与静态假设，有限制；而LLM虽在模式识别和自然语言转化方面突出，但缺乏形式化保证。因此，需融合不同技术扬长避短。

Method: 论文采用愿景性分析，总结现有技术现状、提出协同方式，并探讨未来研究方向与挑战。

Result: 给出RV与LLM结合的愿景图景，以及与现有方案的区别，重点提出未来研究方向、挑战及认证影响。

Conclusion: 本文主张运行时验证（RV）与大语言模型（LLM）应进行协同集成，以提升自主系统的可靠性和信任性。RV和LLM的结合可形成互补、彼此增强的模式。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [9] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出KTester，通过集成项目和测试领域知识，提升LLM自动化测试生成的正确性、可读性和可维护性，并在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）自动化单元测试生成虽然有潜力，但在实际项目中生成的测试代码在正确性和可维护性方面存在不足。研究者希望提高LLM生成测试用例的质量，以满足真实软件工程的需求。

Method: 提出了KTester框架，通过静态分析提取项目结构和使用知识，作为LLM生成测试的丰富上下文。同时，采用了测试领域知识指导的测试用例设计与方法生成分离策略，以及多视角prompt策略，引导LLM考虑多种测试启发式。所生成的测试用例采用结构化模板，以提升代码的清晰度与可维护性。

Result: 在多个开源项目上的实验表明，KTester在六项关键指标上均显著优于当前最强的LLM方法。与最强基线相比，测试执行通过率提升5.69%，代码覆盖率提升8.83%，且耗时更少、生成的测试用例数量更少。人工评估表明，KTester生成的测试在正确性、可读性和可维护性方面也获得了更高评价。

Conclusion: KTester通过融合项目特定知识和测试领域知识，提升了LLM自动化测试生成的效果，在测试质量和工程实际应用上均取得了明显进步。

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [10] [How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?](https://arxiv.org/abs/2511.14367)
*Dulaji Hidellaarachchi,Sebastian Baltes,John Grundy*

Main category: cs.SE

TL;DR: 认知能力与人格特质（尤其是尽责性与经验开放性）显著影响软件问题解决表现，建议教育和招聘中兼顾这两方面特征。


<details>
  <summary>Details</summary>
Motivation: 软件工程不仅仅是技术技能，更是受个体差异（尤其是认知能力和人格特质）深刻影响的认知活动。现有关于这些非技术因素如何影响软件问题解决的研究有限，因此本研究旨在探究二者的联合作用。

Method: 本研究招募了80名参与者（40名软件工程从业者和40名学生），通过三分钟语法推理测试和IPIP NEO 50人格测试评估认知能力和人格特质，并给出九道与编码和逻辑推理相关的问题，进行面试式问题解决测试。采用描述性和相关性分析方法考察影响因素。

Result: 从业者在语法推理准确率和整体任务表现上略优于学生。语法推理准确率与问题解决表现正相关，表现出较高认知能力者在编码与逻辑任务上表现更优。人格特质表现出系统关联：尽责性与问题解决及推理准确率关联最强，经验开放性亦呈正相关；神经质则与准确率和表现呈弱负相关。

Conclusion: 尽责性和经验开放等人格特质能够与推理能力相辅相成，提高软件问题解决能力，而较高的负性情绪（神经质）则可能在压力下影响精度。研究提示教育和业界应重视认知和人格因素，可将结构化推理任务纳入课程，并在招聘与岗位分配时关注候选人的认知和人格特征。

Abstract: Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.

</details>


### [11] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过研讨会分析了LLM在软件工程定性研究中主题分析的作用，发现其提升效率但存在偏见和透明度等方法问题，只能辅助而非替代人类分析。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地被应用于软件工程领域的定性研究，其在解释性分析（如主题分析）中的方法影响尚未充分探讨，尤其是在严谨性、透明度和研究者主动性等方面引发了诸多问题。

Method: 本研究通过一个反思性研讨会，邀请25位ISERN研究者参与，组织结构化讨论，探讨LLM协助下的开放编码、主题生成及审核，使用彩色画布记录机会、局限与建议。

Result: 参与者认为LLM有助于提升效率和可扩展性，但也警示了偏见、语境缺失、可复现性以及模型快速迭代等风险，并提出需加强提示词素养和持续人工监督。

Conclusion: LLM可以作为支持解释性分析的工具，但不可替代研究者主导的分析过程。研究推动了社区关于LLM如何负责任地提升软件工程定性研究的持续反思。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [12] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: 本文提出了FHIRconnect，一种支持openEHR与HL7 FHIR互操作的领域专用语言和开源引擎，通过三层架构实现了标准化、可重用、高效的数据双向映射，显著推进了医疗信息系统的互操作标准化进程。


<details>
  <summary>Details</summary>
Motivation: openEHR和HL7 FHIR是医疗信息化中两种重要的数据标准，由于其数据建模方式的根本差异及缺乏统一转化机制，二者之间的数据互操作一直具有极大挑战。

Method: 本研究设计了一种新颖的领域专用语言（DSL）和开源转化引擎FHIRconnect，通过三层架构，实现数据标准化与双向互操作，并支持国际标准和本地定制。

Result: 通过FHIRconnect，成功实现了24个国际archetype和15个FHIR profile在7个临床领域的映射，具有65%的映射可复用性。此外，提供了正式规范的DSL、开源执行引擎（openFHIR）和覆盖高影响力临床archetype的映射库。

Conclusion: FHIRconnect为openEHR与FHIR之间的标准化、双向数据互操提供了技术基础，推动了基于开源标准的医疗IT系统的语法和语义互操作，减少了对定制ETL方案的依赖，为社区推动的映射标准化提供了保障。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [13] [Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice](https://arxiv.org/abs/2511.14711)
*Aaliyah Chang,Mariam Guizani,Brittany Johnson*

Main category: cs.SE

TL;DR: 通过15次访谈和Gioia方法，本研究归纳了影响软件工程师教育到职业阶段动机与挑战的分类与动力模型，强调归属感障碍的持续影响，并提出优化干预建议以提升内在满足与减少障碍。


<details>
  <summary>Details</summary>
Motivation: 多年来，个人在软件工程领域的进入、坚持与发展受动机与挑战共同影响，但这些两者之间的关系，尤其是在从教育到职业实践的转换阶段，仍然缺乏深入研究。本文旨在探讨教育到职业实践过程中动机和挑战的交互作用。

Method: 研究采用了15次半结构化访谈，并应用Gioia方法（源自组织行为学的归纳性扎根理论方法）归纳出动机和挑战的分类体系，并构建了“Exposure-Pursuit-Evaluation（EPE）过程模型”。

Result: 研究发现，有影响力的早期经历会激发内在动机，而没有影响力的经历则需要外在推动力（如职业或个人目标、外部认可）；好奇心和避开其他选择是独特的教育驱动因素。归属感障碍是唯一贯穿教育和职业阶段的挑战。职业发展中的挑战（如适应企业环境）限制外在满足，而技术培训挑战、归属感障碍和动机威胁则限制内在满足。

Conclusion: 本研究提出了一个基础理论模型，揭示未被满足的动机和反复出现的挑战如何影响个体坚守、转行或离开软件工程领域。研究结果为设计提升内在满足感、减少系统性障碍的教育和实践干预措施提供了理论依据。

Abstract: Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [14] [Towards A Catalogue of Requirement Patterns for Space Robotic Missions](https://arxiv.org/abs/2511.14438)
*Mahdi Etumi,Hazel M. Taylor,Marie Farrell*

Main category: cs.LO

TL;DR: 本文探讨了现有规范模式在太空任务需求形式化中的适用性，开发了5种新模式，丰富了模式库，并通过专家评估验证其效果。


<details>
  <summary>Details</summary>
Motivation: 在安全与关键任务系统（如自主空间机器人任务）开发过程中，需求多用自然语言表达，存在歧义，难以进行形式化验证。因此，需要用可复用、基于逻辑的模板（规范模式）来支持需求的形式化定义。

Method: 作者首先回顾了现有的太空任务文献，并用NASA的FRET工具对这些任务需求进行形式化。接着，利用已有的规范模式对这些需求进行分类，探究其适用性，同时针对未被涵盖的需求，设计了5种新的规范模式及其若干变体，并请领域专家对新模式进行了评估。

Result: 实验结果显示，现有的领域无关规范模式大多可应用于太空任务中，但仍有部分需求无法被覆盖。新增5种需求规范模式和若干变体，专家评价揭示了这些模式的优势与局限性。

Conclusion: 规范模式（包括新增模式）有助于太空任务需求的形式化表达和验证，但现有体系仍需补充扩展以完全涵盖各种空间任务需求。

Abstract: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations.

</details>


### [15] [Probabilistic Verification for Modular Network-on-Chip Systems (extended version)](https://arxiv.org/abs/2511.13890)
*Nick Waddoups,Jonah Boe,Arnd Hartmanns,Prabal Basu,Sanghamitra Roy,Koushik Chakraborty,Zhen Zhang*

Main category: cs.LO

TL;DR: 本文提出一种利用Modest语言进行NoC系统化、模块化建模与统计模型检测验证电源噪声影响的方法，验证了高达8x8规模NoC下路由器和整体网络的功能及PSN相关可靠性，为NoC的早期设计优化与可靠性保障提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 在芯片网络（NoC）设计中，电源噪声（PSN）引起的不稳定性和数据传输错误会严重影响系统可靠性。传统方法难以对不同规模和实现方式的NoC进行系统性验证，因此需要一种统一且可扩展的建模与验证方法。

Method: 本研究采用Modest语言进行NoC的系统化、模块化建模，贴合数字系统的分层设计。利用Modest工具集，针对由通用模块化路由模型实例化出的多种NoC模型，进行了功能和定量正确性验证。同时，采用统计模型检测方法对高达8x8规模NoC的PSN相关性质进行了验证。

Result: 验证了通用路由器、路由器间通信及整个NoC的功能正确性，并通过统计模型检测手段确认了PSN相关性质在多种NoC规模下均能满足。

Conclusion: 采用Modest语言与工具集开展的模块化建模和验证，为复杂NoC在不同设计规模和电源噪声影响下的可靠性评估提供了一种高效且准确的定量验证途径。

Abstract: Quantitative verification can provide deep insights into reliable Network-On-Chip (NoC) designs. It is critical to understanding and mitigating operational issues caused by power supply noise (PSN) early in the design process: fluctuations in network traffic in modern NoC designs cause dramatic variations in power delivery across the network, leading to unreliability and errors in data transfers. Further complicating these challenges, NoC designs vary widely in size, usage, and implementation. This case study paper presents a principled, systematic, and modular NoC modeling approach using the Modest language that closely reflects the standard hierarchical design approach in digital systems. Using the Modest Toolset, functional and quantitative correctness was established for several NoC models, all of which were instantiated from a generic modular router model. Specifically, this work verifies the functional correctness of a generic router, inter-router communication, and the entire NoC. Statistical model checking was used to verify PSN-related properties for NoCs of size up to 8x8.

</details>


### [16] [Context-aware, Ante-hoc Explanations of Driving Behaviour](https://arxiv.org/abs/2511.14428)
*Dominik Grundt,Ishan Saxena,Malte Petersen,Bernd Westphal,Eike Möhlmann*

Main category: cs.LO

TL;DR: 本文提出一种结合交通序列图和运行时监控的自动驾驶车辆解释方法，实现了针对驾驶行为的情境感知和先验解释，提升了自动驾驶车辆的安全性和可信性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的决策过程常常不透明，导致安全性和可信性难以获得社会认可。解释系统行为能够提升安全感和信任度，但AI决策过程解释难度大。因此研究如何在设计和运行时为驾驶行为提供准确且有意义的解释。

Method: 利用一种名为Traffic Sequence Charts（交通序列图）的可视化且形式化语言来正式化解释的上下文和驾驶动作，并通过专门的运行时监控来实现情境识别和提前展示解释。该方法在模拟超车场景中进行了验证和演示。

Result: 提出的新方法结合了设计时和运行时可解释性工程，能够对（不）可预期的驾驶动作进行情境化的提前解释，经过仿真验证效果。

Conclusion: 提出了一种支持自动驾驶车辆在运行时进行情境感知、先验解释驾驶行为的方法，能够促进系统行为的可解释性和可信性，最终有助于提升公众对自动驾驶汽车的信任和接受度。

Abstract: Autonomous vehicles (AVs) must be both safe and trustworthy to gain social acceptance and become a viable option for everyday public transportation. Explanations about the system behaviour can increase safety and trust in AVs. Unfortunately, explaining the system behaviour of AI-based driving functions is particularly challenging, as decision-making processes are often opaque. The field of Explainability Engineering tackles this challenge by developing explanation models at design time. These models are designed from system design artefacts and stakeholder needs to develop correct and good explanations. To support this field, we propose an approach that enables context-aware, ante-hoc explanations of (un)expectable driving manoeuvres at runtime. The visual yet formal language Traffic Sequence Charts is used to formalise explanation contexts, as well as corresponding (un)expectable driving manoeuvres. A dedicated runtime monitoring enables context-recognition and ante-hoc presentation of explanations at runtime. In combination, we aim to support the bridging of correct and good explanations. Our method is demonstrated in a simulated overtaking.

</details>


### [17] [Abstract Scene Graphs: Formalizing and Monitoring Spatial Properties of Automated Driving Functions](https://arxiv.org/abs/2511.14430)
*Ishan Saxena,Bernd Westphal,Martin Fränzle*

Main category: cs.LO

TL;DR: 本文提出抽象场景图（ASG）方法，形式化并自动监控自动驾驶功能的空间属性，在真实交通场景中验证，提升了自动驾驶运行安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶功能在复杂且安全关键的交通场景下，需要持续且自动化地符合各种空间属性。空间属性的高度复杂性要求通过形式化方法进行建模，以支持自动化检查，保障系统安全。

Method: 基于场景图（SG）结构，提出抽象场景图（ASG）用于对ADF的空间属性进行形式化描述，并开发了相应的监控框架及判定算法，在真实交通场景中进行了验证。

Result: 证明了ASG能够有效形式化ADF的空间属性，并在真实场景中进行了应用展示；提出的框架能够在运行时自动监控ADF是否满足已经形式化的空间属性要求。

Conclusion: 通过提出抽象场景图（ASG）形式化空间属性，并在实际场景中应用，实现了对自动驾驶功能（ADF）空间属性的自动化检查和运行时监控，提升了安全性与可靠性。

Abstract: Automated Driving Functions (ADFs) need to comply with spatial properties of varied complexity while driving on public roads. Since such situations are safety-critical in nature, it is necessary to continuously check ADFs for compliance with their spatial properties. Due to their complexity, such spatial properties need to be formalized to enable their automated checking. Scene Graphs (SGs) allow for an explicit structured representation of objects present in a traffic scene and their spatial relationships to each other. In this paper, we build upon the SG construct and propose the Abstract Scene Graph (ASG) formalism to formalize spatial properties of ADFs. We show using real-world examples how spatial properties can be formalized using ASGs. Finally, we present a framework that uses ASGs to perform Runtime Monitoring of ADFs. To this end, we also show algorithmically how a spatial property formalized as an ASG can be satisfied by ADF system behaviour.

</details>


### [18] [Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains](https://arxiv.org/abs/2511.14433)
*Diana C. Benjumea,Marie Farrell,Louise A. Dennis*

Main category: cs.LO

TL;DR: Safe-ROS提出了一种将智能控制与可验证安全功能分开的机器人架构，可在核能等关键领域保障部署安全并满足严格规范，验证和实验测试证实其可靠性及扩展能力。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域部署自主机器人时，必须确保其运行效能和安全符合法规，但现有架构难以提供可验证的安全监督，因此需提出新架构来解决这一需求。

Method: Safe-ROS架构划分为两大子系统：智能控制系统负责日常操作；安全系统由可形式化验证的独立安全功能（SIFs）组成。通过在AgileX Scout Mini机器人上，在模拟和实验室环境中落地，选取具体安全需求，开发SIF为认知智能体实现障碍物接近时自动停车，进行形式化验证与集成测试。

Result: Safe-ROS以一种形式化、可验证的方式，实现了对安全需求的满足。所开发的安全智能体能实时响应潜在危险，验证和实验表明其具备良好的安全性与可拓展性，在英国核能行业等高度受规管环境下表现优异。

Conclusion: Safe-ROS架构能够为安全关键领域自动机器人提供可验证的安全保障，形成了一个可拓展和稳健的框架，可适用于更多安全需求及多种应用场景。

Abstract: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications.

</details>


### [19] [Analyzing Many Simulations of Hybrid Programs in Lince](https://arxiv.org/abs/2511.14436)
*Reydel Arrieta,José Proença,Patrick Meumeu Yomsi*

Main category: cs.LO

TL;DR: 论文通过扩展Lince工具，实现了多仿真方案及属性频率统计功能，并在自适应巡航控制系统案例中展示了其应用价值。


<details>
  <summary>Details</summary>
Motivation: 随着混合系统在重要领域（如医疗设备、基础设施和自动驾驶）应用日增，需要更强大的仿真工具进行系统属性的评估和分析。

Method: 采用在Lince工具中集成多模拟变体执行机制和属性频率生成直方图的功能，并通过自适应巡航控制系统的例子加以说明。

Result: 扩展后的Lince可以有效地执行多种仿真方案，并通过统计分析（直方图）展示目标属性的频率分布。

Conclusion: 增强后的Lince可以支持多重仿真变体运行，并以直方图形式量化属性出现的频率，在关键系统仿真分析中具有实用价值。

Abstract: Hybrid systems are increasingly used in critical applications such as medical devices, infrastructure systems, and autonomous vehicles. Lince is an academic tool for specifying and simulating such systems using a C-like language with differential equations. This paper presents recent experiments that enhance Lince with mechanisms for executing multiple simulation variants and generating histograms that quantify the frequency with which a given property holds. We illustrate our extended Lince using variations of an adaptive cruise control system.

</details>


### [20] [Redundancy rules for MaxSAT](https://arxiv.org/abs/2511.14657)
*Ilario Bonacina,Maria Luisa Bonet,Sam Buss,Massimo Lauria*

Main category: cs.LO

TL;DR: 本文系统性地构建并分析了适用于MaxSAT问题的冗余证明系统层级，其规则检验高效、易于集成于现有工具，并对相关理论问题及实际应用做了深入讨论。


<details>
  <summary>Details</summary>
Motivation: SAT中的冗余概念已显著提升了自动证明技术，且已逐步应用到MaxSAT，但目前对MaxSAT的证明复杂性及冗余规则体系仍不完善。作者希望通过系统地定义冗余证明体系，为MaxSAT领域带来更强的理论基础和实用性。

Method: 作者提出了一套基于阻塞变量和硬子句的冗余推理规则，并构建了相关的证明系统层级（如SPR、PR、SR），比较了与先前SAT和MaxSAT领域的方法，分析了可检验性、规则强度及其与标准MaxSAT分辨率证明系统的兼容性。

Result: 作者建立了结构化的MaxSAT冗余证明系统层级，所有规则均可多项式时间验证，并证明了它们更简单、更弱且便于集成。此外，给出了对弱鸽巢原理的简短证明，并讨论了各系统的强度和局限。

Conclusion: 本文通过定义一组面向MaxSAT的冗余证明系统层级，比较了它们与现有方法的优劣，并分析了其在当前求解器和证明检查器中的集成可能性。作者强调了这些系统的多样性和适用性，以及对弱鸽巢原理的应用。

Abstract: The concept of redundancy in SAT leads to more expressive and powerful proof search techniques, e.g., able to express various inprocessing techniques, and originates interesting hierarchies of proof systems [Heule et$.$al'20, Buss-Thapen'19]. Redundancy has also been integrated in MaxSAT [Ihalainen et$.$al'22, Berg et$.$al'23, Bonacina et$.$al'24].
  In this paper, we define a structured hierarchy of redundancy proof systems for MaxSAT, with the goal of studying its proof complexity. We obtain MaxSAT variants of proof systems such as SPR, PR, SR, and others, previously defined for SAT.
  All our rules are polynomially checkable, unlike [Ihalainen et$.$al'22]. Moreover, they are simpler and weaker than [Berg et$.$al'23], and possibly amenable to lower bounds.
  This work also complements the approach of [Bonacina et$.$al'24]. Their proof systems use different rule sets for soft and hard clauses, while here we propose a system using only hard clauses and blocking variables. This is easier to integrate with current solvers and proof checkers.
  We discuss the strength of the systems introduced, we show some limitations of them, and we give a short cost-SR proof that any assignment for the weak pigeonhole principle $PHP^{m}_{n}$ falsifies at least $m-n$ clauses.
  We conclude by discussing the integration of our rules with the MaxSAT resolution proof system, which is a commonly studied proof system for MaxSAT.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 本文分析了LLM文本水印技术的检测能力、语义保持性及对抗性攻击下的鲁棒性。结果显示，尽管语义能被保持，水印文本风格有明显变化，并且应对例如回译等对抗性攻击时，水印易被移除，影响检测准确性。


<details>
  <summary>Details</summary>
Motivation: 为缓解大语言模型（LLM）生成文本可能带来的危害，研究者提出了水印技术，用于在文本中嵌入可检测信号。但这种技术被广泛采用受到阻力，原因包括影响文本质量及易受对抗性攻击。

Method: 评估多种水印技术对对抗性攻击（如释义和回译攻击）的鲁棒性，并通过语言学指标比较水印文本与原始文本在质量和写作风格上的差异。

Result: 水印技术能够保持语义，但会偏离原始文本的写作风格，且容易受到对抗性攻击（尤其是回译攻击）影响，导致水印信号丢失。

Conclusion: 当前水印方法在文本检测上的鲁棒性仍有不足，易被对抗性手段规避，同时会损害文本的写作风格，妨碍其实际应用和推广。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [22] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: 提出了一种多次推理获取语义表示的新方法RT，可有效提升模型推理能力，对多任务普适，且无损性能。


<details>
  <summary>Details</summary>
Motivation: 增强文本嵌入模型的语义推理能力，解决现有模型在复杂推理任务上的短板。

Method: 通过对文本嵌入模型进行多次前向推理，获得最终的语义表示。

Result: RT在BRIGHT和PJBenchmark1等语义推理任务上取得显著提升，在C-MTEB等通用任务上表现稳定，尤其适用于解码器式文本嵌入模型。

Conclusion: RT方法能显著提升文本嵌入模型的语义推理能力，而在通用语义理解任务上保持一致性能。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [23] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 本文提出两种无需训练的翻译质量改错方法：一种用多个LLM译文+QE选优，另一种依靠LLM针对QE定位的错误小范围修改。首种方法获得WMT 2025比赛子任务第一，有效解决了自动后编辑过度修正问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自动后编辑（APE）系统虽然能改进机器翻译（MT）的质量，但存在“过度修正”问题，导致性能下降。因此，如何在提升翻译质量的同时避免过度修正，成为该领域的挑战。

Method: 提出两种训练无关的质量评估(QE)引导改错方法：第一种通过不同大语言模型（LLMs）生成多个候选译文，利用QE系统选出最高质量的结果。第二种则让LLM依据QE提供的解释，仅替换被标记为错误的子串，并用条件启发式减少编辑次数，以提升单位编辑带来的收益。

Result: 两种方法分别获得了Delta COMET分数0.0201和-0.0108。第一种方法在子任务排行榜上获得第一名。

Conclusion: 训练无关且基于QE的译文选择方法可以有效提升机器翻译输出的质量，优于传统的自动后编辑方法，能够减少不必要的修改，提高整体性能。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [24] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 本研究构建新数据集和评测体系，深入剖析大语言模型在长文本检索场景下的信息丢失现象与性能变化，总结缓解手段的优劣，强调任务需求与选用技术需深入结合。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLMs）在长文本语境下检索能力下降（'lost-in-the-middle'现象），特别是在实际检索应用中带来的挑战。

Method: 提出GM-Extract基准数据集，用于评估LLM在控制变量检索方面的表现；设计两种指标衡量空间与语义检索能力，并对不同参数规模的模型进行系统评测；同时分析缓解长文本丢失信息方法（黑箱与白箱），并在基准测试中应用。

Result: 数据展示检索表现随着数据呈现方式而显著变化，但未始终观察到明确的U型曲线；模型表现可与perplexity分数相关联。不同缓解方法效果复杂，有些情况下能提升表现，有些场景反而带来负面影响。

Conclusion: LLMs在面临长文本检索任务时，如何呈现语境与选用缓解方法将深刻影响性能。不同技术策略需结合具体场景灵活应用，单一方法难以一劳永逸。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [25] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: 本文提出了一种借助大模型解析电商搜索最高级查询意图并高效迁移至轻量模型的方法，大幅提升检索性能并兼顾部署效率。


<details>
  <summary>Details</summary>
Motivation: 带有最高级词汇（如“最受欢迎”、“最好”）的查询涉及多维度比较，需要深入的语言理解和领域知识，传统系统难以精准解析用户隐含意图且实际部署LLM效率低下。

Method: 提出了一个框架，将查询分解为属性-值提示，同时进行检索，结构化表达用户隐含意图，并将超级语义解释迁移至轻量模型以解决直接使用LLM重排序的高延迟问题。

Result: 在Mean Average Precision（MAP）上比分基线提升10.9点，Mean Reciprocal Rank（MRR）提升5.9点，实现了性能与效率的平衡。

Conclusion: 本文提出的方法显著提升了电商查询中带有最高级表达的搜索和排序性能，并实现了高效的部署，提供了将复杂语义解析迁移到轻量模型的新思路。

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [26] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: 本文提出MoRA-RAG结构化灾后调查报告并提升多灾害推理准确率，优于现有LLM和RAG系统，为灾害领域知识获取设立新标杆。


<details>
  <summary>Details</summary>
Motivation: 灾后现场调查报告包含多灾害相互作用的重要证据，但因叙述方式不结构化，难以系统地进行知识传递。本研究旨在解决报告信息结构化和可靠性不足的问题。

Method: 提出了Mixture-of-Retrieval Agentic RAG (MoRA-RAG)框架，通过知识检索和代理分块机制，将调查报告转化为结构化基础。此外，框架设计了查询动态路由与证据核查闭环，确保信息完整性和检索准确性。

Result: 基于GEER全球90个灾害事件，开发了HazardRecQA数据集。MoRA-RAG在多灾害推理任务中准确率高达94.5%，比零样本LLM提升30%，比最新RAG系统提升10%，显著减少了幻觉现象，并让开源LLM接近专有模型表现。

Conclusion: MoRA-RAG建立了把灾后报告转化为可操作、可信灾害韧性情报的新范式，提升了多重灾害知识获取与利用能力。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [27] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: 本文提出了HiEAG框架，通过多模态大模型和分层证据增强，提升了图片-文本对误导信息检测的准确率，并在多个数据集上取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图片-文本对的误导信息检测主要侧重于内部一致性检查，忽视了与外部证据一致性的重要性。如何有效地利用外部知识提升检测准确性，是该领域亟需解决的问题。

Method: 提出了HiEAG（分层证据增强生成框架），利用多模态大语言模型，将外部一致性检测分解为检索、重排序和重写三个模块。具体包括自动证据选择提示（AESP）用于证据重排序，以及自动证据生成提示（AEGP）提升任务适配性，通过指令微调增强模型能力。

Result: 在多个基准数据集上的实验表明，本文提出的HiEAG模型在全体样本准确率上超越了以往的最新方法（SOTA）。

Conclusion: HiEAG创新性地结合了多模态大模型知识，引入分层证据增强，显著提升了多模态背景下误导信息检测的性能，并具备良好的判据解释能力。

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [28] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

TL;DR: 本文通过数据集均衡和多特征融合模型，提高了多标签情感分类在28类情感上的准确性，实验显示综合性能指标均有大幅提升，尤其提升了对稀有情感的识别能力。


<details>
  <summary>Details</summary>
Motivation: 多标签情感分类在自然语言处理中的重要性日益提升，但现有数据集如GoEmotions存在严重的类别不均衡问题，导致模型在识别稀有情感时效果不佳。本文旨在解决该类别不均衡难题。

Method: 1. 构建平衡的多标签情感数据集：整合原始GoEmotions、用RoBERTa模型自动标注的Sentiment140样本，以及GPT-4 mini生成并人工校验的文本。2. 平衡处理确保28个情感类别分布均匀。3. 提出融合多特征的分类模型（FastText预训练词向量+卷积层提取局部特征+双向LSTM学习上下文+注意力机制突出情感词），输出层用sigmoid作多标签预测，并采用混合精度训练提升效率。

Result: 实验结果表明：基于均衡数据集训练的模型，在准确率、精确率、召回率、F1值及AUC等多项指标上均有显著提升，相比使用不均衡数据集表现更佳。

Conclusion: 数据集均衡和模型融合多特征提取方法可有效提升多标签情感分类的性能，尤其对低频情感类别识别能力明显增强。

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [29] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: 论文发现现有视觉-语言推理模型的安全对齐易被突破，提出“隐匿微调”攻击方法，以低成本显著提升攻击成功率的同时保留模型原有功能，警示社区加强相关安全防护措施。


<details>
  <summary>Details</summary>
Motivation: 带有推理能力的视觉-语言模型（RVLMs）虽然经过安全对齐处理以防止产生有害行为，但其公开的链式思考（CoT）痕迹为攻击者提供了新的攻击面。论文旨在揭示当前安全对齐机制的脆弱性。

Method: 提出了一种名为“隐匿微调”（Stealth Fine-Tuning）的新攻击方法。该方法包括基于片段级干预诱导出有害推理轨迹，并将模型自生的输出作为微调监督数据。同时设计了回合加权损失，实现轻量化、分布一致的微调。

Result: 只需499组样本、单张A100卡3小时训练，通过隐匿微调，能使ASR（攻击成功率）相比IDEATOR提升了38.52％，且模型推理能力保持不变，分布未发生明显偏移。在AdvBench以及多个通用基准数据集上实验，均显示该方法可以低成本、高效率突破安全防御。

Conclusion: 安全对齐后的RVLMs并不牢固，隐匿微调方法可以轻易绕过现有防护并引发有害行为，且不影响模型正常推理能力。此类方法需引起社区关注和警惕。

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [30] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 通过生成覆盖稀有ICD编码的高质量合成临床笔记，优化模型训练分布，使宏F1提升，增强了长尾编码的预测公平性，有一定效益但提升有限。


<details>
  <summary>Details</summary>
Motivation: 针对ICD自动编码任务中长尾分布问题，尤其是数据集中数量极少甚至为零的稀有编码，导致模型宏观预测能力低，亟需通过数据增强改善预测公平性和整体性能。

Method: 构建基于真实共现模式、ICD描述、同义词、分类法及相似临床笔记的结构化提示，生成9万条涵盖7,902个ICD编码的合成出院摘要。用新数据微调两个现有Transformer模型（PLM-ICD与GKI-ICD），并与原始数据集对比测试。

Result: 扩展数据集后，模型宏F1有小幅提升，微F1得分保持高水平，整体优于前沿方法，显示合成数据可有效缓解长尾ICD编码的问题。

Conclusion: 利用高质量的合成出院总结扩充稀有ICD编码的训练样本，能够改善模型对长尾ICD编码的预测均衡性，提升宏F1分数，而微F1得分则保持强劲，整体表现超越以往方法。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [31] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: 文章提出HyperABSA动态超图框架，自适应建模文本中方面与观点间复杂关系，显著提升了短文本情感分析的效果，优于现有图方法，且参数更高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法只能处理成对依赖关系，需要构建多个图融合不同视图，带来冗余、参数开销和错误传播，影响了模型在短文本和低资源环境下的健壮性。

Method: 提出了HyperABSA框架，通过样本自适应的层次聚类算法动态生成超图结构，其中创新性地引入了加速-回退层次聚类策略以自适应地确定聚类粒度，从而高效建模方面与观点间复杂关系。

Result: 在Lap14、Rest14、MAMS三个基准数据集的实验中，所提方法较强基线取得一致提升，特别是在结合RoBERTa时表现显著优异。

Conclusion: 动态超图构建方法在基于方面的情感分析（ABSA）中表现优异，是现有图方法的高效、有力替代方案，并有望推广到其他短文本NLP任务。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [32] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 本文提出一种结合Transformer关系抽取与知识图谱的方法，用于自动回答填空选择题并追溯答题过程。该方法通过将句子转换为关系图，并与事实知识图谱匹配验证，实验准确率达70%，题目类别对效果影响显著。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱因构建成本高，一直作为静态数据库应用。近期Transformer关系抽取方法能够动态从自然语言生成KG，使KG可用于表达文本语义。本文利用此特性，结合KG和关系抽取，提出可追溯的选择题自动答题方法。

Method: 首先利用Transformer关系抽取方法，将输入句子转化为关系图。然后根据封闭世界假设，将生成的KG与事实正确的知识图谱进行匹配与验证，以衡量句子的真实性并回答填空选择题。

Result: 实验显示，该方法能够正确回答约70%的选择题，并保证答题过程可追溯。同时发现题目类别对准确率有显著影响。

Conclusion: 结合Transformer关系抽取与知识图谱匹配，能够提升选择题自动答题系统的准确性与可追溯性，但准确率受题目类别影响较大。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [33] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出选择性弱到强泛化方法，通过判断强模型是否能自答来选择使用弱标签或自生成标签，并优化弱标签以提升对齐质量，在多个基准上超过现有方法，对超级模型对齐具有启示意义。


<details>
  <summary>Details</summary>
Motivation: 随着未来超级人工智能模型的能力超过人类，人类只能对其进行弱监督，而高质量对齐数据的缺乏成为一大难题。以往从弱到强泛化(W2SG)的方法，固守弱监督，导致标签噪音影响模型稳健性。

Method: 本文提出了一种选择性W2SG框架，训练二分类器P(IK)辨别强模型能否自答的问题，并用其自标注结果对模型对齐，当无法信任自答时再用弱监督标签。同时，利用图平滑方法对弱标签进一步优化。

Result: 在三个基准测试上的实验显示，该方法性能显著优于主流方法。同时分析表明，P(IK)分类器具有任务及难度的广泛泛化能力。

Conclusion: 选择性W2SG框架有助于提升超级模型对齐过程的稳健性与泛化能力，避免不必要的弱标签干扰，为未来超级AI对齐提供了更优路径。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [34] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 本研究提出利用符号语言学知识的方法，首次详细追踪符号性词元导致LLMs幻觉的内部过程，发现幻觉的来源在于模型对符号语义的处理失败，为幻觉机制研究提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 目前大模型（LLMs）容易在涉及否定、数词、修饰语、专有名词等符号性语言成分时出现幻觉，但这些现象的发生机制尚不清楚。已有的幻觉定位方法没有将符号性语言知识纳入分析，忽略了其在幻觉中的作用。

Method: 提出首个利用符号语言学及语义知识的定位框架，跟踪符号性触发因素在大模型各层内导致幻觉的过程。通过分析5个模型在HaluEval和TruthfulQA数据集上的表现，研究不同符号性词元的注意力变化及其关系。

Result: 通过该符号性知识框架，发现符号性词元（如否定）在模型浅层时注意力方差急剧恶化，导致语义处理从一开始就崩溃。即使模型规模增大，幻觉率依然维持在较高水平（Gemma模型变体为78.3%-83.7%），且深层对符号性语义触发词的注意力大幅下降。

Conclusion: 幻觉本质是一种符号语言处理失败而非一般的生成问题。符号语义知识对于理解和定位大模型幻觉机制至关重要。

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [35] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: 本文提出了DeepEL框架，将LLM全面应用于实体链接全流程，并结合自校验机制，实验证明在多个数据集上均显著优于现有方法，实现了更高的准确率和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的实体链接方法通常只在任务的部分阶段使用大语言模型（LLM），没有充分利用LLM在整个流程中的潜力，同时仅孤立地对实体进行消歧，导致性能提升有限。作者希望通过更深度和全面地整合LLM，突破这些瓶颈。

Method: 提出了DeepEL框架，将LLM融入实体链接任务的每个阶段，并创新性地引入了自校验机制，利用全局上下文信息让LLM自我修正预测结果，更好地识别同句内实体之间的关联。

Result: 在十个实体链接基准数据集上实证评估，DeepEL总体F1分数平均提升2.6%，在出域数据集上提升达4%，显著优于现有最先进方法。

Conclusion: 深度整合LLM并利用自校验机制能显著提升实体链接任务的准确性和泛化能力，推动了该领域的技术进步。

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [36] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: 该论文提出首个阿拉伯语多模型集成语法纠错方法ArbESC+，融合多个神经模型并利用支持技术提升纠错效果，在多个测试集中显著优于单模型，为阿拉伯语文本处理提供了更有效工具。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语因其复杂的形态和句法结构，在语法纠错（GEC）任务中面临更大挑战。现有方法多为单模型，未充分利用多系统集成的潜力。

Method: 提出多系统集成方法ArbESC+，收集多个模型的纠错建议，并将建议转化为数值特征，经分类器筛选决策，结合支持技术过滤重叠纠错并评估决策可靠性。集成的模型包括AraT5、ByT5、mT5、AraBART、AraBART+Morph+GEC、文本编辑系统。

Result: 集成多模型后在QALB-14测试集上F0.5达到82.63%，QALB-15 L1数据达到84.64%，QALB-15 L2数据达到65.55%，均优于单模型结果。

Conclusion: 首次在阿拉伯语文本处理中采用多系统语法纠错集成方法，显著提升纠错性能，对未来阿拉伯语文本工具研究具有实用价值。

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [37] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: 本文提出了一套面向音乐领域的大型语言模型的数据构建及训练方法，包含40B标记的高质量音乐语料库，创新的数据筛选和训练优化机制，并设计了音乐问答基准评测，有效提升模型在音乐专业任务的表现，并为未来领域模型开发提供可复用方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用任务上表现强劲，但在音乐等专业领域受限，原因在于语料库的规模、纯度和数据与训练目标的匹配度不足，特别是在音乐娱乐领域。这促使作者探讨如何构建更适合音乐领域的语料与训练方法。

Method: 构建了包含40B token的音乐相关自然语言语料库，数据来源包括开源及内部数据，通过轻量分类器筛选和加权专业领域文本，实施多阶段清洗、去重和隐私保护。整合多来源音乐文本及元数据，形成结构化领域知识。同时提出基于参考模型的token级柔性打分进行质量控制，采用统一损失比准则用于数据筛选和动态训练权重调整，从而优化任务信号并降低噪声影响。研发MusicSimpleQA基准，设计自动化评测方法，并系统性比较不同数据组合。

Result: 提出了面向音乐领域的大型、纯净且结构化语料库和数据流水线，以及参考模型驱动的数据质量控制与训练优化方法。开发了基于自动化评分的音乐问答评测基准，并展示了数据构成对模型效果的明显影响，提高了音乐专业领域LLMs的预训练与对齐效果。

Conclusion: 该研究推动了音乐领域专用大型模型的建设，通过高质量语料库和优化的训练目标，大幅提升了模型在音乐专业任务的表现，为领域LLMs开发提供了可扩展的数据-训练框架和可复用的评测工具。

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [38] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: 该论文提出了一种融导演-演员交互理念的自动电影配音方法——Authentic-Dubber，利用大模型、多模态检索和渐进式语音生成技术，在情感表达和仿真真实配音流程方面均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自动电影配音方法过于简化，仅模拟演员直接配音的流程，忽略了真实配音工作中导演与演员动态互动、导演帮助演员把握情感等关键环节。

Method: 提出了Retrieve-Augmented Director-Actor Interaction Learning框架（Authentic-Dubber），包含三个创新机制：1）构建多模态参考片段库，融合大语言模型深度理解情感表达；2）通过情感相似性驱动的检索增强策略，检索与目标视频相关性最高的多模态信息；3）设计渐进式图结构语音生成方法，逐步融合检索到的情感知识，模拟演员最终配音流程。

Result: 在V2C Animation基准数据集上，通过主观与客观评测，验证了该方法在配音情感表现力上的全面提升。

Conclusion: Authentic-Dubber框架能够真实还原电影配音流程，并有效提升配音情感表达效果，对自动配音系统的发展具有重要推动作用。

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [39] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: 提出并公开了首个面向非洲多口音多领域的语音识别综合评测套件，比对多类别模型优劣与局限，为实际部署和模型选择提供参考，促进语音AI在非洲公平普及。


<details>
  <summary>Details</summary>
Motivation: 当前语音AI取得了很大发展，但缺乏针对非洲地区、涵盖多种非洲英语口音和特定应用场景的公开评测基准。非洲语言多样性导致主流语音模型难以适应实际需求。

Method: 提出了AfriSpeech-MultiBench，是首个覆盖10余国、100多种非洲英语口音和7大应用领域（金融、法律、医疗、对话、呼叫中心、命名实体和幻觉鲁棒性）的评测套件。对开源、闭源、单模态ASR和多模态LLM等多种语音识别系统，在自发与非自发对话场景下进行系统性评测。

Result: 开源ASR模型在自发对话表现优异但在嘈杂、非母语场景下降显著；多模态LLM对口音更鲁棒，但领域内命名实体表现较弱；闭源模型在干净语音上最准确，但不同国家和领域间差异较大。针对非洲英语微调的模型延迟更低且准确率媲美主流模型。大部分SOTA模型在幻觉问题上仍有待改进。

Conclusion: 通过公开这一首个面向非洲多口音多场景的评测基准，为相关技术在非洲本土的适用性甄选提供依据，推动包容性语音AI的落地，惠及欠服务社区。

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [40] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: 提出了一种熵引导训练方法，有效压缩大型推理模型推理过程长度至原来的20%，准确率持平或更优，兼顾效率与效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现优异，但由于其推理过程过长，计算成本高，实际部署困难。现有压缩方法在训练过程中存在熵冲突，限制了模型进一步压缩和性能提升。

Method: 本文提出了一种熵引导的训练框架：当推理熵降低时，通过鼓励简洁思路，引导模型高效推理；当熵升高时，在保持紧凑推理的前提下加强探索性，提高模型鲁棒性。分析发现熵冲突源于部分高熵词（如逻辑连接词）在性能目标下被鼓励出现，而在压缩目标下被压制，形成冲突。

Result: 实验证明，所提方法在六个数学基准上将推理长度压缩到原来的20%，同时准确率能够保持甚至超越基线方法。

Conclusion: 解决了推理模型推理过程过长及熵冲突的问题，实现了推理高压缩和高准确率的兼得。

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [41] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文发现让LLM输出概率分布型置信度可以促进更全面推理，比只给单个答案更可靠透明，且其推理方式与人类更一致。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在生成答案时，如何衡量其回答的可靠性变得越来越重要。目前的研究主要关注如何让模型生成可表达置信度的文本输出，类似通过链式思考（Chain-of-Thought）提供更逻辑化、透明化的置信度估算。然而，关于不同推理策略如何影响置信度估算仍未被充分研究。

Method: 提出了一种让LLM直接预测概率分布的口头化输出方法，从而促使模型在置信度估算过程中进行更深入的推理。具体做法要求模型在答案空间内考虑所有可能候选，并为每一项分配合适的置信分数以满足概率分布的特性。

Result: 该方法在多种模型和任务（包括已知和未知答案空间）上均展现出明显优势。即使经过强化学习训练后，这种优势依然显著。同时分析指出，该方法下模型的推理模式与人类预期高度一致。

Conclusion: 通过预测概率分布实现口头化置信度估计可以有效促进LLM更深入推理，从而获得更合理、透明且符合人类直觉的置信度评分，且在多模型、多任务中表现出普适优势。

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [42] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出了首个全面评估阿拉伯语LLM语言能力的基准（AraLingBench），发现现有模型多数依赖记忆与模式识别，缺乏深层语言理解，并提供了新的开发与评价框架。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在阿拉伯语领域的真实语言能力尚未被全面评估，以往知识型基准未能反映深层次语言理解。

Method: 设计了一个覆盖语法、形态、拼写、阅读理解和句法五个核心领域的专家多项选择题问卷（共150题），并对35个阿拉伯语和双语LLM进行了系统性测评。

Result: 现有模型在表层能力如模式识别、记忆方面表现优异，但在语法和句法推理等深层理解上存在明显不足，无法实现真正的语言掌握。

Conclusion: AraLingBench揭示出知识型基准成绩与真实语言理解脱钩的现象，并为阿拉伯语大模型的发展提供了针对根本语言技能的诊断工具。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [43] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: 提出ConInstruct数据集，测试LLM应对指令冲突的能力。发现多数LLM能检测冲突，却很少主动提醒用户，未来需改善交互体验。


<details>
  <summary>Details</summary>
Motivation: 目前大多数大型语言模型（LLM）的评估工作主要关注模型对用户指令的遵循程度，但现实中复杂指令常包含互相冲突的约束条件，LLM在此类情境下的表现尚未被深入研究。作者希望填补该领域的研究空白。

Method: 提出ConInstruct基准数据集，专门用于评估LLM检测用户指令中的冲突并解决冲突的能力。通过该数据集对不同LLM（包括开源与闭源模型）的冲突检测和解决行为进行实验评估与分析。

Result: (1) 大多数闭源LLM表现出较强的冲突检测能力，开源模型中只有DeepSeek-R1表现同样优秀。DeepSeek-R1和Claude-4.5-Sonnet分别以91.5%和87.3%的平均F1分数取得前两名。(2) 尽管许多LLM具备冲突检测能力，但在遇到冲突时，极少数明确告知用户或请求澄清。

Conclusion: 当前LLM虽然在冲突检测方面取得较好成绩，但在用户交互方面存在明显短板，即缺乏对冲突的主动提示与澄清请求，未来需要加强此方向的研究和提升。

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [44] [The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models](https://arxiv.org/abs/2511.14365)
*Prathamesh Kalamkar,Ned Letcher,Meissane Chami,Sahger Lad,Shayan Mohanty,Prasanna Pendse*

Main category: cs.CL

TL;DR: 本文提出通过扩展词汇表和继续预训练，使大语言模型更好地处理化学领域的信息，突破了分词瓶颈，并在多项化学任务上取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的分词器主要针对通用文本，导致如SMILES等化学表示被无意义地拆分，降低了模型理解和处理化学数据的能力。

Method: 将化学相关的标记有针对性地加入到预训练大语言模型的词汇表中，并在化学领域文本上进行继续预训练，以统一自然语言和分子结构的建模。

Result: 实验证明，该词汇扩展及专门预训练的方法在多种下游化学任务中表现优越。

Conclusion: 通过扩展预训练大语言模型的词表，并针对化学领域进行持续预训练，可以有效缓解化学符号表示的“分词瓶颈”，提升模型在化学任务中的表现。

Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.

</details>


### [45] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: 提出ATLAS新评测集，解决现有LLM评测难度低和学科单一问题，以原创高难题、深入答案和专家质控，初步实验证明能更好区分模型科学推理能力，有望成为AGI进展的权威参考标准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在许多既有基准上的表现已趋于饱和，难以有效区分最前沿的模型。同时，现有高难度基准普遍存在学科范围狭窄、答案形式过于简单且易受数据污染等问题，与真实科学研究存在差距。作者提出ATLAS，旨在解决这些基准的局限性，为AGI相关模型提供更科学、更严谨的测试环境。

Method: 作者开发了ATLAS，一个规模大、难度高、跨学科的评测集，包括约800道由领域专家（博士及以上）原创或深度改编问题，涵盖数学、物理、化学、生物、计算机、地球科学和材料科学七大领域。其特点为：高度原创与抗污染、跨学科知识整合、高保真复杂开放式答案、多轮专家评审与对抗性测试确保问题质量。并提出由LLM评委自动评价复杂答案的新范式。

Result: 作者的初步实验表明：ATLAS能有效区分主流LLM在高级科学推理能力上的差异。此外，该套件计划持续开放与社区共建，以期成为测量通用人工智能进展的可靠工具。

Conclusion: ATLAS拓宽了LLM科学评测的疆界，通过高质量、跨学科、复杂答案设计及严格质控，显著提升了评测的区分力和真实性，对推动AGI长期发展具有重要意义。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [46] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文发现现有方法未能充分解决多token标签长度偏差问题，提出NCC方法有效提升LLMs在多类别任务中的准确性和可靠性，特别是面对自然多token标签真实场景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在零样本和少样本学习中表现强大，但在对候选选项进行预测时容易受到标签偏差的影响，特别是多token标签导致的长度偏差，现有的方法尚未充分解决该问题。

Method: 提出了一种新的标准化上下文校准（NCC）方法，对完整标签级别进行归一化和校准，从而全面缓解标签长度带来的偏差。

Result: NCC在多个数据集和模型上，F1分数提高最多10%，且能有效扩展到多项选择等更广泛任务。与上下文学习结合时，NCC对少样本选择的不敏感性更高，所需示例更少，置信估计更加可靠。

Conclusion: 缓解多token标签的长度偏差对提升大语言模型，尤其在真实应用场景中的表现和鲁棒性起到关键作用。

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [47] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 作者针对教育大模型面临越狱与微调攻击的安全挑战，提出了EduHarm基准与三阶段屏蔽框架TSSF。实验显示该方法能有效防御有害输入，并在不牺牲正常教育功能的前提下提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）已广泛应用于教育场景，但是仍易受到越狱（jailbreak）和微调（fine-tuning）攻击，导致安全性下降及有害内容输出。而以往研究多数关注于通用安全评测，较少涉及教育场景的特殊安全需求，因此亟需面向教育的系统性安全评估与防护方案。

Method: 本文构建了EduHarm基准集，涵盖五类代表性教育场景的安全-不安全指令对，用于系统化评价教育LLMs的安全性。同时，提出了三阶段屏蔽框架（TSSF）：1）安全感知注意力重定向，对关键不安全词汇重新分配注意力，以区分安全与不安全输入；2）分层安全判断，跨多个模型层聚合安全线索，识别有害特征；3）防御驱动的双路由，将请求分为安全与不安全类别，分别进行常规处理和受控回复。

Result: 实验覆盖8种越狱攻击方法，结果表明TSSF能显著增强模型安全性，避免对正常请求的过度拒绝。同时，在3个微调攻击数据集上，TSSF可有效抵御有害查询且基本不损失对正常请求的服务能力。

Conclusion: 本文提出的TSSF框架在提升教育大模型安全性的同时，保持对良性请求的实用性，为教育场景下LLMs安全防护提供了创新解决方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [48] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4是覆盖全国的云端医疗AI评测平台，设有多专科、多模态、多agent专项，任务与中国临床标准对齐。测评发现基础LLM和多模态模型在推理及安全性方面仍存在较大短板，安全得分较低，而以agent形式“治理编排”后性能和安全性显著提升，最高达85.3/100和88.9/100分。平台为医院及监管部门提供针对医疗AI的实用评审依据。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大模型、多模态模型及智能体（agent）快速发展，亟需能够反映真实临床流程与安全性约束的评测框架。现有评测工具难以全面覆盖多专科、多模态、多智能体场景。

Method: 提出MedBench v4——全国范围、云端部署的医疗AI评测平台，涵盖70多万由专家编写的任务，覆盖24个主要和91个次要专科，分别设有大模型、多模态模型及智能体评测专场。任务经多阶段、多人次临床专家评审，开放式回答由与人类评分校准的大模型判分。评测范围涵盖LLM、多模态模型及agent，且任务设计对齐中国临床指南和政策要求。

Result: 基础LLM总体平均分为54.1/100（最高Claude Sonnet 4.5为62.5/100），但安全伦理得分较低（18.4/100）；多模态模型整体表现更弱（均分47.5/100，最高GPT-5为54.9/100），感知能力尚可但跨模态推理弱；以同样模型为基础打造的Agent显著提升端到端表现（均分79.8/100， Claude Sonnet 4.5 agent最高85.3/100，总体和安全性均显著提升）。

Conclusion: MedBench v4揭示了基础多模态模型在推理和安全性方面的不足，但“治理增强型”agent可显著提升临床可用性和安全性。平台任务对齐中国标准和政策，为医院、开发者及政策制定者提供审计医疗AI的实用参考。

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [49] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: 本文提出Tell Me系统，结合大语言模型和智能代理，为用户与研究者提供个性化对话、数据增强和动态自我关怀服务，拓展了心理健康AI的场景与资源，具有较强实际与研究价值。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康支持存在门槛高、专业数据缺乏和工具静态等问题，亟需基于AI的个性化、动态支持，以及如何利用大模型技术辅助情感处理、数据扩充和个性化建议。

Method: 构建了Tell Me系统，包括：结合检索增强生成（RAG）的知识型对话助手、合成的客户-治疗师对话生成器（条件化生成，用于数据增强和语言研究），以及利用CrewAI的智能代理团队为用户制定自我关怀计划和冥想音频。通过系统架构设计、功能演示，并采用自动化LLM评估与用户调研相结合的方法对对话助手实际应用效果进行评估。

Result: Tell Me系统可提供可访问、上下文相关、个性化的心理健康支持。RAG助手在选定场景下通过自动与人工评价体现出良好效果。客户-治疗师对话生成方式缓解了数据稀缺困境，自我关怀方案则实现了动态个性化辅助，提升心理健康自助资源的可获得性和实用性。

Conclusion: Tell Me系统有效降低了心理健康支持门槛，为AI在人类情感支持、数据增强和自我关怀等领域提供新思路，促进了NLP与心理健康专业的跨学科结合，助力负责任的“人-机”互动创新。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [50] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: 强化学习在LLM智能体领域应用尚不成熟，本文系统梳理相关方法、扩展MDP框架，并提出易用的Agent-R1训练框架，在多跳问答任务中初步验证了其有效性，推进了RL在LLM Agent领域的发展。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在构建能够主动与环境互动，并解决复杂问题的智能体（Agent）方面的应用日益增多，强化学习（RL）被认为是训练此类智能体的关键技术。然而，现有关于将RL有效应用于LLM Agent方面的研究还处于早期阶段，面临诸多挑战，特别是缺乏专门为LLM Agent背景量身定制的RL方法，以及灵活、易扩展的训练框架。

Method: （1）系统性地扩展马尔可夫决策过程（MDP）框架，全面定义LLM Agent的关键组成部分；（2）提出Agent-R1框架，该框架具有模块化、灵活和易用的特点，适用于各种任务场景和交互环境。

Result: 通过在Multihop QA基准任务上的实验，初步验证了所提出方法和框架的有效性。

Conclusion: 本文明确了LLM Agent相关的强化学习方法，提出了一套可扩展的训练框架，并通过实验初步证明其效果，有助于推动该领域的进一步发展。

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [51] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: 本文提出了LiveRAG基准数据集，用于系统评估RAG问答系统，数据集包含多样合成问答、真实答案和难度评分，可帮助提升RAG研究和问答系统性能。


<details>
  <summary>Details</summary>
Motivation: 鉴于RAG在生成式AI中日益重要，迫切需要系统性评估其有效性。

Method: 提出LiveRAG基准数据集，包含895个合成问答，并结合了真实答案、支持性论据，以及基于项目反应理论模型的题目难度与区分度评分。

Result: 分析显示LiveRAG问题多样性高，题目难度分布广，有助于区分系统能力。

Conclusion: LiveRAG基准将推动RAG领域研究，促进系统化评估和更强健的问答系统开发。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [52] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 本文提出新的主张集合对齐和评价框架，发现现有方法用于文件级主张抽取有明显不足，呼吁发展更完善的评价方法。


<details>
  <summary>Details</summary>
Motivation: 文件级主张抽取（claim extraction）是事实核查领域的难题，但对抽取主张的评价方法关注较少。现有评价方法不能充分反映模型性能或主张的本质特征，因此亟需更科学的评价机制。

Method: 探索主张集合之间的对齐和相似度度量；提出对齐评分方法，用于比较模型抽取与人工标注主张集合，并用于评估模型抽取性能和标注者间的一致性。实验基于从捷克、斯洛伐克新闻评论中收集的新数据集。

Result: 实验证明，在非正式、强地域性、语言细微差别多的文本环境下，现有的主张抽取评价方法存在明显局限，难以满足语义相似度、主张原子性、可查证性及去语境化等多维度需求。

Conclusion: 当前主张抽取的评价方法在文件级任务中表现不佳，需要更先进的方法来正确度量主张间的语义及其核心特性。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [53] [Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages](https://arxiv.org/abs/2511.14598)
*Noam Dahan,Omer Kidron,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 该论文提出了一种利用历史报纸头版导读自动收集摘要数据的方法，在多种语言中有效，并首次构建了希伯来语多文档摘要数据集。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言中高质量摘要数据稀缺，而历史报纸所附的编辑摘要是未被利用的自然数据来源。

Method: 通过收集历史报纸上的头版导读，将编辑对长篇文章的摘要作为自然标注数据。设计了一个自动化流程，能够适应不同资源水平的语言用于数据收集。

Result: 方法在七种语言中均适用，支持多文档摘要，最终建立了希伯来语HEBTEASESUM数据集，为该语言首次提供多文档摘要资源。

Conclusion: 提出了一种新的基于头版导读自动收集多文档摘要数据的方法，并成功在希伯来语报纸上构建了第一个多文档摘要数据集。

Abstract: High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.

</details>


### [54] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 本研究利用EHR数据和多状态建模动态刻画AKI患者进展至CKD的过程，识别了高危轨迹与多种影响因素，有助于精准早筛和风险分层。


<details>
  <summary>Details</summary>
Motivation: 急性肾损伤（AKI）患者易进展为慢性肾病（CKD），但如何准确识别高危人群存在挑战。针对这一临床痛点，本文旨在利用电子健康记录（EHR）动态追踪AKI患者，刻画其向CKD转变的过程。

Method: 通过纵向医疗代码和肌酐测定，建立患者向量，并采用聚类方法识别AKI后的临床状态。利用多状态建模估算各状态间转移概率和进展至CKD的风险，再通过生存分析挖掘AKI子群的CKD危险因素。

Result: 共分析20,699例AKI入院患者，3,491例（17%）最终发展为CKD。研究共识别出15种不同的AKI后状态，每种状态进展至CKD的概率不同。大部分患者（75%）在研究期间仅处于单一状态或仅经历一次状态转换。既有（如AKI严重度、糖尿病、高血压等）和新发现的CKD危险因素在不同状态下影响存在差异。

Conclusion: 提出了一套基于数据驱动的方法来识别AKI患者向CKD进展的高危群体，有助于早期干预和辅助决策工具的开发。

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [55] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文比较了人类和多种大语言模型在新闻政治偏见检测上的一致性，发现RoBERTa和GPT分别在不同设置下与人类标注最为一致。研究建议采用结合人类解释能力和模型效率的混合评估方法来提升自动偏见检测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理（NLP）技术已能用于自动化判别新闻媒体的政治偏见，但当前大语言模型（LLM）与人类判断在偏见识别上的一致性，尚未得到充分研究和理解。该研究希望系统比较和评估人类与多种LLM在政治偏见检测上的表现。

Method: 该研究构建了一个人工标注的新闻文章数据集，比较了人类注释与多种LLM（如GPT、BERT、RoBERTa、FLAN）在偏见极性、注释一致性和模型间一致性等维度的差异。对RoBERTa模型进行了微调，并评估了各模型的性能与对人类标注的对齐程度。

Result: 实验显示，在传统transformer模型中，RoBERTa与人类标注的一致性最高；在零样本环境中，生成式模型GPT与人类注释整体一致度最强。经过微调的RoBERTa模型获得了最高准确率和与人类注释标签的最强一致性。

Conclusion: 研究揭示了人类与LLM在政治倾向感知方面的系统性差异，强调自动媒体偏见检测需要结合人类可解释性和模型可扩展性的混合评估框架。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [56] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 用视觉-语言模型提升多智能体科学发现系统，实现更高准确率和可解释性，并能自我纠错和适应新数据。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在自动化科学发现过程中，缺乏有效的自我纠错和适应新数据的能力，且现有方法可解释性和可靠性有限。本文旨在利用视觉-语言模型（VLMs）提升这些方面。

Method: 将视觉-语言模型（VLM）作为评审，利用其根据动态生成的领域专用标准评估数据分析图，从而实时引导和校正多智能体探索过程，辅助发现中的自我纠错和适应能力。案例包括宇宙学和天体化学领域。

Result: 在10项数据驱动发现基准任务中，VLM增强系统的得分为0.7-0.8，显著超过仅用代码（0.2-0.3）和代码加文本基线（0.4-0.5）；系统还能生成可审查的推理过程，提高了解释性。

Conclusion: 多智能体系统如果有VLM指导，可以更好地进行自主科学发现，具备强大的自我修正、数据适应和可解释性，提升了端到端科学探索的效果。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [57] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: 该工作提出RareSeek R1模型，整合知识和叙述语料，通过创新微调和检索方法，大幅提升罕见病诊断准确率和稳定性，支持临床实践。


<details>
  <summary>Details</summary>
Motivation: 罕见病患者全球达数亿，但诊断周期长、信息噪声大、既有模型受限于现实EHR稀缺和领域知识过时，以及生成模型幻觉等问题。亟需有效提升诊断效率和可靠性的新方法。

Method: 该工作通过分阶段指令微调、思维链学习和基于图的检索，开发了专门面向罕见病诊断的RareSeek R1模型。模型融合了多中心EHR叙述语料和专业知识，借助增强检索提升了模糊和重叠病例的推断能力。

Result: RareSeek R1在多中心EHR和公开基准测试中取得了业界最优的准确率和泛化能力，且在噪声或重叠表现下稳定。检索增强对正确诊断提升最大，临床试验表明向医生一致性很高。模型透明性揭示非表型证据在诊断中的重要性。

Conclusion: RareSeek R1显著提高了罕见病诊断的准确性和稳定性，并在实际临床辅助和自动化推理方面表现出与经验医生相当的能力。其透明推理过程，使临床决策更加可审计和可转化。

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [58] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: 本研究通过统计语言模型与行为数据结合，提出定量模型精确预测并解释了语言中的比较错觉效应，验证了噪声通道理论作为统一语言处理机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 人们在语言处理中对某些句子的理解出现系统性误差，被称为比较错觉（comparative illusion），如句子“More students have been to Russia than I have”，即使语义上不合理却常被认为是可接受的。先前研究认为这种错觉源于人类判断句子时的贝叶斯推理过程，即对被噪声污染的语言信号进行概率性推断。作者希望用更精准的模型解释和预测这种现象。

Method: 作者复现并扩展了早期的行为学实验，首次提出并应用了将统计语言模型与人类行为数据结合的定量建模方法，对比较错觉句子的可解释性进行后验概率建模，用以预测错觉的强度。模型不仅覆盖了传统的解释，还加以细化，能解释主语为代词与全名词短语等新发现。

Result: 所提出的模型精确预测了比较错觉效应的强弱细节，并首次解释了“than-从句”中使用代词或名词短语对比较错觉的影响。这一预测获得了实证支持，显示噪声通道理论在句子理解中的广泛适用性。

Conclusion: 结果支持了噪声通道理论作为语言理解计算层面统一解释框架的观点，不仅适用于错觉类现象，也能解释一般的语言处理过程。理论能够对比较错觉以及相关语言现象给出新的精确预测，并获得实证验证。

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


### [59] [Bias in, Bias out: Annotation Bias in Multilingual Large Language Models](https://arxiv.org/abs/2511.14662)
*Xia Cui,Ziyi Huang,Naeemeh Adel*

Main category: cs.CL

TL;DR: 本文系统梳理并分类NLP注释偏见，综述检测与缓解方法，提出适应多语环境的偏见缓解方案，促进大型语言模型的数据公平与文化包容。


<details>
  <summary>Details</summary>
Motivation: NLP数据集中的注释偏见尤其在多语言、文化多样的环境下，影响了大型语言模型的公平性和效果。作者希望系统解决与分析这些注释偏见及其源头。

Method: 提出了一个全面的注释偏见理解框架，区分了指令偏见、注释者偏见以及语境与文化偏见；综述了多种偏见检测方法，包括人工一致性、模型分歧、元数据分析以及前沿技术如多语模型分歧和文化推断，进一步提出了多维缓解措施和伦理分析。

Result: 提出了注释偏见类型学，系统汇总了检测指标，开发出适应多语环境的集成偏见缓解方案，并对注释流程进行伦理分析。

Conclusion: 该研究为LLM注释流程提供了系统的理论和方法支撑，有助于制定更公平、多元和具有文化适应性的注释体系。

Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.

</details>


### [60] [Streamlining Industrial Contract Management with Retrieval-Augmented LLMs](https://arxiv.org/abs/2511.14671)
*Kristi Topollai,Tolga Dimlioglu,Anna Choromanska,Simon Odie,Reginald Hui*

Main category: cs.CL

TL;DR: 本文提出一种检索增强生成（RAG）管道的合同管理自动化系统，融合合成数据、语义检索、分类与奖励机制，能高效识别并优化问题条款，在真实低资源环境下准确率超80%，显著提升合同修订效率。


<details>
  <summary>Details</summary>
Motivation: 合同管理中的条款修订流程繁杂，容易出现问题性修订，而自动化这一流程受限于标注数据稀缺和合同数据结构复杂，亟需一种能高效识别和优化问题条款的系统。

Method: 提出并实现了一个模块化框架，包括合成数据生成、语义条款检索、可接受性分类、基于奖励的对齐等模块，通过RAG（检索增强生成）管道整合各项技术。

Result: 系统在实际应用场景下，合同条款问题识别与优化准确率均超过80%，表现出了强大的性能和实用性。

Conclusion: 该系统能够有效识别并优化合同中存在问题的条款修订，准确率超过80%，为实际低资源条件下的合同管理提供了可行的加速解决方案。

Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

</details>


### [61] [Quadratic Term Correction on Heaps' Law](https://arxiv.org/abs/2511.14683)
*Oscar Fontanelli,Wentian Li*

Main category: cs.CL

TL;DR: 作者发现Heaps' Law在log-log坐标下并不完全成立，提出二次函数拟合能更好描述真实数据，用概率模型解释了这一曲率特性。


<details>
  <summary>Details</summary>
Motivation: 当前Heaps' Law广泛被认为在log-log尺度下成立，但实际数据有细微偏差，需更精确地刻画类型-标记关系。

Method: 作者对20本英语小说和其他英语文本（部分为翻译文学）进行了实证分析。在log(type)-log(token)数据上做二次回归（线性项+二次项），并与“有放回随机抽彩球”的概率模型进行了理论比对。

Result: 二次项拟合参数基本稳定，线性系数略大于1，二次系数约为-0.02。理论模型表明曲率可用“伪方差”解释，该方法在小样本时估算凹曲较为有效。

Conclusion: 文章认为Heaps' Law在log-log坐标下并非严格幂律关系，因为类型-标记曲线仍有轻微凹曲。作者提出用二次函数拟合，拟合效果几乎完美。

Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.

</details>


### [62] [SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://arxiv.org/abs/2511.14684)
*Biaojie Zeng,Min Zhang,Juan Zhou,Fengrui Liu,Ruiyang Huang,Xin Lin*

Main category: cs.CL

TL;DR: 本文提出SMRC方法，结合MCTS和细粒度奖励机制对学生解题过程进行系统纠正，在多个数据集上表现优异，有助于提升大语言模型在数学教育中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在数学推理过程中容易出现错误，且相关纠错方法主要集中于模型自我纠正，这无法满足教育场景中“教师式”系统引导和修正学生解题过程的需求。为弥补这一差距，作者提出了新方法。

Method: 提出SMRC方法，将学生推理建模为多步决策问题，并使用蒙特卡罗树搜索（MCTS）探索最优纠正路径。通过LLM引导的广度优先搜索（BFS）和最终答案评估生成奖励信号，并利用反向传播机制将奖励分配到中间推理步骤，实现细粒度过程监督。同时，构建了包含学生解题步骤和正确推理的高中数学基准数据集，并提出了以解题准确率和正确步骤保留度为核心的双重评估协议。

Result: 实验表明，SMRC方法在两个公开数据集（ProcessBench和MR-GSM8K）以及作者构建的MSEB基准上均显著优于现有方法，无论在纠错有效性还是整体表现上都有优势。

Conclusion: SMRC方法能够结合教师式系统指导理念，实现对学生推理过程的系统纠正，在数学教育场景下展现出更高的适用性和表现力，并推动大语言模型在数学教育中的应用进步。

Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.

</details>


### [63] [Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries](https://arxiv.org/abs/2511.14685)
*Kiera McCormick,Rafael Martínez-Galarza*

Main category: cs.CL

TL;DR: 本文用天体物理为实验场景，探讨了大语言模型能否通过不同提示词和语言表述方式，有效编码科学测量得到的物理摘要信息。通过稀疏自动编码器进行文本特征提取，发现提示词与语言表达均影响模型对物理信息的表示和理解。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型在跨领域泛化与上下文学习能力方面表现优异，研究者希望探讨其能否有效编码通常仅依赖科学测量获得的物理信息，并理解提示词和语言表述对物理信息编码的影响。

Method: 论文利用稀疏自编码器从文本中提取可解释的特征，然后评估这些特征与科学测量得到的物理摘要统计之间的对应关系。

Result: LLM的嵌入能够有效编码科学测量相关的物理信息，且提示词设置和语言表达的不同会影响这种编码效果。

Conclusion: 论文发现大语言模型（LLM）的嵌入能够编码由科学测量得到的物理摘要统计信息，且这种编码受提示词方式和语言特性影响。

Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

</details>


### [64] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: 该研究表明，通过利用大型语言模型生成历史文本标注并微调现有工具，能够显著提高低资源历史语料库上的自然语言处理性能，即使只有较少的合成数据也有积极影响。


<details>
  <summary>Details</summary>
Motivation: 历史和低资源NLP面临诸多挑战，主要原因是标注数据有限且与现代网络语料库存在领域差异，影响模型在历史文本上的表现。

Method: 利用大语言模型（LLM）自动生成历史法语（16至20世纪）和中文（1900-1950年）文本的标注作为“真实标签”，并用这些数据对spaCy进行微调，提升分词、词形还原和命名实体识别的表现。

Result: 在特定历史时期的测试集上，通过微调后的spaCy实现了显著性能提升，显示模型对领域适应能力增强。即便是有限量的合成数据，也能显著提升对低资源语料库的NLP工具支持。

Conclusion: 领域专属模型对于历史和低资源文本处理至关重要，且利用LLM生成标注数据能够有效改善计算人文领域相关NLP工具的效果。

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


### [65] [Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances](https://arxiv.org/abs/2511.14693)
*Rishu Kumar Singh,Navneet Shreya,Sarmistha Das,Apoorva Singh,Sriparna Saha*

Main category: cs.CL

TL;DR: 本研究提出VALOR模型，利用多模态客户支持对话（文本+图片），实现投诉方面与严重性精细分类。基于多专家与Chain-of-Thought推理联动、语义融合，VALOR在真实复杂投诉数据上效果显著提升。研究促进了更强大、责任性AI产品服务体系建设，相关数据和代码已开放。


<details>
  <summary>Details</summary>
Motivation: 现有的投诉分析方法主要依赖单一模态的短文本内容，如推文或产品评论。实际客户支持场景中，用户常常会同时给出文本投诉和视觉证据（如截图、产品照片），但现有方法对多模态的、多轮对话数据利用不足，难以实现细粒度的投诉方面和严重性分类。

Method: 提出VALOR（Validation-Aware Learner with Expert Routing）模型。该模型面向多模态（文本+图片）客户支持对话，采用多专家推理架构，并结合了大规模生成式模型与Chain-of-Thought（CoT）提示，实现更细致的决策。同时引入语义对齐分数，辅以元融合策略，保证多模态信息的一致性和协同。

Result: 在精细标注的多模态投诉数据集上评估VALOR模型，在各方面和严重性标签的分类任务中均优于传统基线模型，优势在于复杂投诉场景下能有效融合图片与文本等分散信息。

Conclusion: 多模态信息交互与专家验证机制对实际投诉分析系统具有重要价值。VALOR不仅技术上推动了多模态、细粒度投诉理解，也与联合国可持续发展目标（如产业创新、负责任生产等）对齐。

Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

</details>


### [66] [Subword Tokenization Strategies for Kurdish Word Embeddings](https://arxiv.org/abs/2511.14696)
*Ali Salehi,Cassandra L. Jacobs*

Main category: cs.CL

TL;DR: 论文对库尔德语不同分词策略的词嵌入进行比较，发现BPE方法虽然初看表现高，但覆盖面低，真实效果被夸大；形态素分词模型在覆盖和表现上更均衡，建议低资源语言处理中关注分词覆盖性的全面评价。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探索低资源语言库尔德语中，不同分词策略（词级、形态素级、BPE）的词嵌入效果，对保持形态相似性的能力进行对比分析，推进低资源语言处理技术发展。

Method: 作者开发了一个BiLSTM-CRF形态分割模型，用于形态素分割，通过极少的人工标注及自举训练，并对Word2Vec词嵌入在包括相似性保持、聚类质量和语义组织等多个指标上进行评测。

Result: 实验证明BPE在保持形态相似性任务中表面表现优秀，但仅覆盖28.6%的测试用例，形态素分词模型则覆盖68.7%，BPE的性能存在人为夸大；全面评价下，形态素分词在词嵌入空间的组织、语义结构和对形态复杂度的覆盖上表现更优。

Conclusion: 低资源语言处理中的分词策略选择需关注评测覆盖度，单纯依赖BPE可能因低覆盖导致结论失真，形态素分词更适合保持嵌入空间的语义和形态结构，对类似库尔德语的低资源语言处理更有实际意义。

Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

</details>


### [67] [Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance](https://arxiv.org/abs/2511.14709)
*Raha Aghaei,Ali A. Kiaei,Mahnaz Boush,Mahan Rofoosheh,Mohammad Zavvar*

Main category: cs.CL

TL;DR: LLMs通过自动化和智能化大幅提升R&D流程效能，推动创新加速落地。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何通过智能自动化促进知识发现、跨学科整合与协作，从而变革R&D流程。

Method: 对科学文献、专利数据库及实验数据进行广泛分析，探讨LLMs在R&D中的应用及作用。

Result: LLMs能自动化知识发现、促进假设生成、整合跨学科见解，并增强创新生态中的协作能力，实现更灵活与高效的研发流程。

Conclusion: LLMs极大地提升了研发流程的效率和效果，通过多重功能变革创新周期，并加快新理念上市时间。

Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

</details>
