<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 105]
- [cs.DM](#cs.DM) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code](https://arxiv.org/abs/2510.06296)
*Lingfei Zeng,Fengdi Che,Xuhan Huang,Fei Ye,Xu Xu,Binhang Yuan,Jie Fu*

Main category: cs.PL

TL;DR: 提出了一个新基准VeriEquivBench，系统评测LLM生成代码与规范的正式验证能力，并展现现有模型仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 正式验证对于确保由大型语言模型（LLM）生成代码的正确性至关重要。目前，代码与形式规范共同生成的方法存在规范质量评估的瓶颈，现有基准依赖人工、专家密集的规范匹配，不仅规模有限且可靠性不足。

Method: 提出了VeriEquivBench，一种包含2389个复杂算法问题的新基准，并设计了基于形式等价分数的评价体系，取代传统的依赖人工地面真值匹配方法，系统、自动地验证生成规范和代码的质量。

Result: 实验结果表明，当前最先进的LLM在生成可正式验证代码方面仍然面临巨大的挑战。该基准暴露了现有模型在代码生成与形式推理中的局限性。

Conclusion: 生成可正式验证的高质量代码对于LLM来说仍具极大困难，亟需VeriEquivBench这样的新型基准来推动高级、可扩展、可靠自动编程体的研究进展。

Abstract: Formal verification is the next frontier for ensuring the correctness of code
generated by Large Language Models (LLMs). While methods that co-generate code
and formal specifications in formal languages, like Dafny, can, in principle,
prove alignment with user intent, progress is bottlenecked by specification
quality evaluation. Current benchmarks rely on matching against ground-truth
specifications, a manual and expertise-intensive process that has limited
existing datasets to a few hundred simple problems and also suffers from a
reliability issue. To address this, we introduce VeriEquivBench, a new
benchmark with $2,389$ complex algorithmic problems that probe the limitations
of current models in both code generation and formal reasoning. Our evaluation
framework replaces ground-truth matching with a formally grounded metric, the
equivalence score, and rigorously verifies the quality of generated
specifications and code. Our results show that generating formally verifiable
code remains a profound challenge for state-of-the-art LLMs. This underscores
both the difficulty of the task and the need for benchmarks like VeriEquivBench
to drive progress toward scalable and reliable coding agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 研究探索本地部署LLM在林业领域安全关键系统风险评估中的辅助价值，结果表明LLM能有效辅助评估与威胁识别，但须人工监督，专家愿意在特定环节部分依赖LLM，有助于推动安全关键系统网络安全评估自动化发展。


<details>
  <summary>Details</summary>
Motivation: 关键安全类软件系统对网络安全活动，尤其是风险评估的需求日益增加，而许多开发团队缺乏足够的网络安全专家，导致专家负担过重，需寻找新工具支持风险评估过程。

Method: 采用设计科学方法，在林业领域与12位专家进行访谈、交互式会议和问卷调查，探索本地部署的检索增强大语言模型（LLM）用于网络安全风险评估的潜力。

Result: LLM能够辅助专家生成初步风险评估、识别威胁并提供冗余检查，但需要人工监督以保障准确性和合规性。尽管存在信任问题，专家仍愿意让LLM在部分评估和辅助环节发挥作用，而不是完全依赖其生成内容。

Conclusion: 本研究证明了基于LLM的智能体在安全关键领域网络安全风险评估中的应用可行性，并为网络物理系统风险评估过程提供了有益见解，鼓励有条件地采用LLM技术。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [3] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 本研究通过用户中心设计与迭代开发，提出并实证验证了基于Git的高校作业提交系统，大幅优化了提交效率、协作和管理，师生普遍认可，且存储与流程成本有效降低，对高校软件工程及相关课程具备实用推广价值。


<details>
  <summary>Details</summary>
Motivation: 解决传统高教作业提交方式中存在的管理效率低、跟踪难、协作弱及师生负担重等问题，引入更现代化工具优化流程。

Method: 采用迭代性软件开发及以用户为中心的设计方法，在高校真实环境中集成系统，包含可用性测试和学生反馈的实证评估过程。

Result: 系统显著提升了作业跟踪、协作及提交效率。85%教师认为系统更易用，84%学生更喜欢，提交与审查时间减少38%，存储需求减少48%。学习曲线等问题通过迭代改善方案加以缓解。

Conclusion: Git为基础的作业提交系统显著提升了教学管理效率与学生体验，在大学实际环境中的应用有明显优势，促进了师生双方的积极参与和学习成果。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [4] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 当前基于模型驱动工程（MDE）的研究在支持视力障碍相关的可访问性方面尚不充分，存在模型方法细节缺乏、实证弱、用户参与少等问题，需进一步优化MDE流程以增强可访问性集成。


<details>
  <summary>Details</summary>
Motivation: 现有软件应用对有视力障碍的用户存在障碍。MDE由于其系统性，理论上可以有效集成可访问性需求并减少人工工作量，但实际操作和集成方法尚未明确。

Method: 通过系统性文献综述，对447篇论文进行了筛选，最终纳入30项主要研究，并分析了这些研究如何在MDE中纳入视力障碍可访问性问题。

Result: 多数研究参考了WCAG，但因个性化调整和验证有限，难以广泛应用。实际给出详细建模技术或完整系统实现的极少，对MDE具体实现（如转换规则、代码模板）描述不足，导致难以复用、普适和复现。此外，受影响用户参与度低，开发者可访问性知识有限，实证验证薄弱。

Conclusion: 当前基于模型驱动工程（MDE）的研究尚未充分支持视力相关的可访问性。未来需进一步在MDE流程中更有效地嵌入对视力障碍用户的支持。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [5] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文提出针对代码仓库高效上下文检索与收集策略，通过文件块粒度的静态分析检索，显著提升了代码补全任务的大模型表现。


<details>
  <summary>Details</summary>
Motivation: 大模型对于代码补全任务需要充足且相关的上下文信息，但仓库较大时，受限于模型上下文长度且包含无关噪声会影响代码生成质量，因此需要有效的上下文收集和检索方法。

Method: 通过对文件级和代码块级的检索策略进行系列实验，重点关注上下文大小和文件排序对大模型性能的影响，采用静态分析进行块级检索。

Result: 块级检索方法比最佳文件检索策略提升了6%，较无上下文基线提升了16%。上下文的数量和顺序会显著影响模型性能。

Conclusion: 检索粒度、顺序和混合策略对于实际开发场景中的有效上下文收集管道至关重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [6] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 本文提出了AiSysRev，利用大语言模型（LLM）优化软件工程文献系统综述中的筛查环节。工具支持多模型、自动与人工结合筛查，在137篇论文实践中展示了能有效减少人工筛查工作量，但边界判定仍需人工介入。


<details>
  <summary>Details</summary>
Motivation: 系统性综述是软件工程领域总结证据状态的标准做法，但在筛查或研究选择阶段，因待筛查论文数量巨大，工作极为繁重。标题-摘要筛查耗时费力，亟需自动化工具以提高效率。

Method: 开发了名为AiSysRev的基于LLM的筛查工具，作为Web应用运行在Docker容器内。用户可上传包含论文标题和摘要的CSV，并自行设定纳入/排除标准。可调用多个LLM进行筛查，支持零样本和少样本筛查，也允许人工介入，通过界面为人工评审提供机器判断结果参考。

Result: 实际试验涉及137篇论文，发现筛查结果可分为四类：易纳入、易排除、边界纳入和边界排除。边界类情况LLM易出错，需人工干预。整体而言，LLM虽不足以取代人工判断，但可显著减轻批量文献筛查压力。

Conclusion: LLM工具如AiSysRev能显著提升文献筛查效率，但在边界案例中仍需人工参与以保证准确性。LLM可作为快速综述的辅助，但系统综述不可完全依赖其自动判断。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [7] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 本文调查了11家软件公司的LLM聊天机器人政策制定过程，分析了影响政策的因素，并为管理者安全集成提供了指导。


<details>
  <summary>Details</summary>
Motivation: LLM聊天机器人的使用带来了风险，因此需要指导和规范以确保安全集成。

Method: 通过调查和分析11家公司的政策制定过程与影响因素。

Result: 详细揭示了公司在制定相关政策时考虑的核心因素，并为管理者集成聊天机器人提供建议。

Conclusion: 研究强调了制定明确政策以安全引入LLM聊天机器人到软件组织的重要性。

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [8] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 不同软件仓库挖掘工具在相同研究问题下数据和结果差异显著，需谨慎选用和复用工具，并倡导对比及可复现研究以提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 软件库挖掘被广泛应用于理解软件项目的演化、监控项目健康、辅助决策及提炼最佳实践。然而，当前广泛使用的挖掘工具的局限性及其结果一致性尚不明确，这影响了相关研究与实践的可靠性。

Method: 作者通过轻量级文献综述，从高水平会议/期刊筛选出三项与协作、维护和质量相关的研究，并使用四个独立、系统选取的软件仓库挖掘工具进行正式复现，定量和定性对比各工具提取的数据、分析结果和结论。

Result: 研究发现，工具设计与实现中的大量技术细节在复杂的挖掘流程中累积，导致基础数据、其派生分析结果、统计分析乃至最终结论存在显著差异。

Conclusion: 用户在选择工具和评估研究有效性时需谨慎，并深入了解工具的局限性。建议优先复用已有工具。同时，研究者和工具开发者可通过再现包和对比研究促进工具重用与结果的不确定性降低。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [9] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: Go开源项目中，提案大多被拒且反馈不足。通过实证分析和大模型预测，论文总结了拒绝原因分类，并提出改进建议，有效提升流程效率和体验。


<details>
  <summary>Details</summary>
Motivation: 开源项目中，提案流程繁琐、资源消耗高，被拒反馈不明确导致参与者沮丧。当前对提案被拒原因缺乏系统理解，制约了流程优化和对贡献者的有效指导。

Method: 对Go项目1091个提案进行混合方法实证研究：量化分析提案结果，构建被拒原因分类法，并用大语言模型（如GPT）预测提案结果。

Result: 多数学术提案被拒，定案时间超过一个月，仅14.7%的被拒提案会重提。总结出九类常见被拒理由。GPT类模型结合早期评论能预测拒绝结果（F1=0.71），有助于流程改进和资源分配优化。

Conclusion: Go语言开源项目的提案通过率较低，且大多数被拒提案不再提交。明确提案被拒的原因有助于改进流程，提升参与者体验和评审效率。GPT模型能较准确预测提案命运，有助于早期筛查。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [10] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 提出了CRAI-MCF框架，基于实证分析打造量化、模块化的大模型文档系统，解决文档不规范、难以横向对比的问题，提升大模型负责任采纳能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型文档碎片化、缺乏量化标准，导致用户难以高效发现、选择和采用专用大模型，阻碍创新和负责任应用。

Method: 以价值敏感设计为基础，通过分析240个开源项目，归纳217项参数，构建了包括八个模块的量化文档架构。提出了量化充分性标准，实现严谨的模型横向对比。

Result: CRAI-MCF实现了技术、伦理与操作维度的均衡，帮助用户高效评估和采用大模型，提高信心与合规性。

Conclusion: CRAI-MCF框架能够有效提升大模型的文档规范化及实用性，促进更负责任、透明的模型选择和采纳。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [11] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文系统记录了AIBOM（AI物料清单）作为AI领域SBOM扩展标准的制定过程，采用行动研究方法协调全球多元化参与，各方面验证后形成标准，并就制定经验和启示进行总结。


<details>
  <summary>Details</summary>
Motivation: 当前开放、社区驱动的标准在软件工程，特别是AI相关领域发展的重要性日益突出，但AI等快速演进领域的标准制定过程尚缺乏深入研究。

Method: 采用Action Research（行动研究）方法，多轮AR周期推动规范制定，联合90多位来自全球的多方利益相关者，通过法规对齐、用例映射、行业访谈和实际案例研究多渠道验证规范。

Result: AIBOM作为对现有SBOM标准的AI扩展，系统描述了AI组件及其训练工件，经多维度验证达到了法规、伦理和工业应用的要求，为推动AI系统合规和透明提供了基础。

Conclusion: 本文总结了AI Bill of Materials (AIBOM)规范的开发经验，并提出了基于Action Research的标准制定模式，能够为未来软件工程标准化工作提供有益借鉴。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [12] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 该论文提出了一种新框架Secure-Instruct，可自动生成安全与脆弱代码示例并对LLM进行微调，显著提升了主流模型生成安全代码的能力，在多项指标上超越了现有方法SafeCoder。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）虽能自动生成代码，但容易产生不安全代码，带来软件安全风险。现有方法如SafeCoder，受限于数据集有限且不均衡，导致生成安全代码的效果和泛化能力不足。

Method: 提出Secure-Instruct框架，能够自动合成高质量的脆弱和安全代码示例，生成微调指令，并用这些指令进行LLM微调，使模型任务描述与安全代码生成能力更一致。使用自建CWEBench和现有CWEval两个基准，评估四种主流LLM。

Result: Secure-Instruct不仅提升了生成代码的安全性，还增强了功能正确性。在CWEBench上，安全代码占比比预训练模型平均提升14.3%，比SafeCoder高7.6%。在CWEval上，CodeLlama-7B提升14%、Mistral-7B提升5.8%，均优于SafeCoder对应指标15.8%和6.8%。

Conclusion: Secure-Instruct能大幅提升LLM生成安全且功能正确的代码，相较于现有方法如SafeCoder有更高的安全性提升和泛化能力，为安全代码生成领域带来显著进展。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Reversible computations are computations](https://arxiv.org/abs/2510.06585)
*Clément Aubert,Jean Krivine*

Main category: cs.LO

TL;DR: 提出并证明了一种支持可逆计算的因果性并发模型，通过对称操作扩展了配置结构，对于理论与实际可逆并发系统建模具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 现有并发系统理论主要关注前向的、不可逆的事件因果关系。近年来，可逆计算在分布式与容错领域渐受关注，但缺乏系统的因果建模。本文动机是将因果性原则推广到可逆计算领域，提出一种支持可逆计算的因果性并发计算模型。

Method: 利用配置结构（configuration structures）的一种对称residuation操作来建模可逆计算，并证明了对于稳定配置结构，此操作不会破坏稳定性。进一步将这种语义扩展到prime event structures，并证明与一种交换冲突与因果性的切换操作等价。

Result: 构建了首个能稳定支持可逆计算的因果性并发模型，给出了稳定性证明，并将其扩展到事件结构，展示了新模型在理论上的良好性质。

Conclusion: 本文证明了研究并发系统中的可逆计算时，因果性原则（事件的可观测需在其因果前提事件后）依然成立，并开发了相应理论模型。

Abstract: Causality serves as an abstract notion of time for concurrent systems. A
computation is causal, or simply valid, if each observation of a computation
event is preceded by the observation of its causes. The present work
establishes that this simple requirement is equally relevant when the
occurrence of an event is invertible. We propose a conservative extension of
causal models for concurrency that accommodates reversible computations. We
first model reversible computations using a symmetric residuation operation in
the general model of configuration structures. We show that stable
configuration structures, which correspond to prime algebraic domains, remain
stable under the action of this residuation. We then derive a semantics of
reversible computations for prime event structures, which is shown to coincide
with a switch operation that dualizes conflict and causality.

</details>


### [14] [Strong Dinatural Transformations and Generalised Codensity Monads](https://arxiv.org/abs/2510.06777)
*Maciej Piróg,Filip Sieczkowski*

Main category: cs.LO

TL;DR: 本文推广了点式codensity monads，提出用混合变异双函子构造monads的新方法，并列举了建立同构的条件，对多态计算和有序非确定性模型给出了新表示。


<details>
  <summary>Details</summary>
Motivation: 现有点式codensity monads仅满足由函子生成monad的需求，难以处理复杂类型与混合变异双函子。希望推广理论以覆盖多种计算类型与结构，特别用于多态λ演算下的持续子monad建模。

Method: 以强弱拟自然性为理论基础，将由混合变异（mixed-variant）双函子生成的monads形式化，分析同构条件，并重点研究用hom-functor及相关内部对象生成monad的特例。

Result: 确立了dicodensity monads的通用构造方法，给出了建立与常规monads同构的充分条件，拓展了monads的表示，尤其是针对半环和有序非确定性计算等理论模型。

Conclusion: 本文提出了dicodensity monads的构造方法，并展示了如何通过特定条件建立与常规monads之间的同构，为理解和构造更复杂的monads打开了新途径。

Abstract: We introduce dicodensity monads: a generalisation of pointwise codensity
monads generated by functors to monads generated by mixed-variant bifunctors.
Our construction is based on the notion of strong dinaturality (also known as
Barr dinaturality), and is inspired by denotational models of certain types in
polymorphic lambda calculi - in particular, a form of continuation monads with
universally quantified variables, such as the Church encoding of the list monad
in System F. Extending some previous results on Cayley-style representations,
we provide a set of sufficient conditions to establish an isomorphism between a
monad and the dicodensity monad for a given bifunctor. Then, we focus on the
class of monads obtained by instantiating our construction with hom-functors
and, more generally, bifunctors given by objects of homomorphisms (that is,
internalised hom-sets between Eilenberg--Moore algebras). This gives us, for
example, novel presentations of monads generated by different kinds of
semirings and other theories used to model ordered nondeterministic
computations.

</details>


### [15] [A simple proof of the coincidence of observational and labeled equivalence of processes in applied pi-calculus](https://arxiv.org/abs/2510.07258)
*Andrew M. Mironov*

Main category: cs.LO

TL;DR: 文章通过提出一种显著更简单的证明方法，证明了应用pi演算中扩展过程的观测等价和标记等价一致。


<details>
  <summary>Details</summary>
Motivation: 由于观测等价和标记等价在applied pi-calculus中具有基础作用，复杂的原有证明方法不利于理解与推广，作者希望通过更简单的方法促进理论理解与应用。

Method: 作者提出了一种更简单的新证明方法，简化了原有的证明流程。

Result: 成功用显著简化的方式证明了两个等价性的一致性，降低了理论门槛。

Conclusion: 该论文给出了一个主要结果，即在应用pi演算（applied pi-calculus）中，观察等价（observational equivalence）与标记等价（labeled equivalence）对于扩展过程是一致的。

Abstract: This paper presents a new, significantly simpler proof of one of the main
results of applied pi-calculus: the theorem that the concepts of observational
and labeled equivalence of extended processes in applied pi-calculus coincide.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 作者构建了针对多语种大学教材的OpenStaxQA数据集，用于大语言模型精调和评测，并展示了其在教育和推理任务中的实际价值。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对高校教育应用的大型语言模型评测基准，且现有基准多以英文为主，资源受限。OpenStaxQA旨在填补多语种（包括英文、西班牙文和波兰文）公开教材问答评测基准的空白。

Method: 作者基于43本开放许可证的大学教材，收集构建了OpenStaxQA多语种问答数据集，并采用大约70亿参数规模的大语言模型进行精调，方法基于QLoRa量化低秩适配器。研究还在AI2推理挑战数据集上进行零样本测试，以评估泛化能力。

Result: 通过实验，OpenStaxQA可以用于有效评估和提升LLM在多语种教育场景下的应用表现，同时在AI2推理挑战等通用任务上也带来性能提升。

Conclusion: OpenStaxQA为多语言、开源教育应用领域提供了权威评测工具，有助于指导LLM算法优化，并探讨了此类数据集在教育与人工智能领域的广泛影响。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [17] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 为安全关键的工业问答场景提升AI模型可靠性，作者提出了融合知识图谱的多智能体蒸馏方法（KG-MASD）。该方法用知识图谱强化状态表征和可验证性，联合蒸馏推理能力和知识基础，显著提升模型准确率和可靠性，适用于边缘部署。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统需要比普通对话模型更高的安全性和可靠性，特别是在设备故障诊断等高风险场景下，错误可能产生严重后果。现有多智能体大模型虽强化了推理能力，但存在不可控迭代和输出不可验证问题，传统蒸馏方法也难以将协同推理能力迁移至轻量可部署的模型。

Method: 提出知识图谱指导的多智能体系统蒸馏（KG-MASD），将蒸馏过程建模为马尔可夫决策过程，并将知识图谱作为可验证结构化先验引入以丰富状态表示并保证收敛。融合协同推理与知识基础，生成高可信度的指令微调数据，同时联合蒸馏推理深度和可验证性至紧凑学生模型，实现边缘部署。

Result: 在工业问答数据集上实验，KG-MASD比基线模型提升了2.4%到20.1%的准确率，并显著增强了模型的可靠性，使其能够安全可信地应用于工业关键场景。

Conclusion: KG-MASD成功将多智能体协同推理和知识验证能力迁移至轻量模型，有效提升工业问答系统的安全性和可靠性，为可信AI在安全关键场合落地提供了技术方案。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [18] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 提出真人问卷作答质量自动评估框架，先过滤胡言乱语，再用大模型从多个维度评估质量，表现优于已有方法，实际应用效果好。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估方法多针对大模型生成的文本，对具有独特特征的真人问卷回答评估不足，导致结果可能有偏差。

Method: 提出一个专为真人问卷回答设计的两阶段评估框架。第一步用无意义（三无）过滤筛掉胡言乱语的回答，第二步用大语言模型，从努力程度、相关性和完整性三个维度对剩余回答进行评估，所有评价标准均基于真实调研数据经验分析。

Result: 在英韩双语数据集上，框架比现有指标表现更优，还能很好地预测回答质量和拒绝低质回答，与专家评审高度相关，展现出实际应用价值。

Conclusion: 所提两阶段框架能高效、准确地评估真人问卷作答的质量，为实际问卷调查提供了有力的数据支撑。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [19] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 该文指出当前主流语言模型合规性问题根源在于缺乏类型化语义，提出用类型理论、模型本体和神经-符号系统（Savassan），关联审查/法域规范，实现跨法域、透明合规的解释式判断。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型缺乏对描述性、规范性及法律维度的显式语义处理，导致输出经常出现幻觉、合规不透明等问题。希望通过类型理论和本体结构提升模型的理解、推理和合规能力。

Method: 提出Savassan神经-符号架构：神经网络从自然语言中提取候选结构，符号组件对其进行类型检查、约束推理及跨法域映射，并将结果投影到多法域本体中，实现透明且可解释的合规决策。支持一次解析多法域投射（如法律风险差异）。

Result: 提出了Savassan架构，将自然语言解析成带型逻辑结构，并嵌入扩展本体以支持法域维度推理，实现了能融合多法域解释的合规判定。论文还制定了使用法律推理基准和多法域合成套件的评测方案。

Conclusion: 论文认为当代语言模型频繁出现幻觉（错误输出）、脆弱的内容审查与不透明的合规性，其根源是缺乏类型理论语义，而非数据或规模问题。提出将意义类型化、结构化，是实现可信合规AI的关键。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [20] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 本论文提出CoT Referring新策略，通过结构化链式训练提升多模态模型在复杂指代任务上的推理与一致性能力。在新构建的高难度基准及公开数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: Referring Expression的理解与分割是考察多模态大模型语言与图像理解集成能力的关键任务，但在复杂查询场景和指代链条上现有方法表现不足，需要新策略提升推理和一致性能力。

Method: 通过链式思维（Chain-of-Thought）训练数据结构，将文本结构解析为多步指代，确保每步关系识别与参考一致性；重构训练数据，设计新的标注和高复杂度基准，融合检测和分割到统一MLLM框架，并用自适应加权损失优化训练。

Result: 在自定义复杂指代基准、RefCOCO/+/g数据集上，所提方法较基线模型准确率提升2.5%以上，显示了结构化训练与联合任务框架的优越性。

Conclusion: 提出的CoT Referring策略有效提升了跨模态推理能力，在复杂指代场景中准确度更高，实验结果在多组基准数据上优于现有模型。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [21] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 该论文针对科学领域，构建了一个下游任务驱动的词表示与分词方法评测套件，并实测了多种算法，为后续领域专属NLP研究提供评测工具和方法。


<details>
  <summary>Details</summary>
Motivation: 不同领域中同一个词可能有不同的含义和表示方式，因此寻找适合领域特定数据的词表示算法变得非常重要。当前的生成式AI及transformer结构虽能生成上下文相关的词嵌入，但在预训练时耗时且计算成本高。为解决科学领域的这一问题，需探索更优的词表示及分词方法。

Method: （1）构建一个包含多个下游任务及相关数据集的科学领域评测套件。（2）利用此评测套件测试不同的词表示和分词算法。

Result: 搭建了用于科学领域下游任务的评测套件，并利用其对多种词表示及分词方法进行了效果评测。

Conclusion: 提出并实现了一个系统化的评测方案，可用于评测和比较科学领域内的各种词表示及分词方法，并为后续新算法的引入与评测提供了基础工具。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [22] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 提出TRepLiNa方法（CKA+REPINA），通过对齐模型中层表征，低成本提升印度低资源语言翻译质量，对于数据缺乏场景尤其有效。


<details>
  <summary>Details</summary>
Motivation: 印度的多样低资源语言在自然语言处理领域极度缺乏数据与支持，限制了AI模型在这些语言上的表现和社会影响。作者旨在弥补这些低资源语言与高资源语言之间的差距，推动多模态和多语言技术对印度社会的正面影响。

Method: 作者提出在多语言解码器模型中，通过在部分内部层强制跨语言表征对齐，提升翻译质量。具体方法是将表征相似性度量CKA和正则化方法REPINA结合，形成TRepLiNa方法，并使用 Aya-23 8B + QLoRA 在MMLoSo挑战任务的语言对上进行零样本、少样本和微调实验。

Result: TRepLiNa（CKA+REPINA）方法能有效对齐中层表征，提高低资源语言到高资源语言的翻译质量，尤其适用于数据稀缺场景。该方法低成本且实用，显著提升了MMLoSo挑战任务中 Mundari、Santali、Bhili 到英语/印地语的翻译效果。

Conclusion: 通过在多语言大模型中对齐中层表征（TRepLiNa），能够以低成本提升低资源语言的翻译质量，为解决语言资源不均和社会公平带来了积极成果。

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [23] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 本论文提出在人机协作下的多语种高质量PII标注框架，覆盖13个地区、数百PII类型，显著提升数据集质量与模型能力，并系统性解决多语标注挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大模型应用加广，对不同国家与地区的个人身份信息监管合规需求加剧，需要能高效、准确地在多语种环境下标注PII数据用于模型训练。

Method: 设计了一种分阶段、结合人工与质控的数据标注流程，通过引入语言专家与严格质量保证，并利用注释员一致性度量和根本原因分析，提升数据的质量。

Result: 框架在13种欠代表地区、336种不同PII类型的多语环境下，显著提升了召回率和降低了误报率，生成了适合监督微调LLM的高质量数据集，并总结了多语PII标注常见挑战和改进措施。

Conclusion: 提出的多语言PII数据标注框架能够系统性发现并解决标注不一致问题，生成高保真数据集，提升LLM在个人身份信息识别任务的可靠性。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [24] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 本文推出了基于标准化问卷收集的阿育吠陀体质（Prakriti）数据集，方法科学，数据高质量，极大地促进了该领域的研究和智能健康应用开发。


<details>
  <summary>Details</summary>
Motivation: 为提升阿育吠陀体质（Prakriti）评估的标准化、自动化和数据可用性，推动计算智能及个性化健康管理研究。

Method: 采用标准化双语（英文-印地语）Prakriti体质评估问卷，包含24个强制选择题，通过Google Forms收集数据，自动评分，映射个体特征至不同体质分数。问卷题目中立、无偏见，且隐藏了Dosha标签。

Result: 获得了结构化且高质量的体质评估数据集，支持特征分布、相关性分析和预测建模，有助于今后Prakriti相关智能健康应用开发及研究。

Conclusion: 该数据集为后续Prakriti体质研究和个性化健康应用提供了有价值的结构化平台，具备广泛的研究和应用前景。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [25] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文提出了在双嵌入式设备上离线运行的EHR自动摘要系统，实现了高效、安全的患者信息提取和摘要，为急诊医生提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中包含大量非结构化临床数据，急诊医生在查找关键资料时容易被信息淹没。当前缺少能在保证病人隐私的情况下，离线高效总结EHR内容的解决方案。

Method: 提出了一个完全在嵌入式设备上运行的两阶段摘要系统。第一阶段（Retrieve）用一台Jetson Nano-R设备检索相关病历片段；第二阶段（Summarize）用另一台Jetson Nano-S生成结构化总结，二者通过轻量级socket链接通信。检索端基于本地存储EHR，将长文本切分为语义相干片段，对医生查询检索相关片段。生成端利用本地部署的小型语言模型（SLM），在Jetson设备资源约束下生成（1）关键发现清单和（2）针对查询的叙述文本。实验对比了六种开源7B参数以内模型，并用“LLM-as-Judge”评测摘要质量。

Result: 在MIMIC-IV和脱敏真实EHR集上，系统能在30秒内生成有用摘要，兼顾事实准确性、完整性和清晰度，且完全离线、不泄露隐私。

Conclusion: 该系统可为急诊医生在资源受限和隐私要求高的环境下，提供高效、离线且有针对性的电子健康记录自动摘要，有助于临床决策。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [26] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 本文综述了LLM幻觉问题，从源头、检测与缓解等全链条分析，并提出未来研究方向与挑战。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在生成流畅文本的同时会输出虚假或无根据信息，在需要高事实准确性的领域极大削弱了其可信度，激发研究者系统分析幻觉问题，以推动更真实可靠的LLM发展。

Method: 通过归纳现有文献，梳理LLM幻觉的类型、成因，并对检测与缓解方法进行分类总结，同时评估相关评测指标，综合分析问题与挑战。

Result: 该论文系统回顾了大语言模型（LLM）在自然语言处理中的幻觉现象，涵盖其类型、产生原因、检测方法、缓解策略及相关评测基准，并分析当前方法的优缺点及未来挑战。

Conclusion: 当前检测和缓解幻觉的方法各有优缺点，未来研究需关注提升LLM的真实性和可靠性，为更可信的自动化语言生成奠定基础。

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [27] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 作者用深度学习与情感分析等技术，分析了七十年来美国Billboard歌曲歌词，发现自1990年起歌词中侮辱和性暗示等不当内容大幅上升，语言变化反映社会风气变迁。


<details>
  <summary>Details</summary>
Motivation: 流行乐曲中不当与性暗示性内容增加，但缺乏系统性定量验证，影响相关政策制定与青少年心理健康。

Method: 使用深度学习和自然语言处理（NLP）方法，对美国过去七十年Billboard榜单歌曲歌词进行了纵向分析，包括情感分析和不当内容（如性暗示、侮辱性内容）检测。

Result: （1）自1990年后，流行音乐中不当内容显著增加；（2）歌词中包含侮辱、性暗示及不当用语的比例上升；（3）纵向分析发现语言模型能够捕捉歌词内容随时间变迁的微妙模式。

Conclusion: 流行音乐中不当内容（如辱骂和性暗示性歌词）自1990年以来显著增加，语言模型的分析显示这些内容与社会规范及语言使用的变化相关。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [28] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 本文认为代码生成大模型评估需同时衡量功能正确性与对用户指令的遵循（vibe check），提出了VeriCode指令体系与Vibe Checker测试库，发现指令遵从性与人类偏好高度相关，为模型对齐提供新方案。


<details>
  <summary>Details</summary>
Motivation: 目前代码生成大模型在评估时大多仅关注功能正确性(pass@k)，但用户的“vibe check”实际还包含代码的风格、易读性、意图保留等主观偏好，这些非功能性需求未能得到有效衡量。本文旨在补充现有评估盲点，提出更贴合人类真实偏好的评价方法。

Method: 构建了VeriCode，包含30类可验证的代码指令及其自动化判定方法。将其结合现有代码功能测试集，形成Vibe Checker评估体系，用以同时衡量模型代码功能正确性和对指令的遵从能力。同时，对当前主流的31个LLM进行了系统性对比评测。

Result: 即使最先进的代码大模型在遵守多种指令和维持功能正确性方面依然存在明显短板。将功能正确性和指令遵从性复合得分后，与人类编程中的实际偏好关联性最强，二者中的指令遵从性成为编程任务表现优劣的主要分界线。

Conclusion: 提出了“vibe check”的核心组成因素，并用VeriCode和Vibe Checker为今后更好对齐用户偏好的模型开发和评价提供了新路径。通过识别和量化指令遵从性，推进了代码生成大模型在人类真实需求上的进步。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [29] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 作者复现了XRec推荐解释框架，采用Llama 3，并扩展分析专家嵌入模块。发现框架能有效生成解释且稳定性提升，但并非所有指标优于基线。


<details>
  <summary>Details</summary>
Motivation: 验证并扩展XRec框架在LLM推荐解释中的效果与稳定性，提高可复现性，并探究专家模块对模型解释结构的影响。

Method: 基于原作者源代码复现并修改模型专家模块嵌入结构，用Llama 3替换原用GPT-3.5-turbo，进行对比实验和扩展分析。

Result: 复现了XRec框架，用于大语言模型（LLM）解释推荐结果。与原论文不同，作者采用Llama 3取代GPT-3.5-turbo，并进一步修改XRec模型中专家模块的输入或输出嵌入。结果显示，XRec可以有效生成个性化解释，且通过协作信息提升稳定性，但不是所有指标均优于基线。进一步分析发现专家嵌入对解释结构影响重大，说明协作信号能与LLM建模交互。开放源代码提升了复现及研究的可访问性。

Conclusion: XRec在个性化推荐解释和稳定性上表现良好，专家嵌入对解释结构有显著作用，但在所有评价指标上未全面超越基线。开放了复现实验代码，便于学界使用。

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [30] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 本文系统比较了多语种transformer模型与统计基线对问题复杂性和类型的建模能力，提出新数据集与方法，并分析各模型表现与适用场景。


<details>
  <summary>Details</summary>
Motivation: 研究多语种transformer模型是如何表征问题的形态句法属性，填补相关领域对问题复杂性和类型建模细致刻画的空白。

Method: 引入QTC数据集，涵盖7种语言并标注问题类型、依存长度、树深度、词汇密度等复杂性指标。扩展探针方法到回归标签并计算选择性，提高推广性评估。比对静态Glot500-m模型表示的不同层次探针效果、subword TF-IDF基线和微调模型性能差异。

Result: 统计特征可在有显式标记的语言中有效判别问题类型；神经探针能更好捕捉问题结构复杂性。进一步分析何时上下文表征优于统计基线，以及参数更新是否削弱预训练语言信息。

Conclusion: 统计与神经方法各有优势，神经模型能更精细处理结构信息，通过微调可能损失部分原有语言知识。为多语种NLP任务选择模型和特征提供了有意义参考。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [31] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 本文提出了针对LLM输出分布偏差的加权自适应损失微调方法，在均等分布和现实分布下显著缓解了性别-职业输出偏差，验证了根据应用目标灵活调整偏差缓解策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 过去偏差缓解工作通常关注促进社会平等和人口统计公平，但很少关注让大语言模型（LLM）的输出分布对齐到实际或预期的分布（如现实世界分布），这对于提升事实依据和应用目标非常关键。

Method: 提出了一种基于加权自适应损失的微调方法，用于让LLM性别-职业输出分布对齐到预期分布（可为均等分布或现实分布），同时保持语言建模能力。实验采用美国劳工统计（2024）分为男主导、女主导、和性别均衡三类职业集，比较自适应方法（现实反映）和非自适应方法（均等目标）的效果。

Result: 在三种掩码语言模型中，无论是均等分布还是现实分布目标，模型都存在偏差。采用提出方法后，实现了几乎完全的均等分布下偏差缓解，以及在现实分布目标下30%-75%的偏差减少。自回归LLM在均等分布下无明显偏差，但在现实分布下仍有显著偏差，Llama Instruct系列（3.2-3B, 3.1-8B）实现了50%-62%的偏差减少。

Conclusion: 提出的方法有效缓解了LLM在性别-职业输出上的偏差，既能实现社会公平目标，也能更好地对齐到现实分布，应根据具体应用需求选择目标分布和缓解策略。

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [32] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: 提出新的基准EVALUESTEER，用于系统性测量LLM与奖励模型在用户价值与风格引导上的表现，结果显示现有模型对复杂用户偏好信息适应性弱，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法支持对奖励模型引导能力的受控评估，因此需要新的基准来系统性测试模型能否根据用户多元化特性调整输出。

Method: 合成生成了165,888组偏好对，系统性地在4个价值维度和4个风格维度上进行变量控制，并设计基准测试考察模型对用户偏好的可引导性。包含六种开源和闭源模型，在多种提示和偏好场景下进行评估。

Result: 最优模型在给出完整用户偏好档案时，仅能达到不足75%的响应选择准确率，而在仅给出相关性偏好时准确率达99%以上，表明奖励模型难以有效利用完整用户信息。

Conclusion: EVALUESTEER揭示了当前的大型语言模型奖励模型在识别并适应用户完整价值与风格偏好方面的能力有限。

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [33] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 作者提出了EverydayMMQA框架，创建了涵盖语音、图片、文本、英语和阿拉伯语等多样输入和文化背景的OASIS数据集，有效提升多模态模型对日常文化相关任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在具文化背景的日常知识问题上表现不佳，尤其在低资源和代表性不足的语言环境中。需要更加综合和本地化的数据集来提升模型能力。

Method: 设计了EverydayMMQA框架，用于创建大规模、多语言、多模态QA数据集；并据此开发了OASIS数据集，结合语音、图片和文本内容，覆盖多种输入组合和文化背景。

Result: OASIS数据集包含~92万张图片和1480万QA对，其中370万是口语问题，涵盖英语及阿拉伯语、多国文化，并已对多种模型进行基准测试，为多模态LLM提供了更好的评估和训练资源。

Conclusion: 提出了一个新的框架和数据集，用于提升大规模多模态模型在文化语境下的日常任务能力，尤其针对低资源和多样语言。

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [34] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: 引入语义正则表达式对LLM特征进行结构化描述，提升了解释准确性与一致性，同时促进模型分析和用户理解。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）特征解释性的自动化方法中，基于自然语言的特征描述往往存在模糊、不一致及需人工修正的问题。为提高特征解释的准确性与一致性，需要一种更结构化、易于理解的描述方式。

Method: 提出了“语义正则表达式（semantic regexes）”方法，将LLM特征转化为结构化语言描述。该方法结合能捕捉语言学和语义特征模式的基本单元，并加入上下文化、组合化和量化等修饰，以获得精确且有表现力的特征描述。通过定量基准测试、定性分析及用户研究评估方法有效性。

Result: 语义正则表达式在准确率上与自然语言描述相当，但在简洁性和一致性方面更优。其结构化特性还能支持特征复杂度量、跨层级分析和模型级解释扩展。用户研究显示，这种描述方式有助于用户准确理解LLM特征的激活。

Conclusion: 语义正则表达式能提升对大语言模型特征的结构化、精确和描述一致性，帮助用户建立更准确的心理模型，同时拓展了自动化解释的应用范围。

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [35] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 提出结合N-gram倒排索引与LLM重写的防关联方法，在法院案例应用中有效防止基于搜索的关联风险，且语义保存良好。


<details>
  <summary>Details</summary>
Motivation: 当前去标识化模型只能隐藏文本中的个人信息，但未能解决通过短语检索实现反向关联的风险，即可通过原始数据集定位去标识化文本的来源。

Method: 提出一种分两步的防关联攻击方法。第一步，建立N-gram倒排索引，找出仅出现在少数(<k)文档中的n-gram；第二步，利用大语言模型(LLM)对这些易关联的片段进行迭代重写，直到无法实现链接。

Result: 在法院案例数据集上实验表明，该方法能有效阻止基于搜索的关联攻击，同时维持语义完整和内容忠实。

Conclusion: 该方法实现了保护去标识化文本内容在防止关联攻击和保持原内容语义之间的平衡，具有实际应用价值。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [36] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: 本文提出RegDiff框架，通过VAE和属性监督训练的潜在扩散模型实现高效且精确的属性可控文本生成，无需预训练分类器，计算成本低，实验结果优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本属性可控生成上的主要两类方法各有缺陷：无分类器指导法（CFG）语义保持好但属性控制弱，有分类器指导法（CG）属性控制好但计算开销大且泛化性差。作者欲实现低计算成本且属性控制强的可控文本生成。

Method: 提出了一种正则化扩散框架RegDiff，利用VAE编码器-解码器架构以保证重构准确性，并在训练阶段用属性监督训练潜在扩散模型。属性信息仅在训练时注入，生成阶段不需分类器。

Result: 在五个涉及多种文体属性的数据集上，RegDiff在属性可控文本生成效果上优于强对比基线，验证了其高效性与有效性。

Conclusion: RegDiff能够无需预训练分类器，在降低计算成本的同时实现高效且可控的属性文本生成。其实验证结果表明优于现有主流方法。

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [37] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 奖励模型在语言模型对齐中的表现存在显著社会群体偏见，不同群体获偏好的表达往往被忽略，引导（如提示）只能部分缓解，难以彻底解决。


<details>
  <summary>Details</summary>
Motivation: 奖励模型作为语言模型对齐的核心，广泛用于模拟和代理人类偏好，引导语言模型的行为，但其本身如何表现、是否存在系统性偏见，迄今了解有限。该研究旨在揭示奖励模型在社会人口统计各群体中的偏见程度及对齐现状。

Method: 提出了一个用于衡量奖励模型（RM）观点对齐的框架，系统性地分析RM的社会人口统计偏见，并研究如何通过提示引导RM更好契合特定群体的偏好。研究内容包括对争议性话题的主观与多元视角分析，以及用定量方法描述RM在观点、态度和价值观上的表现。

Result: 实验发现，当前奖励模型与多个人口统计群体的观点对齐较差，并且有系统性奖励有害刻板印象的倾向。通过提示引导虽能略有改善，但不足以根除偏见。

Conclusion: 研究强调偏好学习和模型对齐过程中需更严谨地审视奖励模型行为，防止社会偏见在实际语言技术中进一步扩散和强化。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [38] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 本文提出了基于LLM的虚拟实验教学问题生成框架，通过理解教师意图和实验内容，结合问题及提示分类法，显著提升问题质量和格式一致性。评估表明大型模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 虚拟实验室为科学学习提供了探索式实践机会，但教师难以将其调整到实际教学目标，现有第三方资源通常不适合课堂需求，自主开发则费时且难以扩展。近期大型语言模型（LLM）的发展为解决这些问题提供了新的可能性。

Method: 提出了一种基于LLM的教学目标对齐提问生成框架，通过自然语言交互，自动生成与虚拟实验内容及教学目标一致的高质量问题。框架包括四部分：通过教师与LLM对话理解教学目标、通过知识点及其关系分析理解实验内容、构建问题分类法以体现认知和教学意图，以及采用TELeR分类法优化问题提示细节。设计过程包含教师参与的案例研究和基于19个开源LLM生成的1100余个问题的评估。

Result: 与教师意图和实验内容对齐的问题生成机制可提升问题质量，开放性和关系型问题能将评分提升0.29-0.39。优化的TELeR问题提示让问题格式可解析性提升至80%，格式符合度超过90%。更大的模型进一步提升性能：可解析性提高37.1%，格式符合度提高25.7%，平均质量提升0.8分（Likert量表）。

Conclusion: 该框架有助于教师利用LLM，在虚拟实验教学中快速、高效地生成与教学目标和实验内容相符的高质量问题，大幅提升问题生成效率和质量，并改善模型对教学需求的适应能力。

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [39] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 作者提出适用于复杂金融问题的长文本问答及多层次归因能力评测基准FinLFQA，涵盖证据、推理与知识三大维度，并首次系统性比较了多种LLMs归因生成方法。研究发现细粒度评价及指导性优化对提升归因质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在回答长文本复杂问题时常出现“幻觉”，即给出看似合理但实际上不正确的答案。现有的归因方法和评测，多以检索文献证据为主，无法满足如金融领域等现实场景对多层次、复杂归因的实际需求。作者认为，需要一个涵盖多维归因能力的专用基准来准确评估LLMs在金融问答中的表现。

Method: 作者提出了FinLFQA基准，用于评估LLMs在复杂金融问题长文回答及其归因能力。FinLFQA通过人工标注，考察三个关键维度：（1）来自金融报告的支持证据；（2）中间数值推理步骤；（3）指引推理过程的金融专有知识。同时，作者还提供了自动化的评测框架，综合考察答案质量和归因质量。实验在八种不同LLMs及多种归因生成范式下进行。

Result: 实验证明，细粒度的评价指标有助于判别模型能力；端到端生成和后处理方式在性能上相当；只有在有外部反馈指导时，迭代优化才有效果。

Conclusion: 对于金融等高要求场景，单一证据检索式归因不够，需构建能涵盖证据、推理和领域知识三者的综合评价框架。FinLFQA为评估LLMs归因和答案生成能力提供了新基准，推动领域更精细化评测和模型发展。

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [40] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST提出了可统一多语言多关系集合的语篇解析器，无需对关系集做修改，采用参数高效的Masked-Union训练方法，取得跨语言语篇分析的最优结果。


<details>
  <summary>Details</summary>
Motivation: 解决多语言和多树库下关系类别不一致性，推动语篇分析统一建模，实现高效的跨语言、跨关系集合的语篇解析。

Method: 提出两种模型训练策略：Multi-Head（为每种关系集合分别设置分类层）、Masked-Union（通过标签掩码实现参数共享），统一训练与分析多语言树库。并对低资源环境引入简单有效的数据增强方法。

Result: 最佳的Masked-Union方法不仅参数高效，还在18个树库中的16个上超越了单一树库模型基线，验证了统一多语言建模和标签共享策略的有效性。

Conclusion: UniRST能够在无需更改关系类别的情况下，统一处理多达11种语言、18个树库，并在大多数单一树库基线上取得更优表现。

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [41] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 文章提出MathRobust-LV测试集，首次系统考查大语言模型解高中新数学题的语言鲁棒性，测试34款模型后发现，无论模型强度，措辞上的变化都会导致准确率下降，表明当前模型在实际教育应用中仍有推理漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前对大模型数学推理能力的评估多关注高难度赛事（如IMO），但实际教育场景下经常出现同一知识点被不同措辞反复考查的现象，探讨模型在真实教学环境下对语言多样性的适应性。

Method: 构建了MathRobust-LV测试集，通过不同措辞重写但难度不变的方式对34个主流大语言模型进行了系统评测，比较了原始题干与重写后的题目下的模型表现。

Result: 小模型在措辞变化下准确率下降显著（9-11%），更强大的前沿模型表现较稳定但依然有可衡量下降，整体揭示出数学推理过程中模型对语言变体的脆弱性。

Conclusion: 对语言变体的鲁棒性是大模型数学推理的基础挑战，现有模型在题干措辞变化时准确率显著下降，说明当前模型推理稳定性不足。

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [42] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 本文系统梳理了LLM-智能体的安全格局，总结了应用和相关威胁以及防御措施，还指出了当前研究中的空白和未来方向，对网络安全领域具有重要参考价值。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从被动工具转变为自主智能体，其在网络安全领域的应用越来越广泛，同时也带来了独特的新型安全威胁。缺乏对这一新兴领域的全景式梳理与结构化分析，阻碍了安全研究的系统推进。

Method: 作者通过文献综述法，梳理了超过150篇相关论文，围绕应用、威胁与防御三大支柱建立了分类体系，并对智能体架构的新趋势与尚未充分覆盖的模型、模态问题进行了深入分析。

Result: 本文构建了应用、威胁与防御三大支柱的安全分类体系，对150余篇论文进行了归类和分析，总结了智能体架构中的发展趋势，揭示了模型类型和处理数据模态方面的关键研究缺口。

Conclusion: 本文提出了首个围绕自主LLM-代理（LLM-agents，基于大型语言模型的自主智能体）安全领域的系统性综述，建立了一个完整的研究框架，总结了现有进展、威胁与防御方法并指出了未来研究的薄弱环节。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [43] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 通过对澳洲土著语言的ASR微调，采用音位化分词，显著提升了识别性能且节省了人工整理时间，证明ASR可用于低资源语言文献编制。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别（ASR）系统依赖于大量数据，导致在资源稀缺语言上的应用非常有限。澳洲土著语言Yan-nhangu的语料极度稀少，有必要探索更适用于该类语言的ASR方案，以及评估ASR在语言文献编制中的实际价值。

Method: 使用wav2vec2 ASR模型在Yan-nhangu语言数据上进行微调，对比采用音位化和正字法化两种分词策略对识别性能的影响。同时考察ASR作为语言文献编制工具的可行性，包括对ASR输出的人工校正效率。

Result: 采用音位化分词的方案显著提升了识别的词错误率（WER）和字错误率（CER），优于传统正字法分词。同时，人工校正ASR输出所需时间远少于从头人工转录音频，说明ASR可用于资源稀缺语言的快速文献整理。

Conclusion: 有针对性的音位分词处理能提升低资源语言ASR效果，且ASR在人工校正辅助下适合用于语言文献编制，能显著提升工作效率。

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [44] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: TTS只对经领域微调或用于后编辑流程的MT模型有明显提升，对常规一次性翻译帮助有限。


<details>
  <summary>Details</summary>
Motivation: 前人在数学和编程任务上证明了推理模型测试时计算扩展（TTS）有效，但TTS在机器翻译上的作用尚不明确。

Method: 评估了12个推理模型，在多个多样化机器翻译基准下，分析TTS在直接翻译、强制推理扩展、后编辑三种场景中的表现。

Result: 普通推理模型在MT上用TTS提升有限且不稳定，表现易于饱和。经过领域微调的模型可通过TTS持续提升直至最佳推理深度。如果强制模型推理超过自然停点，翻译质量下降。而TTS在后编辑场景中可稳定提升自我纠错效果。

Conclusion: TTS对一般机器翻译模型帮助有限，但在特定领域微调和后编辑场景下效果突出。

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [45] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 针对RL数据稀缺问题，作者提出Webscale-RL流水线，把大规模预训练语料转为1.2M多域问答数据，显著提升RL训练效率和模型表现，为LLM扩展RL训练提供可行路径。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）主要通过模仿学习进行训练，但这种方式存在训练-生成差距，限制了模型的强健推理能力。虽然强化学习（RL）更高效且有潜力弥补这一差距，但目前缺乏足够规模和多样性的RL数据集，这成为RL应用的瓶颈。

Method: 提出了Webscale-RL流水线，可以将大规模预训练语料转化为海量、多样且可验证的问答对，以供RL训练使用。据此构建了Webscale-RL数据集，涵盖1.2百万个样本，跨越9个以上领域。实验通过对比基线和常规预训练进行了评估。

Result: 使用Webscale-RL数据集训练的模型在多个基准测试上明显优于持续预训练与数据精炼等强基线方法。在RL训练中，模型使用的数据量比持续预训练少100倍，即可达到同等性能，显著提升了训练效率。

Conclusion: Webscale-RL流水线及数据集实现了RL在大规模语言模型训练中的可行性和高效性，有望推动更强能力和更高效率语言模型的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [46] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 自举式预训练虽能降低语言模型训练成本，但其效果会随着基础模型预训练程度加深而递减。本文通过大量实证分析，揭示了这一规模效率递减规律，并用 scaling law 进行建模，对多阶段预训练提供了实际建议。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练成本极高，通过在预训练模型基础上进一步预训练（bootstrapped pretraining）可以节省重新训练的开销。然而，其在预训练程度较高的基础模型上有效性存疑，尤其对模型扩展和持续预训练场景，现有理解还不充分。

Method: 本文采用实证研究的方法，分析 bootstrapped pretraining 在不同预训练 token 数量下的规模效率变化。重点考察首次和后续预训练 token 对模型性能提升的影响，并用简单的 scaling law 建模其关系。

Result: 结果发现，bootstrapped pretraining 的效率随着基础模型所用预训练 token 数增加会以可预测、对数递减的方式降低。首次和后续预训练 token 的联合影响可用简单的 scaling law 精确刻画。预训练越充分，bootstrapping 的边际收益越小，存在明显的饱和效应。

Conclusion: 多阶段预训练策略必须权衡前期预训练强度与后续 bootstrapping 效果，过度预训练会导致后续预训练收益锐减。研究成果为高效训练语言模型及对过度预训练模型的再利用提供了实践指导和理论依据。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [47] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 本文发现助手型语言模型难以真实模拟用户行为，提出了专门训练的用户型语言模型，用于优化多轮对话评估。结果显示，更真实的用户模拟显著降低了助手模型的表现，凸显了当前评估方式的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估语言模型（LM）作为助手的性能时，常采用模拟用户进行多轮对话，但通常是用助手型LM来充当用户角色。作者发现这种做法并不理想：越优秀的助手型LM，越不适合模拟真实用户行为，从而影响评估的真实性。

Method: 提出了专门后训练的用户型语言模型（User LMs），用于更真实地模拟人在多轮对话中的表现，并与传统的助手型LM模拟方法进行了对比评估。

Result: User LMs在多项评估中更贴近真实用户行为，模拟的鲁棒性优于现有方法。在编程和数学任务的多轮对话场景下，使用User LM模拟用户时，强助手模型GPT-4o的性能从74.6%下降到57.4%，说明更真实的环境使助手模型暴露出更多不足。

Conclusion: 助手型LM不适合充当用户模拟器，专门训练的User LM能更真实模拟用户行为，从而为LM助手性能的评估提供更准确的环境。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [48] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist为大模型驱动的自动科研提供了开放、易用、可扩展的框架，显著降低了多智能体复杂工作流的扩展与维护门槛。


<details>
  <summary>Details</summary>
Motivation: 随着大模型推动的自动化研究日益复杂，现有多智能体工作流扩展和维护难度激增，需要一个更易交互和扩展的框架以适应工具快速迭代和学术前沿发展。

Method: 分析当前自动化研究流程的核心组成，设计并实现一个开放源码的框架，包括网页版演示和PyPI包，支持用户便捷地构建先进的自动化科研流程。

Result: 推出了TinyScientist框架，其代码开放、可在线演示，并提供一站式Python包，便利广大研究和开发者应用先进的自动化研究流程。

Conclusion: TinyScientist提出了一个可交互、可扩展、可控的自动化研究框架，能够灵活适应工具变更，促进迭代发展，有效缓解当前多智能体研究流程难以扩展和维护的问题。

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [49] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 本文揭示了LLM不同隐藏层在越狱与正常提示下的独特行为差异，为基于模型内部特性的新型越狱检测和防御机制奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 针对越狱攻击持续演化且现有防护机制难以完全抵抗的问题，希望从LLM的内部表示出发，寻找新的检测与防御方法。

Method: 以开源LLM模型GPT-J和状态空间模型Mamba2为研究对象，通过对隐藏层在越狱与正常提示下的响应行为进行层级分析，挖掘模型内部的差异特征。

Result: 初步实验发现，LLM在越狱与正常提示下的隐藏层响应存在显著差异，揭示了可用于后续研发越狱检测与防御方案的潜在方向。

Conclusion: 通过对LLM内部隐藏层的分析，发现其在面对越狱与正常提示时存在显著不同层级响应，这为利用内部模型动态特征进行越狱检测和防御提供了新思路。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [50] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文系统剖析了SSM与TBM中信息传递和表征演化机制，发现同质化趋势和过度平滑的异同及根本原因。结果为长序列处理模型的优化和设计提供了关键见解。


<details>
  <summary>Details</summary>
Motivation: 随着长序列处理需求的提高，SSM（状态空间模型）作为Transformer（TBM，基于变换器模型）的高效替代方案逐渐受到关注，但对于这两类架构中上下文信息在层与序列中的传递机制，尚缺乏系统性研究。作者希望弥补这一空白。

Method: 作者提出了首个统一的分析框架，从token层和layer层两个层面，利用centered kernel alignment（CKA）、稳定性度量与探查（probing）等方法，系统性研究了SSMs与TBMs中表征的传播和演化。

Result: 分析发现：TBM会迅速使token表征趋于同质化，直到后期层才重新出现多样性，而SSM则更早保留token独特性，却在深层逐步趋同。理论分析和参数随机化实验还表明，两种模型表征过度平滑的根源不同——TBM主要是结构设计所致，SSM则与训练动态密切相关。

Conclusion: 本研究深化了对SSM与TBM内在归纳偏置的理解，有助于为长上下文推理相关模型与训练方案设计提供理论与实践参考。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [51] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: 该论文提出了完全无需人类或外部AI参与的数据生成和对齐方法（SAO），显著提升了大语言模型的泛用对话能力，对后续模型自我进步具有指导意义。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法（如RLHF和RLAIF）依赖昂贵的数据标注和外部模型，为模型的对齐带来了高昂的成本和繁琐流程。为此，作者希望设计一种无需外部人工或AI标注的、高效的自我对齐方法。

Method: 提出了一种全自动的自我对齐优化（SAO）框架，模型自身生成所有训练数据（包括用户查询、回复和偏好），通过角色扮演生成多样化对话，并自主评估与优化偏好，无需任何人工或外部大模型参与数据构造。

Result: 通过大量实验，SAO在标准评测集（如AlpacaEval 2.0）上大幅提升了模型的对话能力，并且在下游任务（如问答、数学推理）中保持了强劲表现。

Conclusion: SAO为大语言模型的自我改进和对齐提供了一种实用的解决方案，有效降低了对外部数据和模型的依赖，实现了高效的自洽自用式模型对齐。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [52] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: 作者提出ToolMem系统，使智能体能记忆与总结不同工具的优劣，实验表明在文本及多模态生成任务中，ToolMem有效提升预测和选择工具准确率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体往往依赖固定工具，缺乏根据场景灵活选取最优工具的能力，而神经工具(如大型语言模型)性能不确定，因此需要能总结和记忆工具能力的机制，提高工具选择的灵活性和准确性。

Method: 提出ToolMem系统，通过交互总结工具优劣并存储为记忆，在推理时检索相关信息选择最优工具，实验中评估了其在文本生成和文本-图像生成任务中的有效性。

Result: ToolMem增强的智能体在预测工具表现方面分别提升了14.8%（文本生成任务）和28.7%（多模态生成任务），在多个工具选择中分别提升了21%和24%的绝对准确率。

Conclusion: ToolMem让智能体能够记忆和总结不同工具的性能，从而在不同场景下做出更优选择，并显著提升任务准确率。

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [53] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 本文提出高效的PiKa对齐数据集，仅用3万条数据就能训练出优于官方大规模模型的LLM，为开源和低资源环境LLM对齐提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐数据集大多私有或昂贵，制约了复现与规模扩展，且数据需求量巨大并且不清楚最低有效数据规模。本研究旨在降低数据门槛，实现高效对齐并突破资源限制。

Method: 创建了一套名为PiKa的高效对齐数据集，仅使用约3万条高质量SFT示例，并通过微调主流模型（如Llama-3-8B-Base和Qwen2.5系列）进行评估。

Result: 使用PiKa-SFT仅3万条示例微调模型，实际效果超越了在千万级私有数据上训练的官方模型（如Llama-3-8B-Instruct），并在多个主流评测基准上取得领先表现。还在多个模型上验证了数据效率和可扩展性。

Conclusion: 高质量的LLM对齐效果可以通过更少的数据实现，PiKa数据集证明了这一点，为开源和资源有限社区提供了可扩展路径。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [54] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 该系统通过智能判定何时生成简明笔记，结合人工反馈优化模型，有效缩短案件处理时间、提升客服满意度，尤其在复杂场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 客服人员在处理客户咨询时需要频繁切换上下文和重复审查对话内容，传统批量摘要方法会增加工作负担并影响效率。

Method: 将微调后的Mixtral-8x7B模型用于持续笔记生成，并结合基于DeBERTa的分类器过滤琐碎内容；客服人员的编辑行为用于线上优化和离线模型再训练，形成反馈闭环。

Result: 生产部署后，整体处理时长减少3%，复杂案件最高减少9%，客服人员满意度高。

Conclusion: 增量式摘要系统能够提升摘要质量并显著提高客服人员的工作效率和满意度。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [55] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 本文提出针对机器翻译的提示优化新方法，利用小模型和反向翻译，大幅降低训练成本且效果优异，适用于其他下游任务。


<details>
  <summary>Details</summary>
Motivation: 当前多数提示工程方法主要针对LLMs中的指令部分进行优化，但在机器翻译等自然语言生成任务中，输入部分更为关键，现有方法在这类任务上的适用性有限。

Method: 提出一种新型的针对机器翻译任务的提示优化方法。该方法利用小参数模型，并采用反向翻译训练策略，大幅降低了单任务优化的训练成本。

Result: 方法有效提升了机器翻译任务的性能，同时容易迁移应用于其他下游任务。

Conclusion: 本研究为机器翻译等NLG任务的提示优化提供了低成本且高效的新路径，对传统依赖大模型和侧重指令优化的方法进行了有益补充。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [56] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: 人类和LLMs在逻辑推理过程中都会受内容影响。文章发现LLMs内部“合理性”与“有效性”高度对齐，导致内容偏差。通过表征干预可以显著降低偏差并提升推理能力，这为提高模型逻辑性的方向提供了依据。


<details>
  <summary>Details</summary>
Motivation: 在认知科学领域，人类处理推理问题时，内容效应（语义内容的合理性影响逻辑判断）已有充分解释，但大型语言模型（LLMs）中类似效应产生的机制尚不明确。作者希望探究LLMs内部如何编码“有效性”与“合理性”等抽象概念，以及这种编码是否导致内容偏差。

Method: 通过分析LLMs的内部表征，考察“有效性”与“合理性”如何以线性形式在表征空间中被编码和对齐。使用操控向量（steering vectors）实验验证合理性与有效性表征之间的因果关系，并构造去偏向量，测试其对内容效应和推理准确性的影响。

Result: 研究发现，LLMs中的合理性与有效性在表征空间中高度对齐，导致模型将两者混淆。合理性向量可以因果性地影响有效性判断，反之亦然。两者对齐程度可预测模型内容效应的表现。通过构建去偏向量，这两者得以解耦，显著减少内容效应，提高推理准确率。

Conclusion: 本文揭示了LLMs中抽象逻辑概念的底层表征及其偏差机制，同时提出通过表征干预减少偏差、提升模型逻辑推理能力的有效路径。

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [57] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文针对大型语言模型工具使用中受限于上下文长度的问题，提出基于LLM摘要的上下文压缩和端到端优化RL方法（SUPO），能显著提升长期多轮任务成功率，实现上下文长度突破，对复杂任务效果更佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮、长周期工具使用任务中受限于上下文长度，传统RL方法带来高成本、易影响指令遵循、上下文硬限制等问题，因此需要解决如何高效管理上下文和保持性能。

Method: 作者引入基于LLM生成摘要的周期性压缩工具历史，将任务相关信息浓缩为紧凑的上下文，从而让RL agent能够扩展超出固定上下文窗口。借助这一框架，提出了政策梯度表示，使RL可以端到端优化工具使用和摘要策略，并通过名为SUPO的算法实例化该框架。

Result: SUPO算法在交互式函数调用及复杂搜索任务中，相比基线方法显著提升了成功率，在多轮摘要扩展设定下进一步提高了表现，在不增加有效上下文长度的基础上，对复杂任务也具有更佳的扩展能力。

Conclusion: 论文提出的基于摘要的上下文管理方法能够有效突破大语言模型工具使用中的上下文长度限制，在强化学习下实现长周期、多轮交互任务，且能够显著提升任务成功率，同时保持较短的有效上下文。该方法具有可扩展性且对复杂任务同样有效。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [58] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 该论文提出了一种基于动态释义生成的新型句子嵌入模型评测方法，可以更真实反映模型在实际环境下的表现，并发现模型对词汇改写非常敏感，但不同尺寸模型受影响程度相似。


<details>
  <summary>Details</summary>
Motivation: 现有句子嵌入模型评测过度依赖静态测试集，导致模型对真实语境下鲁棒性评价失真。希望用动态评测提升真实性和科学性。

Method: 提出并使用动态生成释义的基准（PTEB），通过LLM生成语义一致但词语多样的释义，在多个任务和多语言集下进行多轮聚合实验。

Result: 验证即使语义保持，编码器对词汇变化敏感；大模型和小模型受影响差别不大，实验涵盖多语言数据，结果统计显著。

Conclusion: 动态基准（PTEB）可以更真实衡量句子嵌入模型的鲁棒性，且模型大小对表现影响并不明显。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [59] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: RAF是一种通过优化token、保持提示语自然性的对抗方法，能够显著提升目标内容在LLM重排序中的排名。研究表明，这暴露了基于LLM的检索系统在安全和可信性上的新隐患。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）越来越多地被用于信息检索中的重排序任务，但它们对小而自然的提示非常敏感，容易被操控。本文旨在揭示这一潜在的安全漏洞。

Method: 提出了Rank Anything First (RAF)方法，并分为两阶段：第一阶段利用Greedy Coordinate Gradient结合可读性得分筛选候选token；第二阶段采用基于熵的动态加权方案对候选token进行排名和可读性损失评估，通过温度控制采样选择最佳token。整个过程以最大化目标排名和保持语言自然性为目标，逐步生成提升目标排名的“自然提示语”。

Result: 在多种大语言模型上的实验表明，RAF方法能以自然语言显著提升目标条目的排名，效果优于现有方法，同时保持更好的自然性和鲁棒性。

Conclusion: 论文明确指出，大语言模型作为重排序工具，存在易受对抗性操控的安全隐患。研究提出的新方法不仅提升了目标排名，还具有更强的自然性和可检测性挑战，揭示了现代检索系统在可信性和鲁棒性方面的新问题。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [60] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出基于权重矩阵快速指纹识别大语言模型的方法，能有效识别复杂后处理下的型号溯源，实验结果显示几乎无误判、速度快，并已开源实现。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型（LLM）训练成本高昂，模型知识产权保护变得非常重要。模型所有者和第三方急需判断某个模型是否为原创训练，还是基于现有模型演化而来。然而，模型在发布前通常会进行如微调、再预训练、多模态扩展等复杂后处理，这使得模型溯源和归属判断极具挑战。

Method: 提出了一种无需重新训练即可指纹识别模型的方法，基于权重矩阵，通过线性分配问题（LAP）算法与无偏的Centered Kernel Alignment（CKA）相结合，消除参数变化带来的影响，从而获得稳健性极高的模型相似度度量。

Result: 在构建的测试集（60对正类和90对负类模型对）上，该方法在所有六类常见模型后处理流程中都表现出极强的稳健性，几乎无假阳性，所有分类指标得分均为满分。整个流程在NVIDIA 3090显卡上仅需30秒完成。

Conclusion: 本文方法为可靠的模型溯源和版权归属提供了高效、准确、泛化能力强的技术方案，有望成为行业标准。代码已开源。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [61] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出了一种无训练、无标签、可扩展的短文本聚类方法，在不同数据集和嵌入器下表现优异，适合真实客服等大规模应用，计算成本低，无需标签和先验知识。


<details>
  <summary>Details</summary>
Motivation: 在实际商业中，面对大量没有标注的用户对话，现有聚类方法依赖标签或需要提前知道类簇数，操作繁琐且不适用。作者希望提出一种无需训练和标注、并且可广泛适用于实际大规模客服场景的聚类方法。

Method: 提出了一种无训练、无标签的短文本聚类方法。核心是基于任意文本嵌入向量，通过代表文本构建稀疏向量，并利用大语言模型（LLM）指导进行迭代式向量更新，从而不断优化聚类结果。该方法对底层嵌入器和聚类算法无依赖，模型无关，并能用较小的LLM完成。

Result: 在多个不同数据集和较小LLM下，方法在不需要预先知道聚类数和标签情况下，取得了与当前最先进对比学习方法相当或更优的表现，并且能很好扩展到大规模数据集，降低了大模型推理代价。

Conclusion: 该方法更贴合实际无标签、低资源及规模化应用场景，具备模型无关、资源需求低、计算成本小、可扩展性好的优点，相比现有聚类方法更适合真实场景。

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [62] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出面向流利度的多参考语法纠错评价框架，支持多语言场景，提出并分析了四种n-gram聚合策略，在多语言数据集验证优越性，促进公平反映多种合法人类改写的评价体系发展。


<details>
  <summary>Details</summary>
Motivation: 现有的语法纠错评价体系多基于英语，且依赖系统与参考之间严格的编辑对齐，导致在多语言和生成式场景下适用性受限，无法合理反映合法修改的多样性。

Method: 提出了基于流利度的多参考评价正式框架，将n-gram相似度建模为对多种合法纠错结果的聚合问题，并以GLEU为例，提出了四种聚合策略：选取最佳、简单平均、加权平均和合并计数，系统分析了其有界性、单调性及对参考多样性的敏感性。

Result: 在捷克语、爱沙尼亚语、乌克兰语和中文数据集上的实验证明，这些聚合策略能够互补性地捕捉流利度和覆盖面。该框架能够整合多参考流利度评价，在不惩罚合理多样性的前提下，科学支持多语言环境下的评价需求。

Conclusion: 该文统一了多参考、多语言语法纠错评价的标准，以流利度为导向，兼顾语言多样性，为生成型语法纠错领域提供了更公平合理的度量方法。

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [63] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级且无须训练的推理优化策略，可根据输入需求在单一模型内调节推理复杂度，降低计算成本而不损失推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型（LRMs）在结构化任务中表现优异，但存在“思虑过度”问题，造成性能下降和资源浪费。如何优化模型部署和推理效率成为关键。

Method: 采用奇异值分解分析，确定最佳低秩投影，对LRM推理能力进行调节，实现动态开关模型部分能力，达到资源优化和推理保真。

Result: 提出了一种无训练、轻量的调节方法，通过分析奇异值的累积能量，实施低秩投影，在推理阶段有选择地让LRM“遗忘”，实现计算资源的缩减且保留必要的推理能力。

Conclusion: 通过该策略可在保留必要的深度推理能力同时，显著降低计算资源消耗，解决了LRM在实际部署时的能效和成本问题。

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [64] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 论文提出自适应神经符号推理框架，自动识别并选用最佳形式推理策略和求解器，大幅提升推理准确率并优于主流基线，为统一复杂推理任务提供新途径。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号NLP方法主要是将大型语言模型与形式逻辑求解器集成，但大多为静态集成，缺乏推理策略的多样性。这限制了适应不同自然语言问题所需的多种形式推理手段。

Method: 提出了一种自适应、多范式的神经符号推理框架。该框架能够自动识别自然语言问题所需的形式推理策略，并通过自动形式化接口动态选择和调用专门的形式逻辑求解器。

Result: 实验显示，LLM在预测所需推理策略方面准确率超过90%，框架整体性能优于GPT-4o和DeepSeek-V3.1，分别高出27%和6%。此外，纯LLM方法结合自适应推理后在不同设置下也有明显提升。

Conclusion: 本研究为自适应LLM-符号推理奠定了基础，推动了在异构推理任务中实现实质推理与形式推理统一的方向。

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [65] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文系统研究了LLM结构化知识抽取的可终止性、可复现性和鲁棒性。结果显示该过程总体可靠，但仍受模型和输入等因素影响，提示今后需进一步提升其健壮性和一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）拥有大量事实性知识，但如何有效测量和系统化这些知识仍具挑战性，特别是将其转化为结构化格式的潜力尚未充分探索。作者关注递归式知识抽取方法（如GPTKB），并希望探究其可行性和局限性。

Method: 采用miniGPTKB（面向特定领域、可处理的子知识爬取），系统分析知识抽取过程的终止性、可复现性和鲁棒性。设计四种变量（种子、语言、随机性、模型变化）和三个领域（历史、娱乐、金融），评估知识抽取结果在产出数量、词汇相似性和语义相似性三类指标上的表现。

Result: （i）知识抽取过程有较高的终止率，但依赖于具体模型；（ii）可复现性表现不一；（iii）鲁棒性因扰动类型而异，对种子和温度参数较高，但对语言和模型变化较低。

Conclusion: LLM知识抽取能够稳定提供核心知识，但对输入语言和模型差异敏感，存在一定局限性。为今后系统化LLM知识提取和结构化提供了基础和参考。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [66] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: 本文提出FURINA-Builder，实现灵活自动化的角色扮演基准构建，打造新基准FURINA-Bench。评测显示推理型LLM在提升RP表现的同时伴随幻觉增加，性能与可靠性存在明显权衡关系。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在角色扮演任务上的进步，现有的基准测试因范围狭窄、交互方式过时和适应性有限而逐渐失效，缺乏灵活且适用于多种应用场景的评测方法。

Method: 提出FURINA-Builder，多智能体协作管道，可自动构建全可定制的角色扮演任务基准，覆盖任意规模。该系统通过模拟被测角色与其他角色的对话，LLM充当评判，自动选择评估维度并调整被测角色应答，从而自动生成评测数据。并据此构建了新基准FURINA-Bench；通过人类评估和可分性分析验证设计有效性。

Result: 利用FURINA-Bench对新一代LLM进行评测，发现o3在英文任务、DeepSeek-R1在中文任务表现最佳。现有角色表现普遍优于合成角色，推理能力进一步扩大这一差距。模型规模增长未必减少幻觉现象。重要的是，推理型LLM存在新型权衡：推理能力提升RP表现但也增加幻觉率，展现出性能与可靠性之间的Pareto前沿曲线。

Conclusion: FURINA-Builder有效支持灵活的RP基准测试构建，FURINA-Bench能精准揭示主流LLM的性能特征和挑战。推理与幻觉之间的权衡值得进一步研究。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [67] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: 本文构建了自动生成抄袭数据集并开展检测任务，现有基线方法虽能识别新型生成抄袭，但在旧数据集上效果差，泛化能力不足，仍需方法创新。


<details>
  <summary>Details</summary>
Motivation: 目前人工智能生成文本的能力提升，使得科学论文中的自动生成抄袭问题变得突出，缺乏合适的数据集和有效检测方法。

Method: 作者构建了一个大规模基于三种主流大语言模型（Llama、DeepSeek-R1、Mistral）生成抄袭文本的数据集，并组织了PAN 2025检测任务，比较了参赛系统与四个基线方法的表现。同时，将这些方法在PAN 2015任务的数据集上进行了评测。

Result: 简单的基于语义嵌入向量的相似度方法在新数据集表现良好，召回率高达0.8，精确率为0.5；但在2015年数据集上表现显著下降，说明其泛化能力有限。

Conclusion: 现有方法能检测自动生成的抄袭文本，但缺乏泛化能力，检测历史数据表现较差，需要开发更具有普适性的方法。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [68] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 论文研究如何通过集成（并行和串行）不同电路定位方法，提高在大语言模型中的电路定位准确性。实验结果显示，集成方法优于单一方法，尤其综合并行与串行集成效果最佳，并在多个任务上超过了官方基线。


<details>
  <summary>Details</summary>
Motivation: 目前在大语言模型（LLM）中，定位负责特定任务行为的子网络（circuit localization）的方法不断发展。单一方法可能存在局限，论文探索是否通过集成多种方法可以提升性能。

Method: 研究了两种集成方法：并行集成（通过对不同方法的归因分数如均值、极值等进行组合），以及串行集成（利用EAP-IG方法的结果作为更精确但计算昂贵的edge pruning方法的初始值）。在最终实验中，还将并行集成应用于包括串行集成在内的多种方法。

Result: 实验表明，两种集成方法都能显著提升评测指标，提升了电路定位的精度。综合并行和串行集成能获得最佳效果，且在多个模型和任务组合上，集成方法的成绩优于官方基线。

Conclusion: 通过集成多种电路定位方法（并行或串行），能够有效提升大语言模型中的子网络定位精度，为相关任务的自动化分析提供更强工具。

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [69] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: 提出了一种用模拟工具轨迹训练工具增强语言模型的新框架MTR，无需实时API即可学习强大的工具推理能力，实验效果媲美甚至优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具的语言模型表现强大，但在训练和部署时对实时API访问的依赖带来了扩展性和可靠性的问题。为了解决这一困难，需要一种不依赖实时API的方法。

Method: 提出了一种名为MTR的训练框架，通过多代理架构模拟工具增强推理过程。该框架包括ToolMaker生成工具接口、AutoAgent生成结构化推理序列、ToolActor模拟工具响应，并采用两阶段训练：第一阶段用全推理轨迹进行监督微调，第二阶段通过群体相对策略优化结合复合轨迹奖励优化策略。

Result: 在四个多跳问答基准（HotpotQA、MuSiQue、2WikiMultiHopQA、Bamboogle）上，MTR取得了与实时API系统相当的EM分数，并在需要复杂推理的任务上表现突出。

Conclusion: MTR框架能够在无需依赖实时API的情况下，通过结构化轨迹学习实现有效的工具推理，具有良好的扩展性和可靠性。

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [70] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 本文首次系统梳理了大型语言模型的mid-training训练中间阶段，提出分类体系，总结方法优势、评测标准与实际提升，阐述理论支撑，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 虽然中间“mid-training”阶段已广泛应用于大型语言模型，但目前缺乏对这一统一范式的系统性综述。本文旨在填补这一研究空白，阐明mid-training的机制及其对模型性能的提升作用。

Method: 文章提出对大型语言模型mid-training阶段进行系统性分类，包括数据分布、学习率调度、长上下文扩展等维度，并总结实用经验、评测基准，还分析了mid-training的有效性理论基础，如梯度噪声尺度、信息瓶颈和课程学习思想。

Result: 本文发布了首个LLM mid-training阶段的分类体系，汇总了实际应用经验及评测指标，并报道了通过mid-training获得的性能提升，为模型间结构化对比提供了参考。同时挖掘了尚待解决的挑战，并为未来研究提出了方向。

Conclusion: mid-training阶段不仅能提升模型泛化与抽象能力，还能解决训练后期的收敛与噪声问题，是大型语言模型发展的关键环节。该工作为mid-training的标准化和进一步完善提供了理论与实践基础。

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [71] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 本文提出了一个覆盖广泛、多语言对齐的大型挑战集，用于系统测试和分析机器翻译自动质量评估指标在职业术语性别歧视上的偏见表现，有助于推动该方向更深层次研究。


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译系统的性别偏见已有较多研究，但质量评估（QE）指标中的性别歧视问题尚缺乏系统探讨。现有分析多受资源规模、职业覆盖和语言多样性的限制，因此亟须构建更完整、系统的数据集来研究这一问题。

Method: 作者基于现有GAMBIT语料库（包含英语的性别模糊职业术语），将其扩展到三个无性别或自然性别的源语言与十一种有语法性别的目标语言，生成33组语言对。每条源句配对生成一组目标句（仅职业词性别不同），从而可检测QE分数对性别的公平性。所设计数据集完全平行，便于跨语言、跨职业细分析。

Result: 本研究发布了涵盖三个源语言、十一种目标语言（共33组语言对）、并严格对齐的挑战数据集，为检测和分析QE指标中有关职业性别偏见提供了强大实验基础。

Conclusion: 研究构建了一个大规模、多语言的挑战集，有助于深入分析机器翻译质量评估（QE）指标中的性别偏见，尤其能支持对职业词汇语境下性别歧视的细致研究。该数据集为今后评估和改进QE系统的公平性提供了基础。

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [72] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 本论文提出通过关注模型自身生成时的自信号（如置信度与注意力），大幅提升多大语言模型辩论的准确率和效率，比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的多大语言模型（LLM）辩论方法主要依赖于外部结构（如辩论图或LLM评判员），很少利用模型自身在生成过程中的自信号（如token logits和注意力机制），导致计算冗余和性能下降。

Method: 提出了一种基于自信号驱动的多LLM辩论方法（SID），结合模型级置信度和token级语义关注，自适应地引导辩论流程。高置信度的代理可以提前退出辩论，且可利用注意力机制压缩冗余内容。

Result: 在多个主流LLM和多模态LLM、具有挑战性的基准测试上，SID方法在精度上优于现有多LLM辩论（MAD）技术，并显著减少了token消耗，提高了系统效率。

Conclusion: 基于自信号驱动的多LLM辩论机制能有效提升多代理辩论系统的准确率和计算效率，应用前景广阔。

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [73] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: 本文提出泰英双语开源大模型 OpenJAI-v1.0，在指令执行、长上下文理解和工具使用表现突出，优于现有泰语开源模型，且无遗忘问题，为泰国AI 社区提供了新的NLP工具。


<details>
  <summary>Details</summary>
Motivation: 泰语和英语领域开源大语言模型资源有限，尤其需要提升实际任务中的模型表现。旨在为泰国AI社区提供更多可用的NLP资源。

Method: 以 Qwen3-14B 为基础，通过精心挑选的数据在指令跟随、长文本理解和工具使用三大应用场景上进行优化，并对模型效果进行评估。

Result: OpenJAI-v1.0 在多个评测基准上表现优于其他开源泰语模型，并且提升了基础模型的能力。

Conclusion: OpenJAI-v1.0 成功提升了 Qwen3-14B 基础模型的能力，且优于其他主流开源泰语模型，并且没有出现灾难性遗忘问题。

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [74] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 本文通过质量感知解码方法（QAD）提升了大语言模型在机器翻译中语篇现象的处理能力，实现了更符合人类偏好的高质量翻译。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在机器翻译中的语篇现象（如指代消解和词汇衔接等）处理能力不足，因此有必要探索提升其处理语篇问题的解码方法。

Method: 通过深入分析LLMs在上下文感知翻译时的语篇现象表现，提出并实验了质量感知解码（QAD）方法，并与其他解码方式进行对比。

Result: 实验表明：QAD不仅能更好地萃取隐含于LLMs中的语篇知识，还能提高译文语义丰富性和人类偏好的一致性。

Conclusion: QAD解码方法能够有效提升大语言模型在语篇现象处理上的表现，生成更加丰富且符合人类偏好的翻译结果。

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [75] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: 本研究提出一种可自适应学习token偏好的λ-GRPO新方法，在多个大模型和推理任务中提升明显，解决了GRPO的长度偏差，且无需增加算力或数据修改。


<details>
  <summary>Details</summary>
Motivation: 当前利用人类反馈的强化学习（RLHF）方法在提升大语言模型（LLMs）推理能力上表现突出，但最新的基于可验证奖励（RLVR）的方法如GRPO面临响应长度偏差问题。现有改进方式（如DAPO和Dr.GRPO）侧重于启发式调整，缺乏解释性。因此，亟需找到一种更自适应且可解释的方法来解决长度偏差，并提升自身效果。

Method: 提出一种可学习的参数λ，通过λ-调控各token在损失聚合过程中的权重，让模型在优化过程中自适应地学习token偏好。该方法统一了现有GRPO、DAPO等方法的公式框架，并用λ-GRPO表示，整个训练流程无需更改数据或增加计算量。

Result: 在多个数学推理基准测试上，λ-GRPO在Qwen2.5（1.5B/3B/7B）模型上分别比GRPO提升了1.9%、1.0%、1.7%的平均准确率，且无需更改训练数据或增加计算成本。

Conclusion: 通过引入学习型token偏好参数λ，λ-GRPO有效缓解了GRPO的长度偏差问题，且保持高效和实际可用性，相比现有方法获得了更优的性能提升。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [76] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MeXtract是一组基于Qwen 2.5微调的轻量级语言模型，用于科学文献元数据抽取，在MOLE基准上实现了最新最优表现，能高效处理跨domain和schema的任务，支持出域泛化，代码和数据全部开源。


<details>
  <summary>Details</summary>
Motivation: 现有的规则或任务专用模型难以跨领域和schema泛化，科学文献元数据抽取面临精准性和效率挑战。需开发更强泛化能力和适应性的抽取方法。

Method: 通过fine-tuning Qwen 2.5系列模型，开发参数量为0.5B至3B的轻量级语言模型家族，并在MOLE基准进行元数据抽取实验。扩展MOLE基准，实现模型特定元数据和出域挑战子集评测。

Result: MeXtract在各种参数规模下于MOLE基准实现了最优效果。fine-tuning于特定schema后不但准确率高，也能有效迁移至未见schema。所有代码、数据和模型均已开源。

Conclusion: MeXtract模型在科学文献元数据抽取任务中表现优越，不仅能高精度抽取指定schema，还能很好地泛化到未见schema，具有鲁棒性和适应性。

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [77] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出长上下文奖励模型评估基准与多阶段训练方法，有效增强奖励模型在复杂上下文下的鲁棒性，新方法小模型性能超越大模型基线，与行业顶尖专有模型相当。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型多关注短上下文，且评估主要集中在回复级别属性，忽视了在长上下文下回复与上下文的一致性。随着大模型应用于需要处理长历史轨迹的任务（如智能体），迫切需要能评估模型在长上下文中的表现。因此，作者希望解决奖励模型在长上下文情境下的一致性评判问题。

Method: 作者提出Long-RewardBench，一个面向长上下文奖励模型评估的基准，包括Pairwise Comparison和Best-of-N任务；还提出了通用多阶段训练策略，以增强奖励模型在长上下文下的稳健性，形成Long-context RMs（LongRMs）。

Result: 实验证明，现有SOTA生成型奖励模型在长上下文场景下表现脆弱，难以做出一致的偏好判断。而作者的训练策略能够显著提升模型在长上下文评估中的表现，并保持短上下文能力。提出的8B LongRM甚至超越了规模更大的70B模型，并能匹敌Gemini 2.5 Pro。

Conclusion: 长上下文奖励模型评估是当前对齐领域的关键痛点。通过Long-RewardBench与多阶段训练策略，本文有效提升了奖励模型在长上下文下的一致性，从而推动了大模型与人类偏好更紧密的对齐。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [78] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS实现了模型在语音对话中边听边思考，能及时打断错误并提前完成任务，大幅提升了语音交互的实时性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）和语音语言模型（SLM）只能在用户说完后才开始思考和行动，这导致不能在用户讲话过程中互动，而且响应延迟较高。对于需要实时、低延迟交换的语音对话来说，这种机制并不理想。作者注意到人类在听的同时也会思考，试图让模型具备类似能力。

Method: 提出SHANKS推理框架，将用户输入的语音按固定时长分片，在接收到每个片段时，模型即开始基于已有语音和推理进行未发声的链式思考，同时用户继续讲话。SHANKS依据这些推理决定是否打断用户，以及是否调用工具完成任务。

Result: 在用户一步步解决数学题时，SHANKS可在用户犯错时准确打断，打断准确率比无思考打断的基线高37.1%。在工具辅助对话中，SHANKS可在用户说完前完成56.9%的工具调用。

Conclusion: SHANKS让模型能在用户讲话过程中持续思考，而不是等用户说完后才行动，提高了语音交互的实时性和智能性。此研究推动了对话模型向全程思考与互动发展。

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [79] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: 本文提出并开放了ASR系统排行榜，标准化评测方法并透明公开数据代码，系统性比较了英文、多语言及长文本ASR的准确率与效率，为技术开发和应用选择提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别（ASR）评测主要集中于英文短文，且很少关注系统效率问题。现有评测缺乏多语言和长文本方面的系统性比较。

Method: 作者开发了Open ASR Leaderboard，这是一个可复现的基准和互动排行榜，涵盖60多个开源及商业ASR系统，在11个数据集（包含多语言和长文本）上进行了对比评测。统一了文本归一化标准，评测指标包括字错误率（WER）和逆实时因子（RTFx），实现了准确性与效率的公平比较。

Result: 英文学录方面，Conformer编码器与大型语言模型（LLM）解码器组合取得最高平均准确率（最佳WER），但速度较慢；CTC和TDT解码器虽准确率略低，但效率高（优异RTFx），更加适合长文本及离线应用。Whisper派生模型专注英文后提升了准确率，但减少了多语种覆盖。

Conclusion: 作者开放了所有代码和数据加载器，推动ASR评测的透明与可扩展性，并为准确率和效率的公平比较树立了新标准。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [80] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 提出并验证大语言模型能够根据教育标准和学生个性化需求生成高质量数学应用题，模型性能优异、数据集创新，师生均认可其应用价值。


<details>
  <summary>Details</summary>
Motivation: 数学应用题（MWPs）在K-12教育中至关重要，针对不同学生兴趣和能力定制MWPs有助于提升学习效果，但教师因工作负担大难以做到个性化定制。

Method: 研究提出利用大语言模型（LLM）自动生成符合学生兴趣和数学教育标准的MWPs，并采用人类专家与LLM联合评审方式对比评估11,000多个由公开及闭源LLM生成的MWPs，同时建立首个教师标注的、符合教育标准的MWP生成数据集。

Result: 基于新数据集训练的12B开源模型在性能上达到更大开源模型的水平，还可利用教师标注数据训练文本分类器，使30B开源LLM无需专门训练便超越现有闭源基线模型。此外，新模型生成的MWPs在风格和内容上较现有模型更接近人类作品。学生实测显示，其对模型生成且个性化的MWP表现与人工题目相当，并更偏好定制化内容。

Conclusion: LLM不仅能生成高质量、符合教育标准且个性化的数学应用题，并得到教师与学生认可，有望减轻教师负担并提升学生学习体验。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [81] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: 本文系统评估了十个中文大模型在对待不同社会身份群体时的偏见现象，发现不仅合成测试中存在内群体优先、外群体贬低的问题，这一现象在真实对话中更为突出，提示实际部署需要高度关注模型在社会语境下加剧偏见的风险。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在用户应用中的广泛部署，其反映和放大社会偏见的风险日益受到关注。以往对偏见的研究多针对英语环境，缺少对中文LLMs的系统性分析。作者希望揭示中文环境下的社会身份偏见问题。

Method: 作者针对十种具有代表性的中文大模型，设计了中文独有的、与社会身份相关（如“我们”和“他们”）的提示词，并覆盖中国社会中240类社会群体，评估模型对内群体和外群体的态度。同时，分析了大规模真实用户与聊天机器人之间的中文对话语料，以补充受控实验。

Result: 无论是在合成的提示词还是自然的对话语料中，模型都表现出系统性的“内群体正面、外群体负面”偏见。而且在真实用户互动中，这种偏见的动态甚至可能被进一步强化。

Conclusion: 本研究提出了适用于中文LLMs的社会身份偏见评估体系，发现英语中已有记录的偏见问题同样适用于中文，且在面向用户的真实语境下偏见更易加剧。

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [82] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 法律领域的RAG系统容易因文档级检索错误导致效果下降。通过对每个文本块注入文档摘要的SAC方法，能够有效减少检索错误并提升系统性能，且通用型摘要方案表现优于专家方案。这项技术简单可扩展，能大幅提高法律领域RAG的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在法律领域，检索辅助生成（RAG）有助于减轻LLM生成幻觉的问题，但其效果高度依赖于检索步骤的准确性。然而，由于法律文档高度结构化且相似内容众多，现有检索系统常常出现从错误源文档检索信息的失误。

Method: 提出并量化了一种关键失败模式：文档级检索不匹配（DRM），并通过“摘要增强分块（SAC）”技术，在每个文本块中添加文档级综合摘要，为分块过程注入全局背景信息。

Result: 在多种法律信息检索任务上，SAC显著减少了DRM，并提升了文本级检索的准确率和召回率。意外发现通用型摘要策略优于结合领域知识的法务专家摘要方案。

Conclusion: SAC是一种实用、可扩展且易于集成的技术，能够在大规模法律文档集上提升RAG系统的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [83] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 通过“人类参与+合成扩充”流程，作者构建了高质量、兼顾多样性与文化细节的印度语言数据集，显著填补了多语言大模型训练的资源短板。


<details>
  <summary>Details</summary>
Motivation: 现有大模型后训练数据缺乏多语言、文化基础和任务多样性，尤其印度语言资源极度稀缺，影响模型效果。

Method: 提出了“人类参与”的流程，综合翻译和合成方法生成高质量、多样化的印度语言后训练数据。按照细致协议，强调任务多样性、多轮对话、指令一致性、安全性和文化细节。

Result: 构建了两个数据集：Pragyaan-IT（2.25万条）和Pragyaan-Align（10万条），涵盖10种印度语言、13大类和56小类，汇聚57个多样化基础数据集。

Conclusion: 这些数据集为多语言大模型特别是印度语言模型的包容性和有效性提供了更牢固的基础。

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [84] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种效率高、准确率强的新型混合注意力架构NHA，将线性与全局注意力有效结合，显著优于Transformer和其他混合方法，并能高效改造预训练大模型，适合实际落地。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer虽然在序列建模方面表现优异，但由于其注意力机制的计算复杂度为二次型，在处理长上下文时效率较低。而线性注意力虽提升了效率，却常常牺牲了对长距离依赖的召回准确率。本文旨在提出一种能兼顾效率与召回准确率的新型混合注意力机制。

Method: 本文提出了Native Hybrid Attention (NHA)架构，将线性与全局注意力在层内部与层之间进行融合。具体做法是：用线性RNN维护长期上下文的 key-value slots，并通过滑动窗口引入短期 token。所有 key 和 value 上应用一次 softmax attention，无需额外融合参数即可实现基于 token 和 head 的加权。层间行为通过滑动窗口大小这一超参数控制，可在纯线性与全局注意力之间平滑切换，同时保持层结构统一。

Result: 实验结果表明，NHA在需要高召回率和常识推理的任务上表现优于Transformer以及其他混合基线方法。此外，预训练的大型语言模型（LLMs）可以用NHA进行结构上的混合改造，获得了有竞争力的准确率，并显著提升了效率。

Conclusion: NHA能实现效率与召回兼备的混合注意力机制，是一种易于扩展、高效且准确的新型架构，对现有Transformer和大模型具有实际部署价值。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [85] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文利用大规模新数据集深入分析了LLM事实性知识，发现其与标准知识库差异大、准确性不足且存在多种质量问题，为未来改进提出了研究方向。


<details>
  <summary>Details</summary>
Motivation: 事实性知识是LLM性能的重要驱动力，但目前对其理解有限且多依赖偏倚样本，亟需深入和系统化探索。

Method: 基于GPTKB v1.5（递归提取的上亿条GPT-4.1模型“信念”数据），系统分析了当前最强前沿LLM模型的事实性知识表现。

Result: 发现LLM在事实性知识上的表现与传统知识库相比差异大，准确性低于以往评测。同时，模型表现出较多的不一致、模糊和幻觉问题。

Conclusion: LLM模型在事实性知识方面存在与传统知识库显著差异，并且其准确率低于以往基准测试的指示。此外，不一致性、模糊性和幻觉等问题突出，需要后续研究关注。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [86] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 该论文首次系统梳理了代码切换领域LLMs的研究进展，总结目前的优缺点和挑战，强调要实现多语智能需改进数据、评测和模型设计，并提供了全面的资源链接。


<details>
  <summary>Details</summary>
Motivation: 代码切换（CSW）广泛存在于多语环境中，但即使是当前的主流大语言模型（LLMs）在处理混合语言输入时仍面临显著挑战：相关数据集稀缺、评测存在偏差等问题，这严重影响了CSW模型在多语社会的真实应用部署。

Method: 本论文以综述形式，系统梳理了CSW感知的大语言模型最新研究，涵盖五个研究方向、12项NLP任务、30多个数据集、80多种语言。作者还从模型结构、训练方法和评测机制等多维度对已知成果进行分类和分析。

Result: 梳理出LLMs在代码切换建模方面取得的进步及仍存在的难题，强调目前模型对混合语处理能力有限，呼吁建立更具包容性的数据集、公正的评测体系和更有语言学基础的LLMs；同时公开了所有文献及相关资源的整合合集。

Conclusion: 实现真正的多语智能仍需攻克数据、评测和模型设计等多重障碍，未来CSW领域应优先关注资源公平、评测公正和语言学深度的融合。

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [87] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: Search-R3让大语言模型能在推理过程中直接生成更有效的搜索嵌入，通过融合有监督学习和强化学习大幅提升检索表现，特别适用于复杂知识任务。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在检索任务中的潜力未被充分利用。本研究旨在探索如何使LLMs发挥更大的检索能力，尤其是在需要复杂推理和知识整合的任务情境中。

Method: 提出Search-R3框架：首先，通过有监督学习阶段训练模型生成高质量的搜索嵌入；其次，利用强化学习方法让模型在推理过程中优化嵌入生成；最后，设计专用的RL环境，高效处理嵌入表示的更新，无需每次训练都重新编码整个语料库。

Result: 在多项基准测试中，Search-R3 在结合推理与嵌入生成后，检索效果显著优于以往方法。

Conclusion: 该框架为知识密集型和复杂任务领域提供了大幅提升推理与信息检索融合能力的新途径，对相关应用有重要推动意义。

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [88] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: Sinclair收购地方新闻台后，这些台变得更关注国家和分化性话题，减少了本地新闻报道。


<details>
  <summary>Details</summary>
Motivation: 当地新闻台通常被认为是可靠且不带政治色彩的信息来源，尤其是在涉及居民关注的本地事务时。然而，Sinclair广播公司收购了许多地方新闻台，作者关注这些收购对新闻报道内容带来的影响。

Method: 用计算方法分析各地新闻台在被Sinclair收购前后的互联网发布内容，并与全国新闻媒体的内容进行比较。

Result: 发现被Sinclair收购后，地方新闻台报道国家新闻的频率明显增加，本地话题被相对忽视，并且对分化性的国家话题报道也有增加。

Conclusion: Sinclair收购后，当地新闻台趋向于更频繁报道国家新闻和具有争议的国家话题，取代了对本地事务的关注。

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [89] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: 本文针对印度语言提出ITEM基准，系统评测26种自动化指标与人工评判的对齐情况，发现LLM指标最优、异常值影响大、摘要与翻译任务指标关注点不同，并为指标设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前自动化评价指标主要集中在英语等高资源语言，导致印度语言的评估方法缺乏普适性，影响评价结果的可靠性。

Method: 提出ITEM基准，覆盖六种印度主要语言，系统性评估26种自动指标与人工评判对齐情况，包括细粒度标注、异常值敏感性、语言特定可靠性、指标相关性及对扰动的鲁棒性等多维考察。

Result: 发现四个核心结论：（1）基于LLM的评估指标在人类判断对齐上表现最佳；（2）异常值对指标与人工一致性影响显著；（3）在自动摘要任务中指标更倾向于捕捉内容保真性，在机器翻译任务中更能反映流畅性；（4）不同指标在鲁棒性和敏感性方面表现不同。

Conclusion: 该工作为印度语言评价指标设计与验证提供了关键参考，有助于推动低资源语言自动评估工具的发展和完善。

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [90] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 本研究通过跨语言数据构建（非机器翻译），提升了卢森堡语大模型的微调和生成能力，为低资源语言模型发展提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 卢森堡语等低资源语言因缺乏高质量的instruction数据集，导致其大语言模型微调（instruction tuning）难以提升，且传统的机器翻译方法会带来语义和文化偏差。

Method: 不通过机器生成翻译，而是利用从英语、法语和德语等高资源语言中对齐获得的数据，创建面向卢森堡语的跨语言instruction tuning数据集。

Result: 实验表明，跨语言instruction tuning不仅提升了不同语言间的表征对齐，还增强了模型在卢森堡语上的生成能力。

Conclusion: 跨语言数据整理避免了机器翻译的常见问题，有效促进了低资源语言大模型的发展。

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [91] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: 提出无训练自适应并行解码方法LocalLeap，大幅提升dLLM推理效率，解码速度提升约7倍，精度基本不损失。


<details>
  <summary>Details</summary>
Motivation: 扩散型大语言模型（dLLMs）在文本生成领域具有并行解码能力，但现有开源实现面临质量与速度的权衡，影响实际部署。保守的采样策略（如贪心解码）为了保证输出质量，只在每步解码最有信心的token，导致重复和冗余迭代，推理效率低下。

Method: 文章通过系统分析dLLM的解码动态，揭示了称为延迟解码的效率问题，进而提出一种无需额外训练的自适应并行解码策略——LocalLeap。LocalLeap基于两个经验原则：以高置信锚点为中心的局部确定性传播和渐进的空间一致性衰退。该方法定位锚点，在有边界的邻域内进行局部放松的并行解码，通过对已经确定的token提前做出承诺，从而减少冗余推理步骤。

Result: LocalLeap算法在多个基准测试上，实现了推理吞吐量提升6.94倍，解码步骤减少到仅为原需求的14.2%，且性能损失微乎其微。

Conclusion: LocalLeap有效解决了dLLM的延迟解码问题，实现了在保证质量情况下的高效并行解码，对实际部署的可用性有显著提升。

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [92] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: 本文发现现有评估方法无法有效识别LLM回答中的关键信息错误。作者提出VITALERRORS数据集和VITAL指标，其能更敏感地检测关键性错误，为LLM事实性评估提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 现有LLM事实性评估方法对所有信息一视同仁，忽略了关键信息错误带来的严重影响，导致评估结果不准确。迫切需要能区分关键信息和次要细节的评估方法。

Method: 构建了VITALERRORS基准数据集，包含6,733个带关键性错误或遗漏的修改过的LLM回答，并设计了新的VITAL指标，依据回答与查询的相关性和重要性进行评价。与现有评估方法进行了实验对比分析。

Result: 实验表明：现有方法对关键性信息错误不敏感，而VITAL指标可更好地检测出关键信息问题，实现更精细的事实性评价。

Conclusion: VITAL指标比以往方法能更可靠地检测出关键性信息错误，为更准确、稳健地评估大型语言模型的事实性奠定了基础。

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [93] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本研究提出结合大语言模型和检索模块的框架，实现更自然、更具讽刺表现力的语音合成，在多项指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 讽刺是一种复杂的非字面语言形式，在语音合成中很难实现，因为它依赖语义、上下文和语调等细微线索。目前的语音合成研究主要关注广泛的情感类别，而对讽刺表达关注较少。作者希望提升合成语音中讽刺表达的自然性和表现力。

Method: 提出了一种结合大语言模型（LLM）增强的检索增强框架，用于讽刺感知语音合成。具体做法包括：利用LoRA微调的LLaMA 3获取语义嵌入，捕捉讽刺语用的不一致和话语层线索；同时通过RAG模块检索语音韵律示例，提供讽刺表达的参考模式。两者结合于VITS主干网络，实现讽刺语音的双重条件生成。

Result: 实验结果显示，所提方法在语音自然度、讽刺表现力和下游讽刺检测任务中均优于基线方法，客观指标和主观评价均有提升。

Conclusion: LLM增强的检索式讽刺感知语音合成方法能够更好地合成自然且具讽刺性的语音，为非字面情感的语音合成开辟了新方向。

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [94] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: 提出了TALENT：结合小型视觉语言模型的OCR和自然语言叙述，协助大型语言模型进行表格VQA推理；显著降低算力同时提升性能，并发布了更具挑战性的新数据集ReTabVQA。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（VLM）虽然能直接从图像回答表格类问题，但需要极大的计算资源，且在细粒度细节上表现一般，不利于移动端部署。更轻量的方法虽然可以采用光学字符识别（OCR）配合大型语言模型（LLM），但数据结构对LLM并不友好，仍然存在较大误差。本文旨在解决表格VQA的高效与准确两大难题。

Method: 提出了TALENT框架，利用小型VLM生成表格的OCR文本及自然语言叙述，并将两者与问题一同输入LLM进行推理，将表格VQA问题重塑为以LLM为中心的多模态推理任务。同时，VLM只需履行感知和叙述角色，而不用作为整体解答者。此外，作者构建了ReTabVQA数据集，用于多步定量推理实验。

Result: 实验证明，TALENT框架通过协作式的小型VLM与LLM，可以在现有公开数据集与新构建的ReTabVQA数据集上，以大幅降低计算消耗的方式达到或超越单一大规模VLM的性能。

Conclusion: TALENT实现了表格VQA任务的轻量化和高效准确推理，为资源受限场景如移动端部署提供了新思路。框架设计与新数据集有效推动了表格VQA的发展。

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [95] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 提出了一套结合LLM上下文学习和元学习的系统，能有效处理有分歧的NLP任务，在LeWiDi竞赛中表现优异。消融研究显示，引入评分者示例和针对数据集微调等策略都对系统性能提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 许多NLP任务涉及主观性、歧义或标注者之间的合法分歧，因此需要能够有效建模这些人类变异性的方法。

Method: 提出了一个系统，利用大型语言模型（LLMs）的上下文学习能力，并结合两步元学习训练过程：1）在许多需要上下文学习的数据集上进行后训练，2）通过上下文元学习专门适应目标数据分布。同时进行消融实验，评估各系统组件的重要性。

Result: 系统在Learning With Disagreements（LeWiDi）竞赛的两项任务中均获冠军。消融实验表明：1）在上下文中引入评分者示例至关重要；2）针对特定数据集进行微调在大型数据集上有帮助；3）在其他上下文数据集上后训练对竞赛数据集有帮助；4）模型规模越大性能越好。

Conclusion: 通过利用LLMs的上下文学习和元学习策略，可以有效建模NLP任务中的标注分歧和人类变异性，各系统模块均对最终性能有关键影响。

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [96] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: 本文提出TRIM方法，无需梯度，只用注意力机制实现高效数据筛选，选出的核心数据集可超越全量数据微调，显著提升效率与性能，是构建微调数据集的新选择。


<details>
  <summary>Details</summary>
Motivation: 指令微调对于大语言模型（LLMs）适应下游任务至关重要，但传统方法依赖庞大且多样的语料库。高质量小数据集（coresets）能带来相当或更优的效果，但其筛选和构建困难且昂贵。现有方法多依赖于粗粒度、样本级信号（如梯度），但计算开销大且忽略细节特征。

Method: 提出TRIM（Token Relevance via Interpretable Multi-layer Attention）方法，通过仅前向、基于token的机制筛选高质量数据子集。与梯度方法不同，TRIM利用注意力机制从少量目标样本中匹配表示模式（“指纹”），高效识别对任务结构至关重要的样本。

Result: TRIM选出的高质量 coresets 在下游任务中超过现有最优基线最高达9%，部分场景甚至超过使用全部数据进行微调的表现。TRIM运行仅需正向计算（避免反向），大幅降低整体算力消耗。

Conclusion: TRIM为快速、高效构建高质量指令微调数据集提供了新思路，是现有方法的可扩展、高性价比替代方案。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [97] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: 本研究系统对比了人类与多种主流LLMs在七类复杂句子结构上的理解能力。结果发现，LLMs整体在高难度结构（如花园路径句）上易犯错误，表现与人类有收敛也有区别，对模型改进和认知研究具有参考价值。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）已经能够流畅地与人类对话，但它们在语言理解层面是否会像人类一样遇到处理难题仍未明确。作者希望通过系统对比揭示LLMs在句子理解上的表现及其与人类的相似性与差异性。

Method: 作者构建了统一的实验框架，收集了人类与五类最新LLMs在理解七种复杂句式上的数据。这些模型在规模与训练流程上各异，并重点测试对花园路径句（Garden Path，GP）和其他结构的理解能力。还对所有结构设计了更易处理的基线句以对比性能差距。

Result: 主流LLMs在复杂结构上的理解整体较弱，尤其对花园路径句困难明显。以表现最好的GPT-5为例，在非GP结构上接近完美（93.7%），但对GP结构准确率仅为46.8%。模型参数越多，其对结构难度的表现排名与人类越接近。与基线句对比，LLMs在人类表现差距较大的地方也有明显性能下降，只是极弱或极强模型则呈现出一致低或高水平。

Conclusion: LLMs在句子理解上既表现出与人类相近的难题应对模式，也表现出明显的分歧。研究揭示了LLMs和人类语言处理机制的部分收敛与差异，为进一步理解和提升LLMs语言能力提供了新视角。

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [98] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: 本文提出了一种新的层次文本分类框架RHC，结合大语言模型、链式推理和强化学习，使分类准确率提升，并具备自动生成解释的能力，广泛适用于多种领域，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 层次文本分类（HTC）任务在专利分类领域尤为困难，需处理复杂的领域知识和大量标签。现有方法仅能输出扁平标签，缺乏推理透明性，无法解释预测原因。

Method: 提出了Reasoning for Hierarchical Classification（RHC）框架，将HTC任务重构为逐步推理过程。首先冷启动阶段让大语言模型（LLM）输出连贯推理文本，随后通过强化学习提升多步推理能力。模型输出不仅标签，还用自然语言解释预测过程。

Result: 在实验中，RHC在准确率和宏F1上超越以往方法约3个百分点。模型规模越大，效果提升越显著。除专利分类外，在多个层次分类基准任务上都取得了SOTA表现。

Conclusion: RHC显著提升了层次文本分类的效果、解释性、扩展性和适用范围，尤其解决了传统方法推理不透明和泛化性差的问题。

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [99] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: 本文系统评估了多种数据构建方法对LLM数学推理能力的影响，发现数据结构化与高质量蒸馏优于简单扩容数据，给实际模型训练和部署提供了有价值的经验和指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多种下游任务中的推理能力很关键，但模型效果高度依赖于训练数据质量。目前已有多种数据构建方法被提出，但这些方法在真实场景中的实际应用价值尚需深入探索。

Method: 本文对开源数学推理数据集和数据合成技术进行了全面分析，并在统一的训练-部署管道下进行评估。此外，作者还提炼了有效的数据选择策略，并筛选出适用于工业应用的实用方法。

Result: 研究发现，将数据结构化为更易解释的格式，或者利用更强模型进行数据蒸馏，往往比单纯增加数据量更能提升模型能力。同时，本研究还为提升LLM推理能力的数据集成提供了有操作性的指导。

Conclusion: 结构优化和高质量蒸馏比一味扩大数据规模更有效，为成本效益高、可扩展的数据选取和模型增强提供了思路。希望本研究能推动社区进一步研究“更多数据”与“更优数据”间的平衡问题。

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [100] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: 本文首次推出针对护理的专业大型语言模型NurseLLM，以及配套的大规模数据集和评测标准。结果显示该模型在护理领域表现优异，强调了专用LLM的重要性，并提出推理和协作系统在护理中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在医学系统中应用广泛，但在护理这一专业领域还未被充分探索。针对护理领域特有的需求，需要开发专门的语言模型，以提升其专业性和应用效果。

Method: 提出了NurseLLM——第一个针对护理多项选择题任务的专业LLM。作者构建了多阶段数据生成流程，建立了首个大规模护理多项选择题数据集，并制定了多个护理领域评测基准。

Result: NurseLLM在多个护理领域基准测试上超越了同规模的通用及医学专用LLM，显示出在护理专业领域的显著优势。试验还探讨了推理能力与多智能体协作在护理领域的潜力。

Conclusion: 专业化的护理领域LLM对于提升护理任务的自动化和智能化至关重要，推理与多智能体协作系统也为未来相关研究和应用指明了方向。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [101] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 该文提出一种体系化方法量化心理测评问卷在LLMs中的数据污染，发现主流问卷已被严重污染，模型可能记住题目并调整答案，影响评估结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 最近有研究通过心理测量问卷来评估大型语言模型（LLMs）的心理特质，如价值观、人格、道德基础和黑暗特质。然而，已有工作担心这些问卷内容可能已污染训练数据，影响评估可靠性，但尚未系统性地量化这种污染程度。

Method: 作者提出了一个框架，从三方面系统测量心理测评问卷在LLMs中的数据污染：（1）题项记忆，（2）评价记忆，（3）目标分数匹配。他们将此框架应用于21种主流模型和4个广泛使用的心理测评问卷。

Result: 研究发现，诸如Big Five Inventory（BFI-44）和Portrait Values Questionnaire（PVQ-40）等流行问卷存在高度污染：模型不仅能够记住问卷题项，还可调整作答以达到特定分数。

Conclusion: 当前流行的心理测评问卷在评估LLMs时存在较强的数据污染，影响了评估结果的可靠性。作者的方法为后续更可靠的心理评估提供了参考。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [102] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: 本文首次提出针对输入方面标签不完备的场景，实现内容驱动的方面筛选与补全，并用相关方面数量预测提升摘要聚焦度，经实验证明LLM在该框架下表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有面向方面的摘要方法通常假设输入已经有预先定义好的方面标签，但真实场景下常出现方面标签不完整、不相关甚至完全缺失的情况，因此用户希望系统能根据文本内容动态调整方面。

Method: 作者提出了“内容感知方面改进摘要”(CARPAS)的新任务，在摘要前，基于文档内容动态调整和完善原始提供的方面。此外，作者构建了三组数据集，设计了四种提示策略对LLM进行任务实验，并提出预测相关方面数量的子任务，用该数量指导LLM生成摘要。

Result: 实验发现，LLM在无指导下倾向于输出过多（过全）的方面，导致摘要冗长且偏离主题。引入方面数量预测作为约束后，LLM生成的摘要更聚焦于核心，任务表现大幅提升。

Conclusion: 引入内容感知的方面动态调整，结合方面数量预测，可有效提升基于方面的摘要质量和实用性，对LLM实际应用具有重要指导意义。

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [103] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: 本文系统评估了GPT-2是否能够分辨人类可能和不可能的语言，发现其对两者的学习难度无显著差异，不具备人类语言的先天偏向，从而否定了LLMs与人类共享语言学习本能的观点。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具有人类语言学习的固有偏向，即是否能区分人类可能和不可能的语言。

Method: 采用前人提出的方式，通过对比LLM在实际语言数据集与通过扰动函数得到的“不可能语言”数据集上的学习曲线，并扩大了语言和扰动类型的覆盖面，主要使用GPT-2进行实验。同时，引入更宽松的判别标准，观察在整体可理解语言集和不可理解语言集之间是否有系统性区分。通过计算困惑度曲线的跨语言变异性等多种指标进行评估。

Result: 在大多数情况下，GPT-2对真实语言和其“不可能”变体的学习同样容易，与之前的结论相反。在更宽松的条件下，GPT-2在自然语言与人为构造的“不可能语言”之间也没有表现出系统性区分。跨语言各种指标的分析均显示不存在典型分隔。

Conclusion: 大型语言模型（如GPT-2）并不具备人类固有的语言学习偏向，其内部并未体现语言类型学中的人类本能偏向。这意味着LLMs与人类的语言习得机制存在重要差异。

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [104] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: 该论文提出聚焦乌干达多语言环境，开发了两款开源大模型，有效提升了乌干达大部分语言的理解水平，为减少语言障碍和推动低资源语言技术发展提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 非洲有超过2000种活跃语言，但多数未被现代语言技术覆盖。现有大型语言模型（LLMs）往往只支持使用人数最多的语言，造成其他语言能力薄弱。作者希望通过区域性方法，更有效地支持多样化语言。

Method: 针对乌干达这一高语言多样性的国家，作者基于Qwen 3开发了Sunflower 14B和32B两款开源模型，专注提升对乌干达大多数语言的理解能力。

Result: Sunflower 14B和32B在乌干达主要语言的理解能力达到业界领先水平，这些模型已开放源码，可直接用于多种实际应用，帮助消除语言障碍。

Conclusion: 区域性、聚焦本地多语种的策略能够更高效地推动低资源语言技术发展。本研究实例展示了通过定制化模型提升本地语言支持的可行性和重要性。

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [105] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 作者提出了一种用极少数据即可无训练识别和操控多语大模型跨语调控维度的新方法，使输出语言切换更高效且解释性强，性能优于传统方式。


<details>
  <summary>Details</summary>
Motivation: 现有多语种大模型在跨语言映射中表现优异，但其跨语转换机制尚不清晰。作者注意到模型在中间层将多语内容映射到英语，再在最终层回投到目标语言，因此猜测跨语机制由少数、稀疏、位置固定的维度主导，从而推动提出高效、解释性强的控制方法。

Method: 利用仅需50句数据（并行或单语），通过分析模型中从中间层到最终层的小而稀疏的语维度，定位并干预这些关键维度以实现语言生成的操控。整个过程无需模型再训练。

Result: 实验证明，通过对这些关键维度的干预，能在不改变语义的前提下切换输出语言，且显著优于以往基于神经元的方法，并且计算成本更低。

Conclusion: 提出了一种简单且无需训练的方法，可以识别并操控大型语言模型中控制跨语言切换的小而稀疏的维度。这种方法能高效地切换输出语言并保持语义一致性，效果优于之前基于神经元的方法。

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [106] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: 针对低资源非洲语言（以Kinyarwanda和Kikuyu为例），Whisper模型在50~200小时训练数据即可取得较好ASR效果；高错误大多由转录质量低引发，建议重视数据清洗和质量管控。附实验数据与模型开放下载。


<details>
  <summary>Details</summary>
Motivation: 非洲低资源语种ASR系统开发受限于标注语音数据稀缺。新模型（如Whisper）虽具潜力，但实际部署所需的数据量及主要失效点尚不清楚，需为实践者提供可靠参考。

Method: 在Kinyarwanda语种上通过不同数据规模（1~1400小时）系统性实验，分析训练数据量对ASR性能影响；在Kikuyu语种上用270小时数据进行详细错误类型分析。

Result: 实验证明（以Whisper为例），仅用50小时训练数据即可达到较实用性能（WER<13%），200小时则进一步提升（WER<10%）；数据质量问题尤其是不准确的转录占高错误情况的38.6%，表明数据质量与数据量同等重要。

Conclusion: 通过实验评估，低资源非洲语言的ASR建模在合理数据量下可取得实用性能，同时高错误率主要源于数据质量问题。

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [107] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 本文通过稀疏结构初始化、进化搜索和知识蒸馏三法则组合，大幅提升小型语言模型（SLM）预训练效率，使新模型在维持性能的同时，预训练token需求减少9.2倍。所有代码与模型已开放，有助于SLM高效、规模化开发。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型资源消耗高昂的背景下，寻找性能与资源效率俱佳的“小型语言模型”（SLM）预训练方法。

Method: 提出了结构稀疏子网络初始化、进化式搜索发现高质量初始化，以及知识蒸馏三步结合的预训练框架。具体包括：结构稀疏子网络优于随机初始化；用进化搜索自动发现更佳的初始权重；通过知识蒸馏提升泛化与加速训练。

Result: 使用进化搜索及大模型权重初始化的SLM，在验证困惑度上与同规模的Pythia SLM相当，但预训练所需token数减少了9.2倍。

Conclusion: 本文提出的SLM预训练策略可以在显著减少计算资源和训练token的情况下获得与现有主流SLM相当的性能，且工具开放，推动SLM高效发展。

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [108] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: 传统LLM模拟的用户行为缺乏个性化，Customer-R1引入用户persona并用RL优化，能高度还原个体行为，仿真效果优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 以LLMs进行逐步人类行为模拟在实际领域应用广泛，但现有方法主要针对群体策略，缺乏对用户个性化特征的建模，导致行为模拟泛化、不个性。

Method: 提出Customer-R1方法，一种以强化学习为基础、融合用户个性设定的个性化行为模拟方法。该方法利用显式persona输入，并通过行为正确性的奖励信号，优化逐步推理和行动生成。

Result: 在OPeRA数据集上，Customer-R1在下一步行动预测任务中显著优于基于prompt和SFT的基线方法，并能更好地匹配用户的行为分布，实现更高的个性化行为保真度。

Conclusion: Customer-R1能有效用于在线购物环境下的个性化、逐步用户行为生成，相比现有LLM模拟方案提升了个性化和仿真精度，为用户行为模拟开辟了新的方向。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [109] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: 论文提出了基于真实经济金融研究的因果推理评测集，全面测试了LLM在各领域因果推理的实际效果，发现即便是最先进模型表现也远不及预期，揭示了当前LLM因果推理能力的重大短板。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在进行因果推理时存在不足，主流评测基准多依赖合成数据且覆盖领域狭窄，因此无法体现模型在实际复杂因果关系面前的真实能力。

Method: 作者从顶级经济与金融期刊中提取经过严格方法（如工具变量、双重差分和断点回归设计）确定的因果关系，构建了一个包含40,379道评估题的新型基准，涵盖健康、环境、技术、法律、文化等领域的五种任务类型，并在八个最先进LLM上进行实验评测。

Result: 实验结果显示，表现最好的模型准确率仅为57.6%，模型规模的提升未必带来更好的因果推理表现，先进模型在基础的因果关系识别上仍有较大困难。

Conclusion: 当前LLM与高要求应用领域中的可靠因果推理能力之间存在显著差距。

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [110] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出LAD-RAG，通过布局结构和动态检索显著提升多页面视觉文档问答表现。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法存在证据检索不完整、多页面推理表现较差等问题，原因是其摄取仅编码内容孤立片段，推理又固定检索页数，无法按需动态响应复杂问题。

Method: 提出了LAD-RAG框架，在文档摄取阶段构建了符号化文档图用于捕捉文档布局结构与跨页依赖，并结合神经嵌入进行整体表征；在推理阶段，采用LLM智能体根据查询动态获取神经和符号索引，实现自适应证据检索。

Result: 在多个基准数据集实验中，LAD-RAG平均完美召回率超过90%，不需要top-k调参，在可比噪声下比基线方法召回率高20%，问答准确率更高且几乎无延迟提升。

Conclusion: LAD-RAG显著提升了对视觉丰富文档多页面推理任务的检索和问答准确率，并且在保持低延迟的情况下超过了传统检索基线方法。

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [111] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: 本文系统性研究了主流事实性评测基准老化对LLM事实性评估的影响。通过分析五个基准与八个模型，结果发现大部分基准样本已过时，会导致评估结果失真。提出了新的量化和测试流程，促进相关可靠性研究。


<details>
  <summary>Details</summary>
Motivation: 主流LLM事实性评测基准已与现实和模型发展脱节，严重影响评估结果的可靠性，相关影响未被充分研究。

Method: 系统性分析了五个主流事实性评测基准和八个不同年份发布的LLMs，采用事实检索流程及三种指标量化基准老化及其对评估的影响。

Result: 大量基准集样本已经过时，实验显示这导致LLMs事实性评估不可靠，并提出可用于检验基准可靠性的测试流程。

Conclusion: 现有事实性评测基准由于内容过时，导致对大型语言模型事实性评估不可靠，需关注基准老化问题。

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [112] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: Red-Bandit是一种自适应自动化红队框架，通过强化学习及动态专家选择，有效识别和利用大型语言模型不同风格下的安全漏洞，在红队测试和可读性上均达到了领先水平，并为模型漏洞诊断提供辅助。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）部署前，需要利用自动化红队审计方法发现模型潜在的安全漏洞。然而，传统方法在推断过程中缺乏针对模型特定弱点的高效适应机制，限制了红队测试的效果和效率。

Method: 提出Red-Bandit框架，在线自适应地识别并利用模型的不同攻击风格下的失效模式。该框架利用参数高效的LoRA专家（针对不同攻击风格如操控、俚语等进行专门化训练），并通过基于规则的安全模型进行奖励，使用强化学习后训练这些专家。在推断时，采用多臂赌博策略动态选择各攻击专家，实现探索与利用的平衡。

Result: Red-Bandit在AdvBench基准测试上取得了最优的ASR@10成绩。同时生成的攻击提示更加易读（困惑度更低），并且其赌博策略可以协助诊断模型特定的安全漏洞，指出哪些攻击风格更易诱发不安全行为。

Conclusion: Red-Bandit不仅提升了红队测试的效能与易读性，更为LLMs漏洞分析提供了有力工具，有助于更精准地发现和修复模型安全问题。

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [113] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: 本文提出了融合检验器与奖励模型信号的新强化学习方法HERO，在多类数学推理任务中显著优于现有方法，兼顾正确性和多样性，有效提升了大语言模型的推理质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过推理任务后训练，主要依靠可验证的奖励（即确定性的0-1检验器反馈），但这种反馈过于二元，无法充分肯定部分正确或有创新的解答，限制了模型的学习和泛化能力。连续反馈的奖励模型可以弥补这一不足，但两者如何有机结合，提升模型推理能力，是亟需解决的问题。

Method: 提出HERO（Hybrid Ensemble Reward Optimization）强化学习框架，将检验器的信号与奖励模型分数结构化整合。具体包括分层归一化（在检验器定义的组内约束奖励模型分数）以确保正确性，同时细化质量区分，以及方差感知加权机制（在需要密集信号的难问题上增强作用）。

Result: 在多个数学推理基准上，HERO明显优于单用奖励模型或单用检验器的基线方法，无论可验证还是难以验证的任务上都有显著提升，兼顾稳定性与表现力。

Conclusion: 混合奖励设计不仅保持了检验器的稳定性，还能充分利用奖励模型的细腻反馈，从而推动大语言模型的推理能力进步。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [114] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: 本文针对法律领域LLM输出评价难题，提出通过划分“法律数据点”并采用无参考数据的新评价方法。该方法与律师的实际评估方式更为一致，评测效果优于传统方法，并开源部分数据集促进领域进步。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在法律领域的输出评价面临挑战：依赖参考数据成本高，标准化方法存在显著局限，而且主流“LLM-as-a-Judge”的评价过程和结果难以令人类法律专家充分信服。

Method: 提出将复杂法律回答拆分为“法律数据点（LDPs）”，即独立的信息单元，并构建了一种全新的、无参考数据的评价方法，模拟律师评估法律答案的实际方式。将该方法应用于专有数据集和开源LegalBench数据集进行对比试验。

Result: 所提出的方法在专有数据集和LegalBench两者上相较多种基线方法表现更优。此外，该方法与人类法律专家的评价结果高度相关，能够提升不同评审者之间的一致性。

Conclusion: 通过LDPs和无参考的评价体系，显著提高了LLM法律问答输出的评估质量和可靠性，为法律领域LLM评估方法的发展提供了重要工具，同时开源部分LDPs数据支持社区后续研究。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [115] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: PA-Tool通过适配工具命名到小模型预训练习得的模式，使低算力模型工具使用能力大幅提升，无需额外训练，可显著减少误用和提升性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型虽然高效，却在工具选择与参数识别上容易因工具命名不一致（schema misalignment）产生错误，常表现为臆造工具名。现有方法多强制模型适应外部命名规范，造成性能瓶颈。为此，作者希望通过调整工具命名适配模型习惯，降低理解和使用障碍。

Method: 提出PA-Tool方法，通过分析模型对工具命名熟悉度（peakedness），对实际工具结构进行自动重命名，使其更贴合小模型预训练习得的命名模式，无需额外训练。通过选取模型输出集中的命名作为最终方案。

Result: 在MetaTool和RoTBench基准测试中，使用PA-Tool后性能提升最高达17个百分点，schema misalignment错误率降低80%。小模型可在无需重新训练前提下接近最优性能，并易于迁移到新工具。

Conclusion: 与传统强制模型适应工具命名方式不同，PA-Tool展示了通过调整工具schema促进模型理解，从而释放小模型工具使用潜力，为低资源场景带来高效可扩展解决方案。

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [116] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 本文提出通过动态在线生成评分标准的方法，解决静态标准易被“奖励黑客”漏洞利用及难以捕捉训练新需求的问题。通过实证获得显著性能提升，并分析了评分重点变化。


<details>
  <summary>Details</summary>
Motivation: 以往使用固定评分标准（rubrics）对大模型进行长期开放性问答训练，面临奖励机制被“绕过”、新目标难以捕捉的问题。

Method: 提出Online Rubrics Elicitation（OnlineRubrics）方法，通过在线动态生成评分标准，利用当前模型与参考模型的答案进行两两比较，调整评估标准。

Result: 该方法在多个数据集（AlpacaEval、GPQA、ArenaHard以及专家问题集）上相较于静态评分标准，取得最高8%的性能提升。对动态评分标准进行了质性分析，发现其中突出主题如透明度、实用性、组织性及推理。

Conclusion: 在线化动态评分标准能够持续发现并纠正模型错误，有效提升长期开放性问答训练效果。

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [117] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 通过分析大模型在道德自我修正时的多轮表现，发现连续自我修正能激活并稳定相关概念，最终提升并收敛回答质量，揭示其机制和潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在被笼统要求“改进回答”时，能否依靠自身知识进行自我修正、为何有效，目前机制尚不明确。

Method: 本文集中探讨道德相关的自我修正，通过多轮交互实验，分析LLMs在自我修正过程中的收敛性，并结合机制性分析，揭示其背后的原理。

Result: 多轮自我修正能够激活模型中的道德概念，减少模型的不确定性，且随着自我修正轮次增加，道德概念逐步稳定，性能出现收敛。

Conclusion: 道德自我修正具有明显的性能收敛特性，展现出LLMs内在自我修正的巨大潜力。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [118] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: M-Thinker模型通过奖励机制专注于多语言下的语言一致性和推理迁移，有效改善了以往LRM在非英语场景下的一致性和准确率不足的问题，在多语言基准上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（LRM）采用“先思考后作答”范式，在复杂推理任务上表现优异，但在处理非英语语言时，存在输入输出语言不一致和推理路径及答题准确率低的问题，严重影响非英语用户体验并阻碍其全球化应用。

Method: 提出M-Thinker模型，并使用包含语言一致性奖励（LC）和跨语言思维对齐奖励（CTA）的GRPO算法进行训练。LC奖励对输入、思维流程和答案的语言一致性加以严格约束，CTA奖励则通过比较模型在英语和非英语下的推理路径，将推理能力由英语迁移至非英语。模型通过迭代的RL（强化学习）过程进行优化。

Result: M-Thinker-1.5B和M-Thinker-7B模型在两个多语言基准（MMATH和PolyMath）上实现了近乎100%的语言一致性和优越的性能，并在域外语言上展现了良好的泛化能力。

Conclusion: M-Thinker通过引入语言一致性与跨语言思维对齐奖励，显著提升了大语言模型在非英语场景下的语言一致性和推理准确性，为多语言推理任务和模型的全球应用提供了有效方案。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [119] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 针对商业领域复杂问题提出了全新text-to-SQL基准CORGI，涵盖多层次业务查询，实验证明现有大模型仍难以胜任高阶业务智能，为进一步研究指明方向并提供数据资源。


<details>
  <summary>Details</summary>
Motivation: 现有的text-to-SQL基准主要关注事实性检索，不足以覆盖现实商业场景中复杂的业务问题。随着数据驱动决策在商业领域愈加重要，亟需一种能体现实际商业环境需求的评测体系。

Method: 作者提出了新的基准CORGI，构建了基于企业（如Doordash、Airbnb、Lululemon）的合成数据库，并设计了涵盖描述性、解释性、预测性和推荐性四类问题的题目，全面考察在商业场景下的复杂查询需求。

Result: 实验发现，主流大模型在高层次商业问题上的表现显著下降，难以做出准确预测或提出可行计划。CORGI基准比现有的BIRD基准困难约21%，揭示了现有模型在真实商业智能场景中的能力缺口。

Conclusion: 当前流行的大模型在复杂商业场景下的推理和规划能力仍有较大提升空间。作者公布了数据集、评测框架及网站，以促进相关研究发展。

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [120] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 本文提出人工海马网络（AHN）加强Transformer的长序列建模，将短期KV记忆与长期压缩记忆结合。结果显示可比甚至超越全注意力模型，同时计算和内存消耗显著降低。


<details>
  <summary>Details</summary>
Motivation: 长序列建模需要在效率与记忆保真度之间进行权衡，传统RNN模型虽然效率高但记忆是有损且固定大小，而Transformer通过注意力机制实现无损但计算和内存开销巨大。本文借鉴认知科学中的多存储模型，旨在平衡长序列建模中的效率与性能。

Method: 提出了一种人工神经网络的记忆框架，将Transformer的KV缓存作为无损的短期记忆，并通过一个叫做人工海马网络（AHN）的可学习模块，将超出滑动窗口的信息递归地压缩到固定大小的长期记忆中。为实现具体框架，AHN采用了现代RNN架构，如Mamba2、DeltaNet和Gated DeltaNet。

Result: 在长序列基准LV-Eval和InfiniteBench上的大量实验证明，加入AHN的模型显著优于仅用滑动窗口的基线，并能达到甚至超越全注意力模型的性能，同时极大降低了计算和内存消耗。例如，在Qwen2.5-3B-Instruct模型上，推理FLOPs降低了40.5%，内存缓存减少了74.0%，LV-Eval（128k序列长度）得分从4.41提升到5.88。

Conclusion: 该方法有效提升了长序列建模的性能，在保证效果的前提下大幅降低计算与内存消耗，兼顾了效率与表达能力；其框架可兼容多种RNN架构，具有应用推广潜力。

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [121] [A Computer-Assisted Proof of the Optimal Density Bound for Pinwheel Covering](https://arxiv.org/abs/2510.06533)
*Akitoshi Kawamura,Yusuke Kobayashi*

Main category: cs.DM

TL;DR: 本文针对pinwheel调度问题的covering版本，首次给出了最佳临界阈值α*=1.264...，证明只要所有代理人可调度的总权重超过该阈值就能保证任务分配可行，解决了长期开放问题，采用了数学创新和大规模计算机搜索。


<details>
  <summary>Details</summary>
Motivation: 解决由Soejima和Kawamura（2020）提出的开放问题：在pinwheel调度的covering版本中，找到一个临界条件保证任务总能分配给代理人。

Method: 采用Kawamura的packing版本技术，结合新的数学洞察，并通过详尽的计算机辅助搜索实现。

Result: 确定了最优界限α*=1.264...，且证明只要任务满足∑1/a_i≥α*条件，调度一定可行。

Conclusion: 本文解决了每个实例可调度性的充分条件，确定了最优常数α*=1.264...，只要∑1/a_i≥α*就保证可覆盖调度。

Abstract: In the covering version of the pinwheel scheduling problem, a daily task must
be assigned to agents under the constraint that agent $i$ can perform the task
at most once in any $a_i$-day interval. In this paper, we determine the optimal
constant $\alpha^* = 1.264\ldots {}$ such that every instance with $\sum_{i}
\frac{1}{a_i} \ge \alpha^*$ is schedulable. This resolves an open problem posed
by Soejima and Kawamura (2020). Our proof combines Kawamura's (2024) techniques
for the packing version with new mathematical insights, along with an
exhaustive computer-aided search that draws on some ideas from G\k{a}sieniec,
Smith, and Wild (2022).

</details>


### [122] [On the distribution of $A_α$-eigenvalues in terms of graph invariants](https://arxiv.org/abs/2510.06933)
*Uilton Cesar Peres Junior,Carla Silva Oliveira,André Ebling Brondan*

Main category: cs.DM

TL;DR: 本文研究了一类由图的邻接矩阵和度矩阵凸组合而成的新矩阵$A_\alpha(G)$特征值的分布规律，依据图的结构参数给出了特征值数量的上下界，并扩展了邻接矩阵和无符号Laplacian已知的谱界。


<details>
  <summary>Details</summary>
Motivation: 已知邻接矩阵和无符号Laplace矩阵的谱分布拥有较多研究，但其间的凸组合$A_\alpha(G)$的谱分布及其与图结构的关联性未经系统分析。

Method: 通过研究$A_\alpha(G)$（邻接矩阵与度矩阵的凸组合）的特征值，在实数轴的子区间内，借助图的结构参数（如悬挂顶点数、支配数、匹配数、覆盖数等）对特征值数量进行界定，并通过构造极值图证明界限可达。

Result: 给出了$A_\alpha(G)$特征值在实数轴子区间内的数量上下界，并展示了这些界达到的极值图家族，推广和扩展了邻接矩阵及无符号Laplace矩阵的相关谱界结论。

Conclusion: 我们对图的$A_\alpha(G)$矩阵的特征值分布给出了上下界，这些界限以图的结构参数表示，对已有谱界结果进行了拓展。

Abstract: Let $G$ be a connected graph of order $n$, and $A(G)$ and $D(G)$ its
adjacency and degree diagonal matrices, respectively. For a parameter $\alpha
\in [0,1]$, Nikiforov~(2017) introduced the convex combination $A_{\alpha}(G) =
\alpha D(G) + (1 - \alpha)A(G)$. In this paper, we investigate the spectral
distribution of $A_\alpha(G)$-eigenvalues, over subintervals of the real line.
We establish lower and upper bounds on the number of such eigenvalues in terms
of structural parameters of $G$, including the number of pendant and
quasi-pendant vertices, the domination number, the matching number, and the
edge covering number. Additionally, we exhibit families of graphs for which
these bounds are attained. Several of our results extend known spectral bounds
on the eigenvalue distributions of both the adjacency and the signless
Laplacian matrices.

</details>


### [123] [Parameterized Complexity of s-Club Cluster Edge Deletion](https://arxiv.org/abs/2510.07065)
*Ajinkya Gaikwad*

Main category: cs.DM

TL;DR: 本文系统研究了s-团簇边删除问题的复杂度，证明了其在树宽参数下的W[1]-难性，在邻域多样性、孪生覆盖等参数下是FPT的；同时提升了经典复杂度理解，提出FPT近似算法，为该领域理论和算法研究带来新突破。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在研究图的s-团簇边删除问题的参数化和经典复杂度，该问题泛化了团簇边删除问题，对多种距离有界的图修改任务具有代表性。之前的工作提出了该问题在某些参数下是固定参数可解的，并提出对参数依赖的开放性问题。

Method: 通过复杂度归约证明W[1]-难性，并对多种参数（如邻域多样性、孪生覆盖、团簇顶点删除数）下设计FPT算法；同时在经典复杂度下分析问题的NP-难性并提出bicriteria近似算法。

Result: 证明了当参数为路径宽度（进而为树宽）时，该问题是W[1]-难的；但当参数为邻域多样性、孪生覆盖或团簇顶点删除数时是FPT的；在分裂图上当s=2时问题是NP-难的；对于解大小k，实现了一个bicriteria FPT近似算法。

Conclusion: 论文解答了关于s依赖性的理论问题，扩展了FPT算法的适用参数，完善了问题在不同参数和图类下的复杂度版图，并推动了k为唯一参数时该问题FPT性的研究进展。

Abstract: We study the parameterized and classical complexity of the s-Club Cluster
Edge Deletion problem: given a graph G = (V, E) and integers k and s, determine
whether it is possible to delete at most k edges so that every connected
component of the resulting graph has diameter at most s. This problem
generalizes Cluster Edge Deletion (the case s = 1) and captures a variety of
distance-bounded graph modification tasks.
  Montecchiani, Ortali, Piselli, and Tappini (Information and Computation,
2023) showed that the problem is fixed-parameter tractable when parameterized
by s plus the treewidth of G, and asked whether the dependence on s is
necessary; that is, whether the problem is FPT when parameterized by treewidth
alone. We resolve this by proving that the problem is W[1]-hard when
parameterized by pathwidth, and hence by treewidth.
  On the algorithmic side, we show that the problem is FPT when parameterized
by neighborhood diversity, twin cover, or cluster vertex deletion number,
thereby extending to all s >= 1 the results of Italiano, Konstantinidis, and
Papadopoulos (Algorithmica, 2023), who established FPT algorithms for the case
s = 1 under the neighborhood diversity and twin cover parameters.
  From a classical perspective, we prove that the problem is NP-hard on split
graphs already for s = 2, complementing the polynomial-time solvability for s =
1 due to Bonomo, Duran, and Valencia-Pabon (Theoretical Computer Science, 2015)
and the trivial case s = 3.
  Finally, while the problem is FPT when parameterized by s + k, its complexity
for the solution size k alone remains open. We make progress on this front by
designing an FPT bicriteria approximation algorithm, which runs in time f(k,
1/epsilon) * n^{O(1)} and, for graphs excluding long induced cycles, outputs a
solution of size at most k whose connected components have diameter at most (1
+ epsilon) * s.

</details>


### [124] [On some 2-binomial coefficients of binary words: geometrical interpretation, partitions of integers, and fair words](https://arxiv.org/abs/2510.07159)
*Gwenaël Richomme*

Main category: cs.DM

TL;DR: 论文研究了二元词中子词出现次数的分布，通过几何解释和代数等价类分析，揭示了词结构和子词计数的深层关系，并证明了与最小二乘逼近猜想相关的问题，对组合词理论有重要意义。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在深入研究二项式记法在二元字母词中的分布特性，探索词的子词出现次数的数学结构和几何解释，并回应近期与最小二乘逼近相关的猜想。

Method: 首先引入并分析了词 (w ab) 和 (w ba) 的几何解释；接着研究了二元词的 2-二项式等价类（即所有长度不超过2的子词出现次数相同的词）结构，提出了一种重写规则并证明了其等价类图与整数分割格之间的同构性；最后分析了二元公平词（出现 ab 和 ba 的次数相同的词），并证明了与最小二乘逼近特例相关的一个新猜想。

Result: 证明了 2-二项式等价类的图与整数分割格之间的同构关系；并对公平词的结构进行了刻画，证实了关于最小二乘逼近特例的一个新的猜想。

Conclusion: 通过几何和代数方法深入分析了二元词的二项式统计性质，不仅揭示了相关等价类的结构，还成功解决了此前的一个公开猜想，这为词的组合理论和相关应用提供了新的理论工具和视角。

Abstract: The binomial notation (w u) represents the number of occurrences of the word
u as a (scattered) subword in w. We first introduce and study possible uses of
a geometrical interpretation of (w ab) and (w ba) when a and b are distinct
letters. We then study the structure of the 2-binomial equivalence class of a
binary word w (two words are 2-binomially equivalent if they have the same
binomial coefficients, that is, the same numbers of occurrences, for each word
of length at most 2). Especially we prove the existence of an isomorphism
between the graph of the 2-binomial equivalence class of w with respect to a
particular rewriting rule and the lattice of partitions of the integer (w ab)
with (w a) parts and greatest part bounded by (w b). Finally we study binary
fair words, the words over {a, b} having the same numbers of occurrences of ab
and ba as subwords ((w ab) = (w ba)). In particular, we prove a recent
conjecture related to a special case of the least square approximation.

</details>
