{"id": "2507.14082", "categories": ["cs.FL", "cs.CC"], "pdf": "https://arxiv.org/pdf/2507.14082", "abs": "https://arxiv.org/abs/2507.14082", "authors": ["Nelma Moreira", "Luca Prigioniero"], "title": "Proceedings of the 15th International Workshop on Non-Classical Models of Automata and Applications", "comment": null, "summary": "The Fifteenth International Workshop on Non-Classical Models of Automata and\nApplications (NCMA 2025) was held in Loughborough, UK, on July 21 and 22, 2025,\norganized by the Department of Computer Science at Loughborough University and\nco-located with the 26th International Conference on Descriptional Complexity\nof Formal Systems (DCFS 2025, 22-24 July).\n  The NCMA workshop series was established in 2009 as an annual event for\nresearchers working on non-classical and classical models of automata, grammars\nor related devices. Such models are investigated both as theoretical models and\nas formal models for applications from various points of view. The goal of the\nNCMA workshop series is to exchange and develop novel ideas in order to gain\ndeeper and interdisciplinary coverage of this particular area that may foster\nnew insights and substantial progress.", "AI": {"tldr": "NCMA 2025\u5728\u82f1\u56fd\u62c9\u592b\u5821\u4e3e\u529e\uff0c\u805a\u7126\u975e\u7ecf\u5178\u81ea\u52a8\u673a\u7b49\u7406\u8bba\u4e0e\u5e94\u7528\u6a21\u578b\uff0c\u65e8\u5728\u5b66\u8005\u95f4\u4ea4\u6d41\u521b\u65b0\u89c2\u70b9\uff0c\u63a8\u52a8\u5b66\u79d1\u6df1\u5165\u53d1\u5c55\u3002", "motivation": "\u63a8\u52a8\u975e\u7ecf\u5178\u53ca\u7ecf\u5178\u81ea\u52a8\u673a\u3001\u6587\u6cd5\u548c\u76f8\u5173\u6a21\u578b\u9886\u57df\u7684\u7814\u7a76\uff0c\u6df1\u5316\u591a\u5b66\u79d1\u4ea4\u6d41\uff0c\u4fc3\u8fdb\u7406\u8bba\u521b\u65b0\u548c\u5e94\u7528\u8fdb\u6b65\u3002", "method": "\u4ee5\u56fd\u9645\u7814\u8ba8\u4f1a\u7684\u5f62\u5f0f\uff0c\u9080\u8bf7\u76f8\u5173\u9886\u57df\u7814\u7a76\u8005\u53c2\u4f1a\uff0c\u8fdb\u884c\u62a5\u544a\u3001\u8ba8\u8bba\u548c\u4ea4\u6d41\uff0c\u6db5\u76d6\u7406\u8bba\u4e0e\u5e94\u7528\u7b49\u591a\u4e2a\u89c6\u89d2\u3002", "result": "\u901a\u8fc7\u6b64\u6b21\u4f1a\u8bae\uff0c\u4fc3\u8fdb\u4e86\u65b0\u601d\u60f3\u7684\u4ea4\u6d41\u548c\u591a\u5b66\u79d1\u4ea4\u53c9\uff0c\u52a0\u6df1\u4e86\u8be5\u9886\u57df\u7814\u7a76\u4eba\u5458\u4e4b\u95f4\u7684\u76f8\u4e92\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7406\u8bba\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u672c\u6b21\u7814\u8ba8\u4f1a\u4e3a\u975e\u7ecf\u5178\u4e0e\u7ecf\u5178\u81ea\u52a8\u673a\u3001\u6587\u6cd5\u7b49\u76f8\u5173\u6a21\u578b\u7684\u7814\u7a76\u8005\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b66\u672f\u4ea4\u6d41\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u521b\u65b0\u6027\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.13494", "categories": ["cs.PL", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.13494", "abs": "https://arxiv.org/abs/2507.13494", "authors": ["Feras A. Saad", "Wonyeol Lee"], "title": "Random Variate Generation with Formal Guarantees", "comment": null, "summary": "This article introduces a new approach to principled and practical random\nvariate generation with formal guarantees. The key idea is to first specify the\ndesired probability distribution in terms of a finite-precision numerical\nprogram that defines its cumulative distribution function (CDF), and then\ngenerate exact random variates according to this CDF. We present a universal\nand fully automated method to synthesize exact random variate generators given\nany numerical CDF implemented in any binary number format, such as\nfloating-point, fixed-point, and posits. The method is guaranteed to operate\nwith the same precision used to specify the CDF, does not overflow, avoids\nexpensive arbitrary-precision arithmetic, and exposes a consistent API. The\nmethod rests on a novel space-time optimal implementation for the class of\ngenerators that attain the information-theoretically optimal Knuth and Yao\nentropy rate, consuming the least possible number of input random bits per\noutput variate. We develop a random variate generation library using our method\nin C and evaluate it on a diverse set of ``continuous'' and ``discrete''\ndistributions, showing competitive runtime with the state-of-the-art GNU\nScientific Library while delivering higher accuracy, entropy efficiency, and\nautomation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9488\u5bf9\u4efb\u610f\u4e8c\u8fdb\u5236\u6570\u503c\u683c\u5f0f\u5206\u5e03\u7684\u968f\u673a\u53d8\u91cf\u5168\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u7406\u8bba\u7cbe\u5ea6\u6709\u4fdd\u8bc1\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u5e93\uff0c\u5728\u6570\u5b66\u548c\u5de5\u7a0b\u5b9e\u8df5\u4e0a\u5747\u5341\u5206\u6709\u7528\u3002", "motivation": "\u5728\u7edf\u8ba1\u5206\u6790\u3001\u4eff\u771f\u7b49\u9886\u57df\uff0c\u968f\u673a\u53d8\u91cf\u751f\u6210\u662f\u57fa\u7840\u73af\u8282\u3002\u4f20\u7edf\u751f\u6210\u5668\u901a\u5e38\u53d7\u9650\u4e8e\u6d6e\u70b9\u7cbe\u5ea6\u3001\u6ea2\u51fa\u98ce\u9669\u6216\u9700\u8981\u9ad8\u6602\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u4e14\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7684\u51c6\u786e\u6027\u4fdd\u8bc1\u3002\u8be5\u8bba\u6587\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u5177\u5907\u5f62\u5f0f\u5316\u7cbe\u5ea6\u4fdd\u969c\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u5168\u81ea\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6982\u7387\u5206\u5e03\u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08CDF\uff09\u4ee5\u6709\u9650\u7cbe\u5ea6\u6570\u503c\u7a0b\u5e8f\u5f62\u5f0f\u63cf\u8ff0\uff0c\u518d\u4ee5\u8be5CDF\u4e3a\u51c6\u751f\u6210\u7cbe\u786e\u7684\u968f\u673a\u53d8\u91cf\u3002\u9488\u5bf9\u4efb\u4f55\u7528\u4e8c\u8fdb\u5236\u6570\u8868\u793a\u7684\u6570\u503c\u683c\u5f0f\uff08\u5982\u6d6e\u70b9\u3001\u5b9a\u70b9\u6216posit\uff09\uff0c\u90fd\u80fd\u81ea\u52a8\u5408\u6210\u751f\u6210\u5668\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u6ea2\u51fa\u548c\u9ad8\u5f00\u9500\u7684\u4efb\u610f\u7cbe\u5ea6\u8ba1\u7b97\uff0c\u5177\u6709\u7edf\u4e00\u63a5\u53e3\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u8fbe\u5230Knuth-Yao\u4fe1\u606f\u71b5\u7387\u6700\u4f18\uff0c\u5373\u6bcf\u4e2a\u8f93\u51fa\u53d8\u91cf\u6d88\u8017\u7684\u968f\u673a\u6bd4\u7279\u6570\u6700\u5c11\u3002", "result": "\u4f5c\u8005\u5728C\u8bed\u8a00\u5b9e\u73b0\u4e86\u8fd9\u4e00\u751f\u6210\u5e93\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u8fde\u7eed\u4e0e\u79bb\u6563\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u71b5\u6548\u7387\u548c\u81ea\u52a8\u5316\u65b9\u9762\u4f18\u4e8eGNU Scientific Library\uff0c\u5e76\u4e14\u8fd0\u884c\u65f6\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6709\u9650\u7cbe\u5ea6\u4e0b\u7684\u6982\u7387\u5206\u5e03\u968f\u673a\u751f\u6210\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u4fdd\u969c\u548c\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u53ef\u5e7f\u6cdb\u7528\u4e8e\u79d1\u5b66\u8ba1\u7b97\u4e0e\u5de5\u7a0b\u5e94\u7528\uff0c\u5bf9\u73b0\u6709\u968f\u673a\u751f\u6210\u673a\u5236\u4f5c\u51fa\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2507.13847", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13847", "abs": "https://arxiv.org/abs/2507.13847", "authors": ["Katsumi Inoue", "Daniil Kozhemiachenko"], "title": "Complexity of Abduction in \u0141ukasiewicz Logic", "comment": null, "summary": "We explore the problem of explaining observations in contexts involving\nstatements with truth degrees such as `the lift is loaded', `the symptoms are\nsevere', etc. To formalise these contexts, we consider infinitely-valued\n{\\L}ukasiewicz fuzzy logic. We define and motivate the notions of abduction\nproblems and explanations in the language of {\\L}ukasiewicz logic expanded with\n`interval literals' of the form $p\\geq\\mathbf{c}$, $p\\leq\\mathbf{c}$, and their\nnegations that express the set of values a variable can have. We analyse the\ncomplexity of standard abductive reasoning tasks (solution recognition,\nsolution existence, and relevance / necessity of hypotheses) in {\\L}ukasiewicz\nlogic for the case of the full language and for the case of theories containing\nonly disjunctive clauses and show that in contrast to classical propositional\nlogic, the abduction in the clausal fragment has lower complexity than in the\ngeneral case.", "AI": {"tldr": "\u672c\u6587\u4ee5\u9c81\u5361\u8c22\u7ef4\u5947\u6a21\u7cca\u903b\u8f91\u4e3a\u57fa\u7840\uff0c\u63a2\u8ba8\u533a\u95f4\u6587\u5b57\u6269\u5c55\u4e0b\u7684\u6eaf\u56e0\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u7279\u5b9a\u5b50\u7406\u8bba\u7684\u63a8\u7406\u590d\u6742\u5ea6\u8fdc\u4f4e\u4e8e\u5168\u7247\u6bb5\uff0c\u4e3a\u5b9e\u9645\u6a21\u7cca\u8bed\u5883\u4e0b\u7684\u89e3\u91ca\u4efb\u52a1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u5e26\u6709\u771f\u503c\u5ea6\u91cf\u7684\u8bed\u53e5\uff08\u5982\u201c\u7535\u68af\u88ab\u52a0\u8f7d\u201d\u3001\u201c\u75c7\u72b6\u4e25\u91cd\u201d\u7b49\uff09\u7684\u89e3\u91ca\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u4e86\u80fd\u591f\u66f4\u597d\u5730\u5f62\u5f0f\u5316\u8fd9\u4e9b\u5e26\u6a21\u7cca\u6027\u53ca\u4e0d\u540c\u771f\u503c\u7a0b\u5ea6\u7684\u63a8\u7406\u8bed\u5883\uff0c\u4f5c\u8005\u4ee5\u65e0\u9650\u503c\u7684\u9c81\u5361\u8c22\u7ef4\u5947\uff08{\u0013}ukasiewicz\uff09\u6a21\u7cca\u903b\u8f91\u4e3a\u57fa\u7840\uff0c\u63d0\u51fa\u7814\u7a76\u5176\u6eaf\u56e0\u89e3\u91ca\u95ee\u9898\u7684\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u5728\u9c81\u5361\u8c22\u7ef4\u5947\u6a21\u7cca\u903b\u8f91\u7684\u8bed\u8a00\u57fa\u7840\u4e0a\uff0c\u6269\u5c55\u4e86\u6240\u8c13\u7684\u201c\u533a\u95f4\u6587\u5b57\u201d\uff08interval literals\uff09\uff0c\u5982$p\\geq\\mathbf{c}$\u3001$p\\leq\\mathbf{c}$\u53ca\u5176\u5426\u5b9a\uff0c\u4ece\u800c\u8868\u8fbe\u53d8\u91cf\u53d6\u503c\u8303\u56f4\u3002\u4ee5\u6b64\u4e3a\u57fa\u7840\uff0c\u5b9a\u4e49\u4e86\u57fa\u4e8e\u8be5\u7cfb\u7edf\u7684\u6eaf\u56e0\u95ee\u9898\u4e0e\u89e3\u91ca\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u63a8\u7406\u4efb\u52a1\uff08\u5305\u62ec\u89e3\u8bc6\u522b\u3001\u89e3\u5b58\u5728\u6027\u3001\u5047\u8bbe\u7684\u76f8\u5173\u6027\u4e0e\u5fc5\u8981\u6027\uff09\u8fdb\u884c\u4e86\u590d\u6742\u5ea6\u5206\u6790\u3002\u5206\u522b\u63a2\u8ba8\u4e86\u5168\u8bed\u8a00\u548c\u4ec5\u542b\u6790\u53d6\u9879\u7406\u8bba\u7684\u60c5\u5f62\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u7ecf\u5178\u547d\u9898\u903b\u8f91\u4e0d\u540c\uff0c\u5728\u9c81\u5361\u8c22\u7ef4\u5947\u903b\u8f91\u4e2d\uff0c\u6eaf\u56e0\u63a8\u7406\u4efb\u52a1\u5728\u6790\u53d6\u7247\u6bb5\uff08\u5373\u4ec5\u542b\u6790\u53d6\u5b50\u53e5\u7684\u7406\u8bba\uff09\u4e2d\u590d\u6742\u5ea6\u4f4e\u4e8e\u4e00\u822c\u60c5\u5f62\u3002\u8be6\u7ec6\u7ed9\u51fa\u4e86\u51e0\u7c7b\u6807\u51c6\u6eaf\u56e0\u63a8\u7406\u4efb\u52a1\u5728\u4e0d\u540c\u7247\u6bb5\u4e0b\u7684\u590d\u6742\u5ea6\u5bf9\u6bd4\u3002", "conclusion": "\u5728\u73b0\u4ee3\u6a21\u7cca\u903b\u8f91\u80cc\u666f\u4e0b\uff0c\u4f5c\u8005\u5c55\u793a\u4e86\u4ee5\u9c81\u5361\u8c22\u7ef4\u5947\u6a21\u7cca\u903b\u8f91\u4e3a\u57fa\u7840\u8fdb\u884c\u6eaf\u56e0\u89e3\u91ca\u7684\u6709\u6548\u6027\uff0c\u5e76\u6307\u51fa\u9009\u62e9\u7279\u5b9a\u7406\u8bba\u7247\u6bb5\uff08\u5982\u6790\u53d6\u7406\u8bba\uff09\u53ef\u6709\u6548\u964d\u4f4e\u63a8\u7406\u590d\u6742\u5ea6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e0b\u7684\u89e3\u91ca\u4efb\u52a1\u5e26\u6765\u66f4\u9ad8\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.13481", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13481", "abs": "https://arxiv.org/abs/2507.13481", "authors": ["Arthur Bueno", "Bruno Cafeo", "Maria Cagnin", "Awdren Font\u00e3o"], "title": "Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Code samples play a pivotal role in open-source ecosystems (OSSECO), serving\nas lightweight artifacts that support knowledge transfer, onboarding, and\nframework adoption. Despite their instructional relevance, these samples are\noften governed informally, with minimal review and unclear ownership, which\nincreases their exposure to socio-technical degradation. In this context, the\nco-occurrence and longitudinal interplay of code smells (e.g., large classes,\npoor modularity) and community smells (e.g., lone contributors, fragmented\ncommunication) become particularly critical. While each type of smell has been\nstudied in isolation, little is known about how community-level dysfunctions\nanticipate or exacerbate technical anomalies in code samples over time. This\nstudy investigates how code and community smells emerge, co-occur, and evolve\nwithin code samples maintained in OSSECOs. A Multivocal Literature Review\nprotocol was applied, encompassing 30 peer-reviewed papers and 17\npractitioner-oriented sources (2013-2024). Thematic synthesis was conducted to\nidentify recurring socio-technical patterns related to smell dynamics. Nine\npatterns were identified, showing that community smells often precede or\nreinforce technical degradation in code samples. Symptoms such as \"radio\nsilence\" and centralized ownership were frequently associated with persistent\nstructural anomalies. Additionally, limited onboarding, the absence of\ncontinuous refactoring, and informal collaboration emerged as recurring\nconditions for smell accumulation. Conclusion: In OSSECOs, particularly within\ncode samples, community-level dysfunctions not only correlate with but often\nsignal maintainability decay. These findings underscore the need for\nsocio-technical quality indicators and lightweight governance mechanisms\ntailored to shared instructional artifacts.", "AI": {"tldr": "\u5f00\u6e90\u751f\u6001\u91cc\u7684\u4ee3\u7801\u793a\u4f8b\u56e0\u793e\u533a\u5931\u8c03\u66f4\u6613\u6280\u672f\u9000\u5316\uff0c\u9700\u5173\u6ce8\u793e\u533a\u6cbb\u7406\u548c\u8d28\u91cf\u6307\u6807\u4ee5\u4fdd\u969c\u53ef\u7ef4\u62a4\u6027\u3002", "motivation": "\u5728\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff08OSSECO\uff09\u4e2d\uff0c\u4ee3\u7801\u793a\u4f8b\u5bf9\u4e8e\u77e5\u8bc6\u4f20\u9012\u3001\u4eba\u5458\u5165\u804c\u548c\u6846\u67b6\u91c7\u7eb3\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4ee3\u7801\u793a\u4f8b\u5e38\u5e38\u7f3a\u4e4f\u6b63\u5f0f\u6cbb\u7406\u3001\u5ba1\u6838\u6709\u9650\u4e14\u6240\u6709\u6743\u4e0d\u660e\uff0c\u5bb9\u6613\u51fa\u73b0\u793e\u4f1a\u6280\u672f\u9000\u5316\u73b0\u8c61\u3002\u76ee\u524d\u5173\u4e8e\u4ee3\u7801\u5473\u9053\u53ca\u793e\u533a\u5473\u9053\u7684\u7814\u7a76\u591a\u4e3a\u5355\u72ec\u5206\u6790\uff0c\u5bf9\u4e24\u8005\u7684\u4ea4\u4e92\u5f71\u54cd\u8ba4\u8bc6\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u58f0\u9053\u6587\u732e\u7efc\u8ff0\uff08Multivocal Literature Review\uff09\u65b9\u6cd5\uff0c\u68b3\u7406\u4e862013-2024\u5e74\u95f430\u7bc7\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca17\u4e2a\u5b9e\u8df5\u8005\u8d44\u6599\uff0c\u901a\u8fc7\u4e3b\u9898\u7efc\u5408\u5206\u6790\u4ee3\u7801\u548c\u793e\u533a\u5473\u9053\u5728OSSECO\u7684\u4ee3\u7801\u793a\u4f8b\u4e2d\u4ea7\u751f\u3001\u5171\u73b0\u548c\u6f14\u5316\u7684\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa\u4e5d\u79cd\u4e0e\u5473\u9053\u52a8\u6001\u76f8\u5173\u7684\u793e\u4f1a\u6280\u672f\u6a21\u5f0f\uff0c\u53d1\u73b0\u793e\u533a\u5473\u9053\u5f80\u5f80\u5148\u4e8e\u6216\u52a0\u5267\u4e86\u4ee3\u7801\u7ed3\u6784\u9000\u5316\u3002\u5e38\u89c1\u75c7\u72b6\u5982\u201c\u65e0\u7ebf\u7535\u9759\u9ed8\u201d\uff08\u7f3a\u4e4f\u4ea4\u6d41\uff09\u3001\u6240\u6709\u6743\u96c6\u4e2d\u7b49\u4e0e\u7ed3\u6784\u5f02\u5e38\u6301\u7eed\u5b58\u5728\u6709\u5173\u3002\u8bc1\u636e\u8fd8\u8868\u660e\uff0c\u6709\u9650\u7684\u5165\u804c\u3001\u7f3a\u4e4f\u6301\u7eed\u91cd\u6784\u53ca\u975e\u6b63\u5f0f\u534f\u4f5c\u52a0\u5267\u4e86\u5473\u9053\u7684\u7d2f\u79ef\u95ee\u9898\u3002", "conclusion": "OSSECO\u4e2d\uff0c\u793e\u533a\u5c42\u9762\u7684\u529f\u80fd\u5931\u8c03\u4e0d\u4ec5\u4e0e\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u4e0b\u964d\u76f8\u5173\uff0c\u4e14\u5e38\u4f5c\u4e3a\u5176\u524d\u5146\u3002\u5f3a\u8c03\u9700\u5236\u5b9a\u7b26\u5408\u5171\u4eab\u6307\u5bfc\u7c7b\u4ee3\u7801\u793a\u4f8b\u7684\u793e\u4f1a\u6280\u672f\u8d28\u91cf\u6307\u6807\u4e0e\u8f7b\u91cf\u6cbb\u7406\u673a\u5236\u3002"}}
{"id": "2507.13357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13357", "abs": "https://arxiv.org/abs/2507.13357", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7ed3\u6784\u5316\u8bed\u4e49\u63a8\u7406\u7684\u9493\u9c7c\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u667a\u80fd\u5b89\u5168\u7cfb\u7edf\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u662f\u4e25\u91cd\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u7684\u68c0\u6d4b\u6280\u672f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5e94\u5bf9\u590d\u6742\u9493\u9c7c\u624b\u6cd5\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u591a\u6a21\u6001\u548c\u8bed\u8a00\u5b66\u7279\u5f81\u7684\u5206\u6790\u4e0d\u5145\u5206\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aFew-shot Adaptive Linguistic Prompting\uff08ALP\uff09\u7684\u7ed3\u6784\u5316\u8bed\u4e49\u63a8\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\u548cGemini 1.5 Pro\uff09\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002ALP\u901a\u8fc7\u5f15\u5bfcLLM\u5206\u89e3\u8bed\u8a00\u6a21\u5f0f\u3001\u68c0\u6d4b\u7d27\u6025\u4fe1\u53f7\u548c\u8bc6\u522b\u64cd\u63a7\u6027\u7528\u8bed\uff0c\u5b9e\u73b0\u5bf9\u6587\u672c\u3001\u56fe\u7247\u548cURL\u7684\u7edf\u4e00\u5206\u6790\u3002", "result": "ALP\u65b9\u6cd5\u80fd\u660e\u663e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u9493\u9c7c\u7f51\u9875\u68c0\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e860.93\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "ALP\u96c6\u6210\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u6784\u5efa\u66f4\u5065\u58ee\u3001\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7684\u57fa\u4e8e\u8bed\u8a00\u5b66\u7684\u9493\u9c7c\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.13533", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13533", "abs": "https://arxiv.org/abs/2507.13533", "authors": ["Priyam Gupta"], "title": "Increasing the Expressiveness of a Gradual Verifier", "comment": "Presented at the 52nd ACM SIGPLAN Symposium on Principles of\n  Programming Languages (POPL 2025) Student Research Competition", "summary": "Static verification provides strong correctness guarantees for code; however,\nfully specifying programs for static verification is a complex, burdensome\nprocess for users. Gradual verification was introduced to make this process\neasier by supporting the verification of partially specified programs. The only\ncurrently working gradual verifier, Gradual C0, successfully verifies heap\nmanipulating programs, but lacks expressiveness in its specification language.\nThis paper describes the design and implementation of an extension to Gradual\nC0 that supports unfolding expressions, which allow more intuitive\nspecifications of recursive heap data structures.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86Gradual C0\uff0c\u652f\u6301\u66f4\u6613\u7528\u7684\u9012\u5f52\u7ed3\u6784\u8868\u8fbe\uff0c\u63d0\u5347\u4e86\u6e10\u8fdb\u5f0f\u9759\u6001\u9a8c\u8bc1\u7684\u89c4\u8303\u80fd\u529b\u3002", "motivation": "\u5f53\u524dGradual C0\u867d\u53ef\u5b9e\u73b0\u90e8\u5206\u89c4\u8303\u7a0b\u5e8f\u7684\u9759\u6001\u9a8c\u8bc1\uff0c\u4f46\u89c4\u8303\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u96be\u4ee5\u4f18\u96c5\u63cf\u8ff0\u9012\u5f52\u5806\u7ed3\u6784\uff0c\u5f71\u54cd\u4e86\u5176\u5b9e\u7528\u6027\u548c\u76f4\u89c2\u6027\u3002", "method": "\u5728\u73b0\u6709Gradual C0\u7cfb\u7edf\u57fa\u7840\u4e0a\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u5bf9\"unfolding expressions\"\u7684\u652f\u6301\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u89c4\u8303\u8bed\u8a00\u4e2d\u4ee5\u8868\u8fbe\u9012\u5f52\u7ed3\u6784\u3002", "result": "\u6269\u5c55\u540e\u7684Gradual C0\u53ef\u652f\u6301\u66f4\u76f4\u89c2\u548c\u81ea\u7136\u63cf\u8ff0\u9012\u5f52\u5806\u6570\u636e\u7ed3\u6784\u7684\u89c4\u8303\uff0c\u5927\u5927\u589e\u5f3a\u4e86\u5176\u8868\u8fbe\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86Gradual C0\u7684\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u76f4\u89c2\u5730\u5bf9\u9012\u5f52\u5806\u6570\u636e\u7ed3\u6784\u8fdb\u884c\u89c4\u8303\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u90e8\u5206\u89c4\u8303\u7a0b\u5e8f\u7684\u9759\u6001\u9a8c\u8bc1\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2507.13895", "categories": ["cs.LO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13895", "abs": "https://arxiv.org/abs/2507.13895", "authors": ["Damiano Azzolini", "Marco Duca", "Stefano Forti", "Francesco Gallo", "Antonio Ielo"], "title": "Application Placement with Constraint Relaxation", "comment": null, "summary": "Novel utility computing paradigms rely upon the deployment of multi-service\napplications to pervasive and highly distributed cloud-edge infrastructure\nresources. Deciding onto which computational nodes to place services in\ncloud-edge networks, as per their functional and non-functional constraints,\ncan be formulated as a combinatorial optimisation problem. Most existing\nsolutions in this space are not able to deal with \\emph{unsatisfiable} problem\ninstances, nor preferences, i.e. requirements that DevOps may agree to relax to\nobtain a solution. In this article, we exploit Answer Set Programming\noptimisation capabilities to tackle this problem. Experimental results in\nsimulated settings show that our approach is effective on lifelike networks and\napplications.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u7528Answer Set Programming\u4f18\u5316\u591a\u670d\u52a1\u5e94\u7528\u90e8\u7f72\u5230\u4e91\u8fb9\u7f51\u7edc\u7684\u95ee\u9898\uff0c\u80fd\u5f88\u597d\u5904\u7406\u504f\u597d\u548c\u4e0d\u53ef\u6ee1\u8db3\u7ea6\u675f\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u591a\u670d\u52a1\u5e94\u7528\u9700\u90e8\u7f72\u5728\u9ad8\u5ea6\u5206\u5e03\u7684\u4e91\u8fb9\u57fa\u7840\u8bbe\u65bd\u4e0a\uff0c\u800c\u5982\u4f55\u6839\u636e\u4e0d\u540c\u7ea6\u675f\u9ad8\u6548\u9009\u62e9\u8ba1\u7b97\u8282\u70b9\u5c5e\u4e8e\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u4e0d\u53ef\u6ee1\u8db3\u5b9e\u4f8b\u4e0e\u504f\u597d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u4f18\u5316\u6280\u672f\u3002", "method": "\u5229\u7528Answer Set Programming\uff08ASP\uff09\u7684\u4f18\u5316\u80fd\u529b\uff0c\u5bf9\u4e91\u8fb9\u7f51\u7edc\u7684\u670d\u52a1\u90e8\u7f72\u95ee\u9898\u8fdb\u884c\u5efa\u6a21\u4e0e\u6c42\u89e3\uff0c\u80fd\u591f\u5904\u7406\u529f\u80fd\u6027\u4e0e\u975e\u529f\u80fd\u6027\u7ea6\u675f\uff0c\u5305\u62ec\u90a3\u4e9b\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u7ea6\u675f\u7684\u60c5\u5f62\u53ca\u504f\u597d\u8c03\u6574\u3002", "result": "\u63d0\u51fa\u7684ASP\u65b9\u6cd5\u5728\u6a21\u62df\u73b0\u5b9e\u7f51\u7edc\u4e0e\u5e94\u7528\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u670d\u52a1\u90e8\u7f72\u4f18\u5316\uff0c\u663e\u793a\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u95ee\u9898\u5904\u7406\u80fd\u529b\uff0c\u5305\u62ec\u5bf9\u504f\u597d\u548c\u4e0d\u53ef\u6ee1\u8db3\u7ea6\u675f\u7684\u652f\u6301\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8eAnswer Set Programming\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4e91\u8fb9\u7f51\u7edc\u4e2d\u591a\u670d\u52a1\u5e94\u7528\u7684\u90e8\u7f72\u51b3\u7b56\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5bf9\u4e0d\u53ef\u6ee1\u8db3\u7ea6\u675f\u548c\u504f\u597d\u8fdb\u884c\u5904\u7406\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e0b\u5177\u6709\u6709\u6548\u6027\u3002"}}
{"id": "2507.13499", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13499", "abs": "https://arxiv.org/abs/2507.13499", "authors": ["Chandra Maddila", "Negar Ghorbani", "James Saindon", "Parth Thakkar", "Vijayaraghavan Murali", "Rui Abreu", "Jingyue Shen", "Brian Zhou", "Nachiappan Nagappan", "Peter C. Rigby"], "title": "AI-Assisted Fixes to Code Review Comments at Scale", "comment": null, "summary": "Aim. There are 10s of thousands of code review comments each week at Meta. We\ndeveloped Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes\nfor reviewer comments in production at scale.\n  Method. We developed an internal benchmark of 64k <review comment, patch>\ndata points to fine-tune Llama models. Once our models achieve reasonable\noffline results, we roll them into production. To ensure that our AI-assisted\nfixes do not negatively impact the time it takes to do code reviews, we conduct\nrandomized controlled safety trials as well as full production experiments.\n  Offline Results. As a baseline, we compare GPT-4o to our small and large\nLlama models. In offline results, our LargeLSFT model creates an exact match\npatch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The\ninternal models also use more modern Hack functions when compared to the PHP\nfunctions suggested by GPT-4o.\n  Safety Trial. When we roll MetaMateCR into production in a safety trial that\ncompares no AI patches with AI patch suggestions, we see a large regression\nwith reviewers taking over 5% longer to conduct reviews. After investigation,\nwe modify the UX to only show authors the AI patches, and see no regressions in\nthe time for reviews.\n  Production. When we roll LargeLSFT into production, we see an\nActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.\nOur results illustrate the importance of safety trials in ensuring that AI does\nnot inadvertently slow down engineers, and a successful review comment to AI\npatch product running at scale.", "AI": {"tldr": "Meta\u9488\u5bf9\u5927\u89c4\u6a21\u4ee3\u7801\u5ba1\u67e5\u5f00\u53d1\u4e86AI\u8f85\u52a9\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u7814Llama\u6a21\u578b\u548c\u4e25\u683c\u7684\u5b89\u5168\u6027\u6d4b\u8bd5\uff0c\u5728\u4e0d\u5f71\u54cd\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8865\u4e01\u5efa\u8bae\u6548\u679c\u4f18\u4e8eGPT-4o\u3002", "motivation": "Meta\u5728\u6bcf\u5468\u9700\u8981\u5904\u7406\u6570\u4ee5\u4e07\u8ba1\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7AI\u8f85\u52a9\uff0c\u63d0\u5347\u5904\u7406\u8bc4\u8bba\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u56e2\u961f\u5f00\u53d1\u4e86\u5185\u90e8\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u542b6.4\u4e07\u4e2a<\u5ba1\u67e5\u8bc4\u8bba\uff0c\u8865\u4e01>\u6837\u672c\uff09\uff0c\u7528\u4e8e\u5bf9Llama\u6a21\u578b\u5fae\u8c03\u3002\u6a21\u578b\u79bb\u7ebf\u6548\u679c\u7406\u60f3\u540e\uff0c\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u5b89\u5168\u6027\u8bd5\u9a8c\u548c\u5168\u9762\u751f\u4ea7\u5b9e\u9a8c\uff0c\u5c06\u6a21\u578b\u90e8\u7f72\u5230\u7ebf\u4e0a\u4ee5\u534f\u52a9\u4ee3\u7801\u5ba1\u67e5\u3002\u8bc4\u4f30\u4e86GPT-4o\u548c\u81ea\u7814\u5927\u5c0fLlama\u6a21\u578b\u7684\u79bb\u7ebf\u8868\u73b0\uff1b\u4e0a\u7ebf\u8bd5\u9a8c\u65f6\u901a\u8fc7\u8c03\u6574UX\uff0c\u4f18\u5316AI\u8f85\u52a9\u65b9\u5f0f\uff0c\u786e\u4fdd\u4e0d\u5f71\u54cd\u5ba1\u67e5\u6548\u7387\u3002", "result": "\u79bb\u7ebf\u8bc4\u6d4b\u4e2d\uff0c\u5927\u578bLlama\u6a21\u578b\uff08LargeLSFT\uff09\u751f\u6210\u7684\u8865\u4e01\u4e0e\u6807\u51c6\u8865\u4e01\u7cbe\u786e\u5339\u914d\u7387\u4e3a68%\uff0c\u6bd4GPT-4o\u9ad89\u4e2a\u767e\u5206\u70b9\u3002\u5b89\u5168\u6027\u8bd5\u9a8c\u53d1\u73b0AI\u8865\u4e01\u76f4\u63a5\u5c55\u73b0\u7ed9\u5ba1\u67e5\u8005\u5bfc\u81f4\u5e73\u5747\u5ba1\u67e5\u7528\u65f6\u589e\u52a05%\uff0c\u8c03\u6574\u4e3a\u53ea\u5c55\u793a\u7ed9\u4ee3\u7801\u4f5c\u8005\u540e\uff0c\u5ba1\u67e5\u65f6\u957f\u4e0d\u518d\u56de\u5f52\u3002\u6700\u7ec8\u843d\u5730\u751f\u4ea7\u65f6\uff0cLargeLSFT\u7684ActionableToApplied\uff08\u5efa\u8bae\u88ab\u5b9e\u9645\u91c7\u7528\uff09\u6bd4\u7387\u8fbe19.7%\uff0c\u6bd4GPT-4o\u9ad89.2\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u7ec6\u7684\u5185\u90e8\u57fa\u51c6\u548c\u5b89\u5168\u6027\u8bd5\u9a8c\uff0cMeta\u6210\u529f\u90e8\u7f72\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684AI\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u5728\u4e0d\u964d\u4f4e\u5ba1\u67e5\u6548\u7387\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u9ad8\u5efa\u8bae\u8865\u4e01\u7684\u8d28\u91cf\u548c\u5e94\u7528\u7387\u3002"}}
{"id": "2507.13380", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13380", "abs": "https://arxiv.org/abs/2507.13380", "authors": ["Keito Inoshita", "Rushia Harada"], "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.", "AI": {"tldr": "PersonaGen\u7ed3\u5408\u865a\u62df\u4eba\u683c\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u60c5\u611f\u6587\u672c\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6709\u671b\u89e3\u51b3\u60c5\u611f\u8bc6\u522b\u9886\u57df\u6570\u636e\u74f6\u9888\u3002", "motivation": "\u60c5\u611f\u8bc6\u522b\u9886\u57df\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u60c5\u611f\u6570\u636e\u96c6\u7684\u7a00\u7f3a\uff0c\u56e0\u4e2a\u4eba\u3001\u6587\u5316\u3001\u73af\u5883\u4e3b\u89c2\u6027\u5f3a\uff0c\u6784\u5efa\u89c4\u6a21\u5316\u3001\u901a\u7528\u7684\u6570\u636e\u96c6\u5177\u6709\u4f26\u7406\u548c\u5b9e\u9645\u6311\u6218\u3002", "method": "\u63d0\u51faPersonaGen\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4ee5\u4eba\u683c\u4e3a\u57fa\u7840\u7684\u6761\u4ef6\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u6587\u672c\u3002PersonaGen\u7efc\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u793e\u4f1a\u6587\u5316\u80cc\u666f\u4e0e\u60c5\u5883\u4e0a\u4e0b\u6587\u7b49\uff0c\u6784\u5efa\u5206\u5c42\u865a\u62df\u4eba\u683c\u4ee5\u6307\u5bfc\u60c5\u611f\u8868\u8fbe\u751f\u6210\u3002\u751f\u6210\u6570\u636e\u7ecf\u591a\u79cd\u7ef4\u5ea6\u8bc4\u6d4b\uff0c\u5305\u62ec\u8bed\u4e49\u591a\u6837\u6027\u805a\u7c7b\u3001\u5206\u5e03\u6307\u6807\u3001\u4eba\u7c7b\u5316LLM\u8bc4\u5206\u3001\u4e0e\u771f\u5b9e\u8bed\u6599\u5bf9\u6bd4\u4ee5\u53ca\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPersonaGen\u5728\u751f\u6210\u591a\u6837\u3001\u8fde\u8d2f\u4e14\u5177\u8fa8\u522b\u6027\u7684\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4eba\u5de5\u6216\u771f\u5b9e\u60c5\u611f\u6570\u636e\u96c6\u7684\u8865\u5145\u6216\u66ff\u4ee3\u80fd\u529b\u3002", "conclusion": "PersonaGen\u80fd\u9ad8\u8d28\u91cf\u751f\u6210\u591a\u6837\u3001\u771f\u5b9e\u7684\u4eba\u683c\u5316\u60c5\u611f\u8868\u8fbe\uff0c\u662f\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u9886\u57df\u91cd\u8981\u7684\u6570\u636e\u589e\u5f3a\u548c\u6570\u636e\u96c6\u66ff\u4ee3\u8def\u5f84\u3002"}}
{"id": "2507.13774", "categories": ["cs.PL", "cs.LO", "D.3.1; F.3.2; F.4.1"], "pdf": "https://arxiv.org/pdf/2507.13774", "abs": "https://arxiv.org/abs/2507.13774", "authors": ["Arthur Adjedj", "Meven Lennon-Bertrand", "Thibaut Benjamin", "Kenji Maillard"], "title": "AdapTT: Functoriality for Dependent Type Casts", "comment": null, "summary": "The ability to cast values between related types is a leitmotiv of many\nflavors of dependent type theory, such as observational type theories,\nsubtyping, or cast calculi for gradual typing. These casts all exhibit a common\nstructural behavior that boils down to the pervasive functoriality of type\nformers. We propose and extensively study a type theory, called AdapTT, which\nmakes systematic and precise this idea of functorial type formers, with respect\nto an abstract notion of adapters relating types. Leveraging descriptions for\nfunctorial inductive types in AdapTT, we derive structural laws for type casts\non general inductive type formers.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdapTT\u7c7b\u578b\u7406\u8bba\uff0c\u901a\u8fc7\u9002\u914d\u5668\u7cfb\u7edf\u5316\u5730\u523b\u753b\u7c7b\u578b\u6784\u9020\u5b50\u7684\u51fd\u5b50\u6027\uff0c\u5e76\u63a8\u5bfc\u5f52\u7eb3\u7c7b\u578b\u6784\u9020\u5b50\u7684\u8f6c\u6362\u5b9a\u5f8b\uff0c\u4e3a\u7c7b\u578bcast\u63d0\u4f9b\u7edf\u4e00\u7ed3\u6784\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u591a\u79cd\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\uff08\u5982\u89c2\u5bdf\u578b\u7c7b\u578b\u7406\u8bba\u3001\u5b50\u7c7b\u578b\u3001\u4ee5\u53ca\u6e10\u8fdb\u5f0f\u7c7b\u578b\u7684cast calculus\uff09\u4e2d\uff0c\u503c\u5728\u76f8\u5173\u7c7b\u578b\u95f4\u8fdb\u884c\u8f6c\u6362\uff08cast\uff09\u5f88\u5e38\u89c1\u3002\u5c3d\u7ba1\u8fd9\u4e9bcast\u770b\u4f3c\u5404\u5f02\uff0c\u4f46\u5728\u7c7b\u578b\u6784\u9020\u5b50\u7684\u51fd\u5b50\u6027\uff08functoriality\uff09\u65b9\u9762\u5177\u6709\u7ed3\u6784\u4e0a\u7684\u5171\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u5171\u6027\u5c1a\u672a\u88ab\u7cfb\u7edf\u6027\u548c\u7cbe\u786e\u5730\u523b\u753b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u7684\u7c7b\u578b\u7406\u8bbaAdapTT\uff0c\u8be5\u7406\u8bba\u4ee5\u9002\u914d\u5668\uff08adapter\uff09\u7684\u62bd\u8c61\u6982\u5ff5\u4e3a\u57fa\u7840\uff0c\u7cfb\u7edf\u5316\u5730\u5c06\u7c7b\u578b\u6784\u9020\u5b50\u7684\u51fd\u5b50\u6027\u7406\u5ff5\u5e94\u7528\u4e8e\u7c7b\u578b\u8f6c\u6362\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5229\u7528AdapTT\u4e2d\u5bf9\u51fd\u5b50\u578b\u5f52\u7eb3\u7c7b\u578b\uff08functorial inductive types\uff09\u7684\u63cf\u8ff0\uff0c \u63a8\u5bfc\u51fa\u4e00\u822c\u5f52\u7eb3\u7c7b\u578b\u6784\u9020\u5b50\u4e4b\u95f4\u7c7b\u578b\u8f6c\u6362\u7684\u7ed3\u6784\u6027\u5b9a\u5f8b\u3002", "result": "AdapTT\u7406\u8bba\u80fd\u591f\u7cfb\u7edf\u5730\u63cf\u8ff0\u7c7b\u578b\u4e4b\u95f4\u7531\u62bd\u8c61\u9002\u914d\u5668\u8054\u7cfb\u4e0b\u7684\u51fd\u5b50\u6027\u7c7b\u578b\u6784\u9020\u3002\u501f\u6b64\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u4e86\u9002\u7528\u4e8e\u4e00\u822c\u5f52\u7eb3\u7c7b\u578b\u6784\u9020\u5b50\u7684\u7c7b\u578b\u8f6c\u6362\u7ed3\u6784\u5b9a\u5f8b\u3002", "conclusion": "AdapTT\u7406\u8bba\u4e3a\u5404\u7c7b\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u4e2d\u7684\u503c\u8f6c\u6362\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u660e\u786e\u4e14\u7cfb\u7edf\u5730\u63ed\u793a\u4e86\u7c7b\u578b\u6784\u9020\u5b50\u51fd\u5b50\u6027\u548c\u7c7b\u578b\u8f6c\u6362\u95f4\u7684\u6df1\u5c42\u6b21\u7ed3\u6784\u5173\u7cfb\u3002"}}
{"id": "2507.13946", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13946", "abs": "https://arxiv.org/abs/2507.13946", "authors": ["Tadeusz Litak", "Katsuhiko Sano"], "title": "Bounded Inquisitive Logics: Sequent Calculi and Schematic Validity", "comment": "This is a modified and expanded version of a paper accepted for\n  TABLEAUX 2025. In particular, readers should note that the numeration of\n  environments is different in the conference version", "summary": "Propositional inquisitive logic is the limit of its $n$-bounded\napproximations. In the predicate setting, however, this does not hold anymore,\nas discovered by Ciardelli and Grilletti, who also found complete\naxiomatizations of $n$-bounded inquisitive logics $\\mathsf{InqBQ}_{n}$, for\nevery fixed $n$. We introduce cut-free labelled sequent calculi for these\nlogics. We illustrate the intricacies of \\textit{schematic validity} in such\nsystems by showing that the well-known Casari formula is \\textit{atomically}\nvalid in (a weak sublogic of) predicate inquisitive logic $\\mathsf{InqBQ}$,\nfails to be schematically valid in it, and yet is schematically valid under the\nfinite boundedness assumption. The derivations in our calculi, however, are\nguaranteed to be schematically valid whenever a single specific rule is not\nused.", "AI": {"tldr": "\u63d0\u51fa\u4e86$n$-\u6709\u754c\u8c13\u8bcd\u63a2\u8be2\u903b\u8f91\u7684\u65e0cut\u6807\u8bb0\u5e8f\u5217\u6f14\u7b97\uff0c\u5206\u6790\u4e86Casari\u516c\u5f0f\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u6709\u6548\u6027\u4e0e\u5177\u4f53\u89c4\u5219\u53ca\u6709\u754c\u6027\u7684\u5173\u7cfb\u3002", "motivation": "\u547d\u9898\u63a2\u8be2\u903b\u8f91\u5728$n$-\u6709\u754c\u8fd1\u4f3c\u4e0b\u6210\u7acb\uff0c\u4f46\u5728\u8c13\u8bcd\u8bbe\u5b9a\u4e0b\u4e0d\u518d\u6210\u7acb\u3002Ciardelli \u548c Grilletti \u5df2\u4e3a\u6bcf\u4e2a\u56fa\u5b9an\u7ed9\u51fa\u4e86$n$-\u6709\u754c\u63a2\u8be2\u903b\u8f91\u7684\u5b8c\u5907\u516c\u7406\u7cfb\u7edf\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u4e3a\u8fd9\u4e9b\u903b\u8f91\u63d0\u4f9b\u65e0cut\u7684\u6807\u8bb0\u5e8f\u5217\u6f14\u7b97\u7cfb\u7edf\uff0c\u5e76\u63a2\u8ba8\u5176\u56fe\u5f0f\u6709\u6548\u6027\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u4e86\u9488\u5bf9$n$-\u6709\u754c\u63a2\u8be2\uff08predicate inquisitive\uff09\u903b\u8f91\u7684\u65b0\u578b\u65e0cut\u6807\u8bb0\u5e8f\u5217\u6f14\u7b97\u7cfb\u7edf\u3002\u901a\u8fc7\u5206\u6790Casari\u516c\u5f0f\u5728\u4e0d\u540c\u5b50\u903b\u8f91\u53ca\u5047\u8bbe\u4e0b\u7684\u56fe\u5f0f\u6709\u6548\u6027\uff0c\u7814\u7a76\u4e86\u63a8\u5bfc\u8fc7\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u4fdd\u8bc1\u6761\u4ef6\u3002", "result": "\u6784\u5efa\u4e86$n$-\u6709\u754cpredicate inquisitive\u903b\u8f91\u7684\u65e0cut\u6807\u8bb0\u5e8f\u5217\u6f14\u7b97\uff0c\u5e76\u5c55\u793a\uff1aCasari\u516c\u5f0f\u5728\u67d0\u5f31\u5b50\u903b\u8f91\u4e2d\u539f\u5b50\u6709\u6548\uff0c\u5728\u5168\u4f53\u903b\u8f91\u4e2d\u4e0d\u8db3\u4ee5\u56fe\u5f0f\u6709\u6548\uff0c\u800c\u5728\u6709\u9650\u6709\u754c\u60c5\u51b5\u4e0b\uff08n\u6709\u754c\uff09\u5219\u53ef\u56fe\u5f0f\u6709\u6548\u3002\u53ea\u8981\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u7528\u67d0\u7279\u5b9a\u89c4\u5219\uff0c\u63a8\u5bfc\u5373\u53ef\u4fdd\u8bc1\u56fe\u5f0f\u6709\u6548\u3002", "conclusion": "\u9996\u6b21\u63d0\u51fa\u4e86$n$-\u6709\u754c\u8c13\u8bcd\u63a2\u8be2\u903b\u8f91\u7684\u65e0cut\u6807\u8bb0\u5e8f\u5217\u6f14\u7b97\uff0c\u5c06\u56fe\u5f0f\u6709\u6548\u6027\u4e0e\u63a8\u5bfc\u89c4\u5219\u5177\u4f53\u8054\u7cfb\uff0c\u5e76\u63ed\u793a\u4e86\u6709\u9650\u6709\u754c\u6761\u4ef6\u4e0b\u4e0e\u5168\u5c40\u6027\u5224\u65ad\u7684\u5dee\u5f02\u3002"}}
{"id": "2507.13553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13553", "abs": "https://arxiv.org/abs/2507.13553", "authors": ["Pragyan K C", "Rambod Ghandiparsi", "Thomas Herron", "John Heaps", "Mitra Bokaei Hosseini"], "title": "Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software", "comment": "Accepted at the 9th International Workshop on Crowd-Based\n  Requirements Engineering (CrowdRE'25)", "summary": "As user demands evolve, effectively incorporating feature requests is crucial\nfor maintaining software relevance and user satisfaction. Feature requests,\ntypically expressed in natural language, often suffer from ambiguity or\nincomplete information due to communication gaps or the requester's limited\ntechnical expertise. These issues can lead to misinterpretation, faulty\nimplementation, and reduced software quality. While seeking clarification from\nrequesters is a common strategy to mitigate these risks, little is known about\nhow developers engage in this clarification process in practice-how they\nformulate clarifying questions, seek technical or contextual details, align on\ngoals and use cases, or decide to close requests without attempting\nclarification. This study investigates how feature requests are prone to NL\ndefects (i.e. ambiguous or incomplete) and the conversational dynamics of\nclarification in open-source software (OSS) development, aiming to understand\nhow developers handle ambiguous or incomplete feature requests. Our findings\nsuggest that feature requests published on the OSS platforms do possess\nambiguity and incompleteness, and in some cases, both. We also find that\nexplicit clarification for the resolution of these defects is uncommon;\ndevelopers usually focus on aligning with project goals rather than resolving\nunclear text. When clarification occurs, it emphasizes understanding user\nintent/goal and feasibility, rather than technical details. By characterizing\nthe dynamics of clarification in open-source issue trackers, this work\nidentifies patterns that can improve user-developer collaboration and inform\nbest practices for handling feature requests effectively.", "AI": {"tldr": "\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u529f\u80fd\u8bf7\u6c42\u5e38\u5e38\u5b58\u5728\u6b67\u4e49\u548c\u4fe1\u606f\u4e0d\u5168\uff0c\u4f46\u5f00\u53d1\u8005\u9c9c\u5c11\u4e3b\u52a8\u6f84\u6e05\uff0c\u4e3b\u8981\u5173\u6ce8\u4e0e\u9879\u76ee\u76ee\u6807\u4e00\u81f4\u3002\u8be5\u7814\u7a76\u603b\u7ed3\u4e86\u6c9f\u901a\u8fc7\u7a0b\u4e2d\u7684\u6a21\u5f0f\uff0c\u53ef\u4e3a\u6539\u8fdb\u7528\u6237-\u5f00\u53d1\u8005\u534f\u4f5c\u63d0\u4f9b\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u7528\u6237\u9700\u6c42\u7684\u53d8\u5316\uff0c\u6709\u6548\u5730\u91c7\u7eb3\u529f\u80fd\u8bf7\u6c42\u5bf9\u8f6f\u4ef6\u4fdd\u6301\u76f8\u5173\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7528\u6237\u7528\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u8bf7\u6c42\u65f6\u5e38\u5e38\u5e26\u6709\u6b67\u4e49\u6216\u4e0d\u5b8c\u6574\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5f00\u53d1\u8005\u8bef\u89e3\u548c\u8f6f\u4ef6\u8d28\u91cf\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5f00\u53d1\u8005\u5b9e\u9645\u662f\u5982\u4f55\u5904\u7406\u8fd9\u4e9b\u6a21\u7cca\u548c\u4e0d\u5b8c\u6574\u8bf7\u6c42\u7684\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u4e2d\u7684\u529f\u80fd\u8bf7\u6c42\uff0c\u8003\u5bdf\u5176\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\uff08\u6b67\u4e49\u6216\u4fe1\u606f\u4e0d\u5b8c\u6574\uff09\u548c\u6f84\u6e05\u5bf9\u8bdd\u52a8\u6001\uff0c\u65e8\u5728\u63ed\u793a\u5f00\u53d1\u8005\u5728\u5904\u7406\u6a21\u7cca\u529f\u80fd\u8bf7\u6c42\u65f6\u7684\u5b9e\u9645\u884c\u4e3a\u548c\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1)\u5f00\u6e90\u5e73\u53f0\u4e0a\u7684\u529f\u80fd\u8bf7\u6c42\u5e7f\u6cdb\u5b58\u5728\u6b67\u4e49\u548c\u4e0d\u5b8c\u6574\u6027\uff0c\u90e8\u5206\u8bf7\u6c42\u4e8c\u8005\u517c\u6709\uff1b2)\u5f00\u53d1\u8005\u5f88\u5c11\u4e3b\u52a8\u8fdb\u884c\u660e\u786e\u6f84\u6e05\uff0c\u591a\u6570\u5173\u6ce8\u4e0e\u9879\u76ee\u76ee\u6807\u4e00\u81f4\u6027\u800c\u975e\u5177\u4f53\u6587\u5b57\u7684\u6d88\u6b67\uff1b3)\u53d1\u751f\u6f84\u6e05\u65f6\u66f4\u6ce8\u91cd\u76ee\u6807/\u53ef\u884c\u6027\u7406\u89e3\uff0c\u800c\u4e0d\u662f\u6280\u672f\u7ec6\u8282\u3002", "conclusion": "\u6f84\u6e05\u5bf9\u8bdd\u5728\u5904\u7406OSS\u529f\u80fd\u8bf7\u6c42\u7f3a\u9677\u4e2d\u8f83\u4e3a\u7f55\u89c1\uff0c\u5f00\u53d1\u8005\u66f4\u503e\u5411\u4e8e\u628a\u91cd\u70b9\u653e\u5728\u9879\u76ee\u76ee\u6807\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u63cf\u8ff0\u6f84\u6e05\u52a8\u6001\uff0c\u7814\u7a76\u603b\u7ed3\u63d0\u5347\u7528\u6237\u4e0e\u5f00\u53d1\u8005\u534f\u4f5c\u3001\u6709\u6548\u5904\u7406\u529f\u80fd\u8bf7\u6c42\u7684\u5178\u578b\u6a21\u5f0f\uff0c\u53ef\u4e3a\u5f00\u6e90\u534f\u4f5c\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.13381", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13381", "abs": "https://arxiv.org/abs/2507.13381", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan G\u00fcnnemann"], "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models.", "AI": {"tldr": "\u63d0\u51faSAFT\u65b9\u6cd5\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u4fbf\u80fd\u5c06AMR\u56fe\u7ed3\u6784\u4fe1\u606f\u6ce8\u5165LLM\uff0c\u5728AMR-to-text\u4efb\u52a1\u4e0a\u5927\u5e45\u63d0\u5347LLM\u751f\u6210\u8d28\u91cf\u5e76\u5237\u65b0SOTA\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u5982\u56fe\u7ed3\u6784\u5316\u8f93\u5165\uff08\u5982AMR\u8bed\u4e49\u56fe\uff09\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u56fe\u7ed3\u6784\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u4ec5\u901a\u8fc7\u4efb\u610f\u7ebf\u6027\u5316\u7ed3\u6784\u6216\u91c7\u7528\u4e0e\u4e3b\u6d41LLM\u4e0d\u517c\u5bb9\u7684\u67b6\u6784\uff0c\u5bfc\u81f4\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u5fae\u8c03\uff08SAFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97AMR\u56fe\u7684\u78c1\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u751f\u6210\u65b9\u5411\u654f\u611f\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230LLM\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u65e0\u9700\u6539\u53d8\u73b0\u6709LLM\u67b6\u6784\u3002", "result": "\u5728AMR 3.0\u57fa\u51c6\u4e0a\u83b7\u5f97\u4e86\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u9ad83.5 BLEU\u7684\u8868\u73b0\u3002\u6027\u80fd\u63d0\u5347\u968f\u56fe\u7684\u590d\u6742\u6027\u589e\u52a0\u800c\u6269\u5927\uff0c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u7ed3\u6784\u611f\u77e5\u8868\u5f81\u7684\u91cd\u8981\u6027\u3002", "conclusion": "SAFT\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u6865\u6881\u65b9\u5f0f\uff0c\u53ef\u63d0\u5347LLM\u5bf9\u56fe\u7ed3\u6784\u8f93\u5165\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2507.13792", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13792", "abs": "https://arxiv.org/abs/2507.13792", "authors": ["Riccardo Bianchini", "Francesco Dagnino", "Paola Giannini", "Elena Zucca"], "title": "Don't exhaust, don't waste", "comment": "Submitted to JFP (Journal of Functional Programming)", "summary": "We extend the semantics and type system of a lambda calculus equipped with\ncommon constructs to be resource-aware. That is, the semantics keep tracks of\nthe usage of resources, and is stuck, besides in case of type errors, if either\na needed resource is exhausted, or a provided resource would be wasted. In such\nway, the type system guarantees, besides standard soundness, that for\nwell-typed programs there is a computation where no resource gets either\nexhausted or wasted.\n  The no-waste extension is parametric on an arbitrary grade algebra, modeling\nan arbitrary assortment of possible usages, and does not require ad-hoc changes\nto the underlying language. To this end, the semantics needs to be formalized\nin big-step style; as a consequence, expressing and proving (resource-aware)\nsoundness is challenging, and is achieved by applying recent techniques based\non coinductive reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u80fd\u8ffd\u8e2a\u5e76\u63a7\u5236\u8d44\u6e90\u4f7f\u7528\u7684lambda\u6f14\u7b97\u7c7b\u578b\u7cfb\u7edf\uff0c\u7406\u8bba\u4e0a\u4fdd\u969c\u7c7b\u578b\u826f\u597d\u7684\u7a0b\u5e8f\u6267\u884c\u4e0d\u4f1a\u5bfc\u81f4\u8d44\u6e90\u8017\u5c3d\u6216\u6d6a\u8d39\uff0c\u5e76\u901a\u8fc7\u5148\u8fdb\u7684\u8bc1\u660e\u6280\u672f\u5b9e\u73b0\u4e86\u7cfb\u7edf\u5065\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684lambda\u6f14\u7b97\u548c\u7c7b\u578b\u7cfb\u7edf\u901a\u5e38\u4e0d\u8003\u8651\u8d44\u6e90\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u73b0\u5b9e\u4e2d\u7a0b\u5e8f\u7684\u8d44\u6e90\u7ba1\u7406\uff08\u5982\u5185\u5b58\u3001\u6743\u9650\u7b49\uff09\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u6b64\u9700\u8981\u5bf9lambda\u6f14\u7b97\u6269\u5c55\u4ee5\u5b9e\u73b0\u5bf9\u8d44\u6e90\u7684\u7cbe\u786e\u8ffd\u8e2a\u548c\u63a7\u5236\u3002", "method": "\u6269\u5c55lambda\u6f14\u7b97\u7684\u8bed\u4e49\u548c\u7c7b\u578b\u7cfb\u7edf\uff0c\u5f15\u5165\u8d44\u6e90\u611f\u77e5\u7279\u6027\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5728\u8bed\u4e49\u5c42\u9762\u8ffd\u8e2a\u8d44\u6e90\u7684\u4f7f\u7528\uff0c\u5728\u8d44\u6e90\u8017\u5c3d\u6216\u6d6a\u8d39\u65f6\u8fdb\u5165stuck\u72b6\u6001\u3002\u8be5\u7cfb\u7edf\u901a\u7528\u3001\u65e0\u9700\u5bf9\u5e95\u5c42\u8bed\u8a00\u4f5c\u51fa\u7279\u6b8a\u4fee\u6539\uff0c\u5e76\u901a\u8fc7\u5c06\u8bed\u4e49\u5f62\u5f0f\u5316\u4e3a\u5927\u6b65\u8bed\u4e49\u548c\u91c7\u7528\u4f59\u5f52\u7eb3\u63a8\u7406\u8bc1\u660e\u8d44\u6e90\u76f8\u5173\u7684\u7c7b\u578b\u7cfb\u7edf\u5065\u5168\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u53c2\u6570\u5316\u7684\u8d44\u6e90\u7c7b\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bbe\u8ba1\uff0c\u65e0\u8bba\u8d44\u6e90\u7528\u5c3d\u8fd8\u662f\u88ab\u6d6a\u8d39\uff0c\u7c7b\u578b\u7cfb\u7edf\u90fd\u80fd\u4fdd\u8bc1\u7c7b\u578b\u826f\u597d\u7684\u7a0b\u5e8f\u59cb\u7ec8\u5b58\u5728\u4e00\u4e2a\u8d44\u6e90\u4e0d\u4f1a\u88ab\u8017\u5c3d\u6216\u6d6a\u8d39\u7684\u6267\u884c\u8def\u5f84\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u8d44\u6e90\u611f\u77e5lambda\u6f14\u7b97\u7684\u5065\u5168\u7c7b\u578b\u7cfb\u7edf\uff0c\u4e3a\u7f16\u7a0b\u8bed\u8a00\u4e2d\u8d44\u6e90\u5b89\u5168\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u8d44\u6e90\u4e25\u683c\u7ba1\u63a7\u76f8\u5173\u7684\u8bed\u8a00\u8bbe\u8ba1\u53d1\u5c55\u3002"}}
{"id": "2507.13987", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13987", "abs": "https://arxiv.org/abs/2507.13987", "authors": ["Simon Fl\u00fcgel", "Martin Glauer", "Till Mossakowski", "Fabian Neuhaus"], "title": "ChemLog: Making MSOL Viable for Ontological Classification and Learning", "comment": null, "summary": "Despite its prevalence, in many domains, OWL is not expressive enough to\ndefine ontology classes. In this paper, we present an approach that allows to\nuse monadic second-order formalisations for ontology classification. As a case\nstudy, we have applied our approach to 14 peptide-related classes from the\nchemistry ontology ChEBI. For these classes, a monadic second-order logic\nformalisation has been developed and applied both to ChEBI as well as to 119\nmillion molecules from the chemistry database PubChem. While this logical\napproach alone is limited to classification for the specified classes (in our\ncase, (sub)classes of peptides), transformer deep learning models scale\nclassification to the whole of the ChEBI ontology. We show that when using the\nclassifications obtained by the logical approach as training data, the\nperformance of the deep learning models can be significantly enhanced.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u903b\u8f91\u65b9\u6cd5\u89e3\u51b3OWL\u8868\u8fbe\u80fd\u529b\u6709\u9650\u96be\u4ee5\u5206\u7c7b\u590d\u6742\u672c\u4f53\u7c7b\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u5316\u5b66\u672c\u4f53\u5206\u7c7b\u6548\u679c\u3002", "motivation": "OWL\u672c\u4f53\u5728\u5f88\u591a\u9886\u57df\u4e2d\u4f7f\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5b9a\u4e49\u590d\u6742\u672c\u4f53\u7c7b\u3002\u4f5c\u8005\u5e0c\u671b\u7a81\u7834OWL\u8868\u8fbe\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u672c\u4f53\u5206\u7c7b\u7684\u7cbe\u5ea6\u548c\u81ea\u52a8\u5316\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u4e00\u9636\u5355\u53d8\u9879\uff08monadic second-order\uff09\u903b\u8f91\u5bf9\u590d\u6742\u672c\u4f53\u7c7b\u8fdb\u884c\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u5c06\u8be5\u903b\u8f91\u5b9a\u4e49\u5e94\u7528\u4e8eChEBI\u672c\u4f53\u768414\u4e2a\u4e0e\u80bd\u76f8\u5173\u7684\u7c7b\u522b\u548cPubChem\u5316\u5b66\u6570\u636e\u5e93\u4e2d\u76841.19\u4ebf\u4e2a\u5206\u5b50\u3002\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u51fa\u7684\u7c7b\u522b\u4f5c\u4e3a\u6837\u672c\u8bad\u7ec3transformer\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u6269\u5c55\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u5355\u53d8\u9879\u4e8c\u9636\u903b\u8f91\u65b9\u6cd5\u6709\u6548\u652f\u6301\u672c\u4f53\u4e2d\u80bd\u7c7b\u76f8\u5173\u7c7b\u522b\u7684\u5206\u7c7b\u63a8\u7406\uff0ctransformer\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5229\u7528\u903b\u8f91\u5206\u7c7b\u7ed3\u679c\u505a\u4e3a\u8bad\u7ec3\u6570\u636e\u540e\uff0c\u63d0\u5347\u4e86\u5bf9\u6574\u4e2aChEBI\u672c\u4f53\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u903b\u8f91\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u4e0d\u4ec5\u7a81\u7834\u4e86OWL\u672c\u4f53\u8868\u8fbe\u7684\u9650\u5236\uff0c\u8fd8\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u5316\u5b66\u672c\u4f53\u5206\u7c7b\u7684\u81ea\u52a8\u5316\u548c\u51c6\u786e\u6027\uff0c\u5bf9\u672c\u4f53\u5de5\u7a0b\u548c\u667a\u80fd\u5206\u6790\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.13555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13555", "abs": "https://arxiv.org/abs/2507.13555", "authors": ["Pragyan K C", "Rambod Ghandiparsi", "Thomas Herron", "John Heaps", "Mitra Bokaei Hosseini"], "title": "Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software", "comment": "Accepted at the 33rd IEEE International Requirements Engineering 2025", "summary": "The growing popularity and widespread use of software applications (apps)\nacross various domains have driven rapid industry growth. Along with this\ngrowth, fast-paced market changes have led to constantly evolving software\nrequirements. Such requirements are often grounded in feature requests and\nenhancement suggestions, typically provided by users in natural language (NL).\nHowever, these requests often suffer from defects such as ambiguity and\nincompleteness, making them challenging to interpret. Traditional validation\nmethods (e.g., interviews and workshops) help clarify such defects but are\nimpractical in decentralized environments like open-source software (OSS),\nwhere change requests originate from diverse users on platforms like GitHub.\nThis paper proposes a novel approach leveraging Large Language Models (LLMs) to\ndetect and refine NL defects in feature requests. Our approach automates the\nidentification of ambiguous and incomplete requests and generates clarification\nquestions (CQs) to enhance their usefulness for developers. To evaluate its\neffectiveness, we apply our method to real-world OSS feature requests and\ncompare its performance against human annotations. In addition, we conduct\ninterviews with GitHub developers to gain deeper insights into their\nperceptions of NL defects, the strategies they use to address these defects,\nand the impact of defects on downstream software engineering (SE) tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u548c\u6539\u8fdb\u7528\u6237\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u51fa\u7684\u7279\u6027\u8bf7\u6c42\u7f3a\u9677\uff0c\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u6e90\u5f00\u53d1\u4e2d\u7279\u6027\u8bf7\u6c42\u7684\u8d28\u91cf\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u968f\u7740\u884c\u4e1a\u5bf9\u8f6f\u4ef6\u5e94\u7528\u9700\u6c42\u7684\u6fc0\u589e\uff0c\u7528\u6237\u7279\u6027\u548c\u6539\u8fdb\u5efa\u8bae\u4ee5\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\u5927\u91cf\u51fa\u73b0\uff0c\u800c\u8fd9\u4e9b\u8bf7\u6c42\u5e38\u5e38\u5b58\u5728\u6a21\u7cca\u3001\u4e0d\u5b8c\u6574\u7b49\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u5728\u5f00\u6e90\u8f6f\u4ef6\u73af\u5883\u4e0b\uff0c\u4f20\u7edf\u7684\u4eba\u5de5\u9a8c\u8bc1\u65b9\u6cd5\u975e\u5e38\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u6539\u8fdb\u8fd9\u4e9b\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b9\u6cd5\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u6539\u8fdb\u7279\u6027\u8bf7\u6c42\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\uff08\u5982\u6b67\u4e49\u3001\u4e0d\u5b8c\u6574\uff09\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u6f84\u6e05\u6027\u95ee\u9898\uff08CQs\uff09\uff0c\u4ee5\u63d0\u5347\u8fd9\u4e9b\u8bf7\u6c42\u5bf9\u5f00\u53d1\u8005\u7684\u53c2\u8003\u4ef7\u503c\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5f00\u6e90\u8f6f\u4ef6\u7684\u7279\u6027\u8bf7\u6c42\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u4eba\u5de5\u6ce8\u91ca\u8fdb\u884c\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u8bbf\u8c08GitHub\u5f00\u53d1\u8005\uff0c\u6df1\u5165\u4e86\u89e3\u4ed6\u4eec\u5bf9\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\u7684\u8ba4\u77e5\u3001\u5bf9\u7b56\u53ca\u5176\u5bf9\u540e\u7eed\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u5728\u5b9e\u9645\u7684\u5f00\u6e90\u7279\u6027\u8bf7\u6c42\u4e2d\u81ea\u52a8\u68c0\u6d4b\u5230\u81ea\u7136\u8bed\u8a00\u7f3a\u9677\uff0c\u5e76\u6709\u6548\u751f\u6210\u6f84\u6e05\u95ee\u9898\u3002\u5bf9\u6bd4\u4eba\u5de5\u6807\u6ce8\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u663e\u793a\u4e86\u81ea\u52a8\u5316\u5de5\u5177\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002\u5f00\u53d1\u8005\u8bbf\u8c08\u63d0\u4f9b\u4e86\u5bf9\u7f3a\u9677\u5f71\u54cd\u4e0e\u5904\u7f6e\u7b56\u7565\u7684\u5b9e\u9645\u53cd\u9988\uff0c\u652f\u6301\u65b9\u6cd5\u4ef7\u503c\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u6539\u8fdb\u81ea\u7136\u8bed\u8a00\u7279\u6027\u8bf7\u6c42\u7f3a\u9677\u7684\u65b9\u6cd5\u5177\u6709\u8f83\u9ad8\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u6539\u5584\u5f00\u53d1\u6d41\u7a0b\uff0c\u63d0\u5347\u7279\u6027\u8bf7\u6c42\u7684\u5f00\u53d1\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2507.13382", "categories": ["cs.CL", "cs.LG", "05-05C12"], "pdf": "https://arxiv.org/pdf/2507.13382", "abs": "https://arxiv.org/abs/2507.13382", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eNLP\u4e0e\u56fe\u7b97\u6cd5\u7ed3\u5408\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u5f02\u5e38\u5047\u65b0\u95fb\u6a21\u5f0f\uff0c\u5b9e\u9a8c\u6db5\u76d6\u65b0\u51a0\u76f8\u5173\u771f\u5047\u65b0\u95fb\u9a8c\u8bc1\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5316\u65f6\u4ee3\u5047\u65b0\u95fb\u4f20\u64ad\u901f\u5ea6\u6781\u5feb\uff0c\u6210\u4e3a\u793e\u4f1a\u548c\u4fe1\u606f\u5b89\u5168\u7684\u91cd\u5927\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5047\u65b0\u95fb\u68c0\u6d4b\u96be\u9898\u3002", "method": "\u9996\u5148\u9009\u53d6Kaggle\u4e0a\u7684\u771f\u5047\u65b0\u95fb\u6570\u636e\u96c6\uff0c\u5e76\u52a0\u5165\u6700\u65b0\u7684\u4e0e\u65b0\u51a0\u75ab\u60c5\u76f8\u5173\u7684\u771f\u5047\u65b0\u95fb\uff0c\u4ee5\u4e30\u5bcc\u5b9e\u9a8c\u6570\u636e\u3002\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\u5c06\u65b0\u95fb\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u56fe\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u7684\u56fe\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff08GBAD\uff09\uff0c\u6316\u6398\u3001\u68c0\u6d4b\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u56fe\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u6570\u636e\u96c6\u4e2d\u7684\u89c4\u8303\uff08\u6b63\u5e38\uff09\u6a21\u5f0f\uff0c\u5e76\u53d1\u73b0\u504f\u79bb\u8fd9\u4e9b\u89c4\u8303\u7684\u5f02\u5e38\uff08\u5047\u65b0\u95fb\uff09\u6a21\u5f0f\u3002", "conclusion": "\u56fe\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u65b0\u95fb\u6570\u636e\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u53ef\u4ee5\u5e2e\u52a9\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5bdf\u89c9\u7684\u590d\u6742\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u5347\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.13661", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13661", "abs": "https://arxiv.org/abs/2507.13661", "authors": ["Changwen Li", "Joseph Sifakis", "Rongjie Yan", "Jian Zhang"], "title": "Testing Autonomous Driving Systems -- What Really Matters and What Doesn't", "comment": null, "summary": "Despite extensive research, the testing of autonomous driving systems (ADS)\nlandscape remains fragmented, and there is currently no basis for an informed\ntechnical assessment of the importance and contribution of the current state of\nthe art. This paper attempts to address this problem by exploring two\ncomplementary aspects.\n  First, it proposes a framework for comparing existing test methods in terms\nof their intrinsic effectiveness and validity. It shows that many methods do\nnot meet both of these requirements. Either because they are based on criteria\nthat do not allow for rapid, inexpensive, and comprehensive detection of\nfailures, or because the degree of validity of the properties tested cannot be\naccurately estimated. In particular, it is shown that most critical test\nmethods do not take into account the nominal operational capabilities of\nautopilots and generate scenarios that are impossible for the tested vehicles\nto handle, resulting in unjustified rejections.\n  Secondly, the paper shows that test effectiveness and validity are highly\ndependent on how autopilots are designed: how they choose between different\ncontrol policies to perform maneuvers, as well as on the reproducibility of the\nresults. In fact, most test methods take for granted two principles underlying\ntraditional methods, but do not generally apply to ADS. We maintain that the\nabsence of rationality and determinacy significantly impairs the effectiveness\nand validity of test methods, and provide test results on eight open\nautopilots, in which most do not satisfy these properties, thereby illustrating\nthis fact.\n  We conclude that under the current state of the art, it is impossible to\nobtain strong enough guarantees for essential autopilot properties and\nrecommend that autopilots be developed with a view to both rationality and\ndeterminacy.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u4e0d\u8db3\uff0c\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u591a\u6570\u6d4b\u8bd5\u65e0\u6cd5\u6709\u6548\u3001\u5408\u7406\u5730\u9a8c\u8bc1\u7cfb\u7edf\u80fd\u529b\uff0c\u5efa\u8bae\u672a\u6765\u5e94\u6ce8\u91cd\u7cfb\u7edf\u7406\u6027\u548c\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u8bc4\u6d4b\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u6d4b\u8bd5\u7814\u7a76\u5e7f\u6cdb\uff0c\u4f46\u5f53\u524d\u6d4b\u8bd5\u9886\u57df\u4f9d\u7136\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u5bf9\u73b0\u6709\u6280\u672f\u624b\u6bb5\u91cd\u8981\u6027\u548c\u8d21\u732e\u7684\u79d1\u5b66\u6027\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u65b9\u6cd5\u8bc4\u4ef7\u6846\u67b6\uff0c\u4ece\u6709\u6548\u6027\u548c\u6709\u6548\u9a8c\u8bc1\u4e24\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\u5df2\u6709\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5728\u516b\u4e2a\u5f00\u6e90\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u7ed3\u679c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u591a\u6570\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u672a\u80fd\u517c\u987e\u6709\u6548\u6027\u548c\u6709\u6548\u9a8c\u8bc1\uff0c\u5e38\u5e38\u57fa\u4e8e\u65e0\u6cd5\u9ad8\u6548\u3001\u5168\u9762\u53d1\u73b0\u5931\u6548\u7684\u6807\u51c6\uff0c\u6216\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6d4b\u8bd5\u5c5e\u6027\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u548c\u6709\u6548\u9a8c\u8bc1\u4e0e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81ea\u8eab\u7684\u8bbe\u8ba1\u65b9\u5f0f\u5bc6\u5207\u76f8\u5173\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u591a\u6570\u6d4b\u8bd5\u5bf9\u8c61\u672a\u6ee1\u8db3\u7406\u6027\u4e0e\u786e\u5b9a\u6027\u8981\u6c42\u3002", "conclusion": "\u5f53\u524d\u6280\u672f\u6c34\u5e73\u4e0b\uff0c\u5c1a\u65e0\u6cd5\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5173\u952e\u5c5e\u6027\u7ed9\u51fa\u5145\u5206\u4fdd\u8bc1\u3002\u5efa\u8bae\u672a\u6765\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u9700\u66f4\u52a0\u5173\u6ce8\u7406\u6027\u548c\u786e\u5b9a\u6027\u7279\u6027\u3002"}}
{"id": "2507.13390", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13390", "abs": "https://arxiv.org/abs/2507.13390", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "PARAM-1 BharatGen 2.9B Model", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9762\u5411\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u7684PARAM-1\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5173\u6ce8\u5370\u5730\u8bed\u548c\u82f1\u8bed\u3001\u5206\u8bcd\u516c\u5e73\u53ca\u672c\u5730\u5316\u8bc4\u6d4b\uff0c\u9a8c\u8bc1\u4e86\u524d\u671f\u5d4c\u5165\u591a\u6837\u6027\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u80fd\u4f5c\u4e3a\u5370\u5ea6\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u5f3a\u5927\u6a21\u578b\u6216\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u5bfc\u81f4\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\uff0c\u5c24\u5176\u662f\u50cf\u5370\u5ea6\u8fd9\u6837\u62e5\u6709\u591a\u8bed\u8a00\u3001\u591a\u65b9\u8a00\u3001\u4ee3\u7801\u5207\u6362\u4e0e\u53cc\u8bed\u5171\u5b58\u73b0\u8c61\u7684\u5730\u533a\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7ed3\u6784\u4e0a\u66f4\u5177\u5305\u5bb9\u6027\u7684\u6a21\u578b\uff0c\u4ee3\u8868\u5370\u5ea6\u7684\u8bed\u8a00\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faPARAM-1\u6a21\u578b\uff0c\u4e00\u4e2a\u4ec5\u67092.9B\u53c2\u6570\u7684decoder-only\u3001text-only\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u6570\u636e\u96c6\u4ec5\u5305\u542b\u5370\u5730\u8bed\u548c\u82f1\u8bed\uff0c\u7a81\u51fa\u9ad8\u8d28\u91cf\u3001\u4e8b\u5b9e\u4e30\u5bcc\u7684\u5185\u5bb9\u3002\u8bbe\u8ba1\u5305\u542b\u4e09\u5927\u539f\u5219\uff1a\u5370\u5730\u8bed\u670925%\u8bed\u6599\u5206\u914d\uff1b\u91c7\u7528\u9002\u5e94\u5370\u5ea6\u5f62\u6001\u7ed3\u6784\u7684SentencePiece\u5206\u8bcd\u5668\u5b9e\u73b0\u5206\u8bcd\u516c\u5e73\uff1b\u4ee5\u53ca\u7528\u672c\u5730\u5316\u8bc4\u6d4b\u57fa\u51c6\uff08IndicQA\u3001\u4ee3\u7801\u6df7\u5408\u63a8\u7406\u548c\u793e\u4f1a\u8bed\u8a00\u7a33\u5065\u6027\u4efb\u52a1\uff09\u505a\u6548\u679c\u8bc4\u4f30\u3002", "result": "PARAM-1\u6a21\u578b\u4ee5\u591a\u6837\u6027\u4e3a\u8bbe\u8ba1\u6838\u5fc3\uff0c\u5728\u524d\u8bad\u7ec3\u9636\u6bb5\u878d\u5165\u516c\u5e73\u548c\u5305\u5bb9\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u4e0d\u4ec5\u662f\u4e00\u4e2a\u6709\u7ade\u4e89\u529b\u7684\u901a\u7528\u6a21\u578b\uff0c\u540c\u65f6\u4e5f\u662f\u5370\u5ea6\u672c\u5730\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u7ebf\u3002", "conclusion": "\u5c06\u591a\u6837\u6027\u5728\u8bed\u8a00\u6a21\u578b\u524d\u8bad\u7ec3\u9636\u6bb5\u6df1\u5ea6\u5d4c\u5165\uff0c\u800c\u4e0d\u662f\u540e\u671f\u5fae\u8c03\u8865\u6551\uff0c\u662f\u5b9e\u73b0\u516c\u5e73\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u7684\u6709\u6548\u9014\u5f84\u3002PARAM-1\u4e3a\u5370\u5ea6\u591a\u8bed\u8a00\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6a21\u578b\u548c\u84dd\u672c\u3002"}}
{"id": "2507.13392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13392", "abs": "https://arxiv.org/abs/2507.13392", "authors": ["Emil H\u00e4glund", "Johanna Bj\u00f6rklund"], "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4ee5\u89c2\u70b9\u5355\u5143\u4e3a\u57fa\u7840\u7684\u60c5\u611f\u8bdd\u9898\u5efa\u6a21\u65b9\u6848\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u60c5\u611f\u5355\u5143\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bdd\u9898\u53ef\u89e3\u91ca\u6027\u548c\u4e0e\u5546\u4e1a\u6307\u6807\uff08\u5982\u8bc4\u5206\uff09\u7684\u5173\u8054\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u8bdd\u9898\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u8bc4\u8bba\u4e2d\u7684\u7ec6\u7c92\u5ea6\u89c2\u70b9\u53ca\u5176\u5bf9\u5e94\u60c5\u611f\uff0c\u8fd9\u9650\u5236\u4e86\u4ece\u5ba2\u6237\u8bc4\u8bba\u4e2d\u63d0\u70bc\u6709\u6548\u5546\u4e1a\u6d1e\u5bdf\u7684\u80fd\u529b\u3002\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8bdd\u9898\u5efa\u6a21\u7684\u89e3\u91ca\u6027\u548c\u4e0e\u5546\u4e1a\u6307\u6807\u7684\u5173\u8054\u6027\u3002", "method": "\u91cd\u6784\u4e86\u8bdd\u9898\u5efa\u6a21\u6d41\u7a0b\uff0c\u4ece\u5904\u7406\u6574\u4f53\u6587\u672c\u8f6c\u5411\u57fa\u4e8e\u201c\u89c2\u70b9\u5355\u5143\u201d\uff08\u5373\u5305\u542b\u76f8\u5173\u6587\u672c\u7247\u6bb5\u548c\u60c5\u611f\u5206\u6570\u7684\u72ec\u7acb\u9648\u8ff0\uff09\u8fdb\u884c\u5efa\u6a21\u3002\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u63d0\u53d6\u89c2\u70b9\u5355\u5143\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u5355\u5143\u8fdb\u884c\u8bdd\u9898\u5efa\u6a21\u548c\u60c5\u611f\u5206\u6790\uff1b\u518d\u5c06\u8bdd\u9898\u3001\u60c5\u611f\u4e0e\u4e1a\u52a1\u6307\u6807\uff08\u5982\u8bc4\u5206\uff09\u8fdb\u884c\u5173\u8054\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u66f4\u5177\u8fde\u8d2f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u8bdd\u9898\uff0c\u540c\u65f6\u51c6\u786e\u6355\u6349\u6bcf\u4e2a\u8bdd\u9898\u5bf9\u5e94\u7684\u60c5\u611f\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u8bdd\u9898\u548c\u60c5\u611f\u4e0e\u5982\u661f\u7ea7\u8bc4\u5206\u7b49\u4e1a\u52a1\u6307\u6807\u76f8\u5173\u8054\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5ba2\u6237\u5173\u6ce8\u70b9\u5982\u4f55\u5f71\u54cd\u4e1a\u52a1\u7ed3\u679c\u3002\u5b9e\u9a8c\u5c55\u793a\u4e86\u672c\u7cfb\u7edf\u5728\u8bdd\u9898\u8fde\u8d2f\u6027\u3001\u89e3\u91ca\u6027\u53ca\u661f\u7ea7\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9762\u5411\u89c2\u70b9\u5355\u5143\u7684\u8bdd\u9898\u5efa\u6a21\u80fd\u66f4\u597d\u5730\u63ed\u793a\u5ba2\u6237\u8bc4\u8bba\u7ed3\u6784\u53ca\u5176\u5546\u4e1a\u5f71\u54cd\uff0c\u76f8\u6bd4\u5176\u4ed6\u8bdd\u9898\u5efa\u6a21\u548c\u5206\u7c7b\u65b9\u6848\uff0c\u5728\u8fde\u8d2f\u6027\u3001\u89e3\u91ca\u6027\u548c\u5546\u4e1a\u6d1e\u5bdf\u65b9\u9762\u5177\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.13395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13395", "abs": "https://arxiv.org/abs/2507.13395", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u5229\u7528\u5355\u8bed\u8bed\u6599\u63d0\u5347NMT\u8bd1\u6587\u98ce\u683c\u4fdd\u771f\u7684Babel\u6846\u67b6\uff0c\u7ed3\u5408\u98ce\u683c\u68c0\u6d4b\u4e0e\u6269\u6563\u6a21\u578b\u4fee\u6b63\u673a\u5236\uff0c\u65e0\u9700\u66f4\u6539\u7cfb\u7edf\u7ed3\u6784\u5373\u80fd\u5927\u5e45\u6539\u5584\u591a\u9886\u57df\u98ce\u683c\u4fdd\u6301\u6548\u679c\u3002", "motivation": "\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u6781\u5927\u4fc3\u8fdb\u4e86\u8de8\u8bed\u8a00\u4ea4\u6d41\uff0c\u4f46\u5728\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u6587\u672c\u98ce\u683c\u5374\u662f\u4e00\u4e2a\u96be\u9898\u3002\u4ee5\u5f80\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5e73\u884c\u8bed\u6599\u6765\u8fdb\u884c\u98ce\u683c\u4fdd\u6301\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u5728\u6ca1\u6709\u5e73\u884c\u98ce\u683c\u8bed\u6599\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u98ce\u683c\u4fdd\u771f\u7684\u7ffb\u8bd1\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aBabel\u7684\u65b0\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5355\u8bed\u8bed\u6599\u63d0\u5347NMT\u7684\u98ce\u683c\u4fdd\u771f\u6027\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u5927\u7ec4\u4ef6\uff1a\uff081\uff09\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5d4c\u5165\u7684\u98ce\u683c\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u8bc6\u522b\u6e90\u6587\u672c\u548c\u8bd1\u6587\u4e4b\u95f4\u7684\u98ce\u683c\u5dee\u5f02\uff1b\uff082\uff09\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u5e94\u7528\u5668\uff0c\u7ea0\u6b63\u98ce\u683c\u4e0d\u4e00\u81f4\u4e14\u4e0d\u5f71\u54cd\u8bed\u4e49\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u540e\u5904\u7406\u6a21\u5757\u4e0e\u73b0\u6709NMT\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u66f4\u6539\u539f\u7cfb\u7edf\u67b6\u6784\u6216\u5f15\u5165\u5e73\u884c\u98ce\u683c\u6570\u636e\u3002", "result": "\u5728\u6cd5\u5f8b\u3001\u6587\u5b66\u3001\u79d1\u5b66\u5199\u4f5c\u3001\u533b\u5b66\u4ee5\u53ca\u6559\u80b2\u5185\u5bb9\u4e94\u4e2a\u9886\u57df\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cBabel\u6846\u67b6\u98ce\u683c\u4e0d\u4e00\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u8fbe\u523088.21%\uff0c\u5e76\u5c06\u98ce\u683c\u4fdd\u6301\u80fd\u529b\u63d0\u5347150%\uff0c\u8bed\u4e49\u76f8\u4f3c\u6027\u5f97\u5206\u9ad8\u8fbe0.92\u3002\u4eba\u5de5\u8bc4\u4ef7\u4e5f\u8bc1\u5b9e\u7ecfBabel\u5904\u7406\u540e\u7684\u8bd1\u6587\u80fd\u66f4\u597d\u4fdd\u7559\u539f\u6587\u98ce\u683c\uff0c\u6d41\u7545\u6027\u4e0e\u5145\u8db3\u6027\u4fdd\u6301\u826f\u597d\u3002", "conclusion": "Babel\u6846\u67b6\u5728\u4e0d\u9700\u8981\u5e73\u884c\u98ce\u683c\u8bed\u6599\u7684\u6761\u4ef6\u4e0b\uff0c\u663e\u8457\u589e\u5f3a\u4e86NMT\u7cfb\u7edf\u7684\u98ce\u683c\u4e00\u81f4\u6027\u4e14\u4e0d\u5f71\u54cd\u8bed\u4e49\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\uff0c\u5177\u6709\u8f83\u9ad8\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.13410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13410", "abs": "https://arxiv.org/abs/2507.13410", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5fae\u8c03\u53ef\u5728\u65e0\u63d0\u793a\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u9ad8\u6548\u63a7\u5236\u5927\u6a21\u578b\u7684\u751f\u6210\u8bed\u79cd\uff0c\u51c6\u786e\u7387\u9ad8\u4e14\u8bed\u4e49\u4e0d\u53d8\uff0c\u65b9\u6cd5\u8f7b\u91cf\u4e14\u53ef\u89e3\u91ca\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\uff08\u65e0\u989d\u5916\u63d0\u793a\u6216\u5fae\u8c03\uff09\u4e0b\u4ecd\u96be\u4ee5\u786e\u5b9a\u5730\u63a7\u5236\u751f\u6210\u8bed\u79cd\u3002\u867d\u7136\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7279\u5f81\u4e0e\u53ef\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u6709\u5173\uff0c\u8be5\u7279\u5f81\u5c1a\u672a\u88ab\u5927\u89c4\u6a21\u5e94\u7528\u4e8e\u8bed\u8a00\u751f\u6210\u63a7\u5236\u3002", "method": "\u4f5c\u8005\u5728Gemma-2B\u548cGemma-9B\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u4e0a\uff0c\u7528\u9884\u8bad\u7ec3\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u627e\u5230\u82f1\u4e0e\u4e2d\u3001\u65e5\u3001\u897f\u3001\u6cd5\u56db\u79cd\u8bed\u79cd\u4e4b\u95f4\u6fc0\u6d3b\u5dee\u5f02\u6700\u5927\u7684\u7279\u5f81\uff0c\u5728\u63a8\u7406\u65f6\u53ea\u4fee\u6539\u5355\u4e00SAE\u7279\u5f81\u4ee5\u5b9e\u73b0\u8bed\u8a00\u751f\u6210\u7684\u5f15\u5bfc\u3002", "result": "\u53ea\u9700\u5728\u67d0\u4e2atransformer\u5c42\u4fee\u6539\u4e00\u4e2aSAE\u7279\u5f81\uff0cFastText\u8bed\u8a00\u5206\u7c7b\u51c6\u786e\u5ea6\u80fd\u8fbe\u523090%\uff0c\u540c\u65f6LaBSE\u8bed\u4e49\u76f8\u4f3c\u5ea6\u663e\u793a\u8bed\u4e49\u4fdd\u6301\u826f\u597d\u3002\u5206\u6790\u8fd8\u53d1\u73b0\u8fd9\u79cd\u65b9\u6cd5\u5728transformer\u4e2d\u540e\u5c42\u6700\u6709\u6548\uff0c\u4e14\u90e8\u5206\u6ce8\u610f\u5934\u4e0e\u8bed\u8a00\u654f\u611f\u7279\u5f81\u5173\u8054\u5f3a\u3002", "conclusion": "\u7a00\u758f\u7279\u5f81\u5f15\u5bfc\u4e3a\u53ef\u89e3\u91ca\u4e14\u8f7b\u91cf\u7684\u53ef\u63a7\u591a\u8bed\u8a00\u751f\u6210\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.13411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13411", "abs": "https://arxiv.org/abs/2507.13411", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5c06\u77e5\u8bc6\u56fe\u8c31\u6ce8\u5165\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u5d4c\u5165\u7a7a\u95f4\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0e\u5b9e\u9645\u6848\u4f8b\u4e2d\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\u3001Gemini\u3001Claude\uff09\u5728NLP\u4efb\u52a1\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5e7b\u89c9\u95ee\u9898\uff08\u5373\u4ea7\u51fa\u4e0d\u51c6\u786e\u6216\u65e0\u4f9d\u636e\u7684\u4fe1\u606f\uff09\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8bb8\u591a\u65b9\u6cd5\u5c1d\u8bd5\u5c06\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u5e93\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faALIGNed-LLM\u65b9\u6cd5\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u7684\u5d4c\u5165\u5411\u91cf\uff08\u5982TransE\u6a21\u578b\u8bad\u7ec3\u7684\u5d4c\u5165\uff09\u901a\u8fc7\u4e00\u4e2a\u53ef\u8bad\u7ec3\u6295\u5f71\u5c42\uff0c\u6ce8\u5165\u5e76\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\u3002\u8fd9\u4e00\u8fc7\u7a0b\u501f\u9274\u4e86LLaVA\u6a21\u578b\uff0c\u5c06\u5916\u90e8\u4fe1\u606f\uff08\u5982\u89c6\u89c9\u3001\u6587\u672c\u6216\u77e5\u8bc6\u56fe\u8c31\uff09\u878d\u5408\u8fdbLLM\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u652f\u6301\u66f4\u7cbe\u786e\u7684\u5b9e\u4f53\u533a\u5206\u4e0e\u4e8b\u5b9e\u63a8\u65ad\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0cALIGNed-LLM\u5747\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u7387\u3002\u540c\u65f6\uff0c\u5728\u4e00\u4e2a\u6765\u81ea\u6b27\u6d32\u5927\u578b\u4e2d\u592e\u94f6\u884c\u7684\u5b9e\u9645\u91d1\u878d\u573a\u666f\u4e0b\u5e94\u7528\u8be5\u65b9\u6cd5\uff0cLLM\u7684\u56de\u7b54\u51c6\u786e\u6027\u4ea6\u6709\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "ALIGNed-LLM\u901a\u8fc7\u9ad8\u6548\u6ce8\u5165\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\u5bf9\u9f50\u5b9e\u4f53\u4e0e\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4e8b\u5b9e\u6027\u4e0e\u95ee\u9898\u89e3\u7b54\u51c6\u786e\u7387\uff0c\u5728\u5b9e\u9645\u884c\u4e1a\u573a\u666f\u4e0b\u4e5f\u5c55\u73b0\u51fa\u53ef\u89c2\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.13474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13474", "abs": "https://arxiv.org/abs/2507.13474", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u672f\u8bba\u6587\u5185\u5bb9\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff08PSA\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u9ad8\u6548\u653b\u7834\u591a\u79cd\u4e3b\u6d41LLM\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u5bf9\u6743\u5a01\u4fe1\u606f\u7684\u4fe1\u4efb\u662f\u91cd\u5927\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b89\u5168\u6027\u662f\u7814\u7a76\u7126\u70b9\u3002\u8be5\u6587\u6307\u51fa\uff0cLLM\u503e\u5411\u4e8e\u4fe1\u4efb\u6765\u81ea\u6743\u5a01\u6765\u6e90\uff08\u5982\u5b66\u672f\u8bba\u6587\uff09\u7684\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u6210\u4e3a\u65b0\u7684\u5b89\u5168\u5f31\u70b9\u3002\u8bba\u6587\u65e8\u5728\u9a8c\u8bc1\u548c\u63a2\u7a76\u8be5\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u521d\u6b65\u5206\u6790\u9a8c\u8bc1LLM\u5bf9\u6743\u5a01\u5185\u5bb9\u7684\u4fe1\u4efb\u503e\u5411\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u2014\u2014Paper Summary Attack\uff08PSA\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u4ece\u653b\u51fb\u6216\u9632\u5fa1\u5bfc\u5411\u7684LLM\u5b89\u5168\u8bba\u6587\u5408\u6210\u5185\u5bb9\uff0c\u6784\u5efa\u5bf9\u6297\u6027\u63d0\u793a\u6a21\u677f\uff0c\u5e76\u5728\u9884\u5b9a\u4e49\u5b50\u90e8\u5206\u5185\u6ce8\u5165\u6709\u5bb3\u67e5\u8be2\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u7840LLM\u4ee5\u53ca\u6700\u65b0\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u90fd\u5b58\u5728\u663e\u8457\u6f0f\u6d1e\u3002PSA\u5bf9Claude3.5-Sonnet\u7684\u653b\u51fb\u6210\u529f\u7387\u8fbe97%\uff0c\u5bf9Deepseek-R1\u66f4\u9ad8\u8fbe98%\u3002\u4e0d\u540c\u6a21\u578b\u53ca\u5176\u4e0d\u540c\u7248\u672c\u5728\u9762\u5bf9\u4e0d\u540c\u7c7b\u578b\u8bba\u6587\u5185\u5bb9\u65f6\uff0c\u5c55\u73b0\u51fa\u5b8c\u5168\u76f8\u53cd\u7684\u8106\u5f31\u503e\u5411\u3002", "conclusion": "LLM\u5bf9\u6743\u5a01\u8bba\u6587\u5185\u5bb9\u7684\u4fe1\u4efb\u5e26\u6765\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0cPSA\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u7a81\u7834\u5f53\u4e0b\u4e3b\u6d41\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u3002\u4e0d\u540c\u6a21\u578b\u7684\u8106\u5f31\u6027\u5dee\u5f02\u4e3a\u5bf9\u6297\u65b9\u6cd5\u4e0e\u5b89\u5168\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u7ebf\u7d22\u3002"}}
{"id": "2507.13490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13490", "abs": "https://arxiv.org/abs/2507.13490", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.", "AI": {"tldr": "\u5927\u6a21\u578b\u4ef7\u503c\u89c2\u63a2\u6d4b\u65b9\u6cd5\u5bf9\u8f93\u5165\u6270\u52a8\u5f88\u654f\u611f\uff0c\u4e0e\u6a21\u578b\u5b9e\u9645\u884c\u4e3a\u76f8\u5173\u6027\u5f31\uff0c\u4eba\u53e3\u7edf\u8ba1\u80cc\u666f\u5f71\u54cd\u4e5f\u5f88\u6709\u9650\uff0c\u9700\u8b66\u60d5\u73b0\u6709\u63a2\u6d4b\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ef7\u503c\u53d6\u5411\u4f1a\u5f71\u54cd\u4e0d\u540c\u4eba\u7fa4\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u73b0\u6709\u7684\u4ef7\u503c\u63a2\u6d4b\u65b9\u6cd5\u5b58\u5728\u6613\u88ab\u8f93\u5165\u6270\u52a8\u5f71\u54cd\u3001\u65b9\u6cd5\u5bf9\u6bd4\u4e0d\u8db3\u4ee5\u53ca\u5bf9\u6a21\u578b\u5b9e\u9645\u503e\u5411\u7684\u5173\u8054\u6027\u4e0d\u660e\u7b49\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u7cfb\u7edf\u6027\u5730\u5206\u6790\u8fd9\u4e9b\u63a2\u6d4b\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3001\u8868\u73b0\u529b\u53ca\u5176\u5c40\u9650\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6bd4\u8f83\u4e09\u79cd\u5e38\u7528\u7684\u5927\u6a21\u578b\u4ef7\u503c\u89c2\u6d4b\u63a2\u6d4b\u65b9\u6cd5\uff0c\u5206\u522b\u5206\u6790\u5b83\u4eec\u5728\u4e0d\u540c\u8f93\u5165\u6270\u52a8\uff08\u5982prompt\u548c\u9009\u9879\u53d8\u5316\uff09\u4e0b\u7684\u8868\u73b0\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u65b0\u4efb\u52a1\uff0c\u8003\u5bdf\u6a21\u578b\u5bf9\u4eba\u53e3\u7edf\u8ba1\u4e0a\u4e0b\u6587\uff08demographic context\uff09\u7684\u54cd\u5e94\u6027\uff0c\u4ee5\u53ca\u6a21\u578b\u5185\u5728\u4ef7\u503c\u89c2\u4e0e\u5b9e\u9645\u884c\u4e3a\u503e\u5411\uff08\u5982\u4ef7\u503c\u76f8\u5173\u51b3\u7b56\u884c\u52a8\uff09\u7684\u4e00\u81f4\u6027\u3002", "result": "\u6240\u6709\u63a2\u6d4b\u65b9\u6cd5\u5728\u8f93\u5165\u8f7b\u5fae\u6270\u52a8\u4e0b\u8868\u73b0\u51fa\u8f83\u5927\u6ce2\u52a8\uff0c\u7a33\u5065\u6027\u4e0d\u8db3\u3002\u52a0\u5165\u4eba\u53e3\u7edf\u8ba1\u80cc\u666f\u5bf9\u81ea\u7531\u6587\u672c\u751f\u6210\u5f71\u54cd\u5f88\u5c0f\uff0c\u4e14\u6a21\u578b\u63a2\u6d4b\u51fa\u7684\u4ef7\u503c\u89c2\u4e0e\u5176\u5b9e\u9645\u5728\u4ef7\u503c\u76f8\u5173\u573a\u666f\u4e2d\u7684\u503e\u5411\u6027\u4ec5\u6709\u5f31\u76f8\u5173\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u4ef7\u503c\u89c2\u63a2\u6d4b\u65b9\u6cd5\u5728\u7a33\u5065\u6027\u548c\u5b9e\u9645\u6548\u5ea6\u4e0a\u5b58\u5728\u8f83\u5927\u5c40\u9650\uff0c\u672a\u6765\u9700\u8981\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u4e0e\u5bf9\u5176\u5c40\u9650\u7684\u5145\u5206\u8ba4\u8bc6\u3002"}}
{"id": "2507.13501", "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "pdf": "https://arxiv.org/pdf/2507.13501", "abs": "https://arxiv.org/abs/2507.13501", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "title": "Encoding syntactic objects and Merge operations in function spaces", "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u7684\u51fd\u6570\u7a7a\u95f4\u548c\u4ee3\u6570\u7ed3\u6784\uff0c\u672c\u6587\u5b9e\u73b0\u4e86\u5c06\u4efb\u610f\u53e5\u6cd5\u5bf9\u8c61\u548cMerge\u64cd\u4f5c\u4ee5\u6570\u5b66\u65b9\u5f0f\u5efa\u6a21\uff0c\u4e3a\u53e5\u6cd5\u7ed3\u6784\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63ed\u793a\u4e86Merge\u4e0e\u7b97\u672f\u7ed3\u6784\u7684\u5185\u5728\u8054\u7cfb\u3002", "motivation": "\u4e4b\u524d\u5173\u4e8e\u53e5\u6cd5\u7ed3\u6784\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u7684\u7406\u8bba\u6784\u67b6\u4ecd\u4e0d\u5b8c\u5584\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u6570\u5b66\u548c\u51fd\u6570\u7a7a\u95f4\u7684\u4e25\u683c\u8bc1\u660e\u65b9\u6cd5\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u6570\u5b66\u65b9\u6cd5\u8bf4\u660e\uff0c\u5c06\u8bcd\u6c47\u9879\u76ee\u7b49\u8868\u5f81\u4e3a\u51fd\u6570\uff08\u5982\u5c0f\u6ce2\uff09\u65f6\uff0c\u53ef\u4ee5\u5728\u76f8\u540c\u51fd\u6570\u7a7a\u95f4\u5185\u5fe0\u5b9e\u8868\u5f81\u4efb\u610f\u53e5\u6cd5\u5bf9\u8c61\uff0c\u4ece\u800c\u4e3a\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u53e5\u6cd5\u7ed3\u6784\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u6587\u7ae0\u5229\u7528\u51fd\u6570\u7a7a\u95f4\u548c\u5c0f\u6ce2\u65b9\u6cd5\uff0c\u5c06\u8bcd\u6c47\u548c\u53e5\u6cd5\u5bf9\u8c61\u6620\u5c04\u4e3a\u7279\u5b9a\u51fd\u6570\u3002\u4e3a\u8be5\u51fd\u6570\u7a7a\u95f4\u5f15\u5165\u53ef\u4ea4\u6362\u975e\u7ed3\u5408\u534a\u73af\u7ed3\u6784\uff08\u501f\u52a9Renyi\u71b5\uff09\u3001\u4ee3\u6570\u7ed3\u6784\u548coperad\u7684\u4ee3\u6570\u8fd0\u7b97\uff0c\u4ee5\u53ca\u4e0eHopf\u4ee3\u6570\u4e0e\u9a6c\u5c14\u79d1\u592b\u94fe\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u53e5\u6cd5\u5408\u5e76\u64cd\u4f5c\uff08Merge\uff09\u7684\u6570\u5b66\u63cf\u8ff0\u548c\u5b9e\u73b0\u3002\u8fd8\u5206\u6790\u4e86\u5728\u6b63\u5f26\u6ce2\u4e0a\u7684\u8de8\u9891\u76f8\u4f4d\u540c\u6b65\uff0c\u4f5c\u4e3aMerge\u5b9e\u73b0\u7684\u5177\u4f53\u6848\u4f8b\u3002", "result": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e25\u5bc6\u7684\u6570\u5b66\u6846\u67b6\uff0c\u4f7f\u5f97\u53e5\u6cd5\u5bf9\u8c61\u548c\u5176\u5408\u5e76\u64cd\u4f5c\uff08Merge\uff09\u80fd\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u83b7\u5f97\u5fe0\u5b9e\u7684\u4ee3\u6570\u548c\u8fd0\u7b97\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u53e5\u6cd5\u7ed3\u6784\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\u7684\u7406\u8bba\u53ef\u884c\u6027\u3002\u5177\u4f53\u6848\u4f8b\u4e2d\uff0cMerge\u53ef\u901a\u8fc7\u534a\u73af\u7684\u540e\u7ee7\u51fd\u6570\u4e0e\u8de8\u9891\u76f8\u4f4d\u540c\u6b65\u5efa\u6a21\uff0c\u9610\u660e\u4e86\u5176\u4e0e\u7b97\u672f\u540e\u7ee7\u51fd\u6570\u7684\u76f8\u4f3c\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u6838\u5fc3\u53e5\u6cd5\u7ed3\u6784\u7684\u795e\u7ecf\u8ba1\u7b97\u5b9e\u73b0\uff0c\u63d0\u4f9b\u4e86\u4e25\u5bc6\u7684\u6784\u9020\u6027\u6570\u5b66\u8bba\u8bc1\uff0c\u8bc1\u660e\u4e86\u7406\u8bba\u4e0a\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u7279\u5b9a\u6848\u4f8b\u5c55\u793a\u4e86Merge\u64cd\u4f5c\u7684\u5177\u4f53\u51fd\u6570\u5b9e\u73b0\u53ca\u5176\u7b97\u672f\u6027\u8d28\u6839\u6e90\u3002"}}
{"id": "2507.13544", "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "pdf": "https://arxiv.org/pdf/2507.13544", "abs": "https://arxiv.org/abs/2507.13544", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u201cFilter & Reconnect\u201d\u65b0\u65b9\u6cd5\u7684\u5bf9\u8bdd\u56fe\u5efa\u6a21\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u63d0\u5347\u548c\u6e05\u6670\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u5bf9\u5927\u89c4\u6a21\u5bf9\u8bdd\u5206\u6790\u573a\u666f\u5177\u6709\u76f4\u63a5\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u5728\u5404\u7c7b\u7528\u6237\u573a\u666f\u4e2d\u7684\u5e94\u7528\u666e\u53ca\uff0c\u5982\u4f55\u6709\u6548\u5206\u6790\u5bf9\u8bdd\u52a8\u6001\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002\u76ee\u524d\u9488\u5bf9\u677e\u6563\u7ed3\u6784\u5bf9\u8bdd\u7684\u5efa\u6a21\u548c\u7b80\u5316\u5c1a\u4e0d\u5b8c\u5584\uff0c\u5b58\u5728\u566a\u58f0\u9ad8\u3001\u7ed3\u6784\u4e0d\u6e05\u6670\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u751f\u6210\u80fd\u53cd\u6620\u201c\u7c7b\u6a21\u5f0f\u201d\u677e\u6563\u5bf9\u8bdd\u7ed3\u6784\u7684\u5bf9\u8bdd\u56fe\uff0c\u5e76\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86\u201cFilter & Reconnect\u201d\u56fe\u7b80\u5316\u65b9\u6cd5\uff0c\u81f4\u529b\u4e8e\u5728\u53bb\u9664\u566a\u58f0\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u548c\u7ed3\u6784\u5b8c\u6574\u3002", "result": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u51fa\u7684\u56fe\u7b80\u5316\u6280\u672f\u540e\uff0c\u8bed\u4e49\u5ea6\u91cf\u6307\u6807S\u63d0\u5347\u4e862.06\u500d\uff0c\u540c\u65f6\u5bf9\u8bdd\u56fe\u5448\u73b00 \u03b4-\u8d85\u66f2\u7387\u7684\u7c7b\u6811\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6e05\u6670\u5ea6\u7684\u5bf9\u8bdd\u5efa\u6a21\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u5957\u6709\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6\u8fdb\u884c\u7ed3\u6784\u5efa\u6a21\u548c\u8bed\u4e49\u63d0\u70bc\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u7cfb\u7edf\u76d1\u63a7\u3001\u5bf9\u8bdd\u7ba1\u7406\u548c\u7528\u6237\u884c\u4e3a\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.13551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13551", "abs": "https://arxiv.org/abs/2507.13551", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u5c06\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u63d0\u53d6\u7684\u505c\u987f\u7279\u5f81\u4e0e\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\u7ed3\u5408\uff0c\u80fd\u66f4\u6709\u6548\u5730\u91cf\u5316\u7cbe\u795e\u5206\u88c2\u75c7\u7684\u601d\u7ef4\u969c\u788d\uff0c\u4e3a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u7cbe\u795e\u75c5\u5b66\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u7cbe\u795e\u5206\u88c2\u75c7\u7b49\u7cbe\u795e\u75be\u75c5\u4e2d\uff0c\u6b63\u5f0f\u601d\u7ef4\u969c\u788d\uff08FTD\uff09\u8868\u73b0\u4e3a\u60a3\u8005\u8a00\u8bed\u7d0a\u4e71\uff0c\u4f20\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u8017\u65f6\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002\u81ea\u52a8\u5316\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6280\u672f\u53ef\u4ee5\u5ba2\u89c2\u91cf\u5316\u8bed\u97f3\u7279\u5f81\uff0c\u4f46\u5982\u4f55\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u7279\u5f81\u4ee5\u8bc4\u4f30FTD\u4e25\u91cd\u7a0b\u5ea6\u8fd8\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u8be5\u7814\u7a76\u6574\u5408\u4e86\u57fa\u4e8eASR\u7684\u505c\u987f\u7279\u5f81\u4e0e\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\uff0c\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff08\u81ea\u5f55\u65e5\u8bb0\u3001\u56fe\u7247\u63cf\u8ff0\u3001\u68a6\u5883\u53d9\u8ff0\uff09\u4e0a\uff0c\u901a\u8fc7\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u6a21\u578b\u9884\u6d4bFTD\u7684\u4e34\u5e8a\u8bc4\u5206\u3002\u5206\u522b\u6bd4\u8f83\u4e86\u505c\u987f\u7279\u5f81\u3001\u8bed\u4e49\u7279\u5f81\u4ee5\u53ca\u4e8c\u8005\u7ed3\u5408\u7684\u9884\u6d4b\u6548\u679c\u3002", "result": "\u505c\u987f\u7279\u5f81\u5355\u72ec\u5c31\u5177\u6709\u7a33\u5065\u7684FTD\u4e25\u91cd\u5ea6\u9884\u6d4b\u529b\u3002\u5c06\u505c\u987f\u7279\u5f81\u4e0e\u8bed\u4e49\u8fde\u8d2f\u6027\u7279\u5f81\u7ed3\u5408\u540e\uff0c\u9884\u6d4b\u6548\u679c\u9ad8\u4e8e\u4ec5\u7528\u8bed\u4e49\u7279\u5f81\uff0c\u90e8\u5206\u6a21\u578b\u6700\u9ad8\u76f8\u5173\u6027\u8fbe\u52300.649\uff0c\u91cd\u75c7\u68c0\u6d4bAUC\u8fbe83.71%\u3002\u4e0d\u540c\u8bed\u6599\u4e0b\u505c\u987f\u6a21\u5f0f\u8868\u73b0\u5404\u5f02\uff0c\u4f46\u4e24\u7c7b\u7279\u5f81\u6574\u5408\u7684\u6027\u80fd\u63d0\u5347\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\u5747\u6210\u7acb\u3002", "conclusion": "\u7ed3\u5408\u8bed\u97f3\u7684\u65f6\u5e8f\u7279\u5f81\u548c\u8bed\u4e49\u7279\u5f81\u7684\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u7cbe\u51c6\u9ad8\u6548\u5730\u8bc4\u4f30\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u7684\u601d\u7ef4\u969c\u788d\uff0c\u4e3a\u7cbe\u795e\u75be\u75c5\u9886\u57df\u7684\u81ea\u52a8\u5316\u8bed\u97f3\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13563", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13563", "abs": "https://arxiv.org/abs/2507.13563", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5927\u578b\u4fc4\u8bed\u8bed\u97f3\u6570\u636e\u96c6Balalaika\uff0c\u914d\u6709\u8be6\u7ec6\u6ce8\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u7684\u6548\u679c\u3002", "motivation": "\u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u9762\u4e34\u4e00\u7cfb\u5217\u72ec\u7279\u96be\u9898\uff0c\u5982\u5143\u97f3\u5f31\u5316\u3001\u8f85\u97f3\u6e05\u97f3\u5316\u3001\u91cd\u97f3\u591a\u53d8\u3001\u540c\u5f62\u5f02\u4e49\u8bcd\u6b67\u4e49\u4ee5\u53ca\u8bed\u8c03\u4e0d\u81ea\u7136\u7b49\u3002\u73b0\u6709\u6570\u636e\u96c6\u5728\u8bed\u97f3\u5408\u6210\u548c\u589e\u5f3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fc4\u8bed\u5927\u89c4\u6a21\u8bed\u97f3\u6570\u636e\u96c6Balalaika\uff0c\u5305\u542b\u8d85\u8fc72,000\u5c0f\u65f6\u7684\u5f55\u97f3\uff0c\u5e76\u914d\u6709\u8be6\u5c3d\u6587\u672c\u6807\u6ce8\uff08\u5305\u542b\u6807\u70b9\u548c\u91cd\u97f3\u6807\u8bb0\uff09\u3002\u540c\u65f6\u4ecb\u7ecd\u4e86\u6570\u636e\u96c6\u7684\u5efa\u6784\u6d41\u7a0b\u3001\u6ce8\u91ca\u65b9\u6cd5\u53ca\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "result": "\u4f7f\u7528Balalaika\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8bed\u97f3\u5408\u6210\u548c\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "Balalaika\u6570\u636e\u96c6\u901a\u8fc7\u9ad8\u8d28\u91cf\u548c\u5168\u65b9\u4f4d\u7684\u6807\u6ce8\u63d0\u5347\u4e86\u4fc4\u8bed\u8bed\u97f3\u5408\u6210\u7684\u8868\u73b0\uff0c\u53ef\u4e3a\u540e\u7eed\u8bed\u97f3\u6280\u672f\u548c\u7814\u7a76\u63d0\u4f9b\u575a\u5b9e\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2507.13614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13614", "abs": "https://arxiv.org/abs/2507.13614", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u4eba\u7c7b\u548c\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u5728\u591a\u5c42\u6b21\u8bed\u8a00\u5b66\u7279\u5f81\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba\u7c7b\u6587\u672c\u66f4\u5177\u591a\u6837\u6027\u548c\u53e5\u6cd5\u7b80\u6d01\u6027\uff0c\u673a\u5668\u6587\u672c\u5219\u8d8b\u4e8e\u540c\u8d28\u5316\uff0c\u5bf9\u6587\u672c\u5224\u522b\u548c\u6a21\u578b\u6539\u8fdb\u5177\u6709\u542f\u793a\u610f\u4e49\u3002", "motivation": "\u8fd1\u5e74\u6765\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9886\u57df\u83b7\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f7f\u5176\u751f\u6210\u7684\u6587\u672c\u4e0e\u4eba\u7c7b\u7f16\u5199\u7684\u6587\u672c\u6108\u53d1\u96be\u4ee5\u533a\u5206\u3002\u5f53\u524d\u5df2\u6709\u8bb8\u591a\u7814\u7a76\u5173\u6ce8\u4e8e\u8bc6\u522b\u548c\u5206\u7c7b\u4eba\u7c7b\u6587\u672c\u4e0e\u673a\u5668\u6587\u672c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6587\u672c\u5728\u591a\u5c42\u9762\u8bed\u8a00\u7279\u5f81\u4e0a\u7684\u7cfb\u7edf\u523b\u753b\u3002", "method": "\u4f5c\u8005\u91c7\u96c6\u4e86\u6db5\u76d68\u4e2a\u9886\u57df\u300111\u79cd\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u53ca\u4eba\u7c7b\u6587\u672c\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4ece\u5f62\u6001\u5b66\u3001\u53e5\u6cd5\u3001\u8bed\u4e49\u7b49\u4e0d\u540c\u8bed\u8a00\u5c42\u9762\u63d0\u53d6\u7279\u5f81\uff08\u5982\u4f9d\u5b58\u957f\u5ea6\u3001\u60c5\u611f\u6027\u7b49\uff09\uff0c\u5206\u6790\u4eba\u7c7b\u4e0e\u673a\u5668\u6587\u672c\u7684\u4e0d\u540c\u3002\u7814\u7a76\u8fd8\u8003\u5bdf\u4e86\u91c7\u6837\u7b56\u7565\u3001\u91cd\u590d\u63a7\u5236\u53ca\u6a21\u578b\u53d1\u5e03\u65e5\u671f\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u98ce\u683c\u5d4c\u5165\u5bf9\u6bd4\u53d8\u5f02\u6027\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u663e\u793a\uff0c\u4eba\u7c7b\u6587\u672c\u901a\u5e38\u6709\u66f4\u7b80\u5355\u7684\u53e5\u6cd5\u7ed3\u6784\u548c\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u65e0\u8bba\u4eba\u7c7b\u8fd8\u662f\u673a\u5668\u6587\u672c\uff0c\u4e0d\u540c\u9886\u57df\u95f4\u5747\u5c55\u73b0\u4e86\u98ce\u683c\u591a\u6837\u6027\uff0c\u4f46\u4eba\u7c7b\u6587\u672c\u5728\u63d0\u53d6\u7279\u5f81\u4e0a\u7684\u53d8\u5f02\u6027\u66f4\u5927\u3002\u6700\u65b0\u5927\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5728\u53d8\u5f02\u6027\u4e0a\u5df2\u4e0e\u524d\u4e00\u4ee3\u6a21\u578b\u8d8b\u540c\uff0c\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u6587\u672c\u540c\u8d28\u5316\u3002", "conclusion": "\u4e0e\u4eba\u7c7b\u6587\u672c\u76f8\u6bd4\uff0c\u673a\u5668\u6587\u672c\u5728\u53e5\u6cd5\u7b80\u5316\u548c\u8bed\u4e49\u591a\u6837\u6027\u65b9\u9762\u4ecd\u6709\u5dee\u8ddd\u3002\u968f\u7740\u5927\u6a21\u578b\u8fed\u4ee3\uff0c\u6587\u672c\u98ce\u683c\u9010\u6b65\u540c\u8d28\u5316\uff0c\u4e14\u96be\u4ee5\u5c55\u73b0\u9886\u57df\u5185\u7684\u4eba\u7c7b\u98ce\u683c\u53d8\u5f02\u6027\u3002\u8be5\u6210\u679c\u4e3a\u7406\u89e3\u548c\u5206\u7c7b\u673a\u5668\u751f\u6210\u6587\u672c\u63d0\u4f9b\u4e86\u8bed\u8a00\u5b66\u89c6\u89d2\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.13618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13618", "abs": "https://arxiv.org/abs/2507.13618", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "AI": {"tldr": "Seed-X\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1LLM\uff0c\u5728\u6027\u80fd\u3001\u6a21\u578b\u5f00\u653e\u6027\u53ca\u4f18\u5316\u65b9\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u51fa\u6210\u679c\uff0c\u6709\u671b\u63a8\u52a8\u591a\u8bed\u8a00\u7ffb\u8bd1\u6280\u672f\u8fdb\u6b65\u3002", "motivation": "\u591a\u8bed\u8a00\u7ffb\u8bd1\u5bf9\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u5176\u9762\u4e34\u590d\u6742\u7684\u8bed\u8a00\u7ed3\u6784\u548c\u751f\u786c\u7684\u81ea\u52a8\u7ffb\u8bd1\u7ed3\u679c\u3002", "method": "\u672c\u6587\u63d0\u51faSeed-X\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u542binstruct\u548creasoning\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4e3a7B\u3002\u57fa\u5ea7\u6a21\u578b\u5728\u5305\u542b28\u79cd\u8bed\u8a00\u7684\u591a\u6837\u9ad8\u8d28\u91cf\u5355\u8bed\u53ca\u53cc\u8bed\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u4e4b\u540e\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\u5fae\u8c03instruct\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "result": "Seed-X\u572828\u79cd\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u53ef\u4e0e\u95ed\u6e90\u9876\u5c16\u6a21\u578b\uff08\u5982Gemini-2.5\u548cGPT-4o\uff09\u5ab2\u7f8e\uff0c\u5e76\u4e14\u5728\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "Seed-X\u8bc1\u660e\u5728\u4ec5\u75287B\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7684\u6570\u636e\u548c\u8bad\u7ec3\u8bbe\u8ba1\uff0c\u80fd\u591f\u5b9e\u73b0\u5353\u8d8a\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1\u3002\u4f5c\u8005\u516c\u5f00\u6a21\u578b\u53c2\u6570\u548c\u4f18\u5316\u5b9e\u8df5\u65b9\u6cd5\u4ee5\u4fc3\u8fdb\u7ffb\u8bd1\u7814\u7a76\u4e0e\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2507.13655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13655", "abs": "https://arxiv.org/abs/2507.13655", "authors": ["Teerapong Panboonyuen"], "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCU-ICU\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u7a00\u758f\u5fae\u8c03T5\u5927\u6a21\u578b\uff0c\u5b9e\u73b0ICU\u4efb\u52a1\u7684\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u9884\u6d4b\uff0c\u5e76\u4e14\u53ea\u9700\u6781\u5c11\u53c2\u6570\u66f4\u65b0\uff0c\u5c55\u73b0\u4e86\u5f3a\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u6548\u5e94\u7528\u4e8e\u50cf\u533b\u7597\u5065\u5eb7\u8fd9\u6837\u7684\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u7279\u522b\u662f\u9886\u57df\u9002\u5e94\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCU-ICU\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7T5\u67b6\u6784\uff0c\u7ed3\u5408few-shot\u63d0\u793a\u548c\u9009\u62e9\u6027\u53c2\u6570\u66f4\u65b0\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7a00\u758f\u5fae\u8c03\uff0c\u4ee5\u9002\u5e94ICU\u6570\u636e\u96c6\u3002", "result": "CU-ICU\u5728ICU\u5173\u952e\u4efb\u52a1\uff08\u5982\u65e9\u671f\u8113\u6bd2\u75c7\u68c0\u6d4b\u3001\u6b7b\u4ea1\u7387\u9884\u6d4b\u548c\u4e34\u5e8a\u7b14\u8bb0\u751f\u6210\uff09\u4e2d\uff0c\u76f8\u8f83\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u5728\u9ad8\u6548\u914d\u7f6e\u4e0b\uff0cCU-ICU\u4ec5\u66f4\u65b0\u4e0d\u52301%\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5728\u8113\u6bd2\u75c7\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534715%\uff0c\u4e34\u5e8a\u89e3\u91ca\u6027\u63d0\u534720%\u3002", "conclusion": "CU-ICU\u80fd\u4ee5\u4f4e\u7b97\u529b\u3001\u5f3a\u53ef\u6269\u5c55\u6027\u7684\u65b9\u5f0f\uff0c\u4e3a\u771f\u5b9eICU\u73af\u5883\u4e0b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u66f4\u51c6\u786e\u4e14\u6613\u89e3\u91ca\u7684\u65b9\u6848\u3002"}}
{"id": "2507.13666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13666", "abs": "https://arxiv.org/abs/2507.13666", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faKiC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u8bed\u4e49\u6bd4\u5bf9\u4f18\u5316\u7ea7\u8054\u63a8\u7406\u6d41\u7a0b\uff0c\u5728\u81ea\u7531\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u964d\u4f4e\u6210\u672c\u540c\u65f6\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8aGPT-4\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u6027\u80fd\u6a21\u578b\u9700\u8981\u901a\u8fc7API\u8bbf\u95ee\uff0c\u63a8\u7406\u6210\u672c\u9ad8\u3002\u4e3a\u964d\u4f4e\u6210\u672c\uff0c\u6709\u7814\u7a76\u901a\u8fc7cascade\u65b9\u6cd5\uff0c\u5148\u7528\u4f4e\u6210\u672c\u6a21\u578b\u9884\u6d4b\uff0c\u4e0d\u786e\u5b9a\u65f6\u518d\u5347\u7ea7\u66f4\u5f3a\u6a21\u578b\u3002\u7136\u800c\uff0c\u73b0\u6709cascade\u65b9\u6cd5\u5bf9\u4e8e\u81ea\u7531\u751f\u6210\u6587\u672c\uff0c\u53ea\u80fd\u7528\u6587\u672c\u7cbe\u786e\u5339\u914d\uff0c\u96be\u4ee5\u9009\u62e9\u548c\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faKeyword-inspired Cascade (KiC)\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u81ea\u7531\u6587\u672c\u3002KiC\u5148\u7528\u5f31\u6a21\u578b\u751f\u6210\u591a\u4e2a\u7b54\u6848\uff0c\u6311\u9009\u6700\u5177\u4ee3\u8868\u6027\u7684\u4e00\u4e2a\uff0c\u518d\u8bc4\u4f30\u5176\u4ed6\u8f93\u51fa\u4e0e\u8be5\u7b54\u6848\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u6839\u636e\u4e00\u81f4\u7a0b\u5ea6\uff0c\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u5f31\u6a21\u578b\u7b54\u6848\u6216\u5347\u7ea7\u81f3\u5f3a\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u81ea\u7531\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\uff0cKiC\u6846\u67b6\u4ee5\u5e73\u5747\u51cf\u5c1128.81%\u7684API\u6210\u672c\uff0c\u5b9e\u73b0\u4e8697.53%\u7684GPT-4\u51c6\u786e\u7387\uff0c\u5e76\u5728\u67d0\u4e00\u57fa\u51c6\u4e0a\u8d85\u8d8aGPT-4\u3002", "conclusion": "KiC\u6846\u67b6\u517c\u987e\u4e86\u6210\u672c\u6548\u7387\u4e0e\u8f93\u51fa\u8d28\u91cf\uff0c\u80fd\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u57fa\u7840\u4e0a\uff0c\u6709\u6548\u51cf\u5c11API\u8c03\u7528\u6210\u672c\uff0c\u5bf9\u81ea\u7531\u6587\u672c\u751f\u6210\u4efb\u52a1\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.13681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13681", "abs": "https://arxiv.org/abs/2507.13681", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.", "AI": {"tldr": "LoopServe\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u5316\u4e0e\u52a8\u6001Kv\u7f13\u5b58\u538b\u7f29\uff0c\u5927\u5e45\u52a0\u901f\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e0bLLM\u63a8\u7406\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u8d8a\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u771f\u5b9e\u4efb\u52a1\u57fa\u51c6\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7136\u800c\u968f\u7740\u5bf9\u8bdd\u5386\u53f2\u53d8\u957f\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u8ba1\u7b97\u548c\u5185\u5b58\u6d88\u8017\u5267\u589e\uff0c\u5f71\u54cd\u4ea4\u4e92\u6548\u7387\u4e0e\u54cd\u5e94\u901f\u5ea6\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5bf9\u591a\u8f6e\u52a8\u6001\u5bf9\u8bdd\u7684\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u9ad8\u6548\u652f\u6301\u771f\u5b9e\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51faLoopServe\u81ea\u9002\u5e94\u53cc\u9636\u6bb5\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff1a\uff081\uff09\u5728prefilling\u9636\u6bb5\u8fdb\u884c\u5728\u7ebf\u7a00\u758f\u5316\uff0c\u52a8\u6001\u6311\u9009\u6700\u91cd\u8981\u7684\u6ce8\u610f\u529b\u77e9\u9635\u90e8\u5206\uff1b\uff082\uff09\u5728\u89e3\u7801\u9636\u6bb5\u5f00\u5c55\u6e10\u8fdb\u5f0fKey-Value\u538b\u7f29\uff0c\u6839\u636e\u6700\u8fd1\u751f\u6210\u7684\u8f93\u51fatoken\u52a8\u6001\u7ef4\u62a4\u9ad8\u6548\u7684\u7f13\u5b58\u3002\u6b64\u5916\uff0c\u6784\u5efa\u5305\u542b11\u4e2a\u591a\u8f6e\u6570\u636e\u96c6\u7684\u65b0\u57fa\u51c6\uff0c\u8986\u76d6\u771f\u5b9e\u67e5\u8be2\u4f4d\u7f6e\u4e0e\u5bf9\u8bdd\u4f9d\u8d56\u3002", "result": "LoopServe\u5728\u5b9e\u9a8c\u4e2d\uff0c\u591a\u8f6e\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u76f8\u6bd4\u5df2\u6709\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f73\uff0c\u5927\u5e45\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6\u4e0e\u5bf9\u8bdd\u6548\u7387\u3002", "conclusion": "LoopServe\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e0b\u7684LLM\u63a8\u7406\u52a0\u901f\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63a8\u52a8\u4e86\u76f8\u5173\u57fa\u51c6\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.13705", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13705", "abs": "https://arxiv.org/abs/2507.13705", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.", "AI": {"tldr": "LLM\u5728\u7fa4\u7ec4\u63a8\u8350\u7cfb\u7edf\u4e2d\u63a8\u8350\u7c7b\u4f3c\u52a0\u6027\u6548\u7528\u6cd5\uff0c\u4f46\u5176\u89e3\u91ca\u5e38\u5305\u542b\u4e0d\u660e\u786e\u5b9a\u4e49\u7684\u989d\u5916\u6807\u51c6\uff0c\u4e14\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u968f\u8bc4\u5206\u89c4\u6a21\u589e\u5927\u800c\u4e0b\u964d\u3002\u9700\u5173\u6ce8\u89e3\u91ca\u751f\u6210\u65b9\u6cd5\u4e0e\u805a\u5408\u7b56\u7565\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7fa4\u7ec4\u63a8\u8350\u7cfb\u7edf\uff08GRS\uff09\u4e2d\u88ab\u4f5c\u4e3a\u51b3\u7b56\u8005\u548c\u89e3\u91ca\u751f\u6210\u5668\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u8bc4\u4ef7LLM\u63a8\u8350\u548c\u89e3\u91ca\u7ed3\u679c\u7684\u51c6\u786e\u6027\u53ca\u5408\u7406\u6027\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u63a2\u8ba8LLM\u63a8\u8350\u673a\u5236\u4e0e\u4f20\u7edf\u793e\u4f1a\u9009\u62e9\u805a\u5408\u7b56\u7565\u7684\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u5176\u8f93\u51fa\u89e3\u91ca\u5bf9\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c06LLM\u751f\u6210\u7684\u63a8\u8350\u53ca\u5176\u89e3\u91ca\u4e0e\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u4e2d\u7684\u805a\u5408\u7b56\u7565\uff08\u7279\u522b\u662f\u52a0\u6027\u6548\u7528\u96c6\u5408ADD\uff09\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8003\u5bdf\u4e0d\u540c\u7fa4\u7ec4\u7ed3\u6784\uff08\u7edf\u4e00\u6216\u5206\u6b67\uff09\u4e0b\uff0cLLM\u7684\u63a8\u8350\u548c\u7406\u7531\u662f\u5426\u4e00\u81f4\uff0c\u4ee5\u53caLLM\u89e3\u91ca\u4e2d\u51fa\u73b0\u7684\u989d\u5916\u6807\u51c6\u548c\u6f5c\u5728\u4e0d\u4e00\u81f4\u6027\u3002", "result": "LLM\u751f\u6210\u7684\u63a8\u8350\u5f80\u5f80\u7c7b\u4f3c\u4e8e\u52a0\u6027\u6548\u7528\u6574\u5408\uff08ADD\u65b9\u6cd5\uff09\uff0c\u4e14\u7fa4\u7ec4\u7ed3\u6784\u5bf9\u63a8\u8350\u7ed3\u679c\u65e0\u663e\u8457\u5f71\u54cd\u3002\u4f46LLM\u7ed9\u51fa\u7684\u89e3\u91ca\u66f4\u52a0\u503e\u5411\u4e8e\u5e73\u5747\u5206\u6216\u5f15\u5165\u5176\u4ed6\u6807\u51c6\uff08\u6bd4\u5982\u7528\u6237\u6216\u7269\u54c1\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\u3001\u6d41\u884c\u5ea6\u7b49\uff09\uff0c\u4e00\u4e9b\u6807\u51c6\u672a\u660e\u786e\u5b9a\u4e49\u3002\u89e3\u91ca\u4e2d\u7684\u6807\u51c6\u6570\u91cf\u4f1a\u968f\u7fa4\u7ec4\u8bc4\u5206\u6570\u91cf\u589e\u591a\u800c\u589e\u591a\uff0c\u5927\u9879\u96c6\u4e0b\u5e38\u89c4\u805a\u5408\u65b9\u6cd5\u6548\u7387\u964d\u4f4e\u3002LLM\u89e3\u91ca\u5b58\u5728\u4e0d\u4e00\u81f4\u4e0e\u6a21\u7cca\uff0c\u524a\u5f31\u4e86\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "LLM\u5728\u7fa4\u7ec4\u63a8\u8350\u7cfb\u7edf\u4e2d\u80fd\u591f\u4ea7\u751f\u7c7bADD\u805a\u5408\u7ed3\u679c\uff0c\u4f46\u5728\u89e3\u91ca\u63a8\u8350\u65f6\u6613\u5f15\u5165\u591a\u4f59\u548c\u6a21\u7cca\u6807\u51c6\uff0c\u4e14\u968f\u7fa4\u7ec4\u8bc4\u5206\u89c4\u6a21\u6269\u5927\uff0c\u8fd9\u4e00\u95ee\u9898\u52a0\u5267\u3002\u8fd9\u79cd\u89e3\u91ca\u673a\u5236\u4e0a\u7684\u7f3a\u9677\u5f71\u54cd\u4e86LLM\u5728GRS\u4e2d\u7684\u900f\u660e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u5173\u6ce8\u5176\u4e0e\u6807\u51c6\u805a\u5408\u7b56\u7565\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u6539\u8fdb\u89e3\u91ca\u751f\u6210\u65b9\u5f0f\u3002"}}
{"id": "2507.13732", "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13732", "abs": "https://arxiv.org/abs/2507.13732", "authors": ["Guillaume Zambrano"], "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.", "AI": {"tldr": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u6790\u6cd5\u56fd\u5224\u51b3\u6570\u636e\uff0c\u53d1\u73b0\u6cd5\u5b98\u4e2a\u4eba\u7279\u70b9\u4f1a\u663e\u8457\u5f71\u54cd\u629a\u517b\u6743\u5224\u51b3\u7ed3\u679c\uff0c\u201c\u4e13\u5bb6\u201d\u6a21\u578b\u9884\u6d4b\u6548\u679c\u8fdc\u8d85\u201c\u901a\u7528\u201d\u6a21\u578b\uff0c\u4e3a\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u7406\u8bba\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5b9e\u8bc1\u652f\u6491\u3002", "motivation": "\u672c\u6587\u4ee5\u6cd5\u56fd\u4e0a\u8bc9\u6cd5\u9662\u7684\u513f\u7ae5\u629a\u517b\u6743\u5224\u51b3\u4e3a\u6848\u4f8b\uff0c\u65e8\u5728\u63a2\u7a76\u6cd5\u5b98\u5728\u6cd5\u5f8b\u5224\u51b3\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\uff0c\u6311\u6218\u201c\u6cd5\u5b98\u4ec5\u4f5c\u4e3a\u6cd5\u5f8b\u4e2d\u7acb\u9002\u7528\u8005\u201d\u7684\u4f20\u7edf\u5047\u8bbe\u3002\u4f5c\u8005\u5173\u6ce8\u4e8e\u4e2a\u4eba\u6cd5\u5b98\u662f\u5426\u5bf9\u6848\u4ef6\u7ed3\u679c\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u8fdb\u800c\u56de\u5e94\u6cd5\u5b66\u4e2d\u7684\u73b0\u5b9e\u4e3b\u4e49\u4e0e\u5f62\u5f0f\u4e3b\u4e49\u4e89\u8bba\u3002", "method": "\u6587\u7ae0\u91c7\u752818,937\u4efd\u6765\u81ea10,306\u4e2a\u6848\u4ef6\u7684\u6cd5\u56fd\u513f\u7ae5\u629a\u517b\u6743\u88c1\u51b3\u6587\u672c\u3002\u9996\u5148\uff0c\u4e25\u683c\u53bb\u6807\u8bc6\u4ee5\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u3002\u7136\u540e\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u7ed3\u6784\u5316\u7279\u5f81\u62bd\u53d6\uff0c\u518d\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982RF\u3001XGB\u3001SVC\uff09\u8fdb\u884c\u7ed3\u679c\u9884\u6d4b\u3002\u6a21\u578b\u5206\u4e3a\u9488\u5bf9\u5355\u4e00\u6cd5\u5b98\u7684\u201c\u4e13\u5bb6\u201d\u6a21\u578b\u3001\u4ee5\u53ca\u9762\u5411\u6240\u6709\u6cd5\u5b98\u7684\u201c\u901a\u7528\u201d\u6a21\u578b\uff0c\u5e76\u4e14\u8fdb\u884c\u4e86In-Domain\u548cCross-Domain\u6709\u6548\u6027\u6d4b\u8bd5\u3002", "result": "\u9762\u5411\u5355\u4e00\u6cd5\u5b98\u8bad\u7ec3\u7684\u201c\u4e13\u5bb6\u201d\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u5927\u5e45\u9886\u5148\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u6700\u9ad8F1\u5206\u6570\u8fbe\u523092.85%\uff0c\u800c\u901a\u7528\u6a21\u578b\u4ec5\u4e3a82.63%\uff08\u6837\u672c\u6570\u91cf\u591a20-100\u500d\uff09\u3002\u4e13\u5bb6\u6a21\u578b\u80fd\u6355\u6349\u5230\u6cd5\u5b98\u4e2a\u4eba\u7684\u7a33\u5b9a\u5224\u51b3\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u65e0\u6cd5\u8f6c\u79fb\u81f3\u5176\u4ed6\u6cd5\u5b98\u3002\u6709\u6548\u6027\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u6cd5\u5b98\u8eab\u4efd\u5bf9\u88c1\u51b3\u7ed3\u679c\u7684\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6cd5\u5b98\u8eab\u4efd\u5bf9\u6848\u4ef6\u7ed3\u679c\u6709\u53ef\u91cf\u5316\u5f71\u54cd\uff0c\u8fdd\u80cc\u4e86\u201c\u6cd5\u5b98\u4ec5\u673a\u68b0\u9002\u6cd5\u201d\u7684\u5047\u8bbe\uff0c\u4e3a\u6cd5\u5f8b\u73b0\u5b9e\u4e3b\u4e49\u63d0\u4f9b\u4e86\u65b0\u7684\u7ecf\u9a8c\u8bc1\u636e\u3002\u6587\u4e2d\u6240\u6709\u6570\u636e\u4e0e\u4ee3\u7801\u5747\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.13743", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13743", "abs": "https://arxiv.org/abs/2507.13743", "authors": ["Maluna Menke", "Thilo Hagendorff"], "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs", "comment": null, "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.", "AI": {"tldr": "\u6587\u7ae0\u53d1\u73b0\u73b0\u6709LLM\u5b58\u5728\u4e25\u91cd\u6027\u522b\u4e0e\u6027\u5c11\u6570\u504f\u89c1\uff0c\u901a\u8fc7LoRA\u9ad8\u6548\u5fae\u8c03\u5927\u5e45\u7f13\u89e3\u504f\u89c1\u4e14\u8ba1\u7b97\u4ee3\u4ef7\u6781\u4f4e\uff0c\u5efa\u8bae\u63a8\u5e7fPEFT\u5e76\u6df1\u5316\u76f8\u5173\u8bc4\u6d4b\u4e0e\u6570\u636e\u5efa\u8bbe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bb9\u6613\u590d\u5236\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u6027\u522b\u4e0e\u6027\u5c11\u6570\uff08LGBTQIA+\uff09\u504f\u89c1\uff0c\u5bfc\u81f4\u8f93\u51fa\u5185\u5bb9\u8fb9\u7f18\u5316\u76f8\u5173\u7fa4\u4f53\uff0c\u56e0\u6b64\u4e9f\u9700\u964d\u4f4e\u8fd9\u7c7b\u504f\u89c1\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u2014\u2014\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u8f6f\u63d0\u793a\u5fae\u8c03\uff0c\u7528\u4ee5\u66ff\u4ee3\u5168\u6a21\u578b\u5fae\u8c03\uff0c\u4ece\u800c\u7f13\u89e3\u504f\u89c1\u3002\u5229\u7528WinoQueer\u57fa\u51c6\u548cQueerNews\u8bed\u6599\uff0c\u5bf9\u4e09\u4e2a\u5f00\u6e90LLM\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91cf\u5316\u504f\u89c1\u5f97\u5206\u540e\u8bc4\u4f30PEFT\u6280\u672f\u7684\u6539\u5584\u6548\u679c\u3002", "result": "\u539f\u59cb\u504f\u89c1\u5206\u6570\u9ad8\u8fbe98\uff08\u6ee1\u5206100\uff0c50\u4e3a\u4e2d\u6027\uff09\u3002LoRA\u5fae\u8c03\uff08\u589e\u52a0\u53c2\u6570\u5c11\u4e8e0.1%\uff09\u80fd\u5c06\u504f\u89c1\u5206\u6570\u964d\u4f4e\u81f350\u5206\uff0c\u6a21\u578b\u4e2d\u6027\u8f93\u51fa\u6bd4\u4f8b\u4ece0%\u63d0\u5347\u523036%\u3002\u8f6f\u63d0\u793a\u5fae\u8c03\uff08\u4ec510\u4e2a\u865a\u62dftoken\uff09\u4ec5\u6709\u5fae\u5f31\u6539\u5584\u3002", "conclusion": "LoRA\u53ef\u4ee5\u7528\u6781\u5c0f\u7684\u989d\u5916\u53c2\u6570\u5b9e\u73b0\u663e\u8457\u7684\u516c\u5e73\u6027\u63d0\u5347\u3002\u547c\u5401\u793e\u533a\u91c7\u7528PEFT\u3001\u5efa\u8bbe\u66f4\u591a\u540c\u6027\u604b\u4f5c\u8005\u8bed\u6599\u4e0e\u66f4\u4e30\u5bcc\u7684\u8bc4\u6d4b\u5de5\u5177\uff0c\u540c\u65f6\u9700\u6301\u7eed\u8fdb\u884c\u516c\u6b63\u6027\u5ba1\u6838\u3002"}}
{"id": "2507.13761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13761", "abs": "https://arxiv.org/abs/2507.13761", "authors": ["Palash Nandi", "Maithili Joshi", "Tanmoy Chakraborty"], "title": "Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models", "comment": null, "summary": "Language models are highly sensitive to prompt formulations - small changes\nin input can drastically alter their output. This raises a critical question:\nTo what extent can prompt sensitivity be exploited to generate inapt content?\nIn this paper, we investigate how discrete components of prompt design\ninfluence the generation of inappropriate content in Visual Language Models\n(VLMs). Specifically, we analyze the impact of three key factors on successful\njailbreaks: (a) the inclusion of detailed visual information, (b) the presence\nof adversarial examples, and (c) the use of positively framed beginning\nphrases. Our findings reveal that while a VLM can reliably distinguish between\nbenign and harmful inputs in unimodal settings (text-only or image-only), this\nability significantly degrades in multimodal contexts. Each of the three\nfactors is independently capable of triggering a jailbreak, and we show that\neven a small number of in-context examples (as few as three) can push the model\ntoward generating inappropriate outputs. Furthermore, we propose a framework\nthat utilizes a skip-connection between two internal layers of the VLM, which\nsubstantially increases jailbreak success rates, even when using benign images.\nFinally, we demonstrate that memes, often perceived as humorous or harmless,\ncan be as effective as toxic visuals in eliciting harmful content, underscoring\nthe subtle and complex vulnerabilities of VLMs.", "AI": {"tldr": "VLM\u5bf9\u63d0\u793a\u6781\u5176\u654f\u611f\uff0c\u591a\u6a21\u6001\u73af\u5883\u53ca\u5de7\u5999\u8bbe\u8ba1\u7684\u63d0\u793a\u5bb9\u6613\u88ab\u5229\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u63d0\u51fa\u7684skip-connection\u65b9\u6cd5\u53ef\u52a0\u5267\u6b64\u95ee\u9898\uff0c\u8868\u660eVLM\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8f93\u51fa\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u63d0\u793a\uff08prompt\uff09\u5982\u4f55\u8bbe\u8ba1\uff0c\u800c\u7ec6\u5fae\u7684\u8f93\u5165\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u6781\u5927\u6539\u53d8\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u5173\u6ce8\u5229\u7528\u8fd9\u79cd\u63d0\u793a\u654f\u611f\u6027\u6765\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u7684\u98ce\u9669\u548c\u6210\u56e0\u3002", "method": "\u7cfb\u7edf\u6027\u5206\u6790\u63d0\u793a\u8bbe\u8ba1\u4e2d\u4e09\u4e2a\u79bb\u6563\u8981\u7d20\u5728\u4fc3\u4f7fVLM\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\uff08\u5373\u201c\u8d8a\u72f1\u201d\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u5206\u522b\u4e3a\uff1a(a)\u8be6\u7ec6\u89c6\u89c9\u4fe1\u606f\u7684\u5f15\u5165\uff0c(b)\u5bf9\u6297\u6027\u6837\u672c\u7684\u5b58\u5728\uff0c(c)\u6b63\u5411\u8868\u8ff0\u7684\u5f00\u5934\u77ed\u8bed\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5728VLM\u5185\u90e8\u4e24\u5c42\u4e4b\u95f4\u5f15\u5165skip-connection\u7684\u6846\u67b6\uff0c\u5e76\u6d4b\u8bd5\u5404\u79cd\u63d0\u793a\u7ec4\u5408\u548c\u6837\u672c\u6570\u91cf\u5bf9\u8d8a\u72f1\u6210\u529f\u7387\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5355\u6a21\u6001\u8f93\u5165\uff08\u4ec5\u6587\u672c\u6216\u4ec5\u56fe\u7247\uff09\u65f6\uff0c\u6a21\u578b\u80fd\u8f83\u597d\u533a\u5206\u826f\u6027\u4e0e\u6709\u5bb3\u5185\u5bb9\uff0c\u4f46\u5728\u591a\u6a21\u6001\u60c5\u5883\u4e0b\uff0c\u8fd9\u79cd\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002\u4e0a\u8ff0\u4e09\u4e2a\u8981\u7d20\u5404\u81ea\u90fd\u80fd\u72ec\u7acb\u5b8c\u6210\u8d8a\u72f1\u4efb\u52a1\uff0c\u4e14\u53ea\u97003\u4e2a\u4e0a\u4e0b\u6587\u6837\u4f8b\u5373\u53ef\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u3002\u6240\u63d0\u51fa\u7684skip-connection\u673a\u5236\u5927\u5e45\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u5373\u4fbf\u4ec5\u7528\u826f\u6027\u56fe\u7247\u3002\u8da3\u5473\u6216\u8868\u9762\u65e0\u5bb3\u7684\u201cmeme\u201d\u56fe\u7247\u4e0e\u6709\u6bd2\u89c6\u89c9\u5185\u5bb9\u4e00\u6837\uff0c\u80fd\u8bf1\u53d1\u6709\u5bb3\u8f93\u51fa\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u5b58\u5728\u5fae\u5999\u4e14\u590d\u6742\u7684\u5b89\u5168\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5bf9\u63d0\u793a\u8bbe\u8ba1\u6781\u5ea6\u654f\u611f\uff0c\u6613\u4e8e\u88ab\u5229\u7528\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\uff0c\u751a\u81f3\u826f\u6027\u56fe\u50cf\u4e5f\u53ef\u88ab\u653b\u51fb\u3002"}}
{"id": "2507.13793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13793", "abs": "https://arxiv.org/abs/2507.13793", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Xuemeng Song", "Tian Gan", "Liqiang Nie"], "title": "An Enhanced Model-based Approach for Short Text Clustering", "comment": null, "summary": "Short text clustering has become increasingly important with the popularity\nof social media like Twitter, Google+, and Facebook. Existing methods can be\nbroadly categorized into two paradigms: topic model-based approaches and deep\nrepresentation learning-based approaches. This task is inherently challenging\ndue to the sparse, large-scale, and high-dimensional characteristics of the\nshort text data. Furthermore, the computational intensity required by\nrepresentation learning significantly increases the running time. To address\nthese issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet\nMultinomial Mixture model (GSDMM), which effectively handles the sparsity and\nhigh dimensionality of short texts while identifying representative words for\neach cluster. Based on several aspects of GSDMM that warrant further\nrefinement, we propose an improved approach, GSDMM+, designed to further\noptimize its performance. GSDMM+ reduces initialization noise and adaptively\nadjusts word weights based on entropy, achieving fine-grained clustering that\nreveals more topic-related information. Additionally, strategic cluster merging\nis employed to refine clustering granularity, better aligning the predicted\ndistribution with the true category distribution. We conduct extensive\nexperiments, comparing our methods with both classical and state-of-the-art\napproaches. The experimental results demonstrate the efficiency and\neffectiveness of our methods. The source code for our model is publicly\navailable at https://github.com/chehaoa/VEMC.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u77ed\u6587\u672c\u805a\u7c7b\u7b97\u6cd5GSDMM\u53ca\u5176\u6539\u8fdb\u7248GSDMM+\uff0c\u5728\u89e3\u51b3\u77ed\u6587\u672c\u805a\u7c7b\u7684\u7a00\u758f\u3001\u9ad8\u7ef4\u548c\u805a\u7c7b\u7c92\u5ea6\u7b49\u95ee\u9898\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u591a\u79cd\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4e14\u6a21\u578b\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u5a92\u4f53\u5982Twitter\u3001Google+\u548cFacebook\u7684\u666e\u53ca\uff0c\u77ed\u6587\u672c\u805a\u7c7b\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u77ed\u6587\u672c\u672c\u8eab\u5b58\u5728\u7a00\u758f\u6027\u3001\u89c4\u6a21\u5927\u548c\u9ad8\u7ef4\u7b49\u95ee\u9898\uff0c\u52a0\u4e4b\u73b0\u6709\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u5bf9\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u77ed\u6587\u672c\u805a\u7c7b\u4efb\u52a1\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u8be5\u7814\u7a76\u65e8\u5728\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u63d0\u51fa\u9ad8\u6548\u3001\u6709\u6548\u7684\u89e3\u51b3\u529e\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9Dirichlet Multinomial Mixture\uff08DMM\uff09\u6a21\u578b\u7684collapsed Gibbs Sampling\u7b97\u6cd5\uff08GSDMM\uff09\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u77ed\u6587\u672c\u7684\u7a00\u758f\u6027\u4e0e\u9ad8\u7ef4\u95ee\u9898\uff0c\u5e76\u80fd\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u786e\u5b9a\u4ee3\u8868\u6027\u8bcd\u8bed\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5GSDMM+\uff0c\u901a\u8fc7\u51cf\u5c11\u521d\u59cb\u5316\u566a\u58f0\u3001\u57fa\u4e8e\u71b5\u81ea\u9002\u5e94\u8c03\u6574\u8bcd\u6743\u503c\uff0c\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u805a\u7c7b\u6548\u679c\u3002\u540c\u65f6\uff0c\u5229\u7528\u805a\u7c7b\u5408\u5e76\u7b56\u7565\u4f18\u5316\u805a\u7c7b\u7c92\u5ea6\uff0c\u4f7f\u9884\u6d4b\u5206\u5e03\u66f4\u8d34\u5408\u771f\u5b9e\u7c7b\u522b\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u4e0e\u7ecf\u5178\u65b9\u6cd5\u548c\u6700\u65b0\u6280\u672f\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002GSDMM+\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u805a\u7c7b\u548c\u63ed\u793a\u4e3b\u9898\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u77ed\u6587\u672c\u9690\u85cf\u7684\u771f\u5b9e\u7ed3\u6784\u3002", "conclusion": "GSDMM\u53ca\u5176\u6269\u5c55\u7248GSDMM+\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u77ed\u6587\u672c\u805a\u7c7b\u4e2d\u7684\u7a00\u758f\u6027\u3001\u9ad8\u7ef4\u6027\u548c\u805a\u7c7b\u7c92\u5ea6\u95ee\u9898\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u805a\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u76f8\u5173\u6a21\u578b\u6e90\u7801\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u5b66\u672f\u4ea4\u6d41\u548c\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2507.13827", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13827", "abs": "https://arxiv.org/abs/2507.13827", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e24\u79cd\u81ea\u52a8\u751f\u6210\u79d1\u5b66\u6587\u732e\u4e3b\u65e8QA\u5bf9\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u77e5\u8bc6\u56fe\u8c31\u548c\u5fae\u8c03\u662f\u63d0\u5347\u4e3b\u65e8\u62bd\u53d6\u8d28\u91cf\u7684\u5173\u952e\u3002", "motivation": "\u5b66\u8005\u5728\u51b3\u5b9a\u9605\u8bfb\u6216\u5f15\u7528\u4e00\u7bc7\u6587\u7ae0\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u8fc5\u901f\u4e86\u89e3\u5176\u4e3b\u8981\u601d\u60f3\u548c\u8d21\u732e\u3002\u56e0\u800c\u9700\u8981\u9ad8\u6548\u63d0\u53d6\u79d1\u5b66\u6587\u732e\u7684\u6838\u5fc3\u5185\u5bb9\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7QA\u5bf9\u5f62\u5f0f\uff0c\u81ea\u52a8\u5316\u62bd\u53d6\u6587\u7ae0\u7684\u6838\u5fc3\u6982\u5ff5\u4e0e\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u81ea\u52a8\u751f\u6210QA\u5bf9\u7684\u65b9\u6cd5\uff1a1\uff09\u4ec5\u7528LLM\u6839\u636e\u6587\u7ae0\u6311\u9009\u7a81\u51fa\u6bb5\u843d\uff0c\u751f\u6210\u95ee\u9898\u5e76\u6309\u7b54\u6848\u8d28\u91cf\u6392\u5e8f\u540e\u751f\u6210\u7b54\u6848\uff1b2\uff09\u901a\u8fc7\u5b9e\u4f53\u5173\u7cfb\uff08ER\uff09\u62bd\u53d6\u6a21\u578b\u5fae\u8c03\u5e76\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\uff0c\u5229\u7528\u663e\u8457\u4e09\u5143\u7ec4\u62bd\u53d6\u7b97\u6cd5\uff0c\u7ed3\u5408TF-IDF\u98ce\u683c\u7684\u4e2d\u5fc3\u6027\u5ea6\u91cf\u9009\u62e9\u91cd\u8981\u4e09\u5143\u7ec4\uff0c\u5e76\u5728KG\u57fa\u7840\u4e0a\u751f\u6210QA\u3002\u4e24\u79cd\u65b9\u6cd5\u7684QA\u5747\u7531\u9886\u57df\u4e13\u5bb6\u7528\u9884\u8bbe\u6307\u6807\u8bc4\u4ef7\u3002", "result": "KG\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u6587\u7ae0\u4e3b\u8981\u601d\u60f3\uff1b\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u5728\u79d1\u5b66\u6587\u732e\u4e0a\u7684\u5fae\u8c03\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\u7684\u62bd\u53d6\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684QA\u751f\u6210\u5728\u79d1\u5b66\u6587\u732e\u4e3b\u65e8\u6355\u6349\u4e2d\u66f4\u4e3a\u6709\u6548\uff0c\u6a21\u578b\u5fae\u8c03\u5bf9\u4e09\u5143\u7ec4\u62bd\u53d6\u8d28\u91cf\u6709\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.13839", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13839", "abs": "https://arxiv.org/abs/2507.13839", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.", "AI": {"tldr": "\u5728\u4e2d\u6587\u5fc3\u7406\u54a8\u8be2\u4e2d\uff0c\u8d1f\u9762\u60c5\u611f\u8bcd\u9891\u4e0e\u6765\u8bbf\u8005\u6291\u90c1\u3001\u7126\u8651\u663e\u8457\u76f8\u5173\uff0c\u4f46\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u672a\u51fa\u73b0\u7c7b\u4f3c\u5173\u8054\uff0c\u63d0\u793a\u6587\u5316\u8bed\u5883\u548c\u5bf9\u8bdd\u4e92\u52a8\u5f71\u54cd\u5fc3\u7406\u8bed\u8a00\u6807\u8bb0\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u4e2d\u6587\u5fc3\u7406\u54a8\u8be2\u4e92\u52a8\u4e2d\uff0c\u8bed\u8a00\u8868\u8fbe\u4e0e\u6291\u90c1\u3001\u7126\u8651\u7b49\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5c24\u5176\u5173\u6ce8\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u548c\u8d1f\u9762\u60c5\u611f\u8bcd\u7684\u4f7f\u7528\u3002\u6b64\u524d\u5927\u591a\u7814\u7a76\u57fa\u4e8e\u82f1\u8bed\u8bed\u5883\uff0c\u5bf9\u4e8e\u4e2d\u6587\u60c5\u5883\u4e0b\u7684\u5177\u4f53\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8be5\u9886\u57df\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u57fa\u4e8e735\u6b21\u5728\u7ebf\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u91c7\u7528LIWC\uff08Linguistic Inquiry and Word Count\uff09\u8f6f\u4ef6\u5bf9\u8bed\u8a00\u6a21\u5f0f\u8fdb\u884c\u91cf\u5316\u5206\u6790\uff0c\u968f\u540e\u901a\u8fc7\u5e7f\u4e49\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bed\u8a00\u7279\u5f81\u4e0e\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\uff0c\u8d1f\u9762\u60c5\u611f\u8bcd\u6c47\u7684\u9891\u7387\u4e0e\u6765\u8bbf\u8005\u7684\u6291\u90c1\u548c\u7126\u8651\u7a0b\u5ea6\u663e\u8457\u6b63\u76f8\u5173\u3002\u4f46\u4e0e\u4ee5\u5f80\u4e3b\u8981\u57fa\u4e8e\u82f1\u6587\u7814\u7a76\u7684\u53d1\u73b0\u4e0d\u540c\uff0c\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u7684\u4f7f\u7528\u9891\u7387\u5e76\u672a\u968f\u5fc3\u7406\u72b6\u51b5\u6709\u663e\u8457\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u663e\u793a\u6587\u5316\u548c\u4f1a\u8bdd\u8bed\u5883\u5bf9\u5fc3\u7406\u5065\u5eb7\u4ea4\u6d41\u4e2d\u7684\u8bed\u8a00\u4f7f\u7528\u5177\u6709\u590d\u6742\u5f71\u54cd\uff0c\u5728\u6c49\u8bed\u8bed\u5883\u4e0b\uff0c\u8d1f\u9762\u60c5\u611f\u8bcd\u53ef\u4f5c\u4e3a\u6291\u90c1\u3001\u7126\u8651\u7684\u91cd\u8981\u8bed\u8a00\u6807\u8bb0\uff0c\u800c\u7b2c\u4e00\u4eba\u79f0\u5355\u6570\u4ee3\u8bcd\u5219\u672a\u8868\u73b0\u51fa\u76f8\u540c\u5173\u8054\u3002\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u5fc3\u7406\u8bed\u8a00\u6807\u8bb0\u53ca\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u610f\u4e49\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.13841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13841", "abs": "https://arxiv.org/abs/2507.13841", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "title": "Modeling Fair Play in Detective Stories with Language Models", "comment": null, "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u8bc4\u4f30\u63a8\u7406\u5c0f\u8bf4\u201c\u516c\u6b63\u53d9\u4e8b\u201d\u4e0e\u610f\u5916\u6027\u7684\u6982\u7387\u6846\u67b6\uff0c\u53d1\u73b0\u73b0\u6709LLM\u751f\u6210\u6545\u4e8b\u96be\u4ee5\u5e73\u8861\u8fd9\u4e24\u8005\uff0c\u5bfc\u81f4\u6574\u4f53\u8d28\u91cf\u4e0d\u8db3\u3002", "motivation": "\u53d9\u4e8b\u827a\u672f\u4e2d\uff0c\u8bfb\u8005\u671f\u671b\u4e0e\u6545\u4e8b\u610f\u5916\u6027\u7684\u5e73\u8861\u81f3\u5173\u91cd\u8981\uff0c\u63a8\u7406\u5c0f\u8bf4\u5c24\u4e3a\u5982\u6b64\u3002\u516c\u6b63\u53d9\u4e8b\uff08fair play\uff09\u5f3a\u8c03\u4f5c\u8005\u4e0e\u8bfb\u8005\u4e4b\u95f4\u5bf9\u8c1c\u9898\u89e3\u7b54\u8303\u56f4\u7684\u9690\u6027\u5171\u8bc6\u3002\u8be5\u6587\u65e8\u5728\u5b9a\u91cf\u5206\u6790\u63a8\u7406\u5c0f\u8bf4\u4e2d\u7684\u8fd9\u79cd\u5e73\u8861\u5173\u7cfb\uff0c\u5e76\u501f\u52a9\u6982\u7387\u6846\u67b6\u8bc4\u4f30\u6545\u4e8b\u54c1\u8d28\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u6587\u672c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e94\u7528\u4e8e\u63a8\u7406\u5c0f\u8bf4\u7684\u6982\u7387\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6b64\u7cbe\u786e\u5b9a\u4e49\u4e86'\u516c\u6b63\u53d9\u4e8b'\u53ca\u5176\u91cf\u5316\u6307\u6807\u3002\u7136\u540e\uff0c\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u7531LLM\u751f\u6210\u7684\u63a8\u7406\u6545\u4e8b\uff0c\u901a\u8fc7\u5bf9\u6545\u4e8b\u8fde\u8d2f\u6027\uff08\u6545\u4e8b\u662f\u5426'\u5408\u7406'\uff09\u4e0e\u610f\u5916\u6027\uff08\u60ca\u559c\u7a0b\u5ea6\uff09\u7684\u5f20\u529b\u8fdb\u884c\u5ea6\u91cf\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0cLLM\u6240\u751f\u6210\u7684\u63a8\u7406\u5c0f\u8bf4\u867d\u7136\u5177\u6709\u4e00\u5b9a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff08\u60ca\u559c\uff09\uff0c\u4f46\u5728\u201c\u516c\u6b63\u53d9\u4e8b\u201d\u4e0e\u610f\u5916\u4e4b\u95f4\u7684\u6743\u8861\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u79cd\u5931\u8861\u76f4\u63a5\u5bfc\u81f4\u4e86\u751f\u6210\u6545\u4e8b\u6574\u4f53\u8d28\u91cf\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6982\u7387\u6846\u67b6\uff0c\u9996\u6b21\u91cf\u5316\u4e86\u63a8\u7406\u5c0f\u8bf4\u4e2d\u201c\u516c\u6b63\u53d9\u4e8b\u201d\u548c\u610f\u5916\u6027\u7684\u5e73\u8861\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u751f\u6210\u63a8\u7406\u6545\u4e8b\u666e\u904d\u8d28\u91cf\u4e0d\u9ad8\u7684\u6839\u6e90\u5728\u4e8e\u672a\u80fd\u59a5\u5584\u5904\u7406\u516c\u5e73\u4e0e\u60ca\u559c\u4e4b\u95f4\u7684\u5f20\u529b\u3002"}}
{"id": "2507.13858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13858", "abs": "https://arxiv.org/abs/2507.13858", "authors": ["Nicol\u00f2 Brunello", "Davide Rigamonti", "Andrea Sassella", "Vincenzo Scotti", "Mark James Carman"], "title": "InTraVisTo: Inside Transformer Visualisation Tool", "comment": "8 pages", "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86InTraVisTo\u5de5\u5177\uff0c\u53ef\u89c6\u5316Transformer\u5185\u90e8\u72b6\u6001\u548c\u4fe1\u606f\u6d41\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u6709\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b9e\u7528\u6027\u53d7\u5176\u8f93\u51fa\u4e0d\u53ef\u9884\u6d4b\u53ca\u884c\u4e3a\u4e0e\u9884\u671f\u4e0d\u7b26\u56f0\u6270\u3002\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u6025\u9700\u7406\u89e3\u6a21\u578b\u5185\u90e8\u673a\u5236\u6765\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u63a7\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u6b3e\u53ef\u89c6\u5316\u5de5\u5177InTraVisTo\uff0c\u80fd\u5c55\u793aTransformer\u6bcf\u5c42\u7684token\u5d4c\u5165\u5e76\u7528Sankey\u56fe\u53ef\u89c6\u5316\u5404\u7ec4\u4ef6\u7684\u4fe1\u606f\u6d41\u3002\u8fd9\u6837\u53ef\u4ee5\u8ddf\u8e2a\u6bcf\u4e2atoken\u7684\u751f\u6210\u8fc7\u7a0b\u548c\u4fe1\u606f\u5728\u6a21\u578b\u5185\u7684\u6d41\u52a8\u3002", "result": "InTraVisTo\u5b9e\u73b0\u4e86\u5bf9Transformer\u5185\u90e8\u72b6\u6001\u7684\u53ef\u89c6\u5316\u548c\u6210\u5206\u4fe1\u606f\u6d41\u8ffd\u8e2a\uff0c\u4f7f\u7814\u7a76\u8005\u80fd\u66f4\u597d\u5730\u7406\u89e3LLM\u7684\u8ba1\u7b97\u4e0e\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "InTraVisTo\u4e3a\u6df1\u5165\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u624b\u6bb5\uff0c\u5bf9\u63d0\u5347\u5176\u53ef\u63a7\u6027\u548c\u5b9e\u9645\u5e94\u7528\u5177\u6709\u79ef\u6781\u610f\u4e49\u3002"}}
{"id": "2507.13870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13870", "abs": "https://arxiv.org/abs/2507.13870", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.", "AI": {"tldr": "\u7f51\u7edc\u5b89\u5168NER\u6807\u7b7e\u4e0d\u7edf\u4e00\uff0c\u6570\u636e\u96c6\u878d\u5408\u96be\u3002\u6807\u7b7e\u7edf\u4e00+BiLSTM\u6cdb\u5316\u80fd\u529b\u5f31\uff0c\u591a\u5934/\u56fe\u6a21\u578b\u6548\u679c\u63d0\u5347\u6709\u9650\uff0c\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u8868\u73b0\u4e00\u822c\uff0c\u4e9f\u9700\u66f4\u4f18\u65b9\u6cd5\u3002", "motivation": "\u7f51\u7edc\u5b89\u5168\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u6807\u7b7e\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u96be\u4ee5\u6574\u5408\u548c\u5229\u7528\u3002\u63d0\u5347\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u53ef\u7528\u6027\u8feb\u5207\u9700\u8981\u6807\u7b7e\u7edf\u4e00\u3002", "method": "\u5bf9\u56db\u4e2a\u7f51\u7edc\u5b89\u5168\u6570\u636e\u96c6\u8fdb\u884c\u7c97\u7c92\u5ea6\u7684\u6807\u7b7e\u7edf\u4e00\uff0c\u5e76\u901a\u8fc7BiLSTM\u6a21\u578b\u8fdb\u884c\u6210\u5bf9\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u3002\u5bf9\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u63a2\u7d22\u9519\u8bef\u3001\u5c40\u9650\u548c\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u591a\u5934\u6a21\u578b\u548c\u57fa\u4e8e\u56fe\u7684\u8fc1\u79fb\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u5c1d\u8bd5\u7f13\u89e3\u7edf\u4e00\u6807\u7b7e\u5e26\u6765\u7684\u5c40\u9650\u6027\u3002", "result": "\u6a21\u578b\u5728\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5f31\u3002\u591a\u5934\u6a21\u578b\u5728\u6743\u91cd\u5171\u4eab\u4e0b\u6bd4\u7edf\u4e00\u8bad\u7ec3\u53ea\u6709\u8f7b\u5fae\u63d0\u5347\uff0c\u57fa\u4e8e\u56fe\u7684\u8fc1\u79fb\u6a21\u578b\uff08\u6784\u5efa\u5728BERT-base-NER\u4e0a\uff09\u4e0e\u666e\u901aBERT-base-NER\u76f8\u6bd4\u6ca1\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f53\u524d\u6807\u7b7e\u7edf\u4e00\u7b56\u7565\u672a\u80fd\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7684\u826f\u597d\u6cdb\u5316\uff0c\u4e0d\u540c\u67b6\u6784\u6539\u8fdb\u4e5f\u672a\u80fd\u5e26\u6765\u660e\u663e\u6027\u80fd\u63d0\u5347\uff0c\u663e\u793a\u51fa\u7f51\u7edc\u5b89\u5168NER\u9886\u57df\u5728\u6570\u636e\u6574\u5408\u4e0e\u5efa\u6a21\u4e0a\u7684\u6311\u6218\u3002"}}
{"id": "2507.13875", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13875", "abs": "https://arxiv.org/abs/2507.13875", "authors": ["Carlos Mena", "Pol Serra", "Jacobo Romero", "Abir Messaoudi", "Jose Giraldo", "Carme Armentano-Oller", "Rodolfo Zevallos", "Ivan Meza", "Javier Hernando"], "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies", "comment": "Accepted at Interspeech 2025", "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u6570\u636e\u77ed\u7f3a\u96be\u9898\uff0c\u4f5c\u8005\u901a\u8fc7\u5408\u6210\u6570\u636e\u3001\u62fc\u63a5\u5355\u8bed\u53ca\u8bed\u8a00token\u7b56\u7565\uff0c\u63d0\u5347\u4e86ASR\u6027\u80fd\uff0c\u6700\u4f73\u7b56\u7565\u4e3a\u201c\u5c11\u91cf\u5408\u6210\u6570\u636e+\u4e3b\u8bedtoken\u201d\uff0c\u5e76\u5f00\u6e90\u76f8\u5173\u6a21\u578b\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u5904\u7406\u4ee3\u7801\u5207\u6362\uff08CS\uff09\u8bed\u8a00\u2014\u2014\u5373\u5728\u540c\u4e00\u5bf9\u8bdd\u4e2d\u4ea4\u66ff\u4f7f\u7528\u591a\u79cd\u8bed\u8a00\u2014\u2014\u65f6\uff0c\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u53ca\u8bed\u8a00\u76f8\u4f3c\u6027\u6311\u6218\u3002\u7f3a\u4e4f\u4e13\u95e8\u7684CS\u8bed\u6599\u5e93\u5bfc\u81f4ASR\u7cfb\u7edf\u8868\u73b0\u53d7\u9650\uff0c\u5c24\u4ee5\u591a\u8bed\u79cd\u793e\u4f1a\uff08\u5982\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed\u5e7f\u6cdb\u4ee3\u7801\u5207\u6362\u7684\u73af\u5883\uff09\u4e3a\u751a\u3002\u8be5\u95ee\u9898\u5c24\u5176\u7a81\u51fa\uff0c\u56e0\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4ee3\u7801\u5207\u6362\u60c5\u5f62\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u5dee\u5f02\u8f83\u5927\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e09\u79cd\u63d0\u5347\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed\u4ee3\u7801\u5207\u6362ASR\u6a21\u578b\u7684\u65b9\u6cd5\uff1a\uff081\uff09\u751f\u6210\u5408\u6210CS\u6570\u636e\uff1b\uff082\uff09\u62fc\u63a5\u5355\u8bed\u8bed\u97f3\u6570\u636e\uff1b\uff083\uff09\u5229\u7528\u771f\u5b9eCS\u6570\u636e\u5e76\u52a0\u5165\u8bed\u8a00\u63d0\u793a\uff08tokens\uff09\u3002\u8fd9\u4e9bCS\u6570\u636e\u4ece\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u8bed\u97f3\u8bed\u6599\u4e2d\u63d0\u53d6\uff0c\u5e76\u5728OpenAI Whisper\u6a21\u578b\u57fa\u7840\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u6700\u7ec8\u6a21\u578b\u53d1\u5e03\u4e8eHugging Face\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c06\u5c11\u91cf\u5408\u6210CS\u6570\u636e\u4e0e\u4e3b\u5bfc\u8bed\u8a00token\u7ec4\u5408\u8fdb\u884c\u5fae\u8c03\uff0c\u80fd\u53d6\u5f97\u6700\u4f73\u7684\u8f6c\u5f55\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u771f\u5b9eCS\u6216\u62fc\u63a5\u5355\u8bed\u6570\u636e\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u4f7f\u7528\uff0c\u5e76\u5f15\u5165\u8bed\u8a00token\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7801\u5207\u6362\uff08\u4ee5\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed-\u897f\u73ed\u7259\u8bed\u4e3a\u4f8b\uff09\u8bed\u97f3\u8bc6\u522b\u7684\u6027\u80fd\u3002\u4f5c\u8005\u7684\u5de5\u4f5c\u4e3a\u591a\u8bed\u79cd\u793e\u4f1aCS\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13881", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13881", "abs": "https://arxiv.org/abs/2507.13881", "authors": ["Cole Walsh", "Rodica Ivan", "Muhammad Zafar Iqbal", "Colleen Robb"], "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test", "comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States", "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3\u4eba\u5de5\u8bc4\u5206\u4ee5\u63d0\u53d6\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\u4e2d\u4e0e\u7ed3\u6784\u76f8\u5173\u7684\u6280\u80fd\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u8bc4\u5206\u7684\u7ed3\u6784\u6548\u5ea6\u96be\u9898\uff0c\u4e3a\u4e2a\u4eba\u548c\u804c\u4e1a\u80fd\u529b\u5927\u89c4\u6a21\u6d4b\u8bc4\u94fa\u8def\u3002", "motivation": "\u5b66\u672f\u9879\u76ee\u65e5\u76ca\u91cd\u89c6\u4e2a\u4eba\u548c\u804c\u4e1a\u6280\u80fd\uff0c\u8ba4\u4e3a\u5176\u4e0e\u6280\u672f\u4e13\u957f\u540c\u7b49\u91cd\u8981\uff0c\u7528\u4e8e\u5b66\u751f\u672a\u6765\u804c\u4e1a\u53d1\u5c55\u3002\u4f46\u8fd9\u4e9b\u6280\u80fd\u7684\u6d4b\u91cf\u4e0e\u53d1\u5c55\u9700\u8981\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u3002\u60c5\u5883\u5224\u65ad\u6d4b\u8bd5\uff08SJT\uff09\u662f\u6807\u51c6\u5316\u6d4b\u91cf\u8fd9\u4e9b\u6280\u80fd\u7684\u4e00\u79cd\u65b9\u6848\uff0c\u4f46\u5176\u5f00\u653e\u5f0f\u4f5c\u7b54\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u8bc4\u5206\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\u3002\u4ee5\u5f80\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u65b9\u6cd5\u81ea\u52a8\u8bc4\u5206\u6548\u679c\u6709\u9650\uff0c\u7f3a\u4e4f\u7ed3\u6784\u6548\u5ea6\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4eceSJT\u4f5c\u7b54\u4e2d\u63d0\u53d6\u4e0e\u7ed3\u6784\u76f8\u5173\u7684\u7279\u5f81\uff0c\u65e8\u5728\u63d0\u5347\u81ea\u52a8\u8bc4\u5206\u7684\u6548\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u4ee5Casper SJT\u4e3a\u4f8b\u8fdb\u884c\u65b9\u6cd5\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4eceSJT\u4f5c\u7b54\u4e2d\u63d0\u53d6\u7ed3\u6784\u76f8\u5173\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u52a8\u5316\u4e2a\u4eba\u548c\u804c\u4e1a\u6280\u80fd\u8bc4\u5206\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "LLMs\u80fd\u6709\u6548\u63d0\u53d6SJT\u4f5c\u7b54\u4e2d\u7ed3\u6784\u76f8\u5173\u7684\u7279\u5f81\uff0c\u662f\u63a8\u8fdb\u5927\u89c4\u6a21\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u6709\u529b\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u53d1\u5c55\u5b66\u751f\u7684\u4e2a\u4eba\u4e0e\u804c\u4e1a\u6280\u80fd\u3002"}}
{"id": "2507.13913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13913", "abs": "https://arxiv.org/abs/2507.13913", "authors": ["Matous Volf", "Jakub Simko"], "title": "Political Leaning and Politicalness Classification of Texts", "comment": null, "summary": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6c47\u603b\u548c\u6269\u5c55\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u653f\u6cbb\u6587\u672c\u81ea\u52a8\u5206\u7c7b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65b0\u65b9\u6cd5\u5bf9\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u9002\u5e94\u6027\u66f4\u5f3a\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8etransformer\u7684\u6587\u672c\u653f\u6cbb\u503e\u5411\u4e0e\u653f\u6cbb\u6027\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u5206\u5e03\u6587\u672c\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u8f83\u5dee\uff0c\u7f3a\u4e4f\u7edf\u4e00\u3001\u591a\u6837\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u5c0612\u4e2a\u653f\u6cbb\u503e\u5411\u5206\u7c7b\u6570\u636e\u96c6\u5408\u5e76\u3001\u6269\u5c5518\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u6807\u6ce8\u653f\u6cbb\u6027\u65b0\u6807\u7b7e\uff0c\u6784\u5efa\u591a\u5143\u5316\u6570\u636e\u96c6\u3002\u91c7\u7528leave-one-in\u548cleave-one-out\u8bc4\u6d4b\u73b0\u6709\u6a21\u578b\u5e76\u8bad\u7ec3\u65b0\u6a21\u578b\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u65b0\u5efa\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4ee5\u53ca\u8bad\u7ec3\u7684\u65b0\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u5b64\u7acb\u5f00\u53d1\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u6570\u636e\u96c6\u548c\u521b\u5efa\u65b0\u7684\u6807\u6ce8\uff0c\u63d0\u5347\u4e86\u6587\u672c\u653f\u6cbb\u503e\u5411\u548c\u653f\u6cbb\u6027\u81ea\u52a8\u5206\u7c7b\u4efb\u52a1\u4e2d\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u80fd\u66f4\u597d\u9002\u5e94\u4e0d\u540c\u5206\u5e03\u7684\u6570\u636e\uff0c\u8f83\u73b0\u6709\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.13919", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13919", "abs": "https://arxiv.org/abs/2507.13919", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "title": "The Levers of Political Persuasion with Conversational AI", "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.", "AI": {"tldr": "\u540e\u8bad\u7ec3\u548c\u63d0\u793a\u5de5\u7a0b\u663e\u8457\u63d0\u5347\u4e86AI\u7684\u8bf4\u670d\u529b\uff0c\u4f46\u4e5f\u964d\u4f4e\u4e86\u5176\u4e8b\u5b9e\u51c6\u786e\u6027\u3002AI\u5bf9\u4eba\u7c7b\u4fe1\u5ff5\u7684\u5f71\u54cd\u5e76\u4e0d\u4e3b\u8981\u6765\u81ea\u6a21\u578b\u89c4\u6a21\u6216\u4e2a\u6027\u5316\uff0c\u800c\u6e90\u81ea\u5176\u4fe1\u606f\u8c03\u914d\u80fd\u529b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u56de\u5e94\u793e\u4f1a\u4e0a\u5bf9\u4e8e\u5bf9\u8bdd\u578bAI\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578bLLMs\uff09\u53ef\u80fd\u5bf9\u4eba\u7c7b\u4fe1\u5ff5\u4ea7\u751f\u5f3a\u5927\u5f71\u54cd\u7684\u62c5\u5fe7\u3002\u5c24\u5176\u5173\u6ce8AI\u5728\u8bf4\u670d\u529b\u4e0a\u7684\u63d0\u5347\u662f\u5426\u6709\u73b0\u5b9e\u6839\u636e\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u63d0\u5347\u7684\u6765\u6e90\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u4e09\u9879\u5927\u89c4\u6a21\u5b9e\u9a8c\uff08\u6837\u672c\u91cf\u4e3a76,977\uff09\uff0c\u4f7f\u752819\u79cd\u4e0d\u540cLLM\uff0c\u5305\u62ec\u90e8\u5206\u4e13\u95e8\u4e3a\u4e86\u63d0\u5347\u201c\u8bf4\u670d\u529b\u201d\u8fdb\u884c\u4e86\u540e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u9488\u5bf9707\u4e2a\u653f\u6cbb\u8bae\u9898\u8fdb\u884c\u6d4b\u8bd5\u3002\u7edf\u8ba1\u6a21\u578b\u4ea7\u751f\u7684466,769\u4e2a\u4e3b\u5f20\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u540e\u8bad\u7ec3\u3001\u63d0\u793a\uff08prompting\uff09\u3001\u4e2a\u6027\u5316\u548c\u6a21\u578b\u89c4\u6a21\u5bf9\u8bf4\u670d\u529b\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u540e\u8bad\u7ec3\u4e0e\u63d0\u793a\u5de5\u7a0b\u63d0\u5347AI\u8bf4\u670d\u529b\u7684\u6548\u679c\u6bd4\u4e2a\u6027\u5316\u6216\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u66f4\u663e\u8457\uff0c\u5206\u522b\u63d0\u5347\u4e8651%\u548c27%\u3002\u4f46\u63d0\u5347\u8bf4\u670d\u529b\u7684\u540c\u65f6\uff0c\u4e8b\u5b9e\u51c6\u786e\u7387\u5374\u7cfb\u7edf\u6027\u4e0b\u964d\u3002AI\u8bf4\u670d\u529b\u7684\u63d0\u5347\u4e3b\u8981\u6e90\u4e8e\u5176\u5feb\u901f\u68c0\u7d22\u548c\u6218\u7565\u6027\u8fd0\u7528\u4fe1\u606f\u7684\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u53ca\u53ef\u9884\u89c1\u7684\u672a\u6765\uff0c\u5bf9\u8bdd\u578bAI\u7684\u8bf4\u670d\u529b\u4e3b\u8981\u53d7\u540e\u8bad\u7ec3\u548c\u63d0\u793a\u65b9\u6cd5\u5f71\u54cd\uff0c\u800c\u975e\u4e2a\u6027\u5316\u6216\u6a21\u578b\u89c4\u6a21\uff0c\u4e14\u63d0\u5347\u8bf4\u670d\u529b\u7684\u540c\u65f6\uff0c\u4e8b\u5b9e\u51c6\u786e\u7387\u4f1a\u88ab\u524a\u5f31\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9AI\u793e\u4f1a\u5f71\u54cd\u7684\u62c5\u5fe7\u8fdb\u884c\u4e86\u5b9e\u8bc1\u68c0\u9a8c\u3002"}}
{"id": "2507.13937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13937", "abs": "https://arxiv.org/abs/2507.13937", "authors": ["Jan Trienes", "Anastasiia Derzhanskaia", "Roland Schwarzkopf", "Markus M\u00fchling", "J\u00f6rg Schl\u00f6tterer", "Christin Seifert"], "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support", "comment": null, "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u9662\u6821\u62db\u751f\u54a8\u8be2\u7684\u5bf9\u8bdd\u7cfb\u7edfMarcel\uff0c\u7ed3\u5408FAQ\u68c0\u7d22\u663e\u8457\u63d0\u5347\u4e86\u7b54\u590d\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\uff0c\u5728\u7528\u6237\u5b9e\u9645\u4f7f\u7528\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u62db\u751f\u54a8\u8be2\u6548\u7387\uff0c\u51cf\u8f7b\u6821\u65b9\u5de5\u4f5c\u4eba\u5458\u8d1f\u62c5\uff0c\u5e76\u4e3a\u5b66\u751f\u63d0\u4f9b\u51c6\u786e\u3001\u4e2a\u6027\u5316\u7684\u5165\u5b66\u76f8\u5173\u7b54\u590d\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08retrieval-augmented generation\uff09\u7ed3\u5408FAQ\u68c0\u7d22\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u95ee\u9898\u6620\u5c04\u5230\u77e5\u8bc6\u5e93\u6761\u76ee\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u5bf9\u7cfb\u7edf\u67b6\u6784\u4e0e\u5404\u7ec4\u4ef6\u8fdb\u884c\u6280\u672f\u8bc4\u6d4b\u548c\u5b9e\u9645\u90e8\u7f72\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684FAQ\u68c0\u7d22\u65b9\u6cd5\u4f18\u4e8e\u5e38\u89c4\u7a20\u5bc6\u6216\u6df7\u5408\u68c0\u7d22\uff0c\u7cfb\u7edf\u5728\u5b9e\u9645\u9662\u6821\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u6613\u4e8e\u5728\u8d44\u6e90\u6709\u9650\u7684\u5b66\u672f\u73af\u5883\u90e8\u7f72\u3002", "conclusion": "Marcel\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u652f\u6301\u5b66\u751f\u62db\u751f\u76f8\u5173\u54a8\u8be2\uff0c\u51cf\u5c11\u5b66\u6821\u5de5\u4f5c\u4eba\u5458\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u548c\u53ef\u9a8c\u8bc1\u7684\u4fe1\u606f\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u5b66\u6821\u90e8\u7f72\u3002"}}
{"id": "2507.13949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13949", "abs": "https://arxiv.org/abs/2507.13949", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "title": "Exploiting Primacy Effect To Improve Large Language Models", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5fae\u8c03\u540e\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u591a\u9879\u9009\u62e9\u9898\u65f6\u9996\u56e0\u504f\u7f6e\u589e\u5f3a\uff0c\u4f5c\u8005\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u6392\u9009\u9879\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u504f\u89c1\u65e2\u662f\u6311\u6218\u4e5f\u662f\u673a\u9047\uff0c\u5bf9\u672a\u6765NLP\u6a21\u578b\u8bbe\u8ba1\u5177\u6709\u542f\u793a\u610f\u4e49\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9879\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u5904\u7406\u591a\u9879\u9009\u62e9\u9898\u65f6\u53ef\u80fd\u53d7\u5230\u7c7b\u4eba\u4f4d\u7f6e\u504f\u7f6e\uff08\u5982\u9996\u56e0\u6548\u5e94\uff09\u7684\u5f71\u54cd\uff0c\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002\u4f5c\u8005\u6709\u610f\u7814\u7a76LLMs\u4e2d\u9996\u56e0\u504f\u7f6e\u7684\u5f71\u54cd\u53ca\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5fae\u8c03\u540eLLM\u7684\u9996\u56e0\u504f\u7f6e\u88ab\u589e\u5f3a\uff0c\u7136\u540e\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e0e\u67e5\u8be2\u8bed\u4e49\u76f8\u4f3c\u6027\u5bf9\u9009\u9879\u91cd\u6392\u5e8f\u7684\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u8be5\u504f\u7f6e\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u77e5\u9053\u6b63\u786e\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u8fdb\u884c\u9009\u9879\u91cd\u6392\u5e8f\u7684\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u591a\u9879\u9009\u62e9\u95ee\u7b54\uff08MCQA\uff09\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u9996\u56e0\u504f\u7f6e\u867d\u7136\u662f\u4e00\u79cd\u504f\u89c1\uff0c\u4f46\u82e5\u52a0\u4ee5\u5408\u7406\u5229\u7528\uff0c\u4e5f\u80fd\u6210\u4e3a\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u7b56\u7565\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u504f\u89c1\u611f\u77e5\u7684\u6a21\u578b\u8bbe\u8ba1\u548cNLP\u5e94\u7528\u5e26\u6765\u542f\u53d1\u3002"}}
{"id": "2507.13966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13966", "abs": "https://arxiv.org/abs/2507.13966", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u539f\u8bed\u5408\u6210\u4efb\u52a1\uff0c\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9886\u57df\u63a8\u7406\u5f3a\u80fd\u529b\u8bad\u7ec3\u3002\u4ee5\u533b\u5b66\u4e3a\u4f8b\uff0c\u8bad\u7ec3\u5f97\u51fa\u7684QwQ-Med-3\u6a21\u578b\u5728\u591a\u4e2a\u533b\u5b66\u63a8\u7406\u8bc4\u6d4b\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5e95\u5c42\u77e5\u8bc6\u539f\u8bed\u7ec4\u5408\u5f0f\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4ee5\u5f80\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u8f83\u5f3a\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u5185\u7f3a\u4e4f\u6df1\u5ea6\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u662f\u56e0\u4e3a\u8bad\u7ec3\u901a\u5e38\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u3001\u6cdb\u5316\u7684\u5927\u89c4\u6a21\u8bed\u6599\uff0c\u800c\u672a\u80fd\u9488\u5bf9\u9886\u57df\u77e5\u8bc6\u7684\u6784\u6210\u6027\u8fdb\u884c\u4f18\u5316\u3002\u6587\u7ae0\u8bd5\u56fe\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\uff0c\u8bf1\u5bfc\u6a21\u578b\u5b66\u4e60\u548c\u7ec4\u5408\u9886\u57df\u5185\u7b80\u5355\u77e5\u8bc6\u539f\u8bed\uff0c\u63d0\u5347\u4e13\u7528\u9886\u57df\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4ee5\u77e5\u8bc6\u56fe\u8c31\uff08Knowledge Graph, KG\uff09\u4e3a\u57fa\u7840\uff0c\u81ea\u52a8\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u7528\u7531KG\u539f\u8bed\u7ec4\u6210\u7684\u3001\u53ef\u8ffd\u8e2a\u63a8\u7406\u8def\u5f84\u7684\u4efb\u52a1\u96c6\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u5728\u533b\u5b66\u9886\u57df\uff0c\u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u6784\u5efa\u4e8624000\u4e2a\u63a8\u7406\u4efb\u52a1\u53ca\u601d\u8003\u8f68\u8ff9\uff0c\u5e76\u636e\u6b64\u5c06QwQ-32B\u6a21\u578b\u5fae\u8c03\u4e3aQwQ-Med-3\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63d0\u51faICD-Bench\u8bc4\u6d4b\u96c6\uff0c\u91cf\u5316\u6a21\u578b\u572815\u4e2a\u533b\u5b66\u5b50\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "QwQ-Med-3\u6a21\u578b\u5728ICD-Bench\u5404\u5206\u7c7b\u4e0b\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u5c55\u73b0\u8f83\u5927\u4f18\u52bf\u3002\u540c\u65f6\uff0c\u5728\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cQwQ-Med-3\u80fd\u591f\u6709\u6548\u5c06\u65b0\u83b7\u5f97\u7684\u4e13\u4e1a\u77e5\u8bc6\u8fc1\u79fb\u5e76\u63d0\u5347\u57fa\u7840\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u539f\u8bed\u7684\u81ea\u52a8\u4efb\u52a1\u751f\u6210\u548c\u5fae\u8c03\u65b9\u6cd5\u80fd\u591f\u6781\u5927\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u63a8\u7406\u548c\u7ec4\u5408\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u5177\u6709\u6548\u7387\u7684\u3001\u53ef\u7ec4\u5408\u7684\u9886\u57df\u8d85\u7ea7\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u8df5\u4f9d\u636e\uff0c\u5bf9\u672a\u6765\u6784\u5efa\u9886\u57df\u4e13\u7528\u4eba\u5de5\u667a\u80fd\u7684\u8def\u7ebf\u5177\u6709\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2507.13977", "categories": ["cs.CL", "eess.AS", "I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13977", "abs": "https://arxiv.org/abs/2507.13977", "authors": ["Lilit Grigoryan", "Nikolay Karpov", "Enas Albasiri", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic", "comment": "Accepted to ICASSP 2025", "summary": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u7528\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u4e0e\u6587\u672c\u5904\u7406\u65b9\u6cd5\uff0c\u57fa\u4e8eFastConformer\u67b6\u6784\u4e3a\u6807\u51c6\u548c\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\u8bad\u7ec3\u4e86\u9886\u5148\u7684ASR\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f00\u6e90\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u4f5c\u4e3a\u4e16\u754c\u4e0a\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u8bed\u8a00\u4e4b\u4e00\uff0c\u5176\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u5f00\u53d1\u56e0\u8bed\u8a00\u7684\u590d\u6742\u6027\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u76ee\u524d\u516c\u5f00\u7684\u963f\u62c9\u4f2f\u8bedASR\u6a21\u578b\u6570\u91cf\u6709\u9650\uff0c\u4e14\u591a\u805a\u7126\u4e8e\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff08MSA\uff09\uff0c\u5bf9\u8bed\u8a00\u53d8\u4f53\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u4e0e\u6587\u672c\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8eFastConformer\u67b6\u6784\u8bad\u7ec3\u4e86\u4e24\u4e2a\u65b0\u6a21\u578b\uff1a\u4e00\u4e2a\u4e13\u4e3aMSA\u8bbe\u8ba1\uff0c\u53e6\u4e00\u4e2a\u4e3a\u9996\u4e2a\u540c\u65f6\u9002\u7528\u4e8eMSA\u548c\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\uff08CA\uff09\u7684\u7edf\u4e00\u516c\u5f00\u6a21\u578b\u3002", "result": "MSA\u6a21\u578b\u5728\u76f8\u5173\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u6027\u80fd\u57fa\u51c6\uff1b\u7edf\u4e00\u6a21\u578b\u5728\u5e26\u5143\u97f3\u7b26\u53f7\uff08diacritics\uff09\u7684CA\u4e0a\u53d6\u5f97\u4e86SOTA\u51c6\u786e\u7387\uff0c\u4e14\u5bf9MSA\u4f9d\u7136\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "\u8be5\u7814\u7a76\u7a81\u7834\u4e86\u963f\u62c9\u4f2f\u8bedASR\u5efa\u6a21\u7684\u5c40\u9650\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u652f\u6301MSA\u548cCA\u7684\u7edf\u4e00\u516c\u7528\u6a21\u578b\uff0c\u5e76\u5f00\u6e90\u4e86\u6240\u6709\u6a21\u578b\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u5e76\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2507.14017", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14017", "abs": "https://arxiv.org/abs/2507.14017", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models", "comment": null, "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.", "AI": {"tldr": "RHYTHM\u901a\u8fc7\u8f68\u8ff9\u5c42\u6b21token\u5316\u4e0e\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\u5efa\u6a21\u5728\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u5904\u7406\u957f\u5e8f\u5217\u65f6\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0c\u800c\u4e14\u96be\u4ee5\u6709\u6548\u5730\u5efa\u6a21\u65e5\u5e38\u4e0e\u5468\u5ea6\u7684\u5c42\u6b21\u4f9d\u8d56\u3002", "method": "\u63d0\u51faRHYTHM\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u8f68\u8ff9\u5206\u5272\u4e3a\u65e5\u5e38\u7247\u6bb5\uff0c\u8f6c\u6362\u4e3a\u79bb\u6563token\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u65e5\u5e38\u4e0e\u5468\u5ea6\u4f9d\u8d56\u3002token\u8fd8\u7ed3\u5408\u9884\u8ba1\u7b97\u7684\u5927\u8bed\u8a00\u6a21\u578bembedding\u4ee5\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u4f53\u88ab\u51bb\u7ed3\u4ee5\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u53472.4%\uff0c\u5468\u672b\u63d0\u53475.0%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1124.6%\u3002", "conclusion": "RHYTHM\u5728\u4fdd\u8bc1\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14022", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14022", "abs": "https://arxiv.org/abs/2507.14022", "authors": ["Jianfei Li", "Kevin Kam Fung Yuen"], "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis", "comment": "35 pages, 33 tables, 6 Figures", "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u591a\u6307\u6807\u6743\u91cd\u7684\u5206\u7c7b\u6a21\u578b\u4f18\u9009\u6846\u67b6\uff08CPC-CMS\uff09\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u6587\u672c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u5728\u6587\u672c\u60c5\u611f\u5206\u6790\u9886\u57df\uff0c\u9009\u62e9\u6700\u4f73\u5206\u7c7b\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u8bc4\u4ef7\u6307\u6807\uff0c\u5ffd\u7565\u4e86\u591a\u6307\u6807\u6743\u91cd\u53ca\u590d\u6742\u5b9e\u9645\u9700\u6c42\u3002\u4e3a\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u591a\u6307\u6807\u7efc\u5408\u5206\u6790\u7684\u6a21\u578b\u9009\u62e9\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86CPC-CMS\uff08\u57fa\u4e8e\u8ba4\u77e5\u6210\u5bf9\u6bd4\u8f83\u5206\u7c7b\u6a21\u578b\u9009\u62e9\uff09\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u5bf9\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u3001\u7279\u5f02\u6027\u3001MCC\u3001Kappa\u548c\u6548\u7387\u7b49\u591a\u9879\u6307\u6807\u8fdb\u884c\u52a0\u6743\uff1b\u5c06\u591a\u4e2a\u4e3b\u6d41\u5206\u7c7b\u6a21\u578b\u7684\u8bc4\u4f30\u5f97\u5206\u548c\u6743\u91cd\u5f62\u6210\u52a0\u6743\u51b3\u7b56\u77e9\u9635\uff0c\u8fdb\u884c\u591a\u7ef4\u5ea6\u6a21\u578b\u4f18\u9009\u3002", "result": "\u5728\u4e09\u4e2a\u793e\u4ea4\u5a92\u4f53\u5f00\u653e\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u53bb\u9664\u65f6\u95f4\u6d88\u8017\u65f6\uff0cALBERT\u6a21\u578b\u6548\u679c\u6700\u597d\uff1b\u8003\u8651\u65f6\u95f4\u6d88\u8017\u540e\uff0c\u4e0d\u540c\u6570\u636e\u96c6\u6700\u4f73\u6a21\u578b\u4e0d\u540c\u3002\u6846\u67b6\u53ef\u9002\u7528\u4e8e\u5176\u4ed6\u5206\u7c7b\u9886\u57df\u3002", "conclusion": "CPC-CMS\u80fd\u7efc\u5408\u591a\u8bc4\u4ef7\u6307\u6807\u548c\u6743\u91cd\uff0c\u66f4\u79d1\u5b66\u5730\u9009\u62e9\u6587\u672c\u60c5\u611f\u5206\u6790\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u5177\u6709\u901a\u7528\u6027\u3002"}}
{"id": "2507.14045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14045", "abs": "https://arxiv.org/abs/2507.14045", "authors": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "Jimmy Huang"], "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks", "comment": "Accepted at Canadian AI 2025", "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u4e2a\u4e3b\u6d41\u95ed\u6e90\u4e0e\u5f00\u6e90\u751f\u7269\u533b\u5b66LLM\u5728\u6587\u672c\u548c\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6ca1\u6709\u4e07\u80fd\u6a21\u578b\uff0c\u5f00\u6e90\u6a21\u578b\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5e94\u7528\u9009\u62e9\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u5f53\u524d\u751f\u7269\u533b\u5b66\u9886\u57df\u9700\u8981\u6709\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5404\u7c7b\u6587\u672c\u4e0e\u56fe\u50cf\u4efb\u52a1\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5404\u79cd\u4e3b\u6d41\u6a21\u578b\u5728\u5177\u4f53\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u9002\u7528\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u95ed\u6e90\u548c\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6db5\u76d6\u751f\u7269\u533b\u5b66\u6587\u672c\u5206\u7c7b\u3001\u751f\u6210\u3001\u95ee\u7b54\u548c\u591a\u6a21\u6001\u56fe\u50cf\u5904\u7406\u7b49\u4efb\u52a1\u3002\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4e0d\u540cLLM\u5728\u5404\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u591a\u79cd\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u65e0\u4e00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u5747\u4e3a\u6700\u4f73\u3002\u5f00\u6e90\u6a21\u578b\u6709\u65f6\u4f18\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u989d\u5916\u597d\u5904\uff0c\u5982\u901f\u5ea6\u548c\u9690\u79c1\u3002", "conclusion": "\u6ca1\u6709\u4efb\u4f55\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u5728\u6240\u6709\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u6700\u4f73\u3002\u4e0d\u540c\u6a21\u578b\u4f9d\u8d56\u5177\u4f53\u4efb\u52a1\uff0c\u5404\u6709\u4f18\u52bf\u3002\u5f00\u6e90LLM\u5728\u5f88\u591a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e0e\u95ed\u6e90LLM\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u8868\u73b0\uff0c\u5e76\u5177\u5907\u66f4\u5feb\u63a8\u7406\u548c\u66f4\u597d\u9690\u79c1\u7b49\u4f18\u52bf\u3002"}}
{"id": "2507.14063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14063", "abs": "https://arxiv.org/abs/2507.14063", "authors": ["Lautaro Estienne", "Gabriel Ben Zenou", "Nona Naderi", "Jackie Cheung", "Pablo Piantanida"], "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog", "comment": null, "summary": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CRSA\u6a21\u578b\uff0c\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u62d3\u5c55RSA\u6846\u67b6\uff0c\u4f7fAI\u5728\u591a\u8f6e\u534f\u4f5c\u5bf9\u8bdd\u4e2d\u8fbe\u6210\u66f4\u597d\u7684\u5408\u4f5c\u548c\u7406\u89e3\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RSA\u53ca\u5176\u6269\u5c55\u5728\u591a\u8f6e\u3001\u534f\u4f5c\u8bed\u5883\u4e0b\u96be\u4ee5\u6269\u5c55\uff0c\u65e0\u6cd5\u6ee1\u8db3AI\u534f\u4f5c\u65f6\u9700\u5bf9\u5171\u4eab\u76ee\u6807\u548c\u4fe1\u5ff5\u63a8\u7406\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u5bf9\u8bdd\u63a8\u7406\u6a21\u578b\u3002", "method": "\u63d0\u51faCRSA\uff08Collaborative Rational Speech Act\uff09\u6a21\u578b\uff0c\u7528\u4fe1\u606f\u7406\u8bba\uff08\u7387\u5931\u771f\u7406\u8bba\uff09\u4e2d\u7684gain\u51fd\u6570\u6269\u5c55RSA\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u8f6e\u3001\u534f\u4f5c\u573a\u666f\u3002\u8bbe\u8ba1\u4e86referential games\u548c\u533b\u7597\u9886\u57df\u7684\u6a21\u677f\u5316\u533b\u60a3\u5bf9\u8bdd\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u7ecf\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u8bc1\u660e\uff0cCRSA\u5728referential games\u548c\u533b\u7597\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u4e00\u81f4\u3001\u53ef\u89e3\u91ca\u548c\u534f\u4f5c\u6027\u7684\u5bf9\u8bdd\u884c\u4e3a\u3002", "conclusion": "CRSA\u6a21\u578b\u80fd\u8ba9AI\u5728\u591a\u8f6e\u534f\u4f5c\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u66f4\u4e00\u81f4\u3001\u53ef\u89e3\u91ca\u548c\u5408\u4f5c\u7684\u884c\u4e3a\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u5177\u793e\u4f1a\u610f\u8bc6\u548c\u5b9e\u7528\u6027\u7684\u8bed\u8a00\u667a\u80fd\u4f53\u53d1\u5c55\u3002"}}
{"id": "2507.14079", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14079", "abs": "https://arxiv.org/abs/2507.14079", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.", "AI": {"tldr": "DENSE\u7cfb\u7edf\u7528\u73b0\u6709\u5206\u6563\u75c5\u5386\u7b14\u8bb0\u9a71\u52a8\u5927\u6a21\u578b\uff0c\u4e0e\u4e34\u5e8a\u6587\u6863\u903b\u8f91\u5bf9\u9f50\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7eb5\u5411\u8fdb\u7a0b\u8bb0\u5f55\uff0c\u63d0\u5347\u533b\u7597\u6570\u636e\u96c6\u7684\u53d9\u8ff0\u5b8c\u6574\u6027\u548c\u4e0b\u6e38\u6548\u80fd\u3002", "motivation": "\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\uff0c\u8fdb\u7a0b\u8bb0\u5f55\uff08progress notes\uff09\u5173\u952e\u4f46\u7f55\u89c1\uff0c\u91cd\u8981\u7684\u7eb5\u5411\u75c5\u4eba\u4fe1\u606f\u56e0\u6b64\u7f3a\u5931\u3002\u73b0\u6709\u5927\u578bEHR\u6570\u636e\u96c6\u5982MIMIC-III\u4e2d\uff0c\u8fdb\u7a0b\u8bb0\u5f55\u8986\u76d6\u7387\u975e\u5e38\u4f4e\uff0c\u5f71\u54cd\u4e34\u5e8a\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faDENSE\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7b14\u8bb0\u5206\u7c7b\u548c\u65f6\u95f4\u5bf9\u9f50\u673a\u5236\uff0c\u6574\u5408\u540c\u4e00\u75c5\u4eba\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u591a\u79cd\u75c5\u5386\u8bb0\u5f55\u3002\u7cfb\u7edf\u91c7\u7528\u68c0\u7d22\u7b56\u7565\uff0c\u6536\u96c6\u5f53\u524d\u548c\u65e2\u5f80\u75c5\u7a0b\u76f8\u5173\u5185\u5bb9\uff0c\u9a71\u52a8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4e34\u5e8a\u8fde\u8d2f\u4e14\u6709\u65f6\u5e8f\u611f\u77e5\u7684\u8fdb\u7a0b\u8bb0\u5f55\u3002", "result": "\u5728\u5305\u542b\u5b8c\u6574\u8fdb\u7a0b\u8bb0\u5f55\u7684\u591a\u6b21\u5c31\u8bca\u60a3\u8005\u4e0a\u8bc4\u4f30DENSE\uff0c\u751f\u6210\u7684\u7b14\u8bb0\u5b9e\u73b0\u4e86\u4f18\u4e8e\u539f\u59cb\u7b14\u8bb0\u7684\u7eb5\u5411\u4e00\u81f4\u6027\uff0c\u65f6\u95f4\u5bf9\u9f50\u6bd4\u8fbe\u52301.089\u3002\u540c\u65f6\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u5982\u6458\u8981\u3001\u9884\u6d4b\u5efa\u6a21\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u8868\u73b0\u3002", "conclusion": "DENSE\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u73b0\u6709\u5f02\u6784\u75c5\u5386\u7b14\u8bb0\uff0c\u91cd\u5efa\u9ad8\u8d28\u91cf\u3001\u65f6\u5e8f\u4e00\u81f4\u7684\u8fdb\u7a0b\u8bb0\u5f55\uff0c\u63d0\u9ad8\u4e86\u5927\u6a21\u578b\u7b14\u8bb0\u5408\u6210\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u533b\u7597\u573a\u666f\u4e2d\u7684\u6570\u636e\u5229\u7528\u4e0e\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.14096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14096", "abs": "https://arxiv.org/abs/2507.14096", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "AI": {"tldr": "PLABA\u8d5b\u9053\u9a8c\u8bc1\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u5c06\u751f\u7269\u533b\u5b66\u6587\u732e\u901a\u4fd7\u5316\uff0c\u4f46\u5728\u8868\u8fbe\u7b80\u7ec3\u548c\u81ea\u52a8\u8bc4\u4ef7\u7b49\u65b9\u9762\u8fd8\u6709\u4e0d\u8db3\uff0c\u9700\u7ee7\u7eed\u6539\u8fdb\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u6709\u671b\u5c06\u4e13\u4e1a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u666e\u53ca\u7ed9\u5927\u4f17\uff0c\u4f46\u5176\u8f93\u51fa\u4e0d\u53ef\u9884\u6d4b\uff0c\u9519\u8bef\u98ce\u9669\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u8bc4\u4f30\u5176\u5b9e\u9645\u80fd\u529b\uff0c\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u5e76\u83b7\u5f97\u9ad8\u8d28\u91cf\u7cfb\u7edf\u8bc4\u4ef7\u3002", "method": "\u901a\u8fc7\u57282023\u548c2024\u5e74\u6587\u672c\u68c0\u7d22\u4f1a\u8bae\u4e3e\u529ePLABA\u8d5b\u9053\uff0c\u8bbe\u7f6e\u4e24\u4e2a\u4efb\u52a1\uff1a1\uff09\u5bf9\u751f\u7269\u533b\u5b66\u6587\u6458\u8fdb\u884c\u5b8c\u6574\u7684\u3001\u9010\u53e5\u7684\u901a\u4fd7\u6539\u5199\uff1b2\uff09\u8bc6\u522b\u5e76\u66ff\u6362\u6587\u4e2d\u7684\u96be\u61c2\u672f\u8bed\u3002\u4efb\u52a11\u7684\u81ea\u52a8\u8bc4\u6d4b\u4f9d\u6258\u56db\u4efd\u4e13\u4e1a\u6539\u5199\u53c2\u7167\uff0c\u4efb\u52a11\u4e0e2\u5747\u7531\u751f\u7269\u533b\u5b66\u4e13\u5bb6\u8fdb\u884c\u4eba\u5de5\u8bc4\u4ef7\u3002", "result": "12\u652f\u961f\u4f0d\u53c2\u52a0\uff0c\u65b9\u6cd5\u6db5\u76d6\u591a\u5c42\u611f\u77e5\u5668\u5230\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u4efb\u52a11\u4e2d\uff0c\u6210\u7ee9\u6700\u4f18\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u4e0a\u63a5\u8fd1\u4eba\u5de5\u6c34\u5e73\uff0c\u4f46\u5728\u8868\u8fbe\u7b80\u7ec3\u548c\u6613\u61c2\u6027\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u5b9a\u4e00\u81f4\u6027\u8f83\u5dee\u3002\u4efb\u52a12\u4e2d\uff0c\u7cfb\u7edf\u5728\u672f\u8bed\u96be\u70b9\u8bc6\u522b\u548c\u66ff\u6362\u5206\u7c7b\u4e0a\u8868\u73b0\u8f83\u5f31\uff0c\u4f46\u751f\u6210\u66ff\u4ee3\u8868\u8fbe\u65f6\uff0c\u57fa\u4e8eLLM\u7cfb\u7edf\u5728\u4eba\u5de5\u8bc4\u4ef7\u4e2d\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u4e0e\u6613\u61c2\u6027\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7b80\u6d01\u6027\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "PLABA\u8d5b\u9053\u5c55\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u751f\u7269\u533b\u5b66\u6587\u732e\u8f6c\u5316\u4e3a\u901a\u4fd7\u8bed\u8a00\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u66b4\u9732\u4e86\u5176\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7b80\u7ec3\u8868\u8fbe\u53ca\u672f\u8bed\u96be\u5ea6\u8bc6\u522b\u7b49\u65b9\u9762\u3002\u6b64\u5916\uff0c\u76f8\u5173\u81ea\u52a8\u8bc4\u4ef7\u65b9\u6cd5\u4e0e\u4eba\u5de5\u8bc4\u5ba1\u7ed3\u679c\u76f8\u5173\u6027\u8f83\u5dee\uff0c\u8868\u660e\u9700\u8981\u6539\u8fdb\u81ea\u52a8\u8bc4\u6d4b\u5de5\u5177\u3002"}}
