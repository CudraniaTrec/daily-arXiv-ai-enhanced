<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LO](#cs.LO) [Total: 7]
- [cs.CL](#cs.CL) [Total: 17]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness](https://arxiv.org/abs/2510.07582)
*Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 本文重新审视编程中纯与副作用的区分，提出一种新的纯度语义和可表达性度量，展示现有系统互补关系，并给出融合式解决方案和形式化工具简化证明流程。


<details>
  <summary>Details</summary>
Motivation: 现有的编程模型中，如何区分纯计算和带有副作用的计算一直是重要的问题。各种方法（如monad、类型与效果系统、能力系统）各有优缺点，在精确性和易用性之间存在权衡，尚无完美解决方案。

Method: 作者提出了一种基于上下文等价的纯度语义定义，这一定义独立于具体类型系统。同时，提出以‘纯度可表达性的完备度’来定量评估不同系统的表达能力。通过对典型最小化的效果与能力系统进行分析，证明了两者在表达能力上不可相互取代，并提出将类型、能力、效果系统结合以扬长避短。作者还构建了形式化模型，并提供逻辑关系以简化纯度等性质的证明。

Result: 分析表明，现有的最小化效果系统和能力系统在表达纯度时互不覆盖。通过融合三者（类型、能力、效果系统），可以达到兼具优势、回避各自弱点的效果。此外，形式化模型和逻辑关系为后续证明提供了便捷工具。

Conclusion: 主流纯与副作用建模方法在纯度表达上各有所长并互不替代，通过综合可兼容各方优点。作者为定量评估表达力和纯度验证提供了新工具。

Abstract: Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.

</details>


### [2] [The Functional Machine Calculus III: Control](https://arxiv.org/abs/2510.07851)
*Willem Heijltjes*

Main category: cs.PL

TL;DR: 该文提出了一种可同时表达函数式和命令式编程要素的新理论模型，并通过简单类型和可归约保证运算安全终止，首次在统一理论下完整表达了复杂控制流和副作用。


<details>
  <summary>Details</summary>
Motivation: 现有理论难以完整统一函数式与命令式两大编程范式，尤其在表达副作用、控制流与复杂计算模型时存在不足。

Method: 在lambda演算的基础上扩展操作语义，利用多操作数栈和continuation栈来支撑全局存储、输入输出、概率性、非确定性及分支、循环等效果，定义了可归约、简单类型保障终止的系统。

Result: 实现在单一的函数演算框架中，忠实嵌入最小完备的命令式语言（含条件、异常、迭代等），并保证可归约、强规范化性质，形成简单类型保障终止的运算模型。

Conclusion: 该研究提出了一种统一函数式和命令式编程范式的新理论基础——Functional Machine Calculus，并成功将条件语句、异常处理和迭代等命令式编程要素纳入函数式框架，保持了可归约性与类型终止等关键性质。

Abstract: The Functional Machine Calculus (Heijltjes 2022) is a new approach to
unifying the imperative and functional programming paradigms. It extends the
lambda-calculus, preserving the key features of confluent reduction and typed
termination, to embed computational effects, evaluation strategies, and control
flow operations. The first instalment modelled sequential higher-order
computation with global store, input/output, probabilities, and
non-determinism, and embedded both the call-by-name and call-by-value
lambda-calculus, as well as Moggi's computational metalanguage and Levy's
call-by-push-value. The present paper extends the calculus from sequential to
branching and looping control flow. This allows the faithful embedding of a
minimal but complete imperative language, including conditionals, exception
handling, and iteration, as well as constants and algebraic data types.
  The calculus is defined through a simple operational semantics, extending the
(simplified) Krivine machine for the lambda-calculus with multiple operand
stacks to model effects and a continuation stack to model sequential,
branching, and looping computation. It features a confluent reduction relation
and a system of simple types that guarantees termination of the machine and
strong normalization of reduction (in the absence of iteration). These
properties carry over to the embedded imperative language, providing a unified
functional-imperative model of computation that supports simple types, a direct
and intuitive operational semantics, and a confluent reduction semantics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: 本研究基于442名开发者的数据，利用JD-R模型与混合研究方法，发现GenAI提升倦怠风险，但足够资源和积极看法能有效缓解影响。


<details>
  <summary>Details</summary>
Motivation: 虽然GenAI为软件开发带来生产力提升，但也可能引入新的压力危及开发者幸福感，因此亟需理解GenAI采纳对开发者倦怠感的影响及机制。

Method: 采用并行嵌入式混合方法研究（concurrent embedded mixed-methods），包括对442名开发者的大规模问卷调查（定量）以及对开放式问答数据的定性分析。使用偏最小二乘法结构方程模型（PLS-SEM）和回归方法进行关系建模。

Result: GenAI的采用增加了开发者的工作压力，从而提升了倦怠风险。不过，工作资源和正面认知可显著缓和该负面效应。

Conclusion: GenAI的采用会通过增加工作压力（job demands）提高开发者的倦怠感，但工作资源（job resources）和对GenAI的积极看法能够缓解这种倦怠，甚至将GenAI的采用转化为机遇。

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [4] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: 本文提出了第一个专注于紧急修复(hot fix)的真实世界开源数据集HotBugs.jar，从大型Apache项目系统性挖掘并人工验证出679个hot fix，其中110个可复现，为相关工具和研究提供评测基准，推动自动化修复和软件弹性领域发展。


<details>
  <summary>Details</summary>
Motivation: 以往关于紧急修复(hot fix)的研究和工具缺乏专门的、具有代表性的评测基准，限制了相关自动化工具和技术的发展。为了解决这个问题，作者动机是创建一个专注于hot fix的高质量数据集，用于支持今后相关研究和工具评测。

Method: 作者从10个活跃的Apache项目中，挖掘了超过19万次提交和15万条问题报告，筛选出746个符合hot-fix标准的软件补丁，通过人工评估确认了679个真实的hot fix，并精细整理其中的110个可复现案例如测试集、元数据等。整体方法基于Bugs.jar框架，结合Jira数据分析、人工评审与系统性的数据封装流程。

Result: 最终，生成了HotBugs.jar数据集，包含了679个经过人工确认的hot fix，及其详细元数据，其中110个可以自动通过测试套件复现。该数据集被SBSE大会官方采纳为挑战赛专用基准，表明其已产生实际影响。

Conclusion: HotBugs.jar作为第一个面向真实世界hot fix的数据集，为快速调试和自动化修复工具的研究、评价和发展提供了基础性资源，有助于推动软件生产环境下的自动化修复与弹性研究。

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [5] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure 利用大模型和差分符号测试方法，自动将 C 代码安全地转为 Rust，可编译率接近 90%，语义保持率约 70%，大幅简化和提升不安全代码的安全迁移效率。


<details>
  <summary>Details</summary>
Motivation: Rust 是一种内存安全的编程语言，能大幅提高软件安全性。想要利用 Rust 的安全性，现有用不安全语言（如 C）编写的代码必须先被转译为 Rust。该论文旨在解决 C 代码自动转为安全的 Rust 代码的问题。

Method: 作者提出了 RustAssure 系统，使用大语言模型（LLM）进行自动转译，通过提示工程提高生成的 Rust 代码的规范性和安全性。同时，采用差分符号测试来对比原 C 代码和转译后 Rust 代码的语义一致性，以弥补传统测试方法遗漏的细微 Bug。

Result: 在五个真实应用和库上的实验表明，RustAssure 能为 89.8% 的 C 函数生成可编译的 Rust 函数，其中 69.9% 的函数在符号返回值上与原 C 函数等价。

Conclusion: RustAssure 能较高效且自动化地将 C 代码转为安全、可用的 Rust 代码，并有效保证语义一致性。该方法为老旧系统的安全迁移提供了新思路。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [6] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: 作者提出了应用级软件开发基准APPFORGE，并在此基准下测试主流LLM，结果显示即使是最先进的模型在复杂系统开发上仍有巨大不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs已在函数级代码生成上取得突破，但实际开发需求远超单一函数，涉及系统层级的协同与生命周期管理，目前缺乏有效评测LLMs软件工程能力的基准。

Method: 提出了APPFORGE基准，从真实世界的Android应用抽取101个软件开发问题，利用多智能体系统自动总结主要功能、生成测试用例并进行自动化评估。结合专家手动验证，构建可重复的评测框架。

Result: 在12个旗舰LLM上的评测结果显示，所有模型在开发完整Android应用时效果很差，最佳（GPT-5）仅有18.8%的功能实现正确，凸显当前LLMs在复杂软件工程任务中的不足。

Conclusion: 当前的大型语言模型（LLMs）在整体软件系统开发上存在显著局限性，很难构建出功能完全正确的应用程序，即使是最先进的模型也表现不佳。

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [7] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: 作者提出FLEX框架，通过神经网络自动生成测试程序、加入多样性和反馈机制，极大提升了MLIR编译器的缺陷挖掘能力，效果远超现有主流模糊测试工具。


<details>
  <summary>Details</summary>
Motivation: 当前用于测试MLIR编译器的模糊测试方法，存在生成用例的多样性和语义有效性不足的问题，难以暴露MLIR在复杂代码空间中的深层次缺陷。因此亟需更智能、更自适应的测试方法。

Method: 提出了一种名为FLEX的新型自适应自动化模糊测试框架。FLEX采用神经网络进行程序生成，利用扰动采样策略提高生成多样性，并加入反馈驱动的增强环，通过对崩溃和非崩溃用例的反馈，迭代提升生成模型的有效性和广度。

Result: 在MLIR编译器上的实证显示，FLEX在30天内发现了80个新漏洞，包括多个新根因及语法解析器缺陷；在24小时定点测试中，对比四种主流模糊测试工具，FLEX发现53个缺陷，是最好基线的3.5倍，代码覆盖率达28.2%，比次优工具高42%。消融实验证明，扰动生成和多样性增强对效果至关重要。

Conclusion: FLEX框架通过自适应生成、模型反馈、语法和语义学习，大幅提升了MLIR编译器的漏洞发现能力和代码覆盖率，远超现有主流工具，能有效支持大规模、深入的自动化测试。

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [8] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 通过自动化从编译器历史bug报告中挖掘变异器，IssueMut显著提高了fuzzer的新bug发现率，共发现65个fuzzer未能检测到的新bug，证实bug历史蕴含了提升fuzzer能力的重要信息。


<details>
  <summary>Details</summary>
Motivation: 现有编译器fuzzer效果依赖于变异器的质量，但此前并未利用编译器bug历史来设计变异器。作者认为bug报告包含诱发bug的元素信息，可以用来指导fuzzer发现类似问题。

Method: 提出了IssueMut自动化方法，从编译器bug报告中挖掘出变异器，并集成到现有编译器fuzzer中。对GCC与LLVM的1760个bug报告提取587个变异器，并在上述编译器上进行测试。

Result: 利用从bug历史中挖掘出的变异器，IssueMut在GCC上发现28个新bug，在LLVM上发现37个新bug，其中60个已被确认或修复，验证了方法有效性。

Conclusion: 通过利用编译器的历史bug报告提取变异器，可以提升fuzzer对编译器新bug的发现能力。

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [9] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: 本研究系统梳理了汽车SoC在AUTOSAR架构下的安全漏洞，揭示了关键模块与主要漏洞类型，并对漏洞修复提出了优先级和定位建议，从而为汽车CPS平台的安全策略提供了参考。


<details>
  <summary>Details</summary>
Motivation: CCAM系统作为复杂的网络物理系统，涉及了高安全性的实时环境。然而，集成了多种功能的车载SoC软件架构在自动化标准（AUTOSAR）下，仍存在诸多安全挑战，尤其是漏洞频发且缺乏系统性根因分析。

Method: 采用系统性实证研究方法，对180个公开的汽车SoC漏洞进行归类和映射分析，结合AUTOSAR分层模型结构，进一步探索各类CWE漏洞的延迟及分布特点。

Result: 通过分析180个公开报告的汽车SoC漏洞，将这些漏洞映射到与AUTOSAR标准对齐的软件架构模型，识别了16类根本原因和56个受影响的软件模块，并分析了各类漏洞的修复延迟和分布模式。

Conclusion: 汽车SoC在AUTOSAR分层架构下存在较多、且修复滞后的安全漏洞。论文为系统构建更安全的车载CPS平台提出了改进检测、优先级设定和定位漏洞的具体建议。

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [10] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: 作者提出用LLM驱动的自动化bug跟踪框架，通过AI报告完善、复现、分类、补丁生成等全流程自动化，解决传统系统中的沟通低效和修复迟缓问题，实现更快响应和更高效的维护。


<details>
  <summary>Details</summary>
Motivation: 传统的bug跟踪系统由于各角色分工以及沟通壁垒，导致从发现到修复过程缓慢且低效。现有系统还高度异步，用户响应等待时间长，增加了用户的挫败感。作者试图解决这个通信效率低、协作复杂和修复流程滞后的现状。

Method: 提出了一个基于AI赋能的大型语言模型（LLM）的bug跟踪框架。该框架让用户用自然语言上报bug，由AI代理自动完善报告、尝试复现、请求细节，并自动分类问题（无效的通过无代码修复，有效的归类给开发人员）。LLM还能生成候选修复补丁，但需人工审核。整个过程实现自动化和智能化，嵌入现有bug跟踪系统。

Result: 该框架可减少bug修复时间，降低人工参与，提升协作效率。用户体验改善，响应更快，开发团队与用户的沟通和维护流程更加流畅。AI自动化覆盖从报告到修复每个环节，加强了软件维护实践。

Conclusion: AI驱动的bug跟踪框架通过引入自动化和智能处理，有效提升了问题响应速度、用户参与度和开发团队协作效率，推动了软件维护流程的现代化发展。

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [11] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: 该论文提出了一种将空白符不敏感的语言模块通过预处理后用于空白符敏感语言开发的方法，并在简化Python语言实验中验证有效性，可提升组件复用、降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 目前在软件语言工程领域，模块化语言组件的可复用性受限，特别是在集成空白符敏感和非敏感语言时存在障碍，导致许多库无法复用，空白符敏感语言常常需要从零开发。

Method: 提出通过在解析前预处理语言工件，使模块化且对空白符不敏感的语言模块可以用于构建空白符敏感语言。并以简化版的Python编程语言为例验证该方法。

Result: 通过预处理技术能够提升已有语言组件的复用性，将空白符不敏感的模块应用于敏感语言构建，成功重建了Python的简化版。

Conclusion: 该技术可以有效提高语言组件的复用性，从而减少开发时间并提升软件语言的整体质量。

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [A Zone-Based Algorithm for Timed Parity Games](https://arxiv.org/abs/2510.07361)
*Gilles Geeraerts,Frédéric Herbreteau,Jean-François Raskin,Alexis Reynouard*

Main category: cs.LO

TL;DR: 针对时序游戏在控制器综合中的难点，论文对语义进行了改进并提出基于区域的高效奇偶算法，已在实际工具中实现，表现良好。


<details>
  <summary>Details</summary>
Motivation: 原始时序游戏语义在控制器综合任务中表现出一些不直观之处，且缺乏适用于奇偶目标的高效算法。为此，作者提出对语义进行调整，并开发新的算方法。

Method: 论文通过对原有时序游戏语义进行调整，使其更适用于控制器综合，并提出基于区域的数据结构实现奇偶目标的高效算法。

Result: 所提出的区块（zone-based）算法有效地解决了时序奇偶游戏，并在UppAal的zone库中进行了实现验证。

Conclusion: 该论文提出的语义修改和面向区域的算法能够高效解决时序奇偶游戏，并已在UppAal的zone库中实现原型，证明了所提方法的可行性。

Abstract: This paper revisits timed games by building upon the semantics introduced in
"The Element of Surprise in Timed Games". We introduce some modifications to
this semantics for two primary reasons: firstly, we recognize instances where
the original semantics appears counterintuitive in the context of controller
synthesis; secondly, we present methods to develop efficient zone-based
algorithms. Our algorithm successfully addresses timed parity games, and we
have implemented it using UppAal's zone library. This prototype effectively
demonstrates the feasibility of a zone-based algorithm for parity objectives
and a rich semantics for timed interactions between the players.

</details>


### [13] [Homomorphism Problems in Graph Databases and Automatic Structures](https://arxiv.org/abs/2510.07422)
*Rémi Morvan*

Main category: cs.LO

TL;DR: 论文系统研究了同态问题在数据库与自动结构领域的判定性、优化算法与可描述性，理论上证明了复杂查询和自动结构上的关键性质，提出了相关高效算法与判定理论，为后续图数据库与逻辑自动结构研究提供重要工具和基础。


<details>
  <summary>Details</summary>
Motivation: 同态映射（结构保持的映射）在数据库查询（特别是有限图状数据）和约束求解（包括可能无限的结构）中的核心作用日益突出。已有研究表明合取查询与同态存在性之间有等价性，但其在更复杂查询与自动结构上的表现及其优化问题亟需深入探讨。

Method: 论文第一部分关注结合正则路径谓词的合取正则路径查询，研究查询的最小化问题（基于原子数目和查询图的树宽两种度量）及其判定性与实际算法。第二部分将同态问题提升至自动结构（用有限自动机可描述的无限结构），分析其判定性二分法，并探究结构的语言理论属性，提出新型代数语言理论判断其可描述性的可判定性。

Result: 第一部分证明了两种度量下的查询最小化判定性，并为实际常用查询提供了高效算法。第二部分揭示了自动结构上同态问题的二分性：部分问题在非确定性对数空间内可判定，但大多数为不可判定；同时，基于提出的代数理论，判断自动结构是否可用任何良好逻辑（伪变体）描述是可判定的。

Conclusion: 同态问题在图数据库与约束求解的理论基础中作用重要；对于合取正则路径查询，最小化与有效求解得到理论保证和实用算法。自动结构上同态问题的理论二分基本阐明，并通过新语言理论手段捕捉了这些结构的逻辑可描述性，提升自动结构理论分析与实际应用能力。

Abstract: This thesis investigates the central role of homomorphism problems
(structure-preserving maps) in two complementary domains: database querying
over finite, graph-shaped data, and constraint solving over (potentially
infinite) structures. Building on the well-known equivalence between
conjunctive query evaluation and homomorphism existence, the first part focuses
on conjunctive regular path queries, a standard extension of conjunctive
queries that incorporates regular-path predicates. We study the fundamental
problem of query minimization under two measures: the number of atoms
(constraints) and the tree-width of the query graph. In both cases, we prove
the problem to be decidable, and provide efficient algorithms for a large
fragment of queries used in practice. The second part of the thesis lifts
homomorphism problems to automatic structures, which are infinite structures
describable by finite automata. We highlight a dichotomy, between homomorphism
problems over automatic structures that are decidable in non-deterministic
logarithmic space, and those that are undecidable (proving to be the more
common case). In contrast to this prevalence of undecidability, we then focus
on the language-theoretic properties of these structures, and show, relying on
a novel algebraic language theory, that for any well-behaved logic (a
pseudovariety), whether an automatic structure can be described in this logic
is decidable.

</details>


### [14] [Verifying Graph Neural Networks with Readout is Intractable](https://arxiv.org/abs/2510.08045)
*Artem Chernobrovkin,Marco Sälzer,François Schwarzentruber,Nicolas Troquard*

Main category: cs.LO

TL;DR: 本文提出了用于分析量化ACR-GNN的逻辑工具，理论与实验均表明：虽然量化模型高效且效果佳，但其安全验证极具挑战性，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 近年来图神经网络（GNN）在许多实际应用中的安全性问题受到关注，尤其是量化模型在资源受限环境下的部署需求。因此，需要对量化GNN的验证和安全性进行深入分析。

Method: 提出了一种用于推理量化ACR-GNN（aggregate-combine-readout图神经网络）的逻辑语言，并用此方法对量化GNN的验证复杂性进行了逻辑分析和实验评估。

Result: 理论上证明了带全局readout的量化GNN的验证任务是（co）NEXPTIME完备；实验显示量化模型既轻量又能保持良好的精度及泛化能力。

Conclusion: 量化的ACR-GNN模型在保证准确性和泛化能力的前提下表现为轻量级，但其验证任务属于（co）NEXPTIME完备，意味着计算上极为困难。

Abstract: We introduce a logical language for reasoning about quantized
aggregate-combine graph neural networks with global readout (ACR-GNNs). We
provide a logical characterization and use it to prove that verification tasks
for quantized GNNs with readout are (co)NEXPTIME-complete. This result implies
that the verification of quantized GNNs is computationally intractable,
prompting substantial research efforts toward ensuring the safety of GNN-based
systems. We also experimentally demonstrate that quantized ACR-GNN models are
lightweight while maintaining good accuracy and generalization capabilities
with respect to non-quantized models.

</details>


### [15] [Implication Problems over Positive Semirings](https://arxiv.org/abs/2510.08112)
*Minna Hirvonen*

Main category: cs.LO

TL;DR: 本文以半环团队语义为基础，统一研究了多种数据库与概率依赖结构，并提出了相应的推理与公理化方法，帮助理解不同语义下依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 希望在信息系统与数据库理论中，将各种依赖性结构在统一的数学框架下进行比较与综合，并便于对不同行为语义（如关系、集合、概率等）进行统一推理。

Method: 将数据库和概率理论中的依赖性概念推广至带有半环注释的团队（即数据库关系），并对这些依赖性概念在半环团队语义下进行公理化和蕴涵问题的研究。

Result: 提出了多种依赖性在半环团队语义下的形式化方法，并系统探讨了这些依赖性在不同半环选择下的推理规则，实现了对不同语义体系依赖推理的统一。

Conclusion: 本文揭示了半环团队语义为多种依赖概念的研究提供了统一且通用的理论框架，不同的半环对应不同的数据解释和推理语义。

Abstract: We study various notions of dependency in semiring team semantics. Semiring
teams are essentially database relations, where each tuple is annotated with
some element from a positive semiring. We consider semiring generalizations of
several dependency notions from database theory and probability theory,
including functional and inclusion dependencies, marginal identity, and
(probabilistic) independence. We examine axiomatizations of implication
problems, which are rule-based characterizations for the logical implication
and inference of new dependencies from a given set of dependencies. Semiring
team semantics provides a general framework, where different implication
problems can be studied simultaneously for various semirings. The choice of the
semiring leads to a specific semantic interpretation of the dependencies, and
hence different semirings offer a way to study different semantics (e.g.,
relational, bag, and probabilistic semantics) in a unified framework.

</details>


### [16] [Complexity Results in Team Semantics: Nonemptiness Is Not So Complex](https://arxiv.org/abs/2510.08122)
*Aleksi Anttila,Juha Kontinen,Fan Yang*

Main category: cs.LO

TL;DR: 本文分析了团队语义下带NE原子的凸逻辑的复杂性，发现其可满足性为NP-完全，有效性为coNP-完全，模型检验在P内。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究在团队语义下，凸逻辑的复杂性理论属性，扩展了经典命题逻辑，引入了非空原子NE。

Method: 分析NE原子扩展后的逻辑在团队语义中的复杂性，并对可满足性、有效性以及模型检验问题进行理论证明。

Result: NE扩展下的逻辑：可满足性问题为NP-完全，有效性问题为coNP-完全，模型检验问题在P内。

Conclusion: 通过引入NE原子的团队语义下的凸逻辑，明确了其主要决策问题对应的复杂性等级，为凸逻辑的理论研究提供复杂性基础。

Abstract: We initiate the study of the complexity-theoretic properties of convex logics
in team semantics. We focus on the extension of classical propositional logic
with the nonemptiness atom NE, a logic known to be both convex and union
closed. We show that the satisfiability problem for this logic is NP-complete,
that its validity problem is coNP-complete, and that its model-checking problem
is in P.

</details>


### [17] [Compression for Coinductive Infinitary Rewriting: A Generic Approach, with Applications to Cut-Elimination for Non-Wellfounded Proofs](https://arxiv.org/abs/2510.08420)
*Rémy Cerda,Alexis Saurin*

Main category: cs.LO

TL;DR: 论文以余归纳法扩展无穷归约系统的理论，统一且证明了多类非终止归约过程的关键压缩性质，并通过典型逻辑演算展现其方法的广泛适用性和理论意义。


<details>
  <summary>Details</summary>
Motivation: 无穷归约系统用于分析非终止但具备生成性的归约过程，Compression 性质具有理论与应用价值。传统方法基于极限收敛，近期余归纳表示更为广泛，因此需要对更复杂或非良基系统提供压缩性质的统一讨论与证明。

Method: 借助余归纳法，设计了一般性的 Compression 证明，将多种无穷归约系统的压缩性质归结为某些系统性特征，并通过例子（如一阶归约、无穷 λ 演算和非良基线性逻辑证明系统）来展示方法的适用性。

Result: 该工作在基于余归纳法的框架下，给出了无穷归约系统的通用 Compression 证明，并明确了“可压缩性”应具备的核心特征。通过一阶演算、λ-演算和 μMALL∞ 非良基线性逻辑等典型例子展示方法效果，推动相关逻辑系统中的剪切消去等关键推理论证。

Conclusion: 作者扩展了以余归纳法为基础的无穷归约系统的一般定义，并在非良基推导的框架下证明了关键性质 Compression（即归约序列可压缩至至多 ω 长度）。该结果为多个无穷归约系统提供了统一性分析工具。

Abstract: Infinitary rewriting, i.e. rewriting featuring possibly infinite terms and
sequences of reduction, is a convenient framework for describing the dynamics
of non-terminating but productive rewriting systems. In its original definition
based on metric convergence of ordinal-indexed sequences of rewriting steps, a
highly desirable property of an infinitary rewriting system is Compression,
i.e. the fact that rewriting sequences of arbitrary ordinal length can always
be 'compressed' to equivalent sequences of length at most {\omega}.
  Since then, the standard examples of infinitary rewriting systems have been
given another equivalent presentation based on coinduction. In this work, we
extend this presentation to the rewriting of arbitrary non-wellfounded
derivations and we investigate compression in this setting. We design a generic
proof of compression, relying on a characterisation factorising most of the
proof and identifying the key property a compressible infinitary rewriting
system should enjoy.
  As running examples, we discuss first-order rewriting and infinitary
{\lambda}-calculi. For the latter, compression can in particular be seen as a
justification of its coinductive presentation in the literature. As a more
advanced example, we also address compression of cut-elimination sequences in
the non-wellfounded proof system {\mu}MALL{\infty} for multiplicative-additive
linear logics with fixed points, which is a key lemma of several
cut-elimination results for similar proof systems.

</details>


### [18] [Dynamic Automated Deduction by Contradiction Separation: The Standard Extension Algorithm](https://arxiv.org/abs/2510.08468)
*Yang Xu,Xingxing He,Shuwei Chen,Jun Liu,Xiaomei Zhong*

Main category: cs.LO

TL;DR: 该论文提出了首个明确实现CSE理论的标准扩展算法，解决了多子句自动推理的关键问题，经多项竞赛实证，提供了可靠基础。


<details>
  <summary>Details</summary>
Motivation: 传统自动推理系统如Prover9、E和Vampire等，基于二元推理，难以实现多子句联合推理，限制了推理能力。CSE框架虽突破了理论瓶颈，但缺少实际算法实现。

Method: 提出了标准扩展算法，首次明确实现了矛盾分离推理。算法通过互补文字扩展动态构建矛盾，并统一用于可满足性和不可满足性检查。方法获得形式化的完备性与正确性证明。

Result: 通过CSE系列系统（如CSE, CSE-E, CSI-E, CSI-Enig）在自动推理竞赛CASC中的表现间接验证了算法的效果，标准扩展机制成为多子句自动推理的可靠基础。

Conclusion: 标准扩展算法将CSE理论有效落地，为动态、多子句自动推理奠定了理论和算法基础，并获得实际验证。

Abstract: Automated deduction seeks to enable machines to reason with mathematical
precision and logical completeness. Classical resolution-based systems, such as
Prover9, E, and Vampire, rely on binary inference, which inherently limits
multi-clause synergy during proof search. The Contradiction Separation
Extension (CSE) framework, introduced by Xu et al. (2018), overcame this
theoretical limitation by extending deduction beyond binary inference. However,
the original work did not specify how contradictions are algorithmically
constructed and extended in practice. This paper presents the Standard
Extension algorithm, the first explicit procedural realization of contradiction
separation reasoning. The proposed method dynamically constructs contradictions
through complementary literal extension, thereby operationalizing the CSE
theory within a unified algorithm for satisfiability and unsatisfiability
checking. The algorithm's soundness and completeness are formally proven, and
its effectiveness is supported indirectly through the performance of CSE-based
systems, including CSE, CSE-E, CSI-E, and CSI-Enig in major automated reasoning
competitions (CASC) in the last few years. These results confirm that the
Standard Extension mechanism constitutes a robust and practically validated
foundation for dynamic, multi-clause automated deduction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

TL;DR: 本研究结合街景图像与社交媒体文本，定量发现了城市居民对城市环境的感知与舆论情感存在显著不一致，且受疫情等因素影响显著。通过可视化分析，为未来城市更新管理提出新策略建议。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台崛起后，人们对城市环境的感知和意见变得更复杂，现有多维度情感分析方法已无法满足城市研究的需求，因此亟需新的方法揭示感知与舆论之间潜在的情感不一致性及其影响因素。

Method: 构建大型多模态数据集（140,750街景图片和984,024社交媒体文本），使用目标检测与自然语言处理技术，提出反应指数，并采用回归分析、图像分割和词频分析方法，从土地利用分布等角度分析和可视化情感反应。

Result: 分析结果显示，感知情感趋于更平均分布，舆论情感则变化更极端；情感反应变化与城市环境元素显著相关，疫情前后均存在感知与舆论情感显著不一致，为城市环境管理和更新提供了有力数据支持。

Conclusion: 该研究展示了城市环境中人类感知与舆论情感之间的显著不一致性，并指出这种差异变化与城市元素（如密集建筑和行人出现）高度相关。疫情前后的情感反应变化为环境管理和城市更新策略提供了数据支持和理论参考。

Abstract: The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [20] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

TL;DR: 本文提出HaystackCraft基准，真实模拟长上下文中的干扰和噪音，并结合多种检索与agentic情景，发现现有顶级大模型在实际复杂场景下鲁棒性仍有不足，HaystackCraft为未来模型改进提供了关键测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文模型与基准测试常忽视现实世界中噪音和干扰来源，无法真实反映模型的实际鲁棒性。作者提出通过“干草堆工程”更真实地构建含噪音的测试环境，关注异质检索和agentic流程中的错误级联。

Method: 打造HaystackCraft基准，从维基百科超链接网络构建多跳问题，结合多种检索策略（稀疏、稠密、混合、图结构），并扩展至动态、模型依赖的agentic测试场景。评估15种长上下文模型在各检索和agentic设置下性能。

Result: 实验发现更加坚固的稠密检索器会引入更多复杂干扰项，但图结构重排序可提升有效检索同时缓解有害干扰。在agentic设置下，顶级模型如Gemini 2.5 Pro和GPT-5也会因自生成干扰而失败或难以执行早停止，显示长上下文推理依然存在挑战。

Conclusion: HaystackCraft可以更真实地模拟长上下文中的噪音和干扰，为模型的长上下文鲁棒性评估提供了更有效的基准。现有长上下文模型在真实场景下仍面临显著挑战。

Abstract: Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [21] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本论文验证了大语言模型无需微调，仅用少量示例即可在多语言词形还原任务上取得最优效果，传统编码器方法在无监督领域仍有竞争力，但LLM明显更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在许多自然语言处理任务中表现优异，但尚无关于它们在词形还原（lemmatization）任务中的有效性证据，尤其是在没有监督训练数据时。作者意在评估LLM在无监督情境下词形还原的能力。

Method: 作者对最新一代LLM进行实证评估，比较三种方式：1）仅编码器的监督方法，在外域数据上微调；2）跨语言方法；3）直接用LLM进行上下文中的词形还原（无微调，仅用少量示例）。在12种不同形态复杂性的语言中开展实验。

Result: 实验结果显示，编码器在外域微调时依然具有竞争力，但直接用LLM在上下文中生成词形还原结果（无微调，仅靠少量示例）在大多数语言上达到了最新的最优水平（SOTA）。数据和代码将在论文发布后公开。

Conclusion: 直接用LLM进行上下文词形还原（无需微调，只需少量示例）即可在多数语言领域实现最佳效果，体现了LLM在这一任务上的强大能力。传统方法在没有领域内训练数据时依然有效，但LLM优势十分显著。

Abstract: Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [22] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

TL;DR: 通过大型语言模型设计的新型评分体系LASER，显著提升了ASR在多语种下的评价公平性，准确性显著高于传统WER，既适用大模型也适用经过微调的小模型。


<details>
  <summary>Details</summary>
Motivation: 传统自动语音识别（ASR）评价指标如词错误率（WER），容易对一些并不影响句子语义的形态和句法细节进行不公平的惩罚。该研究试图提升评价指标的语义合理性。

Method: 该论文提出了一种基于大型语言模型（LLM）的评分方法LASER，利用LLM的上下文学习能力，通过详细的案例提示学习新的评分规则。此外，尝试用Gemini 2.5 Pro进行Hindi语种分析，并尝试在较小的模型（Llama 3）上微调以预测惩罚类型。

Result: 使用Gemini 2.5 Pro的Hindi LASER分数与人工标注高度相关，相关性高达94%。Prompt中的Hindi案例也能有效分析Marathi、Kannada和Malayalam等其他印度语言的错误。微调后的Llama 3在预测惩罚种类时准确率接近89%。

Conclusion: LLM驱动的LASER评分体系能更公平地评估ASR输出，对多语种具备良好的泛化能力，且小模型细致微调后也能取得较高准确性。

Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [23] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文系统分析了手语动作评价的多种方法，验证不同指标的优劣，并公开了评测工具，为后续手语生成与翻译系统的开发和评测奠定基础。


<details>
  <summary>Details</summary>
Motivation: 手语生成与翻译模型逐渐被广泛应用，但如何有效、全面地评价生成的手语动作质量，仍缺乏系统方法。因此，本研究旨在探索与比较多种手语动作评价指标。

Method: 系统性研究三种评价指标：基于关键点距离、基于嵌入、基于反向翻译，采用自动检索元评价和人工相关性分析，对不同指标在多语言场景下的表现进行比较和验证。

Result: 不同指标在不同场景下呈现权衡，实验揭示了各方法的实用性与局限性。研究团队还发布了开源的姿势评价工具包，促进手语生成和翻译系统的可复现开发与评价。

Conclusion: 本研究为手语动作评价提供了全面分析和实用工具，让研究者可以在实际开发和测试中选择合适的评价方法，有助于推动手语技术的进步和标准化。

Abstract: We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [24] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

TL;DR: 本文提出一种基于类人训练流程和链式思考的提示方法，让大语言模型能在民粹主义文本分类任务中达到与专家人工编码员相当的准确率，显著提升了跨语境和大规模语料处理的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有文本分析方法虽然客观有效，但在语言、语境和大规模语料处理上成本高、难以扩展，因此需要更高效且可扩展的自动化方法来衡量民粹主义的思想内容。

Method: 使用类人编码员训练流程的“链式思考”提示方法，结合Global Populism Database（GPD），让LLM参考人工编码文档并进行推理，并在多种模型上复现GPD的评分。

Result: 该提示策略下的大语言模型表现出与专家编码者相当的分类准确性，证明其能处理民粹主义语境中的细微和敏感的问题。

Conclusion: 基于领域特定的提示策略，LLM模型能够在衡量民粹主义语料的分类准确性上媲美专家级人工编码者。

Abstract: Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [25] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文提出了MAPRO框架，系统化优化多智能体提示词，通过语言引导的信念传播算法和拓扑感知机制，提升了MAS协同与性能，显著优于现有方法并提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统（MAS）中，协同专有角色能够超越单智能体的表现，但由于提示词敏感性和系统不稳定性，使得高效设计MAS非常困难。此外，现有自动化提示词优化机制主要关注单智能体场景，多智能体提示词优化存在搜索空间爆炸和信任归因困难等问题，急需系统性且有理论支持的方法。

Method: 提出MAPRO框架，将MAS提示词优化建模为最大后验推断问题，并采用基于语言引导的最大乘积信念传播算法求解。框架内嵌拓扑感知的细化机制，结合执行反馈和下游归因迭代更新各代理的提示词，从而逐步收敛到协调、高效的智能体提示策略集合。

Result: MAPRO在多项基准测试任务中实现了最先进（SOTA）的性能，持续超越人工优化基线和最新自动化方法。

Conclusion: MAPRO能够显著提升多智能体系统性能，为可靠和系统化设计MAS提供了重要理论和实践指导，未来有望推动更严谨的多智能体系统构建。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [26] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

TL;DR: 本论文提出AsyncSpade异步框架，通过轻量级时序回归模块与KV-cache过滤解耦，实现高效的LLM推理，大幅缩短生成时间，兼顾准确率与实用性，解决了TTS场景下的效率与性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的Test-time scaling（TTS）能够提升推理能力，但因KV-cache线性增长使得内存受限成为主要瓶颈，尤其在高并发与长CoT场景下影响推理效率。现有稀疏解码方法虽能在有限算力下实现优异性能，但受限于串行依赖的页过滤和粗粒度的token选择，导致效率和模型效果均受影响。

Method: 提出了AsyncSpade异步框架，包括：（1）轻量级的时序回归模块，预测下一个token的query状态；（2）异步和解耦的架构，将KV-cache过滤与自回归解码环路解耦，并通过异步操作，将token级别的KV选择与前向推理计算重叠，提升效率。该方法实现了无需等待解码环路即可训练外的query感知稀疏性。

Result: 在A100节点上，AsyncSpade能与推理流程完全重叠KV-cache操作，达到理论上最佳的每输出token时间（TPOT）。在Qwen3-8B和Qwen3-32B等模型上，比SoTA基线（Quest）提升TPOT逾20%，比全注意力方案提升至少50%，且在多个TTS基准测试上准确率持平或超越现有方法。

Conclusion: AsyncSpade首次消除了传统方法中的序列依赖，同时保证模型性能，显著加速和优化了LLM在高并发与长链推理场景下的服务效率。

Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [27] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 本研究以团队科学视角，构建并评估LLM驱动的多智能体协作，发现扁平团队更优，多样性影响复杂，协作虽有益但存在协调难题。


<details>
  <summary>Details</summary>
Motivation: 尽管采用LLM驱动的多智能体系统日益受到关注，但针对其团队协作和动态的系统性研究较少。希望通过引入团队科学的视角，深入理解AI代理在协作中的关键因素。

Method: 提出了一个多智能体框架，借鉴人类团队科学理论，重点分析团队结构、多样性和交互动态，并在四类任务（CommonsenseQA, StrategyQA, Social IQa, Latent Implicit Hate）上评估团队表现。

Result: 实验表明，扁平结构的团队相比分层结构表现更好，多样性对团队表现有复杂影响。访谈发现，代理对团队表现信心过高，但任务后反思时既认可协作的价值，也指出集成和协调上的挑战。

Conclusion: 未来应关注提高多智能体团队的协作和协调能力，进一步优化结构和整合机制，以发挥AI代理群体智慧的最大潜力。

Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [28] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

TL;DR: 本文提出在多流语音LLM中应用链式思维微调，并通过“问题完整度”指标提前推理，实现准确率提升2.4倍及显著降低延迟，最终利用DPO优化达到70%延迟减少且无准确率损失。


<details>
  <summary>Details</summary>
Motivation: 尽管语音大型语言模型（LLMs）已实现无缝语音交互，但在复杂推理任务上表现仍不理想。文本LLMs通过链式思维（CoT）提示或微调提升了推理能力，作者想探究这些方法在多流语音LLM上的效果，并解决语音系统响应延迟的问题。

Method: 作者将CoT微调引入多流语音LLM，并在文本空间进行推理；提出基于熵的“问题完整度”指标，指导模型在用户提问尚未结束时提前启动推理，以减少响应延迟。同时利用Direct Preference Optimization（DPO），基于偏好数据进一步优化准确率-延迟的帕累托前沿。

Result: 通过在文本空间推理，语音LLM在语音推理任务中的准确率平均提升2.4倍。基于“问题完整度”的方法，使模型在保持相同延迟条件下，ARC-Easy任务准确率提升4%。使用DPO优化后，延迟降低70%，但准确率未下降。

Conclusion: 链式思维微调显著提升多流语音LLM的推理能力。借助问题完整度提前推理，以及DPO优化，可以有效控制语音系统的准确率与延迟，为语音交互代理提供更优体验。

Abstract: Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [29] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

TL;DR: 为提升长上下文大模型多跳推理能力，本文引入可优化迭代的思维模板，并支持模板向小模型蒸馏，实验证明方法有效且适用性强。


<details>
  <summary>Details</summary>
Motivation: 尽管长上下文大模型能够容纳和处理大量文档，但仅依靠简单堆叠文档难以指导证据如何有效连接，从而影响多跳推理能力。为此，需要结构化方式帮助模型更好组合和利用证据。

Method: 提出了将推理过程结构化为可复用的思维模板（结合以往问题求解轨迹），并使用自然语言反馈迭代优化模板，同时适用于检索型和非检索型场景。最后还将优化后的模板蒸馏进小模型。

Result: 在多种基准和不同长上下文大模型家族中，该方法在检索型与非检索型任务均优于强基线，并且通过蒸馏能够使小型开源模型也受益。

Conclusion: 通过引入思维模板（thought templates），不仅提升了长上下文大模型在多跳推理任务上的性能，还可以将优化后的模板向更小的开源模型蒸馏，实现广泛适用性和透明推理复用。

Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [30] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

TL;DR: 提出了一个针对塔吉克-波斯文书写系统转换的新型序列到序列音译模型，突破领域局限，表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有音译模型数据集局限，适应范围窄，无法覆盖真实多领域场景，且尚无统一的领域基准，阻碍跨国家、跨书写系统的波斯文沟通。

Method: 采用序列到序列的深度学习模型，并整合所有公开可用数据集，同时提供了两个新数据集以提升泛化能力。

Result: 模型在Farsi到Tajik方向获得chrF++分87.91、CER 0.05，在Tajik到Farsi方向获得chrF++分92.28、CER 0.04，并设立了领域广泛的基准测试。

Conclusion: 该论文提出了一个适用于不同领域的塔吉克-波斯文双向音译模型，性能领先且具备广泛适用性。

Abstract: As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [31] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

TL;DR: 论文指出投机解码在长上下文环境下性能大幅下降，提出了新基准和创新模型 OWL，通过三项技术优化将推理效率显著提升，并开放全部代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 虽然投机解码有望加速大模型推理，但现实环境如长上下文实际应用下其性能大幅下降，现有基准和方法普遍针对短上下文，缺乏长上下文评估与优化。

Method: 作者提出了新的长上下文基准 LongSpecBench，并设计了新模型 OWL，包括三个创新：LSTM 独立于长度的 drafter、在 verifier 中加入 [SPEC] 特殊token，以及结合树与非树解码的混合算法。

Result: OWL 在长上下文输入上，接受长度提升至 EAGLE3 的约5倍，显著优于现有方法，解决了长上下文推理速度和效率瓶颈。

Conclusion: OWL 显著提升了LLM长上下文环境中的投机解码效率，且新基准和代码已发布，有助于推动该领域进一步发展。

Abstract: Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [32] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 本文针对小型视觉-语言模型评判图表任务性能欠佳的问题，提出了多指标合并和迁移学习两种低成本优化方法，并在实验中显著提升了模型的专业化和泛化能力，为资源有限环境下的自动评估提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉-语言模型（LVLMs）在图表理解任务中表现良好，但小模型（参数量不超过2B）作为评判者时效果很差，这限制了资源受限环境下的实际使用。本文旨在提升低参模型的评判能力，降低评估开销。

Method: 提出了两种方法：一是多指标提示（multi-criteria prompting），将多个评判标准合并为单一查询；二是领域自适应迁移学习，将一个2B参数的LVLM在合成图表判断数据集上微调，得到ChartJudge模型。

Result: 实验证明多指标提示揭示了鲁棒性差距，导致7B模型表现显著下降，包括专业LVLM评判者如LLaVA-Critic。同时，微调得到的ChartJudge能够有效将知识从一个数据集迁移到另一个，成为更专业的小模型。

Conclusion: 提出的优化策略（多指标提示和迁移学习）显著提升了小型LVLM的评判能力，使其在图表推理任务中的低成本评估成为可能，为实际应用提供了可行路径。

Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [33] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

TL;DR: 作者针对移动端高效NLP模型提出了基于LoRA模块的多任务预微调框架，不仅解决了多任务信号冲突问题，还显著提升了模型在NER和文本分类任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 在移动平台上部署自然语言处理（NLP）模型时，需要模型能够适应各种应用，同时保持内存和计算的高效性。现有的多任务预微调方法存在任务间优化信号冲突，影响整体性能。作者旨在解决这一问题，提高轻量级BERT模型在不同NLP任务（如命名实体识别和文本分类）中的适应性和效能。

Method: 作者提出了一种基于任务主导LoRA模块的多任务预微调框架。该方法采用一个共享编码器主干，并通过模块化适配器实现多任务处理，从而增强模型适应性并满足移动端部署要求。

Result: 在21个下游任务上的实验表明，所提方法在命名实体识别任务上平均提升0.8%，在文本分类任务上平均提升8.8%，效果显著。

Conclusion: 论文提出的多任务预微调框架能在保持模型高效和适应性的前提下，有效提升多种NLP任务的性能，是移动端NLP模型部署的可行方案。

Abstract: Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [34] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

TL;DR: 研究发现，疫情虚假信息语言更复杂、情绪暗示更多，可信度提升但可读性降低，提出语言指标用于识别虚假内容，并建议今后加强情绪识别和动态分析。


<details>
  <summary>Details</summary>
Motivation: 随着疫情期间健康信息和虚假信息大量涌现，急需有效辨别和应对健康虚假信息，从而改进公共健康传播效果。该研究旨在通过语言分析揭示虚假信息与真实信息的语言差异，助力虚假信息识别。

Method: 研究通过计算语言学方法，对三种语料库进行分析，包括COVID-19虚假叙事、一般COVID-19内容和Monkeypox相关推文。采用可读性分数、修辞标记及说服性词语等指标，比较各类信息的语言特征。

Result: COVID-19虚假信息的可读性显著较低，恐惧类或说服类词语的使用频率是其他数据集的两倍以上。与Monkeypox内容的情绪化表达（如感叹号）不同，虚假信息修辞上更复杂、情绪暗示更多。

Conclusion: 虚假健康信息呈现复杂的修辞风格并嵌入丰富情绪提示，这有助提升其表面可信度。研究揭示了有助于虚假信息识别的语言指标，为公共健康信息传播和理论模型提供新见解，但因可读性指标、情绪词库和静态分析存在局限，呼吁未来采用更广泛和动态的方法。

Abstract: This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [35] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

TL;DR: 提出了用LLM辅助设计和生成人工语言的系统，包括音系、词汇、正字法和语法，测试了其语言认知和翻译能力。系统对常见语言模式表现较好，低资源语言翻译需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 一方面为语言爱好者提供有趣、实用的人工语言建构工具，另一方面探索LLM对语言及语言学知识的理解深度和广度，特别是其在通用和罕见语言结构上的处理能力及在高、低资源语言翻译中的潜力。

Method: 系统分为多个步骤：1）代理式迭代生成目标音系；2）英文句子转换成带形态句法标记的语料；3）根据音系模型和语料提取词汇；4）为语言设计正字法；5）自动生成简明语法手册。并对多种LLM和语言设定进行性能比较和分析。

Result: 该研究提出了一个利用大型语言模型（LLM）辅助构造人工语言的系统。系统采用模块化设计，起初通过智能代理逐步完善目标语言的音系，再将英文句子转换为反映目标语言词序和形态句法特征的标记语料。由此语料生成词汇表，并指定正字法，最后自动生成语法手册，并可持续翻译新句子到目标语言。系统旨在探索LLM处理常见与罕见语言特征的能力，并尝试用于高资源到低资源语言的翻译任务。结果显示，不同LLM及语言规格下表现差异大，对常见模式处理效果更好；低资源语言翻译效果尚不理想，但方法有改进空间。

Conclusion: LLM能作为建构语言工具，尤其在处理常见语言特征方面效果突出。但面对罕见语言结构和低资源语言的自动化处理依然存在挑战，有待技术改进。系统为人工语言设计和语言知识研究提供了新思路。

Abstract: We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [36] [Symmetric Rule-Based Achlioptas Processes for Random $k$-SAT](https://arxiv.org/abs/2510.07870)
*Arnab Chatterjee*

Main category: cs.DM

TL;DR: 提出新的在线规则提升了随机k-SAT的可满足性阈值，并拓展了Achlioptas过程在随机CSP中的应用。


<details>
  <summary>Details</summary>
Motivation: 受power-of-two-choices模型的启发，作者希望探索有限选择的在线子句规则能否有效提升随机k-SAT的可满足性阈值，并突破已有的理论上限。

Method: 第一，基于power-of-two-choices模型，提出MIDDLE-HEAVY规则，并分析其对随机k-SAT可满足性阈值的影响。第二，通过偏置2-SAT投影和两类型分支过程证书推导出阈值的闭式表达公式。最后，提出了混合对称偏置规则，并与以往工作进行了对比分析。

Result: 证明MIDDLE-HEAVY规则在k=4（l=5）、k=5（l=4）、k≥6（l=3）时，可将k-SAT可满足性阈值提升至超过2^k ln 2的经典上界。此外，混合对称偏置规则能在保持对称性的基础上达到接近已有结果的阈值。

Conclusion: 本研究提出了一种称为MIDDLE-HEAVY的对称、非自适应、与拓扑无关的在线规则，在某些参数下可以将随机k-SAT问题的可满足性阈值提升到超出传统的一阶矩上界。此外，提出的混合对称偏置规则在保持对称性的同时也能达到与已有研究相当的阈值。此工作拓展了Achlioptas过程在随机约束满足问题（CSP）领域中的应用。

Abstract: Inspired by the "power-of-two-choices" model from random graphs, we
investigate the possibility of limited choices of online clause choices that
could shift the satisfiability threshold in random $k$-SAT.Here, we introduce
an assignment symmetric, non-adaptive, topology-oblivious online rule called
\emph{MIDDLE-HEAVY}, that prioritizes balanced sign profile clauses.Upon
applying a biased $2$-SAT projection and a two-type branching process
certificate, we derive closed-form expressions for the shifted thresholds
$\alpha_{\textbf{SYM}}(k,\ell)$ for this algorithm.We show that minimal choices
$\ell=5$ for $k=4$, $\ell=4$ for $k=5$, and $\ell=3$ for $k\ge 6$ suffice to
exceed the asymptotic first-moment upper bound $\sim 2^k \ln 2$ for random
$k$-SAT.Moreover, to bridge the gap with biased assignment rules used in
maximum of the previous works in this context, we propose a hybrid symmetric
biased rule that achieves thresholds comparable to prior work while maintaining
symmetry.Our results advance the understanding of Achlioptas processes in
random CSPs beyond classical graph-theoretic settings.

</details>


### [37] [A Graph Width Perspective on Partially Ordered Hamiltonian Paths and Cycles II: Vertex and Edge Deletion Numbers](https://arxiv.org/abs/2510.08378)
*Jesse Beisegel,Katharina Klost,Kristin Knorr,Fabienne Ratajczak,Robert Scheffler*

Main category: cs.DM

TL;DR: 本文研究带有优先级约束的哈密尔顿路径与回路问题在不同图宽度参数下的复杂性。发现一些参数会使问题变得极难解（W[1]-hard或para-NP-hard），而部分参数下可用多项式或FPT算法处理，为相关约束下的路径问题提供了复杂性界定与算法方法。


<details>
  <summary>Details</summary>
Motivation: 研究在具有优先级约束（局部有序结构）的图中寻找哈密尔顿路径或回路的问题，并分析其在不同图宽度参数下的复杂性。此类问题的常规版本在某些参数下可用FPT算法解决，但对于引入优先级约束的版本复杂性尚未明确。

Method: 通过将图的优先级约束建模为对顶点集的偏序，分析其在不同图参数（如顶点或边到目标图类别的删除距离）下的复杂性。采用参数化复杂性理论，证明某些参数下问题的W[1]-hard性，同时通过设计算法给出部分参数下的XP或FPT解法，并进一步证明在特定参数下的para-NP-hard性。

Result: 发现对于如“到路径或到团的顶点距离”等参数，哈密尔顿路径/回路问题是W[1]-hard的；但在“到外平面图或到块的顶点距离”参数下，问题可以在XP时间内解决；而对“到块的边距离”等参数则存在FPT算法。此外，证明了当采用‘边团覆盖数’参数时问题为para-NP-hard。

Conclusion: 在引入先序约束的哈密尔顿问题中，多种常见图宽度参数对问题复杂性有显著影响，有些参数导致无法高效求解（W[1]-hard或para-NP-hard），而部分参数下仍可设计有效的算法（XP或FPT），为实际约束下的哈密尔顿问题算法研究提供了丰富的理论基础。

Abstract: We consider the problem of finding a Hamiltonian path or cycle with
precedence constraints in the form of a partial order on the vertex set. We
study the complexity for graph width parameters for which the ordinary problems
$\mathsf{Hamiltonian\ Path}$ and $\mathsf{Hamiltonian\ Cycle}$ are in
$\mathsf{FPT}$. In particular, we focus on parameters that describe how many
vertices and edges have to be deleted to become a member of a certain graph
class. We show that the problems are $\mathsf{W[1]}$-hard for such restricted
cases as vertex distance to path and vertex distance to clique. We complement
these results by showing that the problems can be solved in $\mathsf{XP}$ time
for vertex distance to outerplanar and vertex distance to block. Furthermore,
we present some $\mathsf{FPT}$ algorithms, e.g., for edge distance to block.
Additionally, we prove para-$\mathsf{NP}$-hardness when considered with the
edge clique cover number.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [38] [Languages of Words of Low Automatic Complexity Are Hard to Compute](https://arxiv.org/abs/2510.07696)
*Joey Chen,Bjørn Kjos-Hanssen,Ivan Koswara,Linus Richter,Frank Stephan*

Main category: cs.FL

TL;DR: 本文研究了一种新的自动复杂度，并证明了低该复杂度下的语言既非上下文无关，也不能被某些布尔电路识别，解答了部分公开问题并发现了Shannon效应。


<details>
  <summary>Details</summary>
Motivation: 模仿Sipser判别复杂度，引入并研究有限自动机下的自动复杂度与对应低复杂度语言，探索其与已有复杂度类的关系及识别难度，尤其关注布尔电路和上下文无关性。

Method: 对非确定性自动复杂度定义下的语言类进行理论分析，证明其在形式语言理论中的不可判定性，并用复杂性理论工具分析布尔电路复杂度。

Result: 证明了L_q不属于上下文无关语言，也不能被特定布尔电路类识别，量化了解L_{1/3}的复杂度，并建立了A_{Ne}复杂度下的Shannon效应。

Conclusion: 对于每个q∈(0,1/2)，低A_{Ne}复杂度的语言L_q既不是上下文无关语言，也无法被某类布尔电路识别。文中还解答了Kjos-Hanssen关于L_{1/3}布尔电路复杂度的公开问题，并证明了A_{Ne}复杂度下的Shannon效应。

Abstract: The automatic complexity of a finite word (string) is an analogue for finite
automata of Sipser's distinguishing complexity (1983) and was introduced by
Shallit and Wang (2001). For a finite alphabet $\Sigma$ of at least two
elements, we consider the non-deterministic automatic complexity given by
exactly - yet not necessarily uniquely - accepting automata: a word $x \in
\Sigma^*$ has exact non-deterministic automatic complexity $k \in \mathbb{N}$
if there exists a non-deterministic automaton of $k$ states which accepts $x$
while rejecting every other word of the same length as $x$, and no automaton of
fewer states has this property. Importantly, and in contrast to the classical
notion, the witnessing automaton may have multiple paths of computation
accepting $x$. We denote this measure of complexity by $A_{Ne}$, and study a
class of languages of low $A_{Ne}$-complexity defined as $L_q = \{ \, x \in
\Sigma^* : A_{Ne}(x) < q|x| \, \}$, which is parameterised by rationals $q \in
(0,1/2)$ (generalising a class of sets first studied by Kjos-Hanssen). We show
that for every $q \in (0,1/2)$, this class is neither context-free nor
recognisable by certain Boolean circuits. In the process, we answer an open
question of Kjos-Hanssen quantifying the complexity of $L_{1/3}$ in terms of
Boolean circuits, and also prove the Shannon effect for $A_{Ne}$.

</details>


### [39] [On the Complexity of Language Membership for Probabilistic Words](https://arxiv.org/abs/2510.08127)
*Antoine Amarilli,Mikaël Monet,Paul Raphaël,Sylvain Salvati*

Main category: cs.FL

TL;DR: 本研究探讨概率词下的上下文无关语言成员判定问题，揭示了其复杂性区分：部分可多项式时间解决，部分#P-hard甚至不可判定。通过新语言类、电路技术和复杂性分析，丰富了概率语言处理理论。


<details>
  <summary>Details</summary>
Motivation: 传统上下文无关语言（CFLs）的成员判定，通常针对确定字符串；但许多实际应用中输入是带有概率的信息，如部分未知或模糊的字符串。作者关注在这种概率词输入下，判定被采样字符串属于CFL的概率计算问题，旨在推广和统一计数型CFL问题，并探索其复杂性。

Method: 首先形式化概率词及其相关问题（如n长词属于CFL的计数、已知部分词的补全计数等）；分析不同CFL类别（如单义CFL、并集、计数自动机等）下该判定问题的复杂性；引入知识编译领域的电路结构进行高效计数，并扩展电路模型以覆盖更多语言；最后证明判定CFL概率成员计算是否可 tractable 的元问题具有条件不可判定性。

Result: 单义CFL的概率成员判定问题可多项式时间解决；但即使两个线性单义CFL并集也可能#P-hard。提出poly-slicewise-unambiguous语言类，证明该类及某些本质含糊语言也可多项式时间解决。展示计数自动机类、部分Parikh自动机问题#P-hard。知识编译电路技术能高效计数并扩展至更多语言（如原始词、两个回文拼接），但判定一个CFL在概率成员问题下是否 tractable，本身是条件不可判定的。

Conclusion: 论文系统阐述了上下文无关语言在概率词上的成员判定问题的复杂性谱。通过引入新的语言类及求解方法，拓展了 tractable 问题的范围，但也确立了该问题的部分不可判定性边界。相关结果对概率语言处理和复杂性理论具有理论及应用价值。

Abstract: We study the membership problem to context-free languages L (CFLs) on
probabilistic words, that specify for each position a probability distribution
on the letters (assuming independence across positions). Our task is to
compute, given a probabilistic word, what is the probability that a word drawn
according to the distribution belongs to L. This problem generalizes the
problem of counting how many words of length n belong to L, or of counting how
many completions of a partial word belong to L.
  We show that this problem is in polynomial time for unambiguous context-free
languages (uCFLs), but can be #P-hard already for unions of two linear uCFLs.
More generally, we show that the problem is in polynomial time for so-called
poly-slicewise-unambiguous languages, where given a length n we can tractably
compute an uCFL for the words of length n in the language. This class includes
some inherently ambiguous languages, and implies the tractability of bounded
CFLs and of languages recognized by unambiguous polynomial-time counter
automata; but we show that the problem can be #P-hard for nondeterministic
counter automata, even for Parikh automata with a single counter. We then
introduce classes of circuits from knowledge compilation which we use for
tractable counting, and show that this covers the tractability of
poly-slicewise-unambiguous languages and of some CFLs that are not
poly-slicewise-unambiguous. Extending these circuits with negation further
allows us to show tractability for the language of primitive words, and for the
language of concatenations of two palindromes. We finally show the conditional
undecidability of the meta-problem that asks, given a CFG, whether the
probabilistic membership problem for that CFG is tractable or #P-hard.

</details>
