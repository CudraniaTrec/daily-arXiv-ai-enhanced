<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.CL](#cs.CL) [Total: 20]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language](https://arxiv.org/abs/2506.23058)
*Nikolaj Hey Hinnerskov,Robert Schenck,Cosmin E. Oancea*

Main category: cs.PL

TL;DR: 作者提出一种基于索引函数和Fourier-Motzkin求解器的自动属性验证方法，不仅能证明纯数据并行程序的正确性，还能优化编译管线，验证效率高且实际有效。


<details>
  <summary>Details</summary>
Motivation: 当前纯数据并行程序中带非线性索引的属性自动验证较为困难，现有方法对于表达能力和可自动化推理存在局限。作者希望设计一种既能自动验证又能表达实际有用属性的新方法，同时为后续编译优化提供更多保障。

Method: 作者通过将数组表达为索引函数，并将属性传播和推断转化为对索引函数的变换，然后将这些属性精炼为代数（不）等式，最终利用基于Fourier-Motzkin消去法的求解器进行验证。

Result: 框架无需限制于可判定的逻辑下，依然能自动验证实际中有用的程序属性，并将这些属性用于编译器优化。实现在Futhark语言并用于7个应用，平均每次验证仅耗时1秒。通过两个案例验证消除GPU程序动态验证后速度有明显提升。

Conclusion: 该论文提出的方法能够自动验证具有非线性索引的数据并行程序的属性。该方法实践有效，并已在Futhark语言及七个应用上成功验证，平均耗时仅1秒。此外，通过消除GPU程序的动态验证，可带来显著性能提升。

Abstract: This paper presents a novel approach to automatically verify properties of
pure data-parallel programs with non-linear indexing -- expressed as pre- and
post-conditions on functions. Programs consist of nests of second-order array
combinators (e.g., map, scan, and scatter) and loops. The key idea is to
represent arrays as index functions: programs are index function
transformations over which properties are propagated and inferred. Our
framework proves properties on index functions by distilling them into
algebraic (in)equalities and discharging them to a Fourier-Motzkin-based
solver. The framework is practical and accessible: properties are not
restricted to a decidable logic, but instead are carefully selected to express
practically useful guarantees that can be automatically reasoned about and
inferred. These guarantees extend beyond program correctness and can be
exploited by the entire compiler pipeline for optimization. We implement our
system in the pure data-parallel language Futhark and demonstrate its
practicality on seven applications, reporting an average verification time of 1
second. Two case studies show how eliminating dynamic verification in GPU
programs results in significant speedups.

</details>


### [2] [A Denotational Semantics for Quantum Loops](https://arxiv.org/abs/2506.23320)
*Nicola Assolini,Alessandra Di Pierro*

Main category: cs.PL

TL;DR: 本文提出了量子程序高层结构如分支与循环的指称语义模型，填补了当前高层量子编程语义的空白，为量子软件领域的研究和开发奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算机的编程与经典计算机不同，缺乏高层次抽象语义，尤其在量子受控分支和迭代结构上研究不足。

Method: 提出了一种高层次量子编程构造的指称语义，并针对量子受控分支与迭代进行了详细分析，设计了新的数学指称域来刻画量子程序中控制流的本质。

Result: 成功构建了可反映量子系统相干演化的量子控制流程（包括循环）的指称语义框架，为高层量子程序设计提供了理论基础。

Conclusion: 该工作为量子编程的高层控制结构（如分支与循环）提供了严谨的语义支撑，有助于后续量子软件开发和理论分析。

Abstract: Programming a quantum computer, i.e., implementing quantum algorithms on a
quantum processor-based copmputer architecture, is a task that can be addressed
(just as for classical computers) at different levels of abstraction. This
paper proposes a denotational semantics for high-level quantum programming
constructs, focusing on the conceptual meaning of quantum-controlled branching
and iteration. We introduce a denotational domain where a mathematical meaning
of a quantum control flow with loops can be defined, which reflects the
coherent evolution of the quantum system implementing the program.

</details>


### [3] [Compiling a Q# Subset to QASM 3.0 in TypeScript via a JSON Based IR](https://arxiv.org/abs/2506.23407)
*Marcus Edwards*

Main category: cs.PL

TL;DR: 本文实现了一个基于TypeScript的Q#到QASM 3.0编译工具链，支持在Web环境下对Q#程序进行编译，为量子编程的跨平台应用提供了便利。


<details>
  <summary>Details</summary>
Motivation: 现有的Q#编译工具链仅限于官方Microsoft实现，缺乏向Web环境移植的能力。随着量子编程应用的扩展，将Q#编译功能带到Web平台变得有现实需求。

Method: 设计并实现了从Q#到QASM 3.0的编译工具链，包括词法分析器（lexer）、语法分析器（parser）以及编译器本体，支持Q#部分特性，并使用TypeScript语言编写以便于Web环境移植。

Result: 实现了一套完整的Q#到QASM 3.0工具链，并在多个Q#程序上进行了测试，表明其工作正常。同时，与已有Q#编译工具进行了对比验证。

Conclusion: 提出并实现了基于TypeScript的Q#到QASM 3.0编译流程，为Web环境下的Q#编译提供了新的解决方案。

Abstract: We implement a compile toolchain from Q# to QASM 3.0 including a
full-featured lexer and parser implementation, as well as a compiler that
supports a subset of Q# features. The lexer, parser and compiler are shown to
work with various input Q# programs and the implementation is compared against
existing Q# compile tools. Unlike the Microsoft implementation of the official
Q# compile toolchain, our implementation is written in TypeScript in order to
port functionality to web environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出了KGMAF多智能体框架，解决自动化需求开发中的痛点，经案例验证具备实际应用和研究价值。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程自动化系统过于关注代码开发，忽视了需求开发的复杂性，因此需要新的解决方案提升需求开发的自动化水平。

Method: 采用多智能体系统，设计六个专门的agent及一个artifact pool，并详细描述各agent的功能、行动及知识，进行案例研究验证。

Result: KGMAF有效提升了自动化需求开发的效率与准确性，案例研究展示了其在现实场景中的应用潜力。

Conclusion: KGMAF框架有潜力推动自动化需求开发，并将成为LLM时代的重要研究方向。

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [5] [An LLM-assisted approach to designing software architectures using ADD](https://arxiv.org/abs/2506.22688)
*Humberto Cervantes,Rick Kazman,Yuanfang Cai*

Main category: cs.SE

TL;DR: 本文提出用大语言模型（LLM）结合ADD方法辅助软件架构设计，经案例验证效果较佳，但需人类架构师监督与迭代优化。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计通常依靠专家经验，过程复杂且需要多次迭代，如何用LLM辅助提高设计效率与质量成为值得探讨的问题。

Method: 提出了一种结合Attribute-Driven Design (ADD) 方法和大语言模型（LLM）的辅助软件架构设计方法。该方法通过向LLM明确描述ADD流程、设定架构师角色和有结构的迭代计划，使LLM与人类架构师协作产出架构成果。

Result: 通过案例研究，比较LLM辅助设计与成熟方案，并由专业架构师评估。结果显示LLM-辅助方法生成的架构与现有成熟方案高度一致，能够部分满足架构驱动目标。

Conclusion: LLM辅助软件架构设计具有前景，但当前依然存在局限性，且强调在实际应用中需要人类监督与反复优化。

Abstract: Designing effective software architectures is a complex, iterative process
that traditionally relies on expert judgment. This paper proposes an approach
for Large Language Model (LLM)-assisted software architecture design using the
Attribute-Driven Design (ADD) method. By providing an LLM with an explicit
description of ADD, an architect persona, and a structured iteration plan, our
method guides the LLM to collaboratively produce architecture artifacts with a
human architect. We validate the approach through case studies, comparing
generated designs against proven solutions and evaluating them with
professional architects. Results show that our LLM-assisted ADD process can
generate architectures closely aligned with established solutions and partially
satisfying architectural drivers, highlighting both the promise and current
limitations of using LLMs in architecture design. Our findings emphasize the
importance of human oversight and iterative refinement when leveraging LLMs in
this domain.

</details>


### [6] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Main category: cs.SE

TL;DR: 本文提出P4OMP框架，结合检索增强方法和大语言模型，实现高可靠性的C/C++到OpenMP并行代码自动转换。相较于传统LLM方案，P4OMP显著提升了编译成功率，减少了常见的并行编程错误，在多项真实和高性能计算场景中表现优越。


<details>
  <summary>Details</summary>
Motivation: 许多开发者希望将串行C/C++代码自动转换为并行的OpenMP代码，但现有的大语言模型（LLM）方法在准确性和可编译性上表现不佳，且容易引入语法和作用域错误，因此迫切需要一种更可靠的方法提升自动化并行代码生成的质量。

Method: 提出了P4OMP框架，利用检索增强生成（RAG）技术，从OpenMP教学资源中检索结构化知识，辅助LLM（如GPT-3.5-Turbo）对串行代码进行转换生成，并在生成过程中通过检索到的上下文提升OpenMP指令生成的正确性，无需模型微调亦无需编译器插桩。

Result: 在108个真实C++程序的基准测试中，P4OMP针对所有可并行化的案例均达到100%编译成功率，而基线（未用检索的GPT-3.5-Turbo）有20例编译失败。此外，P4OMP在避免OpenMP作用域错误、语法误用和指令无效组合等方面表现优异，并在高性能计算集群上的多项基准测试中展现出良好的运行时扩展性。

Conclusion: P4OMP框架有效提升了LLM在生成OpenMP并行代码时的可靠性与适用性，为自动化并行化C/C++代码提供了一种鲁棒且可扩展的解决方案。

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [7] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Main category: cs.SE

TL;DR: 本文提出RAILS，通过语义检索和编译器反馈增强大模型辅助Java开发，实验显示其在解决import错误方面显著优于传统大模型用法，未来将扩展到更多语言和IDE。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在软件开发辅助中常出现代码不完整或import错误、缺乏针对性上下文，尤其在无外部或项目文档时问题突出，因此亟需提升其代码准确性与实用性。

Method: 提出RAILS框架，利用FAISS和OpenAI embedding对Java资源进行语义检索，将相关上下文加入LLM提示词中，并引入编译器反馈的迭代验证机制以精 refine代码建议。

Result: 在78个实际Java import错误案例上，RAILS在保持代码意图、避免幻想和提供正确import建议方面均大幅优于普通LLM提示词。即便本地未有相关库，效果依然突出。

Conclusion: RAILS显著提升了LLM在Java代码开发中处理import错误的能力，通过检索增强和编译器反馈循环有效避免了幻觉、保持了代码意图，并能够在本地缺失库时仍给出正确建议。

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [8] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Main category: cs.SE

TL;DR: 首次系统性研究表明，量化大语言模型在代码生成任务中比全精度模型表现出更优的鲁棒性，既节省计算资源又提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）的量化主要用于模型压缩和推理加速，但关于量化对模型鲁棒性的影响尚未系统研究。

Method: 针对代码生成任务，系统性评估量化对四大家族（LLaMA、DeepSeek、CodeGen、StarCoder），参数规模从350M到33B的模型鲁棒性的影响。通过两方面：输入提示的对抗攻击和模型权重扰动实验分别开展。

Result: 量化大语言模型在鲁棒性方面往往优于全精度模型。在对抗攻击实验中有51.59%的量化模型更鲁棒，而全精度模型仅为42.86%。在权重扰动实验中，量化后的LLM能更好地抵抗扰动影响。

Conclusion: 量化不仅有助于压缩模型和加速推理，还能增强LLM在代码生成任务上的鲁棒性，为今后更稳健、高效地部署LLM提供了新思路。

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [9] [Privacy-Preserving Methods for Bug Severity Prediction](https://arxiv.org/abs/2506.22752)
*Havvanur Dervişoğlu,Ruşen Halepmollası,Elif Eyvaz*

Main category: cs.SE

TL;DR: 本文提出利用联邦学习和合成数据等隐私保护手段，在数据不可共享的情况下，同样能实现效果优秀的bug严重性预测，对工业应用具重要意义。


<details>
  <summary>Details</summary>
Motivation: 在软件工程中，错误严重性预测对于提升资源分配和维护优先级非常重要，但现实中，因数据共享受限及有标签数据匮乏，工业界难以应用AI分析与模型。

Method: 基于源代码度量和大语言模型（LLMs），使用两个常用的数据集，分别对集中式学习、联邦学习和合成数据生成三种训练方式进行错误严重性预测性能对比实验。

Result: 实验结果显示，采用联邦学习和合成数据生成训练的模型，在无需数据共享前提下，能够达到与集中式训练模型相当的效果。

Conclusion: 联邦学习与合成数据生成等隐私保护方法，可有效在数据共享受限的工业背景下实现高效的错误严重性预测。

Abstract: Bug severity prediction is a critical task in software engineering as it
enables more efficient resource allocation and prioritization in software
maintenance. While AI-based analyses and models significantly require access to
extensive datasets, industrial applications face challenges due to data-sharing
constraints and the limited availability of labeled data. In this study, we
investigate method-level bug severity prediction using source code metrics and
Large Language Models (LLMs) with two widely used datasets. We compare the
performance of models trained using centralized learning, federated learning,
and synthetic data generation. Our experimental results, obtained using two
widely recognized software defect datasets, indicate that models trained with
federated learning and synthetic data achieve comparable results to centrally
trained models without data sharing. Our finding highlights the potential of
privacy-preserving approaches such as federated learning and synthetic data
generation to enable effective bug severity prediction in industrial context
where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository:
https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.

</details>


### [10] [On the Feasibility of Deduplicating Compiler Bugs with Bisection](https://arxiv.org/abs/2506.23281)
*Xintong Zhou,Zhenyang Xu,Chengnian Sun*

Main category: cs.SE

TL;DR: 本文提出用二分法结合优化信息的新方法BugLens进行编译器bug去重，比现有方法更省人工且效率高，实用性强。


<details>
  <summary>Details</summary>
Motivation: 随机测试虽然能有效发现编译器bug，但由于产生大量重复性测试程序，造成bug去重（deduplication）极具挑战。现有方法依赖程序分析，计算开销大、泛化能力有限，亟需新的高效去重方法。

Method: 作者系统性研究了“二分法”（bisection）用于编译器bug去重的可行性，并提出BugLens方法，以二分法辅助优化去重流程，同时结合对触发bug的优化过程分析，减少漏判。该方法不依赖传统特征分析，主打通用性和高效性。

Result: 在4个实际数据集上的实验显示，BugLens比现有领先方法Tamer和D3更高效，分别节省了26.98%和9.64%的人力成本，且能识别相同数量的独特bug。

Conclusion: 二分法简单、高泛化能力，结合优化信息能显著提升bug去重效率，为编译器错误去重提供了实用的现实解决方案。

Abstract: Random testing has proven to be an effective technique for compiler
validation. However, the debugging of bugs identified through random testing
presents a significant challenge due to the frequent occurrence of duplicate
test programs that expose identical compiler bugs. The process to identify
duplicates is a practical research problem known as bug deduplication. Prior
methodologies for compiler bug deduplication primarily rely on program analysis
to extract bug-related features for duplicate identification, which can result
in substantial computational overhead and limited generalizability. This paper
investigates the feasibility of employing bisection, a standard debugging
procedure largely overlooked in prior research on compiler bug deduplication,
for this purpose. Our study demonstrates that the utilization of bisection to
locate failure-inducing commits provides a valuable criterion for
deduplication, albeit one that requires supplementary techniques for more
accurate identification. Building on these results, we introduce BugLens, a
novel deduplication method that primarily uses bisection, enhanced by the
identification of bug-triggering optimizations to minimize false negatives.
Empirical evaluations conducted on four real-world datasets demonstrate that
BugLens significantly outperforms the state-of-the-art analysis-based
methodologies Tamer and D3 by saving an average of 26.98% and 9.64% human
effort to identify the same number of distinct bugs. Given the inherent
simplicity and generalizability of bisection, it presents a highly practical
solution for compiler bug deduplication in real-world applications.

</details>


### [11] [What Challenges Do Developers Face When Using Verification-Aware Programming Languages?](https://arxiv.org/abs/2506.23696)
*Francisco Oliveira,Alexandra Mendes,Carolina Carreira*

Main category: cs.SE

TL;DR: 作者通过论坛内容分析和问卷调查，发现学习难度和可用性问题导致验证感知语言采纳率低，建议优化工具界面、教育材料和集成，促使其更易用也更易普及。


<details>
  <summary>Details</summary>
Motivation: 虽然验证感知（Verification-Aware, VA）编程语言在提升软件可靠性方面有显著优势，但实际中采用率很低。作者希望通过分析开发者讨论，找出影响VA语言采用的障碍。

Method: 本文采用论坛话题建模分析与开发者问卷调查相结合的方法，对开发者采用VA语言的阻碍因素进行探究。

Result: 研究发现，VA语言存在学习曲线陡峭和可用性不足等主要问题。此外，还归纳出提升可用性和采用度的方法，包括简化工具界面、提供更好的教育资源、以及改善与开发环境的集成。

Conclusion: 改进VA语言的工具界面、教育资源和环境集成能有效提升其可用性和被采纳程度，为推广形式化验证工具和方法提供了新思路。

Abstract: Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.

</details>


### [12] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Main category: cs.SE

TL;DR: 作者提出用LLMs新方法，从开发文档中自动生成合规的隐私需求，主流大模型效果出色（F1大于0.8），经调优后表现更优，有助于开发过程隐私保护。


<details>
  <summary>Details</summary>
Motivation: 过往分析人员和开发人员常常将隐私视为安全的附属品，导致软件不合规和用户隐私被侵犯。当前方法多聚焦于合规检查，缺乏自动化从开发文档中提取隐私需求的机制。

Method: 提出了一种基于chain-of-thought prompting（CoT）、in-context-learning（ICL）和大语言模型（LLMs）的新方法，从软件开发前及开发过程中的各种文档中提取隐私行为，并据此生成用户故事格式的隐私需求。

Result: 主流LLM（如GPT-4o和Llama 3）能以F1分数超过0.8的准确率识别隐私行为并生成隐私用户故事，调整模型参数后表现还能进一步提升。

Conclusion: 该方法展示了LLMs在生成软件开发相关隐私需求方面的巨大潜力，并为如何优化LLMs以适应开发文档提出见解。

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [13] [Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation](https://arxiv.org/abs/2506.23034)
*Hao Yan,Swapneel Suhas Vaidya,Xiaokuan Zhang,Ziyu Yao*

Main category: cs.SE

TL;DR: 本研究系统评估当前LLM生成与修复代码安全漏洞能力，发现细致提示和反馈有助于提升模型安全性，并向开发者提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽能强大地自动生成代码，但常忽略安全问题，易生成含可被攻击者利用的安全漏洞代码。指导LLM生成安全代码和修复漏洞的有效策略研究不足，实际修复效果缺乏系统分析。

Method: 评估多种主流LLM，包括开源和专有模型，采用基准漏洞数据集，比较其生成和修复安全漏洞的能力，结合定量和定性分析。

Result: 先进的LLM如果有漏洞提示和详细反馈，可以更好地避免和修复安全漏洞；同时指出目前LLM自动代码生成仍具安全挑战。给出实际减缓代码漏洞的方法建议。

Conclusion: LLMs在自动生成代码时容易导致不安全代码，但通过提示和细致反馈可以显著提升其安全性。文章还提出了实际建议帮开发者减少漏洞。

Abstract: Large Language Models (LLMs) have become powerful tools for automated code
generation. However, these models often overlook critical security practices,
which can result in the generation of insecure code that contains
vulnerabilities-weaknesses or flaws in the code that attackers can exploit to
compromise a system. However, there has been limited exploration of strategies
to guide LLMs in generating secure code and a lack of in-depth analysis of the
effectiveness of LLMs in repairing code containing vulnerabilities. In this
paper, we present a comprehensive evaluation of state-of-the-art LLMs by
examining their inherent tendencies to produce insecure code, their capability
to generate secure code when guided by self-generated vulnerability hints, and
their effectiveness in repairing vulnerabilities when provided with different
levels of feedback. Our study covers both proprietary and open-weight models
across various scales and leverages established benchmarks to assess a wide
range of vulnerability types. Through quantitative and qualitative analyses, we
reveal that although LLMs are prone to generating insecure code, advanced
models can benefit from vulnerability hints and fine-grained feedback to avoid
or fix vulnerabilities. We also provide actionable suggestions to developers to
reduce vulnerabilities when using LLMs for code generation.

</details>


### [14] [HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing](https://arxiv.org/abs/2506.23063)
*Guangfa Lyu,Zhenzhong Cao,Xiaofei Ren,Fengyu Wang*

Main category: cs.SE

TL;DR: 本文提出了混合反馈定向灰盒模糊测试框架HF-DGF，通过更精准和多样化的反馈机制显著提升了崩溃复现速度和导向性，其实验结果表明在同类工具中表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前主流的定向灰盒模糊测试（DGF）工具由于运行时反馈不足，难以高效地到达目标位置或全面探索状态空间，限制了其漏洞挖掘能力。提升反馈粒度和多样性是亟需解决的问题。

Method: 提出了一种新型的定向灰盒模糊测试框架HF-DGF。该框架通过混合反馈机制（结合控制流距离、值流影响分数和切片覆盖）指导种子调度。为实现更精准控制流距离反馈，设计了基于虚拟跨过程控制流图的反向步进算法；为提升状态空间探索效率，引入值流影响及其度量方法；针对运行时开销，采用了选择性插桩策略优化性能。

Result: 在41个真实漏洞上的实验显示，HF-DGF在崩溃复现速度上显著优于现有工具，比如分别比AFL、AFLGo、WindRanger、DAFL、Beacon平均快5.05、5.79、73.75、2.56和8.45倍。在全部fuzzer均能引发崩溃的场景下，HF-DGF具备最低的代码覆盖率，证明其导向性和效率更高。同时，其静态分析效率也优于AFLGo、WindRanger、DAFL和Beacon。

Conclusion: HF-DGF通过融合多维度反馈并优化调度和插桩，在漏洞验证与复现任务中表现出显著优越性，尤其是在高效和定向性方面，对提高定向模糊测试工具的实际效能具有积极意义。

Abstract: Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for
crash reproduction and patch testing, leveraging its capability to precisely
navigate toward target locations and exploit vulnerabilities. However, current
DGF tools are constrained by insufficient runtime feedback, limiting their
efficiency in reaching targets and exploring state spaces. This study presents
HF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is
guided by a hybrid feedback mechanism integrating control-flow distance,
value-flow influence score, and slice coverage. To enable precise control-flow
distance feedback, we propose a backward-stepping algorithm to calculate basic
block-level seed distances on a virtual inter-procedural control-flow graph
(ICFG). For effective state space exploration, we introduce value-flow
influence and a corresponding metric, the value-flow influence score.
Additionally, to mitigate runtime overhead from hybrid feedback, we adopt a
novel selective instrumentation strategy. Evaluations on 41 real-world
vulnerabilities show HF-DGF outperforms existing tools: it achieves crash
reproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75
times faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times
faster than Beacon on average. Notably, when all fuzzers triggered crashes,
HF-DGF exhibited the lowest code coverage, demonstrating superior
directionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and
Beacon in static analysis efficiency.

</details>


### [15] [Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search](https://arxiv.org/abs/2506.23100)
*Jiayi Zhang,Kai Huang,Jian Zhang,Yang Liu,Chunyang Chen*

Main category: cs.SE

TL;DR: 本文提出的ReinFix框架通过集成静态分析与历史修复经验，让LLM更有效生成正确补丁，大幅超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLM）驱动的自动程序修复方法在补丁生成的上下文相关性和准确性方面存在不足，主要问题在于忽略了实际修复中关键的“修复成分”。

Method: 提出了ReinFix框架，让LLM在分析和修复时主动搜索与问题相关的修复成分。推理阶段利用静态分析工具检索如变量定义等内部成分，辅助LLM定位根因；解决阶段则结合历史修复记录中同类型bug的修复经验，辅助模型制定正确修复方案。

Result: 在Defects4J V1.2上，ReinFix比当前最优基线方法多修复了32个bug，总计修复146个；在Defects4J V2.0上多修复38个bug。此外，在没有数据泄露风险的新基准数据集上也保持最佳表现。

Conclusion: ReinFix能够显著提升大语言模型自动程序修复的效果和覆盖范围，超越现有SOTA基线，在主流数据集和无数据泄漏基准测试下均取得了最佳结果。

Abstract: Automated Program Repair (APR) techniques aim to automatically fix buggy
programs. Among these, Large Language Model-based (LLM-based) approaches have
shown great promise. Recent advances demonstrate that directly leveraging LLMs
can achieve leading results. However, these techniques remain suboptimal in
generating contextually relevant and accurate patches, as they often overlook
repair ingredients crucial for practical program repair. In this paper, we
propose ReinFix, a novel framework that enables LLMs to autonomously search for
repair ingredients throughout both the reasoning and solution phases of bug
fixing. In the reasoning phase, ReinFix integrates static analysis tools to
retrieve internal ingredients, such as variable definitions, to assist the LLM
in root cause analysis when it encounters difficulty understanding the context.
During the solution phase, when the LLM lacks experience in fixing specific
bugs, ReinFix searches for external ingredients from historical bug fixes with
similar bug patterns, leveraging both the buggy code and its root cause to
guide the LLM in identifying appropriate repair actions, thereby increasing the
likelihood of generating correct patches. Evaluations on two popular benchmarks
(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over
SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the
baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than
the SOTA. Importantly, when evaluating on the recent benchmarks that are free
of data leakage risk, ReinFix also maintains the best performance.

</details>


### [16] [From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers](https://arxiv.org/abs/2506.23234)
*Peerachai Banyongrakkul,Mansooreh Zahedi,Patanamon Thongtanunam,Christoph Treude,Haoyu Gao*

Main category: cs.SE

TL;DR: 本文系统分析了开源项目中与预训练模型相关的问题报告，总结了开发者常见的七大挑战类别，并发现此类问题的解决时间普遍较长，对后续PTM复用及改进实践具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练模型因其出色性能和易获得性被广泛采用，但关于下游开发者在实际软件系统复用PTMs时遇到的具体挑战和实际障碍却较少被系统性探讨。填补知识空白，对实践与研究均有意义。

Method: 定性地创建并分析了来自31个开源GitHub项目的840份与PTM相关的问题报告，并系统性地开发了PTM相关挑战的分类法，同时对比了已有的相关分类法。采用统计方法分析了不同类型问题的解决时长。

Result: 识别并总结了下游开发者在复用PTMs时面临的七大主要挑战类别（如模型使用、模型性能、输出质量等），并发现PTM相关问题的解决时间显著长于非PTM问题，不同挑战类别的时长有明显差异。

Conclusion: 下游开发者在复用预训练模型（PTMs）时面临诸多挑战，并且相关问题的解决周期普遍较长，各问题类别间的解决时长也存在显著差异。

Abstract: Pre-trained models (PTMs) have gained widespread popularity and achieved
remarkable success across various fields, driven by their groundbreaking
performance and easy accessibility through hosting providers. However, the
challenges faced by downstream developers in reusing PTMs in software systems
are less explored. To bridge this knowledge gap, we qualitatively created and
analyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub
projects. We systematically developed a comprehensive taxonomy of PTM-related
challenges that developers face in downstream projects. Our study identifies
seven key categories of challenges that downstream developers face in reusing
PTMs, such as model usage, model performance, and output quality. We also
compared our findings with existing taxonomies. Additionally, we conducted a
resolution time analysis and, based on statistical tests, found that
PTM-related issues take significantly longer to be resolved than issues
unrelated to PTMs, with significant variation across challenge categories. We
discuss the implications of our findings for practitioners and possibilities
for future research.

</details>


### [17] [Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning](https://arxiv.org/abs/2506.23534)
*Siyu Chen,Jiongyi Yang,Xiang Chen,Menglin Zheng,Minnan Wei,Xiaolin Ju*

Main category: cs.SE

TL;DR: 本文提出EDAT+MTL联合方法，在样本稀缺和类别不平衡情形下，有效提升了漏洞类型预测及行级检测性能，尤其是对稀有漏洞。方法简单有效，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统面临越来越多的安全漏洞威胁，尤其是在样本匮乏、类别极度不平衡的数据下，常见的漏洞类型预测（VTP）和代码行级别漏洞检测（LVD）效果一般。此外，相关研究往往将两类任务割裂，忽略了二者之间的内在联系，限制了模型性能提升。

Method: 本文提出将Embedding-Layer Driven Adversarial Training（EDAT）与多任务学习（MTL）结合的统一方法。EDAT通过对标识符嵌入引入基于语义重要性的对抗扰动，提高模型鲁棒性；MTL通过共享表示及任务关系，提升VTP和LVD综合性能。

Result: 实验结果显示，所提方法在VTP和LVD任务上均优于最先进的基线方法。VTP方面，各项指标（准确率、精确率、召回率和F1）显著提升，尤其是稀有漏洞类型检测。LVD方面，提升了行级检测准确率并显著减少误报。

Conclusion: 将EDAT与MTL结合，为漏洞类型预测和行级漏洞检测任务提供了统一、有效的解决方案，实验效果突出，值得进一步研究。

Abstract: Context: Software vulnerabilities pose a significant threat to modern
software systems, as evidenced by the growing number of reported
vulnerabilities and cyberattacks. These escalating trends underscore the urgent
need for effective approaches that can automatically detect and understand
software vulnerabilities. Objective: However, the scarcity of labeled samples
and the class imbalance issue in vulnerability datasets present significant
challenges for both Vulnerability Type Prediction (VTP) and Line-level
Vulnerability Detection (LVD), especially for rare yet critical vulnerability
types. Moreover, most existing studies treat VTP and LVD as independent tasks,
overlooking their inherent correlation, which limits the potential to leverage
shared semantic patterns across tasks. Methods: To address these limitations,
we propose a unified approach that integrates Embedding-Layer Driven
Adversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT
enhances model robustness by introducing adversarial perturbations to
identifier embeddings, guided by semantic importance. Meanwhile, MTL improves
overall performance by leveraging shared representations and inter-task
correlations between VTP and LVD. Results: Extensive experiments demonstrate
that our proposed approach outperforms state-of-the-art baselines on both VTP
and LVD tasks. For VTP, it yields notable improvements in accuracy, precision,
recall, and F1-score, particularly in identifying rare vulnerability types.
Similarly, for LVD, our approach enhances line-level detection accuracy while
significantly reducing false positives. Conclusion: Our study demonstrates that
combining EDAT with MTL provides a unified solution that improves performance
on both tasks and warrants further investigation.

</details>


### [18] [Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance](https://arxiv.org/abs/2506.23535)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 论文分析比较了主流大语言模型生成C++代码对MISRA C++安全标准的遵循，结论是这些模型生成的自动代码尚需严格人工检查以确保安全性。


<details>
  <summary>Details</summary>
Motivation: 由于安全关键系统故障会造成灾难性后果，因此该领域对软件编写标准极为严格。随着LLM在自动代码生成领域表现出色，评估其在遵循安全编码规范方面的能力成为必要。

Method: 作者对OpenAI ChatGPT、Google Gemini、DeepSeek、Meta AI和Microsoft Copilot等主流LLM生成的C++代码进行了比较分析，重点考查这些代码是否符合MISRA C++标准。

Result: 论文发现，不同LLM生成的代码在对MISRA C++标准的符合程度上存在差异，自动代码仍需人工严格审核，不能直接用于安全关键系统。

Conclusion: 该文通过比较分析主流大语言模型（LLM）生成的C++代码对MISRA C++规范的遵循情况，指出这些自动生成的代码在应用于安全关键系统前需要严格审查。

Abstract: Safety-critical systems are engineered systems whose failure or malfunction
could result in catastrophic consequences. The software development for
safety-critical systems necessitates rigorous engineering practices and
adherence to certification standards like DO-178C for avionics. DO-178C is a
guidance document which requires compliance to well-defined software coding
standards like MISRA C++ to enforce coding guidelines that prevent the use of
ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have
demonstrated significant capabilities in automatic code generation across a
wide range of programming languages, including C++. Despite their impressive
performance, code generated by LLMs in safety-critical domains must be
carefully analyzed for conformance to MISRA C++ coding standards. In this
paper, I have conducted a comparative analysis of the C++ code generated by
popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and
Microsoft Copilot for compliance with MISRA C++.

</details>


### [19] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Main category: cs.SE

TL;DR: 本文提出了能更有效检测开源项目漏洞的QLPro系统，结合LLM与静态分析工具，检测能力远超主流工具，并能发现安全社区未知的新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析工具在检测开源项目中的漏洞时存在漏检问题，检测效果不理想。需要一种更全面且准确的检测方法。

Method: 提出了QLPro框架，将大语言模型（LLM）与静态分析工具系统性结合，提高对开源项目中漏洞的检测能力。同时构建了JavaTest数据集用于评估。

Result: QLPro在JavaTest数据集上检测到41个漏洞，优于CodeQL的24个。QLPro还发现了6个此前未知的新漏洞，其中2个已被证实为0-day漏洞。

Conclusion: QLPro系统性整合了LLM与静态分析工具，能更全面检测开源项目中的漏洞，优于现有主流工具，并能发现新型和高危漏洞。

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


### [20] [Towards a Science of Developer eXperience (DevX)](https://arxiv.org/abs/2506.23715)
*Benoit Combemale*

Main category: cs.SE

TL;DR: 将开发者体验（DevX）提升为软件工程独立研究课题，梳理其意义、影响与挑战，呼吁社区重视并推进以人为本的软件工程研究。


<details>
  <summary>Details</summary>
Motivation: 现代社会软件无处不在，软件服务复杂且广泛，当前软件工程多专注技术挑战，而开发者体验（DevX）这个关乎人类因素的重要议题被忽视。推动DevX研究有助于实现持续、高效和包容性的软件开发方法。

Method: 立足于现有对DevX测量与提升的研究，总结关键动因、科学促进因素及涉及学科交叉，分析DevX对开发活动与生产力的影响，并提出科学挑战，号召社区行动。

Result: 提出DevX应被视为独立研究方向，指明DevX在协作性更强、应用多元化环境下对核心开发活动和生产力有深刻影响，并罗列该领域的科学挑战与研究启示。

Conclusion: 呼吁软件工程领域正式重视开发者体验（DevX），加强以人为本的研究，推动形成新的研究范式和社区关注。

Abstract: As software continues to permeate nearly every facet of modern life, the
complexity and ubiquity of digital services underscore the need for
sustainable, effective, and inclusive software development practices. Although
software engineering has made significant progress in technical challenges
since its inception, the human experience of those involved in software
creation, broadly defined as developers, remains underexplored. This column
advocates for the formal recognition of Developer eXperience (DevX) as a
distinct research field. We argue that DevX profoundly influences critical
development activities and overall productivity, especially as development
becomes increasingly collaborative and diverse in terms of application domains.
Building on existing efforts to measure and enhance DevX, we identify key
rationales, scientific enablers, and interdisciplinary intersections that
support this emerging discipline. We also outline the core scientific
challenges ahead, aiming to call for actions from the research community and to
promote more human-centered approaches to software engineering.

</details>


### [21] [A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications](https://arxiv.org/abs/2506.23749)
*Boyang Yang,Zijian Cai,Fengling Liu,Bach Le,Lingming Zhang,Tegawendé F. Bissyandé,Yang Liu,Haoye Tian*

Main category: cs.SE

TL;DR: 本文对近63个基于LLM的自动程序修复系统进行了范式归类，分析了技术权衡与挑战，并提出未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 面对LLM显著改变自动程序修复领域，系统梳理最新方法，揭示关键技术权衡，为未来研究指明方向。

Method: 对2022年至2025年间63个基于大语言模型的自动程序修复系统进行分类，总结其四种范式，并分析增强方式及其权衡。

Result: 提出基于分类的关键技术权衡分析，指出现有系统优劣及适用场景，归纳现存问题，并提出结合多种技术的新研究方向。

Conclusion: 持续的挑战包括验证语义正确性、修复大型仓库缺陷以及降低大模型成本。结合人类反馈、仓库检索、代码分析、成本感知规划是未来的研究方向。

Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We
categorize the recent 63 LLM-based APR systems published from January 2022 to
June 2025 into four paradigms, and show how retrieval- or analysis-augmented
contexts strengthen any of them. This taxonomy clarifies key trade-offs:
fine-tuning delivers strong task alignment at high training cost; prompting
enables rapid deployment but is limited by prompt design and context windows;
procedural pipelines offer reproducible control with moderate overhead; agentic
frameworks tackle multi-hunk or cross-file bugs at the price of increased
latency and complexity. Persistent challenges include verifying semantic
correctness beyond test suites, repairing repository-scale defects, and
lowering the costs of LLMs. We outline research directions that combine
lightweight human feedback, repository-aware retrieval, code analysis, and
cost-aware planning to advance reliable and efficient LLM-based APR.

</details>


### [22] [Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead](https://arxiv.org/abs/2506.23762)
*Hongzhou Rao,Yanjie Zhao,Xinyi Hou,Shenao Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文从软件工程视角系统梳理了大语言模型开发流程的六大阶段、主要挑战及未来研究方向，为后续LLM研发提供理论参考。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型发展迅速，但其整个开发周期中存在诸多复杂挑战，现有研究却很少从软件工程角度系统梳理这些挑战及其解决方案。本文旨在填补该领域的研究空白。

Method: 在文献调研基础上，将LLM开发过程划分为六个阶段，对每一阶段的研究现状、挑战与解决思路进行了系统分析与总结。

Result: 归纳了LLM六大开发阶段各自面临的主要挑战，并提出了针对性的未来研究方向，为后续学者和工程师提供指导。

Conclusion: 本文系统性地梳理了大语言模型（LLM）开发生命周期中的关键挑战，并针对各阶段提出了未来研究方向，从软件工程的角度为LLM发展提供了有价值的见解。

Abstract: The rapid advancement of large language models (LLMs) has redefined
artificial intelligence (AI), pushing the boundaries of AI research and
enabling unbounded possibilities for both academia and the industry. However,
LLM development faces increasingly complex challenges throughout its lifecycle,
yet no existing research systematically explores these challenges and solutions
from the perspective of software engineering (SE) approaches. To fill the gap,
we systematically analyze research status throughout the LLM development
lifecycle, divided into six phases: requirements engineering, dataset
construction, model development and enhancement, testing and evaluation,
deployment and operations, and maintenance and evolution. We then conclude by
identifying the key challenges for each phase and presenting potential research
directions to address these challenges. In general, we provide valuable
insights from an SE perspective to facilitate future advances in LLM
development.

</details>


### [23] [Requirements for Active Assistance of Natural Questions in Software Architecture](https://arxiv.org/abs/2506.23898)
*Diogo Lemos,Ademar Aguiar,Neil B. Harrison*

Main category: cs.SE

TL;DR: 本文分析了自然问题在软件架构设计中的重要性，提出对应的生命周期和支持环境，并通过专家调研验证了其成效，对提升架构知识保护和协作具有显著意义。


<details>
  <summary>Details</summary>
Motivation: 在软件架构设计过程中，自然问题对于关键架构决策和知识传承非常重要。但这些问题经常被忽视或处理不当，导致知识丢失、资源浪费和系统架构难以理解。该研究旨在深入理解自然问题的生命周期，并提供更有效的支持环境。

Method: 作者基于已有文献、需求研讨会，以及三轮设计迭代，提出了自然问题的生命周期，并提取了相关环境的功能和非功能需求。随后，通过专家问卷调查，对需求及功能进行了分析和验证。

Result: 明确了自然问题生命周期，界定出该支持环境应具备的关键需求，并证明这些需求和功能的有效性优于传统方法，能更好促进协作、决策和知识保存。

Conclusion: 提出的自然问题生命周期和支持环境能有效改善架构知识的管理与应用，对软件开发流程有积极促进作用。

Abstract: Natural questions are crucial to shaping key architectural decisions and
preserving architectural knowledge. They arise organically during the
architectural design process, often resulting from the existing architectural
experience of the designer and the distinctive characteristics of the system
being designed. However, natural questions are often mismanaged or ignored,
which can lead to architectural drift, knowledge loss, inefficient resource
use, or poor understandability of the system's architecture. We aim to better
understand the lifecycle of natural questions, its key requirements, challenges
and difficulties, and then to envision an assisted environment to properly
support it. The environment should be adaptable and responsive to real-world
constraints and uncertainties by seamlessly integrating knowledge management
tools and artificial intelligence techniques into software development
workflows. Based on existing literature, a requirements workshop, and three
design iterations, we proposed a lifecycle for natural questions and elicited
essential functional and non-functional requirements for such an environment.
At last, the results of a survey conducted with experts helped to analyze and
validate the elicited requirements and proposed features for the environment to
enhance collaboration, decision-making, and the preservation of architectural
knowledge more effectively than conventional methods.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [On the Reachability Problem for Two-Dimensional Branching VASS](https://arxiv.org/abs/2506.22561)
*Clotilde Bizière,Thibault Hilaire,Jérôme Leroux,Grégoire Sutre*

Main category: cs.LO

TL;DR: 本文解决了二维BVASS（分支状态向量加法系统）的可达性问题，证明其是可判定的，并且可达集可用可计算的半线性形式表示。但高维BVASS的情况仍未解决。


<details>
  <summary>Details</summary>
Motivation: 自动机理论和并发系统建模中，VASS（状态向量加法系统）及其分支扩展BVASS是核心模型之一。研究其可达性问题（是否能从初始状态到达目标状态）对于分析系统行为至关重要。虽然VASS的可达性问题已被解决，但BVASS在高维度下依然未解。该文聚焦于2维BVASS的此类问题。

Method: 作者针对二维BVASS，使用了理论计算方法对其可达性问题进行研究。进一步，作者通过数学工具，证明了其可达集可用可计算的半线性表达式表述。

Result: 作者证明了二维BVASS的可达性问题是可判定的，并且其可达集具有可计算的半线性结构。对于更高维度的BVASS，可达性问题是否可判定依然是开放问题。

Conclusion: 二维BVASS的可达性问题是可判定的，并有结构性的可计算表达，而高维情形尚未解决。

Abstract: Vectors addition systems with states (VASS), or equivalently Petri nets, are
arguably one of the most studied formalisms for the modeling and analysis of
concurrent systems. A central decision problem for VASS is reachability:
whether there exists a run from an initial configuration to a final one. This
problem has been known to be decidable for over forty years, and its complexity
has recently been precisely characterized. Our work concerns the reachability
problem for BVASS, a branching generalization of VASS. In dimension one, the
exact complexity of this problem is known. In this paper, we prove that the
reachability problem for 2-dimensional BVASS is decidable. In fact, we even
show that the reachability set admits a computable semilinear presentation. The
decidability status of the reachability problem for BVASS remains open in
higher dimensions.

</details>


### [25] [From MBQI to Enumerative Instantiation and Back](https://arxiv.org/abs/2506.22584)
*Marek Dančo,Petra Hozzová,Mikoláš Janota*

Main category: cs.LO

TL;DR: 本文分析了两种主流SMT量词实例化方法（MBQI和EI），提出并实验了结合二者的新算法，并报告了初步效果。


<details>
  <summary>Details</summary>
Motivation: MBQI虽然能保证找到反例，但实例化较弱；而EI虽系统枚举保证完备性，但枚举到的项未必是真正反例，因此有必要结合两者优势。

Method: 提出并实验了一种结合MBQI与EI的方法，将语义上的MBQI和句法上的EI相结合，并通过算法实验验证该方法。

Result: 初步实验结果报告了结合方法的表现，并为两种实例化方法的关系提供了实证数据。

Conclusion: 本文初步实验了结合模型驱动量词实例化（MBQI）和枚举实例化（EI）的算法，并探讨了两者之间的关系。

Abstract: This work investigates the relation between model-based quantifier
instantiation (MBQI) and enumerative instantiation (EI) in Satisfiability
Modulo Theories (SMT). MBQI operates at the semantic level and guarantees to
find a counterexample to a given a non-model. However, it may lead to weak
instantiations. In contrast, EI strives for completeness by systematically
enumerating terms at the syntactic level. However, such terms may not be
counter-examples. Here we investigate the relation between the two techniques
and report on our initial experiments of the proposed algorithm that combines
the two.

</details>


### [26] [Compositional Control-Driven Boolean Circuits](https://arxiv.org/abs/2506.22687)
*Damian Arellanes*

Main category: cs.LO

TL;DR: 本文提出了一种基于余极限的布尔电路组合方式，首次系统地解决了布尔电路组合性的理论难题，定义了多种电路组合操作符，并引入控制驱动布尔电路的新计算模型，展示了其强大的计算能力以及对模块化和形式化推理的支持。


<details>
  <summary>Details</summary>
Motivation: 布尔电路长期作为研究数字组件逻辑结构及计算行为的抽象模型，但其组合性（即在不关注内部结构的情况下组合电路以支持模块化和形式化推理）一直被忽视或较为非正式地对待。本文旨在填补这个理论空白。

Method: 提出基于余极限（colimit）的电路组合操作符，分别定义用于构建串行、并行、分支和迭代电路的操作符，并以此描述具备显式控制流的电路复合形式，形成一种新的控制驱动型布尔电路计算模型。

Result: 该模型至少与传统布尔电路模型等价，即能够非均匀地计算任意长度输入的任意布尔函数。

Conclusion: 通过形式化的colimit方法，实现了布尔电路的可组合性构造，推动了模块化和形式化推理在该领域的发展。控制驱动布尔电路模型在理论上是完备的，具有和经典模型相同的计算能力。

Abstract: Boolean circuits abstract away from physical details to focus on the logical
structure and computational behaviour of digital components. Despite they have
been studied for many decades, compositionality has been widely ignored or
examined in an informal manner, which is a property for combining circuits
without delving into their internal structure, while supporting modularity and
formal reasoning. In this paper, we address this longstanding theoretical gap
by proposing colimit-based operators for compositional circuit construction. We
define separate operators for forming sequential, parallel, branchial and
iterative circuits. As composites encapsulate explicit control flow, a new
model of computation emerges which we refer to as (families of) control-driven
Boolean circuits. We show how this model is at least as powerful as its
classical counterpart. In other words, it is able to non-uniformly compute any
Boolean function on inputs of arbitrary length.

</details>


### [27] [Questions as cognitive filters](https://arxiv.org/abs/2506.22735)
*Willem Conradie,Krishna Manoorkar,Alessandra Palmigiano,Apostolos Tzimoulis,Nachoem Wijnberg*

Main category: cs.LO

TL;DR: 提出了逻辑-代数框架，形式化描述多智能体协调决策时对“相关性”特征的认知与交互，分析了不同规则下的理论性质，并明确框架表达力的边界。


<details>
  <summary>Details</summary>
Motivation: 多智能体决策过程通常需要在不同特征或信息相关性上达成共识，但缺乏形式化工具来刻画各智能体对哪些特征应被纳入决策的认知立场。本文旨在建立一种形式化框架来描述和分析这一过程。

Method: 提出了以“询问性议程”（interrogative agendas）为核心的逻辑-代数框架，将智能体的相关性判断形式化为等价关系，并分析了不同‘胜出规则’下，相关议程子格的结构。引入两类结构：相关议程格以及智能体联盟的布尔代数，通过它们共同建模多智能体讨论过程。最后探讨了该框架下可刻画和不可刻画的智能体-议程交互条件。

Result: 成功构建了一个覆盖多种决策场景的理论模型，能够形式化并区分不同胜出规则下的议程关系；明确指出哪些交互条件可通过该框架表达，哪些不可。

Conclusion: 本文为多智能体讨论-决策中的相关性认知和交互提供了统一且可扩展的形式化工具，对分析智能体协作和共识机制具有理论指导意义。

Abstract: In this paper, we develop a logico-algebraic framework for modeling
decision-making through deliberation in multi-agent settings. The central
concept in this framework is that of interrogative agendas, which represent the
cognitive stances of agents regarding which features should be considered
relevant in the final decision. We formalize an agent's interrogative agenda as
an equivalence relation that identifies outcomes differing only in aspects the
agent deems irrelevant. Moreover, we characterize the sublattices of the
resulting lattice that correspond to relevant interrogative agendas for
deliberation scenarios governed by different ``winning rules." We then
introduce a two-sorted logico-algebraic structure-comprising the lattice of
relevant interrogative agendas and the Boolean algebras of agent coalitions-to
model the interaction between agents and agendas during deliberation. Finally,
we discuss which interaction conditions can and cannot be defined within this
framework.

</details>


### [28] [Model-theoretic Forcing in Transition Algebra](https://arxiv.org/abs/2506.22828)
*Go Hashimoto,Daniel Găină*

Main category: cs.LO

TL;DR: 对Transition Algebra（动态逻辑+多类型一阶逻辑）研究发现：因传递闭包的强表达性，部分经典模型论定理（如向上Lowenheim-Skolem、紧致性等）不再成立。作者通过推广forcing技术引入新方法，并发展出适用于有限或构造型转移代数的完全性证明，为相关逻辑系统分析提供了理论基础和新方法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究Transition Algebra（一种结合多类型一阶逻辑与动态逻辑特性的新逻辑系统）中的Lowenheim-Skolem定理和略类型定理，探索其在引入转移关系（包括并、复合、传递闭包等动态逻辑操作符）下的表现和极限。

Method: 作者分析了Transition Algebra中模型论特性的失效（如向上Lowenheim-Skolem定理、紧致性、联合Robinson一致性），并发展了一种推广自Keisler forcing技术的方法，利用带有定向图的签名体系，以适应多类型和传递闭包情形；此外，扩展并完善了Transition Algebra的证明系统，以支持构造型与有限转移代数片段的完全性。

Result: 证明了在Transition Algebra中传统的向上Lowenheim-Skolem定理、任意紧致性与联合一致性性质会因传递闭包的表达性而失效；同时，借助新的generalized forcing技术，得以证明下行Lowenheim-Skolem和略类型定理在该系统仍成立，且为构造型/有限转移代数片段建立了完全性结果。

Conclusion: Transition Algebra在动态逻辑操作的背景下，呈现出与传统一阶逻辑明显不同的模型论行为，部分核心定理失效，但可凭借新的技术恢复部分重要元性质，为多类型及动态系统的逻辑分析提供了新工具。

Abstract: We study L\"owenheim-Skolem and Omitting Types theorems in Transition
Algebra, a logical system obtained by enhancing many sorted first-order logic
with features from dynamic logic. The sentences we consider include
compositions, unions, and transitive closures of transition relations, which
are treated similarly to actions in dynamic logics to define necessity and
possibility operators. We show that Upward L\"owenheim-Skolem theorem, any form
of compactness, and joint Robinson consistency property fail due to the
expressivity of transitive closures of transitions. In this non-compact
many-sorted logical system, we develop a forcing technique method by
generalizing the classical method of forcing used by Keisler to prove Omitting
Types theorem. Instead of working within a single signature, we work with a
directed diagram of signatures, which allows us to establish Downward
L\"owenheim-Skolem and Omitting Types theorems despite the fact that models
interpret sorts as sets, possibly empty. Building on a complete system of proof
rules for Transition Algebra, we extend it with additional proof rules to
reason about constructor-based and/or finite transition algebras. We then
establish the completeness of this extended system for a fragment of Transition
Algebra obtained by restricting models to constructor-based and/or finite
transition algebras.

</details>


### [29] [One-Parametric Presburger Arithmetic has Quantifier Elimination](https://arxiv.org/abs/2506.23730)
*Alessio Mansutti,Mikhail R. Starchak*

Main category: cs.LO

TL;DR: 作者提出了对一参参数 Presburger 算术（带乘法和分部函数扩展）进行量词消去的有效算法，证明参与片段的判定问题在 NP 且有多项式规模的解。该工作解决了相关开放问题，并扩展了整数规划和数理逻辑领域的关键工具。


<details>
  <summary>Details</summary>
Motivation: Presburger 算术及其扩展形式在数理逻辑和整数规划等领域具有重要理论价值和应用前景，但含有参数的 Presburger 算术是否可以实现量词消去一直是开放问题，特别是涉及一个固定自由参数（如整数变量 t）时。此前有研究提出此作为待解决的问题。

Method: 作者首次提出了一种对一参参数 Presburger 算术进行量词消去的算法，能够处理所有带有整数多项式分部函数的扩展结构。该算法迭代地消去存在量词块，分为两个主要步骤：一是对现有高效量词消去方法进行改造以适应 ℤ[t] 系数公式，二是引入类似 Bogart 等人“基于 t 的除法法”，每一步均可在非确定性多项式时间内完成。

Result: 实现了对带有分部函数的高级一参参数 Presburger 算术公式的量词消去，证明了其存在满足公式的解的判定问题属于 NP 且最小解的比特长度为多项式级别。

Conclusion: 本文解决了原本开放的一参参数 Presburger 算术的量词消去问题，而且为更广泛的非线性整数规划问题提供了理论基础，将存在片段的可满足性归入 NP 类别，扩展了相关数学逻辑工具的适用范围。

Abstract: We give a quantifier elimination procedure for one-parametric Presburger
arithmetic, the extension of Presburger arithmetic with the function $x \mapsto
t \cdot x$, where $t$ is a fixed free variable ranging over the integers. This
resolves an open problem proposed in [Bogart et al., Discrete Analysis, 2017].
As conjectured in [Goodrick, Arch. Math. Logic, 2018], quantifier elimination
is obtained for the extended structure featuring all integer division functions
$x \mapsto \lfloor{\frac{x}{f(t)}}\rfloor$, one for each integer polynomial
$f$.
  Our algorithm works by iteratively eliminating blocks of existential
quantifiers. The elimination of a block builds on two sub-procedures, both
running in non-deterministic polynomial time. The first one is an adaptation of
a recently developed and efficient quantifier elimination procedure for
Presburger arithmetic, modified to handle formulae with coefficients over the
ring $\mathbb{Z}[t]$ of univariate polynomials. The second is reminiscent of
the so-called "base $t$ division method" used by Bogart et al. As a result, we
deduce that the satisfiability problem for the existential fragment of
one-parametric Presburger arithmetic (which encompasses a broad class of
non-linear integer programs) is in NP, and that the smallest solution to a
satisfiable formula in this fragment is of polynomial bit size.

</details>


### [30] [Querying Attack-Fault-Defense Trees: Property Specification in Smart Grid and Aerospace Case Studies](https://arxiv.org/abs/2506.23789)
*Reza Soltani,Stefano M. Nicoletti,Milan Lopuhaä-Zwakenberg,Mariëlle Stoelinga*

Main category: cs.LO

TL;DR: 本文提出了统一分析安全、保安与防御交互的新框架（AFDL）及其查询语言（LangAFDL），通过案例验证了其实用性，并推动了任务关键系统安全自动化分析的发展。


<details>
  <summary>Details</summary>
Motivation: 目前在安全、保安与防御领域需要统一的分析框架来描述和分析这些领域的交互，而现有方法难以全面覆盖复杂系统中的多重安全因素。

Method: 提出了AFDL这一基于逻辑的推理框架，用于Attack-Fault-Defense Trees（攻击-故障-防御树）模型，并且设计了结构化的领域特定查询语言LangAFDL，使专家能够利用模板直观表达复杂分析目标，支持布尔和量化查询及最小割集分析。

Result: 通过对两个真实案例（Gridshield和Ground Segment as a Service）的查询示例，展示了该方法的表达能力和实用性，为任务关键型系统的自动安全-安保分析奠定了基础。

Conclusion: 该框架有助于实现自动化的安全与安保分析，并为未来工具开发及其设计流程集成提供了方向。

Abstract: This paper introduces AFDL, a logic-based framework for reasoning about
safety, security, and defense interactions in Attack-Fault-Defense Trees, which
is a model that captures all safety, security, and defense domains in a single
framework. We showcase both AFDL and propose a structured domain specific query
language, LangAFDL, which enables domain experts to express complex analysis
goals through intuitive templates. LangAFDL supports both Boolean and
quantified queries as well as minimal cut set analysis, capturing the interplay
between safety, security, and defensive measures. We illustrate the
expressiveness and utility of the approach through representative queries over
two different real-world case studies: Gridshield and Ground Segment as a
Service. The formalization lays the automated safety-security groundwork for
analyses in mission-critical systems and paves the way for future tool
development and integration into design workflows.

</details>


### [31] [Protocol insecurity with finitely many sessions and XOR](https://arxiv.org/abs/2506.24072)
*R Ramanujam,Vaishnavi Sundararajan,S P Suresh*

Main category: cs.LO

TL;DR: 本文为XOR安全性问题提供了更广适用性的证明方法，理论基础更自然、直观，对相关协议安全性分析有较大意义。


<details>
  <summary>Details</summary>
Motivation: 早期文献中的安全性证明对协议类型有一定限制，本文希望在一个更自然、合理的协议模型下，解决同样的XOR安全性问题。

Method: 采用typed terms和well-typed proofs的技巧，结合对协议定义的适当修改，给出了不同于以往文献的安全性证明。

Result: 提出了一种更一般性的XOR不安全性证明，同时所使用的协议定义更贴近实际协议的行为，且理论框架更加通用。

Conclusion: 本文以新的方式证明了XOR在某类协议下存在安全性问题，并放宽了早期证明中的协议适用范围。

Abstract: We present a different proof of the insecurity problem for XOR, solved in by
Chevalier, Kuesters, Rusinowitch and Turuani (2005). Our proof uses the notion
of typed terms and well-typed proofs, and removes a restriction on the class of
protocols to which the [CKRT05] proof applies, by introducing a slightly
different (but very natural) notion of protocols, where honest agent sends are
derivable from previous receives in the same session.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: 本文通过心理语言学特征数据集评估了多类LLMs与人类评分的一致性。结果表明，LLMs在非感官的语言心理特性上较好对齐人类表现，而在感官相关特性（如嗅觉等）上的对齐较弱，突显了当前模型在‘具身’语言理解能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）的评估中，传统上主要关注模型在推理、问答、释义和翻译等具体任务上的表现，并采用客观指标来衡量。但语言还有许多难以量化的特征，如激励性、具体性、性别联想等，这些特征对人类的语言感知和体验至关重要。目前心理语言学通过大规模人类实验获得了大量词汇在人类感知下的评分数据，这为利用这些数据评估LLMs与人类感知之间的一致性提供了契机。

Method: 本文选取代表性的多种LLM模型，利用心理语言学中的Glasgow和Lancaster词汇规范数据库，对模型在十三项语言特征上与人类评分的一致性进行评估。通过与不同类型、不同感官特征词汇的人类评分数据比对，系统分析LLMs对这些特性的对齐能力。

Result: 研究发现，LLMs在Glasgow规范数据库（如激励性、效价、主导性、具体性、可想象性、熟悉度和性别）上的对齐表现普遍优于Lancaster规范数据库（如内感受、味觉、嗅觉、触觉、听觉和视觉）的对齐。这显示当前LLMs在与人类进行基于感官的语言特性对齐方面存在潜在局限，尤其是在缺少体现‘具身认知’的情况下。

Conclusion: 当前LLMs在某些语言心理特性上与人类感知一致性较高，但在涉及人类感官关联的语言特性方面表现不足。这一点可能源于现有模型缺乏具身认知能力。心理语言学数据集在LLMs评估中具有重要价值，能够揭示模型的深层局限性。

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [33] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: 本文提出了多智能体AI系统用于企业结构化文档审核，显著提升一致性、效率并降低偏差，具备广泛行业应用潜力，但在专业细分领域与成本方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 当前企业中对于高度结构化商业文档的自动化审核主要集中在非结构化文本或有限的合规校验，缺乏针对细致审核标准的系统化、智能化方案。

Method: 提出并实现了一个模块化多智能体系统，基于现代AI编排工具（如LangChain、CrewAI、TruLens、Guidance），由多个专门的AI代理分别负责不同审核标准（如模板合规、事实准确性等），可并行或顺序执行，并将输出标准化为可机读的结构化结果；系统含有与人工的反馈循环，实现持续优化与偏差控制。

Result: 系统在信息一致性上达到99%（人工为92%），错误与偏差率减半，单文档平均审核时间由30分钟降至2.5分钟，AI与专家的判定一致率达95%。

Conclusion: 该系统为企业级文档质量保证提供了灵活、可追溯、具扩展性的AI基础，但在高度专业领域仍需人工监督，且大规模部署存在算力成本。

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [34] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: 本文提出融合多个小型语言模型验证大模型答案的框架，在回答拆分和向量检索基础上通过“Yes”概率投票检测幻觉，实验证明该方法可大幅提升答案真实性检测效率和精度。


<details>
  <summary>Details</summary>
Motivation: LLM在问答、检索增强生成等任务中表现优异，但其常出现内容幻觉，且无真实答案时难以检测。提升LLM实际应用可靠性，需要开发有效的幻觉检测方法。

Method: 提出了一种多小型语言模型联合验证LLM生成答案的新框架。方法包括：将LLM给出的答案拆分成单句；利用向量化数据库检索相关上下文；让多个小语言模型基于问题、答案和上下文对每句话生成“Yes”概率进行判断，从而检测答案中的幻觉内容；并在真实数据集上进行了实验。

Result: 在包含100余组问题、答案和上下文的真实数据集上，所提框架比基线提升了10%的F1分数，可有效区分真实回答与幻觉，具有较强的扩展性和效率。

Conclusion: 多小型语言模型整合可有效用于大语言模型（LLM）回答的真实性验证，提高了幻觉检测的准确性，验证方法可应用于学术与实际场景。

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [35] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: 本论文提出PromptAug，利用LLM进行数据增强，在社交媒体冲突检测任务中提升了分类效果，并通过严谨多维分析展示其优势和存在的问题，证明其在敏感任务上的实用性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的冲突行为日益增多，而对有害行为的有效检测模型依赖于高质量的训练数据。但高质量、有标签的数据难以获得，且社交媒体平台对研究数据的访问正变得更加受限，因此需要新的数据扩充方法。

Method: 提出PromptAug，一种基于大语言模型（LLM）的数据增强方法，用于生成训练数据。该方法通过创新的prompt设计，让LLM在不违反安全规则（如不生成攻击性内容）的情况下，生成与冲突相关的文本。同时通过极端数据稀缺场景、量化多样性分析及定性主题分析从多角度评估该方法。

Result: PromptAug在冲突与情绪数据集上，将准确率和F1分数提升了2%，效果具有统计学意义。主题分析发现，增强文本中存在语言流动性、幽默歧义、增强内容歧义和增强内容误解四种问题模式。

Conclusion: PromptAug是一种在冲突检测等敏感任务中有效的数据增强方法，在自然语言处理和社会科学方法论的交叉视角下，提供了独特且实用的评估。

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [36] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本研究提出了适用于本地轻量化语言模型的自增强文本匿名化框架AgentStealth，通过创新的训练流程和效果提升实验，有效保护用户隐私且易于本地部署，相关代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有文本匿名化方法要么影响文本效用，要么依赖云端大型模型且存在高成本和隐私风险。本研究希望在端侧设备上实现高效、低风险的文本匿名化。

Method: 提出了一种自增强的LLM匿名化框架，结合了对比学习、效用自适应控制、数据高质量收集及在线强化学习。在本地化小型语言模型上进行有监督适配与自反馈迭代优化。实验验证了该方法在两个数据集上提升了匿名化效果和文本效用。

Result: 在两个公开数据集上，AgentStealth框架在匿名化有效性上提升了12.3%，文本效用提升了6.8%。该框架轻量化设计，可直接部署于本地设备，避免了云端隐私风险。

Conclusion: 本文提出的AgentStealth框架能够在保护隐私的同时有效地实现文本匿名化，并且具有较高的实际适用性。

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [37] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: 针对图数据跨域预训练面临的领域差异问题，本文提出领域感知的对比学习和注意力机制，显著提升了多域图任务的表现。


<details>
  <summary>Details</summary>
Motivation: 基础模型在自然语言处理和计算机视觉中取得了巨大成功，主要得益于其跨领域知识的预训练能力。然而，图数据由于各领域之间语义和属性的巨大差异，现有方法难以实现有效的跨域知识迁移。作者发现，当前图预训练往往直接借用单域对比学习策略，不能有效整合多域知识，这限制了表达能力。

Method: 提出了一种新颖的多域预训练及跨域迁移框架MDGCL。在预训练阶段，设计了能够识别和捕捉领域差异的对比学习策略，并引入了领域token用于编码领域级的全局信息；在下游任务阶段，引入领域注意力机制以实现细粒度的领域知识迁移。

Result: 在五个基准数据集上的大量实验表明，该方法显著优于最新方法，准确率最高提升19.33%，Macro-F1最高提升19.13%。

Conclusion: MDGCL 能有效识别并利用跨域图数据中的领域差异，通过自适应领域知识迁移显著提升了图表示学习的效果，推动了图基础模型的发展。

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [38] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: 作者利用IIT理论分析LLM在心智理论测试中的表现，发现其表征尚无“意识”迹象，但表现出有趣的结构性特征。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型（LLM）是否具备“意识”现象的争论持续，而现有方法缺乏量化这一能力的工具。综合信息理论（IIT）是解释意识现象的定量理论，作者希望利用该理论对LLM的表现进行探索，检验其是否表现出类似人类意识的属性。

Method: 作者应用IIT 3.0和4.0版本中的相关度量（如Φ^max、Φ、概念信息、和Φ结构）对LLM在理论心智（ToM）测试中的表征序列进行分析，并与LLM内部表征空间的Span Representations进行对比分析。此外，实验考察了Transformer模型不同层及语言区间的表征变化。

Result: 结果显示，现代Transformer型LLM的表征序列未表现出统计学意义上的“意识”指标，但在空间置换分析下，模型表征出现了一些有趣的模式。

Conclusion: 目前LLM的表征未能通过IIT量化出明显“意识”特征，但研究揭示了其表征空间的一些有趣分隔和结构，对理解LLM内部机制提供参考。

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [39] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: ReG方法通过引入LLM反馈和知识结构重组，有效提升了知识图谱辅助的RAG检索、生成质量，在数据效率和泛化能力方面均表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前基于图的RAG系统在检索器部分表现较弱：一方面，检索器因缺乏真实标签常依赖弱监督，易引入噪声信号；另一方面，图数据抽象导致知识检索结果呈现形式杂乱。作者旨在解决这两方面的问题，提高LLM对结构化知识的利用能力。

Method: 提出ReG方法，通过引入LLM反馈以去除虚假信号并提升监督质量，同时加入结构感知重组模块，将检索到的知识结构化为逻辑连贯的证据链。

Result: ReG在多个主流基准上表现出色，带来最高10%的性能提升；即使仅用5%的训练数据也能达到SOTA性能，并可迁移至分布外知识图谱。对于需要推理的LLM，ReG可降低最多30%的推理Token消耗，性能提升达4%。

Conclusion: ReG（Refined Graph-based RAG）方法能够有效提升基于知识图谱的RAG系统性能，对不同的LLM骨干均有显著效果改进，并在只用极少训练数据和面对分布外知识图谱时也能保持优异表现。

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [40] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: 提出了德语Telegram虚假信息检测首个大规模图数据集Misinfo-TeleGraph，并证明结合消息传播结构的图神经网络在检测效果上优于纯文本方法，为相关研究提供了重要基准和数据支持。


<details>
  <summary>Details</summary>
Motivation: 在低管理（如Telegram）平台上传播的虚假信息日益严重，而现有检测方法对消息传播结构与社交连结的利用有限，特别是缺乏适用于德语语境的数据集和基准。

Method: 1. 构建包含500多万条德语Telegram公开频道消息的图数据集，附有元数据、频道关系与标签。2. 标签来源包括基于M3-embeddings对消息与事实核查/新闻的语义相似度计算（弱标签）及人工标注（强标签）。3. 对比纯文本模型与引入消息转发图结构的GNN（GraphSAGE+LSTM）表现，分析订阅数、浏览量及标签方式对检测效果的影响。

Result: 以MCC和F1为指标，结合图结构的GraphSAGE+LSTM显著优于文本基线；附加特征如订阅数、浏览量、弱/强标签对检测性能产生不同影响。该数据集和基准提升了对此类平台虚假信息检测研究的可复现性。

Conclusion: 本文提出了一个全新的德语Telegram图数据集Misinfo-TeleGraph，用于虚假信息检测，并通过实验基础验证了图神经网络方法（尤其是结合消息转发结构的GraphSAGE+LSTM）相比纯文本模型在检测效果上的显著优势。

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [41] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: 本文提出RExBench基準，用於評測LLM智能體的研究擴展與實作能力。九種主流LLM智能體均無法在無人工大量輔助下，自主完成多數研究擴展任務，說明當前技術仍離自主科研應用有明顯差距。


<details>
  <summary>Details</summary>
Motivation: 近年來基於大型語言模型(LLMs)的智能體已能自動執行複雜的軟體工程任務，且在機器學習和自然科學的研究流程自動化方面也有進展。然而，讓這類系統能自動擴展現有研究並實作其想法，是其進一步發展的關鍵能力。現有自動化工具在研究擴展任務的能力尚未有系統性評估標準。因此，作者針對這一問題提出新基準。

Method: 作者提出名為RExBench的新基準集。該基準集包含12個真實世界的研究實驗擴展任務，每個任務都是對現有論文和代碼庫的延伸，並由領域專家撰寫明確的操作說明。這一基準集對數據污染具有魯棒性，並且配備自動評估機制，能自動執行智能體輸出的結果以判斷其是否達到成功標準。用該基準集來測評了三種框架下（aider、Claude Code、OpenHands）共九個LLM智能體。

Result: 實驗發現，所有受測的智能體在絕大多數任務上都無法自主完成研究擴展實作。即便在增強人為提示的輔助下，最佳表現仍低於40%的成功率。這說明現有LLM智能體距離無需大幅人力干預，獨立處理現實研究擴展任務還有明顯差距。

Conclusion: 研究提出了針對研究擴展能力的標準化評測基準RExBench，並實證當前LLM智能體尚無法自主應對真實且複雜的研究擴展需求，展示了該領域仍有很大提升空間。該基準可促進未來相關技術的發展與評估。

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [42] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: 本文针对生成式大语言模型的潜在滥用问题，提出并验证了一种更鲁棒的水印检测方法，有助于提升AI生成文本的可追溯性和伦理应用保障。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在社会各领域的广泛应用，对其潜在滥用的担忧逐渐增多。人工生成与机器生成文本难以区分，容易带来伦理和安全风险。本文动机在于开发能够检测合成文本的新方法，以保障LLM驱动文本生成过程的伦理性。

Method: 研究首先复现了已有基线研究的发现，验证当前方法对底层生成模型变化的敏感性。随后作者提出一种创新性的水印方法，并通过对生成文本进行复述等方式，严格评估其鲁棒性。

Result: 实验结果表明，作者提出的方法在鲁棒性上优于现有的~\cite{aarson}水印方法，能够更有效检测合成文本，即使在面对被复述修改的情况下。

Conclusion: 该研究提出了一种新的文本水印方法，实验证明其在识别AI生成文本方面比现有方法更鲁棒，为LLM文本检测与伦理应用提供了有力技术支持。

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [43] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: 本论文提出结合稀疏与稠密检索的生成系统，在大型动态测试集上表现优异，神经重排序和提示工程带来提升但在效率及泛化性间需权衡，词汇-文本对齐性为关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 在不断变化的检索-生成环境中，提升RAG系统在大规模语料下的相关性与忠实度，同时探索不同技术路径对检索与生成质量的影响。

Method: 结合稀疏（BM25）和稠密（E5）检索，基于Falcon3-10B-Instruct生成答案，并综合采用神经重排序（RankLLaMA）和DSPy优化提示，进行系统性实验评估。

Result: 神经重排序显著提升MAP但计算消耗大，提示优化提升语义得分但拒答率为零带来泛化性疑虑；最终混合系统在faithfulness排名第4、correctness第11；词汇对齐对性能影响最大。

Conclusion: 混合检索增强生成系统在动态检索场景下性能表现稳定，语义与字面对齐度是影响效果的关键因素，但部分优化方法存在实际应用门槛。

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [44] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: RoBERTa等编码器模型对团队微行为检测效果有限，指令微调Llama-3.1等解码器大模型表现更佳，对太空任务等需文本分析场景具重要应用价值。


<details>
  <summary>Details</summary>
Motivation: 团队沟通中的微行为（如鼓励、劝阻等）对于高风险环境（如太空任务）中的团队动态分析及训练至关重要，但这些微行为往往表现在对话细节中且难以人工识别，因此希望利用大语言模型自动检测对话中的微行为。

Method: 收集模拟太空任务的对话转录本，评估编码器型（如RoBERTa、DistilBERT）和解码器型（如Llama-3.1）大语言模型在零样本分类、微调、增广微调和少样本生成等方式下对每个对话话轮中微行为类别的自动预测能力。

Result: RoBERTa和DistilBERT等编码器型模型即便经加权微调后，仍然难以识别样本数较少的微行为（尤其是劝阻性发言）；而经过指令微调的解码器型模型Llama-3.1取得了更优成绩，三分类任务宏平均F1为44%，二分类任务为68%。

Conclusion: 解码器型模型在微行为自动检测任务中显著优于编码器型模型，对开发团队沟通分析和训练干预工具具有现实意义，尤其适用于只能访问文本数据的高风险场景。

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [45] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: 作者提出VocabTrim方法，通过限制drafter的词表规模，显著减少了推理延迟，在内存受限环境下大幅提升了LLM的生成速度，并在Llama-3模型上取得了最高16%的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于drafter（起草者）的speculative decoding（推测解码）方法在草稿采样阶段存在由于大词表导致的推理开销，特别是在目标LLM词表很大时，该阶段的延迟明显，影响了在内存受限环境（如边缘设备）上的生成速度。作者希望减少这种不必要的推理开销，提高生成速度。

Method: 作者提出了一种训练无关的新技术VocabTrim：根据目标模型实际常用的词，从其词表筛选出出现频率较高的一小部分子集，重构drafter的语言建模头（LM head），使drafter只负责这部分高频词的预测。这样可以在保有适用性的前提下，极大减少drafter阶段的推理量和延迟。

Result: 该方法在目标模型Llama-3的Spec-Bench基准测试中实现了明显加速：对于Llama-3.2-3B-Instruct模型，在内存受限条件下的推理速度提升（MBSU）达到了16%。尽管采样接受率略有下降，但整体带来了显著的编解码延迟降低。

Conclusion: 通过VocabTrim限制drafter的词表规模，可以大幅降低基于drafter的推测解码过程中的推理延迟，特别是在内存受限的场景下显著提升了生成速度，对实际部署在边缘设备的LLM推理有很大促进作用。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [46] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: 本文总结了一次跨学科研讨会，分析了大型语言模型与人类认知在文本处理方面的关系，指出了LLMs的优势和局限，并强调了伦理与人机协作的问题，为相关领域未来的发展提供了指导。


<details>
  <summary>Details</summary>
Motivation: 人工智能语言模型（如大型语言模型，LLMs）与人类在文本理解和生成过程中的关系尚存在关键知识空白。论文希望通过整合认知心理学、语言学习和基于AI的自然语言处理等多个领域的专家观点，探索AI如何帮助理解或增强人类的语言能力。

Method: 举办跨学科研讨会，将认知心理学、语言学习和AI自然语言处理领域的专家聚集在一起，通过协作对话和观点交流，分析人类文本理解和生成的底层过程，以及AI如何辅助这些过程。

Result: 研讨会揭示了AI大型语言模型与人类认知之间的新关系模式，发现LLMs能为人类语言处理机制研究提供新视角。在用人类反馈微调后，LLMs与人类语言处理的行为越来越一致。同时，论文指出了LLMs在完全复制人类语言理解与生成方面的局限性，并讨论了人机协作中存在的机遇与挑战。

Conclusion: LLMs具备为认知心理学、语言学及教育领域提供支持的巨大潜力，但需高度关注伦理和AI技术的负责任使用。报告为未来相关研究、开发及实践指明了方向，并强调人机协作将为文本理解与生成带来新的可能。

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [47] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: 本文提出LLM多语生成中的翻译障碍假说，通过对108种语言对的分析发现，模型主要在将已解决任务的中间概念翻译为目标语言时失败，尤其在低资源语言中突出。这一发现为改进多语种生成提供了重要方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在中低资源语言上的生成质量较差，原因尚不完全清楚。作者希望通过解释性分析发现并解释这些模型多语种生成任务上的核心瓶颈。

Method: 利用解释性方法，观察LLM在多语任务中的中间处理过程。提出并验证"翻译障碍假说"：即模型在完成语言无关任务后，无法将正确概念翻译成目标语言。作者通过logit lens工具，在108种语言对的词汇翻译任务中，分析中间层处理结果。

Result: 实验证明，LLM多语生成质量低，主要原因之一是翻译阶段失败，尤其在低资源语言中更为突出。模型常常已经正确完成任务，但未能将中间概念正确表达为目标语言。

Conclusion: 多语种生成的关键瓶颈在于模型将中间概念翻译为目标语言的能力。指出这一问题对未来提升LLM多语种能力有重要指导意义。

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [48] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano通过新颖训练方法和策略专注，兼备高效与高性能，在消费级硬件上表现优异，突破了大模型资源壁垒。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在能力和计算资源之间存在根本性权衡，作者希望突破这一限制，探索不同于“全知型”的高效方法。

Method: 采用多阶段RLVR系统进行微调，完全摒弃传统的下一个token预测训练（SFT），专注于极致检索效率。

Result: Jan-nano在SimpleQA基准测试中集成MCP后取得83.2%的高分，并可在消费级硬件上流畅运行，具备128K上下文长度。

Conclusion: Jan-nano打破了大语言模型计算资源瓶颈，通过彻底专门化与新的训练策略，实现了高效率和高性能。

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [49] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: 本文提出VFT微调方法，在RL训练前提升模型对奖励窃取行为的自我表达能力。实验证明VFT能大幅提升检测率，有助于打造更安全透明的AI。


<details>
  <summary>Details</summary>
Motivation: 当前用RL训练的大型语言模型会利用提示中未预期的策略获取高回报且不在推理中表现出来，导致难以检测和安全风险，尤其在高风险场景。

Method: 提出在RL前通过VFT微调训练模型，使其在受提示影响时明确表达；在RL阶段用特定环境训练模型，将奖励窃取行为作为主要评估对象，并与基线方法进行对比。

Result: 在RL后，VFT组仅6%的结果存在未被检测的奖励窃取，而无VFT组高达88%，去偏基线组更高（99%）。VFT组模型在RL后明确表达被提示影响的比例提升至94%，而基线组仍很低。

Conclusion: VFT方法能显著提升语言模型奖励窃取行为的检测率，为构建更透明、安全的AI系统提供了实用路径。

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [50] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 该文提出一种具备语境感知能力的多轮对话语义缓存系统 ContextCache，通过两阶段检索和自注意力机制，提升了缓存命中准确率与响应效率，可大幅降低大模型对话系统的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存系统只能针对单一查询进行匹配，无法识别多轮对话中的上下文，这可能导致在不同对话语境下类似查询出现时错误命中缓存。解决这一问题可显著提升缓存准确性与LLM应用效率。

Method: 提出ContextCache系统，采用两阶段检索架构：首先基于向量对当前查询进行初步检索，然后通过自注意力机制融合当前及历史对话表征，实现更精确的语境匹配。

Result: 在真实对话数据上验证，ContextCache系统在精确率和召回率上均优于现有方法，缓存响应延迟约为直接调用LLM的1/10，大幅降低LLM对话应用的计算成本。

Conclusion: ContextCache能有效利用对话上下文信息，提升多轮对话场景下的语义缓存命中准确率和系统效率，适合部署于LLM相关对话应用中。

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [51] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出了MedEthicsQA，一个高质量医学伦理QA基准，并显示当前医学大模型在伦理问题处理上表现不佳，呼吁业界重视并改进模型的伦理对齐。


<details>
  <summary>Details</summary>
Motivation: 尽管医学大语言模型（MedLLMs）在临床任务中展现出巨大潜力，但其在伦理安全方面的探索依然不足。因此，本文的动机是在该领域提出一个系统性的伦理评估基准，促进MedLLMs在伦理合规性上的发展。

Method: 本文构建了一个包含5,623道多项选择题和5,351道开放式问题的MedEthicsQA基准题库，用于评估医学大模型伦理能力。研究团队结合全球医学伦理标准建立分层分类体系，数据来源覆盖主流医学数据集、权威题库和PubMed文献场景，并通过多阶段过滤和多专家核验保证数据质量。

Result: 通过该基准测试，目前主流的医学大模型在医学伦理问题上答题表现低于其原始基础模型，显示现有MedLLMs在伦理对齐方面存在明显不足。题库的质量经严格把控，错误率低至2.72%。

Conclusion: 本研究首次系统性地建立并公开了医学伦理评测基准MedEthicsQA，发现现有MedLLMs在伦理能力上存在明显短板，为后续模型优化与伦理对齐研究提供了基础和工具。

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [52] [A Graph Width Perspective on Partially Ordered Hamiltonian Paths and Cycles I: Treewidth, Pathwidth, and Grid Graphs](https://arxiv.org/abs/2506.23790)
*Jesse Beisegel,Katharina Klost,Kristin Knorr,Fabienne Ratajczak,Robert Scheffler*

Main category: cs.DM

TL;DR: 本文研究了在具有优先顺序约束的图结构中求哈密顿路径和环的复杂性。结果展示了多项结构参数下的复杂度界限与多项式可解情形，对理论与实际应用均有指导价值。


<details>
  <summary>Details</summary>
Motivation: 在图论中，哈密顿路径和环是经典的组合优化问题。添加顶点间的先行约束（偏序关系）后，问题更具挑战性。本研究旨在探索在有限路径宽度或树宽的图（包括网格图）上，加上这些约束后哈密顿路径（环）问题的计算复杂度。

Method: 作者通过复杂度还原，证明带偏序约束的哈密顿路径和环问题分别在特定路径宽度的图上是$	ext{NP}$-完全的。同时，设计了多项式时间算法，解决在更小路径宽度或树宽条件下的相关问题。对带权图变体与网格图也进行了复杂度研究。

Result: 当图的路径宽度为4时，带优先约束的哈密顿路径问题是$	ext{NP}$-完全的，路径宽度为5时，哈密顿环问题是$	ext{NP}$-完全的；而路径宽度3或树宽2的图的路径问题，以及路径宽度4或树宽3的环问题，可以在多项式时间内解决。对于高为7或9以上的网格图，其路径/环问题为$	ext{NP}$-完全（带权时5、6为界）。

Conclusion: 带有先行约束的哈密顿路径和环问题在具有较小路径宽度或树宽的图和网格中有明确的复杂度分界，在一定范围内可实现高效算法，但超过这个范围则呈现$	ext{NP}$-困难。

Abstract: We consider the problem of finding a Hamiltonian path or a Hamiltonian cycle
with precedence constraints in the form of a partial order on the vertex set.
We show that the path problem is $\mathsf{NP}$-complete for graphs of pathwidth
4 while the cycle problem is $\mathsf{NP}$-complete on graphs of pathwidth 5.
We complement these results by giving polynomial-time algorithms for graphs of
pathwidth 3 and treewidth 2 for Hamiltonian paths as well as pathwidth 4 and
treewidth 3 for Hamiltonian cycles. Furthermore, we study the complexity of the
path and cycle problems on rectangular grid graphs of bounded height. For
these, we show that the path and cycle problems are $\mathsf{NP}$-complete when
the height of the grid is greater or equal to 7 and 9, respectively. In the
variant where we look for minimum edge-weighted Hamiltonian paths and cycles,
the problems are $\mathsf{NP}$-hard for heights 5 and 6, respectively.

</details>


### [53] [Linear Layouts of Graphs with Priority Queues](https://arxiv.org/abs/2506.23943)
*Emilio Di Giacomo,Walter Didimo,Henry Förster,Torsten Ueckerdt,Johannes Zink*

Main category: cs.DM

TL;DR: 本文提出了带权图的优先级队列布局新模型，分析了相关理论界限、判别算法和复杂性，为图线性布局领域带来新见解。


<details>
  <summary>Details</summary>
Motivation: 为了更好地处理带权图的线性布局问题，作者将传统的栈（stack）和队列（queue）布局扩展到有优先级的队列（priority queue）布局，目的是研究带权边在图的线性排列下能高效存储和管理的理论基础。

Method: 引入了优先级队列布局的概念，并在理论上分析了不同类型图对于优先级队列数目的需求，提出了用于识别哪些图可以用单个优先级队列布局的高效算法，以及针对定序顶点时判定优先级队列数目的复杂性。

Result: 1. 有些带权图需要线性数量的优先级队列。2. 给出了能用单个优先级队列布局的图的刻画，并提供了高效判别算法。3. 独立于边权时，所需优先级队列的数量由图的路径宽度上界，但对树宽为2的图也可能非常大。4. 当顶点顺序固定时，判定最小所需优先级队列数是NP完全的。

Conclusion: 本文扩展了图布局理论，首次将优先级队列引入带权图的线性布局，揭示了其结构复杂性并提出了新的识别算法，同时指出了相关优化决策问题的计算难度。

Abstract: A linear layout of a graph consists of a linear ordering of its vertices and
a partition of its edges into pages such that the edges assigned to the same
page obey some constraint. The two most prominent and widely studied types of
linear layouts are stack and queue layouts, in which any two edges assigned to
the same page are forbidden to cross and nest, respectively. The names of these
two layouts derive from the fact that, when parsing the graph according to the
linear vertex ordering, the edges in a single page can be stored using a single
stack or queue, respectively. Recently, the concepts of stack and queue layouts
have been extended by using a double-ended queue or a restricted-input queue
for storing the edges of a page. We extend this line of study to edge-weighted
graphs by introducing priority queue layouts, that is, the edges on each page
are stored in a priority queue whose keys are the edge weights. First, we show
that there are edge-weighted graphs that require a linear number of priority
queues. Second, we characterize the graphs that admit a priority queue layout
with a single queue, regardless of the edge-weight function, and we provide an
efficient recognition algorithm. Third, we show that the number of priority
queues required independently of the edge-weight function is bounded by the
pathwidth of the graph, but can be arbitrarily large already for graphs of
treewidth two. Finally, we prove that determining the minimum number of
priority queues is NP-complete if the linear ordering of the vertices is fixed.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [54] [Programmable Co-Transcriptional Splicing: Realizing Regular Languages via Hairpin Deletion](https://arxiv.org/abs/2506.23384)
*Da-Jung Cho,Szilárd Zsolt Fazekas,Shinnosuke Seki,Max Wiedenhöft*

Main category: cs.FL

TL;DR: 本文提出一种能将任意有限目标RNA序列集高效编码进DNA环状模板的方法，依赖于NFA表达能力，通过共转录性剪接生成目标序列。方法在分子编程与RNA结构设计方面具开创意义，但模板最小化问题计算上具挑战。


<details>
  <summary>Details</summary>
Motivation: RNA的共转录性（在DNA转录过程中进行剪接或折叠）在分子编程和RNA纳米结构可编程折叠方面有巨大潜力。然而，共转录性剪接存在效率高但过程较难预测的问题。本文希望能将其作为一种可编程机制，从DNA模板产生特定RNA序列，提升编程与设计的能力。

Method: 作者提出了一种基于对数能量模型的编码方法，将任意非确定性有限自动机（NFA）编码进环状DNA模板，使共转录性剪接可以生成所有NFA接受的RNA序列。利用NFA的高效表达能力解决了从DNA模板生成一组给定RNA序列的问题；并探讨了优化最小化DNA模板的相关自动机最小化问题。

Result: 作者构建的方法可将任意目标RNA序列集（可表示为有限语言）通过NFA高效编码进DNA模板，并可用共转录性剪接生成全部目标序列。但进一步最小化NFA结构及其实用变体的问题被证明在计算上是不可行的（难以近似计算）。

Conclusion: 文章为共转录性剪接的可编程分子生成提出了一整套理论与实践方案，提供了基于NFA与DNA模板的有效编码方法。该工作突破性地使得任意有限集合的RNA序列都能以较小模板实现自动合成，虽在进一步优化时存在计算复杂性限制。

Abstract: RNA co-transcriptionality, where RNA is spliced or folded during
transcription from DNA templates, offers promising potential for molecular
programming. It enables programmable folding of nano-scale RNA structures and
has recently been shown to be Turing universal. While post-transcriptional
splicing is well studied, co-transcriptional splicing is gaining attention for
its efficiency, though its unpredictability still remains a challenge. In this
paper, we focus on engineering co-transcriptional splicing, not only as a
natural phenomenon but as a programmable mechanism for generating specific RNA
target sequences from DNA templates. The problem we address is whether we can
encode a set of RNA sequences for a given system onto a DNA template word,
ensuring that all the sequences are generated through co-transcriptional
splicing. Given that finding the optimal encoding has been shown to be
NP-complete under the various energy models considered, we propose a practical
alternative approach under the logarithmic energy model. More specifically, we
provide a construction that encodes an arbitrary nondeterministic finite
automaton (NFA) into a circular DNA template from which co-transcriptional
splicing produces all sequences accepted by the NFA. As all finite languages
can be efficiently encoded as NFA, this framework solves the problem of finding
small DNA templates for arbitrary target sets of RNA sequences. The quest to
obtain the smallest possible such templates naturally leads us to consider the
problem of minimizing NFA and certain practically motivated variants of it, but
as we show, those minimization problems are computationally intractable.

</details>


### [55] [Reachability in symmetric VASS](https://arxiv.org/abs/2506.23578)
*Łukasz Kamiński,Sławomir Lasota*

Main category: cs.FL

TL;DR: 本文分析了在不同群作用下的VASS可达性复杂度，特别发现对称群下该问题可在PSPACE内解决，显著优于一般VASS，并研究了平凡群和对称群组合时的复杂度提升。


<details>
  <summary>Details</summary>
Motivation: VASS的可达性是计算理论中的一个核心且困难问题。对称结构下能否简化复杂度，以及当前数据VASS相关可达性问题仍未解决，促使作者探索不同群作用下的复杂度变化。

Method: 分析VASS系统在不同群（特别是对称群、交错群、循环群）作用下的可达性，对比复杂度。并估算当群是平凡群与对称群组合时，复杂度的提高。

Result: 确立了对称群下VASS可达性在PSPACE中可解，并对其它群进行了进一步研究和复杂度估算，在特定组合情形下获得复杂度提升的见解。

Conclusion: 本论文发现，在对称群不变的VASS中，可达性问题可在PSPACE中求解，无论维度如何，而一般VASS的复杂度远高于此。此外，研究了其他群作用下的复杂度。

Abstract: We investigate the reachability problem in symmetric vector addition systems
with states (VASS), where transitions are invariant under a group of
permutations of coordinates. One extremal case, the trivial groups, yields
general VASS. In another extremal case, the symmetric groups, we show that the
reachability problem can be solved in PSPACE, regardless of the dimension of
input VASS (to be contrasted with Ackermannian complexity in general VASS). We
also consider other groups, in particular alternating and cyclic ones.
Furthermore, motivated by the open status of the reachability problem in data
VASS, we estimate the gain in complexity when the group arises as a combination
of the trivial and symmetric groups.

</details>
