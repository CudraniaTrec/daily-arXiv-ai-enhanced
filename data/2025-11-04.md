<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 42]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.CL](#cs.CL) [Total: 53]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Equality Saturation Guided by Large Language Models](https://arxiv.org/abs/2511.00403)
*Wentao Peng,Ruyi Ji,Yingfei Xiong*

Main category: cs.PL

TL;DR: 本文提出一种结合LLM和e-graph的重写系统LGuess，通过从LLM中学习概率模型选取高层次重写检查点，并用e-graph完成低层链推理，显著提升了因式分解等任务的正确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在保证正确性方面存在关键问题，尤其是在需要严格推理和形式化重写链的场景下。传统方法若直接依赖LLM生成重写链，难以保证其完整性和正确性。

Method: 本文提出了一种新的方法LLM-guided equality saturation（LGuess），将e-graphs作为中间层，连接LLM与重写系统。LGuess只向LLM查询高层次重写的检查点，而低层次重写链则依靠e-graphs自动补足。为了解决从e-graph中有效提取合适检查点的问题，LGuess采用从LLM学习得到的概率模型，预测最有可能的检查点，并确保其提取过程简单有效。作者实现了LGuess的原型，并在多变量多项式因式分解问题上进行了评估。

Result: 实验结果表明，LGuess方法相较于传统的直接查询LLM重写链或直接应用equality saturation方法，具有明显优势，能够生成更可靠和高效的重写链。

Conclusion: 通过引入e-graph为中间层并结合LLM学习到的概率模型提取检查点，LGuess成功提升了LLM在复杂重写任务中的正确性和实用性，为结合AI与形式化方法带来了新的解决思路。

Abstract: One critical issue with large language models (LLMs) is their inability to
guarantee correctness. Although this problem can be addressed by applying LLMs
to formal rewrite systems, current LLMs are still far from adequate to generate
sound rewrite chains. To bridge this gap, this paper proposes LLM-guided
equality saturation, dubbed LGuess, by incorporating e-graphs as an
intermediate layer between LLMs and rewrite systems. LGuess queries LLMs only
for high-level rewrite checkpoints and uses e-graphs to supply low-level
rewrite chains between these checkpoints. The key technical challenge in this
procedure lies in effectively extracting a suitable checkpoint from a saturated
e-graph, which LGuess addresses by learning a probabilistic model from the LLM.
The model predicts probable checkpoints while remaining simple enough for
effective extraction. We implement a prototype of LGuess and evaluate it on the
problem of factorizing multivariable polynomials. The results demonstrate a
significant advantage of LGuess compared to both straightforward equality
saturation and the approach that queries the LLM directly for the rewrite
chain.

</details>


### [2] [\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs](https://arxiv.org/abs/2511.00488)
*Jun Gao,Yun Peng,Xiaoxue Ren*

Main category: cs.PL

TL;DR: 本文发现现有大语言模型在归纳代码推理任务中存在固有缺陷，针对生成与推理能力差距、代码来源偏见及零样本泛化弱等挑战，提出多智能体协作框架ReMind，有效提升推理性能并在多模型和基准下取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在代码任务上的表现虽优秀，但在程序执行过程的推理（即归纳代码推理）方面仍存在不足，而其具体原因未被充分探究。

Method: 作者首先通过实证研究，揭示了三大挑战：生成与推理能力的固有差距、对代码来源的偏见以及在复杂基准上的零样本泛化能力弱。对此，提出了一个名为ReMind的多智能体框架，包括Mutator（生成代码变体）、Executor（逐步跟踪变量状态）、Inspector（识别推理问题并优化控制流），以系统提升代码归纳推理能力。

Result: ReMind框架能通过智能体协作有效发现并改进逻辑推理中的问题，在两套基准、五种LLM上的广泛实验表明，其在代码归纳推理任务上优于现有方法，且带来强健的零样本泛化能力。

Conclusion: ReMind框架通过结构化智能体合作系统性解决LLM在归纳代码推理上的关键缺陷，提升了推理准确率与泛化能力，在实证测试中表现优异。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in
code-related tasks. Despite their advancement, empirical evidence reveals that
they still struggle with \emph{deductive code reasoning}, the ability to reason
about the program execution process. While prior studies have recognized this
limitation, the underlying causes remain largely underexplored. In this paper,
we begin by presenting a comprehensive empirical study that reveals three key
challenges undermining deductive code reasoning: (1) an intrinsic gap between
generation and reasoning abilities, (2) a consistent bias towards code sources,
and (3) weak zero-shot generalization on complex benchmarks. In light of these
challenges, we propose \texttt{ReMind}, a multi-agent framework composed of
\texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The
\texttt{Mutator} generates code variants to mitigate bias towards code sources,
the \texttt{Executor} traces variable states step-by-step to expose
inconsistency, and the \texttt{Inspector} identifies problematic reasoning
steps and provides control-flow refinement to bridge the intrinsic reasoning
gap. Through their coordinated collaboration, \texttt{ReMind} systematically
identifies and refines reasoning flaws, achieving outstanding performance and
enabling robust zero-shot generalization. Extensive experiments on two
benchmarks with five LLMs demonstrate the superior advantages of
\texttt{ReMind} compared to baseline approaches in deductive code reasoning.

</details>


### [3] [Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization](https://arxiv.org/abs/2511.00592)
*Massinissa Merouani,Islem Kara Bernou,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: 本文提出利用通用LLM与编译器闭环反馈互动优化代码循环，ComPilot框架在基准测试中取得显著加速效果，表现超越部分主流优化器，显示LLM有望成为代码优化新利器。


<details>
  <summary>Details</summary>
Motivation: 自动化代码优化，尤其是对现代硬件下复杂循环嵌套的优化，仍然非常具有挑战性。

Method: 提出ComPilot框架，利用无需特殊微调的大型语言模型（LLM）作为交互式优化代理，通过与编译器反馈闭环进行代码优化。

Result: 在PolyBench基准测试中，ComPilot实现了对原始代码2.66倍（单次运行）和3.54倍（五次最佳运行）的平均加速比，并在许多情况下超越了最先进的Pluto多面体优化器。

Conclusion: 通用LLM在结合编译器反馈的情况下，可以有效引导代码优化，为智能体化AI在代码优化领域开辟了新的研究方向。

Abstract: Automatic code optimization remains a difficult challenge, particularly for
complex loop nests on modern hardware. This paper investigates a novel approach
to code optimization where Large Language Models (LLMs) guide the process
through a closed-loop interaction with a compiler. We present ComPilot, an
experimental framework that leverages off-the-shelf LLMs, without any
task-specific fine-tuning, as interactive optimization agents. ComPilot
establishes a feedback loop where an LLM proposes transformations for a given
loop nest to a compiler. The compiler attempts the transformations, reporting
back legality status and measured speedup or slowdown. The LLM utilizes this
concrete feedback to iteratively refine its optimization strategy. Our
extensive evaluation across the PolyBench benchmark suite demonstrates the
effectiveness of this zero-shot approach. ComPilot achieves geometric mean
speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original
code. Furthermore, ComPilot demonstrates competitive performance against the
state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.
This experimental study demonstrates that general-purpose LLMs can effectively
guide the code optimization process when grounded by compiler feedback, opening
promising research directions for agentic AI in code optimization.

</details>


### [4] [Typed Embedding of miniKanren for Functional Conversion](https://arxiv.org/abs/2511.00740)
*Igor Engel,Ekaterina Verbitskaia*

Main category: cs.PL

TL;DR: 该文提出了一种将miniKanren关系型编程语言类型化地嵌入到Haskell中的方法，降低了冗余，提高了性能，解决了以往转换中类型和生成器管理的不足。


<details>
  <summary>Details</summary>
Motivation: 早期的关系型编程转换存在类型不敏感、需要确定性注释和隐式生成器传递等问题，导致性能受限且转换方式不优雅。作者旨在解决这些问题，优化编程体验和效率。

Method: 提出了将miniKanren以类型化、tagless-final的方式嵌入Haskell。通过这种嵌入减少了编码冗余，并提升类型安全性和表达能力。

Result: 该方法显著减少了样板代码，同时保持甚至提升了此前的性能加速效果。

Conclusion: 使用typed tagless-final嵌入大幅优化了miniKanren在Haskell中的性能和编码简洁性。

Abstract: Relational programming enables program synthesis through a verifier-to-solver
approach. An earlier paper introduced a functional conversion that mitigated
some of the inherent performance overhead. However, the conversion was
inelegant: it was oblivious to types, demanded determinism annotations, and
implicit generator threading. In this paper, we address these issues by
providing a typed tagless-final embedding of miniKanren into Haskell. This
improvement significantly reduces boilerplate while preserving, and sometimes
enhancing, earlier speedups.

</details>


### [5] [Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra](https://arxiv.org/abs/2511.01736)
*Charles Yuan*

Main category: cs.PL

TL;DR: 本文提出了Cobble语言，简化并优化量子线性代数算法的开发，通过自动生成高效电路和创新资源分析技术，在多项基准任务中实现了远超现有优化器的加速效果，推动了量子线性代数在实际硬件上的应用进程。


<details>
  <summary>Details</summary>
Motivation: 量子线性代数算法有潜力在如仿真和回归等应用中带来指数级加速，被认为是量子硬件实现的重要候选。然而，这类算法不能像经典算法一样高效地在内存中存储矩阵，开发者需要用复杂的量子电路表达矩阵运算，其优化方法和经典情况不同。

Method: 提出Cobble语言，允许开发者用高级符号直接表达和操作量子矩阵表示（块编码），并自动编译为正确的量子电路。Cobble还包括时空资源分析和优化机制，能够降低开销并生成高效电路，特别利用量子奇异值变换等技术。

Result: 在对仿真、回归、搜索等基准测试中的核心部分进行评估，Cobble展现了2.6倍到25.4倍的加速，这些结果是现有电路优化器未能达到的。

Conclusion: Cobble语言能够显著提升量子线性代数算法的开发效率和运行性能，解决了传统优化手段不适用于量子环境下的问题，并通过在多种应用场景下实现高倍速提升证明其有效性。

Abstract: Quantum algorithms for computational linear algebra promise up to exponential
speedups for applications such as simulation and regression, making them prime
candidates for hardware realization. But these algorithms execute in a model
that cannot efficiently store matrices in memory like a classical algorithm
does, instead requiring developers to implement complex expressions for matrix
arithmetic in terms of correct and efficient quantum circuits. Among the
challenges for the developer is navigating a cost model in which conventional
optimizations for linear algebra, such as subexpression reuse, can be
inapplicable or unprofitable.
  In this work, we present Cobble, a language for programming with quantum
computational linear algebra. Cobble enables developers to express and
manipulate the quantum representations of matrices, known as block encodings,
using high-level notation that automatically compiles to correct quantum
circuits. Cobble features analyses that estimate leading factors in time and
space usage of programs, as well as optimizations that reduce overhead and
generate efficient circuits using leading techniques such as the quantum
singular value transformation. We evaluate Cobble on benchmark kernels for
simulation, regression, search, and other applications, showing 2.6x-25.4x
speedups not achieved by existing circuit optimizers on these benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [ScaleCall - Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights](https://arxiv.org/abs/2511.00074)
*Richard Osuagwu,Thomas Cook,Maraim Masoud,Koustav Ghosal,Riccardo Mattivi*

Main category: cs.SE

TL;DR: 论文针对企业受监管环境中的工具调用挑战，开发并评估了多种工具检索方法，提出并在Mastercard部署了ScaleCall框架。结果显示，不同检索方案各适用于不同场景，并总结了工具检索系统设计中的关键权衡，为企业级LLM部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在工具调用方面表现突出，但在金融科技等受监管企业环境中部署这些能力存在独特挑战，如本地部署限制、合规要求，以及需对大量功能重叠的工具进行有效区分。

Method: 构建并部署名为ScaleCall的工具调用原型框架，在Mastercard内部进行API编排和数据工程自动化工作流程，并系统性评估了基于嵌入的检索、基于提示的列表排序以及混合方法。通过企业实际基准测试，比较不同方法在精度、效率和可操作性上的表现。

Result: 嵌入式检索方法在大型工具库中表现出较低延迟，列表排序方法在功能重叠工具的区分上更优，混合方法则在特定场景中展现潜力。将这些发现集成到ScaleCall的灵活架构中，并通过在Mastercard实际受监管环境下部署进行验证。

Conclusion: 检索方法的有效性主要取决于具体领域因素而非算法本身优劣。工作为受监管行业企业级应用的工具调用系统设计提供了实用见解，明确了检索准确性、算力效率与运营需求之间的权衡。

Abstract: While Large Language Models (LLMs) excel at tool calling, deploying these
capabilities in regulated enterprise environments such as fintech presents
unique challenges due to on-premises constraints, regulatory compliance
requirements, and the need to disambiguate large, functionally overlapping
toolsets. In this paper, we present a comprehensive study of tool retrieval
methods for enterprise environments through the development and deployment of
ScaleCall, a prototype tool-calling framework within Mastercard designed for
orchestrating internal APIs and automating data engineering workflows. We
systematically evaluate embedding-based retrieval, prompt-based listwise
ranking, and hybrid approaches, revealing that method effectiveness depends
heavily on domain-specific factors rather than inherent algorithmic
superiority. Through empirical investigation on enterprise-derived benchmarks,
we find that embedding-based methods offer superior latency for large tool
repositories, while listwise ranking provides better disambiguation for
overlapping functionalities, with hybrid approaches showing promise in specific
contexts. We integrate our findings into ScaleCall's flexible architecture and
validate the framework through real-world deployment in Mastercard's regulated
environment. Our work provides practical insights into the trade-offs between
retrieval accuracy, computational efficiency, and operational requirements,
contributing to the understanding of tool-calling system design for enterprise
applications in regulated industries.

</details>


### [7] [Adding New Capability in Existing Scientific Application with LLM Assistance](https://arxiv.org/abs/2511.00087)
*Anshu Dubey,Akash Dhruv*

Main category: cs.SE

TL;DR: 本文提出并实践了一种结合LLM与代码工具Code-Scribe的新算法代码生成方案，成功实现了对新算法从零代码生成，拓展了自动编程的应用范畴。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在自动编程领域表现突出，但对于全新算法的代码生成先例较少，因其训练数据中往往不包含类似代码。本文关注如何利用LLM为全新算法从零生成代码。

Method: 提出了一套利用LLM辅助从零编写新算法代码的方法，并改进了已有的代码翻译工具Code-Scribe以支持新代码生成。

Result: 改进后的Code-Scribe具备了对新算法进行代码生成的能力，可以在缺乏现有样本的情况下，辅助开发者完成编程任务。

Conclusion: 通过整合LLM和增强版Code-Scribe，解决了新算法无样例情况下的代码生成问题，推动了编程自动化研究的进一步发展。

Abstract: With the emergence and rapid evolution of large language models (LLM),
automating coding tasks has become an im- portant research topic. Many efforts
are underway and liter- ature abounds about the efficacy of models and their
ability to generate code. A less explored aspect of code generation is for new
algorithms, where the training data-set would not have included any previous
example of similar code. In this paper we propose a new methodology for writing
code from scratch for a new algorithm using LLM assistance, and describe
enhancement of a previously developed code- translation tool, Code-Scribe, for
new code generation.

</details>


### [8] [Inferring multiple helper Dafny assertions with LLMs](https://arxiv.org/abs/2511.00125)
*Álvaro Silva,Alexandra Mendes,Ruben Martins*

Main category: cs.SE

TL;DR: 论文提出DAISY系统，利用大语言模型自动补全Dafny程序缺失的断言，显著提升了验证自动化，验证效率在单、多断言缺失实验中表现良好，为形式化证明自动化提供新思路。


<details>
  <summary>Details</summary>
Motivation: Dafny验证器可以为程序提供强有力的正确性保证，但实际使用中往往需要程序员手动添加大量辅助断言，这成为其推广和应用的一大障碍。本文旨在探索利用大语言模型（LLM）自动推断Dafny程序中缺失的辅助断言，特别关注缺失多个断言的场景，以减少手动工作量，提高验证效率和易用性。

Method: 本文扩展了DafnyBench基准测试集，构建了包含不同断言缺失情况的数据集（一个、两个或所有断言被移除），并提出了一套断言类型的分类方法用于分析推断难度。同时，作者提出了一种混合方法，将LLM预测和基于错误信息的启发式方法结合以提升故障定位精度，并将该方法集成到新工具DAISY中（Dafny Assertion Inference SYstem）。

Result: DAISY在单断言缺失情况下可以验证63.4%的程序，在多断言缺失情况下可以验证31.7%的程序。实验还发现，许多程序即使没有恢复所有原始断言也能被正确验证，说明证明过程本身可能有多种有效修复方式，不必完全依赖于原断言的恢复。

Conclusion: 自动断言推断能够显著减轻形式化证明工程的负担，提高自动化水平，有助于Dafny这类形式化工具的规模化和普及。论文展示的方法和工具为更易用、更高效的程序验证提供了有力支持。

Abstract: The Dafny verifier provides strong correctness guarantees but often requires
numerous manual helper assertions, creating a significant barrier to adoption.
We investigate the use of Large Language Models (LLMs) to automatically infer
missing helper assertions in Dafny programs, with a primary focus on cases
involving multiple missing assertions. To support this study, we extend the
DafnyBench benchmark with curated datasets where one, two, or all assertions
are removed, and we introduce a taxonomy of assertion types to analyze
inference difficulty. Our approach refines fault localization through a hybrid
method that combines LLM predictions with error-message heuristics. We
implement this approach in a new tool called DAISY (Dafny Assertion Inference
SYstem). While our focus is on multiple missing assertions, we also evaluate
DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one
missing assertion and 31.7% with multiple missing assertions. Notably, many
programs can be verified with fewer assertions than originally present,
highlighting that proofs often admit multiple valid repair strategies and that
recovering every original assertion is unnecessary. These results demonstrate
that automated assertion inference can substantially reduce proof engineering
effort and represent a step toward more scalable and accessible formal
verification.

</details>


### [9] [What a diff makes: automating code migration with large language models](https://arxiv.org/abs/2511.00160)
*Katherine A. Rosenfeld,Cliff C. Kerr,Jessica Lundin*

Main category: cs.SE

TL;DR: 本文利用大型语言模型（LLMs）和diff上下文，显著提升了代码依赖迁移的自动化性能，开发的AIMigrate工具在真实项目中迁移准确率可达80%，为软件维护带来了新的高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现代软件依赖大量不断更新的技术栈，这些更新可能带来新功能，但同时也可能破坏已有项目的兼容性。维护依赖项的兼容性，尤其在其发生语义版本变化时，是一个关键且复杂的问题，亟需有自动化、高效的解决方案。

Method: 本文探索了利用大型语言模型（LLMs）进行代码迁移，特别是在依赖项发生主/次版本更新时保持兼容性。研究通过引入diff上下文，衡量测试覆盖率和变更比较等指标，评估LLM的迁移效果。并开发了一个开源的Python工具包AIMigrate和相关数据集。

Result: 实验表明，在上下文中加入diff信息能显著提升LLM的迁移性能，有时甚至超过直接用代码。AIMigrate在实际迁移TYPHOIDSIM到不同STARSIM版本时，在单次运行中能识别65%的变更，多次运行提升到80%，其中47%的变更生成完全正确。

Conclusion: 将代码diff信息融入大型语言模型迁移流程能提升自动化迁移依赖兼容性的表现，并且AIMigrate工具在现实场景下表现优异，验证了该方法的可行性和有效性。为后续依赖迁移研究和工具开发提供了重要资源和数据集。

Abstract: Modern software programs are built on stacks that are often undergoing
changes that introduce updates and improvements, but may also break any project
that depends upon them. In this paper we explore the use of Large Language
Models (LLMs) for code migration, specifically the problem of maintaining
compatibility with a dependency as it undergoes major and minor semantic
version changes. We demonstrate, using metrics such as test coverage and change
comparisons, that contexts containing diffs can significantly improve
performance against out of the box LLMs and, in some cases, perform better than
using code. We provide a dataset to assist in further development of this
problem area, as well as an open-source Python package, AIMigrate, that can be
used to assist with migrating code bases. In a real-world migration of
TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of
required changes in a single run, increasing to 80% with multiple runs, with
47% of changes generated perfectly.

</details>


### [10] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 本文通过大规模分析Java项目，首次将SATD评论与具体代码结构建立关联，发现SATD往往集中在关键代码区，是开发者主动标记变更风险和权衡的行为。


<details>
  <summary>Details</summary>
Motivation: 过去的研究主要关注SATD的检测和优先级，很少关注SATD实际影响的源代码位置。本研究旨在将SATD注释与其相关联的源代码结构建立联系。

Method: 利用PENTACET这一包含九千多个Java开源项目代码注释的大型SATD数据集，定量分析SATD最常发生的位置及其影响的代码结构或语句类型。

Result: 将22.5万条SATD注释与其关联代码进行映射分析，发现SATD多出现在关键结构（定义、条件、异常处理）附近，且反映开发者在面临不确定性和权衡时的主动表达。

Conclusion: SATD主要出现在靠近定义、条件语句和异常处理的代码中，反映了开发者的主动意识，是在变更过程中故意传达的信号而非单纯的疏忽。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


### [11] [Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories](https://arxiv.org/abs/2511.00197)
*Oorja Majgaonkar,Zhiwei Fei,Xiang Li,Federica Sarro,He Ye*

Main category: cs.SE

TL;DR: 本论文通过分析三种主流代码代理解决软件问题的过程轨迹，发现成功和失败过程有明显差异，并总结了影响成功的行为模式和故障定位特点，为智能软件工程系统的改进提供关键方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型代理在复杂软件工程任务中的应用增加，简单的成功率指标已无法全面衡量其表现，因此亟需更深入理解其解决问题的行为过程。

Method: 对三种最先进的代码代理模型（OpenHands、SWE-agent、Prometheus）在SWE-Bench基准上的执行轨迹进行实证分析，包括成功与失败的案例，重点研究代理在自动解决软件问题时的具体步骤。

Result: 发现代理采取了不同的解决策略（如防御性编程与上下文收集），并根据场景取得成功。失败轨迹通常更长且变化更大，且不同代理在失败时表现模式不同。故障定位分析显示大多数轨迹即使失败也能正确识别有问题的文件（72-81%），但最终成功更依赖于实现“近似”代码修改而非完全正确修改。

Conclusion: 该研究揭示了通过轨迹分析理解代理行为的有效途径，对提升自动化软件工程代理的稳健性和可解释性具有基础性意义。

Abstract: The increasing deployment of Large Language Model (LLM) agents for complex
software engineering tasks has created a need to understand their
problem-solving behaviours beyond simple success metrics. While these agents
demonstrate impressive capabilities in automated issue resolution, their
decision-making processes remain largely opaque. This paper presents an
empirical study of agent trajectories, namely the execution traces capturing
the steps agents take when attempting to resolve software issues. We analyse
trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and
Prometheus) on the SWE-Bench benchmark, examining both successful and failed
attempts. Our investigation reveals several key insights into agent behaviour.
First, we identify how distinct problem-solving strategies, such as defensive
programming and context gathering, enable success in different scenarios.
Second, we find that failed trajectories are consistently longer and exhibit
higher variance than successful ones, with failure patterns differing
significantly between agents. Third, our fault localisation analysis shows that
while most trajectories correctly identify problematic files (72-81\% even in
failures), success depends more on achieving approximate rather than exact code
modifications. These and other findings unveiled by our study, provide a
foundation for understanding agent behaviour through trajectory analysis,
contributing to the development of more robust and interpretable autonomous
software engineering systems.

</details>


### [12] [Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification](https://arxiv.org/abs/2511.00202)
*Jacqueline Mitchell,Yasser Shaaban*

Main category: cs.SE

TL;DR: vibe coding在与大语言模型协作开发软件时容易积累技术债务且代码一致性差。作者呼吁借助形式化方法及“侧车系统”，自动规范和验证开发流程，提升vibe coding的可靠性和可用性。


<details>
  <summary>Details</summary>
Motivation: 近年来，“vibe coding”（通过与大语言模型迭代对话进行软件开发）的方式变得非常流行，但开发者在实际应用过程中遇到了技术债务、安全问题和频繁修改代码等一系列局限性。作者旨在分析这些问题产生的根本原因，并探讨提升开发可靠性的解决方法。

Method: 作者通过分析LLM在vibe coding中的表现，认为其不能很好地协同累积的约束条件，导致开发者不易发现并解决矛盾。此外，作者提出应采用一种侧车系统(side-car system)，贯穿vibe coding流程，包括自动形式化规范、验证目标、给LLM提供可执行反馈，并能让开发者直观影响规范。

Result: 作者认为将形式化方法引入到vibe coding流程，并通过新型侧车系统来自动生成和验证规范、及时反馈LLM，可以有效缓解传统vibe coding中遇到的困境，提高开发可靠性。

Conclusion: 论文指出现有LLM驱动的vibe coding方式存在结构性缺陷，应通过与形式化方法深度结合、借助侧车系统对整个开发流程加以优化，从而提升软件开发的可控性和安全性。

Abstract: ``Vibe coding'' -- the practice of developing software through iteratively
conversing with a large language model (LLM) -- has exploded in popularity
within the last year. However, developers report key limitations including the
accumulation of technical debt, security issues, and code churn to achieve
satisfactory results. We argue that these pitfalls result from LLMs' inability
to reconcile accumulating human-imposed constraints during vibe coding, with
developers inadvertently failing to resolve contradictions because LLMs
prioritize user commands over code consistency. Given LLMs' receptiveness to
verification-based feedback, we argue that formal methods can mitigate these
pitfalls, making vibe coding more reliable. However, we posit that integrating
formal methods must transcend existing approaches that combine formal methods
and LLMs. We advocate for a side-car system throughout the vibe coding process
which: (1) \emph{Autoformalizes} specifications (2) Validates against targets,
(3) Delivers \emph{actionable} feedback to the LLM, and (4) Allows intuitive
developer influence on specifications.

</details>


### [13] [DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies](https://arxiv.org/abs/2511.00215)
*Xiaomeng Xu,Zahin Wahab,Reid Holmes,Caroline Lemieux*

Main category: cs.SE

TL;DR: 本文提出了DocPrism工具并引入LCEF方法，大幅降低了代码-文档不一致检测的误报率和提升了准确率，具备较强的跨语言应用能力。


<details>
  <summary>Details</summary>
Motivation: 代码与文档不一致现象普遍存在，会导致开发者误解和软件缺陷亟需高效检测工具。

Method: 提出DocPrism工具，利用标准的大型语言模型（LLM），并设计了“局部分类、外部过滤”（LCEF）方法来减少误报率。

Result: LCEF方法将误报率从98%降至14%，准确率从14%提升至94%；DocPrism在多种语言上的标记率维持在15%，精度为0.62，无需微调。

Conclusion: LCEF方法有效提升了使用LLM检测代码-文档不一致时的准确性，DocPrism工具实现了跨语言、高精度的一致性检测。

Abstract: Code-documentation inconsistencies are common and undesirable: they can lead
to developer misunderstandings and software defects. This paper introduces
DocPrism, a multi-language, code-documentation inconsistency detection tool.
DocPrism uses a standard large language model (LLM) to analyze and explain
inconsistencies. Plain use of LLMs for this task yield unacceptably high false
positive rates: LLMs identify natural gaps between high-level documentation and
detailed code implementations as inconsistencies. We introduce and apply the
Local Categorization, External Filtering (LCEF) methodology to reduce false
positives. LCEF relies on the LLM's local completion skills rather than its
long-term reasoning skills. In our ablation study, LCEF reduces DocPrism's
inconsistency flag rate from 98% to 14%, and increases accuracy from 14% to
94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism
maintains a low flag rate of 15%, and achieves a precision of 0.62 without
performing any fine-tuning.

</details>


### [14] [LLM-Driven Cost-Effective Requirements Change Impact Analysis](https://arxiv.org/abs/2511.00262)
*Romina Etezadi,Sallam Abualhaija,Chetan Arora,Lionel Briand*

Main category: cs.SE

TL;DR: 本文提出ProReFiCIA，一种利用大语言模型自动识别需求变更影响的方法，评估结果显示其召回率高、成本低，可显著提升需求管理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 需求在软件开发过程中经常变化，人工分析这些变化对其他需求的影响既费时又易出错，可能导致关键需求被忽略，进而影响后续开发工作。

Method: 提出一种基于大语言模型（LLM）的自动化方法——ProReFiCIA，用多个LLM及不同提示词对该方法进行实验性评估。

Result: 在不同数据集上的最佳结果，召回率分别达到93.3%（标准数据集）和95.8%（企业数据集），且审查工作量仅为全部需求的2.1%-8.5%。

Conclusion: ProReFiCIA能高效、准确地识别需求变更影响，极大减少人工审查成本，有助于需求工程的自动化和可靠性提升。

Abstract: Requirements are inherently subject to changes throughout the software
development lifecycle. Within the limited budget available to requirements
engineers, manually identifying the impact of such changes on other
requirements is both error-prone and effort-intensive. That might lead to
overlooked impacted requirements, which, if not properly managed, can cause
serious issues in the downstream tasks. Inspired by the growing potential of
large language models (LLMs) across diverse domains, we propose ProReFiCIA, an
LLM-driven approach for automatically identifying the impacted requirements
when changes occur. We conduct an extensive evaluation of ProReFiCIA using
several LLMs and prompts variants tailored to this task. Using the best
combination of an LLM and a prompt variant, ProReFiCIA achieves a recall of
93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,
demonstrating its strong effectiveness in identifying impacted requirements.
Further, the cost of applying ProReFiCIA remains small, as the engineer only
needs to review the generated results, which represent between 2.1% and 8.5% of
the entire set of requirements.

</details>


### [15] [Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework](https://arxiv.org/abs/2511.00417)
*Marcel Valovy*

Main category: cs.SE

TL;DR: 本论文提出ROMA框架，通过实证研究发现依据人格分配与优化人机协作角色能显著提升动机和团队表现，并为标准实践提出具体扩展建议。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能改变软件开发方式，研究者希望找出开发者与AI系统协作的最优方式，特别关注人类与AI在编程中的角色分配和动机匹配。

Method: 基于自我决定理论和人格心理学，提出ROMA框架。通过设计科学研究分五个周期，结合200位实验参与者与46名访谈对象，实证分析人格特质与编程角色偏好及协作成果的关系。

Result: 发现在分配编程角色时，基于人格特质的优化显著提升了自我决定感与团队动力，专业人员动机提升23%，本科生提升高达65%。归纳出五种人格原型，分别对应不同编程角色偏好。角色分配方式影响满意度。

Conclusion: 论文提出并验证了人格-角色偏好-自我决定之间的联系，建立了AI协作模式分类法与ISO/IEC 29110扩展，允许小型实体按人格优化协作角色，提升协作效果和动机。

Abstract: As artificial intelligence transforms software development, a critical
question emerges: how can developers and AI systems collaborate most
effectively? This dissertation optimizes human-AI programming roles through
self-determination theory and personality psychology, introducing the Role
Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes
empirically-validated connections between personality traits, programming role
preferences, and collaborative outcomes, engaging 200 experimental participants
and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization
significantly enhances self-determination and team dynamics, yielding 23%
average motivation increases among professionals and up to 65% among
undergraduates. Five distinct personality archetypes emerge: The Explorer (high
Openness/low Agreeableness), The Orchestrator (high
Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low
Extraversion), The Architect (high Conscientiousness), and The Adapter
(balanced profile). Each exhibits distinct preferences for programming roles
(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for
satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking
personality traits to role preferences and self-determination outcomes; (2) a
taxonomy of AI collaboration modalities mapped to personality profiles while
preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small
Entities to implement personality-driven role optimization within established
standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral
software engineering, self-determination theory, personality psychology,
phenomenology, intrinsic motivation, pair programming, design science research,
ISO/IEC 29110

</details>


### [16] [SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin](https://arxiv.org/abs/2511.00450)
*Vahid Etemadi,Gregorio Robles*

Main category: cs.SE

TL;DR: 该论文提出一款IntelliJ IDEA插件SmartDoc，结合方法及其调用上下文，使用AI技术自动生成高质量方法注释，实验证明其注释准确率高（BERTScore 0.80~0.90），有效提升代码理解和维护效率。


<details>
  <summary>Details</summary>
Motivation: 软件维护阶段涉及代码重构、修复漏洞、代码审查和测试等多个活动。程序理解对于这些活动至关重要，而方法作为程序的主要构建块，是开发者获取代码知识的重要来源。但完整阅读方法体内容十分困难，因此需要精确且最新的注释。

Method: 提出了一个名为SmartDoc的IntelliJ IDEA插件。SmartDoc作为AI代理，通过收集目标方法及其嵌套方法的上下文，建立调用图，并利用深度优先搜索（DFS）遍历该图，为大语言模型（LLM）生成更丰富的注释提示。插件可并发更新方法注释，并跨流程共享内存以避免冗余调用。

Result: SmartDoc插件适用于Java代码库，可在IntelliJ IDEA中安装。插件对生成的注释与标准答案进行了BERTScore、BLEU和ROUGE-1等指标的评估。BERTScore的精确率、召回率和F1均达到了0.80至0.90的较高水平，证明注释生成效果优秀。

Conclusion: SmartDoc插件能高效生成上下文感知的准确方法注释，提升软件维护阶段的方法理解和开发效率，其准确性表现优异，能为开发者带来良好助力。

Abstract: Context: The software maintenance phase involves many activities such as code
refactoring, bug fixing, code review or testing. Program comprehension is key
to all these activities, as it demands developers to grasp the knowledge (e.g.,
implementation details) required to modify the codebase. Methods as main
building blocks in a program can offer developers this knowledge source for
code comprehension. However, reading entire method statements can be
challenging, which necessitates precise and up-to-date comments. Objective: We
propose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists
developers in generating context-aware method comments. Method: This plugin
acts as an Artificial Intelligence (AI) agent that has its own memory and is
augmented by target methods' context. When a request is initiated by the
end-user, the method content and all its nested method calls are used in the
comment generation. At the beginning, these nested methods are visited and a
call graph is generated. This graph is then traversed using depth-first search
(DFS), enabling the provision of full-context to enrich Large Language Model
(LLM) prompts. Result: The product is a software, as a plugin, developed for
Java codebase and installable on IntelliJ IDEA. This plugin can serve
concurrently for methods whose comments are being updated , and it shares
memory across all flows to avoid redundant calls. o measure the accuracy of
this solution, a dedicated test case is run to record SmartDoc generated
comments and their corresponding ground truth. For each collected result-set,
three metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will
determine how accurate the generated comments are in comparison to the ground
truth. Result: The obtained accuracy, in terms of the precision, recall and F1,
is promising, and lies in the range of 0.80 to 0.90 for BERTScore.

</details>


### [17] [A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements](https://arxiv.org/abs/2511.00467)
*Liu Wang,Dong Wang,Shidong Pan,Zheng Jiang,Haoyu Wang,Yi Wang*

Main category: cs.SE

TL;DR: 本研究系统评价了苹果App Privacy Report的实际隐私保护作用，发现存在关键细节缺失导致用户实际受益有限。通过用户小组反馈，提出并验证了提升数据访问目的与域名清晰度的技术方案，有助于未来增强用户隐私透明度。


<details>
  <summary>Details</summary>
Motivation: 移动应用广泛使用，涉及用户隐私数据，因此需要提升数据访问和分享的透明度，让用户在数据被访问前能知情并同意。苹果推出了App Privacy Report，旨在增强用户对数据实践的了解，但其实际效果尚未被深入探讨。

Method: 开展端到端系统研究，包括对App Privacy Report实际效益和局限的系统性评估，集成大模型和多技术改进措施，并从系统和用户两个角度进行全面分析。采用焦点小组研究，调查12位普通iOS用户对该功能的认知和体验。

Result: App Privacy Report在实际应用中对用户隐私的提升有限，主要由于缺乏关键细节。用户关切集中于数据访问目的和域名描述的清晰度。研究提出了目的推断框架和域名澄清流程，实际验证了这些增强措施对用户隐私透明度的提升效果。

Conclusion: 现有隐私报告功能存在实用性不足，需通过技术优化提升关键信息的透明度。提出的增强方案可有效提升用户知情和控制能力，为未来提升移动应用隐私实践提供了方向。

Abstract: The prevalent engagement with mobile apps underscores the importance of
understanding their data practices. Transparency plays a crucial role in this
context, ensuring users to be informed and give consent before any data access
occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to
inform users about detailed insights into apps' data access and sharing. This
feature continues Apple's trend of privacy-focused innovations (following
Privacy Nutrition Labels), and has been marketed as a big step forward in user
privacy. However, its real-world impacts on user privacy and control remain
unexamined. We thus proposed an end-to-end study involving systematic
assessment of the App Privacy Report's real-world benefits and limitations,
LLM-enabled and multi-technique synthesized enhancements, and comprehensive
evaluation from both system and user perspectives. Through a structured focus
group study with twelve everyday iOS users, we explored their experiences,
understanding, and perceptions of the feature, suggesting its limited practical
impact resulting from missing important details. We identified two primary user
concerns: the clarity of data access purpose and domain description. In
response, we proposed enhancements including a purpose inference framework and
domain clarification pipeline. We demonstrated the effectiveness and benefits
of such enhancements for mobile app users. This work provides practical
insights that could help enhance user privacy transparency and discusses areas
for future research.

</details>


### [18] [Issue-Oriented Agent-Based Framework for Automated Review Comment Generation](https://arxiv.org/abs/2511.00517)
*Shuochuan Li,Dong Wang,Patanamon Thongtanunam,Zan Wang,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 现有代码评审自动评论生成模型受限于单一视角，RevAgent创新地采用多类别代理分工机制，实现评论的专业性和上下文相关性，大幅提升评测指标与实际效果，是代码自动评审领域的重要进展。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码评审评论生成技术依赖单一模型，导致模型难以处理多样且具体的问题类型，尤其在复杂场景（如bug修复）下生成的评论缺乏信息性。

Method: 提出RevAgent框架，将评论生成任务分为三个阶段：（1）五个面向特定类别的评论员代理分别从不同问题视角分析代码变更并生成候选评论；（2）评论筛选阶段由评论家代理选出最佳问题-评论对；（3）所有代理基于精选、面向问题类别的数据进行微调，以实现任务专精化。

Result: RevAgent在BLEU、ROUGE-L、METEOR和SBERT四项指标上分别领先最新PLM和LLM基线12.90%、10.87%、6.32%、8.57%。在复杂场景下的问题类别识别准确率也更高，经人工评价证实其生成的评论具有较高准确性、可读性和上下文感知，且性能与效率兼顾，表现优越。

Conclusion: RevAgent通过多代理分工和面向问题的专项微调，有效提升了自动代码评审评论生成的质量和实用性，特别适用于复杂和多变的问题场景，优于当前主流模型。

Abstract: Code review (CR) is a crucial practice for ensuring software quality. Various
automated review comment generation techniques have been proposed to streamline
the labor-intensive process. However, existing approaches heavily rely on a
single model to identify various issues within the code, limiting the model's
ability to handle the diverse, issue-specific nature of code changes and
leading to non-informative comments, especially in complex scenarios such as
bug fixes. To address these limitations, we propose RevAgent, a novel
agent-based issue-oriented framework, decomposes the task into three stages:
(1) Generation Stage, where five category-specific commentator agents analyze
code changes from distinct issue perspectives and generate candidate comments;
(2) Discrimination Stage, where a critic agent selects the most appropriate
issue-comment pair; and (3) Training Stage, where all agents are fine-tuned on
curated, category-specific data to enhance task specialization. Evaluation
results show that RevAgent significantly outperforms state-of-the-art PLM- and
LLM-based baselines, with improvements of 12.90\%, 10.87\%, 6.32\%, and 8.57\%
on BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively
higher accuracy in issue-category identification, particularly for challenging
scenarios. Human evaluations further validate the practicality of RevAgent in
generating accurate, readable, and context-aware review comments. Moreover,
RevAgent delivers a favorable trade-off between performance and efficiency.

</details>


### [19] [HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models](https://arxiv.org/abs/2511.00527)
*Robab Aghazadeh-Chakherlou,Qing Guo,Siddartha Khastgir,Peter Popov,Xiaoge Zhang,Xingyu Zhao*

Main category: cs.SE

TL;DR: 本文提出HIP-LLM框架，通过层次化不精确概率理论提升大语言模型可靠性评估的严谨性和实用性，在多个数据集上取得更优结果，并已开源实现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在各个领域的广泛应用，如何严格评估其可靠性变得日益重要。但现有基于基准的评估方法多局限于统计准确率，难以反映LLM在实际应用场景下的概率行为。

Method: 提出了一种新的层次化不精确概率（HIP-LLM）框架，用于建模和推断LLM的可靠性。该方法结合软件可靠性工程理念，将LLM的可靠性定义为在特定操作环境下未来任务中的无故障运行概率。通过分层建模（包含子域-系统多层次）和引入不精确先验以刻画认知不确定性，并结合实际操作环境进行可靠性评估。输出为反映先验和数据不确定性的后验可靠性包络线。

Result: 在多个基准数据集上实验证明，HIP-LLM比现有方法能更准确、标准化地表征LLM的可靠性。同时开源了HIP-LLM的实现。

Conclusion: HIP-LLM为LLM可靠性表征提供了理论完善且可操作的解决方案，相比现有方法，在应对不确定性和多层级应用条件下表现更优。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
domains, raising the need for rigorous reliability assessment methods. Existing
benchmark-based evaluations primarily offer descriptive statistics of model
accuracy over datasets, providing limited insight into the probabilistic
behavior of LLMs under real operational conditions. This paper introduces
HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and
inferring LLM reliability. Building upon the foundations of software
reliability engineering, HIP-LLM defines LLM reliability as the probability of
failure-free operation over a specified number of future tasks under a given
Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains
hierarchically, enabling multi-level inference from subdomain to system-level
reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty
and incorporates OPs to reflect usage contexts. It derives posterior
reliability envelopes that quantify uncertainty across priors and data.
Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a
more accurate and standardized reliability characterization than existing
benchmark and state-of-the-art approaches. A publicly accessible repository of
HIP-LLM is provided.

</details>


### [20] [Employee Performance when Implementing Agile Practices in an IT Workforce](https://arxiv.org/abs/2511.00528)
*Muhammad Hamid Raza Mookadam,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 南非IT行业敏捷实践显著提升了员工绩效和团队协作，但仍面临采纳难题、团队参与和领导力等障碍。解决这些问题并加强支持后，员工表现可大幅提升。


<details>
  <summary>Details</summary>
Motivation: 目前在非洲地区，关于IT行业采用敏捷实践后员工绩效的系统性研究较少。该研究旨在填补这一研究空白，专注于南非IT行业员工在敏捷环境下的表现。

Method: 本研究采用解释主义单方法的定性研究，通过半结构化访谈作为研究工具，共采访了来自不同岗位的17名敏捷从业者，深入探索其在敏捷环境下的真实体验与绩效影响因素。

Result: 结果显示，敏捷实践对员工绩效有显著影响，具体包括规划、沟通、员工发展和福祉、协作、团队文化及进展。同时也发现了敏捷实践面临的障碍，如采纳过程、团队参与度、领导力以及植入敏捷思维的挑战。敏捷实践能够促进团队动力、协作、效率提升、风险管理、规划、持续改进、学习以及个人发展与福祉方面的提升。

Conclusion: 只要解决敏捷实践过程中的挑战，并给予额外支持，员工的绩效将能得到显著提升。

Abstract: Adoption of agile practices has increased in IT workforces. However, there is
a lack of comprehensive studies in the African context on employee performance
when implementing agile practices. This study addresses this gap by exploring
employee performance in agile environments for IT workforces in South Africa.
An interpretivist mono-method qualitative approach was used, with the use of
interviews as a research strategy. Seventeen semi-structured interviews were
conducted with agile practitioners from various roles. Our results indicated
that agile practices influence employee performance significantly, with
participants reporting on aspects which included planning, communication,
employee development and well-being, collaboration, team culture and progress.
Additionally, our results reported obstacles when using agile practices that
included adoption, team engagement, leadership and instilling an agile mindset.
Agile practices influence employee performance in IT workforces by fostering
improved team dynamics, enhanced collaboration, improved efficiencies, risk
management, planning, continuous improvement, learning, personal development
and well-being. Conclusively, our findings suggest that if agile challenges are
addressed and additional support is provided, employee performance can be
significantly improved.

</details>


### [21] [GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android](https://arxiv.org/abs/2511.00619)
*Huaijin Ran,Haoyi Zhang,Xunzhu Tang*

Main category: cs.SE

TL;DR: 本文提出了首个GDPR合规检测安卓代码基准（GDPR-Bench-Android），并通过11种方法系统评测。结果显示，各自动化方法在不同任务上表现有明显差异。该基准为未来相关研究提供了重要基础；相关资源已公开。


<details>
  <summary>Details</summary>
Motivation: 自动检测源代码中是否违反欧盟通用数据保护条例（GDPR）是一个重要但尚未被充分研究的挑战。目前缺乏完整的基准来系统评价针对GDPR合规性检测的自动化方法。

Method: 本论文提出了GDPR-Bench-Android，这是首个用于GDPR合规检测的安卓应用代码基准，拥有从15个开源项目中手工标注的1951个违规实例，覆盖23项GDPR条款，分为文件、模块和代码行3种粒度。作者还提出了源码原生的形式化方法Formal-AST作为基准，并定义了两项任务：1）多粒度定位违规（Accuracy@k评测）；2）代码片段多标签分类（以Macro-F1等分类指标评测）。随后对11种方法进行了评测，涵盖8个前沿大语言模型（LLMs）、Formal-AST、检索增强法（RAG）、agentic法（ReAct）。

Result: 没有一种范式在所有任务上都表现最优。在任务1中，ReAct方法在文件级定位上准确率最高（17.38%），Qwen2.5-72B在行级定位上表现最好（61.60%），均远超Formal-AST（1.86%）。在更难的多标签任务2中，Claude-Sonnet-4.5 LLM获得最高Macro-F1（5.75%），RAG方法Macro-Precision最高（7.10%）。

Conclusion: 不同自动化方法在不同任务上各具优势，没有通用最优方案。新提出的GDPR-Bench-Android为评测和诊断这些方法提供了有价值的工具，有助于推动GDPR合规检测研究的发展。

Abstract: Automating the detection of EU General Data Protection Regulation (GDPR)
violations in source code is a critical but underexplored challenge. We
introduce \textbf{GDPR-Bench-Android}, the first comprehensive benchmark for
evaluating diverse automated methods for GDPR compliance detection in Android
applications. It contains \textbf{1951} manually annotated violation instances
from \textbf{15} open-source repositories, covering 23 GDPR articles at file-,
module-, and line-level granularities. To enable a multi-paradigm evaluation,
we contribute \textbf{Formal-AST}, a novel, source-code-native formal method
that serves as a deterministic baseline. We define two tasks: (1)
\emph{multi-granularity violation localization}, evaluated via
Accuracy@\textit{k}; and (2) \emph{snippet-level multi-label classification},
assessed by macro-F1 and other classification metrics. We benchmark 11 methods,
including eight state-of-the-art LLMs, our Formal-AST analyzer, a
retrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings
reveal that no single paradigm excels across all tasks. For Task 1, the ReAct
agent achieves the highest file-level Accuracy@1 (17.38%), while the
Qwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the
Formal-AST method's 1.86%. For the difficult multi-label Task 2, the
Claude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method
yields the highest Macro-Precision (7.10%). These results highlight the
task-dependent strengths of different automated approaches and underscore the
value of our benchmark in diagnosing their capabilities. All resources are
available at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.

</details>


### [22] [Can Large Language Models Detect Real-World Android Software Compliance Violations?](https://arxiv.org/abs/2511.00624)
*Haoyi Zhang,Huaijin Ran,Xunzhu Tang*

Main category: cs.SE

TL;DR: 提出并公开了CompliBench框架，对现有大语言模型在代码合规检测方面的能力进行系统化评估及提升，强调跨粒度和法域一致性，为今后合规自动化工具发展提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在检测Android应用在不同法律框架下的合规违规行为时表现不佳，尤其跨法域与多粒度上的稳定性和一致性评估存在不足。

Method: 提出了CompliBench，一个面向合规违规检测的新型评估框架。该框架设计了两个任务：一是按文件、模块、代码行等多个粒度进行违规定位与检索，二是对代码片段进行多标签合规判断。同时引入跨粒度、法域稳定性相关的综合评估指标（如SGS、RCS、CRGS、OCS），用于更全面地衡量模型性能。

Result: 通过对六种主流大语言模型（包括GPT-4O和Claude-3.5等）的实验，CompliBench框架显示能提升模型的合规检测能力。其中，Claude-3.5-sonnet-20241022的OCS评分最高（0.3295），Gemini-2.5-pro最低（0.0538）。

Conclusion: CompliBench评估框架有助于提升大语言模型在合规任务上的表现，同时为满足数据保护法规，开发更可靠的自动化合规工具奠定了基础。

Abstract: The rapid development of Large Language Models (LLMs) has transformed
software engineering, showing promise in tasks like code generation, bug
detection, and compliance checking. However, current models struggle to detect
compliance violations in Android applications across diverse legal frameworks.
We propose \emph{CompliBench}, a novel evaluation framework for assessing LLMs'
ability to detect compliance violations under regulations like LGPD, PDPA, and
PIPEDA. The framework defines two tasks: Task 1 evaluates \emph{retrieval and
localization} at file, module, and line granularities, and Task 2 assesses
\emph{multi-label judgment} for code snippets. These tasks mirror the audit
process, where auditors locate problematic code and determine implicated
provisions. Traditional metrics fail to capture important aspects like
cross-granularity stability and jurisdictional consistency. Thus, we introduce
stability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive
assessment. Experiments with six models, including GPT-4O and Claude-3.5, show
\emph{CompliBench} improves compliance detection, with
Claude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and
Gemini-2.5-pro the lowest (0.0538). This work demonstrates \emph{CompliBench}'s
potential for improving LLM performance in compliance tasks and provides a
foundation for future tools aligned with data protection standards. Our project
is available at https://github.com/Haoyi-Zhang/CompliBench.

</details>


### [23] [Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare](https://arxiv.org/abs/2511.00658)
*Guilherme H. Travassos,Sabrina Rocha,Rodrigo Feitosa,Felipe Assis,Patricia Goncalves,Andre Gheventer,Larissa Galeno,Arthur Sasse,Julio Cesar Guimaraes,Carlos Brito,Joao Pedro Wieland*

Main category: cs.SE

TL;DR: 本文通过使用生成式AI开发临床试验管理系统的实际案例，总结了其在软件开发各环节的应用经验，尽管技术尚不成熟，但为未来AI驱动的软件开发创新提供了启示。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（AI）技术的进步正在推动各行业的变革，软件工程作为技术密集型领域也面临着如何利用生成式AI提升生产力和开发质量的问题。目前生成式AI在软件工程中的应用尚处于初期阶段，缺乏系统的研究结论与成熟的技术方案，因此作者希望通过实践探索其在软件开发流程中的价值。

Method: 作者团队将生成式AI集成到临床试验相关的网络软件系统开发中，并记录和分析了在项目管理、需求分析、设计、开发以及质量保障等各个环节使用生成式AI的学习经验与实际成效。

Result: 尽管尚未获得突破性的技术证据，但在开发流程中的探索为如何借助生成式AI赋能软件开发积累了经验，并为软件团队创新开发实践提供了指导性建议。

Conclusion: 生成式AI在软件工程领域应用尚不成熟，但实践报告显示，其在提升开发流程和软件质量方面存在潜力，报告中的见解可为希望引入生成式AI的组织提供有益参考。

Abstract: The advances and availability of technologies involving Generative Artificial
Intelligence (AI) are evolving clearly and explicitly, driving immediate
changes in various work activities. Software Engineering (SE) is no exception
and stands to benefit from these new technologies, enhancing productivity and
quality in its software development processes. However, although the use of
Generative AI in SE practices is still in its early stages, considering the
lack of conclusive results from ongoing research and the limited technological
maturity, we have chosen to incorporate these technologies in the development
of a web-based software system to be used in clinical trials by a thoracic
diseases research group at our university. For this reason, we decided to share
this experience report documenting our development team's learning journey in
using Generative AI during the software development process. Project
management, requirements specification, design, development, and quality
assurance activities form the scope of observation. Although we do not yet have
definitive technological evidence to evolve our development process
significantly, the results obtained and the suggestions shared here represent
valuable insights for software organizations seeking to innovate their
development practices to achieve software quality with generative AI.

</details>


### [24] [Repairing Responsive Layout Failures Using Retrieval Augmented Generation](https://arxiv.org/abs/2511.00678)
*Tasmia Zerin,Moumita Asad,B. M. Mainul Hossain,Kazi Sakib*

Main category: cs.SE

TL;DR: 本文提出了ReDeFix系统，结合LLM与Stack Overflow知识，自动生成CSS修复方案，有效修复响应式网站的布局失败，准确率达88%，且视觉、美观性兼顾，对前端开发有很大助力。


<details>
  <summary>Details</summary>
Motivation: 响应式网站在某些屏幕尺寸下常出现布局变形（RLFs），人工修复需要繁琐的调试和调整，效率低下、易出错，所以需要自动化修复方法。

Method: 提出了一种结合大语言模型（LLM）与领域知识的自动化修复方法。通过RAG（检索增强生成）机制，利用Stack Overflow（SO）讨论中的知识，结合具体RLF场景生成优化的Prompt，再由LLM生成针对性CSS修复补丁。该方法取名为ReDeFix。

Result: 评估结果显示ReDeFix能以88%的准确率修复RLFs。同时，软件工程师的主观评价表明生成的修复具备视觉正确性和美观性。

Conclusion: ReDeFix能够高效、准确地自动修复响应式网站中的布局失败，为前端开发者提供实用解决方案，同时保证视觉效果和美观性。

Abstract: Responsive websites frequently experience distorted layouts at specific
screen sizes, called Responsive Layout Failures (RLFs). Manually repairing
these RLFs involves tedious trial-and-error adjustments of HTML elements and
CSS properties. In this study, an automated repair approach, leveraging LLM
combined with domain-specific knowledge is proposed. The approach is named
ReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes
Stack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting
relevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that
is sent to the LLM to generate CSS patches. Evaluation demonstrates that our
approach achieves an 88\% accuracy in repairing RLFs. Furthermore, a study from
software engineers reveals that generated repairs produce visually correct
layouts while maintaining aesthetics.

</details>


### [25] [An Empirical Investigation of the Experiences of Dyslexic Software Engineers](https://arxiv.org/abs/2511.00706)
*Marcos Vinicius Cruz,Pragya Verma,Grischa Liebel*

Main category: cs.SE

TL;DR: 本文通过定性研究，首次系统性分析了阅读障碍软件工程师的经验，发现他们在编程学习阶段面临挑战但能克服，且具有视觉思维和创造力优势，适合的软件工程工具有助于支持他们。研究为行业实践和未来相关研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 阅读障碍在软件工程领域及其对核心任务如编程的影响尚未被深入探索，既有研究也未将阅读障碍者的优势与困难联系起来。该研究动机是填补关于阅读障碍软件工程师实际经验的研究空白。

Method: 采用社会技术扎根理论的基本阶段，通过定性研究方法收集和分析数据，包括10位阅读障碍软件工程师的访谈、3篇博客帖子及153条Reddit社交媒体发帖。

Result: 阅读障碍软件工程师在学习编程阶段面临较大困难，但在掌握后能够在软件工程任务中取得成功甚至表现优异。常见的支持工具如代码补全和语法检查显著缓解其障碍。此外，他们在视觉思维和创造力等方面展现出突出优势。

Conclusion: 阅读障碍者虽然在编程学习阶段存在挑战，但掌握技能后能出色完成软件工程任务。合适的工具能有效帮助他们克服困难，且他们的独特优势对软件工程实践具有积极意义。这为理解和支持阅读障碍软件工程师提供了重要参考，也提出了未来研究方向，如提升代码可理解性对不同群体的影响。

Abstract: Dyslexia is a common learning disorder that primarily impairs an individual's
reading and writing abilities. In adults, dyslexia can affect both professional
and personal lives, often leading to mental challenges and difficulties
acquiring and keeping work. In Software Engineering (SE), reading and writing
difficulties appear to pose substantial challenges for core tasks such as
programming. However, initial studies indicate that these challenges may not
significantly affect their performance compared to non-dyslexic colleagues.
Conversely, strengths associated with dyslexia could be particularly valuable
in areas like programming and design. However, there is currently no work that
explores the experiences of dyslexic software engineers, and puts their
strengths into relation with their difficulties. To address this, we present a
qualitative study of the experiences of dyslexic individuals in SE. We followed
the basic stage of the Socio-Technical Grounded Theory method and base our
findings on data collected through 10 interviews with dyslexic software
engineers, 3 blog posts and 153 posts on the social media platform Reddit. We
find that dyslexic software engineers especially struggle at the programming
learning stage, but can succeed and indeed excel at many SE tasks once they
master this step. Common SE-specific support tools, such as code completion and
linters are especially useful to these individuals and mitigate many of the
experienced difficulties. Finally, dyslexic software engineers exhibit
strengths in areas such as visual thinking and creativity. Our findings have
implications to SE practice and motivate several areas of future research in
SE, such as investigating what makes code less/more understandable to dyslexic
individuals.

</details>


### [26] [A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI](https://arxiv.org/abs/2511.00776)
*Cuiyun Gao,Guodong Fan,Chun Yong Chong,Shizhan Chen,Chao Liu,David Lo,Zibin Zheng,Qing Liao*

Main category: cs.SE

TL;DR: 本文全面梳理了代码相关大模型的幻觉问题，从定义、成因、解决方案到评测方法，并强调需发展专用于幻觉检测的评价体系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码智能领域应用愈发广泛，但幻觉问题严重影响模型可靠性，特别是在高风险软件工程任务中，因此对幻觉问题进行系统综述具有重要意义。

Method: 通过调研60篇相关论文，梳理幻觉定义、成因与应对策略，并比对NLP与软件工程社区最新进展，同时归纳代码智能中的独特挑战与当前幻觉检测、评测方法。

Result: 总结了代码幻觉的主要成因（如数据噪声、暴露偏差等）、应对策略（知识增强、受限解码、后处理等），分析了代码任务中的特别挑战，并归纳当前幻觉相关的评测手段和存在的不足。

Conclusion: 本文系统性地综述了代码领域大模型幻觉问题，从定义、原因、应对方法、特定挑战及评测等多个维度进行了深入分析，并指出需要专门的幻觉评测基准。

Abstract: Model hallucination is one of the most critical challenges faced by Large
Language Models (LLMs), especially in high-stakes code intelligence tasks. As
LLMs become increasingly integrated into software engineering tasks,
understanding and mitigating hallucination in code becomes essential. In this
survey, we provide a systematic review of hallucination phenomena in
code-oriented LLMs from four key perspectives. First, we begin by surveying 60
papers to define hallucination in the context of code and summarize its primary
causes, such as data noise, exposure bias, and insufficient semantic grounding,
while also tracing recent trends in literature across natural language
processing (NLP) and software engineering communities. Second, we review model
hallucination surveys in a broader span and summarize representative
hallucination mitigation strategies, such as knowledge-enhanced generation,
constrained decoding, and post-editing. Third, we review approaches targeted
for code intelligence and highlight code-specific challenges that aggravate
hallucination, including syntax sensitivity, strict type systems, and
dependence on external libraries. Meanwhile, we analyze how emerging code
intelligence tasks, e.g., program analysis, symbolic execution, and unit
testing, are utilized to detect and mitigate hallucinations. Fourth, we
summarize current evaluation benchmarks, ranging from static metrics to dynamic
checks, e.g., compilation and execution correctness, and emphasize the need for
hallucination-oriented benchmarks.

</details>


### [27] [Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems](https://arxiv.org/abs/2511.00780)
*Chenyu Zhao,Shenglin Zhang,Zeshun Huang,Weilin Jin,Yongqian Sun,Dan Pei,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Minghua Ma*

Main category: cs.SE

TL;DR: 本文提出了首个针对跨指令集架构软件迁移修复的LLM评测基准Build-bench，覆盖真实失败构建案例和工具链辅助流程。实验表明，现有LLMs最高修复成功率为63%，不同模型对工具的利用方式差异较大。该基准填补了跨ISA软件修复领域的评测空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在软件工程领域展现了越来越强的能力，但其在指令集架构（ISA）迁移修复软件方面的能力缺乏有效评测。跨ISA迁移（如x86_64与aarch64之间）需要应对复杂的依赖关系、异构工具链和冗长的构建日志，并保证可执行性验证，现有基准尚未覆盖这一难点。

Method: 提出了Build-bench，这是一个端到端基准，系统地评估LLMs在跨ISA环境下修复构建失败的能力。通过收集268个真实软件包的失败案例，并集成结构提取、文件内容提取、内容修改、构建验证等工具，支持模型自主演化和工具辅助推理。修复过程采用迭代循环，模型在每次失败后获取最新构建日志和前一次修复结果，以优化后续尝试。

Result: 对六种代表性LLMs进行对比评测，结果显示当前模型最高构建成功率为63%，且模型之间的工具使用模式差异显著。

Conclusion: Build-bench通过将真实构建环境与可验证结果结合，建立了首个面向架构的、可用于LLM软件构建和修复研究的基准。

Abstract: Large language models (LLMs) have shown growing potential in software
engineering, yet few benchmarks evaluate their ability to repair software
during migration across instruction set architectures (ISAs). Cross-ISA
migration, such as between x86_64 and aarch64, requires handling complex
dependencies, heterogeneous toolchains, and long build logs while ensuring
executable verification. To address this challenge, we present Build-bench, an
end-to-end benchmark that systematically evaluates the capability of LLMs to
repair build failures in cross-ISA settings. Build-bench collects 268
real-world failed packages and integrates auxiliary tools including Structure
Extraction, File Content Extraction, Content Modification, and Build
Verification to support autonomous, tool-augmented reasoning. The repair
process operates in an iterative loop where, upon failure, the model receives
updated build logs and previous repair outcomes to refine subsequent attempts.
Through a comparative evaluation of six representative LLMs, Build-bench
reveals that current models achieve a maximum build success rate of 63% and
tool usage patterns differ significantly across models. By coupling real build
environments with verifiable outcomes, Build-bench establishes the first
architecture-aware benchmark for studying LLM-based software build and repair.

</details>


### [28] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 本文提出并验证了通过LLM代理进行OPE代码优化的方法，极大提升了效果，证明了LLM作为自动优化工具的实际可用性。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试资源消耗大且风险高，OPE成为关键技术。但目前鲜有研究利用LLM及其代理机制提升OPE效率和效果。

Method: 提出了GrowthHacker基准测试，设计了基于代理的方法（如two_agent框架），进行代码优化、结果评估和循环优化实验。实现了对OPE主流基线的改进，并在Open Bandit Pipeline与Scope-RL数据集上验证了方法有效性。

Result: two_agent框架在所有实验中实现100%可靠性，并在正向结果中达到106.7%的平均提升率。two_agent与CrewAI的成功率为45%，显著优于AutoGen的34%。

Conclusion: 研究表明，基于LLM的智能体能够作为自动化的“增长黑客”，有效改进OPE（离线A/B测试）系统，并在大规模现实数据集上实现了性能提升，显示了其在实际数据驱动决策中的应用前景。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [29] [CodeClash: Benchmarking Goal-Oriented Software Engineering](https://arxiv.org/abs/2511.00839)
*John Yang,Kilian Lieret,Joyce Yang,Carlos E. Jimenez,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: 本文提出了CodeClash评测体系，创新性考察模型在无具体指导下实现高阶目标的能力。实验显示现有代码大模型在策略规划和代码维护上均远逊于人类。CodeClash开放源代码，可用于未来自动化编程能力的提升与研究。


<details>
  <summary>Details</summary>
Motivation: 当前编程评测主要关注于语言模型在具体、明确任务中的表现，如修复特定的bug或编写测试用例。但这并不能真实反映人类程序员在软件开发中面对的开放性、高阶目标。本文旨在评估语言模型是否能够自主地为实现开放性目标进行迭代式代码开发。

Method: 提出CodeClash评测基准，让语言模型在多轮锦标赛中竞争，每轮包括自主编辑代码和在竞技场中依据任务目标（如得分最大化、资源获取或生存）进行代码对抗。比赛共包括1680场、25200轮，对8个语言模型在6个竞技场进行评测。

Result: 不同模型在开发风格上存在差异，但在策略推理和长期代码维护上均有明显局限，随着比赛进行，代码库普遍变得杂乱和冗余。所有顶级模型在与人类程序员对抗时，每轮均完败。

Conclusion: 现有语言模型在真实自主、目标驱动的软件开发任务中存在明显不足，难以达到人类程序员水平。CodeClash作为开源基准，有助于推进模型在自驱动代码开发上的研究。

Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete,
well-specified tasks such as fixing specific bugs or writing targeted tests.
However, human programmers do not spend all day incessantly addressing isolated
tasks. Instead, real-world software development is grounded in the pursuit of
high-level goals, like improving user retention or reducing costs. Evaluating
whether LMs can also iteratively develop code to better accomplish open-ended
objectives without any explicit guidance remains an open challenge. To address
this, we introduce CodeClash, a benchmark where LMs compete in multi-round
tournaments to build the best codebase for achieving a competitive objective.
Each round proceeds in two phases: agents edit their code, then their codebases
compete head-to-head in a code arena that determines winners based on
objectives like score maximization, resource acquisition, or survival. Whether
it's writing notes, scrutinizing documentation, analyzing competition logs, or
creating test suites, models must decide for themselves how to improve their
codebases both absolutely and against their opponents. We run 1680 tournaments
(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal
that while models exhibit diverse development styles, they share fundamental
limitations in strategic reasoning. Models also struggle with long-term
codebase maintenance, as repositories become progressively messy and redundant.
These limitations are stark: top models lose every round against expert human
programmers. We open-source CodeClash to advance the study of autonomous,
goal-oriented code development.

</details>


### [30] [A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks](https://arxiv.org/abs/2511.00872)
*Zhuowen Yin,Cuifeng Gao,Chunsong Fan,Wenzhang Yang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: 本文系统评估了七种通用agent框架在多项代码任务中的表现，揭示了不同框架在有效性、效率、成本等维度的优劣，为agent技术在软件工程领域的应用与优化提供了实践建议。


<details>
  <summary>Details</summary>
Motivation: 目前关于AI agents在软件工程领域的应用，多数研究聚焦于单一任务或局部能力，缺乏对其整体实用性的系统评估。本研究旨在全面评估主流agent框架在多项代码相关任务中的表现及其优缺点。

Method: 本文选取了七种通用agent框架，在软件开发、漏洞检测和程序修复三项代表性代码任务中进行测试。所有任务均采用标准基准集，系统从有效性（任务完成率）、效率（执行过程）和开销（令牌消耗）三个维度进行多角度分析，并深入探讨效果和效率之间的关系。

Result: 不同agent框架在任务表现上各有优势与权衡。例如，有效性整体中等，AgentOrchestra执行过程最繁琐，OpenHands反思能力突出，GPTswarm性价比最高。软件开发任务的开销最大。

Conclusion: 本研究为agent在软件工程中的实际应用提供了全景式洞察，为高效agent框架的后续研发和实际部署提供参考。

Abstract: Unlike traditional automation tools or static LLM-based systems, agents
combine decision-making and tool utilization to accomplish complex tasks,
showing great potential in software engineering. However, existing studies
largely focus on specific tasks or isolated aspects, providing an incomplete
picture of agents' practical capabilities. To address this, we conduct a
comprehensive empirical study evaluating seven general-purpose agent frameworks
across three representative code-centric tasks: software development,
vulnerability detection, and program repair. Each task is assessed using
standard, widely adopted benchmarks to ensure objective and comparable
evaluation. Agent performance is systematically analyzed from three
complementary perspectives: effectiveness (task success), efficiency (execution
process), and overhead (token consumption). Our findings reveal distinct
capability patterns and trade-offs among the evaluated frameworks. In terms of
effectiveness, agents achieve moderate overall performance. Regarding
efficiency, AgentOrchestra tends to exhibit the longest trajectories and the
most correction attempts due to coordination overhead, whereas OpenHands
demonstrate stronger reflective reasoning abilities. For overhead, software
development incurs the highest monetary cost, while GPTswarm remains the most
cost-efficient. Furthermore, we conduct an in-depth cross-analysis of the
relationship between effectiveness and efficiency, exploring the underlying
reasons behind their interplay. These findings guide both practical adoption
and future research toward more efficient software engineering agents.

</details>


### [31] [Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective](https://arxiv.org/abs/2511.00901)
*Vincenzo De Martino,Stefano Lambiase,Fabiano Pecorelli,Willem-Jan van den Heuvel,Filomena Ferrucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文通过质性访谈和问卷调查揭示ML系统中可持续性理念虽逐渐被关注，但实践上具体应用明显不足，尤其非环境层面的可持续性。呼吁制定具体实施指南和测量标准，加强政策及监管，促进ML开发中的全面可持续性落地。


<details>
  <summary>Details</summary>
Motivation: 软件可持续性作为一项关键的多维非功能性需求，涵盖环境、社会、经济等方面，但在机器学习（ML）系统开发中的融入依然面临挑战。现有研究多为原则性或政策建议，且主要聚焦于环境问题，缺乏对实际工作流程中可持续性管理的实证研究，忽视了其它维度及工程师真实遇到的问题。

Method: 采用实证研究方法：先通过对8位有经验的ML工程师进行访谈进行定性分析，再通过对203位ML从业者的问卷调查进行大规模定量研究。

Result: 发现了ML工程师对可持续性认知与其在实际工作中系统性应用之间存在显著脱节，工程实践中缺乏有体系的指导、可衡量的框架及监管支持。

Conclusion: ML系统领域在可持续性意识虽有提升，但缺少配套的落地措施、工具和政策，亟需开发系统性指导、测量标准与监管机制，以推动可持续性在实际开发中的实现。

Abstract: Software sustainability is a key multifaceted non-functional requirement that
encompasses environmental, social, and economic concerns, yet its integration
into the development of Machine Learning (ML)-enabled systems remains an open
challenge. While previous research has explored high-level sustainability
principles and policy recommendations, limited empirical evidence exists on how
sustainability is practically managed in ML workflows. Existing studies
predominantly focus on environmental sustainability, e.g., carbon footprint
reduction, while missing the broader spectrum of sustainability dimensions and
the challenges practitioners face in real-world settings. To address this gap,
we conduct an empirical study to characterize sustainability in ML-enabled
systems from a practitioner's perspective. We investigate (1) how ML engineers
perceive and describe sustainability, (2) the software engineering practices
they adopt to support it, and (3) the key challenges hindering its adoption. We
first perform a qualitative analysis based on interviews with eight experienced
ML engineers, followed by a large-scale quantitative survey with 203 ML
practitioners. Our key findings reveal a significant disconnection between
sustainability awareness and its systematic implementation, highlighting the
need for more structured guidelines, measurement frameworks, and regulatory
support.

</details>


### [32] [Empirical Derivations from an Evolving Test Suite](https://arxiv.org/abs/2511.00915)
*Jukka Ruohonen,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 本文分析了NetBSD系统十余年内的自动化测试数据，发现其测试体系在持续扩展且整体失败率稳定，代码和内核变动对失败率影响有限。


<details>
  <summary>Details</summary>
Motivation: 随着大型开源系统（如NetBSD）逐步演变，需要系统地分析其自动化测试体系随时间变化的表现和影响因素，为软件工程的实践和测试套件改进提供依据。

Method: 对NetBSD操作系统自动化、持续、虚拟化的测试套件进行长期实证分析，从2010年代初至2025年，结合测试用例、失败率、构建与安装失败等多维度数据。

Result: 测试用例数量持续增长，覆盖已超一万例；多数测试失败长期保持稳定，但有短期波动；构建、安装和测试过程失败也呈类似特征；代码变更和内核修改一般难以长期解释这些失败。

Conclusion: 尽管代码变化和内核修改偶有影响，长期来看它们与测试失败的关联较弱，测试套件整体表现稳定。

Abstract: The paper presents a longitudinal empirical analysis of the automated,
continuous, and virtualization-based software test suite of the NetBSD
operating system. The longitudinal period observed spans from the initial roll
out of the test suite in the early 2010s to late 2025. According to the
results, the test suite has grown continuously, currently covering over ten
thousand individual test cases. Failed test cases exhibit overall stability,
although there have been shorter periods marked with more frequent failures. A
similar observation applies to build failures, failures of the test suite to
complete, and installation failures, all of which are also captured by the
NetBSD's testing framework. Finally, code churn and kernel modifications do not
provide longitudinally consistent statistical explanations for the failures.
Although some periods exhibit larger effects, including particularly with
respect to the kernel modifications, the effects are small on average. Even
though only in an exploratory manner, these empirical observations contribute
to efforts to draw conclusions from large-scale and evolving software test
suites.

</details>


### [33] [DPO-F+: Aligning Code Repair Feedback with Developers' Preferences](https://arxiv.org/abs/2511.01043)
*Zihan Fang,Yifan Zhang,Yueke Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: 本文提出了DPO-f+框架，通过针对开发者画像优化代码修复反馈，显著提升了修复准确率及反馈对齐效果，有助于人机协同和代码理解。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在代码修复等软件工程领域广泛应用，但开发者在理解模型输出方面存在困难，制约了人机协作的效率。以往研究主要关注修复代码本身，较少关注能促进理解和迭代改进的自然语言反馈。

Method: 提出DPO-f+框架，（1）形式化开发者画像和领域相关的反馈对齐指标；（2）自动从代码修复任务生成成对偏好数据集；（3）利用直接偏好优化（DPO），并引入轻量级边际信号进行微调；（4）设计自动化反馈评估协议。

Result: 在新手编程任务上，DPO-f+较基线模型提升top-1通过率5.71个百分点，较标准DPO提升3.30个百分点；在更难的SWE-bench Lite基准上，问题解决率较DPO提升1.67个百分点，较基线提升4.67个百分点。同时，其反馈对齐效果也优于DPO和基线。

Conclusion: DPO-f+能够更好地将代码修复反馈与开发者需求对齐，将LLM辅助代码修复由一次性输出转变为协同分析流程，从而提升代码理解水平与人机协作效率，为增强软件工程中的人机协作提供了实用方法。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
tasks, especially code repair. However, developers often struggle to interpret
model outputs, limiting effective human-AI teaming. Prior work largely
optimizes repaired code while under-addressing the natural-language feedback
that enables comprehension and iterative improvement. We present DPO-f+, a
novel framework that aligns code-repair feedback with developer needs and
profiles. It (1) formalizes developer-profiled, domain-specific metrics for
feedback alignment; (2) automatically constructs pairwise preference datasets
from code-repair tasks; (3) fine-tunes using Direct Preference Optimization
(DPO) augmented with a lightweight margin signal; and (4) provides an automated
feedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline
and standard DPO on generated-code accuracy and overall feedback alignment. On
novice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage
points (pp) over the baseline and by 3.30 pp over DPO. On the more challenging
SWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp
over DPO and by 4.67 pp over the baseline. It also achieves the largest
improvement in feedback alignment, outperforming DPO and the baseline. By
aligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted
repair from one-shot outputs into a collaborative sensemaking workflow,
providing a practical approach to enhancing code comprehension and fostering
more effective human-AI teaming in software engineering.

</details>


### [34] [HAFixAgent: History-Aware Automated Program Repair Agent](https://arxiv.org/abs/2511.01047)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出 HAFixAgent，将仓库历史信息结合到自动化程序修复智能体流程中，在修复复杂 bug 上效果显著优于现有方法，且效率和成本可控。结合不同历史启发式可进一步提升修复率，为智能体 APR 提供实用的思路。


<details>
  <summary>Details</summary>
Motivation: 自动化程序修复（APR）领域已经开始采用大语言模型和基于智能体的系统，但现有系统主要依赖本地快照上下文，忽略了代码仓库的历史信息。既有研究表明，历史信息可用于修复单行 bug，因为最后一次提交往往是 bug 的根源。本文动机是探索仓库历史是否同样能提升智能体 APR 系统，尤其在修复复杂多段 bug 时的有效性。

Method: 提出 HAFixAgent，一种历史感知的 bug 修复智能体，将基于 'blame' 的仓库历史启发式注入到修复流程中。通过对 Defects4J 数据集中 854 个真实 bug 的预分析，确认历史信息具有广泛性和高相关性。然后对 HAFixAgent 与两个先进系统进行实证对比。

Result: HAFixAgent 在修复多段复杂 bug 的效果上显著超越了两个基线系统（与智能体基线相比提升 212.3%，与多段基线相比提升 29.9%）。在效率上，加入历史信息并未显著增加智能体步骤，token 成本也无明显提升。对于复杂多文件多段 bug，HAFixAgent 的中位成本更低。历史启发式的多样组合能修复更多 bug，实现明确的性价比提升。

Conclusion: 将版本控制历史用于智能体 APR，有助于提升修复复杂 bug 的能力，同时保证效率和成本的可控性。历史感知方法提供了一套实用的智能体 APR 设计方案，即结合历史上下文和差异信息，并适时加入互补启发式。

Abstract: Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.

</details>


### [35] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: 该论文提出HarnessLLM，创新性地利用LLM生成测试harness代码，提升了BUG发现效率和测试多样性，并优化了代码生成表现。


<details>
  <summary>Details</summary>
Motivation: 目前基于大型语言模型（LLM）的自动化测试生成方法，多以输入-输出对的形式，对正确程序的预期行为进行分类。这种方法存在测试样本多样性有限和调试信息不足的问题。

Method: 提出了HarnessLLM，一种两阶段训练流程，使LLM能够生成用于测试的harness代码。具体做法是让LLM生成用于合成输入和验证输出的代码，从而支持复杂测试案例和灵活的输出验证（如不变量检查）。训练流程包括有监督微调（SFT）和定制化奖励设计的RLVR（强化学习+人类反馈）。

Result: 实验结果表明，HarnessLLM在发现BUG和测试策略多样性方面均优于基于输入-输出的测试方法。通过用生成的测试用例进行推理阶段验证，还提升了代码生成性能。

Conclusion: HarnessLLM能够生成更复杂和多样化的测试案例，对程序进行更有效的验证，并提升了LLM对代码任务的适应能力。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [36] [An Empirical Study of LLM-Based Code Clone Detection](https://arxiv.org/abs/2511.01176)
*Wenqing Zhu,Norihiro Yoshida,Eunjong Choi,Yutaka Matsubara,Hiroaki Takada*

Main category: cs.SE

TL;DR: 本研究系统评估了LLMs在代码克隆检测任务中的表现和一致性，发现模型在不同数据集间的性能差异较大，但内部一致性较高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在代码生成和调试等软件工程任务中表现出色，但其在不同数据集上的表现一致性和答复一致性尚未被充分研究。

Method: 作者构建了七个代码克隆数据集，使用两套代码集合（CodeNet和BigCloneBench），并基于编辑距离进行样本抽取。随后，论文对五种主流LLMs在四种提示语下进行评测。

Result: LLMs在CodeNet相关数据集上表现良好，其中o3-mini模型的F1分数达到0.943。但在BigCloneBench相关数据集上的表现明显下降。大多数模型的答复一致性较高，五次提交中判断结果90%以上保持一致，F1分数波动很小（<0.03）。

Conclusion: 大语言模型在代码克隆检测中的性能依赖于具体数据集，虽然在一致性方面表现稳定，但跨数据集表现存在较大差异。建议后续研究关注模型在不同类型数据集上的泛化能力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various software engineering tasks, such as code generation and debugging,
because of their ability to translate between programming languages and natural
languages. Existing studies have demonstrated the effectiveness of LLMs in code
clone detection. However, two crucial issues remain unaddressed: the ability of
LLMs to achieve comparable performance across different datasets and the
consistency of LLMs' responses in code clone detection. To address these
issues, we constructed seven code clone datasets and then evaluated five LLMs
in four existing prompts with these datasets. The datasets were created by
sampling code pairs using their Levenshtein ratio from two different code
collections, CodeNet and BigCloneBench. Our evaluation revealed that although
LLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943
F1 score, their performance significantly decreased in BigCloneBench-related
datasets. Most models achieved a high response consistency, with over 90\% of
judgments remaining consistent across all five submissions. The fluctuations of
the F1 score affected by inconsistency are also tiny; their variations are less
than 0.03.

</details>


### [37] [Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing](https://arxiv.org/abs/2511.01252)
*Siyuan Li,Yaowen Zheng,Hong Li,Jingdong Guo,Chaopeng Dong,Chunpeng Yan,Weijie Wang,Yimo Ren,Limin Sun,Hongsong Zhu*

Main category: cs.SE

TL;DR: Lares提出了一种不依赖编译流程的补丁检测方法，通过语义搜索和大模型，实现了高精度、强扩展性的漏洞补丁检测，显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统广泛使用代码复用，这使得“1-day漏洞”成为重大的安全风险。现有针对二进制包的漏洞检测方法不可用性和准确性有限，且难以区分代码变化是由补丁还是编译造成的，因此亟需更高效、更准确的补丁检测方案。

Method: 提出了Lares方法。Lares通过代码切片语义搜索，直接从补丁源代码提取特征，并在目标二进制的伪代码中识别语义等价的代码切片。与传统方法不同，Lares无需依赖编译流程，增强了易用性。采用大语言模型分析代码，并结合SMT（可满足性模理论）求解器进行逻辑推理，提升了准确性。

Result: 实验结果表明，Lares在精度、召回率以及易用性方面均优于现有方法。Lares也是首个对不同优化层级、架构以及编译器环境下补丁检测效果进行评估的工作。所有使用的数据集和源码已开源。

Conclusion: Lares为补丁检测带来更高的可扩展性和准确性，显著提升了软件安全领域的技术水平，并为补丁检测相关研究提供了公开的数据和工具。

Abstract: In modern software ecosystems, 1-day vulnerabilities pose significant
security risks due to extensive code reuse. Identifying vulnerable functions in
target binaries alone is insufficient; it is also crucial to determine whether
these functions have been patched. Existing methods, however, suffer from
limited usability and accuracy. They often depend on the compilation process to
extract features, requiring substantial manual effort and failing for certain
software. Moreover, they cannot reliably differentiate between code changes
caused by patches or compilation variations. To overcome these limitations, we
propose Lares, a scalable and accurate method for patch presence testing. Lares
introduces Code Slice Semantic Search, which directly extracts features from
the patch source code and identifies semantically equivalent code slices in the
pseudocode of the target binary. By eliminating the need for the compilation
process, Lares improves usability, while leveraging large language models
(LLMs) for code analysis and SMT solvers for logical reasoning to enhance
accuracy. Experimental results show that Lares achieves superior precision,
recall, and usability. Furthermore, it is the first work to evaluate patch
presence testing across optimization levels, architectures, and compilers. The
datasets and source code used in this article are available at
https://github.com/Siyuan-Li201/Lares.

</details>


### [38] [Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation](https://arxiv.org/abs/2511.01316)
*Chong Wang,Chen Zhang,Jiajun Wu,Wunan Guo,Jianfeng Qu,Yewen Tian,Yang Liu*

Main category: cs.SE

TL;DR: 本文研究LLM在CI平台（Travis CI向GitHub Actions）配置翻译中的应用。分析实际迁移数据，发现迁移需较多人工编辑和多次提交；LLM自动翻译问题多，主要为逻辑及平台差异。通过优化提示和迭代，提高了自动配置生成的成功率，有助于提升CI迁移效率和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，CI平台的迁移变得普遍，但由于配置文件复杂、平台间语义差异，配置翻译是迁移中的难题。大语言模型（LLM）近来被认为有潜力解决这一问题。

Method: 分析811条从Travis CI迁移到GitHub Actions的记录，量化迁移相关工作量；评估四种LLM生成的CI配置翻译质量；归类翻译问题并测试三种改进策略。

Result: 开发者平均阅读38行Travis配置，编写58行GitHub Actions配置，近一半迁移过程需多次提交。LLM在生成配置中出现1,121个问题，主要为逻辑不一致、平台差异、环境错误和语法错误。通过结合指导性提示和迭代优化，构建成功率达75.5%，较基础GPT-4o提升约三倍。

Conclusion: LLM能够辅助CI平台迁移中的配置翻译，但现有模型直译易产生大量错误。结合优化策略后，能大幅提升配置生成质量，为自动化CI迁移提供了可行路径。

Abstract: Continuous Integration (CI) is a cornerstone of modern collaborative software
development, and numerous CI platforms are available. Differences in
maintenance overhead, reliability, and integration depth with code-hosting
platforms make migration between CI platforms a common practice. A central step
in migration is translating CI configurations, which is challenging due to the
intrinsic complexity of CI configurations and the need to understand semantic
differences and relationships across CI platforms.
  With the advent of large language models (LLMs), recent advances in software
engineering highlight their potential for CI configuration translation. In this
paper, we present a study on LLM-based CI configuration translation, focusing
on the migration from Travis CI to GitHub Actions. First, using 811 migration
records, we quantify the effort involved and find that developers read an
average of 38 lines of Travis configuration and write 58 lines of GitHub
Actions configuration, with nearly half of the migrations requiring multiple
commits. We further analyze translations produced by each of the four LLMs and
identify 1,121 issues grouped into four categories: logic inconsistencies
(38%), platform discrepancies (32%), environment errors (25%), and syntax
errors (5%). Finally, we evaluate three enhancement strategies and show that
combining guideline-based prompting with iterative refinement achieves the best
performance, reaching a Build Success Rate of 75.5%-nearly a threefold
improvement over GPT-4o with a basic prompt.

</details>


### [39] [AI for Requirements Engineering: Industry adoption and Practitioner perspectives](https://arxiv.org/abs/2511.01324)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: AI在需求工程领域正逐步被采用，实践中更倾向人机协同而非全自动化，当前重点应放在提升人机协作能力及完善AI治理体系。


<details>
  <summary>Details</summary>
Motivation: AI应用于需求工程（RE）具有显著潜力，但相关研究有限，需要理解实际采用情况及面临的挑战。

Method: 调查55位软件从业者，梳理AI在RE四个阶段（需求获取、分析、规格说明、验证）中的使用，以及决策方式（人工、AI验证、人机协作、全自动化），并收集其体验及看法。

Result: 58.2%的受访者已在RE中应用AI，69.1%认为影响积极。人机协作占主导（54.4%），全自动化仅占5.4%，被动AI验证甚至更少（4.4%-6.2%）。

Conclusion: AI在需求工程中最有效的应用方式是与人类协作而非完全替代，强调需开发专门的人机协作框架及健全的AI治理机制。

Abstract: The integration of AI for Requirements Engineering (RE) presents significant
benefits but also poses real challenges.Although RE is fundamental to software
engineering, limited research has examined AI adoption in RE.We surveyed 55
software practitioners to map AI usage across four RE phases:Elicitation,
Analysis, Specification, and Validation, and four approaches for decision
making: human only decisions, AI validation, Human AI Collaboration (HAIC), and
full AI automation.Participants also shared their perceptions, challenges, and
opportunities when applying AI for RE tasks.Our data show that 58.2% of
respondents already use AI in RE, and 69.1% view its impact as positive or very
positive.HAIC dominates practice, accounting for 54.4% of all RE techniques,
while full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to
6.2%) lags even further behind, indicating that practitioners value AI's active
support over passive oversight.These findings suggest that AI is most effective
when positioned as a collaborative partner rather than a replacement for human
expertise.It also highlights the need for RE specific HAIC frameworks along
with robust and responsible AI governance as AI adoption in RE grows.

</details>


### [40] [The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project](https://arxiv.org/abs/2511.01348)
*Robin Gröpler,Steffen Klepke,Jack Johns,Andreas Dreschinski,Klaus Schmid,Benedikt Dornauer,Eray Tüzün,Joost Noppen,Mohammad Reza Mousavi,Yongjian Tang,Johannes Viehmann,Selin Şirin Aslangül,Beum Seuk Lee,Adam Ziolkowski,Eric Zie*

Main category: cs.SE

TL;DR: 本文以GENIUS项目为依托，分析了生成式AI在软件工程中全面应用的挑战与前景，提出未来五年行业发展的关键突破，并展现了工具创新与产业验证的实际进展，为科研和工业界提供发展基础和方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中展现了巨大的潜力，但其在整个软件开发生命周期（SDLC）中的全面应用尚未充分探索，还面临可靠性、责任性、安全性和数据隐私等重大挑战。因此，需要协调行动和深入研究来推动AI在软件开发全流程的集成。

Method: 本文通过GENIUS项目的跨行业视角，结合文献综述，总结了当前GenAI在SDLC应用的主要挑战，展望未来五年的关键技术及方法突破，讨论软件工程师角色与技能变化，并介绍GENIUS通过创新工具和工业验证实践推动行业转型的举措。

Result: 提出了对基于GenAI的软件工程未来的统一展望，涵盖挑战梳理、技术与方法创新、技能和角色变革，以及GENIUS项目的具体贡献，并为科研和工业制定相关战略提供基础。

Conclusion: 通过技术创新与商业相关性结合，为软件工程团队打造可靠、可扩展、可落地的GenAI解决方案，助力行业实现变革升级。

Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in
Software Engineering, capable of generating code, suggesting fixes, and
supporting quality assurance. While its use in coding tasks shows considerable
promise, applying GenAI across the entire Software Development Life Cycle
(SDLC) has not yet been fully explored. Critical uncertainties in areas such as
reliability, accountability, security, and data privacy demand deeper
investigation and coordinated action. The GENIUS project, comprising over 30
European industrial and academic partners, aims to address these challenges by
advancing AI integration across all SDLC phases. It focuses on GenAI's
potential, the development of innovative tools, and emerging research
challenges, actively shaping the future of software engineering. This vision
paper presents a shared perspective on the future of GenAI-based software
engineering, grounded in cross-sector dialogue and experience within the GENIUS
consortium, supported by an exploratory literature review. The paper explores
four central elements: (1) a structured overview of current challenges in GenAI
adoption across the SDLC; (2) a forward-looking vision outlining key
technological and methodological advances expected over the next five years;
(3) anticipated shifts in the roles and required skill sets of software
professionals; and (4) the contribution of GENIUS in realizing this
transformation through practical tools and industrial validation. By aligning
technical innovation with business relevance, this paper aims to inform both
research agendas and industrial strategies, providing a foundation for
reliable, scalable, and industry-ready GenAI solutions for software engineering
teams.

</details>


### [41] [Characterizing Build Compromises Through Vulnerability Disclosure Analysis](https://arxiv.org/abs/2511.01395)
*Maimouna Tamah Diao,Moustapha Awwalou Diouf,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Gervais Mendy,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本研究系统梳理并分类了软件构建过程中的安全威胁，发现近1/4供应链攻击源于构建环节，依赖混淆和脚本注入最为突出，提出的攻击向量体系为防御研究提供重要参考。


<details>
  <summary>Details</summary>
Motivation: 构建过程是软件开发中将源码转化为可部署产物的关键阶段，但其复杂的多组件系统和构建过程中的非确定性带来了独特且严重的安全挑战。目前安全社区对构建过程中的攻击途径缺乏系统化理解，阻碍了有效防御措施的设计。

Method: 本文利用大规模CVE挖掘（分析NVD数据库中621个漏洞披露），通过实证方法构建了一套针对构建过程攻击向量的分类法。分类依据是不同攻击向量在构建流程中的注入点，如源码篡改、依赖注入、构建工具或编译器被攻陷等。分类法通过分析168起已知的软件供应链攻击案例加以验证。

Result: 分析发现，在168起供应链攻击中，有40起明确针对构建环节，构建相关漏洞占比达23.8%。其中最常见的攻击向量是依赖混淆和构建脚本注入。此外，作者还公开了支持分析的数据集。

Conclusion: 构建环节的安全漏洞是供应链攻击的重要切入点，现有攻击向量以依赖混淆和脚本注入为主。系统化的攻击向量分类有助于提升防御措施的针对性，并为后续研究提供基础。

Abstract: The software build process transforms source code into deployable artifacts,
representing a critical yet vulnerable stage in software development. Build
infrastructure security poses unique challenges: the complexity of
multi-component systems (source code, dependencies, build tools), the
difficulty of detecting intrusions during compilation, and prevalent build
non-determinism that masks malicious modifications. Despite these risks, the
security community lacks a systematic understanding of build-specific attack
vectors, hindering effective defense design.
  This paper presents an empirically-derived taxonomy of attack vectors
targeting the build process, constructed through a large-scale CVE mining (of
621 vulnerability disclosures from the NVD database). We categorize attack
vectors by their injection points across the build pipeline, from source code
manipulation to compiler compromise. To validate our taxonomy, we analyzed 168
documented software supply chain attacks, identifying 40 incidents specifically
targeting build phases. Our analysis reveals that 23.8\% of supply chain
attacks exploit build vulnerabilities, with dependency confusion and build
script injection representing the most prevalent vectors.
  Dataset available at:
https://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.

</details>


### [42] [VeriODD: From YAML to SMT-LIB - Automating Verification of Operational Design Domains](https://arxiv.org/abs/2511.01417)
*Bassel Rafie,Christian Schindler,Andreas Rausch*

Main category: cs.SE

TL;DR: 该论文提出的VeriODD工具能自动将自动驾驶领域用YAML描述的运行条件规范转换为可形式化验证的命题逻辑和SMT-LIB格式，并连接主流求解器实现自动一致性检查和运行时合规性验证，为自动驾驶系统的安全性评估提供了高效、易用、可扩展的新路径。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶系统的运行条件（ODD和COD）虽常以易读的YAML格式描述，以便利益相关方理解，但这种格式并不适合直接用于求解器验证。将这些描述人工翻译为正式语言（如SMT-LIB）过程缓慢且易出错。

Method: 提出VeriODD工具，利用ANTLR编译器技术将YAML格式的ODD/COD规格自动转换为人可读的命题逻辑以及可供求解器（如Z3）直接处理的SMT-LIB格式，并提供图形界面支持编辑、公式检查和一键验证。

Result: VeriODD实现了ODD/COD从易于理解到可自动验证的双重表达，支持自动一致性检查和运行时验证，大大提高了安全性验证的效率和可靠性。

Conclusion: VeriODD工具有效地将面向利益相关方的ODD/COD规范与形式化验证衔接起来，促进了自动驾驶系统运行边界的可扩展、自动化保障。

Abstract: Operational Design Domains (ODDs) define the conditions under which an
Automated Driving System (ADS) is allowed to operate, while Current Operational
Domains (CODs) capture the actual runtime situation. Ensuring that a COD
instance lies within the ODD is a crucial step in safety assurance. Today, ODD
and COD specifications are frequently expressed in YAML to remain accessible
for stakeholders, but such descriptions are not directly suitable for
solver-based verification. Manual translation into formal languages such as
SMT-LIB is slow and error-prone. We present VeriODD, a tool that automates this
translation. VeriODD uses ANTLR-based compiler technology to transform
YAML-based ODD/COD specifications into both human-readable propositional logic,
for lightweight review on a simple basis, and solver-ready SMT-LIB. The tool
integrates with SMT solvers such as Z3 to provide automated consistency checks
of ODD specifications and verification of COD conformance. A graphical user
interface supports editing specifications, inspecting generated formulas, and
performing verification with a single click. VeriODD thereby closes the gap
between stakeholder-friendly ODD/COD notations and formal verification,
enabling scalable and automated assurance of operational boundaries in
autonomous driving. Video demonstration: https://youtu.be/odRacNoL_Pk Tool
available at: https://github.com/BasselRafie/VeriODD

</details>


### [43] [LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations](https://arxiv.org/abs/2511.01423)
*Ruidi He,Yu Zhang,Meng Zhang,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文提出利用大语言模型辅助生成地图语义转换的逻辑公式和谓词，有效提升了自动驾驶高清地图变换的可扩展性与自动化程度。实验显示，方法可减少人工工作，同时保证语义正确性，为地图验证提供了一种高效的解决思路。


<details>
  <summary>Details</summary>
Motivation: 高清地图的语义转换对自动驾驶系统至关重要，但现有的基于规则的方法依赖于人工编写公式和领域特定函数，难以扩展且维护成本高。

Method: 提出了一种基于大语言模型（LLM）辅助的自动化流程，能够在计算一阶逻辑（FOL）框架下，联合生成逻辑公式和可执行谓词，并扩展了CommonRoad场景设计器的地图验证器以支持高程信息。该方法通过prompt生成语法合规的规则及谓词，实现直接集成到现有系统。

Result: 在人工合成的桥梁和坡道场景上实现原型，并进行实验验证。结果显示该方法能够减少人工工程投入，同时维持语义正确性。

Conclusion: 提出了一种可扩展的、半自动化的人工参与地图转换验证方法，既提高了效率，又保证了转换的正确性，证明了该方法的可行性。

Abstract: High-definition map transformations are essential in autonomous driving
systems, enabling interoperability across tools. Ensuring their semantic
correctness is challenging, since existing rule-based frameworks rely on
manually written formulas and domain-specific functions, limiting scalability.
  In this paper, We present an LLM-assisted pipeline that jointly generates
logical formulas and corresponding executable predicates within a computational
FOL framework, extending the map verifier in CommonRoad scenario designer with
elevation support. The pipeline leverages prompt-based LLM generation to
produce grammar-compliant rules and predicates that integrate directly into the
existing system.
  We implemented a prototype and evaluated it on synthetic bridge and slope
scenarios. The results indicate reduced manual engineering effort while
preserving correctness, demonstrating the feasibility of a scalable,
semi-automated human-in-the-loop approach to map-transformation verification.

</details>


### [44] [From Pre-labeling to Production: Engineering Lessons from a Machine Learning Pipeline in the Public Sector](https://arxiv.org/abs/2511.01545)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 该论文通过分析巴西政府平台的实际经验，指出公共部门部署机器学习面临技术与组织双重挑战。加速ML开发的工程策略虽具效率，但若缺乏严谨治理与验证，将导致新风险。要提升政府ML系统的可信度与可持续性，核心在于建设可审计、透明且可问责的数据与流程基础设施。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习越来越多地被嵌入政府数字平台，如何在公共部门中构建既准确、可审计又可持续运营的ML系统，成为亟需解决的问题。除了技术难题外，团队还面临来自组织层面的诸多障碍。

Method: 以巴西的Brasil Participativo (BP)平台为案例，分析主流工程实践，如LLMs预标注、分路分类器模型、合成数据生成等在实际开发中的应用效果，以及这些方法在缺乏严格数据治理和人工验证情境下可能引发的风险。

Result: 虽然上述工程策略可加快开发流程，但如果没有严密的数据治理和人工校验，反而会带来新的可追溯性、可靠性和成本风险。公共部门的ML问题已不单是建模问题，更是制度化工程问题。ML流程应视作公民基础设施来构建和维护。

Conclusion: 公共部门ML的成功关键，不在于模型准确性突破，而在于机构是否能打造透明、可复现且具备问责性的可信数据基础设施。

Abstract: Machine learning is increasingly being embedded into government digital
platforms, but public-sector constraints make it difficult to build ML systems
that are accurate, auditable, and operationally sustainable. In practice, teams
face not only technical issues like extreme class imbalance and data drift, but
also organizational barriers such as bureaucratic data access, lack of
versioned datasets, and incomplete governance over provenance and monitoring.
Our study of the Brasil Participativo (BP) platform shows that common
engineering choices -- like using LLMs for pre-labeling, splitting models into
routed classifiers, and generating synthetic data -- can speed development but
also introduce new traceability, reliability, and cost risks if not paired with
disciplined data governance and human validation. This means that, in the
public sector, responsible ML is not just a modeling problem but an
institutional engineering problem, and ML pipelines must be treated as civic
infrastructure. Ultimately, this study shows that the success of machine
learning in the public sector will depend less on breakthroughs in model
accuracy and more on the ability of institutions to engineer transparent,
reproducible, and accountable data infrastructures that citizens can trust.

</details>


### [45] [Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy](https://arxiv.org/abs/2511.01757)
*Shamse Tasnim Cynthia,Banani Roy*

Main category: cs.SE

TL;DR: 针对Galaxy等生信领域工作流检索难题，本文提出了联合密集向量检索和大模型语义再排序框架，搭建了新的标注数据集并进行全面评测，显著提升了检索准确率，已实现原型，将促进工作流平台的智能搜索能力。


<details>
  <summary>Details</summary>
Motivation: Galaxy等科学工作流管理系统已成为生物信息学的基础设施，现有基于关键词的检索系统在遇到术语不精确匹配时，难以发现相关工作流，缺乏语义层面的检索支持。

Method: 提出一个任务感知的两阶段检索框架：首先利用嵌入模型做密集向量检索，随后用微调的大型语言模型（如GPT-4o、Mistral-7B）对候选工作流进行语义对齐再排序。同时搭建了含语义主题标注和合成任务型查询的新基准数据集，对比中文本检索、密集检索与再排序模型。

Result: 实验表明，该方法显著提升了top-k准确率和相关性，尤其在查询长或描述不明时改进明显。系统已作为原型集成到Galaxy，展示了LLM增强工作流检索的可行性。

Conclusion: 本研究系统性提升了科学工作流的可用性与易获取性，特别有助于新手和跨学科研究者，推动了Galaxy等平台工作流检索技术的发展。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become
essential infrastructure in bioinformatics, supporting the design, execution,
and sharing of complex multi-step analyses. Despite hosting hundreds of
reusable workflows across domains, Galaxy's current keyword-based retrieval
system offers limited support for semantic query interpretation and often fails
to surface relevant workflows when exact term matches are absent. To address
this gap, we propose a task-aware, two-stage retrieval framework that
integrates dense vector search with large language model (LLM)-based reranking.
Our system first retrieves candidate workflows using state-of-the-art embedding
models and then reranks them using instruction-tuned generative LLMs (GPT-4o,
Mistral-7B) based on semantic task alignment. To support robust evaluation, we
construct a benchmark dataset of Galaxy workflows annotated with semantic
topics via BERTopic and synthesize realistic task-oriented queries using LLMs.
We conduct a comprehensive comparison of lexical, dense, and reranking models
using standard IR metrics, presenting the first systematic evaluation of
retrieval performance in the Galaxy ecosystem. Results show that our approach
significantly improves top-k accuracy and relevance, particularly for long or
under-specified queries. We further integrate our system as a prototype tool
within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.
This work advances the usability and accessibility of scientific workflows,
especially for novice users and interdisciplinary researchers.

</details>


### [46] [Context-Guided Decompilation: A Step Towards Re-executability](https://arxiv.org/abs/2511.01763)
*Xiaohan Wang,Yuxin Hu,Kevin Leach*

Main category: cs.SE

TL;DR: ICL4Decomp通过上下文学习引导LLM生成可执行源码，反编译可重编译性提升约40%，适用于优化后的二进制，增强了实际分析与复用能力。


<details>
  <summary>Details</summary>
Motivation: 二进制反编译在源代码不可用时对于软件安全分析、逆向工程和恶意软件理解非常重要。当前的反编译技术，尤其面对经过优化的二进制文件，往往无法生成可成功重新编译与执行的源码。新出现的语言模型虽有助于生成语义合理的代码，但实际可执行性不足，导致实用性有限。其核心问题在于编译优化和语义线索丢失让LLM难以无上下文恢复信息。

Method: 本文提出一种混合式反编译框架ICL4Decomp，通过上下文学习（ICL）引导大语言模型（LLM）生成可重新执行的源码。方法结合神经网络生成能力与上下文补充信息，提升代码可重用性和执行性。

Result: 实验表明，ICL4Decomp在多个数据集、优化等级和编译器下，较最新反编译技术在源码可重新执行性上提升约40%，且保持了方法鲁棒性。

Conclusion: ICL4Decomp有效提升了基于LLM的反编译源码可重编译和可执行性，为没有源码时的安全分析、逆向工程等场景带来更实用的解决方案。

Abstract: Binary decompilation plays an important role in software security analysis,
reverse engineering, and malware understanding when source code is unavailable.
However, existing decompilation techniques often fail to produce source code
that can be successfully recompiled and re-executed, particularly for optimized
binaries. Recent advances in large language models (LLMs) have enabled neural
approaches to decompilation, but the generated code is typically only
semantically plausible rather than truly executable, limiting their practical
reliability. These shortcomings arise from compiler optimizations and the loss
of semantic cues in compiled code, which LLMs struggle to recover without
contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid
decompilation framework that leverages in-context learning (ICL) to guide LLMs
toward generating re-executable source code. We evaluate our method across
multiple datasets, optimization levels, and compilers, demonstrating around
40\% improvement in re-executability over state-of-the-art decompilation
methods while maintaining robustness.

</details>


### [47] [SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring](https://arxiv.org/abs/2511.01850)
*Jiawei Jin,Yingxin Su,Xiaotong Zhu*

Main category: cs.SE

TL;DR: 本文设计了融合大语言模型助理和自动化MLOps的IDE“SmartMLOps Studio”，显著提高ML开发效率、可复现性和监控能力，推动IDE向智能化、全生命周期平台转型。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能和机器学习应用迅速扩展，对一体化的环境需求加剧。传统IDE侧重代码编写，缺乏对整个机器学习生命周期的智能支持；而现有MLOps平台又与编码工作流程割裂，无法实现无缝协同。

Method: 本研究设计了一款集成大型语言模型（LLM）助理和自动化MLOps流水线的IDE。该系统支持持续模型开发和监控，内嵌LLM助理用于生成代码、调试建议和自动配置流水线。后端包括自动数据校验、特征存储、漂移检测、再训练触发和CI/CD部署编排。开发原型SmartMLOps Studio，并在UCI Adult和M5数据集进行分类与预测任务的评估。

Result: 实验显示，SmartMLOps Studio将流水线配置时间缩短61%，实验可复现性提高45%，漂移检测准确率提升14%，均优于传统工作流。

Conclusion: 论文提出并验证了将智能代码助手与自动化运维流水线深度融合的新型IDE架构，有效提升了AI模型开发的效率与可扩展性。此举推动IDE由静态工具转变为面向全生命周期的智能平台。

Abstract: The rapid expansion of artificial intelligence and machine learning (ML)
applications has intensified the demand for integrated environments that unify
model development, deployment, and monitoring. Traditional Integrated
Development Environments (IDEs) focus primarily on code authoring, lacking
intelligent support for the full ML lifecycle, while existing MLOps platforms
remain detached from the coding workflow. To address this gap, this study
proposes the design of an LLM-Integrated IDE with automated MLOps pipelines
that enables continuous model development and monitoring within a single
environment. The proposed system embeds a Large Language Model (LLM) assistant
capable of code generation, debugging recommendation, and automatic pipeline
configuration. The backend incorporates automated data validation, feature
storage, drift detection, retraining triggers, and CI/CD deployment
orchestration. This framework was implemented in a prototype named SmartMLOps
Studio and evaluated using classification and forecasting tasks on the UCI
Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio
reduces pipeline configuration time by 61%, improves experiment reproducibility
by 45%, and increases drift detection accuracy by 14% compared to traditional
workflows. By bridging intelligent code assistance and automated operational
pipelines, this research establishes a novel paradigm for AI engineering -
transforming the IDE from a static coding tool into a dynamic, lifecycle-aware
intelligent platform for scalable and efficient model development.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Runtime Verification of Interactions Using Automata](https://arxiv.org/abs/2511.00531)
*Chana Weil-Kennedy,Darine Rammal,Christophe Gaston,Arnault Lapitre*

Main category: cs.LO

TL;DR: 本论文提出两种自动机方法，用于在无全局时钟的分布式系统中对多子系统间轨迹进行运行时验证；方法比对规范时效率高，可扩展且能细致报告错误类型，适合大规模系统实用。


<details>
  <summary>Details</summary>
Motivation: 动机是解决分布式系统中运行时验证的问题，不假设全局时钟，如何有效验证各子系统的通信行为满足全局规范。分布式系统常常需要监控子系统之间的信息传递以确保系统正确性。

Method: 提出了两种基于自动机理论的方法，通过收集分布式系统中每个子系统的本地发送和接收事件，利用多轨迹(multitrace)和交互模型进行验证。一种方法比较直接，另一种则在错误类型信息与可复用性上进行了优化，并引入预处理以提升效率。

Result: 实现了两种方法并进行了比较。优化后的方法能提供更丰富的错误类型信息，并借助一次性预处理提升了多次验证的效率。

Conclusion: 基于自动机理论的多轨迹验证方法可以有效提升分布式系统运行时验证的准确性与效率，尤其在没有全局时钟的场景下更具实用价值。研究为分布式系统通信行为的规范验证提供了可扩展、可复用的技术途径。

Abstract: Runtime verification consists in observing and collecting the execution
traces of a system and checking them against a specification, with the
objective of raising an error when a trace does not satisfy the specification.
We consider distributed systems consisting of subsystems which communicate by
message-passing. Local execution traces consisting of send and receive events
are collected on each subsystem. We do not assume that the subsystems have a
shared global clock, which would allow a reordering of the local traces.
Instead, we manipulate multitraces, which are collections of local traces. We
use interaction models as specifications: they describe communication scenarios
between multiple components, and thus specify a desired global behaviour. We
propose two procedures to decide whether a multitrace satisfies an interaction,
based on automata-theoretic techniques. The first procedure is straightforward,
while the second provides more information on the type of error and integrates
the idea of reusability: because many multitraces are compared against one
interaction, some preprocessing can be done once at the beginning. We implement
both procedures and compare them.

</details>


### [49] [Proceedings Twelfth Workshop on Fixed Points in Computer Science](https://arxiv.org/abs/2511.00626)
*Alexis Saurin*

Main category: cs.LO

TL;DR: 本文集围绕计算机科学中的不动点理论，精选了研讨会的最新成果，反映了该领域的最新进展。


<details>
  <summary>Details</summary>
Motivation: 不动点理论在计算机科学多个分支有重要应用，通过专题研讨会促进学者交流，推动理论发展。

Method: 本文集是对会议中部分学术报告经过后续完善后的汇编与出版。每篇论文采用了各自学科领域的理论和方法。

Result: 收录了多篇具有代表性的、不动点理论及其在计算机科学中的应用的最新研究成果，为相关领域科研人员提供参考与借鉴。

Conclusion: 本文集收录了第十二届计算机科学中的不动点国际研讨会的部分工作，展现了该领域最新的理论研究进展。

Abstract: This EPTCS volume contains the post-proceedings of the Twelfth International
Workshop on Fixed Points in Computer Science, presenting a selection of the
works presented during the workshop that took place in Naples (Italy) on the
19th and 20th of February 2024 as a satellite of the International Conference
on Computer Science Logic (CSL 2024).

</details>


### [50] [A Simple Logic of Cohesive Group Agency](https://arxiv.org/abs/2511.00888)
*Nicolas Troquard*

Main category: cs.LO

TL;DR: 作者提出了用图结构（凝聚网络）来描述群体内部的合作与支持关系，并用行动逻辑建立了分析群体凝聚力和协作的理论框架，生成了适用于不同群体结构的逻辑体系。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在形式化表示和研究一个群体内部的社会结构及合作行为，探索群体凝聚力（cohesiveness）形成和表现的机制。

Method: 作者提出了“凝聚网络”(cohesion network)的概念，将群体的社会结构抽象为一个图，其中顶点为严格子群体，边表示一个子群体对另一子群体的亲社会行为（如辅助）。进一步通过“bringing-it-about”行动逻辑，构建了凝聚群体代理的逻辑理论框架。

Result: 该方法得到一系列描述凝聚群体代理行为的逻辑体系，每种逻辑体系对应不同类型的凝聚网络，能刻画不同群体结构下的合作与支持模式。

Conclusion: 本文为群体凝聚力与合作行为的理论研究提供了形式化工具，并拓展了群体行动与代理理论，可以广泛应用于社会心理学、群体决策与多主体系统分析等领域。

Abstract: We propose a structure to represent the social fabric of a group. We call it
the `cohesion network' of the group. It can be seen as a graph whose vertices
are strict subgroups and whose edges indicate a prescribed `pro-social
behaviour' from one subgroup towards another. In social psychology, pro-social
behaviours are building blocks of full-blown cooperation, which we assimilate
here with `group cohesiveness'. We then define a formal framework to study
cohesive group agency. To do so, we simply instantiate pro-social behaviour
with the more specific relation of `successful assistance' between acting
entities in a group. The relations of assistance within a group at the moment
of agency constitute the social fabric of the cohesive group agency. We build
our logical theory upon the logic of agency "bringing-it-about". We obtain a
family of logics of cohesive group agency, one for every class of cohesion
networks.

</details>


### [51] [Dynamic Logic of Trust-Based Beliefs](https://arxiv.org/abs/2511.00899)
*Junli Jiang,Pavel Naumov,Wenxuan Zhang*

Main category: cs.LO

TL;DR: 本论文提出并公理化了结合数据公告的数据驱动信念动态逻辑，还设计了多项式模型检测算法，为智能体信念建模提供了理论与算法支持。


<details>
  <summary>Details</summary>
Motivation: 随着现代社会信息化深入，智能体的信念不再仅限于自身感官获取，更多来源于外部数据。如何形式化描述数据驱动的信念变化成为理论与应用的关键问题。

Method: 作者提出了一种结合数据公告、数据驱动信念变化的动态逻辑系统，并进行了公理化处理，分析了数据公告和信念之间的关系。

Result: 作者给出了该逻辑系统的完备性和一致性公理化，并提出了一个用于模型检测的多项式时间算法，有效验证逻辑公式在模型中的成立。

Conclusion: 本文为数据驱动信念动态逻辑提供了严密的理论基础和高效的算法工具，推动了智能体认知建模的理论发展。

Abstract: Traditionally, an agent's beliefs would come from what the agent can see,
hear, or sense. In the modern world, beliefs are often based on the data
available to the agents. In this work, we investigate a dynamic logic of such
beliefs that incorporates public announcements of data. The main technical
contribution is a sound and complete axiomatisation of the interplay between
data-informed beliefs and data announcement modalities. We also describe a
non-trivial polynomial model checking algorithm for this logical system.

</details>


### [52] [pacSTL: PAC-Bounded Signal Temporal Logic from Data-Driven Reachability Analysis](https://arxiv.org/abs/2511.00934)
*Elizabeth Dietrich,Hanna Krasowski,Emir Cem Gezer,Roger Skjetne,Asgeir Johan Sørensen,Murat Arcak*

Main category: cs.LO

TL;DR: 论文提出pacSTL方法，将PAC理论与STL逻辑结合，实现了在不确定性存在时对系统安全规范的严格监控与评估，在实际任务和实验中被证实有效，并能扩展应用于复杂系统。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器人系统在不确定环境下需要满足安全性要求，而标准的信号时序逻辑（STL）无法处理环境与模型中的不确定性，因此亟需一种既可表达规范，又能应对不确定性的工具。

Method: 论文提出pacSTL框架，将PAC（Probably Approximately Correct）有界预测集合与STL的区间扩展相结合，并在原子命题层面通过优化问题实现。该方法为规范层次提供PAC有界鲁棒性区间，可用于监控任务。

Result: pacSTL在海事导航任务中验证了其有效性，且在模型船的仿真与真实实验中展示了方法的效率和可扩展性。

Conclusion: pacSTL能够为不确定环境中的机器人系统提供有界鲁棒性判据，弥补了标准STL无法处理不确定性的不足。

Abstract: Real-world robotic systems must comply with safety requirements in the
presence of uncertainty. To define and measure requirement adherence, Signal
Temporal Logic (STL) offers a mathematically rigorous and expressive language.
However, standard STL cannot account for uncertainty. We address this problem
by presenting pacSTL, a framework that combines Probably Approximately Correct
(PAC) bounded set predictions with an interval extension of STL through
optimization problems on the atomic proposition level. pacSTL provides
PAC-bounded robustness intervals on the specification level that can be
utilized in monitoring. We demonstrate the effectiveness of this approach
through maritime navigation and analyze the efficiency and scalability of
pacSTL through simulation and real-world experimentation on model vessels.

</details>


### [53] [A Physical Analogy between Molecular Ordering and SAT-to-Ising Annealing](https://arxiv.org/abs/2511.01216)
*ShivKishan Dubey,Rohit Sharma*

Main category: cs.LO

TL;DR: 本文提出将SAT问题与物理Ising模型建立类比，通过模拟退火过程揭示可满足逻辑解与分子低能有序状态对应，为理解计算过程的热力学本质提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 在分子系统中，随温度降低会出现自发有序结构，此研究试图将该热力学有序现象类比到逻辑复杂问题的求解过程。研究动机是在计算复杂性与物理系统间建立统一的理论视角。

Method: 将布尔SAT问题实例映射为双体Ising哈密顿模型。利用模拟退火法，通过热进化方式模拟系统从高熵随机状态降温到低熵有序状态，采用分子冷却类比逻辑解的求解过程。

Result: 发现系统在降温过程中出现快速的“一级”或“逻辑结晶”现象，即可满足约束的逻辑配置快速形成有序结构。主干刚性与物理有序程度无强关联，主要发生的是局部约束满足的对齐。

Conclusion: 逻辑可满足配置可类比为分子系统中的低能结晶态，研究为计算一致性和复杂性的热力学统一视角提供了实证支持。

Abstract: As temperature drops, molecular systems may undergo spontaneous ordering,
moving from random behavior to orderly structure. This research demonstrates a
direct analogy between this type of thermodynamic ordering in molecular systems
and the development of coherent logic in computationally complex problem sets.
We have proposed a mapping of Boolean SAT problem instances to pairwise Ising
Hamiltonian models. Using simulated annealing, we then applied phenomenal
cooling to the system through thermal evolution from high entropy random
assignment to lower entropy, ordered assignments (the energy minima) using
molecular cooling analogs. This indicated that there was a rapid "first-order"
or "logical crystallization" of satisfiable logical configurations. The degree
of backbone rigidity did not strongly correlate with the level of physical
ordering observed in the system; thus, it appears that there is primarily a
local alignment of constraint satisfaction occurring in the system. Thus, we
have provided empirical evidence that satisfiable logical configurations are
analogous to the low energy crystalline states observed in molecular systems
and provide evidence for a unified thermodynamic view of computational
coherence and complexity.

</details>


### [54] [SM-based Semantics for Answer Set Programs Containing Conditional Literals and Arithmetic](https://arxiv.org/abs/2511.01753)
*Zachary Hansen,Yuliya Lierler*

Main category: cs.LO

TL;DR: 作者提出了一种无需归结的 ASP 条件文字语义，使用 SM 算子理论证明了和传统语义之间的对应性，增强了表达效率和知识表示能力。


<details>
  <summary>Details</summary>
Motivation: 当前的 ASP 求解器如 CLINGO 支持条件文字等高级语言结构，使得逻辑程序表述更加灵活和简洁。然而，这些结构的语义和实现方式存在不同方案，传统方案往往依赖对程序进行无限扩展的命题逻辑翻译，处理复杂且成本高。

Method: 本文提出了一种基于 SM 算子的逻辑程序条件文字及算术表达式的语义定义。该语义不依赖于程序的归结（grounding），而是直接通过 SM 算子操作原始程序来描述其语义。

Result: 该方法与现有的依赖翻译到无限命题逻辑的语义有严格对应关系，理论上证明了两者一致性。

Conclusion: 本文实现了无需归结的条件文字和算术表达式语义，提升了知识表示的效率和表达能力，并与已有语义方案建立了精确对应。

Abstract: Modern answer set programming solvers such as CLINGO support advanced
language constructs that improve the expressivity and conciseness of logic
programs. Conditional literals are one such construct. They form "subformulas"
that behave as nested implications within the bodies of logic rules. Their
inclusion brings the form of rules closer to the less restrictive syntax of
first-order logic. These qualities make conditional literals useful tools for
knowledge representation. In this paper, we propose a semantics for logic
programs with conditional literals and arithmetic based on the SM operator.
These semantics do not require grounding, unlike the established semantics for
such programs that relies on a translation to infinitary propositional logic.
The main result of this paper establishes the precise correspondence between
the proposed and existing semantics.

</details>


### [55] [Access Hoare Logic](https://arxiv.org/abs/2511.01754)
*Arnold Beckmann,Anton Setzer*

Main category: cs.LO

TL;DR: 本文提出了“访问霍尔逻辑”，扩展了霍尔逻辑在程序访问安全（如访问控制）上的应用，证明其完备与可靠，并展示了其与传统霍尔逻辑的联系和区别。


<details>
  <summary>Details</summary>
Motivation: 霍尔逻辑（Hoare logic）被广泛用于推理计算机程序的正确性，但在访问安全（如访问控制）上还缺乏专门的形式化方法。本文动机是提出一种新方法，用于推理程序的访问安全问题。

Method: 定义了“访问霍尔逻辑”（access Hoare logic）的形式系统，并通过实例说明其有用性和与传统霍尔逻辑的根本区别。同时，证明了访问霍尔逻辑的完备性与可靠性，并建立了其与标准霍尔逻辑之间的联系。

Result: 证明了访问霍尔逻辑的可靠性和完备性，并通过例子表现出该逻辑在推理访问安全方面的独特优势。论文还探讨了访问霍尔逻辑与传统霍尔逻辑之间的联系。

Conclusion: 访问霍尔逻辑为分析程序的访问安全问题提供了一个有效的新工具，具有形式化基础并与传统霍尔逻辑相辅相成。该逻辑适用于推理和验证程序的安全性属性，扩展了现有形式化方法的应用范围。

Abstract: Following Hoare's seminal invention, later called Hoare logic, to reason
about correctness of computer programs, we advocate a related but fundamentally
different approach to reason about access security of computer programs such as
access control. We define the formalism, which we denote access Hoare logic,
and present examples which demonstrate its usefulness and fundamental
difference to Hoare logic. We prove soundness and completeness of access Hoare
logic, and provide a link between access Hoare logic and standard Hoare logic.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [56] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了PlotCraft可视化任务基准和SynthVis-30K数据集，评估了23种LLM的可视化能力，并开发了高效的新模型PlotCraftor，在处理复杂数据可视化方面性能领先难题提升逾50%。数据和工具全部开放。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLMs）在代码生成方面表现优异，但在处理复杂数据可视化方面能力尚未充分评估和开发。因此，本文旨在填补该领域的空白。

Method: 本文提出了PlotCraft，一个包含1000个具有挑战性的可视化任务的新基准，涵盖金融、科研、社会学等主题，包含七类高度抽象的任务和48种不同图表类型，系统地评估单轮和多轮生成与改进。作者评估了23个主流LLM，并开发了SynthVis-30K，一个由多智能体协作生成的大规模高质量复杂可视化代码数据集。进一步，基于该数据集开发了PlotCraftor新模型。

Result: 评测结果显示，现有LLMs在复杂可视化任务处理上存在明显不足。新模型PlotCraftor在VisEval、PandasPlotBench和PlotCraft基准上的表现与主流闭源模型相当，尤其在难任务上提升超50%。

Conclusion: 作者发布了PlotCraft基准、数据集和代码，为复杂数据可视化领域的LLM评估与发展提供了新的工具和数据资源。PlotCraftor模型展示了显著性能提升和创新能力。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [57] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 本文提出基于原型理论的MBTI人格识别方法ProtoMBTI，通过LLM增强数据集、轻量编码器微调和动态原型检索，大幅提升文本人格推断的准确性和泛化能力，且结果更易解释。


<details>
  <summary>Details</summary>
Motivation: 现有文本人格识别方法通常采用硬标签分类，忽略了人类人格判断的原型式和渐变特性。因此，研究者尝试用更符合认知心理学理论的方法提升人格识别的准确性和解释性。

Method: 提出了ProtoMBTI框架，结合原型理论和大模型推理。先利用LLM构建平衡且高质量的数据集，再通过LoRA微调轻量级编码器，生成判别性嵌入并标准化人格原型库。推理时，从原型库检索top-k原型，对输入进行“检索-复用-修订-保留”循环，并根据预测结果动态扩充原型库。

Result: ProtoMBTI在Kaggle和Pandora等多个基准数据集上，无论是MBTI的四维度还是完整16型任务，都优于传统方法，并能实现较强的跨数据集泛化能力。

Conclusion: ProtoMBTI方法在MBTI人格识别任务中，比传统基线方法取得了更好的效果，具有更高的准确性、可解释性和跨数据集的泛化能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [58] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 新方法能探测语言模型对未来长程信息的编码，有望进一步提升模型解释性和监控能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够处理更长时间跨度的任务，现有的模型可解释性方法往往只关注具体概念或单一词元，而无法有效理解模型在长范围内的激活表示。

Method: 提出Residual Stream Decoders框架，通过解码模型激活，探测其在段落或文档级别的长程计划信息。并测试了多种方法。

Result: 在较小规模模型中，实验表明可成功解码等同于五个以上未来词元的信息。

Conclusion: 该方法为更加有效地监控语言模型，以及理解其对长期计划信息的编码方式奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [59] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出通过信息丰富token选择改进LLM训练方式，在多类任务上证实其能提升训练效果和模型性能，同时优化计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练依赖下一个token预测，计算成本高、效率有限。作者认为选择更信息丰富的token进行训练可以提升训练效率和模型性能。

Method: 在算术、文本多标签分类、自然语言生成三类任务上，研究了信息丰富token的选择及其对模型训练的影响。

Result: 采用信息丰富token训练的方法在不同任务上都表现出更优的训练效果，提高了模型性能，并促进了对目标token选择策略的理论理解。

Conclusion: 本文提出通过预测信息丰富的token来训练大语言模型，相比传统的下一个token预测方法，能够更有效优化训练过程和模型性能。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [60] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 论文针对大模型在角色模拟时一致性不足的问题，提出三项自动一致性指标，并结合多轮强化学习优化，实现了角色模拟的一致性提升（减少55%以上角色偏移），显著增强了LLM在教育、治疗、社交等场景的用户角色表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在模拟用户时经常出现角色偏离、前后矛盾、行为不符等问题，影响在社交、教育、治疗等应用中的可靠性。为解决这一痛点，论文力求提升模型在对话过程中的角色一致性。

Method: 提出了一个统一框架来评估和提高 LLM 生成对话中的角色一致性。具体包括三类自动评价指标：提示到语句一致性、语句与语句一致性以及问答一致性，并将这些指标作为奖励信号，通过多轮强化学习细化LLM对三类用户角色（患者、学生、社交伙伴）的模拟能力。所有指标均通过人工标注进行验证。

Result: 利用新方法微调后，模型在一致性方面提升显著，角色偏移减少了55%以上，使生成的模拟用户更具连贯性和角色忠实度。

Conclusion: 该论文提出了一种统一框架及三项指标，有效提升了LLM在多角色模拟中的一致性和可靠性。经强化学习优化后，模型能够更好地保持所分配角色，适用于多种场景的虚拟用户模拟。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [61] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: 该论文提出通过大型语言模型增强网络安全桌面演练，有效解决了传统演练剧本化、资源消耗大、难于规模化的问题。初步实验显示新系统更易用、可扩展，未来将扩展多用户与深入对比研究。


<details>
  <summary>Details</summary>
Motivation: 传统的网络安全桌面演练虽然有助于培训，但存在剧本化、资源消耗大和难以规模化的问题。本文旨在通过技术创新，解决这些限制，提升桌面演练的可用性和扩展性。

Method: 提出了AgentBnB系统，将大型语言模型与检索增强的Copilot（C2D2）结合，构建浏览器端的桌面演练工具。系统通过Prompt工程的智能体，使用脚手架式教学策略，结合认知目标提示，为学习者提供逐步减少辅助的培训体验。并通过与传统实体牌组的对比，进行了初步的单人实验。

Result: 在针对四名研究生的单人试点实验中，参与者表示更倾向于使用基于智能体的新系统，相较于实体牌组更具可扩展性。但在简单的知识测验上出现了“天花板效应”。通过这些初步数据，展示了语言模型强化的系统在提供低成本、可重复演练方面的潜力。

Conclusion: 尽管样本量小且主要为单人实验，且语料有限，但初步研究表明大型语言模型增强的网络安全桌面演练具有简便、可扩展、低资源需求的优势。后续将扩展到多玩家模式、遥测驱动辅导，并研究更大样本的对比效果。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [62] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出并构建了印度法律检索统一语料库IL-PCR，系统性评估多种模型并通过LLM重排序提升效果，推动法规与先例联合检索的发展。


<details>
  <summary>Details</summary>
Motivation: 律师在处理法律案件时需要检索相关法规及先例，然而现有研究往往将两者分开处理，缺乏融合两者的统一方法和数据集。

Method: 提出IL-PCR（Indian Legal corpus for Prior Case and Statute Retrieval）语料库，为法规检索和先例检索任务提供统一测试平台，探索任务间的相互依赖。实验证包括词汇模型、语义模型及基于图神经网络的集成模型，并开发了基于大语言模型的重排序方法以提升检索表现。

Result: 基于LLM的重排序方法在法规和先例检索任务上取得了最佳表现。

Conclusion: 联合处理法规与先例检索任务能更好地利用二者关联关系，统一语料库和方法有助于提升法律检索系统的性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [63] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 通过利用语言模板生成合成句子对并引入POSESTITCH-SLT预训练方案，在低资源手语翻译任务中实现了大幅度性能提升，展现了模板驱动的合成数据在这种场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 手语翻译任务受限于缺乏大规模、句子级对齐的数据集，现有方法多关注特征提取和网络架构改进，仍难以突破数据稀缺的瓶颈。

Method: 提出了一种新颖的预训练方案POSESTITCH-SLT，其灵感来源于基于语言模板的句子生成技术，并在训练过程中利用模板生成的句子对。采用Transformer为基础的编码器-解码器架构。

Result: 在How2Sign和iSign两个数据集上，BLEU-4分数分别从1.97提升到4.56，以及从0.55提升到3.43，均显著优于以往基于姿态、无词汇表的翻译方法。

Conclusion: 基于模板生成的合成监督显著提升了低资源手语翻译任务的表现。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [64] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 本文提出 Factorization Memory，一种高效且支持稀疏激活的 RNN 架构，在短上下文任务上媲美 Transformer，在长上下文任务中表现更佳，推理时高效且内存占用低。


<details>
  <summary>Details</summary>
Motivation: 当前大多数 Transformer 模型在长上下文语言建模任务中的泛化能力有限，而传统 RNN 在内存和计算上的效率较高。作者希望设计一种既能在短上下文任务上达到 Transformer 级别表现，又能在长上下文任务中提升泛化能力，同时保持高计算和内存效率的 RNN 架构。

Method: 提出了 Factorization Memory，一种高效的循环神经网络（RNN）架构，基于 Mamba-2 进行改进。该模型在训练时利用并行计算以提升效率，推理时保持计算和内存复杂度常数。此外，引入了稀疏版本，每步只更新部分记忆状态，在保持性能的前提下进一步优化模型效率。

Result: Factorization Memory 在短上下文语言建模任务上其性能与 Transformer 模型相当，在长上下文任务上表现更优。稀疏版本有效提升了效率，并且依然保持了与稠密版本相当的强表现。

Conclusion: 本文提出的 Factorization Memory 是首个将稀疏记忆激活与在短、长上下文场景中的强表现结合的 RNN 架构，对比 Transformer 和 Mamba-2 展示了系统性的优越结果。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [65] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文发现主流自回归语言建模目标无法区分文本顺序，导致模型对方向依赖性敏感度下降，建议未来设计更重视语言时间方向的新预训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前CLM预训练目标在方向上对称，与人类语言和推理的时间非对称性不符。希望揭示这一对称性带来的局限，并推动模型更好地模拟语言的方向性。

Method: 形式化并分析了CLM目标的逆序不变性（reversal invariance），并通过理论论证阐释其影响。提出将预训练视为时间非对称性问题。

Result: 发现模型在反向文本与正向文本上训练后性能接近，证实了预训练目标的方向盲性。呼吁开发能体现语言方向性的损失函数与模型结构。

Conclusion: CLM预训练的目标在方向上具有对称性，这是一种局限性，可能无法捕捉自然语言中的方向性依赖。未来应探索显式建模语言方向性的预训练方法。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [66] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym为检验LLM在低资源语言和未见结构上的语言推理泛化能力而提出，通过结构化语言信息提升推理表现，既显示了LLM的应用前景，又指明了改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM（大型语言模型）评估通常集中于特定下游任务，缺乏对模型在低资源语言及未见结构上的泛化、推理能力的系统检验。因此，该论文希望填补这方面的空白。

Method: 提出了LingGym基准，利用18种类型多样的参考语法，结合插间注释文本（IGT）和语法描述，设计了一个受控评测任务：Word-Gloss Inference。模型需根据不同层次的语言信息（如注释、语法解释、翻译等）来推断缺失的词和注释。

Result: 实验结果表明，加入结构化的语言提示能稳定提高各类模型的推理表现，尤其是在低资源语言及未见结构方面。

Conclusion: 本研究展示了LLM在基于语言类型信息的分析和低资源语言文献整理上的潜力，同时也揭示了其当前的局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [67] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本研究首次提出为编程调试中的Socratic对话自动生成推理轨迹，并构建数据集。使用LLM能较好自动完成此任务，生成的对话和轨迹经评估表现优异，对编程教育有积极意义。


<details>
  <summary>Details</summary>
Motivation: 初学者编程错误常由对编程概念的误解引起。Socratic debugging旨在通过师生对话而非直接给出答案，让学生自行发现和修正错误，但如何系统生成和评估指导推理过程是挑战。

Method: 定义“推理轨迹”任务，手工标注相关调试数据集，并提出基于大型语言模型（LLM）自动生成推理轨迹及与其对应的Socratic对话的方法。最后，用大规模LLM评价自动生成的结果。

Result: 前沿的大型语言模型能生成高达91%的正确推理轨迹和98.7%有效对话轮次，验证了方法的有效性。

Conclusion: 提出推理轨迹生成任务、相应数据集及基于LLM的解决方案，为Socratic debugging在编程教育中的应用奠定了基础。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [68] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: AI文本检测器对直接生成内容表现优秀，但对迭代释义文本识别失败，特别无法区分人类作者和被释义生成内容。作者提出PADBen基准，发现现有方法在某些关键场景完全失效，强调亟需突破现有检测方法。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器对直接LLM输出有高准确率，但对逐步释义的AI文本检测失败，亟需研究原因及提升检测能力。

Method: 通过机制分析揭示释义文本的“中间洗白区”，并提出PADBen基准，系统评估现有检测器对释义攻击的鲁棒性，涵盖5类文本和5种检测任务，对11个主流检测器进行测试。

Result: 释义攻击下，检测器在识别抄袭规避场景上有效，但对作者身份混淆场景表现极差，无法覆盖“中间洗白区”。

Conclusion: 现有AI生成文本检测器无法有效识别经过迭代式释义的文本，尤其在作者身份混淆方面表现不佳，亟需新的检测架构以应对这一挑战。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [69] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 本文首次提出了跨英语和日语的医学错误处理基准MedRECT，并系统评估了多个主流大语言模型的错误检测、定位与纠正能力。结果表明，推理架构显著提升模型表现，微调手段进一步提高纠错效果，部分指标甚至超过人类专家。该工作为医疗AI安全落地和多语种能力提供了重要的新基准和工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域表现出巨大潜力，但在临床文本中识别和纠正错误的能力，特别是在英语以外的语种，尚未得到充分评估。该能力是模型安全应用的前提。

Method: 作者提出了MedRECT，一个涵盖日文和英文的跨语言医学错误处理基准，包括错误检测、错误定位（句子抽取）、错误纠正三大任务。采用自动化流程从日本医师资格考试与精选英文材料构建语料库，并在不同类别的大模型（专有、开源与推理型）上进行评估。还对模型进行了目标化LoRA微调。

Result: 推理型大模型在错误检测和句子抽取任务上明显优于标准架构，性能提升最大达13.5%（检测）和51.0%（抽取）；日文与英文性能存在5-10%差距，推理模型差别较小；LoRA微调对错误纠正有非对称提升（日文+0.078，英文+0.168），且不影响推理能力；微调模型在结构化医疗错误纠正任务上超越人类专家表现。

Conclusion: MedRECT是首个涵盖多语种医学错误纠正的基准和框架，推动医疗大模型跨语言安全性提升，为行业模型开发与评价提供新资源和标准。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [70] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 大语言模型输出内容过于相似，现有提升多样性方法会损失质量。本文提出了G2，无需训练、可快速集成的方法，通过Guide引导生成过程，使输出既多样又保持高质量，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型尽管在多个任务中表现出色，但在输出多样性方面存在明显的局限，常常生成非常相似的内容。这对于需要多样化输出的任务如创意写作和推理造成了影响。现有方法如温度调整虽然能提升多样性，但会牺牲输出质量。

Method: 提出了一种无需训练的新方法Guide-to-Generation（G2），可即插即用，提高输出多样性且保持质量。G2包含一个基础生成器和两个Guide，利用解码干预方式，引导生成过程，根据原始查询生成更具多样性的输出。

Result: 实验表明，G2方法在提升输出多样性的同时，能够很好地维持输出的质量和平衡。

Conclusion: G2是一种有效提升大语言模型输出多样性的工具，可在不降低质量的前提下应用于各类需多样化生成的任务。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [71] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 论文分析了三种主流LLM模型在生成学术合作网络时，因记忆效应带来的信息展示偏见。整体来看，模型偏好高被引学者，但少数学科和地区体现更大公平性，提示训练数据的均衡重要性。这揭示了LLMs在科研发现应用中既有风险也有潜在机会。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）不断取得突破并在搜索与推荐平台广泛应用，出现了潜在的公平性和偏见问题，尤其在科研信息获取领域更加突出。作者关注LLMs在学术工具中的应用，特别是其基于记忆生成内容可能带来的学术生态失衡风险。

Method: 本文以记忆效应为切入点，对比分析了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三种主流LLM模型在学科与地区维度上的输出。通过全球和细分领域的数据，评估LLMs在生成合作网络过程中反映和放大哪些固有偏见。

Result: 分析结果表明，LLMs整体上表现出对高被引研究者的偏向，但该偏向在全球范围内并不均匀。例如，临床医学及部分非洲地区的表现更为均衡，说明某些领域或地区的训练数据更加公平。

Conclusion: LLMs在学术发现中的部署既带来机遇，也存在放大科学群体内外不平等风险的问题。对LLMs的记忆效应和数据偏见应保持警惕，合理利用其优势，推动学术信息生态公平发展。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [72] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文提出全球最大的Bhili-印地语-英语三语平行语料库BHEPC，为低资源机器翻译研究建立基准。实验证明，经微调的NLLB-200多语言模型在Bhili翻译任务表现最佳，推动了低资源语言NLP技术进步。


<details>
  <summary>Details</summary>
Motivation: 印度拥有丰富的语言多样性，尤其是像Bhili这样的部落语言资源稀缺，严重制约了相关机器翻译的研究和应用发展。为突破这一障碍，论文致力于填补Bhili等低资源语言缺乏高质量平行语料库的空白。

Method: 作者构建了全球首个、最大规模的Bhili-Hindi-English三语平行语料库（BHEPC），涵盖教育、行政、新闻等领域，共精选11万句，全部经专家人工翻译。并在该语料库上系统评估了多种专有和开源多语言大型语言模型（MLLMs）在英/印<->Bhili双向翻译任务中的表现，包括微调NLLB-200模型和多语言LLM的生成式翻译能力、跨领域泛化及分布性差异分析。

Result: 微调后的NLLB-200 distilled 600M变体模型展现出最佳双向翻译效果，优于其他主流多语言模型；多语言LLM在低资源场景下利用上下文学习具备一定跨领域泛化能力。

Conclusion: 该研究首次构建了Bhili-印地语-英语三语大规模平行语料库，为低资源机器翻译提供了重要基准。系统评测显示经微调的多语言模型在低资源语言上表现优异，有助于推动包容性自然语言处理技术发展，为全球边缘语言提供更多研究与应用支持。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [73] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次系统性研究了数据集规模对差分隐私文本重写机制隐私与效用表现的影响。实验表明，随着数据量的变化，隐私-效用权衡亦发生变化，提示未来相关领域需加强对数据规模因素的评估与重视。


<details>
  <summary>Details</summary>
Motivation: 近年来差分隐私（DP）在自然语言处理（NLP）中的应用取得了许多进展，特别是文本重写机制方面。但是在这些机制的评估中，数据集规模对效用和隐私保护的影响常常被忽视。本文旨在研究数据集规模对DP文本重写机制表现的影响。

Method: 本文首次将数据集规模这一因素引入到DP文本隐私处理的评估。设计了针对大规模数据集（最多100万文本）并动态分割的数据集上的效用和隐私测试，用于量化数据集规模增加时隐私-效用权衡的变化。

Result: 实验发现，数据集规模在评估DP文本重写机制中起着重要作用。随着数据集规模的变化，隐私与效用之间的权衡也发生明显变化。呼吁未来在DP NLP领域进行更加严谨的评估，并为DP NLP大规模实际应用提供了新视角。

Conclusion: 数据集规模显著影响差分隐私文本重写机制的隐私-效用权衡，呼吁在DP NLP研究和实际应用中重视规模因素，并采用更严格的评估方法进行分析。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [74] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了基于文档层次结构的树型MapReduce推理方法（ToM），能更好抓取长文本的逻辑与依赖，在大语言模型的长文本推理任务中显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在长文本推理任务中，由于上下文窗口有限，常常出现性能下降。RAG和DCF等常用方法虽能在一定程度上缓解此问题，但存在逻辑连贯性不足或无法抓取长程依赖等局限。

Method: 提出Tree-oriented MapReduce（ToM）框架，利用文档的层次结构，通过层级语义解析构建DocTree。在Map步骤生成各子节点的推理结果，在Reduce步骤通过同级节点间的聚合解决冲突或达成一致，实现递归推理。

Result: 在70B+参数的LLM模型上，ToM显著优于现有DCF与RAG方法，在逻辑连贯性和长文本推理能力方面表现更佳。

Conclusion: ToM框架能有效克服现有方法在长文本推理中的局限，提升了大模型对于复杂长文档的理解与推理能力。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [75] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG提出利用LLM内部知识，裁剪冗余语料并优化检索流程，有效提升检索增强生成系统的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）知识库的扩展，外部知识语料库与LLM之间出现了大量知识冗余，这不仅增加了密集检索的成本，还可能影响检索增强生成（RAG）的性能。

Method: 提出Zero-RAG框架，包括Mastery-Score冗余知识识别与语料裁剪、Query Router查询路由机制和Noise-Tolerant Tuning耐噪训练方法，优化LLM内部知识利用并减少冗余。

Result: Zero-RAG方法将维基百科语料缩减30%，检索阶段加速22%，且不影响RAG的整体性能。

Conclusion: 通过裁剪冗余知识并优化LLM内部知识利用，可显著优化RAG系统的效率和效果。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [76] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 本研究证明，针对农村常见疾病的会话数据微调后，离线小型对话模型可实现靠谱的医疗交流，在无网环境下医疗AI有推广价值。


<details>
  <summary>Details</summary>
Motivation: 在尼泊尔农村等资源受限地区，医疗服务缺乏，尤其是无法依赖互联网和云基础设施。因此，需要离线的会话代理来辅助当地医疗服务。

Method: 对可离线运行的轻量级生成对话模型DialoGPT进行微调，训练数据为人工构建的涵盖农村常见十种疾病的医患交流语料。

Result: 微调后的模型能生成连贯、相关且医学上合适的回复，体现了对症状、疾病背景和同理心沟通的理解。

Conclusion: 轻量级、可离线部署的对话模型在低资源医疗环境下具有高度可适应性，利用针对性语料可实现有效领域适配，展现出医疗会话AI在农村应用的潜力。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [77] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文提出可量化Transformer模型性别偏见的新指标，并用性别均衡数据继续预训练来有效缓解偏见，在不损失模型性能的基础上，显著减少了多种主流模型中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 近年来，语言模型中的性别偏见问题受到越来越多自然语言处理领域研究者的关注。尤其是编码器型Transformer模型在很多任务中取得优异表现，但也表现出强烈的数据继承性别偏见。论文旨在揭示并量化这些模型中的性别偏见，并提出相应缓解方法。

Method: 论文聚焦于BERT、ALBERT、RoBERTa和DistilBERT等主流Transformer架构中的上下文词嵌入，提出了一种新颖的偏见度量指标（MALoR），基于模型填充被mask词的概率量化偏见。此外，论文通过对抗事实数据增强（Counterfactual Data Augmentation）生成的性别均衡数据集，继续预训练（Continued Pre-training）模型以减缓偏见。

Result: 利用所提出的方法，BERT-base模型中“He-She”偏见分数从1.27降低至0.08，“His-Her”从2.51降至0.36，BERT-large的“Male-Female”偏见从1.82降至0.10。其他模型也有类似显著下降。缓解后模型在下游任务上的性能未受影响。

Conclusion: 通过新的性别偏见指标和性别均衡数据增强继续预训练方法，能够有效降低主流Transformer模型中的性别偏见，并保持其在实际任务中的性能。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [78] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: LRM生成大量无用重复token，WSC可高效检测并移除这些word salad，节省资源且几乎无质量损失，对实际应用至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LRMs）在推理时，生成输出token的成本很高，而且大量token为自我重复、无意义的“word salad”，浪费了计算资源和解码预算。

Method: 通过观察LRM隐藏状态，在每个推理块后跟随的token展示出可检测的word salad模式。利用单层线性分类器识别这一行为，检测后用简单的截断和再生成提示，仅移除语义冗余的token。

Result: WordSaladChopper（WSC）大幅减少输出长度，质量损失极小，同时具有较低系统开销和强节省效果。

Conclusion: WSC为LRM提供低侵入、高效的冗余token去除方案，对提升用户体验非常重要，是未来LRM应用的必须组件。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [79] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: CISEA-MRFE框架通过增强语义基础和特征提取，有效解决了情感分析中的细腻情感识别、领域迁移和类别偏差问题，在多个主流数据集上实现了显著准确率提升，证明了其优越性与广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和预训练语言模型已在情感分析上取得了进展，但现有方法在处理细腻情感线索、领域迁移和不均衡情感分布时表现不佳，主要原因包括语义基础不足、泛化能力弱和对主导情感类别的偏向。

Method: 提出了CISEA-MRFE框架，集成了情境指令（CI）、语义增强扩展（SEA）和多精细特征提取（MRFE）。CI通过域感知指令引导情感解析，SEA利用情感一致的释义扩展提升鲁棒性，MRFE结合多尺度特征专用的编码器与情感评估上下文编码器实现情感感知的序列建模。

Result: 在IMDb、Yelp、Twitter和Amazon四个基准数据集上，CISEA-MRFE在准确率上相较于强基线提升分别为4.6%、6.5%、30.3%、4.1%，显示其方法在多领域情感分类中的有效性和泛化能力。

Conclusion: 通过集成情境指令、语义增强和多精细特征提取，CISEA-MRFE有效提升了情感分析的语义基础和泛化能力，在多个数据集上取得了显著性能提升，验证了方法的通用性和实用性。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [80] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 论文提出意图转移攻击（ISA），通过对攻击请求进行微妙重构，使其更难被LLM检测为恶意，极大提升了攻击成功率。现有防御措施均难敌ISA，暴露出LLM意图识别存在根本性安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御主要针对上下文或对抗token干扰，但攻击意图多未改变。需要探索更隐蔽且高效的攻击方法，从意图层面规避模型检测。

Method: 提出了意图转移攻击（Intent Shift Attack, ISA）框架，通过构建意图变换分类体系，并基于此生成攻击样例。ISA只对原始攻击请求做最小化、自然化编辑，变得难以被检测为恶意。实验覆盖开源与商业LLM，并测试不同防御方式效果。

Result: ISA在攻击成功率上较直接有害请求提升70%以上，若进一步将ISA格式的良性请求用于微调，可使攻击成功率接近100%。现有防御方法对ISA效果不佳。

Conclusion: 现有防御方法无法有效应对意图转移攻击（ISA），表明LLM安全中意图识别仍是重大挑战，需要开发更有效的防御机制。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [81] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA显著优化Transformer推理内存和吞吐，但在检索类任务表现还有待提升。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理领域取得了优异的表现，但推理时需要大量内存，尤其是需要维护全部上下文信息，成为使用瓶颈。研究动机是减少Transformer推理的内存消耗，同时保持性能。

Method: 本文提出了FlashEVA，这是一种高效的EVA（通过控制变量实现高效注意力）实现，并探讨了如何微调Transformer模型以适应FlashEVA注意力机制。实验中仅用1.5B tokens进行微调，并支持超参数调节以便在吞吐量和精度之间做权衡。

Result: FlashEVA在推理过程中的吞吐量高达标准Transformer的6.7倍，GPU峰值内存消耗降低至标准Transformer的五分之一。

Conclusion: FlashEVA极大提升Transformer在推理时的效率和灵活性，尤其在内存受限场景下表现突出，但在检索相关任务仍有局限。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [82] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: 该论文提出了OpenSIR，一个基于自我博弈的无监督大型语言模型推理框架，实现了从简单到高阶数学能力的开放式提升，大幅改善现有模型的表现，突破了人工标注奖励的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大型语言模型推理依赖人工标注数据集和可验证奖励，这限制了模型超越人类水平的能力。自我博弈（self-play）是潜在的替代方法，但现有方法要么依赖外部验证器，要么无法进行开放式学习。

Method: 提出了Open-Ended Self-Improving Reasoner（OpenSIR）框架，通过自我博弈让LLM在无外部监督下交替扮演老师和学生角色，自主生成和解决新问题。该框架通过优化问题的难度和多样性，奖励有挑战性且覆盖不同概念的问题，实现开放式数学探索。

Result: OpenSIR能从一个单一的简单种子问题出发，显著提升指令模型的性能。例如，Llama-3.2-3B-Instruct在GSM8K从73.9提升到78.3，在College Math从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K从38.5提升到58.7。分析显示，OpenSIR通过教师-学生角色协同进化，自适应难度和多样化探索，实现了自主的基础到高级数学的进步。

Conclusion: OpenSIR是一种无需外部监督的自我改进推理框架，能够实现开放式、自主的数学学习和推理任务提升，突破了依赖人工奖励的限制，推动了模型能力的持续进化。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [83] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2通过非自回归草稿和校准技术，突破现有LLM推理速度瓶颈，平均加速55%且准确率不变，达到了新性能标杆。


<details>
  <summary>Details</summary>
Motivation: 现有推理加速方法受限于起草阶段的自回归依赖（限制并行性）与draft/verifier模型不一致导致的高拒绝率。

Method: 提出SpecDiff-2框架，采用离散扩散作为非自回归起草模型，并通过新技术校准与自回归验证器的契合度。

Result: SpecDiff-2在推理、编码和数学等任务中，令平均tokens-per-second提升55%，比标准解码快最多5.5倍，同时保持准确率不降。

Conclusion: SpecDiff-2显著加速LLM推理，在多个基准上实现了速度和准确性的双重提升。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [84] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 本文发现GPT-4.1和DeepSeek-Chat在概率性任务中虽然能输出正确答案，但其token概率与理论概率分布存在明显偏差，说明传统不确定性估算方法在此类场景下较为有限。


<details>
  <summary>Details</summary>
Motivation: 在关键的决策支持和知识密集型应用中，确保大型语言模型的不确定性量化（UQ）可靠性至关重要。当前常用基于token logits的不确定性估算方法，可能无法准确反映模型在概率性任务中的理论分布对齐程度。

Method: 利用GPT-4.1和DeepSeek-Chat，分别在有无概率性提示的十组概率任务（如掷骰子）下，评估模型对(1)情景约束的响应有效性，以及(2)token输出概率与理论概率分布的对齐程度。

Result: 两种模型在所有场景下均能给出完全正确的响应，但其token级别的概率与熵与理论概率分布存在一致且持续的偏离。

Conclusion: 虽然现有大型语言模型在结果准确性方面表现优异，但其概率输出无法与理论概率分布很好地对齐，这意味着当前基于token概率的UQ方法在严格概率场景下存在局限性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [85] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 本研究用计算方法分析法国侦探小说侦探原型在150年间的演变，发现该原型从边缘角色变为核心人物，并因社会变化出现更复杂的特征。


<details>
  <summary>Details</summary>
Motivation: 探索侦探小说中侦探原型随时间的演化，并分析各时期社会因素对该原型的影响。

Method: 采用定量方法与角色级嵌入，通过监督模型分析侦探原型的演变过程。

Result: 模型证明侦探原型在长时间跨度内具有统一性，但角色功能及复杂性随着文学和社会环境变化显著增长。

Conclusion: 法国侦探小说的侦探原型在150年的文学中表现出统一性，并且随着时间推移逐渐演变，其角色从辅助性变为故事核心，并在二战后随着硬汉侦探传统的引入呈现出更复杂的面貌。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [86] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA数据集揭示多语言大模型在不同语言和文化间文化素养理解能力有限，英文优势突出但不代表地域性文化理解更强，亟需改进跨文化迁移能力。


<details>
  <summary>Details</summary>
Motivation: 目前多语言问答基准虽然涵盖了多种语言，但通常未能考虑信息的地域多样性，容易偏向西方内容，导致评估多语言模型时在地区文化理解上的不公平。

Method: 作者提出了名为XNationQA的新数据集，包含九个国家、七种语言、共计49,280个关于地理、文化和历史的问题，旨在评测多语言大模型（LLM）的文化素养。并基于两种创新的迁移评估指标，对八个主流多语言大模型在该数据集上的表现进行了测试和分析。

Result: 分析发现，多语言模型在不同语言之间获取特定文化事实的能力有很大差异。模型在英语下对相关文化的了解程度往往高于本地主要语言。同时，模型在西方语言上的总体表现更好，但这并不意味着它们对西方国家更有文化素养。此外，模型在不同语言间迁移文化知识的能力极为有限，尤其是在开源模型上表现更差。

Conclusion: 当前多语言大模型在地域文化信息理解和跨语言知识迁移上存在明显短板，未来应关注数据多样性和跨文化迁移能力的提升。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [87] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 首次进行了大语言模型多语言越狱攻击与防御的系统评测，发现现有攻防手段跨语言泛化性弱，需构建跨语言安全评测体系。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在训练和微调后会进行安全对齐，但近期研究发现可通过越狱攻击（jailbreak）绕过安全措施。然而，目前对这些攻击和防御手段在多语言环境下的泛化能力研究不足。

Method: 首次系统性地在十种语言（涵盖高、中、低资源语言）下，利用HarmBench和AdvBench，针对六种大语言模型评估两类越狱攻击（基于逻辑表达式和对抗性提示）及其防御方法。

Result: 对于两种越狱攻击，攻击成功率及防御鲁棒性在不同语言间存在显著差异：高资源语言在标准查询下更安全，但在对抗性查询下更易受攻。简单的防御方法虽然有效，但依赖于语言和模型。

Conclusion: 结果表明，LLMs安全防护需考虑语言差异，并应建立具备跨语言能力的安全基准。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [88] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 提出了对NSA的关键改进：交替局部/全局注意力+潜在注意力分支，有效提升了常识推理与长文本理解，同时减少一半的KV-cache内存消耗，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在长文本序列建模中，全注意力机制的计算和内存开销极大，因此稀疏注意力（Sparse Attention）成为主流方案之一，但现有的Native Sparse Attention（NSA）仍有性能和效率的进一步提升空间。本文旨在系统分析NSA并提出改进方案，以提升其在长序列任务中的建模与推理能力。

Method: 作者提出交替使用局部（滑动窗口）与全局（压缩、选择性）注意力的方法，替代原有的固定模式。这种方式能够更有效地传播长距离依赖关系。此外，在注意力分支内引入潜在注意力（Latent Attention）：滑动窗口分支增强为多头潜在注意力（MLA），压缩和选择性分支采用组头潜在注意力（GLA）。这些设计优化了注意力机制的结构。

Result: 改进后的稀疏注意力机制在减少50% KV-cache内存消耗的同时，提升了模型的常识推理能力及长文本理解能力。实验覆盖340M至1.3B参数模型，训练数据量分别为15B与100B tokens，结果显示新方法在常识推理和长上下文理解任务上达到或超过全注意力及原生NSA方案。

Conclusion: 通过引入局部与全局注意力层交替结构，结合多头与组头潜在注意力，本文显著提升了稀疏注意力在长文本建模和推理中的效率与表现，在更低内存消耗下展现出优异的效果。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [89] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 本文提出TriCon-Fair对比学习框架，有效去除语言模型中的社会偏见，同时兼顾模型整体能力，在减少歧视性输出和保持性能方面优于传统方法，是一种适用于敏感领域的创新去偏技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型日益普及，但它们往往会传播社会偏见，导致有害和不公平的结果。现有去偏方法独立处理有偏和无偏样本，忽略了它们之间的关系，这可能导致改善一组时损害另一组，残留社会偏见。

Method: 提出TriCon-Fair对比学习框架，结合三元组损失与语言建模损失来消除正负耦合。为每个锚点分配一个明显有偏的负样本和一个无偏正样本，从而解耦推拉动态，并联合优化语言建模目标以保持模型能力。

Result: TriCon-Fair在减少歧视性输出方面优于现有去偏方法，同时保持良好的下游性能。

Conclusion: TriCon-Fair为敏感NLP应用提供了实用且符合伦理的去偏解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [90] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文提出了一套新颖的评估框架，系统检测LLM推理过程中的知识基础，能有效识别推理中的知识缺失或误用，并支持偏好优化等实际应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂任务中通常采用逐步推理的方法，但如何验证模型的推理是否真正基于知识仍是一个关键问题。作者希望通过新的评估方式解决这一问题。

Method: 本文提出了一个新的评估套件，包括三个核心部分：1）主要知识收集，构建用于推理的大规模基础知识库；2）基于知识的评估指标，用于衡量模型在推理过程中对前置知识的回忆与应用；3）评估者LLM，一个轻量级模型，用于高效和可靠地计算上述指标。

Result: 该评估套件能有效发现LLM推理过程中缺失或错误应用的知识，为发现模型的基本推理不足提供了重要见解。同时，作者还展示了这些指标在偏好优化中的应用，扩展了基于知识评估的实际价值。

Conclusion: 通过系统化知识收集和基于知识的指标评估，可有效检测和分析LLM推理中的知识不足，为提升模型的推理能力和优化路径提供支持。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [91] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate通过OCR驱动的预训练和新的对比学习目标，有效提升了多模态文档检索精度和泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档检索方法往往借鉴文本检索的技术，在编码文档、训练目标和相似度计算等方面存在局限，难以充分利用多模态数据的视觉和结构特征。

Method: 提出了ColMate模型，通过新颖的基于OCR的预训练目标、自监督的Masked Contrastive Learning目标，以及更适合多模态文档结构和视觉特征的晚期交互评分机制来改进文档检索。

Result: 在ViDoRe V2基准测试上，ColMate模型相较于现有检索模型取得了3.61%的提升，并在域外基准表现出更强的泛化能力。

Conclusion: ColMate有效弥补了多模态表征学习与文档检索之间的鸿沟，提升了多模态文档检索的性能与泛化能力。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [92] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 论文评估了两款LLM在医学诊断沟通中的可理解性和同理心表现，发现LLM虽然能因人而异调整解释，但存在内容复杂和同理心偏差，需进一步校准以实现公平有效的患者沟通。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）已被用于协助临床医生与患者进行诊断沟通，但其实际生成的内容是否具有可理解性和同理心尚不明确。

Method: 选择两款主流LLM，对医学诊断场景进行评估。利用可读性指标评估解释的易懂程度，以LLM和人工评判测量同理心。

Result: LLM能够根据社会人口学变量和患者情况调整解释内容，但也常生成过于复杂的信息，并在情感同理心上表现出偏差，影响了沟通的公平与支持效果。

Conclusion: 需有系统化的校准机制以确保患者沟通的公平和易懂，优化诊断解释内容的同理心与可访问性。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [93] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文分析了主流大模型在七种印度语言下解谜和自我评估能力，发现推理表现最好者自我检错能力最弱，显示了多语种推理与自知存在双重挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型(LLMs)是否能在非英语语境下进行具备文化背景的推理能力仍未被充分研究。本文旨在探究LLMs在印度七大主要语言背景下的推理能力及自我评估能力。

Method: 1. 构建包含传统谜语和上下文重构谜语的多语种谜语数据集。
2. 选择五种主流LLM（Gemini 2.5 Pro/Flash、Mistral-Saba、LLaMA 4 Scout/Maverick）在七种提示策略下进行评测。
3. 第一阶段评估谜语解答效果。
4. 第二阶段进行自我评估实验，考察模型发现自身错误的能力。

Result: 整体上，Gemini 2.5 Pro 的谜语推理表现最佳，但few-shot方法增益有限，且各语言准确率波动明显。自我评估实验发现，模型推理初始准确率越高，其发现自身错误的能力越弱。Gemini 2.5 Pro 过于自信（真阴性率4.34%），而准确率较低的LLaMA 4 Scout自我意识更强（真阴性率42.09%）。

Conclusion: 现有主流LLM在多语种文化推理任务中存在显著局限，不仅需要提升实际推理能力，还需增强对自身推理错误的识别与自知。模型过于自信可能掩盖其错误率，高自我觉察性对于多语言应用同样重要。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [94] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 论文发现机器生成文本检测存在标签不精确问题，提出易到难增强训练框架把较弱但易于监督的长文本检测任务作为可靠监督，理论和实验均证明该方法提升了多场景下的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有MGT检测方法假设标签精确，而实际存在边界模糊和认知局限等无法避免的不精确学习问题，因此需要新的监督和训练方式以提升检测器鲁棒性和准确性。

Method: 采用由易到难（Easy-to-Hard）增强训练框架，利用针对长文本的弱监督者，通过结构化地将检测器集成到监督者中，理论建模为性能下界并优化。实验在多种现实场景下检验了方法有效性。

Result: 本方法在跨模型、跨领域、混合文本和同义攻击等多场景下均显著提升了检测效果。代码已开源。

Conclusion: 提出的Easy-to-Hard增强框架能在标签不精确的情况下显著提升机器生成文本（MGT）检测的效果。监督者的优化间接带动检测器向“黄金标准”标签逼近。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [95] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL采用多智能体系统和交互式强化学习，有效解决了复杂自然语言到SQL的自动转换问题，实验结果达到了业界领先水平。


<details>
  <summary>Details</summary>
Motivation: 复杂自然语言到SQL转换困难，尤其在需要环境交互和自我修正的复杂查询场景，现有方法难以准确、动态地生成SQL。

Method: 提出了包含Grounding Agent（模式链接）、Generation Agent（查询生成、多轮强化学习）、Validation Agent（结果验证）的多智能体架构。核心Generation Agent采用ReAct风格的“思考-行动-观察”循环，根据数据库执行反馈动态修正生成策略，推理阶段生成多条推理轨迹，Validation Agent通过生成概率选择最优解。

Result: 在BIRD dev set上获得77.84%的执行准确率，在Spider test set上获得89.75%的执行准确率，均为当前最优性能。

Conclusion: MARS-SQL通过多智能体协作与交互式强化学习显著提升了复杂自然语言到SQL的翻译能力，成功实现了高精度与鲁棒性的查询生成。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [96] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出高效、可靠的指令约束评估模型IF-CRITIC，通过约束拆分和筛选机制优化训练，实验效果优于主流基线且计算成本更低，能显著增强LLM的指令遵循表现。


<details>
  <summary>Details</summary>
Motivation: 现有关于指令遵循的评价模型成本高、结果不可靠。因此，需要更高效、可靠的解决方案来提升LLM对输入指令约束的遵循和评估能力。

Method: 提出了IF-CRITIC模型，包括指令拆解与约束清单生成、多阶段筛选获取高质量批评数据，并在约束层面进行偏好优化训练。通过与现有基线进行对比实验验证其有效性。

Result: IF-CRITIC不仅在评价准确性上超过主流基线模型Deepseek-R1和o4-mini，同时在算力消耗上也更低。其提供的奖励信号有助于大幅提升LLM指令遵循能力。

Conclusion: IF-CRITIC能够高效且可靠地评估LLM对指令约束的遵循情况，在评价性能上优于现有强大的LLM-as-a-Judge基线模型，并显著降低了计算开销。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [97] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 通过强化学习框架Prompt-R1，用小型LLM协助大型LLM解决复杂问题，自动化生成高效提示，超越人工互动，性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 先进的大语言模型（LLM）虽已广泛应用，但用户难以生成有效的复杂提示，影响LLM性能。该研究旨在解决普通用户无法与LLM高效互动的问题。

Method: 提出Prompt-R1端到端强化学习框架，让小型LLM协同大型LLM，通过多轮提示交互自动生成高质量提示，替代用户人工输入。设计了双约束奖励机制，优化正确性、生成质量与推理精度。

Result: 在多个公开数据集上的实验显示，Prompt-R1在各类任务上显著优于基线模型。验证了该方法的有效性和广泛适用性。

Conclusion: Prompt-R1通过小型LLM与大型LLM协同，显著提升了复杂任务的解决能力，实现了高质量自动化提示生成，可广泛用于训练和推断阶段。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [98] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI将大语言模型与NOAA实时海洋数据结合，确保生成的回答均有权威数据支持。盲测显示其输出比主流产品更精准可验证，适用于海洋科学多个领域，提升AI科学应用的可信度和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前的通用对话式人工智能系统在科学领域应用时，往往会产生未经验证的“幻觉”，影响科学严谨性，亟需具备数据可验证能力的AI系统以提升科学研究的透明度和可信度。

Method: 提出OceanAI平台，将开源大型语言模型的自然语言处理能力，与美国NOAA权威海洋数据流的实时、参数化访问结合。用户每次查询会触发实时API调用，自动检索和处理相关数据集，并以可复现的自然语言响应和数据可视化给出答案。

Result: OceanAI在与三款主流AI聊天产品的盲测对比中，唯一能够给出NOAA原始数据来源和具体数值，其它系统要么无法回答，要么结果缺乏数据支持。OceanAI能够扩展支持多个NOAA数据产品和变量，适用于海洋灾害预测、生态系统评估及水质监测等领域。

Conclusion: OceanAI通过接入权威实时数据，显著提升了AI系统在科学对话和决策中的透明度、可复现性和可信度，为AI驱动的海洋科学与决策提供了可规模化的基础框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [99] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 本文首次发布了针对微服务自动修复的LLM评测基准MicroRemed，并提出多智能体推理框架ThinkRemed，在解决复杂系统故障恢复任务上，显著超越了传统方法，推动无人值守智能运维的发展。


<details>
  <summary>Details</summary>
Motivation: 近年来，大语言模型（LLM）结合基于代理的推理框架在自动化决策和系统级操作中显示出巨大潜力。在微服务修复这个具有实际意义但研究较少的方向，急需推进相关自动化能力，减少对人工的依赖。当前方法仍需现场可靠性工程师（SRE）手动编写提示，且LLM仅进行文本到代码的转换，自动化水平有限。

Method: 作者提出了MicroRemed，这是第一个针对微服务修复的LLM端到端评测基准，任务目标是让模型直接根据诊断报告自动生成可执行的Ansible playbooks，实现系统恢复。同时，他们提出了ThinkRemed，一个多智能体框架，模拟SRE的反思性和感知性推理能力，实现推理-反思-生成的闭环。

Result: 实验显示，MicroRemed对现有LLM提出了较高挑战，而ThinkRemed通过迭代推理和系统反思，提升了端到端系统修复的表现。

Conclusion: 自动化微服务修复尚不成熟，现有LLM难以直接解决，需引入更复杂的推理与协作机制。MicroRemed提供了系统性的评测平台，ThinkRemed验证了多智能体推理体系的有效性，为后续AI智能运维研究提供了方向。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [100] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat通过大语言模型，将空气质量等多源数据集成至对话平台，实现空气污染数据的简单查询与可视化，为决策者和公众提供了强大的数据分析支持。


<details>
  <summary>Details</summary>
Motivation: 当前空气污染导致大量死亡，但基于分散数据的决策工具门槛高、形式单一，缺乏易用性和互动性，需要更便捷、直观的数据分析平台。

Method: 系统集成了空气质量、气象和政策数据，基于大语言模型，通过自然语言问答，生成可执行Python代码与交互式可视化。

Result: VayuChat实现了数据集成和便捷分析，简化了政策制定、研究和公众查询流程。用户通过对话即可获得复杂分析和可视化结果。

Conclusion: VayuChat让复杂的环境数据分析通过自然语言对话实现，促进了空气污染治理的科学决策和信息透明。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [101] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 本文提出一个用GPT生成、基于临床指南的数据集，能系统评估大型语言模型在医疗中的推理与指南遵循能力，为其临床应用和评价标准化提供支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗领域应用日益广泛，但缺乏用于评估其基于临床指南推理能力的标准化基准。

Method: 本研究基于公开可获得的多种诊断指南，利用GPT协助生成包含真实患者情境和临床问题的数据集，并用该数据集对多种主流LLMs进行基准测试。

Result: 成功构建了一个经验证的数据集，可用于系统性评估LLMs在临床中的实用性和其对指南的遵循性。

Conclusion: 该数据集和评估框架为LLMs在医疗领域临床推理能力的标准化客观评估提供了重要工具，促进其在实际医疗场景中的应用。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [102] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 本文介绍了一个正在进行的大型开放高质量多语种LLM预训练数据集构建与评测项目，包括200种语言、30万亿词，涵盖全面数据处理与标注流程，创建多语言评测基准与丰富单/多语种模型，极大促进多语种NLP发展。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大型语言模型（LLM）的训练受到高质量、多语种、大规模数据集缺乏的限制，因此论文提出了建立最大规模多语种开放预训练数据集的目标，旨在推动多语言LLM的发展与评测。

Method: 构建开放、丰富标注的多语种文本数据集（涵盖约200种语言，包含30万亿词），通过完整开源流程实现网页文档挑选、文本抽取、语言识别、去重、文本质量和隐私信息标注，并进行最终筛选。采用多种数据质量评估手段（对比与分析统计、人工检查、端到端模型训练等）。此外，打造评价基准（尤其是欧洲九种语言）并训练多语种/单语种模型，采集平行语料并合成新型语料库。

Result: 成功构建了目前最大开放的多语种LLM预训练数据集，并形成了完善的多语种数据处理与评测工具链。完成了对24种语言的数据质量分析，建立了九种欧洲语言的评测基准、训练了57个单语种编码器-解码器模型与参考GPT类模型，以及生成和采集了多语种平行语料库。

Conclusion: 该工作为多语种大型语言模型的研究与应用提供了高效的开放数据基础和标准评测体系，为全球多语言NLP社区带来实质推动，并降低高质量多语种模型开发门槛。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [103] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文研究如何为弱势语言（如罗马尼亚语）生成高质量预训练语料，通过LLM辅助多层次筛选显著提升了模型在各类评测上的表现，与英语语料对比分析揭示了有趣的话题分布差异，说明高质量语料的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）在很多任务上表现已超越人类，但高质量训练数据的获取和构建仍是关键，特别是对于低资源、数据稀缺的语言如罗马尼亚语。该研究关注如何提升罗马尼亚语等弱势语言的数据质量，以实现更好的模型训练效果。

Method: 分析罗马尼亚语训练语料的特点和覆盖范围，并与英语语料进行了对比。通过对罗马尼亚语文本进行LLM辅助注释，训练了一个轻量级多任务模型，能够对语料进行多层次过滤（如教育价值、话题、文本格式）以生成高质量训练数据集。

Result: 实验证明罗马尼亚语和英语数据在话题分布上存在显著差异。同时，通过多层次过滤产生的数据显著提升了预训练LLM在多个评测基准上的表现，验证了高质量数据筛选方法的有效性。

Conclusion: 多层次过滤和高质量数据的筛选方法对于提升弱势语言（如罗马尼亚语）LLM预训练效果具有积极作用，并可推广应用于其他低资源语言领域。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [104] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文构建了一个高质量的时间序列事实核查数据集TSVer，包括真实主张与多领域时间序列证据，并通过LLM辅助标注达到高一致性。现有最先进模型在该数据集表现有限，反映时间序列推理的挑战，为未来相关领域提供了新基准和研究动力。


<details>
  <summary>Details</summary>
Motivation: 当前针对时间序列、数值型数据的事实核查系统面临评估瓶颈，现有数据集存在结构化证据缺失、判决依据不足或依赖于合成主张的问题。

Method: 构建名为TSVer的新数据集，聚焦于基于时间序列证据的时序与数值推理事实核查。数据集包含287个真实主张，覆盖38家事实核查机构及400组多领域时间序列，并通过LLM辅助多步注释流程提升标注质量，获得较高标注者一致性。提出针对时间序列证据的主张核查基线方法，并测试当前先进推理模型的表现。

Result: Gemini-2.5-Pro等先进推理模型在此任务仍有挑战，仅在判决上达到63.37的准确率，判决依据合理性得分Ev2R为48.63。TSVer数据集评注质量高，主张判决间kappa值为0.745。

Conclusion: 本文提出TSVer数据集，显著提升了事实核查领域时序与数值推理的评估能力，揭示当前SOTA模型在时间序列事实核查上的不足，为后续模型改进和方法发展提供重要数据支持。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [105] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 本文研究了销售人员在高频外呼场景下何时放弃客户对话的问题，提出了基于大语言模型的最优停止算法。实证结果显示，该算法显著减少了无效沟通时间并提升总销售额，且比人工决策更高效，有助于优化销售团队的整体表现。


<details>
  <summary>Details</summary>
Motivation: 销售人员在与客户交流过程中，常常面临是否继续对话还是放弃并转向下一个潜在客户的动态决策，但相关决策机制和效率尚不清楚。研究的动机是探索销售人员在高频率外呼环境下如何进行动态筛选决策，以及如何通过算法优化决策过程。

Method: 将销售过程中的动态筛选决策形式化为最优停止问题，并开发了基于生成式语言模型的序列决策代理（停止代理），通过模仿回溯推断出的最优停止策略来学习何时终止对话。算法可以处理高维文本状态，适配大型语言模型，支持开源和专有模型。

Result: 在欧洲某大型电信公司真实销售电话中应用后，停止代理减少了54%在失败通话上的时间，同时几乎保留了全部销售成果；将节省的时间用于更多潜在客户后，预期销售增加高达37%。还发现销售人员容易高估消费者表达冷淡的影响，并错误预测通话失败风险，表明其实时决策存在认知限制。

Conclusion: 人工智能算法能有效纠正销售人员因认知边界导致的决策偏差，大幅提升销售团队效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [106] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 本文提出DebateBias-8K多语种辩论型偏见评测数据集，发现大型语言模型在多语言开放式场景下仍频繁展现严重刻板印象和文化偏见，且低资源语言偏见加剧，呼吁改进现有全球模型对齐方法以实现更公平和安全的AI。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLMs）应用广泛，但主流的偏见评估方法多集中于英文及分类任务，无法全面揭示模型在真实多语境生成场景中的叙事偏见。

Method: 提出了DebateBias-8K多语言辩论型基准数据集，涵盖包含8,400个结构化辩论提示，涉及女性权利、社会经济发展、恐怖主义和宗教四个敏感领域，并覆盖七种语言（包括高资源语言如英语、中文以及低资源语言如斯瓦希里语和尼日利亚皮钦语）。利用四种主流模型（GPT-4o、Claude 3、DeepSeek、LLaMA 3）生成并自动分类超过100,000条响应。

Result: 所有模型在生成响应时均再现了刻板印象和偏见，如将阿拉伯人与恐怖主义和宗教强烈关联（>=95%），将非洲人与社会经济“落后”联系（最高达77%），而西方群体则始终被描述为现代或进步。低资源语言场景下偏见问题更加突出，显示以英文为主的模型对齐训练难以全球化泛化。

Conclusion: 当前的模型对齐方法虽然在一定程度上减少了有害内容，但无法有效遏制多语种开放式生成中的叙事偏见。进一步的多语种偏见评估和文化包容性模型对齐方法亟需提升，以实现更公平的人工智能。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [107] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 提出了ZoFia框架，通过层级显著性关键词提取和多 LLM 协作辩论，有效提升零样本虚假新闻检测的效果，显著优于现有方法，并将开源代码以推动社区进步。


<details>
  <summary>Details</summary>
Motivation: 虚假新闻的快速传播威胁到社会稳定和公共信任，因此亟需有效的检测方法。然而，当前大语言模型（LLM）虽然具备强大的文本理解能力，但由于知识更新滞后及幻觉生成等问题，在应对新闻流时效率和可靠性有限。此外，基于静态数据集训练的模型往往缺乏对新兴话题的泛化能力。

Method: 提出了ZoFia，一种两阶段的零样本虚假新闻检测框架。第一阶段引入层级显著性方法来衡量新闻内容中的实体重要性，并通过SC-MMR算法选择信息丰富且多样化的关键词，这些关键词作为检索外部实时证据的查询。第二阶段，构建多大语言模型交互系统，各模型以不同角色对新闻及相关信息进行多视角协作分析与对抗性辩论，最终产出可解释且更具鲁棒性的结论。

Result: 在两个公开数据集上的实验表明，ZoFia在零样本检测条件下明显优于现有基线，甚至超过了大部分小样本方法。

Conclusion: ZoFia框架有效提升了虚假新闻检测在复杂环境下的准确性和鲁棒性，为相关领域提供了新的思路和工具，且代码将开源以促进社区发展。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [108] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: 提出了Self-Harmony方法，利用单一模型对原始与重述问题答案的调和平均聚合伪标签，显著提升无监督测试时增强学习准确性和鲁棒性，排名业内领先。


<details>
  <summary>Details</summary>
Motivation: 测试时增强学习（TTRL）通过在推理阶段仅依靠合成信号进行模型自适应，然而其核心挑战在于如何构建可靠的学习信号。传统方法如多数投票容易陷入错误但流行的答案，导致模型鲁棒性不足。

Method: 提出Self-Harmony框架，核心思想是正确答案在原始问题及其释义之间应保持稳定。框架让单一模型以两种互补角色工作：Solver（求解者）生成答案，Reframer（重述者）改写输入。采用伪标签机制，不用多数投票，而是通过原始及重述问题的答案频率的调和平均进行聚合，从而挑选对重述稳定的答案，无需人工监督或额外模型。

Result: 在多种推理基准上，Self-Harmony在无标签测试场景下取得了最优表现，在30个测试设置中有28个排名第一。除准确性外，Self-Harmony显示出极高的鲁棒性，所有实验零训练失败，表现出优秀的稳定性和可靠性。

Conclusion: Self-Harmony框架显著提升了测试时增强学习在无监督环境下的准确性和稳定性，通过对答案的稳定性筛选，有效避免了主流方法易出现的伪答案问题。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>
