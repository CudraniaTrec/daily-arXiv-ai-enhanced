<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 本文提出了DecoRTL解码策略，专为RTL代码自动生成优化，通过语法感知和自一致机制，解决传统LLM解码在此领域的局限，大幅提升代码有效性和多样性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）应用于RTL代码自动生成，但传统的解码方法因原本为自然语言设计，导致在结构和语义上难以满足RTL的需求，常产生虚假、重复或无效的代码。

Method: 首先通过对RTL生成过程中token级别熵的经验分析，揭示现有解码失败的根本原因。然后提出DecoRTL方法：结合自一致采样（产生多个候选并基于token级一致性重排序）和语法感知的温度自适应（根据token语法和功能角色调整采样温度），在推理阶段无需微调即可实现。

Result: DecoRTL在多个开源LLM及VerilogEval基准测试上，显著提升了语法有效性、功能正确性和输出多样性，同时基本没有性能损耗。

Conclusion: 提出的DecoRTL是一种针对RTL代码生成的解码策略，在不影响推理性能的前提下，有效提升了生成代码的质量。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 技术面试准备很难，学校课程支持有限，考生感到压力大和准备不足。研究呼吁各方改进培训方式以帮助求职者。


<details>
  <summary>Details</summary>
Motivation: 软件工程求职需要通过技术面试，但技术面试的复杂性和压力很大，而高校课程很少涉及相关准备。了解考生如何准备这些面试以及现有教育的作用，是本文的出发点。

Method: 向正在准备技术面试的候选人发放了问卷调查，共收集131份有效反馈，分析其准备方式和教育支持情况。

Result: 研究发现，候选人很少在真实环境中训练，课程在帮助技术面试准备方面作用有限，导致候选人普遍感到压力大且准备不足。

Conclusion: 学校课程难以有效支持技术面试准备，考生缺乏真实场景训练，因而感到压力和缺乏自信。研究为面向软件工程岗位的技术面试准备改进，提出了针对不同利益相关方的建议。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [3] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 本论文提出用LLM将自然语言转为结构化代码查询，降低了结构化代码搜索的门槛。方法在多个指标上明显优于传统和语义搜索，拓展了代码检索的便捷性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化代码搜索工具虽然能基于代码的语法结构进行高效检索，但需要开发者掌握特定的领域特定语言（DSL），学习和编写这些DSL查询十分困难。因此，降低门槛、提升可用性成为亟需解决的问题。

Method: 本文提出了一种将自然语言查询与结构化代码搜索工具相结合的一般性方法：通过大型语言模型（LLM）将自然语言查询翻译为结构化代码搜索DSL查询，随后利用如Semgrep和GQL等工具进行代码检索。

Result: 在Java项目上构建了包含400条查询的新基准后，实验证明该方法精确率和召回率均达到了55%-70%。此外，该方法在F1分数上比基于语义代码搜索和纯LLM检索的基线分别高出57%和14%。

Conclusion: 通过结合LLM和结构化代码搜索工具，本方法有效地提升了自然语言代码检索的准确性和鲁棒性，显著优于传统搜索方式，极大简化了开发者的搜索流程。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [4] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本文发现，虽然单纯用源代码可测指标难以精确预测移动应用的人气，但如果对流行与否进行二分类，准确率能显著提升，证明代码内部特征对预判应用市场表现具有参考价值。


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的移动应用市场，开发者希望能够在发布前预测其应用的流行度，从而获得战略优势。但目前使用内部软件指标（如源代码可度量特征）来预测流行度仍具有挑战性。

Method: 作者使用F-Droid上的446个开源Android应用作为数据集，抽取系统级、类级、方法级代码度量、代码异味及元数据特征，并结合Google Play的用户评论、下载量等。试验了三种特征集（仅规模、人工筛选、特征选择投票），采用回归与二分类方法进行预测。

Result: 回归模型因数据偏斜表现较差。但当问题转换为二分类（流行与否）后，效果显著提升。最佳模型（多层感知机+Voting特征集）F1-score达0.72，显示了内部代码度量对流行度具有一定预测力。

Conclusion: 内部代码度量虽然解释能力有限，但能作为应用流行度的有用预测指标，这对早期认为其无关质量的结论提出了挑战。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [5] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 本研究对比了心理测量与生物特征指标在压力检测中的效果，结果发现通过时间压力诱发的实验未能在心理测量和多数生物特征指标上显现差异，仅在EDA峰值上有显著差异，提出未来相关研究的方法改进建议。


<details>
  <summary>Details</summary>
Motivation: 传统上，幸福感、压力及其他人因因素的研究主要依赖自报告工具来评估关键变量。但即使经过充分验证和标准化，这些工具也可能存在偏差。因此，研究者越来越关注结合这些自报告方法与更客观方法（如生理测量）来提高研究的可靠性。

Method: 本研究设计了一项实验：参与者在实验前填写问卷，随后佩戴生物特征传感器完成两个编程任务，并在每个任务后填写简短的问卷，最后进行简短的访谈。研究旨在比较心理测量压力与生物特征指标，并在软件工程任务中识别与压力相关的生物特征数据模式。

Result: 心理测量工具未检测到压力。访谈显示，参与者报告的感受分为无压力和感受到时间压力两种。生物特征指标中，仅EDA（皮肤电活动）的快速峰值在任务中有显著性差异。

Conclusion: 通过本实验采用更严格时间限制方式并未成功诱发压力。研究为今后结合压力、生物特征及心理测量工具的研究提供了方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [6] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 本研究系统分析了开发者沟通数据集的特征和14种情感分析工具的性能，提出自动推荐合适分析工具的方法，验证了基于数据特征优化工具选择的有效性，并指出需要持续关注语境变化对工具表现的影响。


<details>
  <summary>Details</summary>
Motivation: 软件开发过程中大量依赖文本交流，因此情感分析能够帮助理解团队动态并为可信的AI分析提供支持。然而，不同平台间沟通风格和内容的差异导致现有情感分析工具在不同数据集上表现不一致。

Method: 作者分析了来自五个平台的10个开发者沟通数据集的语言和统计特征，并评估了14种情感分析工具的性能。据此提出基于数据集特征自动推荐合适情感分析工具的方法和问卷。

Result: 实验表明，不同平台的数据集在语言和统计属性上存在较大差异，利用这些特征能提升工具的选择效果。基于transformer的模型（如SetFit和RoBERTa）整体表现良好，但工具效果依然依赖具体情境。

Conclusion: 论文提出方法能帮助研究人员和实践者根据数据集特性挑选可信赖的情感分析工具，同时强调在交流背景不断变化下需持续评估工具的适用性。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [7] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 本文提出多智能体LLM协作方法，有效提升COBOL代码（函数、文件、项目级别）的自动解释能力，在多个指标上明显优于传统单一LLM方法，为遗留代码维护带来价值。


<details>
  <summary>Details</summary>
Motivation: COBOL仍被金融、企业和政府广泛使用，但开发者数量减少且维护困难，尤其是由于缺乏文档，新开发者难以理解和维护COBOL系统。LLM虽可用于代码解释，但COBOL因代码长度和语法结构特殊，常常超出LLM的Token窗口，现有方法难以适用。

Method: 提出一种多智能体方法，引入两个基于LLM的智能体协作，结合项目代码的上下文信息，分别针对函数、文件、整体项目生成解释。通过将代码上下文融入提示，有效扩展了传统LLM在处理长代码上的能力。方法在14个真实的COBOL开源项目上评测。

Result: 在函数级别，提出的方法在代码功能解释上显著优于基线，在METEOR、chrF、SentenceBERT评估指标上分别提升12.67%、18.59%、0.62%。在文件级别，无论文件长短（包括超出LLM token窗口的文件），本方法在目的、功能、清晰度解释方面分别提升4.21%、10.72%、14.68%。在项目级别，82%的示例项目能够生成准确传达功能和目的的解释。

Conclusion: 多智能体LLM方法可有效提升COBOL代码的自动解释效果，特别是在大体量、长代码和缺乏文档场景下。所提方法优于现有基线，为遗留系统代码理解和维护提供了新思路。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [8] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 该论文提出 RTED，一种结合类型约束分析与反射验证的 Python 类型错误自动检测新技术，实验证明其比现有方法更有效、误报更少，并发现了新类型错误。


<details>
  <summary>Details</summary>
Motivation: Python 的类型错误常常在运行时才显现，影响软件可靠性和开发效率。虽然已有的静态分析工具可以在不执行代码的情况下检测类型错误，但它们的误报率高。最新的单元测试生成技术虽有很高的覆盖率，但缺乏针对类型错误的有效引导，因此有效发现类型错误依然困难。

Method: 提出了一种新的类型感知测试生成技术 RTED。该方法结合了逐步类型约束分析与反射式验证，引导测试生成过程，并有效抑制误报。

Result: 在 BugsInPy 和 TypeBugs 两个主流基准上，相比四种最新技术，RTED 多检测出 22 至 29 个基准类型错误，并且误报更少，精度提升 173.9%-245.9%。此外，RTED 在六个大型开源项目中还发现了 12 个此前未知的类型错误。

Conclusion: RTED 是一种更高效、准确的 Python 类型错误检测方法，不仅提升了类型错误检测的数量，也显著降低了误报，在真实世界的项目中也展现出较强的实际价值。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [9] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 本文提出了VeFIA框架，用于在不泄露数据和无额外延迟的前提下，审计纵向联邦学习中数据方推理软件的执行正确性，且具备极高的异常检测能力，是VFL领域审计推理正确性的首创性工作。


<details>
  <summary>Details</summary>
Motivation: 纵向联邦学习（VFL）是跨组织AI协作的一种分布式软件部署方案，但当前研究没有解决数据方推理软件执行正确性的审计问题。

Method: 作者提出了Vertical Federated Inference Auditing（VeFIA）框架，让任务方可通过结合受信任执行环境（TEE）和协调方，对数据方的推理结果进行验证，并采用随机抽样方法进行验证。

Result: VeFIA在异常推理比例超过5.4%时，能以99.99%的概率检测到执行异常，且在大规模推理场景下不会引入额外延迟。验证结果显示异常检测的阳性预测值、阴性预测值和真正率均为100%。

Conclusion: 论文首次系统化探讨了VFL推理软件执行正确性的问题，提出的VeFIA框架能高效、准确且无信息泄漏地进行大规模推理正确性审计。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [10] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: Meta-Fair提出以变形测试结合LLM自动生成与评估，实现高效自动化的大型语言模型公平性测试，实验显示效果显著，为未来提升自动化测试奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前关于大型语言模型（LLM）公平性的测试方法多依赖人工评估、固定模板、确定性启发式和精心挑选的数据集，导致资源消耗大且难以扩展。因此，急需一种能自动化且可推广的公平性测试方法。

Method: 提出了一种新颖的自动化公平性测试方法Meta-Fair，核心包括（1）采用变形测试，通过控制输入提示的变换（MRs）检查模型输出的变化以发现偏见；（2）利用LLM自身生成多样化测试用例并评估输出。此外，研究还发布了三个支持LLM驱动生成、执行和评估测试用例的开源工具。

Result: Meta-Fair对12个预训练LLM、14种变形关系、5个偏见维度、7900个自动生成测试用例进行实验；有效发现模型偏见，平均查准率达92%，在29%执行中揭示出偏见行为。LLM对输出评估也具较好一致性，最佳F1可达0.79，通过合理MR设计可缓解非确定性影响。

Conclusion: Meta-Fair显著提升了LLM公平性测试的自动化与可扩展性，并在实测中能有效发现模型偏见。虽然仍有进一步提升空间，但为LLM公平性自动测试拓展了新方向。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [11] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文提出LLMREI聊天机器人辅助需求获取访谈，采用零样本及分步提示策略，在模拟实验中表现出与人类访谈员相近的能力，特别适用于大规模自动化访谈。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈在收集系统需求中至关重要，但高度依赖有经验的分析师，导致其资源消耗大、容易出现人为偏见和沟通失误。随着大语言模型的发展，自动化部分需求获取过程成为可能。

Method: 提出并实现了LLMREI聊天机器人，通过两种主要方法（zero-shot prompting和least-to-most prompting）优化机器人用于需求访谈，同时在33次模拟访谈中评估其表现。起初还尝试了微调方法，但因效果不佳被放弃。评估关注机器人在减少常见错误、需求提取、根据上下文调整问题等方面的表现。

Result: LLMREI在出错数量上与人类访谈员类似，能够提取大部分需求，并具备根据上下文生成高度相关问题的能力。其最大潜力在于自动化大规模访谈。

Conclusion: LLMREI可有效自动化需求获取访谈，表现接近人类分析师，特别适合大规模涉众参与的场景，有望减轻人工负担，提高访谈效率。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [12] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 本文提出了在自适应CPS中实现公平、高效的人机团队协作的方法，既关注人机高效合作，又强调人类隐私和伦理价值，通过新方法推动CPS系统的可持续发展。


<details>
  <summary>Details</summary>
Motivation: 适应性网络-物理系统（CPS）正越来越多地集成人机协作，以发挥人和机器各自的优势。然而，实现高效、无缝的人机协作（HMT）面临诸多挑战，尤其是在将人类纳入已有的反馈回路和保护人类隐私及价值观方面存在空白。

Method: 本研究提出两大方法：1）开发新的方法与流程，将人机团队协作（HMT）原则系统性地融入CPS中的自适应反馈回路；2）从需求工程阶段开始，设计了一套整合、验证和保持伦理与人类价值观的框架，贯穿整个系统生命周期。

Result: 研究成功开发了融合HMT到CPS自适应反馈回路的创新方法，并建立了从需求到系统实现的伦理与人文价值整合与验证机制。

Conclusion: 该研究为在人机协作高度融合的自适应CPS中实现可持续、高效与伦理兼顾的人机团队工作提供了系统性的理论与技术框架。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [13] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 本文发现研究型软件工程师和软件工程研究者在基本术语使用上存在差异，但有合作空间。提出了一套术语映射的系统方法，并为今后通过众包继续研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: RSEs和SERs虽然从事相关领域，但往往针对相似概念使用不同术语，导致沟通障碍。了解这种分歧有助于促进双方合作与知识交流。

Method: 采用系统化的术语映射方法，比较和分析研究型软件工程师与软件工程研究者在基础SE概念上的解读与使用差异。

Result: 初步研究不仅识别了术语对齐和知识差异，还揭示了互补协作的可能性，提出可通过众包扩展和验证术语映射的方法。

Conclusion: 该研究初步发现RSEs和SERs在基本概念上的用词存在差异，但也存在相互学习和合作的机会。基于术语映射的方法为未来众包扩展和验证打下了基础。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [14] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种融合RL和异构图神经网络的业务流程下一活动预测方法，能够针对不同业务流程复杂度自适应地选择最优流程图结构，实现高精度和高效率的预测，在多个真实数据集上表现突出，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 在微服务环境、分布式企业系统和云原生平台等服务导向架构中，业务流程的优化依赖于对下一步活动的准确预测。当前主流的基于序列的方法难以捕捉流程中的并行执行与条件依赖等非顺序关系，基于图的方法虽然保留流程结构信息，但在应对不同复杂性的业务流程时仍存在表示单一和结构静态的问题。

Method: 提出RLHGNN框架，将事件日志转化为具有三种不同边类型的异构流程图，并基于流程挖掘理论构建四种灵活的图结构来适应不同复杂程度的流程。通过强化学习（马尔可夫决策过程）自动为每个流程实例选择最优图结构，再利用异构图卷积结合关系特定的聚合策略进行下一活动预测。

Result: RLHGNN在六个真实数据集上的实验结果显示，在预测性能上优于现有最先进方法，并保持1ms级别的预测时延，兼具高效性和实用性。

Conclusion: RLHGNN能够自适应地对不同复杂度的流程建模，准确预测下一活动，具备优越的性能与实时推断能力，适用于实际业务流程监控场景。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [15] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 作者提出“可持续性标志”，用于辅助识别云架构论坛帖子中的可持续性问题，实验证实其效果优于单一定义，且更易于实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着云计算兴起，软件系统可持续性越来越受关注。但现有论坛讨论中，软件从业者很难准确识别和讨论可持续性问题，主要原因是缺乏清晰的操作性指引。

Method: 本文通过分析多个云服务商的可持续性最佳实践，采用主题分析法提出了“可持续性标志”概念。随后，通过对比实验评估这些标志在识别云架构帖中可持续性问题的有效性。

Result: 实验结果显示，使用可持续性标志后，被分类为可持续性相关的帖子减少，但分类的确定性中等提高，整体性能显著提升。参与者认为标志比仅依赖定义更有用且更易理解。

Conclusion: 引入可持续性标志有助于在线云架构讨论中更高效、准确地识别可持续性内容，也提升了用户的使用体验和信心。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [16] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文提出将法律文本自动解析为Python可执行结构，模型不依赖大量手工标注，泛化性强。实验表明该方法具有较高准确率和适用性，有望帮助小型组织进行高效合规。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统需要遵守日益复杂的法律法规，尤其是对缺乏法律专业知识的小型组织和初创企业而言，理解和提取法律要求成为一项资源消耗巨大且困难的工作。现有自动化方法在提取法律元数据时存在泛化能力差、对属性间关系处理不足等问题。

Method: 本文提出了一种基于文本蕴涵与in-context learning的方法，实现对法律文本的规范化表示，并将其编码为可执行Python代码。构建了一个手工设计的Python类结构作域特定元模型，用于捕捉结构和语义元数据及其相互关系。该方法减少了对大规模手工标注数据集的需求，提高了对新法规的适用性。

Result: 在13项美国各州的数据泄露通知法规上评估后，方法生成的表示有89.4%的测试用例通过，精度达到82.2，召回率为88.7。

Conclusion: 该方法可以有效自动化地、结构化地从法律文本中提取元数据及其复杂关系，适用于法律合规领域，尤其适合缺乏充足法律资源的中小型组织。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [17] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本研究表明，GPT-4o等LLM能有效生成软件需求访谈问题，既不比人工差，且受错误类型引导时表现更佳，有助于提升访谈效果。


<details>
  <summary>Details</summary>
Motivation: 访谈在软件需求获取中起关键作用，但主持人要实时提出合适问题常常面临领域不熟、认知负担重、信息过载等挑战。近期大语言模型（LLM）在文本任务中表现优异，因此研究希望用LLM辅助访谈提问。

Method: 文章利用GPT-4o生成访谈跟进问题，基于常见访谈错误类型建立问题生成框架，并提出从采访对象讲话内容自动生成问题的方法。通过两个对照实验评价LLM生成与人工预设问题的质量，并进一步考察引导LLM规避常见错误时的效果。

Result: 两项实验均显示，LLM生成问题在清晰度、相关性和信息量上与人工问题不相上下。当通过常见错误类型引导时，LLM生成的问题优于人工撰写的问题。

Conclusion: LLM在需求访谈中，能实时辅助主持人，提高提问质量和访谈效率，具有实际应用潜力。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: SAT sweeping传统上用于比特级硬件验证，难以适应现代字级操作。本文提出首个面向字级SMT的sweeping方法SMT-Sweep，大幅提升硬件验证效率，且已开源。


<details>
  <summary>Details</summary>
Motivation: 传统的SAT sweeping技术在硬件验证中主要用于比特级别的逻辑简化和等价检查，但随着硬件设计越来越多采用字级（word-level）构造如位向量操作、算术和数组，缺乏对应的高效字级sweeping技术。

Method: 提出SMT-Sweep方法，将SAT sweeping的思想拓展到字级，基于SMT（Satisfiability Modulo Theories）理论。该方法利用模拟和等价检测技术，设计了适用于包含丰富位向量和数组语义的SMT项的sweeping流程，并结合了随机和约束驱动的字级仿真。

Result: SMT-Sweep在实验中，相较于最先进的比特级SAT sweeping和字级一体化SMT求解，平均速度分别提升约44倍和69倍。

Conclusion: SMT-Sweep是首个将sweeping技术应用于SMT驱动硬件验证的方案，在支持更复杂硬件设计的同时带来了显著的性能提升。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [19] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 本文提出一种新方法，可将包含实数连续可微函数及其属性的无量词可满足性问题，转化为实数初等代数的可满足性问题，并可通过Tarski判定法解决，大大拓展了自动判定的可处理范围。


<details>
  <summary>Details</summary>
Motivation: 目标在于扩展现有对仅含实数的无量词语言的可满足性判定，涵盖包含连续可微函数及其性质的表达式，解决对应函数间关系和区间性质在决策过程中如何处理的问题。

Method: 先对公式进行预处理，转化为等价可满足（equisatisfiable）的实数初等代数无量词公式，无需直接涉及函数变量，而是用一组实变量替代函数变量；最终利用Tarski判定法处理。方法关键在于，构造一种足够灵活的$C^1$插值函数族，以保证若目标公式可满足，则原公式亦有模型。

Result: 在构造的语言片段下，实现了将有关函数、本性及导数的无量词可满足性问题，规范高效地转化为传统实数代数框架问题，使得复杂函数关系的自动判定成为可能。

Conclusion: 提出了一种将包含一阶可导实数函数与算子表达的无量词公式，归约为Tarski初等实数代数无量词公式的新方法，为带有复杂函数性质表达的可满足性判定问题提供有效途径。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [20] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 本文提出了两套直觉主义条件逻辑系统及其推理系统，并推广到支持“might”条件算子的逻辑，实现了模型化与公理化，为直觉主义条件推理提供了理论工具。


<details>
  <summary>Details</summary>
Motivation: 现有条件逻辑的语义和证据证明主要以经典逻辑为基础，而本文旨在为构成主义和直觉主义的条件推理建立合适的逻辑框架，特别关注台词 'would' 与 'might' 条件算子的不可互定义性。

Method: 文章基于已知的CK条件逻辑和直觉主义模态逻辑，提出并开发了嵌套演算（用于IntCK）和序列表演算（用于CCKbox），并进而推广定义了包含might算子的CCK逻辑，为其设计了模型和公理系统。

Result: 建立了IntCK与CCKbox两种直觉主义条件逻辑的证明系统，并扩展到了包含might算子的CCK逻辑，提出了其模型、完整性和可扩展的公理系统。

Conclusion: 本文提出了一套新的直觉主义条件逻辑系统及其推理方法，并为其提供了模型与公理化，为进一步分析直觉主义条件推理奠定了基础。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


### [21] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: 本文将难以在决断型系统中实现的狭义类型和商类型优雅地集成入依赖类型高阶逻辑DHOL，并保持了与HOL的自动化证明兼容，拓展了表达力且理论健全。


<details>
  <summary>Details</summary>
Motivation: 提升形式化系统的表达能力，并满足实践者对狭义类型（refinement types）和商类型（quotient types）的迫切需求，这些特性很难在传统决断型类型系统中支持。

Method: 基于依赖类型高阶逻辑（DHOL）的设计基础，将refinement types和quotient types作为子类型的特例引入，调整其映射方式以减少实现复杂度，并从语法、语义及其到HOL的翻译进行系统性扩展及论证。

Result: 成功将refinement types与quotient types优雅且简洁地集成进DHOL系统，并保持与HOL间翻译的健全性与完备性；证明了系统的理论正确性。

Conclusion: 在牺牲类型判定性的前提下，DHOL系统能够高效地支持更为丰富的类型结构（包括refinement、quotient类型），兼顾表达能力与定理证明自动化的支持。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文针对当前LLMs偏见评测在中文领域的不足，提出了多任务中文偏见评测基准（McBE），实现了偏见类型、覆盖内容和评测任务的多维扩展，并通过实验揭示主流LLMs在中文场景下存在的偏见现象和差异，为后续研究提供了基础和新思路。


<details>
  <summary>Details</summary>
Motivation: 现有关于大型语言模型偏见的评测数据多集中于英语以及北美文化，偏见类别有限，且与中文及中国文化背景的偏见差异较大。而中文及其文化背景下的偏见评测数据集稀缺，且大多只支持单一任务、不能从多个方面评估LLMs偏见水平。因此亟需一个多类别、多任务、适用于中文语境的偏见评测基准。

Method: 作者提出了McBE（多任务中文偏见评测基准）。该基准包含4077条评测实例，覆盖12类（涉及82个子类）偏见，并引入5种评测任务，能够多角度、多层次地评估LLMs的偏见。同时，作者用McBE对不同系列和参数规模的主流LLMs进行了评测和对比分析。

Result: McBE构建了丰富的偏见类别和多样化评测内容，使LLMs的偏见可以被更全面、细致地度量。多款主流LLMs在McBE评测下表现出差异化的偏见水平，实验展示了LLMs在中文环境中的偏见现状和问题。

Conclusion: 所有受评的大型语言模型（LLMs）均表现出不同程度的偏见。作者通过深入分析实验结果，对LLMs中的偏见问题提出了新的见解。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [23] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本研究系统比较了推理与非推理型LLMs在多范式对话摘要中的表现，发现显式链式推理并不总能改善质量，反而易带来冗长和不一致问题，强调了推理模型在对话摘要应用中的局限性及亟需有针对性的改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在摘要任务上取得了显著进展，但针对对话摘要中需要同时进行抽象和简洁表达的场景，长期链式思维（CoT）推理架构在该领域的性能尚未被系统性研究。作者希望明确这些推理模型在不同对话摘要范式下的实际表现。

Method: 本文对最先进的推理型与非推理型LLMs在通用、角色导向型和查询导向型三大对话摘要范式进行了首次全面、系统的评估。评测涵盖多语言、多领域、多摘要长度，采用了SAMSum、DialogSum、CSDS和QMSum等强基准，并结合了基于LLM的自动指标和人类灵感标准的先进评估方法。

Result: 结果显示，与其它需要深度推理的任务不同，显式链式推理不一定提升对话摘要质量。推理型LLMs更易产生冗长、事实不一致和不够简洁的摘要，反而在许多场景下表现不如非推理模型。

Conclusion: 当前推理型LLMs在复杂对话摘要中的优势有限，甚至可能适得其反。需要针对实际对话摘要场景进行细致建模与评测策略设计，以弥补推理LLMs的不足。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [24] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 本文系统分析了可复用层Transformer模型内隐式推理涌现的现状，发现其可解释性不足且性能有限，远不及显式链式思维方法。


<details>
  <summary>Details</summary>
Motivation: 随着链式思维（CoT）方法提升Transformer语言模型在复杂数学和多步推理任务上的表现，越来越多研究尝试将推理过程内化至模型隐空间（latent space）来提升效率。然而，目前相关模型（如仅解码架构）推理步骤多以自然语言显式呈现，牺牲效率。本文针对Huginn-3.5B（一种推理时可复用层的depth-recurrent Transformer）研究其是否能够涌现出隐式CoT推理结构。

Method: 作者使用算术任务对模型内部行为进行分析，具体应用了Logit Lens与Coda Lens等探针分析方法，追踪最终及中间结果token的秩变化。同时，评估层复用（recurrence depth）对模型表现的影响。

Result: 实验发现，仅有有限证据表明模型内部存在可解释的隐式CoT推理结构，不同复用层的可解释性高度依赖于层索引与解码方式。此外，复用深度的提升仅带来微弱性能提升，表现显著落后于明确外显推理步骤的模型。

Conclusion: 当前复用层的深度递归Transformer（如Huginn-3.5B）在隐空间内未能有效涌现出高级的可解释推理结构，其性能与解释能力仍然不及采用显式外化推理步骤的模型。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [25] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 本文提出并开源了GDC Cohort Copilot工具，允许用户用自然语言便捷创建癌症基因组队列。工具基于本地大语言模型，在实际效果上优于GPT-4o，极大提升了数据访问与精细化分析的便利性。


<details>
  <summary>Details</summary>
Motivation: GDC提供了优质的癌症基因组数据，但由于数据字段和属性众多，用户（尤其是新手）在创建特定队列时查找相关描述比较困难。然而，用户往往更擅长用自然语言自由描述所需队列。

Method: 提出并实现了GDC Cohort Copilot，这是一款基于自然语言输入自动生成GDC队列筛选条件的开源工具，支持交互式界面进一步精细化队列。针对该应用开发和评估了多种大语言模型（LLMs），包括本地开源模型。

Result: 所提出的本地开源GDC Cohort LLM在生成GDC队列时表现优于GPT-4o。用户可以通过自然语言描述轻松创建和调整队列。相关工具与模型均已开源并可获取。

Conclusion: GDC Cohort Copilot显著降低了癌症基因组学数据队列构建的门槛，提高了数据可访问性和利用效率。其本地开源模型效果优异，适用于实际科研需求。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [26] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 本文提出的MemAgent架构，通过创新性内存管理和高效算法，有效应对了超长文本处理的性能和效率难题，在多个超长文本测试上实现了近乎无损的外推表现。


<details>
  <summary>Details</summary>
Motivation: 长文本处理中，虽然现有方法通过序列外推进行了提升，但在保持性能不下降的前提下，以线性复杂度处理无限长文档仍然是重大挑战。

Method: 文中提出MemAgent，通过段落级阅读和覆盖式内存更新策略，直接以端到端方式优化长文本任务，并扩展DAPO算法以提升独立上下文多对话生成的训练。

Result: MemAgent在长文本任务中展现出卓越能力，从8K context训练的32K文本外推到350万的QA任务时性能损失小于5%，在512K的RULER测试中准确率超过95%。

Conclusion: MemAgent为长文本处理带来了可扩展性强、性能优异的新方案，在超长文本上实现了线性复杂度和小性能损失。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [27] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 本文提出了DoMIX方法，利用LoRA模块高效地实现了对多个领域数据的持续自适应预训练，显著降低了资源消耗，并为每一具体任务生成更合适的模型。


<details>
  <summary>Details</summary>
Motivation: 现在流行的领域自适应预训练（DAP）在微调已有预训练模型时表现突出，但在持续领域自适应预训练（continual DAP）中，现有方法存在计算开销大、对数据增量顺序敏感，以及只能生成通用模型无法针对具体任务优化等问题。

Method: 提出了DoMIX方法，引入了LoRA模块作为参数高效微调（PEFT）的代表，实现了低计算和内存消耗的高效、并行领域自适应预训练。同时，使模型对数据域顺序鲁棒，并能根据具体任务提供定制化的预训练模型。该方法还能推广应用到标准大模型（LLM）的微调场景。

Result: DoMIX能高效地实现并行、灵活且低资源消耗的领域自适应预训练，不仅解决了领域顺序敏感和资源消耗高的问题，还能为每个具体任务提供更合适的模型。

Conclusion: DoMIX是一种高效、可扩展且能灵活应对多领域连续预训练需求的新方法，显著提升了现有持续DAP方案的效率与实用性。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [28] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本论文提出的多模态大语言模型集成系统，在SciVQA 2025科学视觉问答挑战赛中取得第三名，验证了few-shot检索和置信度加权的有效性。


<details>
  <summary>Details</summary>
Motivation: 科学视觉问答（SciVQA）任务需要理解并结合图像和文本信息，现有方法在不同图表类型和问题类型上的表现存在差异，且模型置信度利用不足。作者希望提升模型的鲁棒性和准确率。

Method: 提出采用两种多模态大语言模型组成的集成系统，并结合多样的few-shot示例检索策略。根据图表和问题类型动态选择模型及few-shot配置，通过模型置信度加权选择最终答案。

Result: 在SciVQA 2025挑战赛的盲测数据上，系统在七支队伍中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数达到85.12。

Conclusion: 集成多模态模型和多样化few-shot策略能显著提升科学视觉问答系统的表现，依据图表和问题类型自适应调整配置，且置信度加权有助于提升最终答案的正确率。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [29] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 将BERT的FFN部分用量子电路替换后，模型在某些任务上更高效且参数更少，特别适用于少样本学习。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer结构中的前馈网络（FFN）模块占据了大部分参数，存在模型参数量庞大但表达能力仍有限的问题。本文旨在通过用参数化量子电路（PQC）取代FFN部分，以提升神经网络表达能力并减少参数量。

Method: 作者提出QFFN-BERT，将紧凑型BERT的FFN模块替换为PQC层。该PQC层采用残差连接、$R_Y$与$R_Z$旋转门，以及交替纠缠策略，系统性探索PQC深度、表达性与可训练性间的权衡。实验证明了模型在SST-2和DBpedia基准上的表现，并通过消融实验验证了设计要素的重要性。

Result: 1. 精心设计的QFFN-BERT在标准数据集上可达或略超原BERT精度（最高可达102.0%），且FFN参数量减少99%以上。2. 在少样本下表现出更强的数据效率和对比优势。3. 消融实验显示，PQC设计对学习能力至关重要。

Conclusion: 当PQC与深度学习基本结构协同设计时，可作为传统FFN的有力、参数高效替代方案。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [30] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 通过高质量数据选择显著提升代码大模型的性能和训练效率，只需1/9数据量即可超越全量训练结果。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在代码生成和理解方面取得了进步，但主要依靠大量数据提升性能，忽视了数据质量，导致训练效率降低。

Method: 提出了一种基于参数模型的代码数据选择方法，通过优化参数模型，确保所选子集的数据分布一致性和多样性，从而保证数据的高质量。

Result: 在只使用1万个样本的情况下，该方法在HumanEval和MBPP榜单上的表现分别比9.2万个全体样本提升2.4%和2.3%，优于其它采样方法，在性能和效率上更佳。

Conclusion: 该方法能够在大幅降低计算成本的同时，有效提升模型性能，显示了数据选择和质量在代码大模型训练中的重要性。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [31] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 按现有方法训练的Akan语音识别模型在非训练领域表现明显下降，Whisper与Wav2Vec2各有优缺点，提示要提升低资源语言ASR泛化能力需采用领域自适应与多语种训练方案。


<details>
  <summary>Details</summary>
Motivation: 目前多数自动语音识别(ASR)研究只在同一领域数据集上评估模型，很少关注模型在不同语境中的泛化能力，尤其是低资源语言如Akan。该研究旨在弥补这一评估不足。

Method: 该研究使用基于transformer结构（如Whisper和Wav2Vec2）的七个Akan ASR模型，在包含四种不同领域（文化相关图片描述、非正式对话、圣经经文朗读、金融对话）的语料库上进行对比评估。通过比较字错误率（CER）和词错误率（WER）分析跨领域表现及误差类型。

Result: 模型在训练领域内表现最佳，但在领域不匹配时识别准确率大幅下降。Whisper模型的转录更流畅但有时误导性较强，Wav2Vec2在遇到陌生输入时输出更明显但难以解读。不同体系结构存在可读性与透明性权衡。

Conclusion: ASR在低资源语言中领域适应性差，需要发展针对特定领域的适应性策略、多语种训练框架等方法，以提升模型的跨领域泛化能力。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [32] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本文提出社区驱动的数据采集和模型构建方案，首次公开Akan语语音障碍者数据集及相关资源，并验证该方法提升了自适应ASR模型的效果，促进了包容性语音识别技术的发展。


<details>
  <summary>Details</summary>
Motivation: 目前自动语音识别（ASR）技术很难覆盖语音障碍者，尤其是在资源匮乏的语言环境下，社区缺乏数据采集和模型构建的有效方法。

Method: 提出采用社区驱动的方式采集语音障碍者的语音数据，并开发了一份最佳实践手册（cookbook），指导数据收集和ASR模型的构建。以加纳Akan语为例，收集并公开了首个语音障碍开源数据集，同时利用该数据集微调现有的开源ASR模型。

Result: 成功建立了Akan语语音障碍者的首个公开数据集及工具，并公开了cookbook。初步实验表明，微调后的ASR模型对语音障碍的Akan语有更好的识别效果。

Conclusion: 通过社区参与和开源实践，可以显著推动低资源语言和语音障碍群体的ASR技术发展，为包容性语音技术建设奠定基础。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [33] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 作者构建了一个涵盖1200份印度保释裁决的新数据集IndianBailJudgments-1200，通过GPT-4o自动标注，多属性标签，助力法律NLP任务，是印度该领域首个公开数据集。


<details>
  <summary>Details</summary>
Motivation: 法学自然语言处理（NLP）在印度等地区发展缓慢，主要原因是缺乏结构化的数据集。

Method: 提出并构建了名为IndianBailJudgments-1200的数据集，该数据包含1200份关于印度保释裁决的法院判决书，通过GPT-4o提示工程生成并经过一致性核查，涵盖20多个属性标签。

Result: 该数据集可支持多种法律NLP任务，包括判决结果预测、内容摘要和公平性分析。是首个专注于印度保释法理、并公开发布的数据集。

Conclusion: 本文为法律NLP在印度的发展奠定了基础，为领域相关任务提供了重要资源。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [34] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor通过创新训练流程和RL算法，有效缩小了开源与闭源模型在高复杂信息检索任务中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前开源大模型在极度复杂的信息检索任务上远不如部分闭源的代理系统（如DeepResearch），这些系统展现出超越人类的能力，开源模型缺乏系统性减少极大不确定性的推理能力。

Method: 提出了WebSailor，一种包含后训练流程、通过结构化采样与信息模糊化生成高不确定性任务、RFT冷启动和高效agentic RL算法（DUPO: Duplicating Sampling Policy Optimization）的完整方法。

Result: WebSailor在复杂信息检索任务上显著超越所有开源代理，表现媲美闭源代理系统，大幅缩小了能力差距。

Conclusion: 通过WebSailor方法，开源大模型代理在系统性减少不确定性和复杂信息搜寻任务上能与领先的闭源方案竞争。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [35] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文综述标签多样性和主动学习的现有问题，提出以人类标签差异为核心的新主动学习理论框架，并探讨LLM辅助标注。


<details>
  <summary>Details</summary>
Motivation: 现实数据中同一实例存在多样的有效标注，且人工标注往往体现多种正确观点，然而现有监督学习和主动学习实践大多假设唯一标准真值，忽视人类标签差异（HLV），导致信息浪费与训练偏差。

Method: 首先综述主动学习（AL）和标签变异（LV/HLV）研究领域对真值与标签本质假设的处理及其不足。随后，提出将HLV纳入AL流程的概念性框架，涉及样本选择、标注者选择及标签表达。此外，讨论了大语言模型（LLM）作为标注者的可能性与挑战。

Result: 文中并未报告具体实验结果，而是系统梳理和审视了HLV在主动学习场景中的作用与挑战，并提出理论框架，指导后续方法与实践，对复杂现实注释提供新方向和基础。

Conclusion: 本文提出了一种关注并融合人类标签差异（HLV）的主动学习（AL）新框架，强调理解标签变异性不仅是噪声，也是信息，呼吁社区在数据收集和模型训练过程中更好地体现真实标签的复杂性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [36] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF是一种新颖可扩展的LLM偏见对齐方法，通过多视角分解和采样加权，显著提升了对齐与泛化能力，无需复杂微调。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）中存在偏见问题，需要一种简单、有效的偏见缓解方法。现有方法在解释性、可扩展性上存在局限，因此作者提出新方法以满足实际需求。

Method: 提出了Multiperspective Fusion（MPF）对齐框架，基于SAGED流程，将人类基准分解为多视角成分，通过采样、平衡输出分布来引导生成，按分解获得的概率加权响应，实现更加人性化、可解释的偏见对齐。

Result: MPF 能有效将LLM输出的情感分布与反事实基线（绝对平等）及HR基线（有高校偏见）对齐，KL散度较低，预测校准误差减小，对新问题有较好泛化能力。

Conclusion: MPF是一种可扩展、可解释、不需大规模微调或提示工程、适用于已部署LLM的偏见对齐和缓解方法。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [37] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文提出新数据集和解释性分析框架，系统揭示性别偏见除职业外体现在多类语境，并在多数据集上验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 当前对性别与语境偏见的相互作用研究不足，尤其是针对动作动词、名词及职业等多重元素，缺乏可解释性强的分析工具和数据集。

Method: 作者提出了一套能估算上下文偏见及其相关性别偏见的分析框架，并开发了一个新数据集GenderLexicon，对五个多样化数据集（含日语数据集）进行验证。偏见可以量化为分数，提高了可解释性。

Result: 新数据集和框架能有效估测并解释语境及相关性别偏见，实验在五个数据集上验证了方法的适用性，发现性别偏见广泛存在。

Conclusion: 本文证实了性别偏见不仅体现在职业刻板印象上，还广泛存在于其他上下文因素中。提出的方法和数据集有效评估和解释了这些性别偏见。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [38] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本研究通过提出一套局限性类型分类法和全新的LimitGen数据集，并结合文献检索增强技术，显著提升了大模型协助识别和生成论文局限性的能力，有望进一步减轻和优化科学论文同行评审流程。


<details>
  <summary>Details</summary>
Motivation: 随着论文数量剧增，同行评审工作变得愈发繁重且需要专业知识。人工智能大模型（LLMs）在科学任务中的应用潜力显现，但在协助同行评审、特别是识别论文局限性方面尚缺乏深入研究。

Method: 作者提出了科学研究中（尤其是AI领域）论文局限性类型的详细分类法，并建立了LimitGen基准数据集，包括通过高质量论文扰动产生的合成数据（LimitGen-Syn）和真实人工编写的局限性数据（LimitGen-Human）。同时，方法还结合了文献检索以增强LLMs识别局限性的能力。

Result: 增强了LLM系统在科学论文中发现和生成局限性的能力，更有效地为早期反馈和同行评审提供了具体且建设性的建议。

Conclusion: 提出的LimitGen数据集和结合文献检索的增强方法，提升了LLMs在协助同行评审、识别和生成科学论文局限性方面的实用性和贡献，为未来这一领域的发展打下了基础。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [39] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究量化了人类在发音时能区分元音的最小听觉距离（JPD），为理解语音控制精度和元音系统结构提供了数据基础。


<details>
  <summary>Details</summary>
Motivation: 此前研究表明元音发音的复杂协调动作部分由以听觉空间为目标的控制机制主导，但尚不清楚这一控制的准确度。本文旨在探究人类模仿元音时，在听觉空间中需多大距离才能区分出不同模仿。

Method: 采用元音模仿范式，对两组英语说话人在前元音发音时，测量两个元音刺激在听觉空间（F1 x F2）中的最小可产生差异（JPD）。

Result: JPD估计在F1xF2空间中为14至51 mels。

Conclusion: 结果为语音产生的表征理论提供支持，并为元音系统的结构提供下限推测，有助于解释元音音位数目及分布的观察趋势。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [40] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 本文揭示了LLM在自我纠错上的盲点问题，提出bench系统并实验证明，大多数模型未能有效纠正自身错误，但通过简单干预可大幅提升能力，为未来模型改进提供思路。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）表现强大，但它们仍会犯错并走入低效的推理路径。提升LLM的自我纠错能力对其可信度至关重要。本文旨在系统性探讨LLM在自我纠错上的局限。

Method: 提出Self-Correction Bench，通过在不同复杂性下注入控制错误，系统性地评估LLM的自我纠错盲点现象；测试了14种模型，并分析其表现。还通过对训练数据与策略差异进行追踪比较，提出干预实验（如添加'Wait'指令）。

Result: 大多数LLM在面对自身输出的错误时，平均表现出64.5%的“自我纠错盲点”率。其原因与训练阶段人类演示通常只展示无错或正确信息，而RL模型则通过结果反馈能学到纠错。简单添加‘Wait’指令可显著（89.3%）减少盲点。

Conclusion: 当前LLM存在严重的自我纠错盲点，这限制了其可靠性。通过调节训练数据和生成指令，可以显著激发其已有潜力、提升自我纠错能力和可信度。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [41] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本研究发现，推理能力并未增强语言模型对社会偏见的抵抗力，反而带来新的偏见风险，应在未来的推理设计中加以重视和改进。


<details>
  <summary>Details</summary>
Motivation: 当前的推理语言模型（RLMs）在多步推理任务上表现出色，但其对社会偏见的鲁棒性尚不明确。本文希望探究推理能力对模型公平性和健壮性的实际影响，回应对RLM社会风险的关注。

Method: 作者利用CLEAR-Bias基准体系，系统性评估了多种先进RLM在多元社会文化维度下的安全性。通过“用大模型判官”方式实现自动化评分，并使用‘jailbreak’技术测试模型安全防护机制的强度。重点比较了两类模型——推理能力是通过微调获得还是仅用推理提示（CoT）引入——在面对偏见诱导时的表现。

Result: 评测发现，不论是通过推理提示（CoT）还是微调推理轨迹获得推理能力的模型，在激发偏见方面普遍比未显式推理的基础模型更脆弱。推理设计反而为刻板印象强化开辟了新途径。具备推理能力的模型略优于单用CoT提示者，后者尤其容易被情境重构（如虚构角色、故事提示或奖励型指令）攻破。

Conclusion: 显式推理并不必然带来更好的公平性和鲁棒性，相反可能增加偏见风险。呼吁在推理机制设计时引入更多偏见感知考量。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [42] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 通过构建多解题路径的数据集和创新强化学习方法，Qwen-VL-DP显著提升了多模态数学推理的准确与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLMs在数学推理通常只利用一对一图文对和单一解答，不足以反映多样的推理路径和内在反思，限制了模型泛化与理解能力。亟需引入多样化的解题方式作为监督。

Method: 提出并构建了MathV-DP数据集，包含每个图像-问题对的多种解题轨迹。从Qwen-VL基础上出发，通过监督学习微调，再用群体相对策略优化（GRPO）结合正确性判别与多样性奖励，以提升模型的推理多样性和准确性。

Result: Qwen-VL-DP在MathVista的minitest和Math-V基准测试中精准性和生成多样性均显著优于以往基础MLLMs。

Conclusion: Qwen-VL-DP模型在多模态数学推理任务上取得了比以往MLLMs更高的准确率和生成多样性，强调了多视角和反思性推理对提升模型能力的重要性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [43] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 提出了SynapseRoute动态路由框架，在医学问答中通过智能分流显著提升效率和准确率，同时降低成本，并提出综合评估模型权衡的新指标。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）在实际应用中普及，选择合适的模型需权衡性能和运营成本。尤其是在医学等高需求领域，复杂推理模型与简单快速模型的成本差距越来越大，动机在于通过更高效的路由机制，优化模型的推理资源分配和整体体验。

Method: 本文提出了一种名为SynapseRoute的机器学习动态路由框架。该框架能够根据问题复杂度智能分配输入查询到高推理模式或低推理（非推理）模式。实验在多个医学数据集上评估，并提出综合权衡准确率、延迟和Token消耗的AIT新指标。

Result: SynapseRoute框架在医学领域测试中表现优异。相比仅使用高推理模式，SynapseRoute整体准确率提升，从0.8272增长到0.8390，推理时间减少36.8%，Token消耗降低39.66%。定性分析显示，对于简单问题，过度推理反而会使延迟和准确率变差，而动态适配有效避免了这些问题。

Conclusion: 动态路由策略能有效兼顾推理成本和模型性能，提升用户体验，避免简单问题的资源浪费与准确率降低。新提出的AIT指标能够更全面评估模型选择带来的多维收益。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [44] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出IFBench新基准，并将强化学习与可验证奖励结合，显著改善了模型对人为精确指令的泛化遵循能力，并公开数据和工具以促进行业发展。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在精确遵循人类指令，特别是具有限定输出约束时，仍表现不佳，因此需要开发更好的评测指标和训练方法以促进模型在这一能力上的提升。

Method: 提出了全新的评测基准IFBench，涵盖58种多样且具有挑战性的全新可验证约束，用于评估模型在精确遵循指令方面的泛化能力。同时设计了约束验证模块，并采用基于可验证奖励的强化学习（RLVR）进行训练。公开了29项全新手工标注的训练约束与验证函数、RLVR训练提示和代码。

Result: 现有模型在小型可验证约束集上过拟合，未能很好泛化至新的、复杂的输出约束。引入的RLVR训练策略显著提升了模型在精确指令遵循能力上的泛化表现。

Conclusion: 通过提出IFBench基准，设计验证模块并引入RLVR方法，为提升大语言模型在精确指令遵循及其泛化能力上提供了有效评测工具和训练方案，为该领域后续研究奠定了基础。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [45] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文发现，基于用户反馈微调的大语言模型可被单一用户通过点赞/点踩方式持续污染，无需特殊权限即可改变模型知识和行为，最终带来虚假知识注入、代码安全性下降等风险。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索和揭示训练依赖用户反馈的大语言模型的安全风险，尤其是用户如何利用简单反馈（点赞、点踩）方式对模型长期性能产生不可预期乃至恶意影响。

Method: 攻击者通过反复向语言模型输入特定提示，让模型有概率输出“污染”或正常回复，并分别为有害回复点赞或对正常回复点踩。在之后的偏好微调阶段，模型根据这些反馈信号，逐渐增加输出攻击者期望内容的概率。

Result: 本方法可用于：1）注入模型原本不具备的虚假知识；2）改变代码生成方式，植入安全漏洞；3）制造虚假金融新闻。这揭示了偏好微调带来的新安全隐患。

Conclusion: 本研究揭示了基于用户反馈训练的大语言模型中存在的新型脆弱性：单个用户可利用反馈机制，持续影响模型知识和行为。即使在极度受限的用户反馈形式下，模型依然可被精准操控，导致知识、行为偏向攻击者的目标。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [46] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 提出了MOTIF多轮思考强化学习微调方法，有效提升了大语言模型在复杂推理任务中的能力，改善了准确率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在推理能力上取得了进展，但受到上下文长度的限制，导致模型无法在需要大量“思考”token时持续关注已生成内容，影响复杂推理。为突破这一瓶颈，需要设计能分多轮进行推理的策略。

Method: 提出了一种新的强化学习微调方法MOTIF（Modular Thinking via Reinforcement Finetuning），使模型能够分多轮生成推理token，扩展模型的有效上下文长度。该方法在Qwen2.5-3B-Instruct模型上进行了参数高效微调，并在GSM8K数据集上训练。

Result: 在MATH500和AIME2024两个推理测试集上，MOTIF方法比原始GRPO训练分别提高了3.8%和3.3%的准确率，且仅用15%的样本就获得了提升，显示出较强的样本效率。

Conclusion: MOTIF能突破大语言模型上下文长度的推理限制，通过多轮模块化思考来提升推理能力，在提升准确率和样本效率方面相较现有方法表现更优。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [47] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 选择题评测会被模型投机取巧，影响准确性。答案匹配方法让模型生成自由答案，用大模型判断是否正确，与人工评分高度一致。建议今后以答案匹配为主导评测办法。


<details>
  <summary>Details</summary>
Motivation: 目前主流的语言模型评测多依赖选择题，这种方式虽然方便客观，但实际评测中发现模型往往不需理解题目即可根据选项套路获得高分，暴露了基于判别式评测的根本局限性，而自由生成式答案的评测又缺乏可扩展的自动化方法。该论文动机在于寻找选择题之外的、优良的自动化评测替代方案。

Method: 提出“答案匹配（answer matching）”的生成式评测方法：不给选项，让模型自由生成回答，然后借助现代大语言模型判断生成答案是否与参考标准答案匹配。为比较不同评测方式的有效性，作者在人类真实评分数据（MMLU-Pro与GPQA-Diamond）上，衡量不同自动评测方式与人工评分的一致性。

Result: 答案匹配法在与人工评分一致性上几乎达到人工标注者间的一致性水平，甚至小型模型也能实现高一致性；而多项选择题评分以及让LLM直接打分（却不给标准答案）与人类评分的符合度较低。此外，采用答案匹配后，多模型排名发生显著变化，说明评测方式会直接影响模型性能的判定。

Conclusion: 当前选择题评测虽然方便，但可靠性和准确性都有限，答案匹配为LLM带来了更接近人工判断的评测方法，对评测生态有重大提升。作者建议逐步以答案匹配替代传统选择题评测。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [48] [On Obtaining New MUBs by Finding Points on Complete Intersection Varieties over $\mathbb{R}$](https://arxiv.org/abs/2507.02492)
*Arindam Banerjee,Kanoy Kumar Das,Ajeet Kumar,Rakesh Kumar,Subhamoy Maitra*

Main category: cs.DM

TL;DR: 本文提出了研究 MUBs 可扩展性的等价标准，并首次将其与极大可交换的正交正规矩阵基建立一一对应，有助于更好地理解 MUBs 的代数与物理结构。


<details>
  <summary>Details</summary>
Motivation: MUBs 在量子物理中非常重要，但其结构和可扩展性仍待深入理解，尤其是在代数结构与其联系方面。

Method: 通过研究某一仿射代数簇的实点，以及分析正交正规矩阵的可交换类，建立等价条件。

Result: 提出了 MUBs 可扩展性的等价条件，指出相关代数簇的部分点是完整交点域，并证明了 MUBs 与正交正规矩阵极大可交换基的一一对应关系。

Conclusion: MUBs 与正交正规矩阵的极大可交换基一一对应，极大可交换基的存在保证了完整的 MUB 集。

Abstract: Mutually Unbiased Bases (MUBs) are closely connected with quantum physics,
and the structure has a rich mathematical background. We provide equivalent
criteria for extending a set of MUBs for $C^n$ by studying real points of a
certain affine algebraic variety. This variety comes from the relations that
determine the extendability of a system of MUBs. Finally, we show that some
part of this variety gives rise to complete intersection domains. Further, we
show that there is a one-to-one correspondence between MUBs and the maximal
commuting classes (bases) of orthogonal normal matrices in $\mathcal
M_n({\mathbb{C}})$. It means that for $m$ MUBs in $C^n$, there are $m$
commuting classes, each consisting of $n$ commuting orthogonal normal matrices
and the existence of maximal commuting basis for $\mathcal M_n({\mathbb{C}})$
ensures the complete set of MUBs in $\mathcal M_n({\mathbb{C}})$.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [49] [Engineering an LTLf Synthesis Tool](https://arxiv.org/abs/2507.02491)
*Alexandre Duret-Lutz,Shufang Zhu,Nir Piterman,Giuseppe de Giacomo,Moshe Y Vardi*

Main category: cs.FL

TL;DR: 作者提出了一种使用MTBDD优化的LTLf合成方法，并开发的工具在实验中优于已有解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有LTLf实时合成器在执行效率和内存使用方面存在瓶颈，需要更高效的转换和求解机制。

Method: 采用LTLf到DFA的直接转换，并用多终端二叉决策图(MTBDD)表示DFA，通过节点共享降低内存消耗，并以此为基础在构建阶段实时求解可达性博弈。

Result: 新工具在所提供的基准测试集上表现优于目前同类工具，展现出更高的性能。

Conclusion: 提出了一种基于MTBDD的新的LTLf到DFA转换方法，通过共享节点优化了合成器的效率，在基准测试中超越了现有工具。

Abstract: The problem of LTLf reactive synthesis is to build a transducer, whose output
is based on a history of inputs, such that, for every infinite sequence of
inputs, the conjoint evolution of the inputs and outputs has a prefix that
satisfies a given LTLf specification. We describe the implementation of an LTLf
synthesizer that outperforms existing tools on our benchmark suite. This is
based on a new, direct translation from LTLf to a DFA represented as an array
of Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based
representation can be interpreted directly as a reachability game that is
solved on-the-fly during its construction.

</details>
