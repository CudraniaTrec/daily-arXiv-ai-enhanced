<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: LLM用于自动生成RTL代码时传统解码策略失效，作者提出DecoRTL新策略，通过自一致采样和语法感知温度调节，显著提升了代码的有效性和多样性，无需模型微调，开销极小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自动生成寄存器传输级（RTL）代码方面展现出前景，但传统为自然语言设计的解码策略在结构和语义要求严格的RTL代码生成任务中表现不佳，导致生成的代码经常出现虚构、重复或无效的问题。因此，需要研究解码失效的根本原因并提出更适用于RTL代码的解码策略。

Method: 作者首先通过实证分析，研究了LLM在RTL生成过程中Token级别的熵，揭示模型在结构和语义复杂区域的不确定性，说明传统解码无法区分语法关键与需要探索性创造的区域。为此，作者提出了名为DecoRTL的新推理时解码策略，核心包括两点：1）自一致采样，通过生成多个候选并基于Token层一致性进行重排序，提升正确性和多样性；2）语法感知温度自适应，根据Token在语法结构和功能中的作用调整采样温度，实现语法关键Token的确定性和设计关键Token的多样性。

Result: DecoRTL方法完全基于推理阶段，无需额外微调，针对开源LLM在VerilogEval基准上测试，显著提升了语法有效性、功能正确性以及输出多样性，并且几乎没有性能开销。

Conclusion: 面向RTL代码生成，作者提出的DecoRTL通过语法感知和对比式的采样策略，有效解决了传统LLM解码出现的结构及语义错误，明显改善了代码质量且无需改变原始模型。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 本文提出用LLM把自然语言查询转为结构代码搜索语句的方法，在多个Java项目上实现了高准确率和召回，并显著胜过传统语义搜索和LLM直接检索的方式。


<details>
  <summary>Details</summary>
Motivation: 当前结构化代码搜索工具虽然强大，但需要开发者掌握专用领域语言（DSL），学习门槛高。因此，作者期望简化开发者表达结构性代码搜索意图的方式。

Method: 提出了一种结合大语言模型（LLM）自然语言理解能力与结构化代码搜索工具的高效检索能力的通用方法。通过将开发者的自然语言查询转化为DSL查询，实现对Semgrep和GQL两种DSL的结构化代码搜索。并构建了包含400个查询、涵盖10个Java项目的新基准数据集进行评测。

Result: 该方法能够有效、稳健地将自然语言转换为结构代码搜索查询。在评测中达到55%-70%的高精度与召回率，且在F1分数上显著超过语义代码搜索及直接用LLM检索的基线方法，最高提升至57%和14%。

Conclusion: 基于LLM将自然语言转化为结构代码查询的方式能够降低使用门槛，同时实现高效和精确的结构化代码搜索，优于现有基线方法。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 技术面试难以准备，现有课程支持有限，候选人大多感到压力和缺乏准备。作者调查了131名候选人的备考方法，并为各相关方提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管技术面试是在求职过程中必不可少的一环，但高校课程很少涵盖相关内容，导致应聘者缺乏准备。作者希望理解候选人如何为技术面试做准备，探索备考方式和教育的作用。

Method: 作者通过向正在准备技术面试的候选人发放问卷调查（样本量n=131），收集并分析其备考行为和感受。

Result: 调查结果显示，候选人很少在真实情景下训练，而课程也未有效支持技术面试准备，导致候选人感到压力大且准备不足。

Conclusion: 高校课程在技术面试准备方面支持有限，候选人存在明显的压力和准备不足等问题。作者基于调研结果，提出了改善各方（如教育工作者、企业等）支持措施的建议。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [4] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本研究发现，通过源代码及应用相关的内部指标，借助机器学习二分类模型，可以在一定程度上预测应用的流行度（F1=0.72），提示内部代码属性对应用受欢迎度有参考意义。


<details>
  <summary>Details</summary>
Motivation: 在应用发布前预测其流行度对开发者具有战略意义，但如何仅用源代码可测的内部指标进行有效预测尚存挑战，相关研究结论不一。

Method: 收集了F-Droid上的446个开源Android应用，提取了系统、类、方法层次的代码指标、代码异味和应用元数据，并结合Google Play的用户评论、下载量、权限等。分别以回归和二分类方法（多层感知机等）构建预测模型，比较不同特征集（仅体积、人工挑选、算法筛选集合）的表现。

Result: 回归模型表现不佳，但将问题转换为热门/不热门二分类后显著提升，最优模型F1达到0.72。内部代码特征虽非决定性，也具重要参考价值。

Conclusion: 内部代码指标虽然解释能力有限，但可以作为应用流行度的有用预测因子，这与早期认为内部指标无法预测软件质量的观点形成了挑战。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [5] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 本研究比较了心理量表与生理指标在检测编程任务压力时的表现，发现生理和心理测量结果并不一致，仅EDA生理指标有显著变化，时间压力诱导不充分。为相关研究提出了方法建议。


<details>
  <summary>Details</summary>
Motivation: 传统上，对幸福感、压力等人类因素的研究主要依赖自我报告量表，但这些工具可能存在偏差，因此研究者希望结合更客观的方法（如生理指标）进行评估。

Method: 进行实验：被试完成前测问卷，然后在佩戴生物传感器的情况下完成两个编程任务，每个任务后简短问卷，最后简短访谈。主要比较心理测量与生物指标对压力的反映。

Result: 心理测量工具没有检测到压力，访谈中有些被试表示无压力，有些表示有时间压力。生物指标仅在EDA phasic peaks上有显著差异。

Conclusion: 通过缩短时间限制诱导压力的方法不够充分。论文为压力、生物测量与心理学工具结合研究提供了方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [6] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 该研究分析了多个开发平台的数据特性与情感分析工具表现，提出了一套推荐系统，能依据数据集特征帮助选出更合适的情感分析工具，从而提升软件工程领域的文本分析可信度。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具在不同开发者交流平台数据集上表现不一致，主要由于沟通风格与内容的差异，影响了在需求工程等领域可信分析的效果。

Method: 分析了来自五个平台的10个开发者沟通数据集的语言与统计特征，并评估了14种情感分析工具的表现。基于分析结果，提出了一种映射方法与问卷工具，可根据新数据集特征推荐合适的情感分析工具。

Result: 数据集的特征可以提高情感分析工具的选择效果。虽然像SetFit和RoBERTa等transformer模型表现较好，但情感分析工具的效果仍然依赖于具体上下文。该方法有助于在不断变化的沟通环境下，指导研究人员和实践者选择合适且可信的分析工具。

Conclusion: 通过结合数据集特性进行工具推荐，可以提升情感分析的准确性和可信度，但工具选择需持续随沟通语境变化动态评估。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [7] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 本文提出多智能体LLM方法，显著提升COBOL代码多粒度自动解释的准确率和实用性，有助于支持COBOL代码库的维护和新开发者的理解。


<details>
  <summary>Details</summary>
Motivation: COBOL由于其年龄和复杂性，加上开发者逐渐减少，维护变得愈发困难；且缺乏文档，加剧了新开发者理解维护的难度。现有LLM方法难以处理COBOL特有的架构和语法差异，且常因代码体量过大超出token窗口。

Method: 提出了一种多智能体方法，利用两个基于LLM的智能体协同工作，通过将代码库上下文信息融入代码解释提示中，生成对函数、文件及整个项目的解释。

Result: 在14个开源COBOL项目上评估，该方法在函数解释上的METEOR、chrF和SentenceBERT评分分别提升12.67%、18.59%及0.62%；在文件级别上，对短文件和超出token窗口的长文件均能有效解释，且在目的、功能和清晰度方面比基线方法分别提升4.21%、10.72%和14.68%；在项目级别，所生成的解释能准确传达82%的项目功能与目的。

Conclusion: 该多智能体LLM方法有效提升了COBOL代码解释的质量，解决了现有方法在COBOL代码体量和语言结构上的难题，为维护和理解COBOL代码库提供了更优解决方案。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [8] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出类型感知的自动测试生成方法 RTED，实现更低误报、更高检出率，显著优于现有方法并发现新类型错误。


<details>
  <summary>Details</summary>
Motivation: Python 的类型错误通常导致运行时失败，影响软件可靠性和开发效率。现有静态分析工具虽然能在不运行程序的情况下检测类型错误，但误报率高。近年来单元测试生成技术有覆盖率优势，但难以定向发现类型错误。

Method: 提出了一种新颖的、具备类型感知能力的测试生成技术 RTED。其方法融合了逐步类型约束分析和反射式验证，有针对性地引导测试生成过程，从而有效抑制误报。

Result: 在 BugsInPy 和 TypeBugs 两个主流基准上，RTED 比四种最先进的技术多检测出 22-29 个类型错误，误报更少，精度提升 173.9%-245.9%。此外，RTED 还在六个实际开源项目中发现了 12 个此前未知的类型错误。

Conclusion: RTED 有效提升了 Python 类型错误检测的覆盖率和准确率，为开发者提供了更可靠的自动化检测手段。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [9] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 该论文首次提出面向垂直联邦学习推理环节的软件执行正确性审计框架VeFIA，具备高检测率、无隐私泄露、无额外延迟的特点。


<details>
  <summary>Details</summary>
Motivation: 现有的垂直联邦学习（VFL）缺乏对数据方推理软件执行正确性的审计机制，存在推理错误无法被及时发现的问题。该问题影响多方协作中模型推理结果的可信度和安全性。

Method: 设计了一套垂直联邦推理审计（VeFIA）框架。VeFIA通过在受信任执行环境（TEE）和协调方配合下，随机采样并验证推理结果，无需获取数据方隐私数据，也不会引入额外的推理延迟。

Result: VeFIA保证只要异常推理比例超过5.4%，任务方便能以99.99%的概率发现异常，且在线推理无额外延迟。实验证明其对异常推理检测的准确率（PPV、NPV、TPR）均为100%。

Conclusion: VeFIA有效弥补了VFL领域长期存在的推理执行正确性审计空白，实现了高效、无隐私泄露且无性能损耗的推理软件审计。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [10] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 本文提出自动化LLM公平性测试方法Meta-Fair，基于变形测试和LLM驱动用例生成评估。实验检验了12个模型，显示方法有效，平均精度高达92%，为偏见检测自动化提供新思路。


<details>
  <summary>Details</summary>
Motivation: 公平性是人工智能系统开发的核心原则，但当前大语言模型（LLM）的公平性测试方法多依赖人工评估、固定模板和专门数据集，成本高且难以扩展，因此需要新的自动化测试方法，以降低资源依赖并拓宽适用性。

Method: 提出了一种新的自动化公平性测试方法Meta-Fair，基于两大思想：采用变形测试（metamorphic testing），通过对输入提示进行受控修改来考察模型输出的偏差变化；利用LLM自身生成多样化测试用例并自动评估输出，配套开发三套开源工具辅助自动化用例生成、执行与评估。

Result: 对12个预训练LLM、14种变形关系、5个偏见维度和7900个自动生成测试用例的实验表明，Meta-Fair在挖掘LLM偏见方面有效，平均精度92%，29%的执行检测到偏见。LLM本身作为评估者具备较高可靠性，一流模型F1-score最高达0.79。非确定性带来的一致性问题可通过优化变形关系设计缓解。

Conclusion: Meta-Fair方法显著提升了LLM偏见测试的自动化水平，具备良好扩展性和适用性，尽管仍存在一致性挑战，但为实用LLM公平性检测提供了有前景的自动化技术路径。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [11] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文提出的LLMREI聊天机器人能够在需求获取访谈中实现与人类访谈员相当的错误水平，并高效提取需求，尤其适合大量涉众的自动化需求访谈场景。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈对于系统需求的收集至关重要，但高度依赖于分析师的经验，导致成本高、易受主观偏见和沟通失误影响，因此亟需自动化和提升效率的新方法。

Method: 提出了LLMREI聊天机器人，用于自动化需求获取访谈。研究尝试了零样本提示（zero-shot prompting）和由浅入深提示（least-to-most prompting）两种优化方式，并在33场模拟访谈中评估其表现。微调（fine-tuning）方式因预研表现差而被放弃。评估指标包括减少常见访谈错误、提取有效需求以及根据上下文自适应提问能力。

Result: LLMREI在访谈错误数量上与人类访谈者表现相似，能够提取大部分相关需求，并在生成高度依赖上下文的问题上有较好表现。

Conclusion: LLMREI尤其适用于大规模涉众自动访谈，有望提升需求获取的可扩展性与效率，同时降低人为偏差和资源消耗。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [12] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 本文聚焦自适应网络物理系统中的人机团队协作难题，针对反馈集成和伦理隐私保障提出了创新方法和框架，为实现高效安全的人机共融系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前自适应的网络物理系统（CPS）需实现人机高效协作，但人机节奏不同、隐私与人类价值保障等问题阻碍了真正的人机团队（HMT）融合。

Method: 1）设计新方法与流程将人机团队（HMT）理念集成到CPS自适应反馈环（如MAPE-K）中，重在优化人机交互机制；2）提出全生命周期的伦理与人类价值集成、验证与验证框架，从需求工程阶段介入。

Result: 提出了一套新的集成人机团队与自适应反馈环的CPS方法，并开发了保障伦理与人类价值的系统框架。

Conclusion: 有效的人机团队协作在CPS中仍需应对自适应反馈与隐私等挑战，本研究为实现无缝HMT贡献了方法和实践框架。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [13] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 本研究梳理了研究软件工程师与软件工程研究者之间的术语差异，提出了系统性术语映射方法，有助于促进跨领域沟通与合作，并为后续更广泛的众包验证奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程师（RSEs）和软件工程研究者（SERs）虽然领域相关，但常用不同术语描述相似概念，造成沟通障碍。该论文旨在理解两者间基本术语的差异。

Method: 论文采用系统性的方法进行术语映射，分析SE基础概念在RSE社区内的理解，识别对齐点、知识空白和适应空间。

Result: 初步结果表明，RSE和SER之间存在相互学习和合作的机会。同时，作者的方法为未来以众包方式扩展和验证术语映射提供了基础。

Conclusion: RSE和SER之间在基本术语理解上有异同，通过系统化术语映射可促进知识交流和合作，未来工作可基于此进一步扩展和完善。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [14] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: 提出RLHGNN异构图神经网络结合强化学习优化流程建模与预测，在实际数据集上全面优于主流方法，且推理速度快，适合实用。


<details>
  <summary>Details</summary>
Motivation: 现有的业务流程中，服务优化依赖于准确预测下一个活动，但主流序列方法无法捕获流程执行过程中的并发和条件依赖等非顺序关系。即使是图结构方法，也因同质表达和静态结构，对不同复杂度流程建模不足，影响预测精度。

Method: 提出RLHGNN新框架，将事件日志转化为包含三类关系的异构流程图。通过灵活组合这些关系点，生成四种结构以适应不同流程复杂性，用强化学习自动为每个实例选最优图结构，并采用异构图卷积进行关系特定聚合以预测下一步活动。

Result: 在六个真实数据集上，RLHGNN准确率优于现有主流方法。同时，在每次推理1毫秒的延迟下，具备实时业务流程监控的应用价值。

Conclusion: RLHGNN框架有效地解决了流程预测中序列与非序列关系的联合建模问题，显著提升了预测准确率，兼具高效推理速度，适用于实际实时业务场景。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [15] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 本文提出了通过“可持续性标记”来辅助识别云架构讨论帖中的可持续性内容，经实验验证，其在准确性、实用性和可理解性上均优于传统定义方法。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的兴起，软件系统的可持续性越来越受关注。但在实际的软件从业者讨论中，缺乏明确手段识别与可持续性相关的内容，影响架构决策。

Method: 通过对多个云服务商的可持续性最佳实践进行主题分析，提出“可持续性标记”，并通过受控实验评估标记在云架构论坛帖子中的有效性。

Result: 实验显示，使用“可持续性标记”相比对照组，虽然归类为可持续性相关的帖子数较少，但准确性和操作表现显著提升；且实践者普遍认为这种方法更有用、更易理解。

Conclusion: 引入“可持续性标记”（sustainability flags）有助于提升在云架构论坛中识别与可持续性有关内容的效率和准确性，实践者认为这比仅基于定义判断更加有用和易于理解。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [16] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文提出了一种新方法，结合文本蕴含与上下文学习，将法律文本自动化转为结构化、可执行的Python代码，有效提升法规元数据抽取效率且无需大量人工标注，实验结果显示具有较高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 许多小型组织和初创公司缺乏法律专业知识，难以高效从复杂冗长的法规文本中提取软件合规所需的法律要求，现有自动化方法普适性和自动化程度不足。

Method: 采用文本蕴含与in-context learning 的自动方式，将法律文本生成可以编码和执行的Python类结构，实现结构与语义元数据及其关系的表达。

Result: 在13个美国州数据泄露通知法规上实验，所提出的方法在测试用例中通过率约为89.4%，精确率82.2，召回率88.7，显示出良好的适用性和准确性。

Conclusion: 本文提出的方法能够有效自动提取和表示法律文本的结构和语义元数据，可以转化为可执行的Python代码，并且在测试中表现优异，减少了对人工标注数据的需求。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [17] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文探讨了利用GPT-4o辅助需求访谈中自动生成后续问题的方法，并通过实验显示，LLM生成的问题至少与人工问题同等，有时更优，表明LLM可有效提升访谈质量和效率。


<details>
  <summary>Details</summary>
Motivation: 需求访谈在软件需求获取中被广泛使用，但访谈过程中的实时提问对访谈者有较高要求，包括领域不熟悉、认知负荷大和信息过载等问题。作者希望借助大语言模型（LLM）来辅助访谈者生成高质量的后续问题，缓解这些挑战。

Method: 本文应用GPT-4o模型，并结合常见访谈者错误框架，开发生成跟进访谈问题的方法，包括基于访谈对象发言自动生成问题。通过两个对照实验评估：一是LLM与人工编写问题在最小指导下的表现，二是在错误类型指导下的LLM生成问题表现。

Result: 实验结果显示无论在有无错误类型指导下，LLM生成的问题在清晰性、相关性和信息量上均不逊于人工编写问题。而在错误类型指导下，LLM生成问题优于人工编写问题。

Conclusion: LLM如GPT-4o可以实时辅助访谈者生成高质量问题，提升需求访谈的质量和效率，尤其是在针对常见错误进行指导时，改进效果更为显著。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: 本文提出了SMT-Sweep，将传统SAT sweeping提升到支持位向量和数组等字级操作的层面。实验显示其在字级硬件验证中效能显著超越传统技术，是该方向的首次创新探索。


<details>
  <summary>Details</summary>
Motivation: 传统的SAT sweeping技术在比特级逻辑简化与等价性检测中表现突出，但难以处理广泛被采用的字级（word-level）硬件验证构造如位向量运算、算数与数组操作。因此，缺乏对应的字级SAT sweeping方法。

Method: 提出了SMT-Sweep，一种基于可满足性模理论（SMT）的字级SAT sweeping扩展。框架结合了针对SMT术语（涵盖丰富的位向量与数组语义）的模拟与等价检测，支持随机化和基于约束的仿真方法，以应对超出纯布尔逻辑的符号表达式与运算符。

Result: 实验表明，SMT-Sweep在性能上显著优于现有比特级SAT sweeping和单一字级SMT求解技术，平均加速分别达44倍和69倍。

Conclusion: SMT-Sweep首次将sweeping技术拓展至SMT驱动的硬件验证领域，极大提升了字级验证效率，并已开源实现。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [19] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: 本文在DHOL中创新性地支持了精炼类型和商类型，增强了语言表达力，同时保持了与HOL之间的自动化定理证明支持，并保证了拓展完备性与正确性。


<details>
  <summary>Details</summary>
Motivation: 自动化定理证明领域中，实践者常常需要更具表现力的类型系统，尤其希望支持精炼类型和商类型，但这类特性常因类型系统判定性牺牲而难以实现。近年来提出的DHOL为了增强表达能力，牺牲了可判定性，并与HOL之间实现了自动化证明的桥接。作者希望进一步丰富DHOL的类型表达能力，解决现有自动化证明工具难以优雅支持精炼与商类型的问题。

Method: 作者在DHOL现有设计基础上，将精炼类型（refinement types）和商类型（quotient types）作为子类型（subtyping）的特例加入语言中，从而将相关的包含和投影映射简化为恒等映射，极大避免了表示上的巨大变更。同时，提供了扩展后语言的语法、语义和向HOL的自动化可证明翻译方法，并给出了完备性与正确性的理论证明。

Result: 成功将精炼类型和商类型优雅地整合入DHOL，不仅扩展了语言的表达能力，还保持了与HOL之间的自动化定理证明支持。通过恒等映射的设计，有效降低了扩展带来的复杂性，并通过形式化证明确保了扩展后的系统依然具有完备性与正确性。

Conclusion: 在保持现有自动化证明能力的基础上，DHOL通过对子类型的创新利用，实现了对精炼类型与商类型的原生支持。这一扩展不仅回应了实践需求，也为类型理论与自动化证明工具的发展提供了新思路。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


### [20] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 本文提出了一种将涉及可微实函数的逻辑公式归约为Tarski初等代数无量词公式的通用可满足性判定方法，扩展了相关谓词对区间上的性质判定，并证明了该方法的理论正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的可满足性检验方法难以处理含有一阶连续可微函数的无量词语言，特别是在涉及函数单调性、凸性及导数相关比较等性质时，缺乏统一有效的判定工具。

Method: 将涉及实数和连续可微函数变量的公式，经过预处理，翻译成等价的、无量词的Tarski初等代数公式。该翻译消除了公式中的函数变量，通过一组实数变量来替代。利用Tarski判定法检测目标公式可满足性。为确保翻译的保满足性，需构造灵活的$C^1$插值函数族，使得一旦目标公式有解，就能还原出原始公式的解。

Result: 提出了一种基于Tarski判定法的通用决策方法，实现了对带有一阶连续可微函数的初等代数扩展语言进行可满足性判断，拓展了相关谓词的适用范围，包括在（半）开区间上的性质判定。

Conclusion: 该方法能有效将函数相关的复杂可满足性问题归约为纯代数问题，用Tarski方法进行判定，理论上保证了方法的正确性与可实现性，并扩展了以往结果在区间上的应用。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [21] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 本文以直觉主义与建构性框架下，提出了面向条件推理的多个新逻辑系统，包括嵌套演算与序列演算，并扩展了含might算子的条件逻辑及其公理化方法。


<details>
  <summary>Details</summary>
Motivation: 直觉主义条件逻辑旨在为条件推理提供建构性分析。在该框架下，“会(would)”和“可能(might)”两种条件算子不再可互相定义。文献中的直觉主义条件逻辑是在选择函数下的Chellas条件逻辑CK的基础上，结合直觉主义模态逻辑的建构性与直觉主义框架给出。此举意图推动条件逻辑的建构性研究进展。

Method: 作者结合已有的CK逻辑和直觉主义模态逻辑的证明系统，针对IntCK引入嵌套演算，针对CCKbox引入序列演算。以序列演算为基础，定义了包含might算子的保守扩展CCK，提出了CCK的一类模型和公理化，并将结果扩展到CCK的多个扩展体系。

Result: 提出了IntCK的嵌套演算系统和CCKbox的序列演算系统，定义了包含might算子的条件逻辑CCK，并为其给出了模型和公理化方案。这些结果还推广到了多个CCK的扩展版本。

Conclusion: 本文在建构性和直觉主义条件逻辑研究基础上，丰富和拓展了相关逻辑体系，为复杂条件算子的逻辑建模和推理提供了新的技术与理论工具。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文针对中文语境下大语言模型的偏见评估空白，构建了首个多任务、多类别的中文偏见评测基准（McBE），评测表明主流模型普遍存在多维偏见，并为理解和消除模型偏见提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在自然语言处理任务中的广泛应用，其内在偏见逐渐暴露。现有的偏见评估数据集大多聚焦于英语和北美文化，缺乏针对中文及其文化的评测资源，且功能单一，难以多角度衡量偏见问题。

Method: 作者提出了一个多任务中文偏见评测基准（McBE），涵盖4077个评估实例，覆盖12类、82个子类偏见，并引入5种评测任务，从多维度、多角度评估LLMs的偏见。接着，作者利用该基准测试了多种不同系列及参数规模的主流LLMs，并进行深入分析。

Result: 通过对主流LLMs的测试发现，所有模型在不同程度上都存在偏见，并通过具体实验和数据揭示了模型偏见的多样性和表现形式。作者还提供了对偏见问题的新见解。

Conclusion: McBE为衡量和分析中文及相关文化中的大模型偏见提供了全新、全面的工具，有助于推动未来LLMs公平性与伦理风险方向的研究。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [23] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 首次系统评估推理型与非推理型LLM在多场景对话摘要上的表现，发现显式推理未必带来更优摘要，甚至因冗长与不一致而表现不佳，呼吁开发更针对现实需求的对话摘要模型与评估体系。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在文本摘要领域进展迅速，但“长链式推理”架构在对话摘要（需要抽象与简洁并重）上的作用未明确，缺乏全面、系统性的公开评测。

Method: 系统性地评估了多种最先进推理型和非推理型LLM在对话摘要中的表现，涵盖通用、角色导向和查询导向三种重要范式，使用多个高质量基准（SAMSum、DialogSum、CSDS、QMSum）与多样的自动化和人为灵感评估指标，并辅以案例分析。

Result: 推理型LLM并没有一贯提升对话摘要质量，反而可能因过度推理导致内容冗长、不一致甚至不如非推理型模型。研究详细揭示并分析了推理为何在特定对话场景中失效。

Conclusion: 显式逐步推理的LLM在对话摘要任务中，并不总是带来更高的摘要质量，甚至可能因冗长、事实不一致和不简洁而表现不佳。当前推理型LLM在实际对话摘要中的有效性有限，需开发更有针对性的模型和评估方法。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [24] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 论文系统分析了一种循环变换器模型（Huginn-3.5B），发现其内部的隐式链式推理有限且难以解释，循环层数增加作用甚微，大幅落后于显式推理外化的方法。


<details>
  <summary>Details</summary>
Motivation: 链式推理（CoT）提升了变换器模型在复杂任务上的能力，但通常模型需要将推理步骤显式表达为自然语言，从而影响模型效率。因此，学者探索利用循环结构将推理过程内化于模型隐空间，是否能实现隐式的链式推理（latent CoT）尚无定论。

Method: 本论文分析了一种名为 Huginn-3.5B 的深度循环变换器模型，在不增加参数量的情况下通过循环使用层来内化推理能力。研究通过 Logit Lens 和 Coda Lens 等探针技术，系统评估了该模型在算术任务中的内部推理结构和隐藏状态的可解释性。

Result: 实验证明，尽管 Huginn-3.5B 在部分情况下展示了有限的可解释隐式链式推理迹象，但整体上，这种结构在模型内部表现有限。不同循环区块的探针结果不一致，且隐状态解释性严重依赖于层索引和解码方式。此外，增加模型循环的深度对性能提升作用有限，效果远逊于显式外化推理步骤的模型。

Conclusion: 深度循环结构虽可一定程度内化推理过程，但在算术等任务上的隐式链式推理结构较难清晰显现，探针分析显示层级和解码方式对可解释性影响极大，且循环加深带来的性能提升十分有限。推理步骤的显式外化目前仍然对模型性能提升更为有效。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [25] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 本文提出并实现了GDC Cohort Copilot，通过自然语言描述自动生成GDC队列，使癌症基因组学研究者更便捷有效地筛选数据。工具基于本地大语言模型，效果优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: GDC为癌症基因组学数据提供了高质量的统一访问，但用户在使用可视化建群工具时面对大量属性和字段，尤其是新手，容易难以描述和筛选特定队列。而自然语言可能更符合用户直观需求。

Method: 提出GDC Cohort Copilot工具，让用户用自然语言描述所需队列，系统自动生成GDC兼容的队列过滤条件，并可导出到GDC继续分析。工具集成了交互式界面，并开发评估了多种本地大语言模型（LLM），用于优化队列生成。

Result: GDC Cohort Copilot能够根据自然语言精确生成GDC队列过滤条件，交互界面支持进一步细化选择。实验表明，其本地开源LLM在任务表现上优于GPT-4o提示方式。

Conclusion: GDC Cohort Copilot简化了癌症基因组数据队列筛选流程，让用户可以用自然语言高效并准确地构建个性化研究队列，提升了GDC工具使用体验。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [26] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 本文提出MemAgent，创新性地通过分段阅读和覆盖式记忆管理，高效支持超长文本处理，在低性能损失下实现出色的长文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的高效注意力和记忆模块虽然提升了长文本处理能力，但在保证线性复杂度的同时，处理超长文本（甚至无限长）且不降低性能依然是难题。

Method: 作者提出了一种名为MemAgent的新型代理工作流，能够以端到端方式处理长文本。MemAgent通过分段阅读文本并采用覆盖式更新策略管理记忆，同时扩展了DAPO算法，实现独立上下文多会话生成的训练。

Result: MemAgent在长文本推理中表现出卓越的能力：仅在8K上下文训练、32K文本学习基础上，就能外推到3.5M问答任务并保持不到5%的性能损失，在512K RULER测试中准确率超过95%。

Conclusion: MemAgent有效实现了线性复杂度下的超长文本处理，并在多项长上下文任务中展现出极高的外推能力和性能。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [27] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX方法通过引入LoRA，高效解决了持续领域自适应预训练中的性能瓶颈和适应性不足，可为不同任务输出个性化模型，具备广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 当前持续性领域自适应预训练（continual DAP）虽能增量式适应多领域，但面临高算力消耗、对数据顺序敏感以及仅提供通用模型等局限。

Method: 提出DoMIX方法，结合LoRA（高效参数微调技术），实现高效、可并行、对领域顺序不敏感的领域自适应预训练。该方法能为不同任务提供有针对性的模型。

Result: DoMIX显著提升了持续领域自适应预训练的效率与灵活性，不仅克服了算力和顺序敏感等问题，还能为具体任务提供更合适的模型。方法也能拓展应用于LLM的标准微调。

Conclusion: DoMIX是一种基于LoRA的创新方法，极大改善了持续领域自适应预训练的实用性和适应性，拓展了方法的适用场景，为多任务模型精调带来新思路。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [28] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文提出结合多模态大语言模型与少样本检索的集成系统，在SciVQA 2025比赛中取得第三名，F1均值达85.12，展示了该方法在科学视觉问答中的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在应对科学视觉问答（SciVQA）领域的挑战，通过科学图表理解与问答，提升模型在该任务上的表现。

Method: 提出了一套多模态大语言模型(Multimodal Large Language Models, MLLMs)集成系统，结合不同的少样本示例检索策略，并根据图形和问题类型自适应选择模型与示例；最终根据模型的置信度选择答案。

Result: 在SciVQA 2025任务上的盲测数据中，该系统在七组参赛队伍中排名第三，ROUGE-1、ROUGE-L及BERTS的F1均值达到85.12。

Conclusion: 多模态大模型与灵活的少样本检索策略结合，能有效提升科学视觉问答的准确率，但距离最佳系统仍有提升空间。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [29] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出用参数化量子电路（PQC）取代BERT中庞大的FFN层，可大幅减少模型参数，并在准确率和小样本学习能力上超越经典结构，显示了高效、创新的量子-经典混合Transformer设计的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究表明，Transformer架构中FFN模块占据了大多数参数（约三分之二），同时参数化量子电路（PQC）被认为可以增强神经网络的表达能力。以往大多将PQC用于自注意力模块，本工作尝试将其引入FFN中，以探究PQC在Transformer结构中的新型应用场景及优劣权衡。

Method: 提出QFFN-BERT：将紧凑型BERT变体中的FFN以基于PQC的新型层构建，并设计包含残差连接、$R_Y$和$R_Z$旋转、交替纠缠等结构以优化可训练性和表达性。通过在经典模拟器上，分别在SST-2和DBpedia数据集上进行系统实验和消融分析，测试模型表现和参数效率。

Result: 合理配置的QFFN-BERT在全数据集（full-data）设定下，准确率可达原基线模型的102%，实现准确率超越，并将FFN层相关参数减少99%以上；在小样本（few-shot）学习场景下模型依然表现出更佳的数据利用效率。消融实验表明，非优化的PQC结构无法有效学习，这验证了合适的PQC设计的重要性。

Conclusion: PQC与深度学习原理相结合，可作为参数高效且表达力强的经典FFN替代方案，尤其适用于需要模型压缩或提升小样本学习能力的场景。理论和实验均表明精心设计的PQC在Transformer FFN中具有实用价值。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [30] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 通过参数模型优化数据选择，只需少量高质量样本即可超越常规大数据训练效果，显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在代码生成和程序理解方面取得显著进展，但普遍依赖数据量提升模型表现，忽视了数据质量问题，导致训练效率降低。

Method: 提出一种基于参数模型的代码数据选择方法，通过优化该模型确保选取子集具有分布一致性和多样性，从而保证高质量数据用于训练。

Result: 实验结果显示，只使用1万样本时，该方法在HumanEval和MBPP基准上分别较9.2万全样本提升2.4%和2.3%，性能和效率均优于其他采样方法。

Conclusion: 该方法在大幅减少计算成本的同时，有效提升了模型表现，证明了高质量样本选择优于单纯增加数据量。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [31] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本文对Akan语音识别模型做了跨域评测，发现不同架构在域外表现大幅下降且错误类型不同，表明低资源语言ASR需针对域适应和模型选择进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型大多侧重于本领域测试，很少评估跨语境迁移能力，尤其在Akan等低资源语言领域缺乏系统性比较。本文旨在揭示模型域适应性短板并为低资源语种ASR模型设计提供经验依据。

Method: 基于transformer的七个Akan语音识别（ASR）模型（如Whisper、Wav2Vec2）在四个不同Akan语音语料库（覆盖多个真实语言场景）上进行基准测试，主要通过单词错误率（WER）和字符错误率（CER）来评估各模型的推广能力和错误分布。

Result: ASR模型极度依赖训练域，跨域表现急剧下滑。Whisper模型转录更流畅但易产生日常误导性错误，Wav2Vec2则在未知输入下输出易识别但难以解释的文本。强调未来需通过领域适应、多语种训练等策略提升低资源语种泛化能力。

Conclusion: Whisper和Wav2Vec2等现有Akan语音识别模型在本域表现最佳，但遇到域外数据会大幅降准，且两种架构的错误类型各异。低资源语言应用中需平衡转录可读性和透明度，因此有必要开发针对性的领域适应与多语种训练方法。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [32] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本文提出一套适用于低资源语言障碍语音ASR的数据采集和模型训练流程，并以加纳Akan语为例，构建并开放了首个相关数据集及工具，初步实验提升了障碍语音识别效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别（ASR）技术对有语言障碍者及低资源语言的支持严重不足，阻碍了这些群体的数字参与和包容性。该研究希望通过低门槛的数据采集和模型训练方法，让更多社区能够自主建设适应本地需求的ASR模型。

Method: 开发一套社区主导的数据采集和ASR模型训练“操作手册”，并以加纳广泛使用的Akan语为案例，组织有语言障碍的不同背景参与者采集并整理首个针对Akan语障碍语音的开源数据集。同时，使用这些数据初步微调并评估开源ASR模型在识别Akan障碍语音上的表现。

Result: 1. 建立了首个Akan语障碍语音开源数据集；2. 提供了基于社区驱动的ASR数据采集及模型训练操作手册和开源工具；3. 初步实验证明，利用该数据可以提升开源ASR模型对Akan语障碍语音的识别能力。

Conclusion: 本研究为低资源语言和障碍语音ASR的发展奠定了基础，提出了可复用的标准流程和工具，并展示了社区参与对于实现语音技术普惠的重要意义，推动了人工智能技术的包容性发展。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [33] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 本文构建并发布了印度首个公开的1200篇保释判决数据集，支持多种法律NLP任务，对促进本地法律AI研究具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 在印度等地区，法律自然语言处理（Legal NLP）领域的发展受限于缺乏结构化的数据集。

Method: 作者利用GPT-4o进行提示工程，自动标注判决文本中的20余项属性，内容涵盖保释结果、涉及法条、犯罪类型与法律推理，并通过人工核查保证数据一致性，最终构建了一个结构化的印度保释判决数据集。

Result: 构建了包含1200份印度法院保释判决、标注超过20项属性的结构化数据集，为预测结果、总结及公平性分析等多种法律NLP任务提供了基础数据资源。

Conclusion: 该数据集是首个专注于印度保释法理、且公开可用的判决数据集，将促进印度及类似区域法律NLP应用与发展。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [34] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了WebSailor训练范式，赋予开源LLM像DeepResearch等闭源系统一样，在复杂信息检索任务上系统减少不确定性的能力，使其性能大幅提升并接近专有系统水平。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在应对极为复杂的信息检索任务时受限于人类认知能力，现有开源模型在极端不确定性环境下表现不佳，但专有系统如DeepResearch已达到超越人类水准，迫切需要赋予开源模型类似能力。

Method: 提出WebSailor方法，包括通过结构化采样与信息遮蔽生成高不确定性任务、RFT冷启动、以及高效的代理型强化学习训练算法——DUPO（Duplicating Sampling Policy Optimization）。

Result: WebSailor显著优于所有开源agent，在复杂信息检索任务上表现与专有系统持平，缩小了开源与专有间的能力差距。

Conclusion: WebSailor通过后训练方法，使开源模型具备在极端不确定性环境下系统性降低不确定性的推理能力，实现了闭源系统的超人类检索表现。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [35] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文指出多数主动学习和标签框架忽视标签差异的信号价值，提出应区分人类标签变异与噪音，并给出整合该信息的新理念框架，支持更真实有效的主动学习。


<details>
  <summary>Details</summary>
Motivation: 当前监督学习受限于高质量标注数据的获取，尤其是在自然语言处理中，同一实例的标签往往存在差异（标签变异），但大多数注释框架仍假设单一真实标签，忽视了这种变异背后的价值。主动学习虽然可优化标注预算，但其前提假设往往未考虑到真实的人类标签变异。

Method: 该论文梳理了监督学习中关于“真值”和“标签本质”的基本假设，提出应将标签变异细分为有用信号（如人类标签变异）与噪声（如标注错误）。作者回顾了主动学习和标签变异领域对这些区别的关注与不足，并提出了一个将人类标签变异纳入主动学习流程（包括样本选择、标注者选择及标签表示）的概念框架。同时探讨了大语言模型作为标注者的可能角色。

Result: 作者提出的框架有助于推动主动学习系统在考虑人类标签多样性的情况下更好地反映现实标注过程的复杂性。相关讨论为后续HLV感知主动学习研究提供了理论基础。

Conclusion: 忽视人类标签变异是当前主动学习方法的重要不足，本文通过系统梳理和概念框架的提出，强调将标签变异纳入主动学习流程中的必要性，推动更贴近现实的标注和学习实践。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [36] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF方法能在不需复杂工程和微调的情况下，解释性地消除并对齐大型语言模型中的偏见，既高效又实用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）应用的普及，人们对简便、有效的偏见消除方法需求日益增长，因此需要能够在不进行复杂微调或工程的条件下，对模型输出中的偏见进行对齐调整。

Method: 本文提出了Multiperspective Fusion（MPF）方法，作为一种后训练的对齐框架。该方法基于SAGED流程，先自动构建偏见基准，并提取可解释的基线分布，然后将这些分布分解为多个可解释的视角组成部分，通过对生成结果进行采样及概率加权，实现对LLM输出偏见的曝光和对齐。

Result: 实验证明，MPF能够有效将LLM的情感分布调整到与反事实基线（绝对平等）或HR专业人士带有“名校”偏见的基线一致，表现为KL散度较小、校准误差减少，并能泛化到未见过的问题。

Conclusion: MPF是一种可扩展、可解释且无需复杂微调或提示工程的LLM后训练偏见对齐和消除方法，兼容已部署的大语言模型。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [37] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 该论文提出了新数据集与框架，实现对语言中性别和情境偏见的可解释度量，经多数据集实验有效，并拓展了对性别偏见的理解。


<details>
  <summary>Details</summary>
Motivation: 性别与语言中的情境偏见密切相关，尤其在动作动词、物体名词及职业描述中显著，但现有研究缺乏可解释的方法来度量和揭示这些偏见。作者希望解决这种偏见的量化与解释难题。

Method: 提出了一个新的数据集GenderLexicon和一个框架，能够估算情境性偏见以及相关性别偏见，通过对偏见打分提升结果的可解释性。并在五个不同语料库（包括日语数据集）上进行实验评估其有效性。

Result: 验证了该方法在多语料库上的有效性，并证实性别偏见不仅仅存在于职业刻板印象，还出现在其它语言元素中。

Conclusion: 提出的框架和数据集可用于分析和解释语言中的情境性性别偏见，为理解和改进性别公平性提供了新工具和视角。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [38] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了LimitGen基准，用于评价LLMs发现论文章节局限性的能力，并通过与文献检索结合，显著提升LLM辅助同行评审的效果，为科研评审流程带来新的自动化手段。


<details>
  <summary>Details</summary>
Motivation: 随着科学论文数量的持续增长，同行评审这一依赖专业知识的过程面临越来越大的挑战，特别是如何高效、高质量地发现论文的局限性。

Method: 作者首先构建了一个关于AI领域论文局限性的详细分类体系，并据此提出了LimitGen基准，以评估大语言模型（LLMs）在早期反馈和辅助人类评审中的能力。LimitGen包含人工构建的合成数据（LimitGen-Syn）和真实的人类撰写的局限性集合（LimitGen-Human）。此外，作者结合文献检索功能，增强LLM在识别局限性时的表现。

Result: 结合文献检索后的LLM在发现和生成研究论文局限性方面能力提升，能够为论文提供更具针对性和建设性的反馈。

Conclusion: 通过LimitGen基准和文献检索增强，LLM更能辅助早期同行评审，提高发现论文局限性的效率和质量，对推动学术评审现代化具有积极意义。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [39] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究通过模仿实验，首次量化了前元音在听觉空间被区分的最小可识别发音距离（JPD），结果为14至51 mels，对语音理论和元音系统解释有重要参考价值。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究说明人类元音发音是以听觉空间的目标为控制机制，但在亚音位层级上控制精度尚未知。本研究旨在量化两元音在听觉空间中被可靠区分所需的最小距离。

Method: 采用元音模仿范式，让被试模仿不同前元音刺激，以测定在F1×F2听觉空间中两刺激被可靠区分模仿所需的最小距离JPD。

Result: 首次测得JPD在14至51 mels（F1×F2空间）范围内。该结果对言语产生的情节理论、元音系统构成理论下限、以及元音音位的数量和分布趋势等有重要意义。

Conclusion: 本研究首次为英国英语说话者在前元音发音过程中的Just Producible Difference（JPD）提供了量化范围（14至51 mels），为解释元音系统的结构和限定元音音位之间的最小距离提供了理论依据。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [40] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 主流大语言模型难以发现并纠正自身生成内容中的错误，提出新评测框架量化此问题，并指出数据和训练流程调整可大幅改善该能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽然强大，但仍容易犯错且难以自我修正。可靠性需要模型具备自我纠错能力，但现有LLM在自身输出中往往难以及时纠正错误，这阻碍了其信任度和应用扩展。

Method: 提出并实现了Self-Correction Bench框架，系统性地通过受控注入不同复杂度级别的错误，评估14个主流大模型在自我纠错上的盲区表现。并通过模型训练数据分析，探讨其原因。

Result: 实验发现，平均有64.5%的自我纠错盲区。此外，主要原因在于训练数据多为无错误演示，缺少错误-纠正的示例；而RL训练模型通过结果反馈能学会纠错。附加“Wait”提示能大幅（89.3%）减少盲区。

Conclusion: 当前主流LLM存在严重的自我纠错盲区，其能力本身部分存在，但需要提示激活。未来提高LLM可靠性需改进训练数据结构或机制。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [41] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本研究系统评估了当前推理型大语言模型在社会偏见对抗上的安全性，发现引入推理能力（如链式思维或推理微调）并不总能提升偏见鲁棒性，反而可能暴露新的安全弱点，提示未来设计需更关注偏见防范。


<details>
  <summary>Details</summary>
Motivation: 推理能力被认为可以提升大模型的可解释性与可靠性，但其对抗社会偏见（bias）的鲁棒性仍不明确。认知推理型语言模型（RLMs）广泛采用链式思维（CoT）或微调推理轨迹方法，但这些推理机制是否有助于降低甚至提升模型的社会偏见尚未系统研究。

Method: 利用原为大语言模型（LLMs）设计的CLEAR-Bias基准，对多种最先进RLMs在社会文化多样性维度下进行系统评估。具体方法包括采用LLM-as-a-judge（由LLM自动评分安全性）和jailbreak攻击技术，测试模型内置安全机制的强度。通过量化对抗攻击下偏见激发成功率，比较不同推理策略（如微调推理、CoT提示）对模型公平性与安全性的影响。

Result: 研究发现，具备显式推理能力的模型（无论是CoT还是微调推理模型），在偏见激发方面比基础模型更脆弱，容易被攻击者利用强化刻板印象。推理微调模型在一定程度上比简单CoT模型更安全，但CoT模型尤其容易被借助故事编写、虚构角色或激励设定的提示攻击。总之，引入推理机制未必提升模型偏见鲁棒性，反而可能引入新的脆弱点。

Conclusion: 推理型语言模型（RLMs）在偏见安全性上的表现不如预期，推理机制可能会无意中增加模型被偏见诱发的风险。需发展更关注偏见鲁棒性的推理机制设计，挑战了“推理即更安全”的普遍假设。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [42] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文通过引入涵盖多种解题路径的新数据集并结合多样性奖励的强化学习，提出Qwen-VL-DP模型，显著提升了多模态数学推理的准确性和解答多样性，为MLLMs带来更强的泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型（MLLMs）在数学推理任务中，通常只利用一对一的图像-文本配对及单一解决方案进行训练，忽视了有效推理方式的多样性和内部反思能力，这限制了模型的推理表现和泛化能力。

Method: 作者提出了MathV-DP数据集，为每个图像-问题对提供多个多样化的解题路径，从而实现更丰富的推理监督。同时，基于Qwen-VL模型，提出Qwen-VL-DP，并通过监督学习与基于规则的群体相对策略优化（GRPO，结合正确性判别与多样性感知奖励）联合优化，强调从不同推理视角学习并区分正确但不一样的答案。

Result: 在MathVista's minitest与Math-V基准数据集上，Qwen-VL-DP在准确率与生成解答的多样性方面都显著优于现有基础MLLM模型。

Conclusion: 引入多样化推理解答和反思机制对提升多模态数学推理模型的准确性和生成能力至关重要。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [43] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 本文提出SynapseRoute动态路由框架，可智能判断问题复杂度，选择合适的LLM推理模式，提高准确率同时大幅降低推理延迟和成本，解决了高推理模式资源浪费的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型应用普及，不同推理能力模式间的成本差距增大。很多简单问题用高推理能力浪费资源，亟需一种动态选择合理模型策略，以优化成本-准确率等多重目标。

Method: 提出并实现了SynapseRoute动态路由框架，通过机器学习预测将输入问题分配到高推理或低推理LLM模式，实现在医药领域上的实验验证，并引入AIT指标来评估准确率、速度和成本的权衡。

Result: 实验表明，约58%医学问题仅需低推理即可准确解答，SynapseRoute模型在多医学数据集上相较单一高推理模式，提升了准确率（0.8390 vs. 0.8272），且推理速度提升36.8%、token消耗降低39.66%。还发现对简单问题过度推理反而降低表现。

Conclusion: 该论文证明了动态地将问题按照复杂度分配到不同推理能力的LLM模式，可以在保持准确率的同时显著节省成本并优化用户体验。提出的SynapseRoute模型优于只用高推理模式。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [44] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 当前大型语言模型在“严格按照人类指令”特别是输出限制方面缺乏泛化能力。论文构建了新基准IFBench，并提出强化学习加可验证奖励训练，大幅提升这一能力，并开放数据与工具。


<details>
  <summary>Details</summary>
Motivation: 语言模型和聊天机器人在与人类交互时，是否能严格遵循人类指令（包括各种输出限制）非常关键。目前顶尖模型仍难以泛化遵循未见过的输出限制，而仅在少量已知测试约束上过拟合。

Method: 作者提出了新的基准IFBench，用于评估模型在58个新颖、复杂且具可验证性的“域外”输出约束下的严格指令遵循能力。此外，设计了约束验证模块，并结合可验证奖励的强化学习（RLVR）方法对模型进行训练，以提升其泛化能力。并公开了额外的手工注释数据与代码。

Result: （1）现有语言模型在指令遵循上的泛化能力普遍较差。（2）使用RLVR训练能显著提升模型对于严格指令遵循的泛化表现。（3）相关数据及工具已开放共享。

Conclusion: 通过新基准和训练方法验证，论文提出的RLVR和验证模块能显著增强模型对未见过指令约束的响应能力。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [45] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 大模型如果直接用点赞/点踩等用户反馈做微调，恶意用户可能利用这一过程对模型持久植入错误知识、安全漏洞代码或假新闻，显示出偏好微调存在重要安全隐患。


<details>
  <summary>Details</summary>
Motivation: 探讨基于用户反馈微调（如点赞/点踩）的大语言模型潜在安全风险。当前大模型常通过用户交互数据提升性能，但该过程是否会被恶意用户影响尚未充分研究。

Method: 提出一种攻击方式：攻击者通过设计prompt使模型输出随机“中毒”或正常回应，并对“中毒”回应点赞、对正常回应点踩。攻击在后续偏好微调时利用这些反馈信号，最终改变模型输出倾向。

Result: 实验发现攻击不仅能持续改变模型输出，而且能：（1）植入模型原本不具备的事实知识；（2）引入有安全隐患的代码生成模式；（3）注入虚假财经新闻。甚至在正常对话场景下也能有效生效。

Conclusion: 此研究揭示了基于用户反馈的偏好微调模型存在新型攻击面，攻击者可利用有限反馈实现对模型知识和行为的精细操控，为后续安全防护研究提供重要线索。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [46] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 本文提出了一种名为MOTIF的强化学习微调方法，实现了超越上下文窗口限制的多轮推理，有效提升了LLM在数学推理任务中的表现，并具有样本高效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在推理任务中的能力受限于有限的上下文长度，即每次只能关注有限数量的已生成token。这一“上下文窗口”限制成为LLM进行更复杂多轮推理时的瓶颈。为了解决这一问题，研究者希望找到方法让模型能够突破上下文长度限制，实现跨多轮的推理能力。

Method: 本文提出了一种新的强化学习微调方法——MOTIF（Modular Thinking via Reinforcement Finetuning）。该方法让LLM通过多轮的形式生成推理token，从而有效增加模型的“思考”上下文长度。作者采用参数高效微调方式，基于Qwen2.5-3B-Instruct模型，在GSM8K数据集上进行训练，并在MATH500与AIME2024基准测试上进行评估。

Result: 在MATH500和AIME2024两个基准测试上，所提MOTIF方法相比于传统的GRPO方法分别提升了3.8%和3.3%的准确率。此外，该提升仅使用了原训练样本的15%，显示了较高的数据利用效率。

Conclusion: MOTIF方法能有效突破传统LLM的上下文长度限制，通过模块化多轮推理提升LLM在复杂数学推理任务的表现，且具有较高的训练样本利用效率。代码和模型均已开源。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [47] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 多项选择题在语言模型评估中存在根本性缺陷，生成式“答案匹配”评测可以更好反映模型真实性能，自动评价与人工一致性极高，建议业界转向主推答案匹配评测。


<details>
  <summary>Details</summary>
Motivation: 长期以来，多项选择题一直是语言模型评估的主流方法，因为其评分客观且易于自动化。然而，作者发现流行基准中的选择题经常可以不看问题本身而直接猜对答案，这暴露了判别式评测方式的根本局限。随着生成式模型的快速发展，亟需寻找更准确、具代表性的评测方法。

Method: 文中提出用生成式的“答案匹配”作为新的评测方式：不给模型选项，只给问题，让模型自由生成答案，然后用现代语言模型参照标准答案判断其匹配度。作者进一步在MMLU-Pro和GPQA-Diamond基准上人工标注数据，通过与人工评分的对齐度来评估各种评测方法的有效性。

Result: 实验表明，使用当下的语言模型（即使是小型模型）进行答案匹配，其自动评价结果与人工评分的一致性几乎完美，甚至达到人工标注者之间的水平。而多项选择题和无参考答案的LLM判分则与人工一致度较差。此外，改用答案匹配后，不少模型的排名均发生了显著变化。

Conclusion: 多项选择题已不再适合作为语言模型评估主流，答案匹配是一种更加有效、可靠的评测方式。今后，评测生态应向生成式答案匹配方向转变。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [48] [On Obtaining New MUBs by Finding Points on Complete Intersection Varieties over $\mathbb{R}$](https://arxiv.org/abs/2507.02492)
*Arindam Banerjee,Kanoy Kumar Das,Ajeet Kumar,Rakesh Kumar,Subhamoy Maitra*

Main category: cs.DM

TL;DR: 论文用代数簇方法刻画了互不相干基(MUBs)在$C^n$下的可扩展性，建立了其与最大对易正规正交矩阵族的严格对应，为MUBs存在与结构研究提供了新的数学工具。


<details>
  <summary>Details</summary>
Motivation: 互不相干基(MUBs)在量子物理中扮演重要角色，其数学结构丰富。论文旨在通过代数几何方法，研究MUBs体系可扩展性的等价标准，并探索其代数结构与矩阵理论的联系。

Method: 通过研究某一仿射代数簇的实点，分析MUBs在$C^n$空间的扩展性，利用该簇所反映的可扩展关系，进一步考察其中完备交域的形成。同时，建立MUBs与$m{m M}_n({f C})$中最大对易正规正交矩阵族间的一一对应关系。

Result: 证明了MUBs的可扩展性与某代数簇的实点性质等价，并指出部分代数簇形成完备交域。此外，明确了$C^n$中$m$组MUBs与$m$个对易矩阵基的一一对应关系，每组包括$n$个对易正规正交矩阵；存在最大对易基时，则存在MUBs的完全集。

Conclusion: 论文将MUBs的理论推广到代数几何及矩阵理论，给出了MUBs扩展的等价标准，并构建了与正规矩阵最大对易基的一一对应，为理解MUBs的存在性提供新的视角。

Abstract: Mutually Unbiased Bases (MUBs) are closely connected with quantum physics,
and the structure has a rich mathematical background. We provide equivalent
criteria for extending a set of MUBs for $C^n$ by studying real points of a
certain affine algebraic variety. This variety comes from the relations that
determine the extendability of a system of MUBs. Finally, we show that some
part of this variety gives rise to complete intersection domains. Further, we
show that there is a one-to-one correspondence between MUBs and the maximal
commuting classes (bases) of orthogonal normal matrices in $\mathcal
M_n({\mathbb{C}})$. It means that for $m$ MUBs in $C^n$, there are $m$
commuting classes, each consisting of $n$ commuting orthogonal normal matrices
and the existence of maximal commuting basis for $\mathcal M_n({\mathbb{C}})$
ensures the complete set of MUBs in $\mathcal M_n({\mathbb{C}})$.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [49] [Engineering an LTLf Synthesis Tool](https://arxiv.org/abs/2507.02491)
*Alexandre Duret-Lutz,Shufang Zhu,Nir Piterman,Giuseppe de Giacomo,Moshe Y Vardi*

Main category: cs.FL

TL;DR: 该文提出了基于MTBDD的LTLf到DFA直接转换及合成方法，实验上显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: LTLf反应式综合问题旨在构建一个基于历史输入信息产生输出的转换器，以确保每个输入序列的输入输出演化能在前缀上满足LTLf规范。当前工具在效率和性能方面存在不足。

Method: 提出了一种从LTLf到DFA的新型直接转换方法。该DFA用共节点的二元决策图数组（MTBDD）表示。在构建过程中，直接将其作为可达性博弈进行动态求解，从而提升效率。

Result: 实验结果表明，所实现的LTLf合成器在基准测试集上的性能优于现有工具。

Conclusion: 提出的基于MTBDD的直接转换及合成方法在实际LTLf反应式综合任务中实现了更高效率，可作为现有方法的重要改进。

Abstract: The problem of LTLf reactive synthesis is to build a transducer, whose output
is based on a history of inputs, such that, for every infinite sequence of
inputs, the conjoint evolution of the inputs and outputs has a prefix that
satisfies a given LTLf specification. We describe the implementation of an LTLf
synthesizer that outperforms existing tools on our benchmark suite. This is
based on a new, direct translation from LTLf to a DFA represented as an array
of Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based
representation can be interpreted directly as a reachability game that is
solved on-the-fly during its construction.

</details>
