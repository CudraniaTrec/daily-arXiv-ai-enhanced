<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Mechanizing Synthetic Tait Computability in Istari](https://arxiv.org/abs/2509.11418)
*Runming Li,Yue Yao,Robert Harper*

Main category: cs.PL

TL;DR: 本文在Istari证明助手中机械化了合成Tait可计算性（STC）方法，实现了阶段区分、模态等核心构造，并在两个类型理论规范性模型中验证其有效性，强化了类型理论元定理的可形式化证明。


<details>
  <summary>Details</summary>
Motivation: 范畴拼接是用于证明类型理论元定理（如规范性与归一化）的有力工具。然而，拼接模型通常很复杂。合成Tait可计算性（STC）通过将拼接范畴内化到带有阶段区分的模态依赖类型理论中，以抽象手法处理这些模型。该工作旨在机械化STC，提升形式化证明与重用性。

Method: 作者在Istari证明助手中机械化实现STC。Istari是一种基于Martin-Löf风格的外延类型理论，支持等价反射（简化了强制传递推理）。作者开发了一个可重用的合成阶段区分库，包括模态、扩展类型和严格粘合类型，并将其应用于两个案例研究：1）含依赖积和大消去布尔值的依赖类型理论规范性模型；2）面向成本逻辑框架的Kripke规范性模型。

Result: 作者表明，核心的STC构造可以在Istari中几乎原样形式化，机器校验下能保持纸上论证的优美和正确性。

Conclusion: STC的核心结构可以在Istari证明助理中机械化，既确保了优雅的理论结构，也实现了高可靠性的机械验证。作者开发的可重用库为合成类型理论提供了新工具，并通过案例验证了其实用性和正确性。

Abstract: Categorical gluing is a powerful technique for proving meta-theorems of type
theories such as canonicity and normalization. Synthetic Tait Computability
(STC) provides an abstract treatment of the complex gluing models by
internalizing the gluing category into a modal dependent type theory with a
phase distinction. This work presents a mechanization of STC in the Istari
proof assistant. Istari is a Martin-L\"{o}f-style extensional type theory with
equality reflection. Equality reflection eliminates the nuisance of transport
reasoning typically found in intensional proof assistants. This work develops a
reusable library for synthetic phase distinction, including modalities,
extension types, and strict glue types, and applies it to two case studies: (1)
a canonicity model for dependent type theory with dependent products and
booleans with large elimination, and (2) a Kripke canonicity model for the
cost-aware logical framework. Our results demonstrate that the core STC
constructions can be formalized essentially verbatim in Istari, preserving the
elegance of the on-paper arguments while ensuring machine-checked correctness.

</details>


### [2] [Expressive Power of One-Shot Control Operators and Coroutines](https://arxiv.org/abs/2509.11901)
*Kentaro Kobayashi,Yukiyoshi Kameyama*

Main category: cs.PL

TL;DR: 该论文对一类称为“one-shot”控制操作符（包括效果处理器、有界延续、非对称协程等）的表达能力进行了理论层面的比较和分类。


<details>
  <summary>Details</summary>
Motivation: 尽管“multi-shot”控制操作符已获得广泛的理论研究，但实际工程实现和效率需求推动了对“one-shot”操作符的关注。one-shot操作符在表现力和执行效率上达成平衡，但其正式的表达能力关系尚未被严格分析，因此该研究填补了该空白。

Method: 采用Felleisen提出的宏表达性作为表达能力的度量方法，对比了各种one-shot控制操作符间的表达能力，并对之前的非正式论证进行修正，给出了规范的宏转换证明流程。

Result: 形式化地比较了一系列one-shot控制操作符的表达能力，澄清了one-shot效果处理器、有界延续与非对称协程之间的表达关系，同时纠正了之前相关非正式推理的缺陷，给出更严密的理论支撑。

Conclusion: 论文证明：one-shot效果处理器和delimited continuations都可以通过非对称协程以宏方式表达（macro-expressed），但反之不成立（即非对称协程表达能力更强）。

Abstract: Control operators, such as exceptions and effect handlers, provide a means of
representing computational effects in programs abstractly and modularly. While
most theoretical studies have focused on multi-shot control operators, one-shot
control operators -- which restrict the use of captured continuations to at
most once -- are gaining attention for their balance between expressiveness and
efficiency. This study aims to fill the gap. We present a mathematically
rigorous comparison of the expressive power among one-shot control operators,
including effect handlers, delimited continuations, and even asymmetric
coroutines. Following previous studies on multi-shot control operators, we
adopt Felleisen's macro-expressiveness as our measure of expressiveness. We
verify the folklore that one-shot effect handlers and one-shot
delimited-control operators can be macro-expressed by asymmetric coroutines,
but not vice versa. We explain why a previous informal argument fails, and how
to revise it to make a valid macro-translation.

</details>


### [3] [Efficient Decrease-and-Conquer Linearizability Monitoring](https://arxiv.org/abs/2410.04581)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 本文提出并实现了一个系统性的线性化监控算法框架，首次统一、有效地解决了对主要数据结构进行大规模线性化检测的效率与可扩展性问题，性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 线性化（linearizability）已经成为并发数据结构实现的事实标准正确性规范。然而，形式化验证并发数据结构仍然具有挑战性，因此线性化监控成为开发自定义实现时排除早期问题和压力测试的关键工具。此前，何时线性化监控问题可变得易处理尚不明晰。

Method: 本文提出了一种统一的“减而治之”（decrease-and-conquer）算法框架，通过识别历史中保持线性化的特殊值——即移除后可维持线性化子历史的值。证明了对该值的多项式时间识别算法能带来线性化监控的多项式时间算法。作者还将该框架用于集合、栈、队列和优先队列等热门数据类型，并在每种情况下推导出最优的对数线性时间复杂度算法。

Result: 该框架针对具备不歧义性（每次插入为唯一值）限制的数据类型，推导了集合、栈、队列和优先队列等情况下的多项式时间甚至最优对数线性时间复杂度算法。实验证明方法能处理大型历史并优于现有工具。

Conclusion: 作者提出了统一、系统、可高效实现的线性化监控算法框架，在多个主流数据结构中均达到了理论上和实践上的优良性能，有助于提升并发数据结构正确性检测的实用性和规模化能力。

Abstract: Linearizability has become the de facto correctness specification for
implementations of concurrent data structures. While formally verifying such
implementations remains challenging, linearizability monitoring has emerged as
a promising first step to rule out early problems in the development of custom
implementations, and serves as a key component in approaches that stress test
such implementations. In this work, we investigate linearizability monitoring
-- check if an execution history of an implementation is linearizable. While
this problem is intractable in general, a systematic understanding of when it
becomes tractable has remained elusive. We revisit this problem and first
present a unified `decrease-and-conquer' algorithmic framework for
linearizability monitoring. At its heart, this framework asks to identify
special linearizability-preserving values in a given history -- values whose
presence yields an equilinearizable sub-history when removed, and whose absence
indicates non-linearizability. We prove that a polynomial time algorithm for
the problem of identifying linearizability-preserving values, yields a
polynomial time algorithm for linearizability monitoring, while conversely,
intractability of this problem implies intractability of the monitoring
problem. We demonstrate our framework's effectiveness by instantiating it for
several popular data types -- sets, stacks, queues and priority queues --
deriving polynomial time algorithms for each, with the unambiguity restriction,
where each insertion to the underlying data structure adds a distinct value. We
optimize these algorithms to achieve the optimal log-linear time complexity by
amortizing the cost of solving sub-problems through efficient data structures.
Our implementation and evaluation on publicly available implementations show
that our approach scales to large histories and outperforms existing tools.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: 本文提出一个结合统计异常检测和LLM自动生成规则与代码的数据质量分析框架，通过自动化、知识增强和守护机制显著提升了表格数据的验证效率和可靠性，并在实际数据集上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有规则基础的数据验证方法存在效率低、需要人工干预、计算成本高等问题，迫切需要更自动化且可靠的解决方案用于表格数据质量分析。

Method: 首先通过聚类等传统统计方法过滤异常数据，然后多轮提示LLM生成语义有效的数据质量规则，最后通过代码生成型LLM自动生成验证代码，并用检索增强生成（RAG）和领域特定的实例辅助，最后通过守护机制保证规则和代码的一致性和准确性。

Result: 在多组基准数据集上进行了广泛评估，验证了该方法在数据质量检测上的有效性。

Conclusion: 提出了一个结合统计方法和大语言模型（LLM）的三阶段数据质量检测框架，能够高效、自动生成高质量的数据验证规则和代码。

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [5] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 本文提出了一个可智能复用实验数据的管理框架，用于提升数字化工程中的实验效率并减少资源消耗，通过基于案例推理判断实验结果可否复用，并通过实际工业案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型驱动系统工程(MBSE)趋势向数字化工程及早期验证与确认推进，实验用于估算系统参数和探索设计决策变得日益重要。然而，实验配置元数据及结果的管理变得至关重要，尤其是为了加速整体设计工作并避免不必要且资源密集型的实验，需要有效管理与复用实验数据。

Method: 本文提出了一种实验管理框架，重点基于领域知识进行基于案例推理，以智能地复用实验数据。该框架能够判断既有实验（或其结果）是否能用于回答新的工程师或用户提出的问题，无需重新进行新的实验。同时，作者给出了此实验管理器的一般架构，并用工业车辆能源系统设计的案例进行了验证。

Result: 该框架能够有效管理和复用实验元数据及结果，通过智能判断实验的复用性，减少重复实验，节省时间和资源。工业实例验证了方法的有效性。

Conclusion: 实验管理与智能数据复用机制可加速系统设计流程，提高研发效率，且框架具有实际应用价值。

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [6] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: 本文提出了专为zkVM设计的自动化测试工具Arguzz，能有效发现约束系统及执行逻辑中的安全漏洞，通过变形测试和故障注入技术，在多个主流zkVM实测中成功发现严重漏洞，强调了此类系统化测试工具的实用价值和重要性。


<details>
  <summary>Details</summary>
Motivation: zkVM在去中心化应用和区块链中广泛应用，用于实现可验证的链外计算。然而复杂性高，存在约束系统或执行逻辑中的重大缺陷，可能影响其正确性和安全性，因此亟需高效的自动化测试方法。

Method: 提出了一种结合变形测试和故障注入的新颖方法工具Arguzz。具体做法是生成语义等价的程序对，将其合并成为单一Rust程序并在zkVM内运行，通过向VM中注入故障，模拟恶意或有缺陷的证明者，以此发现约束过于宽松的问题。

Result: 利用Arguzz测试了六个主流zkVM系统（如RISC Zero等），在其中三款中发现了十一个漏洞。其中RISC Zero的一个漏洞还获得了$50,000赏金，虽然此前已经经过审计。

Conclusion: Arguzz展示了系统化自动化测试zkVM的必要性与有效性，能发现此前审计未能发现的关键漏洞，对保障区块链应用与rollup安全性有重要意义。

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [7] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: 针对注入攻击日益成为软件安全重大威胁的问题，本文提出一种基于历史测试结果动态调整防御强度的SQL注入漏洞测试优先级方法，提升了检测效率与防御效能。


<details>
  <summary>Details</summary>
Motivation: 随着网络应用激增，网络攻击显著增多，尤其是注入攻击已成为软件安全的主要隐患，导致测试复杂性及负担显著提升，亟需高效的测试方法以支持敏捷开发。

Method: 利用前一次测试结果动态调整后续测试的防御强度，并优化测试流程，根据不同软件需求定制防御机制。

Result: 所提方法能够在SQL注入漏洞测试中实现优先级排序和动态调节，有效提高测试效率和漏洞发现率。

Conclusion: 本文提出了一种新的SQL注入漏洞测试用例优先级排序方法，通过调整后续测试的防御强度向量，提升了漏洞检测与防御的效率。

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [8] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 本文针对 LLM 自动生成嵌入式机器学习代码经常出现静默失败等问题，系统实证分析了多类 LLM 的典型失败模式，总结了通用失效类别，并提出了提升 LLM 生成代码可靠性的改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）已被广泛用于自动化嵌入式机器学习的代码生成，但其生成的代码常常会静默失效或表现出不可预测的行为，因此有必要系统性剖析这些失效模式以提升其可靠性。

Method: 作者基于一个自动驾驶框架，通过实际 orchestrate 数据预处理、模型转换和设备端推理代码生成的流程，实证调研 LLM 驱动的嵌入式 ML 管道中各种失效模式，系统分析 prompt 格式、模型行为和结构假设对代码生成成败的影响，并对多种 LLM 的错误案例进行分类和归因。

Result: 分析揭示了多样的容易出错行为，包括受格式影响的误解、结构上假设的问题，以及虽然能够编译但会导致下游失效的代码。最终形成了失效类型的分类体系，并指出了多个 LLM 共有的根本性脆弱点。

Conclusion: 现有验证手段难以发现 LLM 代码生成中的深层失效问题，该研究不仅发现 LLM 驱动嵌入式 ML 系统在可靠性和可追溯性上存在普遍挑战，还为后续提高 LLM 驱动系统可靠性和可追溯性提供了建议和方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Proceedings 9th edition of Working Formal Methods Symposium](https://arxiv.org/abs/2509.11877)
*Andrei Arusoaie,Horaţiu Cheval,Radu Iosif*

Main category: cs.LO

TL;DR: 第九届形式方法研讨会在罗马尼亚成功举办，论文集展示了该领域最新研究成果，并促进了学术交流。


<details>
  <summary>Details</summary>
Motivation: 促进行业学术交流，推动形式方法领域的最新研究进展，汇聚专家心得，共同探讨相关技术。

Method: 本论文采用会议论文集的编辑与汇总方式，收录了研讨会的研究论文。

Result: 论文集展示了本领域最新的方法和成果，扩展了学术视野，加强了学者间的交流与合作。

Conclusion: 本文献集已成功举办第九届形式方法研讨会，并汇总了相关学术研究成果。

Abstract: This volume contains the proceedings of the 9th Working Formal Methods
Symposium, which was held at the Alexandru Ioan Cuza University, Ia\c{s}i,
Romania on September 17-19, 2025.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本文提出风险隐蔽攻击（RCA）新框架，揭示九款主流金融大模型在监管合规方面易被绕过，平均攻击成功率达93%。现有模型合规防护存在严重缺陷，需尽快加强金融大模型的审核与对齐。


<details>
  <summary>Details</summary>
Motivation: 目前大模型的红队测试主要关注有害内容，却忽视了金融应用中的合规与监管风险。本研究旨在揭示金融大模型在合规性上的潜在脆弱性。

Method: 提出了一种新的多轮对话攻击框架——风险隐蔽攻击（RCA），并构建了金融领域专用测试集FIN-Bench，对九款主流大模型在金融合规性方面进行系统性红队评测。

Result: RCA能以93.18%的平均攻击成功率绕过九个主流金融大模型的监管风险检测，个别模型如GPT-4.1成功率高达98.28%。这暴露了现有对齐技术的重大漏洞。

Conclusion: 现有的大模型对金融领域的合规性风险防护存在明显不足，RCA方法能够大幅绕过主流LLMs 的合规机制。需要更强有力的审核机制来提升金融大模型的安全性和合规性。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [11] [Vanishing Signatures, Orbit Closure, and the Converse of the Holant Theorem](https://arxiv.org/abs/2509.10991)
*Jin-Yi Cai,Ben Young*

Main category: cs.DM

TL;DR: 本文用不变量理论方法，证明Holant定理逆向近似成立，只受消失signatures阻碍，首次解决了有界度图同态不可区分性问题，并揭示其与图同构复杂性的紧密联系。


<details>
  <summary>Details</summary>
Motivation: Valiant的Holant定理在计数问题的算法和归约中至关重要，理解它的逆定理能够进一步推动该领域的发展，但之前关于逆定理的猜想已经被反例否定。本论文旨在澄清Holant定理逆的可能性及其限制。

Method: 作者利用不变量理论的方法，构造了两个近似逆定理，通过分析张量的GL_q轨道闭包和消失signatures来刻画Holant不可区分性。

Result: 提出了两个Holant定理的近似逆命题：(1) Holant-不可区分的两个签名集能够通过序列的全息变换互相逼近，其GL_q轨道闭包相交；(2) 唯一阻碍Holant定理逆的是消失签名，并据此首次刻画了有界度图的同态不可区分性问题。此外，证明Holant不可区分性对于复杂性类TOCI是完全的，从而与图同构问题等价难。

Conclusion: 本文为Holant定理的逆问题给出精确刻画，澄清了消失签名是唯一障碍，并首次解决了有界度图的同态不可区分性判别，为相关计数复杂性和图同构难度的联系提供了新视角。

Abstract: Valiant's Holant theorem is a powerful tool for algorithms and reductions for
counting problems. It states that if two sets $\mathcal{F}$ and $\mathcal{G}$
of tensors (a.k.a. constraint functions or signatures) are related by a
\emph{holographic transformation}, then $\mathcal{F}$ and $\mathcal{G}$ are
\emph{Holant-indistinguishable}, i.e., every tensor network using tensors from
$\mathcal{F}$, resp. from $\mathcal{G}$, contracts to the same value. Xia
(ICALP 2010) conjectured the converse of the Holant theorem, but a
counterexample was found based on \emph{vanishing} signatures, those which are
Holant-indistinguishable from 0.
  We prove two near-converses of the Holant theorem using techniques from
invariant theory. (I) Holant-indistinguishable $\mathcal{F}$ and $\mathcal{G}$
always admit two sequences of holographic transformations mapping them
arbitrarily close to each other, i.e., their $\text{GL}_q$-orbit closures
intersect. (II) We show that vanishing signatures are the only true obstacle to
a converse of the Holant theorem. As corollaries of the two theorems we obtain
the first characterization of homomorphism-indistinguishability over graphs of
bounded degree, a long standing open problem, and show that two graphs with
invertible adjacency matrices are isomorphic if and only if they are
homomorphism-indistinguishable over graphs with maximum degree at most three.
We also show that Holant-indistinguishability is complete for a complexity
class \textbf{TOCI} introduced by Lysikov and Walter, and hence hard for graph
isomorphism.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [12] [A Unifying Approach to Picture Automata](https://arxiv.org/abs/2509.12077)
*Yvo Ad Meeres,František Mráz*

Main category: cs.FL

TL;DR: 本文提出用DAG自动机通过不同编码（输入无关或输入驱动）来识别二维图片语言，证明输入驱动编码能识别更复杂的语言，并超越经典的自动机方法。


<details>
  <summary>Details</summary>
Motivation: 希望提高对二维字符串或图片语言的识别能力，利用有向无环图（DAG）进行更灵活的编码，以克服目前自动机方法在二维输入处理上的限制。

Method: 提出将二维输入编码为DAG，并提出两类编码方法：输入无关编码（仅基于输入大小）和输入驱动编码（依赖于具体符号），通过不同DAG结构对应不同类型自动机，用于识别不同类别的图片语言。

Result: 输入无关编码可分别刻画出返回型有限自动机、牛耕自动机和在线镶嵌自动机所接受的图片语言类。将字符串编码为简单有向路径仅能识别正规语言，而输入驱动编码允许DAG自动机识别部分上下文相关语言，并在二维语言上优于在线镶嵌自动机。

Conclusion: 利用DAG及其灵活编码方式，能够显著提升对复杂二维输入语言的表达能力和识别能力，尤其输入驱动编码在识别高级语言方面表现优越。

Abstract: A directed acyclic graph (DAG) can represent a two-dimensional string or
picture. We propose recognizing picture languages using DAG automata by
encoding 2D inputs into DAGs. An encoding can be input-agnostic (based on input
size only) or input-driven (depending on symbols). Three distinct
input-agnostic encodings characterize classes of picture languages accepted by
returning finite automata, boustrophedon automata, and online tessellation
automata. Encoding a string as a simple directed path limits recognition to
regular languages. However, input-driven encodings allow DAG automata to
recognize some context-sensitive string languages and outperform online
tessellation automata in two dimensions.

</details>
