<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dependence-Driven, Scalable Quantum Circuit Mapping with Affine Abstractions](https://arxiv.org/abs/2510.27067)
*Marouane Benbetka,Merwan Bekkar,Riyadh Baghdadi,Martin Kong*

Main category: cs.PL

TL;DR: 文章提出一种基于依赖权重的量子比特映射新算法，通过依赖信息优化SWAP插入，显著降低电路深度及SWAP门数量，优于已有工具，并具备较强扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于现代量子处理单元（QPU）受限于比特耦合图，仅允许最近邻的交互，因此量子编译中的量子比特映射任务尤为关键。插入SWAP门以填补非相邻二比特门的连接性会导致电路延迟增加，进而造成量子程序错误率指数级上升。现有方法多依赖启发式分层技术，但未充分利用电路中的依赖关系信息。

Method: 提出了一种基于传递依赖权重的全新量子比特映射算法。通过仿射抽象方法建模量子电路，可计算传递依赖关系。此信息被用于依赖距离分层，进一步高效地为每层计算出不同权重，从而优化映射过程。

Result: 实验在IBM和Rigetti QPU上，使用QUEKO和QASMBench等大型基准测试集，并与四种主流工具（QMAP、Sabre、Cirq、TKET）对比。结果表明，所提出方法在电路深度和SWAP数方面有显著改善，并具备良好的扩展性。

Conclusion: 利用电路依赖关系权重优化量子比特映射，不仅有效减少SWAP门数量及电路深度，还保持较强的可扩展性，优于现有主流工具。

Abstract: Qubit Mapping is a critical task in Quantum Compilation, as modern Quantum
Processing Units (QPUs) are constrained to nearest-neighbor interactions
defined by a qubit coupling graph. This compiler pass repairs the connectivity
of two-qubit gates whose operands are not adjacent by inserting SWAP gates that
move the state of qubits between directly connected qubits. Deciding when to
introduce SWAPs while minimizing their count is critical because the error in
quantum programs increases exponentially with the circuit latency, measured in
number of gates along the critical path of the circuit. Prior work for this
problem relied on heuristics and exact methods that partition the circuit into
two or more layers, but failed to exploit valuable dependence information in
any form.
  This paper introduces a novel qubit mapping algorithm based on the weight of
transitive dependences. The introduced mapper models quantum circuits with
affine abstractions thereby yielding the ability to compute transitive
dependences. In turn, the newfound information is used to partition circuits by
dependence distances and compute, efficiently, distinct weights for each layer.
We evaluate the efficiency of our mapper on IBM and Rigetti QPUs, using the
large datasets from the QUEKO and QASMBench benchmark suites, and against four
baseline tools (QMAP, Sabre, Cirq and TKET), demonstrating notable improvements
in circuit depth and swap count while delivering competitive scalability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Empirical Studies on Quantum Optimization for Software Engineering: A Systematic Analysis](https://arxiv.org/abs/2510.27113)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 本文系统分析了量子、量子启发式及混合算法在软件工程优化问题中的实证研究现状，揭示了报告完善度、评价标准和真实案例应用等方面的不足，并对未来规范化研究与评价提出具体建议。


<details>
  <summary>Details</summary>
Motivation: 虽然量子及相关算法在软件工程优化问题中日渐表现出色，但关于如何规范进行实证研究仍缺乏公认最佳实践，因此需要系统性梳理现有实证方法与标准缺失。

Method: 通过对最新有关量子优化在软件工程中应用的系统性文献回顾中识别出的主要研究，开展系统分析，考察其实验设计、超参数设定、案例研究、基线对比、工具使用与评价指标等方面。

Result: 发现当前实证研究中存在诸如重复次数、实验轮次（shots）等报告不足、噪声处理考虑不充分，缺乏量子特有评价指标等多项缺陷。提出未来应加强真实案例研究、评价标准统一，引入量子专用质量指标，并推动评估与对比的实证规范。

Conclusion: 本文为基于量子、量子启发式与混合算法在软件工程优化问题中的实证研究现状提供了详尽分析。指出当前实验操作中存在标准化不足、报告内容不全等问题，并为未来研究提出了建议。

Abstract: In recent years, quantum, quantum-inspired, and hybrid algorithms are
increasingly showing promise for solving software engineering optimization
problems. However, best-intended practices for conducting empirical studies
have not yet well established. In this paper, based on the primary studies
identified from the latest systematic literature review on quantum optimization
for software engineering problems, we conducted a systematic analysis on these
studies from various aspects including experimental designs, hyperparameter
settings, case studies, baselines, tooling, and metrics. We identify key gaps
in the current practices such as limited reporting of the number of
repetitions, number of shots, and inadequate consideration of noise handling,
as well as a lack of standardized evaluation protocols such as the adoption of
quality metrics, especially quantum-specific metrics. Based on our analysis, we
provide insights for designing empirical studies and highlight the need for
more real-world and open case studies to assess cost-effectiveness and
practical utility of the three types of approaches: quantum-inspired, quantum,
and hybrid. This study is intended to offer an overview of current practices
and serve as an initial reference for designing and conducting empirical
studies on evaluating and comparing quantum, quantum-inspired, and hybrid
algorithms in solving optimization problems in software engineering.

</details>


### [3] [MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems](https://arxiv.org/abs/2510.27163)
*Jieshan Chen,Suyu Ma,Qinghua Lu,Sung Une Lee,Liming Zhu*

Main category: cs.SE

TL;DR: 本论文提出边际风险评估框架，以相对评估代替传统的绝对风险评估，有效解决真实数据不足问题，并帮助团队在部署AI系统时更好地权衡改进与风险。


<details>
  <summary>Details</summary>
Motivation: 在部署AI系统以替换现有流程之前，需要评估其改进效果和风险，但传统评估依赖于真实数据，这在许多场景下不可得。长期安全运行的系统通常缺乏可用的真实结果。

Method: 提出了边际风险评估框架，通过相对评价系统间的风险差异，避免依赖绝对风险或真实数据。重点包括可预测性、能力和交互优势三种相对评估方法。

Result: 该框架能够为软件团队提供实际指导，包括识别AI系统能在哪些方面提升结果、在哪些方面可能引入风险，以及如何负责任地采纳这些系统。

Conclusion: 通过从绝对评估转向相对评估，不依赖真实数据，边际风险评估框架使AI系统部署更可控，有助于更安全、有效地替换现有流程。

Abstract: Before deploying an AI system to replace an existing process, it must be
compared with the incumbent to ensure improvement without added risk.
Traditional evaluation relies on ground truth for both systems, but this is
often unavailable due to delayed or unknowable outcomes, high costs, or
incomplete data, especially for long-standing systems deemed safe by
convention. The more practical solution is not to compute absolute risk but the
difference between systems. We therefore propose a marginal risk assessment
framework, that avoids dependence on ground truth or absolute risk. It
emphasizes three kinds of relative evaluation methodology, including
predictability, capability and interaction dominance. By shifting focus from
absolute to relative evaluation, our approach equips software teams with
actionable guidance: identifying where AI enhances outcomes, where it
introduces new risks, and how to adopt such systems responsibly.

</details>


### [4] [On the Marriage of Theory and Practice in Data-Aware Business Processes via Low-Code](https://arxiv.org/abs/2510.27229)
*Ali Nour Eldin,Benjamin Dalmas,Walid Gaaloul*

Main category: cs.SE

TL;DR: 该论文提出BPMN-ProX低代码测试工具，通过数据注入及模型检查，大大提升了业务流程模型的数据验证能力，为流程管理行业带来更高的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 业务流程模型虽然被广泛采用，但缺乏正式的形式化表征，尤其在数据与流程的结合方面。如何提升模型的验证能力成为业界与学界关注的问题。

Method: 提出了一个低代码测试框架BPMN-ProX，通过将数据注入到可执行的BPMN模型中，结合先进的数据处理方式及模型检查工具，提升数据感知型BPMN的验证效率。

Result: BPMN-ProX可以有效增强数据感知型BPMN流程的验证，促进理论分析与实际建模的结合，并缩小非技术人员与专业人员之间的沟通与操作差距。

Conclusion: 创新性地实现了业务流程管理语言的形式化与数据结合验证，推动了更敏捷、可靠和以用户为中心的业务流程管理落地。

Abstract: In recent years, there has been a growing interest in the verification of
business process models. Despite their lack of formal characterization, these
models are widely adopted in both industry and academia. To address this issue,
formalizing the execution semantics of business process modeling languages is
essential. Since data and process are two facets of the same coin, and data are
critical elements in the execution of process models, this work introduces
Proving an eXecutable BPMN injected with data, BPMN-ProX. BPMN-ProX is a
low-code testing framework that significantly enhances the verification of
data-aware BPMN. This low-code platform helps bridge the gap between
non-technical experts and professionals by proposing a tool that integrates
advanced data handling and employs a robust verification mechanism through
state-of-the-art model checkers. This innovative approach combines theoretical
verification with practical modeling, fostering more agile, reliable, and
user-centric business process management.

</details>


### [5] [Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes](https://arxiv.org/abs/2510.27244)
*Ora Nova Fandina,Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky,Orna Raz*

Main category: cs.SE

TL;DR: 本文提出SparseAlign框架，用于在有限人工标注数据下判别和选择与人工一致性高的LaaJ评判者，有效提升传统遗留语言现代化过程中AI评估的可靠性和实践可行性。


<details>
  <summary>Details</summary>
Motivation: 应用现代化常常涉及COBOL、PL/I、REXX等遗留语言，但相关专家和高质量人工评估数据稀缺。虽然大型语言模型作为评审工具（LaaJ）有望替代人工，但其可靠性需要有原则的验证，否则可能导致评估失真与决策失误。因此，研究解决有限人工标注数据场景下LaaJ验证方法具有实际重要性。

Method: 提出SparseAlign框架，通过创新的成对置信度和分数敏感对齐度量，联合考虑排名一致性和分数接近性，适用于人工标注样本稀疏场景下的LaaJ对齐度评估，并与传统统计方法进行对比。将其应用于COBOL代码解释任务，选出最优LaaJ集成到实际评估流程中。

Result: SparseAlign在COBOL代码解释应用场景下有效甄选出与人工判断高度一致的LaaJ评审者，并成功集成到模型发布决策流程。通过四个LaaJ案例研究，展示了该方法在真实世界评测中的实用价值。

Conclusion: SparseAlign为在人工评价样本稀疏时，可靠衡量LaaJ和人工判断一致性提供了有效工具，为大型模型在传统编程语言现代化场景的评估和部署提供理论与实践支持。

Abstract: Application modernization in legacy languages such as COBOL, PL/I, and REXX
faces an acute shortage of resources, both in expert availability and in
high-quality human evaluation data. While Large Language Models as a Judge
(LaaJ) offer a scalable alternative to expert review, their reliability must be
validated before being trusted in high-stakes workflows. Without principled
validation, organizations risk a circular evaluation loop, where unverified
LaaJs are used to assess model outputs, potentially reinforcing unreliable
judgments and compromising downstream deployment decisions. Although various
automated approaches to validating LaaJs have been proposed, alignment with
human judgment remains a widely used and conceptually grounded validation
strategy. In many real-world domains, the availability of human-labeled
evaluation data is severely limited, making it difficult to assess how well a
LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework
for assessing LaaJ alignment with sparse human-labeled data. SparseAlign
combines a novel pairwise-confidence concept with a score-sensitive alignment
metric that jointly capture ranking consistency and score proximity, enabling
reliable evaluator selection even when traditional statistical methods are
ineffective due to limited annotated examples. SparseAlign was applied
internally to select LaaJs for COBOL code explanation. The top-aligned
evaluators were integrated into assessment workflows, guiding model release
decisions. We present a case study of four LaaJs to demonstrate SparseAlign's
utility in real-world evaluation scenarios.

</details>


### [6] [Efficient Integration of cross platform functions onto service-oriented architectures](https://arxiv.org/abs/2510.27344)
*Thomas Schulik,Viswanatha Reddy Batchu,Ramesh Kumar Dharmapuri,Saran Gundlapalli,Parthasarathy Nadarajan,Philipp Pelcz*

Main category: cs.SE

TL;DR: 本论文提出了一种面向软硬件无关的汽车应用开发与集成方法，通过标准化接口和半自动化工具，实现了在多种平台高效集成与开发，验证了其实用性并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 随着汽车电气/电子和软件架构日益复杂，研发和集成高效的软件定义汽车（SDV）应用变得至关重要。当前行业面临多样化硬件及中间件平台，导致开发和集成效率低下。因此，亟需平台和硬件无关的软件开发方法。

Method: 提出了一种应用开发与集成的理念，实现了针对平台和硬件无关性的应用设计，并对应用接口进行标准化。同时，将应用与中间件相关信息以机器可读的方式描述，辅助集成流程。此外，开发了相关工具，实现开发和集成过程的半自动化。

Result: 成功将示例应用分别集成到AUTOSAR Adaptive和ROS 2上，证明了方法的可行性，并用度量指标展示了其集成和开发效率。

Conclusion: 所提出的方法有效促进了软件作为产品（SaaP）理念的落地，提高了跨平台软件开发与集成的效率和可复用性，对未来软件定义汽车应用开发具有现实指导意义。

Abstract: The automotive industry is currently undergoing a major transformation with
respect to the Electric/Electronic (E/E) and software architecture, driven by a
significant increase in the complexity of the technological stack within a
vehicle. This complexity acts as a driving force for Software-Defined Vehicles
(SDVs) leading to the evolution of the automotive E/E architectures from
decentralized configuration comprising multiple Electronic Control Units (ECUs)
towards a more integrated configuration comprising a smaller number of ECUs,
domain controllers, gateways, and High-Performance Computers (HPCs) [2]. This
transition along with several other reasons have resulted in heterogeneous
software platforms such as AUTOSAR Classic, AUTOSAR Adaptive, and prototypical
frameworks like ROS 2. It is therefore essential to develop applications that
are both hardware- and platform/middleware-agnostic to attain development and
integration efficiency. This work presents an application development and
integration concept to facilitate developing applications as Software as a
Product (SaaP), while simultaneously ensuring efficient integration onto
multiple software architecture platforms. The concept involves designing
applications in a hardware- and software platform-agnostic manner and
standardizing application interfaces [6]. It also includes describing the
relevant aspects of the application and corresponding middleware in a
machine-readable format to aid the integration of developed applications.
Additionally, tools are developed to facilitate semi-automation of the
development and integration processes. An example application has been
developed and integrated onto AUTOSAR Adaptive and ROS 2, demonstrating the
applicability of the approach. Finally, metrics are presented to show the
efficiency of the overall concept.

</details>


### [7] [Agentic LLMs for REST API Test Amplification: A Comparative Study Across Cloud Applications](https://arxiv.org/abs/2510.27417)
*Jarne Besjes,Robbe Nooyens,Tolgahan Bardakci,Mutlu Beyazit,Serge Demeyer*

Main category: cs.SE

TL;DR: 本研究探讨了利用LLM扩展REST API测试用例的方法，显著提升了测试覆盖率和缺陷发现能力，同时评估了不同配置下的成本和效率，为自动化API测试提供新的解决方案。


<details>
  <summary>Details</summary>
Motivation: REST API在云原生系统中非常重要，但现有自动测试用例设计十分困难且耗费大量资源。该研究努力提升自动测试效率和多样性。

Method: 评估了单智能体和多智能体的LLM配置，在四个不同云应用中扩展测试用例，并分析了计算成本、运行时间与能耗。

Result: 智能体LLM系统能够对不同API架构进行泛化，提高端点和参数的测试覆盖率，并发现更多缺陷；同时深入分析了算法的准确性、可扩展性与效率之间的权衡。

Conclusion: 基于LLM的测试扩展能够提升REST API自动化测试的质量和可持续性，能够在复杂云环境中有效应用，提升覆盖率并揭示缺陷。

Abstract: Representational State Transfer (REST) APIs are a cornerstone of modern cloud
native systems. Ensuring their reliability demands automated test suites that
exercise diverse and boundary level behaviors. Nevertheless, designing such
test cases remains a challenging and resource intensive endeavor. This study
extends prior work on Large Language Model (LLM) based test amplification by
evaluating single agent and multi agent configurations across four additional
cloud applications. The amplified test suites maintain semantic validity with
minimal human intervention. The results demonstrate that agentic LLM systems
can effectively generalize across heterogeneous API architectures, increasing
endpoint and parameter coverage while revealing defects. Moreover, a detailed
analysis of computational cost, runtime, and energy consumption highlights
trade-offs between accuracy, scalability, and efficiency. These findings
underscore the potential of LLM driven test amplification to advance the
automation and sustainability of REST API testing in complex cloud
environments.

</details>


### [8] [CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments](https://arxiv.org/abs/2510.27565)
*Forough Mehralian,Ryan Shar,James R. Rae,Alireza Hashemi*

Main category: cs.SE

TL;DR: 本文构建了一个多语言、可扩展的基准测试平台，自动化评估大语言模型对指令的遵循与改进能力，揭示模型跨语言、多目标任务的实际表现与不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型代码生成能力评估主要关注功能正确性，忽视了现实编码任务的多样性及开发者的不同需求。作者希望建立一个更全面、可扩展的评估方式，涵盖多语言、多场景且能够考察模型的实际指令遵循能力。

Method: 提出了一个多语言基准测试，包括预设约束的指令遵循和后续指令的改进能力测试。通过LiveBench的编程任务自动翻译为Python、Java和JavaScript，实现自动化评估，并分析模型在不同维度上的表现。

Result: 实验证明不同模型在多语言、多维度指令遵循能力上表现不一。新基准测试能够更细致评估模型代码生成质量，揭示其跨语言和目标的优势与局限。

Conclusion: 本工作提出了广泛、多维的自动化评估基准，为分析大语言模型在代码生成领域的实际能力提供了更全面的工具，针对多语言和复杂指令需求具有现实意义。

Abstract: As large language models become increasingly capable of generating code,
evaluating their performance remains a complex and evolving challenge. Existing
benchmarks primarily focus on functional correctness, overlooking the diversity
of real-world coding tasks and developer expectations. To this end, we
introduce a multi-language benchmark that evaluates LLM instruction-following
capabilities and is extensible to operate on any set of standalone coding
problems. Our benchmark evaluates instruction following in two key settings:
adherence to pre-defined constraints specified with the initial problem, and
the ability to perform refinements based on follow-up instructions. For this
paper's analysis, we empirically evaluated our benchmarking pipeline with
programming tasks from LiveBench, that are also automatically translated from
Python into Java and JavaScript. Our automated benchmark reveals that models
exhibit differing levels of performance across multiple dimensions of
instruction-following. Our benchmarking pipeline provides a more comprehensive
evaluation of code generation models, highlighting their strengths and
limitations across languages and generation goals.

</details>


### [9] [Enhancing software product lines with machine learning components](https://arxiv.org/abs/2510.27640)
*Luz-Viviana Cobaleda,Julián Carvajal,Paola Vallejo,Andrés López,Raúl Mazo*

Main category: cs.SE

TL;DR: 针对在软件产品线中集成机器学习带来的变体管理和复用难题，本文提出了一个结构化方法，部分实现于VariaMos工具。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统越来越多地集成ML以提升决策能力，但在SPL场景下，这会让变体管理与复用变得更复杂。当前研究很少关注SPL与ML结合的变体管理问题，亟需专门的模型和方法。

Method: 提出了一个结构化框架，通过系统性建模和复用，辅助SPL集成ML组件，并在VariaMos工具中部分实现。

Result: 本文提出了一个结构化的框架，旨在扩展软件产品线工程（SPL），以便更好地集成机器学习（ML）组件。该框架支持系统化的变体建模和复用，并已通过VariaMos工具进行部分实现。

Conclusion: 该框架有效填补了在结合ML组件时，SPL工程管理变体方面的研究空白，为设计具备ML能力的软件产品线提供了系统化解决方案。

Abstract: Modern software systems increasingly integrate machine learning (ML) due to
its advancements and ability to enhance data-driven decision-making. However,
this integration introduces significant challenges for software engineering,
especially in software product lines (SPLs), where managing variability and
reuse becomes more complex with the inclusion of ML components. Although
existing approaches have addressed variability management in SPLs and the
integration of ML components in isolated systems, few have explored the
intersection of both domains. Specifically, there is limited support for
modeling and managing variability in SPLs that incorporate ML components. To
bridge this gap, this article proposes a structured framework designed to
extend Software Product Line engineering, facilitating the integration of ML
components. It facilitates the design of SPLs with ML capabilities by enabling
systematic modeling of variability and reuse. The proposal has been partially
implemented with the VariaMos tool.

</details>


### [10] [On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection](https://arxiv.org/abs/2510.27675)
*Md Abdul Hannan,Ronghao Ni,Chi Zhang,Limin Jia,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.SE

TL;DR: 本文探讨了如何用两种策略（模型表现与示例相似性）选择few-shot示例，显著提升了大语言模型对代码漏洞检测任务的能力，并在公开数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs虽在代码相关任务中表现优异，但在代码漏洞检测方面仍有挑战。提升性能常用的方法是上下文学习(few-shot)，但示例选择的不当会限制这种提升，亟需研究如何有效选择示例以提高模型能力。

Method: 提出并比较两种few-shot示例选择策略：一是基于LLM在示例上的表现（是否易出错），二是基于与目标程序的相似性（k近邻法）；在多个开源模型和数据集上进行评测。

Result: 两种few-shot选择策略都能提高LLM在代码漏洞检测任务中的性能；不同策略组合可进一步增强模型效果。实验结果验证了两种标准及其组合的有效性。

Conclusion: 通过合理选择few-shot示例作为上下文，可以提高LLMs在代码漏洞检测任务中的表现；两种示例选择标准各有优势，结合使用能进一步提升检测效果。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities for
many coding tasks, including summarization, translation, completion, and code
generation. However, detecting code vulnerabilities remains a challenging task
for LLMs. An effective way to improve LLM performance is in-context learning
(ICL) - providing few-shot examples similar to the query, along with correct
answers, can improve an LLM's ability to generate correct solutions. However,
choosing the few-shot examples appropriately is crucial to improving model
performance. In this paper, we explore two criteria for choosing few-shot
examples for ICL used in the code vulnerability detection task. The first
criterion considers if the LLM (consistently) makes a mistake or not on a
sample with the intuition that LLM performance on a sample is informative about
its usefulness as a few-shot example. The other criterion considers similarity
of the examples with the program under query and chooses few-shot examples
based on the $k$-nearest neighbors to the given sample. We perform evaluations
to determine the benefits of these criteria individually as well as under
various combinations, using open-source models on multiple datasets.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [Internalizing Extensions in Lattices of Type Theories](https://arxiv.org/abs/2510.26839)
*Jonathan Chan*

Main category: cs.LO

TL;DR: 现有证明助手对扩展追踪依赖外部机制，精度有限。作者提出用DCOI类型系统实现内嵌追踪机制，将每组扩展转化为依赖等级，通过格结构实现更精细的扩展管理。从而提升定义复用性和逻辑安全性。


<details>
  <summary>Details</summary>
Motivation: 现有证明助手系统允许使用增加表达能力的特性或公理，但这些扩展有时会导致逻辑不一致，因此需要追踪扩展使用情况。然而，目前的扩展追踪机制是外部实现的，无法精确指定定义所依赖的扩展，导致不能复用定义，且无法在不一致情况下合理引用定义。这影响了扩展间的元理论推理能力。作者旨在解决这些问题，提高扩展追踪的精度。

Method: 探索使用DCOI（Dependent Calculus of Indistinguishability）——一种依赖性追踪的依赖型系统，在该系统中，项和变量会分配依赖等级，这些等级形成一个格，描述访问权限。论文把每组扩展对应为一个依赖等级，通过格结构描述扩展怎样允许交互，以此实现更精确的扩展追踪机制。

Result: 提出将扩展追踪集成到类型系统内部，并利用DCOI的结构实现对扩展依赖的细粒度追踪，有望提升定义复用和元理论推理的灵活性，解决现有外部追踪机制存在的精度和安全性不足问题。

Conclusion: 将扩展追踪机制内嵌到依赖类型系统（通过DCOI）能显著提升证明助手系统的表达力，同时减少因过度扩展估计带来的兼容性和复用性障碍，对形式化化工工具的开发和使用具有重要意义。

Abstract: Many proof assistants allow the use of features and axioms that increase
their expressive power. However, these extensions must be used with care, as
some combinations are known to lead to logical inconsistencies. Therefore,
proof assistants include mechanisms that track which extensions are used in a
proof development or module, ensuring that incompatible extensions are not used
simultaneously.
  Unfortunately, existing extension tracking mechanisms are external to the
type system. This means that we cannot specify precisely which extensions a
definition depends on. Having the ability to write more precise specifications
means we are not picking an overapproximation of the extensions needed, which
prevents reusing definitions in the presence of incompatible extensions.
Furthermore, we cannot refer to definitions that use incompatible extensions
even if they are never used in inconsistent ways. The reasoning principles of
one extension therefore cannot be used as a metatheory to reason about the
properties of an incompatible extension.
  In this report, I explore the use of the Dependent Calculus of
Indistinguishability (DCOI) by Liu et al. for extension tracking. DCOI is a
dependent type system with dependency tracking, where terms and variables are
assigned dependency levels alongside their types. These dependency levels form
a lattice that describes which levels are permitted to access what. To instead
track extensions, each set of extensions would correspond to a dependency
level, and the lattice would describe how extensions are permitted to interact.

</details>


### [12] [Cut-free Deductive System for Continuous Intuitionistic Logic](https://arxiv.org/abs/2510.26849)
*Guillaume Geoffroy*

Main category: cs.LO

TL;DR: 本文提出了连续直觉主义逻辑和连续仿射逻辑的AC-代数语义，详细讨论逻辑系统及相关代数特性，通过Macneille完成证明模型嵌入，并实现了具有cut可容性的经典连续逻辑序列系统，为连续逻辑理论发展提供了新工具和基础。


<details>
  <summary>Details</summary>
Motivation: 现有关于连续逻辑的代数语义及序列系统尚不完善，尤其经典连续逻辑未有享有cut可容性的序列系统。作者欲填补这些理论空白，推进连续逻辑和相关逻辑系统的数学基础研究。

Method: 主要方法是以AC-代数为核心，研究从[0,1]到积分交换残差完备格的上确界保持函数，给出AC-代数的代数公理化，并利用Macneille完成证明每个阿基米德模型可嵌入至某AC-代数。对各类逻辑变体构建序列系统并证明完备性和cut可容性。

Result: 1）证明了USC(ℒ)满足v⊕v=2v当且仅当ℒ为locale；2）USC(ℒ)的否定反演性与ℒ一致；3）添加这些条件可恢复经典连续逻辑。每种逻辑变体均构建序列推理系统并证实完备性及cut可容性，首次实现经典连续逻辑的cut可容序列系统。

Conclusion: 论文提出了连续直觉主义逻辑和连续仿射逻辑的完全代数语义，并为每种变体（仿射、直觉主义、反演、经典）提供了序列风格的演绎系统且证明了完备性和cut可容性。这是首次实现具有cut可容性的经典连续逻辑的序列风格形式化。

Abstract: We introduce and develop propositional continuous intuitionistic logic and
propositional continuous affine logic via complete algebraic semantics. Our
approach centres on AC-algebras, which are algebras $USC(\mathcal{L})$ of
sup-preserving functions from $[0,1]$ to an integral commutative residuated
complete lattice $\mathcal{L}$ (in the intuitionistic case, $\mathcal{L}$ is a
locale). We give an algebraic axiomatisation of AC-algebras in the language of
continuous logic and prove, using the Macneille completion, that every
Archimedean model embeds into some AC-algebra. We also show that (i)
$USC(\mathcal{L})$ satisfies $v \dot + v = 2v$ exactly when $\mathcal{L}$ is a
locale, (ii) involutiveness of negation in $USC(\mathcal{L})$ corresponds to
that in $\mathcal{L} $, and that (iii) adding those conditions recovers
classical continuous logic. For each variant -affine, intuitionistic,
involutive, classical -we provide a sequent style deductive system and prove
completeness and cut admissibility. This yields the first sequent style
formulation of classical continuous logic enjoying cut admissibility.

</details>


### [13] [The Skolem Problem in rings of positive characteristic](https://arxiv.org/abs/2510.27603)
*Ruiwen Dong,Doron Shafrir*

Main category: cs.LO

TL;DR: 本文证明了Skolem问题在有限生成的正特征交换环中可判定，并给出了其零集结构的描述，方法依赖于近期关于S-单位方程和线性方程的新成果。


<details>
  <summary>Details</summary>
Motivation: Skolem问题作为数论和理论计算机科学中的核心判定问题，其在有限生成正特征交换环中的可判定性长期未知。

Method: 结合了关于S-单位方程和线性方程在$p^e$-挠性模块及乘法独立数的最新结果，发展算法判定线性递推序列的零项。

Result: 证明了Skolem问题在单位交换环$rac{	ext{Z}}{T}[X_1,m{	ext{...}}, X_n]/I$中可判定，并且零集是有限个$p_i$-normal集的有效并。

Conclusion: Skolem问题在有限生成的正特征交换环中是可判定的，存在算法决定线性递推数列是否包含零项。

Abstract: We show that the Skolem Problem is decidable in finitely generated
commutative rings of positive characteristic. More precisely, we show that
there exists an algorithm which, given a finite presentation of a (unitary)
commutative ring $\mathcal{R} = \mathbb{Z}_{/T}[X_1, \ldots, X_n]/I$ of
characteristic $T > 0$, and a linear recurrence sequence $(\gamma_n)_{n \in
\mathbb{N}} \in \mathcal{R}^{\mathbb{N}}$, determines whether $(\gamma_n)_{n
\in \mathbb{N}}$ contains a zero term. Our proof is based on two recent
results: Dong and Shafrir (2025) on the solution set of S-unit equations over
$p^e$-torsion modules, and Karimov, Luca, Nieuwveld, Ouaknine, and Worrell
(2025) on solving linear equations over powers of two multiplicatively
independent numbers. Our result implies, moreover, that the zero set of a
linear recurrence sequence over a ring of characteristic $T = p_1^{e_1} \cdots
p_k^{e_k}$ is effectively a finite union of $p_i$-normal sets in the sense of
Derksen (2007).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文系统分析了SSM与注意力机制混合模型的架构差异及性能，并提出通过数据增强提升模型召回率，比单纯架构优化更有效，为不同场景结构设计提供参考建议。


<details>
  <summary>Details</summary>
Motivation: 虽然SSM与注意力机制结合效果强，但架构设计原理不清晰，因此需要深入解析不同集成方式的性能表现和优化策略。

Method: 分析记忆利用和整体性能，比较SSM与注意力机制的串行与并行集成；提出利用带同义词的数据集持续训练的方法提升模型效果。

Result: 串行混合模型更适合短上下文， 并行混合模型适用于长上下文；通过同义句数据增强提高召回率，且比架构修改更有效，具有良好泛化能力。

Conclusion: 混合型SSM-注意力模型的架构设计对性能有显著影响，针对不同上下文长度应采用不同结构，并且数据增强能增强模型效果。

Abstract: Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [15] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 该论文提出一种通过语义模式在电子病历开放文本中识别需通报事件的方法，并成功用于发现性别暴力的漏报问题，具有很好的精度和推广性，可支持公共卫生系统伦理、可解释的NLP应用。


<details>
  <summary>Details</summary>
Motivation: 解决医疗记录性别暴力事件主动报告不足的问题，通过自动化方法发现未上报事件，提升公共卫生监测的有效性。

Method: 采用语义框架定义精细化模式，将其应用于非结构化数据（电子病历开放文本字段）。在巴西葡萄牙语的e-SUS APS语料库中定义并搜索了8个事件模式，结果通过语言学家手动评估并测量精度。

Result: 该方法在21百万句子语料库中以0.726的精度识别出暴力报告，证实了方法的稳健性，并且管线具有透明、高效、低碳和语言通用性，可应用于其他健康监测场景。

Conclusion: 该方法能够有效识别医疗记录中的性别暴力报告，具有高度的精确度和可扩展性，能够支持公共卫生系统中的伦理与可解释性NLP应用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [16] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: 提出用于电子病历医嘱抽取的首个挑战任务MEDIQA-OE 2025，六支团队基于医生-患者对话，应用不同方法和语言模型进行实验，并公布了数据集、排行榜和方法。


<details>
  <summary>Details</summary>
Motivation: 传统语音识别和自动摘要已应用于临床文档，但将对话内容直接转化为电子病历可执行医嘱尚属未研究课题。该技术能显著减轻医生文书压力，提升医疗效率。

Method: 论文通过设立共享挑战（shared task），邀请六个团队利用多种方法（包括闭源与开源大模型）进行医生-患者对话中的医嘱抽取。结果通过排行榜进行评比。

Result: 六支团队参与挑战，尝试了多种方案及不同类型的大语言模型，论文公布了任务数据集、最终排名和方案细节。

Conclusion: 本文首次提出抽取医嘱（medical orders）任务，有望减轻医生文书负担，并提升患者护理流程。通过举办MEDIQA-OE 2025 shared task，证明了不同团队探索的方法和模型的有效性。

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [17] [Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services](https://arxiv.org/abs/2510.27016)
*Jayden Serenari,Stephen Lee*

Main category: cs.CL

TL;DR: LOPSIDED是一种保护用户隐私的新框架，通过语义合理地替换敏感信息，确保模型回复既隐私安全又有上下文完整性，效果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI系统的广泛应用，用户在与大语言模型交互时频繁分享敏感信息，导致隐私和安全隐患亟需解决。现有隐私保护方法常常影响回复质量，新的方案需要在隐私和语义保真之间做平衡。

Method: 提出了一种基于语义的隐私保护代理LOPSIDED，通过在用户与远程LLM对话时，将敏感PII实体替换为语义一致的伪名，并在模型生成回复后自动去伪名，保障上下文完整性。方法在实际会话（来源于ShareGPT）上进行了实验和标注。

Result: LOPSIDED框架能够将语义效用错误减少5倍，同时提升了隐私保护效果。

Conclusion: LOPSIDED框架能够在保护隐私的前提下，有效降低语义效用错误，并且优于现有的基线方法。

Abstract: With the increasing use of conversational AI systems, there is growing
concern over privacy leaks, especially when users share sensitive personal data
in interactions with Large Language Models (LLMs). Conversations shared with
these models may contain Personally Identifiable Information (PII), which, if
exposed, could lead to security breaches or identity theft. To address this
challenge, we present the Local Optimizations for Pseudonymization with
Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a
semantically-aware privacy agent designed to safeguard sensitive PII data when
using remote LLMs. Unlike prior work that often degrade response quality, our
approach dynamically replaces sensitive PII entities in user prompts with
semantically consistent pseudonyms, preserving the contextual integrity of
conversations. Once the model generates its response, the pseudonyms are
automatically depseudonymized, ensuring the user receives an accurate,
privacy-preserving output. We evaluate our approach using real-world
conversations sourced from ShareGPT, which we further augment and annotate to
assess whether named entities are contextually relevant to the model's
response. Our results show that LOPSIDED reduces semantic utility errors by a
factor of 5 compared to baseline techniques, all while enhancing privacy.

</details>


### [18] [Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral](https://arxiv.org/abs/2510.27017)
*Ayoub Hammal,Pierre Zweigenbaum,Caio Corro*

Main category: cs.CL

TL;DR: 本文提出利用小型模型在测试时引导大型LLM进行对齐，可有效降低计算成本并提升性能，方法基于将token选择建模为0-1背包问题并进行近似；实验验证方法在多任务及推理速度上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的下游任务对齐过程计算成本极高，尤其LLM规模持续扩大，需寻找低成本且高效的对齐方案。

Method: 提出了一种在测试阶段通过代理模型进行对齐的新方法。具体为基于token的级联方案，将token选择转化为0-1背包问题，并进行了原始与对偶两种近似推理。

Result: 本方法通过小型对齐模型引导，实现了对大型LLM的低成本、可扩展的对齐，取得了更好的下游任务表现和更快的推理速度。

Conclusion: 实验结果表明，论文提出的方法在提升任务性能和加速推理速度方面有效。

Abstract: Several previous works concluded that the largest part of generation
capabilities of large language models (LLM) are learned (early) during
pre-training. However, LLMs still require further alignment to adhere to
downstream task requirements and stylistic preferences, among other desired
properties. As LLMs continue to scale in terms of size, the computational cost
of alignment procedures increase prohibitively. In this work, we propose a
novel approach to circumvent these costs via proxy-based test-time alignment,
i.e. using guidance from a small aligned model. Our approach can be described
as token-specific cascading method, where the token-specific deferral rule is
reduced to 0-1 knapsack problem. In this setting, we derive primal and dual
approximations of the optimal deferral decision. We experimentally show the
benefits of our method both in task performance and speculative decoding speed.

</details>


### [19] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

TL;DR: 本文提出Elastic Language Model (ELM)，通过改进神经架构搜索与知识蒸馏机制，能高效发现更紧凑且表现更优的语言模型，有助解决大模型在算力和经济上带来的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模预训练语言模型在自然语言理解任务中表现优异，但其高昂的计算和存储需求带来了经济和环境方面的问题。因此，亟需开发更紧凑且高效的模型架构，应对这些挑战。

Method: 提出了Elastic Language Model (ELM) 的神经架构搜索（NAS）方法。ELM扩展了现有NAS策略，采用灵活的搜索空间，结合高效的transformer模块及可动态调整维度和头数的结构。同时，设计了新颖的知识蒸馏损失，有助于在搜索过程中保留每个模块的特性，以提升模型架构选择的判别性。

Result: 在掩码语言建模和因果语言建模任务中，经ELM发现的模型显著优于现有方法。

Conclusion: ELM通过灵活的NAS和模块创新，能高效探索并发现更优的紧凑型语言模型架构，有效提升模型性能并减轻运算和存储负担。

Abstract: As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [20] [Constructive Characterization and Recognition Algorithm for Grafts with a Connected Minimum Join](https://arxiv.org/abs/2510.26975)
*Nanano Kita*

Main category: cs.DM

TL;DR: 本文刻画了哪些graft具有连通最小join，并设计了更快的判定与构造算法（$O(n(m+n\log n))$），在非稠密图比以往方法更优。


<details>
  <summary>Details</summary>
Motivation: 寻找连通最小join的图结构，并改进连通最小join判定算法的效率。相应问题中，最小join与最大$T$-cut packing等大小的图结构具有理论与算法意义，但缺乏结构性和判定方法。

Method: 本文提出了对拥有连通最小join的graft进行结构性刻画，并基于此设计了一种判定算法。算法关键步骤是计算最小join及求解对$m{	ext{conservative}~oldsymbol{oldsymbol{m{m{m{m{m{m{m{m{m{}}}}}}}}}}}}}}}}}}}}}}}}$ ±1权重图的单源所有汇最短路问题，实现时间复杂度$O(n(m+n\log n))$。

Result: 给出了拥有连通最小join的graft的结构性刻画；提出了一种多项式时间、复杂度为$O(n(m+n\log n))$的判定与构造算法，并在非稠密情形对已有结论实现复杂度改进。

Conclusion: 本文系统解析了哪些graft具有连通最小join，给出了可判定的结构性质，并构造了更优的多项式时间算法，因此理论和算法均取得进展，为相关图论及组合优化问题提供工具。

Abstract: Minimum joins in a graft $(G, T)$, also known as minimum $T$-joins of a graph
$G$, are said to be connected if they determine a connected subgraph of $G$.
Grafts with a connected minimum join have gained interest ever since Middendorf
and Pfeiffer showed that they satisfy Seymour's min-max formula for joins and
$T$-cut packings; that is, in such grafts, the size of a minimum join is equal
to the size of a maximum packing of $T$-cuts. In this paper, we provide a
constructive characterization of grafts with a connected minimum join. We also
obtain a polynomial time algorithm that decides whether a given graft has a
connected minimum join and, if so, outputs one. Our algorithm has two
bottlenecks; one is the time required to compute a minimum join of a graft, and
the other is the time required to solve the single-source all-sink shortest
path problem in a graph with conservative $\pm 1$-valued edge weights. Thus,
our algorithm runs in $O(n(m + n\log n) )$ time. In the nondense case, it
improves upon the time bound for this problem due to Seb\H{o} and Tannier that
was introduced as an application of their results on metrics on graphs.

</details>
