{"id": "2506.12202", "categories": ["cs.PL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12202", "abs": "https://arxiv.org/abs/2506.12202", "authors": ["Stephen Mell", "Botong Zhang", "David Mell", "Shuo Li", "Ramya Ramalingam", "Nathan Yu", "Steve Zdancewic", "Osbert Bastani"], "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions", "comment": null, "summary": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9002\u7528\u4e8eLLMs\u7684Quasar\u7f16\u7a0b\u8bed\u8a00\uff0c\u53d6\u4ee3Python\u7528\u4e8e\u4ee3\u7801\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u7ecf\u5b9e\u9a8c\u8bc1\u5b9e\u6bd4\u539f\u5148Python\u65b9\u6848\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u4e0bLLMs\u64c5\u957f\u4f7f\u7528Python\u5b9e\u73b0\u4ee3\u7801\u63a7\u5236\uff0c\u4f46Python\u5728\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u6709\u5c40\u9650\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u5207\u5408\u5b9e\u9645\u9700\u6c42\u7684\u65b0\u578b\u8bed\u8a00\u4ee5\u652f\u6301LLMs\u66f4\u4f18\u5730\u8c03\u7528\u5916\u90e8\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7f16\u7a0b\u8bed\u8a00\u2014\u2014Quasar\uff0c\u5e76\u5b9e\u73b0Python\u5b50\u96c6\u81ea\u52a8\u8f6c\u8bd1\u5668\uff0c\u518d\u5728ViperGPT\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u548cGQA\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4Python\u548cQuasar\u4ee3\u7801\u52a8\u4f5c\u7684\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "Quasar\u8bed\u8a00\u5728ViperGPT\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u6267\u884c\u65f6\u95f4\u6700\u591a\u53ef\u7f29\u77ed42%\uff0c\u5b89\u5168\u6027\u63d0\u5347\uff0c\u7528\u6237\u5ba1\u6279\u4e0b\u964d52%\uff0c\u4e14\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u624b\u6bb5\u5b9e\u73b0\u8986\u76d6\u7387\u4fdd\u969c\uff0c\u7efc\u5408\u6548\u679c\u4f18\u4e8e\u4f20\u7edfPython\u4ee3\u7801\u52a8\u4f5c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Quasar\u8bed\u8a00\u80fd\u591f\u6709\u6548\u66ff\u4ee3Python\u4f5c\u4e3aLLM\u751f\u6210\u4ee3\u7801\u52a8\u4f5c\u7684\u76ee\u6807\u8bed\u8a00\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u65e2\u63d0\u5347\u6548\u7387\u53c8\u63d0\u5347\u7cfb\u7edf\u8d28\u91cf\u3002"}}
