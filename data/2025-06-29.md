<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 13]
- [cs.DM](#cs.DM) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本论文系统梳理了领域知识在需求工程中的应用，总结现状、挑战和未来方向，为知识驱动的需求工程提供了理论和实践参考。


<details>
  <summary>Details</summary>
Motivation: 领域知识在需求工程（RE）中至关重要，但科学文献仍缺乏关于如何有效利用和操作领域知识的系统总结。

Method: 采用系统映射研究，结合数据库检索和递归的前向与后向溯源法进行文献筛查和分析。

Result: 共筛选出75篇相关文献，分析了需求类型、常见质量属性，以及领域知识的形式化、获取和长期维护中遇到的挑战，总结了已建立的方法和未解决的问题，并提出未来应注重可扩展、自动化和可持续的解决方案。

Conclusion: 本研究通过全面综述现有文献，为知识驱动的需求工程提供了概念和方法基础，有助于后续研究与实际应用。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [2] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 文献回顾了ML系统敏捷管理的现状，整理出8大主题与相应框架，认定工作量估算困难为核心挑战，并呼吁后续加强实证研究。


<details>
  <summary>Details</summary>
Motivation: 当前传统项目管理方式难以满足机器学习（ML）系统快速变化与实验驱动的开发特性，因此需要探索适合ML系统的敏捷管理方法。

Method: 采用系统映射研究，通过数据库检索结合正反向滚雪球法，对现有文献进行综述与整理。

Result: 共识别出27篇（2008-2024），提取了8个管理框架并将相关建议归纳为8大主题（如迭代灵活性、创新性ML工件、最小可行模型等），发现最大难点为ML任务的工作量估算。

Conclusion: 本研究系统梳理了当前ML系统敏捷管理方法的发展现状与存在问题，指出需要更多实证评估来支撑这些实践与框架的有效性。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [3] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 本文提出了一种基于Python rdflib库，自动化集成Neo4j数据库与OWL本体的新方法。该方法操作简便、无需深厚本体学知识，有效解决了知识图谱构建中的集成难题，并验证了在药品安全数据场景的实用性。


<details>
  <summary>Details</summary>
Motivation: 数据和知识的快速扩展导致本体（ontology）生成方法论的重要性提升，尤其是在面对数据体量快速增长与内容变化频繁的背景下，知识图谱构建和数据集成面临技术挑战。以往将Neo4j数据库与OWL本体语言集成时，不仅要求对DL语法有较深了解，也存在一定使用门槛，限制了更广泛用户的使用。

Method: 提出了一种更易用的方法，利用Python及其rdflib库来支持本体开发。具体采用Python脚本，从Neo4j数据库中自动生成OWL本体所需的类及其公理，实现了过程自动化。在实验中，作者将FDA不良反应报告系统（FAERS）数据集导入Neo4j，进而通过脚本实现数据与本体的高效衔接。

Result: 该方法无需深入掌握描述逻辑（DL）语法，极大降低了操作门槛，实现了数据从Neo4j数据库到OWL本体的自动化、高效集成。通过FDA FAERS数据库的实际应用验证了方法的可行性。

Conclusion: 作者提出的方法在大规模不良药物事件数据集快速增长的背景下，为本体生成提供了实践性的、用户友好的解决方案，有助于药品安全监测和公共健康决策。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [4] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文开发并评测了五个面向实际领域的RAG系统，通过100人参与多维度用户调查，总结了12条经验，揭示了RAG系统实践中的关键挑战及改进空间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)在事实准确性和上下文相关性方面存在局限，检索增强生成(RAG)系统成为解决这一问题的重要方法。然而，现有很少有关于RAG在真实场景下开发和用户评价的实证研究，同时也缺乏系统的经验总结。

Method: 开发了五个面向治理、网络安全、农业、工业研究和医疗诊断等领域的RAG系统，每个系统都集成了多语种OCR、基于向量嵌入的语义检索和领域适应的LLMs，并通过本地服务器或云API部署。随后，通过包含100名参与者的基于网页的评价流程，从易用性、相关性、透明度、响应性、准确性和推荐意愿六个维度对系统进行评估。

Result: 通过100人用户评价，系统在六个维度上得到反馈，总结出技术、运维和伦理等方面共12条实际经验，这些经验对RAG系统在真实应用中的可靠性和可用性有深刻影响。

Conclusion: RAG系统在多个实际领域具有良好的实用性。用户多维度评价提供了宝贵反馈，同时研究系统性地总结了开发和部署中的关键技术与伦理挑战，为今后RAG系统的实践和改进提供重要参考。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [5] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 本文提出利用强化学习结合人类建议的方法，用于自动开发复杂的模型转换链。结果表明，即使建议存在不确定性，也能提升RL性能和开发效率，为模型工程领域的人机协作提供新思路。


<details>
  <summary>Details</summary>
Motivation: 在模型驱动工程中，复杂的模型转换（MT）往往需要多个步骤串联，人工开发这些复杂转换容易出错且效率低下。现有的强化学习（RL）方法虽能自主探索最佳转换序列，但在复杂问题上性能有限，因此迫切需要提升其有效性。

Method: 提出了一种利用强化学习并结合可能不确定的人类建议引导的方法和技术框架。该框架可以将用户定义的模型转换映射到RL的基本操作中，并以RL程序的方式执行，寻找最优转换序列。

Result: 实验表明，即使是存在不确定性的人类建议，也能显著提升强化学习在复杂模型转换开发中的表现，提高开发效率。

Conclusion: 通过在强化学习过程中引入人类建议，实现了人机协同的复杂模型转换开发，有效提升了RL的性能并推动了工程方法的自动化和智能化。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [6] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: 本文提出了IFMA-VD框架，通过超图网络建模跨函数多边关联，显著提升了漏洞检测的效果，并在公开和真实数据集上都获得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大多数基于深度学习的漏洞检测方法主要关注于单独函数，忽略了函数间的复杂多边关联，从而可能遗漏这些关联中的漏洞。

Method: 提出了一个面向漏洞检测的跨函数多边关联分析框架（IFMA-VD）。该框架通过构建代码行为超图，并利用超边卷积提取多边关联特征，具体包括：将函数解析为代码属性图生成函数内特征，分割程序依赖图构建行为特征超边，最后通过超图网络捕获多边关联知识以增强漏洞检测。

Result: 在三个广泛使用的漏洞数据集上的实验显示，IFMA-VD比基线方法在F-measure和Recall上均有提升。此外，多边关联特征可以提升代码特征表达，在真实数据集上也验证了该方法的有效性。

Conclusion: 考虑函数之间多边关联能够显著提升漏洞检测性能，IFMA-VD框架在多项指标上优于传统方法，并且在实际场景下验证有效。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [7] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 文章提出了一个系统性的需求合成数据生成方法，在特定任务中合成数据的性能可超过人工数据，为AI4RE领域解决数据集稀缺指明了方向。


<details>
  <summary>Details</summary>
Motivation: AI在需求工程领域的应用受限于缺乏公开可用且有标注的需求数据集。虽然大语言模型能合成数据，但如何系统提升生成需求数据的质量尚未被深入研究。

Method: 提出了Synthline v1，这是一种基于产品线的增强合成需求生成方法，通过改进生成策略和后处理技术，探讨了提示策略、多样本生成、自动化优化（如PACE）及后期筛选等方式对数据集质量的影响，并在四项分类任务上进行了评估。

Result: 多样本生成显著提升了数据的实用性和多样性，F1分数提升6~44点；PACE优化对某些任务（如功能分类）效果明显（+32.5点），但会削弱其他任务表现；基于相似度的后处理提升多样性却常削弱分类性能。部分任务上，生成数据表现超越真人数据（安全任务提升+7.8点，缺陷任务+15.4点）。

Conclusion: 系统性的合成数据生成可以有效缓解需求工程领域数据集匮乏问题，并在特定任务中达到甚至超过人工数据的效果。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [8] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 本文提出了一种新APR框架$T^3$，结合了LLM推理和树搜索，在自动修复程序缺陷和提升修复方案精度方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs和CoT技术的发展，推理能力得到增强，但现有CoT在APR领域应用不足，特别是在需要复杂逻辑与多步推理的场景。作者希望提升APR任务中的自动化与精度。

Method: 系统性评估了多种主流CoT技巧在APR任务中的表现，并创新性地提出了 $T^3$ 框架，将大语言模型（LLM）的推理能力与树搜索算法结合。

Result: $T^3$ 框架提升了APR任务的候选修复生成精度，并为自动调试提供了更加高效、稳健的解决方案。

Conclusion: 提出的 $T^3$ 框架将LLMs的推理能力与树搜索结合，提升了自动程序修复（APR）的候选修复方案的精度，并为样本选择和修复策略优化提供了指导。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [9] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: 本文提出了一款名为KOALA的JetBrains IDE插件，能够灵活、详细地收集学生在编程任务中的行为和编码快照，弥补了现有工具的不足，并用实际数据展示了其应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有的数据收集工具在编程教育场景下对学生编程任务的收集存在诸如收集粒度不可控、无法收集具体编程环境事件、配置复杂等局限性。研究者和教育者迫切需要更灵活、易配置且功能全面的数据收集工具以改进教学和研究。

Method: 作者提出了一款名为KOALA的插件，该插件可以集成在JetBrains IDE中。KOALA允许根据需求配置收集代码快照和IDE功能使用的粒度，还可以分发编程任务、控制IDE特性（如启用/禁用代码补全）、以及运行问卷调查。插件会在学生编程时收集代码快照、IDE行为（运行、调试）、使用热键及文件切换等详细数据，并将数据发送至配套服务器，还支持转换为标准ProgSnap2数据格式。

Result: 作者使用KOALA插件，在两个课程共28名学生完成编程任务时收集了相关数据，并用这些数据展示了一些初步分析和洞察，证实了KOALA工具的有效性和实用性。

Conclusion: KOALA是一款高度可配置、便于部署且能细致采集数据的JetBrains IDE插件，能够弥补现有工具的不足，为编程教育研究与教学改进提供有力的数据支持。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [10] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: 该论文分析了微前端架构在工业应用中的适用性，通过实证研究发现虽然其实施可解决系统耦合和体验问题，但其必要性需具体权衡，微前端最适用于已有微服务和需拆分单体系统的场景。


<details>
  <summary>Details</summary>
Motivation: 动机是为了理解在何种情况下值得在工业环境中采用微前端架构，尤其是在面对主系统与前端高度耦合、技术陈旧、开发体验差等问题时。

Method: 该论文首先基于学术和灰色文献调查当前微前端的现状，随后在一个已使用微服务的手工艺品市场平台实施微前端架构，并通过面向开发者的半开放式问卷对实施效果进行评估。

Result: 微前端的实施成功改善了原有问题，但通过问卷分析结果显示：实现公司需求并非必须依靠微前端，单体前端等替代方案也可达到类似效果。最终选择微前端，主要因公司已有微服务架构，有利于基础设施复用和团队知识共享。

Conclusion: 微前端架构在某些公司背景下更方便，但并不是解决耦合和提升开发体验的唯一选择。其采用的必要性需结合具体场景评估。

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [11] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: 本文提出了一个用于整合物联网与业务流程数据的核心数据模型，弥补了现有模型碎片化和协作难题。通过原型实现验证，表现出较强的数据共享和适配能力，对流程挖掘领域的数据集成具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）技术的发展，IoT设备越来越多地融入到企业流程（BP）中。然而，IoT数据与传统流程数据在粒度和特性上有很大不同，如何有效整合它们成为流程挖掘中的挑战。现有数据模型各自关注不同方面，导致数据碎片化，妨碍了PM领域的数据共享与协作。

Method: 本文提出一个综合现有不同数据模型关键特征的核心模型，旨在覆盖主流的数据整合需求，并通过原型化的Python实现进行用例验证。

Result: 提出的核心模型能够很好地满足主流数据整合需求，大大促进了流程挖掘领域的数据共享与协作。通过Python原型实现对各种用例进行了有效验证。

Conclusion: 设计的核心模型融合了现有IoT与流程数据整合模型的优点，提升了数据共享性和协作性，为流程挖掘领域提供了统一高效的数据整合解决方案。

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [Pebble Games and Algebraic Proof Systems](https://arxiv.org/abs/2506.21149)
*Lisa-Marie Jaser,Jacobo Toran*

Main category: cs.LO

TL;DR: 本论文首次系统建立了Pebbling博弈与代数证明系统NS、MC、PC的复杂度精确联系，展示了对应策略/变量空间复杂度的等价，以及系统间的复杂度分离与下界结果。


<details>
  <summary>Details</summary>
Motivation: 探索计算复杂性理论中Pebbling方法与主流代数证明系统之间的联系及其彼此之间的复杂度分离问题，从而加深对推理和证明系统效率的理解。

Method: 通过分析0压石公式Peb(G)的反证法，考察单一汇点DAG（有向无环图）在Monomial Calculus中的反证复杂度，并将这些复杂度特征与Pebbling博弈的空间、时间参数进行严格比较。同时使用已知的图Pebbling界限推导代数证明系统中的空间测度分离。

Result: 证明了若Peb(G)在Monomial Calculus中的反证具有一定次数和规模，则在对应的DAG上有与之空间和时间匹配的black-pebbling策略，反之亦然，并拓展了此前仅限于reversible-pebbling与Nullstellensatz的结果。此外，获得了三种系统之间的次数分离结果以及MC系统的次数-规模权衡下界。

Conclusion: 论文建立了Pebbling游戏与代数证明系统之间的强关联，详细揭示了在不同类型的Pebbling游戏与不同类别的代数证明系统（NS, MC, PC）之间变量空间复杂度和策略的精确对应关系。

Abstract: Analyzing refutations of the well known 0pebbling formulas Peb$(G)$ we prove
some new strong connections between pebble games and algebraic proof system,
showing that there is a parallelism between the reversible, black and
black-white pebbling games on one side, and the three algebraic proof systems
Nullstellensatz, Monomial Calculus and Polynomial Calculus on the other side.
In particular we prove that for any DAG $G$ with a single sink, if there is a
Monomial Calculus refutation for Peb$(G)$ having simultaneously degree $s$ and
size $t$ then there is a black pebbling strategy on $G$ with space $s$ and time
$t+s$. Also if there is a black pebbling strategy for $G$ with space $s$ and
time $t$ it is possible to extract from it a MC refutation for Peb$(G)$ having
simultaneously degree $s$ and size $ts$. These results are analogous to those
proven in {deRezende et al.21} for the case of reversible pebbling and
Nullstellensatz. Using them we prove degree separations between NS, MC and PC,
as well as strong degree-size tradeoffs for MC.
  We also notice that for any directed acyclic graph $G$ the space needed in a
pebbling strategy on $G$, for the three versions of the game, reversible, black
and black-white, exactly matches the variable space complexity of a refutation
of the corresponding pebbling formula Peb$(G)$ in each of the algebraic proof
systems NS, MC and PC. Using known pebbling bounds on graphs, this connection
implies separations between the corresponding variable space measures.

</details>


### [13] [Deciding Robust Instances of an Escape Problem for Dynamical Systems in Euclidean Space](https://arxiv.org/abs/2506.21481)
*Eike Neumann*

Main category: cs.LO

TL;DR: 提出了一种在实数位模型下，对点在连续映射下逃离闭集问题的最优部分决策算法，覆盖了一般函数与特殊函数族，并给出条件下Mandelbrot集可计算性的新证据。


<details>
  <summary>Details</summary>
Motivation: 本文关注在连续映射下，判断一个点是否会逃离闭子集的问题。现有方法在实数位模型下较难获得完整决策过程，因此需要新的解决方案。

Method: 提出了一种在实数位模型下用于该问题的可靠部分决策算法。该算法在所有可靠部分决策方法的停机集合中为最大，即对所有在微小扰动下答案稳定的问题实例都能给出结论。并将方法应用于一般连续函数、仿射线性系统及二次复多项式。

Result: 算法对于解答在小扰动下鲁棒的问题实例总能停机，并且其停机集在所有问题实例中是稠密的。对特殊函数族（如仿射线性系统、二次复多项式）也能给出完整结果，在后者的情形下完整性依赖于复动力学中的超越性稠密性假设。此外得到Mandelbrot集可计算性的另一种证明，回应了Penrose提出的问题。

Conclusion: 该方法为判断点是否能在连续映射下逃离闭集提供了理论上最完备的部分决策过程，并拓展/完善了相关函数族的可计算性理解，尤其在复杂动力系统领域具有重要价值。

Abstract: We study the problem of deciding whether a point escapes a closed subset of
$\mathbb{R}^d$ under the iteration of a continuous map $f \colon \mathbb{R}^d
\to \mathbb{R}^d$ in the bit-model of real computation. We give a sound partial
decision method for this problem which is complete in the sense that its
halting set contains the halting set of all sound partial decision methods for
the problem. Equivalently, our decision method terminates on all problem
instances whose answer is robust under all sufficiently small perturbations of
the function. We further show that the halting set of our algorithm is dense in
the set of all problem instances. While our algorithm applies to general
continuous functions, we demonstrate that it also yields complete decision
methods for much more rigid function families: affine linear systems and
quadratic complex polynomials. In the latter case, completeness is subject to
the density of hyperbolicity conjecture in complex dynamics. This in particular
yields an alternative proof of Hertling's (2004) conditional answer to a
question raised by Penrose (1989) regarding the computability of the Mandelbrot
set.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文提出LUCARIO基准和一种结合贝叶斯网络与大语言模型的概率问答方法，实现了在大表格数据上概率问题的高效回答，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前的表格问答方法（如NL2SQL）能较好处理直接从表中检索答案的事实性问题，但在面对需要不确定性推理的概率性问题时表现不佳。为此，作者试图提升表格问答在概率性问题上的能力。

Method: 提出了LUCARIO基准，以及一个针对大规模表格数据的概率问答框架。方法包括：从表格中归纳出贝叶斯网络，将自然语言查询转化为概率查询，并利用大语言模型（LLM）生成最终答案。采用符号推理和神经推理混合的方法。

Result: 实证结果显示，该方法在概率问答任务上相比基线有显著提升，体现了混合符号-神经推理方法的优势。

Conclusion: 混合贝叶斯网络归纳和大语言模型的方法能显著提升表格数据中的概率性问答能力。新提出的LUCARIO基准有助于推进该领域发展。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [15] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 本文通过构建跨语言功能型基准，发现当前多语言大模型在实际多语言任务中的表现与静态评测结果存在明显差距，且在不同语言间的鲁棒性表现不均。新基准为多语言能力的真实评估提供了更好的工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的多语言能力主要通过静态数据基准进行评估，如Belebele、M-MMLU和M-GSM，但这些评估未能充分反映模型在实际多语言环境下的表现和稳健性，因此需要更具实用性的多语言功能型基准。

Method: 研究创建了两类多语言功能型基准：跨语言的Grade School Math Symbolic（CL-GSM Symbolic）和跨语言指令跟随评测（CL-IFEval），即将现有的功能型基准模板从英语翻译至法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种不同资源水平的语言。随后在这些新基准上对大模型进行评测。

Result: 发现不同静态多语言基准捕捉功能型性能的准确性差异较大。例如，在模型间，M-GSM到CL-GSM Symbolic之间在英语、法语和西班牙语上的性能分别下降24%、17%和18%；Belebele到CL-IFEval，各语言性能下降15-24%；而M-MMLU到CL-IFEval仅下降0.5%-3%。同时，模型在不同语言下的稳健性显著不同，部分语言（如阿拉伯语和英语）在多次评测中表现最为稳定。

Conclusion: 传统静态多语言基准未能全面反映大模型的实际多语言能力。新构建的功能型多语言基准更好检测了模型的泛用和稳健性能，并揭示了不同语言间模型能力的不均衡。建议后续评估多语言模型时，采用更贴近实际任务的多语言功能型测试方法。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [16] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLM虽然能产生看起来新颖的研究想法，但经过专家实际执行后，研究效果远不如人类专家的想法，揭示出大模型在科研创意生成上仍有明显短板。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM能在创意阶段产生新颖想法，但研究创意的好坏不仅在于新颖，更在于付诸实践后的研究成果优劣。本研究试图实证验证：LLM产出的想法是否在实际执行后能转化为优质的研究结果。

Method: 作者组织43名NLP领域专家，在不知情的情况下，分别执行由专家和LLM产出的研究想法。每位专家投入100小时以上，将想法落实为实验并写成短论文，然后由独立专家匿名评审并比较各个阶段（创意、执行后）评分。

Result: LLM产出的想法在实践执行后，各项评审指标（新颖性、激动人心、有效性和整体评分）均显著下跌，甚至在部分指标上被专家创意反超，体现LLM在高效科研想法生成上的局限性。

Conclusion: 当前的大语言模型（LLM）在生成科学研究创意时，虽然表面上显示出新颖性，但其创意经过实际执行后，效果显著逊于人类专家提出的想法，说明LLM尚不能替代人类专家在形成高质量研究议题方面的作用。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [17] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: 本文提出了MultiFinRAG，一套针对金融多模态文档问答的高效RAG方案，在复杂场景下准确率显著优于ChatGPT-4o。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型和检索增强生成（RAG）在处理金融文档时存在诸多挑战，如Token限制、布局丢失和跨模态上下文碎片化，难以应对复杂的跨文本、表格和图像的推理任务。

Method: 提出MultiFinRAG框架，利用轻量化的开源多模态LLM对表格和图片进行批量抽取，为其生成结构化JSON和简明摘要，并与叙述文本共同采用模态感知的相似度嵌入和检索。框架引入分层回退策略，根据需要动态从文本推理升级到文本+表格+图片联合推理。

Result: 在普通硬件上，MultiFinRAG在复杂金融问答任务中，准确率比ChatGPT-4o（免费版）高出19个百分点，特别是在需要文本、表格、图片和多模态联合推理场景下表现突出。

Conclusion: MultiFinRAG是一种专为金融领域多模态问答设计的检索增强生成框架，通过高效抽取与分层检索显著提升复杂金融文档的问答能力，超越现有主流LLM表现。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [18] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 本研究创新性地用暴力行为情景问卷系统评估主流LLM在真实道德困境下的表现，发现其内在倾向与生成文本常有不符，且在多维人设下表现出与主流社会科学认知相悖的偏差，表明LLM在处理现实冲突时存在显著风险。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）被广泛用于网络暴力内容识别与响应，但其处理道德模糊的、现实情境的能力尚未被充分研究。

Method: 首次使用社会科学领域验证过的“暴力行为情景问卷”(VBVQ)评估LLM在人类日常冲突下的反应，并通过基于角色的人设提示（包含种族、年龄、地理身份变换）考察可能的偏见。在统一的zero-shot设定下，评估6个跨不同地缘及组织开发的LLM。

Result: （1）LLM在文本生成的表面结果与其内部对于暴力响应的偏好存在差异；（2）其暴力倾向在不同人口统计特征间有所不同，且经常与犯罪学、社会科学以及心理学的主流结论相违背。

Conclusion: LLM不仅在表述上与实际“态度”有出入，且在人口统计学维度还存在不一致乃至偏见，揭示出其在实际伦理冲突场景处理上的局限性与风险。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [19] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 本文分析了医学领域自动事实核查系统应用受限的原因，发现主要挑战包括声明与证据的难以对应、声明含糊和评价主观性等。作者建议将医学事实核查当作一个基于互动的传播问题来解决，而非单纯的端到端自动任务。


<details>
  <summary>Details</summary>
Motivation: 医学决策关系广大民众，但大部分人缺乏医学素养，难以理解和判断医学文献的信息。同时，社交媒体上医学相关信息真假难辨，因此需要自动化事实核查系统来帮助判断医学声明的真伪。尽管这一需求强烈，实际使用的自动化医学事实核查系统仍非常有限。

Method: 本研究通过观察和分析临床专家在实际社交媒体医学声明的核查过程，考察他们如何检索、整合和利用医学证据，并提出将事实核查任务视为一个互动式传播问题来研究。

Result: 研究揭示了自动化端到端医学事实核查面临的关键难题：难以将真实世界的声明与具体的临床试验证据对应；声明表述含糊且意图各异，导致理解歧义；声明真伪的判断带有固有的主观性。

Conclusion: 医学事实核查不应被简化为一个端到端过程，而应当被视作一个需要互动和沟通的过程来设计和评估。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [20] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 针对语言模型在特定任务适应方面面临的效率、稳健性、资源消耗等问题，论文提出了一系列新型的预训练、微调和评测方法，显著提升了模型的性能和适用范围，推动了人工智能的通用化进程。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型（LMs）规模和复杂度的增加，其高效且稳健地适应特定任务仍面临挑战。当前的微调方法存在未充分利用无标签数据、在小样本集上过拟合、计算资源消耗大等问题，限制了语言模型在开放性真实世界任务中的应用。

Method: 1. 提出了基于无标签数据提取任务相关知识的新型继续预训练方法，优于现有半监督方法。2. 开发了高效的参数微调方法，在保持性能的前提下降低了内存与计算需求。3. 优化了监督微调方法，使语言模型在标注数据稀缺时更好地遵循指令，提升开放性生成等多类NLP任务表现。4. 构建了新的评测方式和基准（如多跳空间推理任务），更全面地评估语言模型的能力与适应性。

Result: 在多个不同的NLP任务上的大量实验表明，这些方法可显著提升语言模型的稳健性、效率和泛化能力，使其能更好地适应多样化应用场景。

Conclusion: 本文提出的方法有效提升了语言模型在下游任务适应性，在提升稳健性与效率方面起到关键作用，是朝向更通用人工智能迈出的重要一步。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [21] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 本文提出了一种通用多语言数据集构建流程，大规模收集处理1000+种语言共20TB数据，显著提升非英语大模型训练效果，开源了数据与相关代码。


<details>
  <summary>Details</summary>
Motivation: 当前高性能多语言大模型训练面临数据集难以清洗、去重等挑战，尤其是支持多语种的过滤和去重流程难以高效扩展。已有的高质量英文数据集进展较快，但多语言领域滞后。

Method: 提出了一种基于FineWeb的数据预处理管道，可自动适配任意语言，并在九种语言上消融实验以优化各项设计；通过创新的任务选择流程指导评估，同时提出根据去重数与数据质量重新平衡数据集的简洁原则化方法。最终，利用大规模网络抓取数据（Common Crawl）扩展至1000+种语言生成FineWeb2多语言数据集。

Result: 实验表明，所提出的管道生成的非英语数据集训练的大模型在性能上超过现有数据集方法。重新平衡方法进一步提升了模型性能。FineWeb2数据集规模达20TB（50亿文档），公开了数据集、管道和训练、评测代码。

Conclusion: 该研究提出了一种高效、可扩展的多语言数据集构建方案，使各语种模型性能显著提升，同时推动多语言预训练基础设施的开源发展。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [22] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文关注于MTEB基准测试平台的工程实现，提出了一系列确保平台可复现性与可扩展性的工程措施与流程管理，提升了平台质量，为类似领域的基准平台维护提供了宝贵经验。


<details>
  <summary>Details</summary>
Motivation: MTEB已经成为文本嵌入模型的标准评测平台，但随着规模扩大，如何保障评测结果的可复现性和拓展性成为主要挑战。本文旨在解决大规模基准测试在工程管理方面的难题。

Method: 作者详细介绍了如何维护持续集成流水线，验证数据集的完整性，实现自动化测试，并评估基准结果的泛化能力。此外，还描述了应对社区贡献及添加新任务和数据集的工程策略。

Result: 通过上述工程实践，MTEB基准不仅变得更加全面，而且在质量和可用性方面也得到了维持和提升，对整个领域仍具高度相关性。

Conclusion: 论文总结了提升基准测试平台可复现性与可用性的工程方法和经验，为其他机器学习评测平台的维护者提供了有益参考。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [23] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: KaLM-Embedding-V2是一种紧凑高效的文本嵌入模型，结构和训练方法均有创新，性能显著超越同规模模型，并能媲美数量级大得多的模型，适合多场景应用。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本嵌入模型虽然性能优越，但参数规模庞大，资源消耗较高，限制了其通用性与部署。为此，有必要设计体积小巧且性能强劲的通用文本嵌入模型，以兼顾实际应用中的效率与效果。

Method: 提出KaLM-Embedding-V2模型，创新点包括：（1）移除因果注意力掩码，采用全双向Transformer结构和均值池化生成定长嵌入；（2）采用多阶段训练流程：大规模弱监督语料预训练、高质量检索与非检索数据微调，以及模型参数平均增强泛化能力；（3）引入focal-style样本加权和在线hard-negative混合机制。同时，广泛收集多类别训练数据以提升效果和泛化能力。

Result: KaLM-Embedding-V2在MTEB中英文大规模文本嵌入基准测试中，大幅优于同等规模模型，并可媲美远超自身体量（3倍、14倍、18倍、26倍大）的大型模型，成为小型通用嵌入模型新标杆。

Conclusion: KaLM-Embedding-V2凭借创新结构和高效训练方法，实现了低于1B参数的小体量下优异的中英文通用文本嵌入性能，推动了轻量级、高效文本嵌入模型的发展。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [24] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 该论文提出一种元训练语言模型的方法，使微调过程能够模拟提示方式带来的高效泛化能力，仅用模型自身输出进行训练，无需标签，实验验证了其在多项任务上取得了与提示类似甚至更好的效果。


<details>
  <summary>Details</summary>
Motivation: 在将新信息融入语言模型时，现有方法主要包括更改提示和参数微调，但两者各有优缺点。该论文关注于微调如何模拟提示的效果，从而结合两种方法的优势。

Method: 提出了一种元训练（meta-training）方法，使得通过梯度更新，微调能够模拟条件化（prompting）的效果。该方法利用语言模型自身的提示预测作为训练目标，无需外部真实标签，结合梯度型元学习技巧实现。

Result: 实验表明，通过这种元训练初始化后，微调能够恢复甚至达到提示模型的表现，尤其在“逆转诅咒”任务和文本快速问答等任务上取得明显提升。

Conclusion: 该方法表明，通过适当的初始化，基于梯度的微调可高度表达并接近提示的泛化能力，为长上下文建模和梯度学习的泛化能力研究开辟了新方向。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [25] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本研究提出了基于16PF人格模型和新控制框架，使大语言模型可以更细致、连续和真实地表达多维度个性特征，显著提升了其类人格的表现和调控能力，拓展了其在现实应用场景中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在人机交互中越来越被期待具备“类人”个性。然而，现有研究主要基于Big Five（OCEAN）人格结构，维度较粗糙且缺乏对人格特质强度的精细控制。

Method: 本文将Machine Personality Inventory（MPI）扩展为采用16 Personality Factor（16PF）模型，从而可细致控制16种不同的人格特质。提出了Specific Attribute Control（SAC）结构化框架，采用基于形容词的语义锚定，并结合行为问题量表（五种强度因子）来动态诱发和评价LLM的人格特质强度。

Result: 实验表明，将人格特质强度视为连续光谱（而非二元开关）能够使LLM展示出更一致、可控的人格表达。此外，特质强度的变化会以心理学上相关的方向影响相近的人格特质，说明LLM具备多维人格结构的内部化能力。

Conclusion: 本文方法提升了LLM人格表达的细致度和可控性，为健康医疗、教育、面试等领域实现更具人性化、细腻的社会型人工智能交互提供了新路径。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [26] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 本文提出面向印度财会领域的CA-Ben基准，评估六大主流LLM在会计、法律及量化推理上的能力。结果显示Claude 3.5 Sonnet和GPT-4o表现最佳，尤其在法律和概念推理上优于其他模型，但所有模型在数值和法律解释方面仍有明显不足，建议通过混合推理和增强检索方法加以改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在金融领域的应用越来越广泛，但尚不清楚它们是否能够有效理解和应用专业的金融知识，特别是在印度这样庞大且复杂的金融环境中。因此，亟需一个专门的基准来评估LLMs在金融、法律和量化推理方面的能力。

Method: 本文提出了CA-Ben基准数据集，通过整理印度注册会计师协会（ICAI）考试中的各类题目，涵盖基础、中级和高级课程阶段，生成结构化问答数据集。采用统一评测流程，对GPT-4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4六种主流LLMs进行评测。

Result: 评测结果显示，不同模型在金融、法律和量化推理任务中的表现存在明显差异，Claude 3.5 Sonnet和GPT-4o在概念性与法律推理方面表现优异。所有模型在数值计算和法律解释任务上均遇到较大挑战。

Conclusion: 当前LLMs在财会法律问答方面表现优劣不一，但在量化分析和法律解释上仍有突出短板。未来可通过混合推理和检索增强生成等方法提升LLMs在金融等高专业领域的应用能力。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [27] [Making Graphs Irregular through Irregularising Walks](https://arxiv.org/abs/2506.21254)
*Julien Bensmail,Romain Bourneuf,Paul Colinot,Samuel Humeau,Timothée Martinod*

Main category: cs.DM

TL;DR: 在1-2-3猜想已被证明的基础上，本文进一步探讨并行边需构成原图walk的情形下，将一般或特定类型的图变为局部不规则图时所需walk的性质及长度界，并给出了具体的结构和算法结果。


<details>
  <summary>Details</summary>
Motivation: 1-2-3猜想已被证明后，作者关注其在更严格限制条件下的推广，尤其是加入“添加的边必须构成原图walk”这一新限制，以探究更自然或实际场景下的问题复杂性。

Method: 在引入walk约束下，分析最短irregularising walk的结构特征，针对一般图与特殊类别，利用结构、组合和算法方法，得到不同情况下walk长度的上界、下界及相关性质。

Result: 提出了若干关于在walk约束下的结构、组合和算法性质；对一般图和特殊类型图，给出了最短irregularising walk长度的上、下界以及优化算法。

Conclusion: 论文在1-2-3猜想的基础上，研究了在额外“并行边需形成一条原图的walk”约束下，不同图及特殊图类中使图局部不规则所需walk的长度、结构、组合与算法性质。

Abstract: The 1-2-3 Conjecture, introduced by Karo\'nski, {\L}uczak, and Thomason in
2004, was recently solved by Keusch. This implies that, for any connected graph
$G$ different from $K_2$, we can turn $G$ into a locally irregular multigraph
$M(G)$, i.e., in which no two adjacent vertices have the same degree, by
replacing some of its edges with at most three parallel edges. In this work, we
introduce and study a restriction of this problem under the additional
constraint that edges added to $G$ to reach $M(G)$ must form a walk (i.e., a
path with possibly repeated edges and vertices) of $G$. We investigate the
general consequences of having this additional constraint, and provide several
results of different natures (structural, combinatorial, algorithmic) on the
length of the shortest irregularising walks, for general graphs and more
restricted classes.

</details>


### [28] [Playing Snake on a Graph](https://arxiv.org/abs/2506.21281)
*Denise Graafsma,Bodo Manthey,Alexander Skopalik*

Main category: cs.DM

TL;DR: 基于蛇游戏，作者研究了无向图上snake-winnable问题，其判定为NP-hard，并对特定类型的图给出了完全刻画。


<details>
  <summary>Details</summary>
Motivation: 探索蛇游戏在任意无向图上的玩法及性质，丰富经典游戏与图论的交叉研究。

Method: 定义了“snake-winnable”图，引入形式化模型，讨论在不同类型图（如网格图、二分图、连通度为1的图）下蛇能否覆盖所有顶点，并分析对应的计算复杂性。

Result: 证明判断一个图是否为snake-winnable是NP-hard问题（即使在网格图下仍然困难）；完整刻画了奇数阶二分图和连通度为1的图何时snake-winnable；指出所有哈密顿图都snake-winnable，且非哈密顿snake-winnable图的圈长最大为6，且此上界是严格的。

Conclusion: 该研究表明snake-winnable问题普遍具有高度计算复杂性，同时对特定图类进行了具体刻画，丰富了图论与游戏算法的交叉领域。

Abstract: Snake is a classic computer game, which has been around for decades. Based on
this game, we study the game of Snake on arbitrary undirected graphs. A snake
forms a simple path that has to move to an apple while avoiding colliding with
itself. When the snake reaches the apple, it grows longer, and a new apple
appears. A graph on which the snake has a strategy to keep eating apples until
it covers all the vertices of the graph is called snake-winnable. We prove that
determining whether a graph is snake-winnable is NP-hard, even when restricted
to grid graphs. We fully characterize snake-winnable graphs for odd-sized
bipartite graphs and graphs with vertex-connectivity 1. While Hamiltonian
graphs are always snake-winnable, we show that non-Hamiltonian snake-winnable
graphs have a girth of at most 6 and that this bound is tight.

</details>
