<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LO](#cs.LO) [Total: 7]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.DM](#cs.DM) [Total: 3]
- [cs.FL](#cs.FL) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop提出一种可编程语义约束解码框架，使语言模型生成满足复杂语义属性的代码，实现更高鲁棒性和系统性。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成代码但无法保证其正确性，现有约束解码方法仅限于表浅语法或依赖脆弱、特殊的语义编码，缺乏系统性。亟需面向丰富语义性质的可编程约束解码框架。

Method: ChopChop利用余归纳形式主义，将令牌级生成与抽象程序结构推理结合，将约束 enforcement 转化为正则协数据上的可实现性问题。实验在类型安全和程序等价性等约束下验证其通用性。

Result: ChopChop能生成形式上保证类型安全和程序等价等复杂语义属性的代码，将语义约束解码方法拓展为 LMs 的系统性扩展，并且提升了解码成功率且延迟可控。

Conclusion: ChopChop将形式化方法系统性地引入到语言模型代码生成中，实现了满足语义约束的安全、高效解码。它提升了不同模型和任务下的成功率，并保持了实用的解码延迟。

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [2] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: 本文提出了一种基于群作用的新语法和Hoare逻辑，专门用于程序对称性性质的验证，并开发了工具SymVerif，实验结果显示该工具能有效发现实际模型中的错误。


<details>
  <summary>Details</summary>
Motivation: 现有的形式化方法对对称性相关的程序正确性性质支持不足，因此需要新的方法来表达和验证这类性质。

Method: 设计了一套用于描述群作用（group actions）的语法，并提出了类似于Hoare逻辑的方法，将群作用用作前断言和后断言，从而能够验证程序的对称性性质。最后开发了名为SymVerif的原型工具，并通过一系列手工设计的基准测试验证其实用性。

Result: SymVerif工具成功验证了多个程序中的对称性性质，并且在一个动力系统模型中发现了此前未发现的错误。

Conclusion: 提出的方法和工具有效地支持了程序对称性性质的形式化验证，能够发现实际模型中的错误，展示了群作用逻辑及工具的实用性。

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [3] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: 该论文提出将3D打印G-code提升为点云立方体集合的算法，并通过实验证明其可以有效检查模型错误、对比不同切片器、评估网格修复工具，为3D打印工艺流程提供了实用的异常分析工具。


<details>
  <summary>Details</summary>
Motivation: 3D打印制备流程中的G-code及模型转换过程类比于编译器流程。目前传统编译器有较成熟的程序不变性检查、差分测试等提升可靠性的手段，但这些手段难以直接应用到3D打印模型及G-code表述的特殊领域。本研究旨在提升3D打印制造流程的可靠性，填补该领域缺乏有效分析手段的空白。

Method: 提出了一种新的算法，将G-code程序提升为一组立方体集合，并以近似点云表示用于高效操作。在该算法基础上，开发了原型工具GlitchFinder，可用于错误定位、不变性检测、切片器间定量比较及网格修复评估。

Result: 该算法通过GlitchFinder工具实现，并在58个真实CAD模型上进行测试。结果表明：GlitchFinder对由细小特征导致的切片问题尤为敏感、能够突出不同主流切片器处理同一模型的差异，并可检测常用网格修复工具在修复过程中引入的新错误。

Conclusion: 本研究提出的G-code提升与近似点云算法，为3D打印制备流程的错误分析与可靠性提升引入新思路。原型工具GlitchFinder实验证明其实用性，有助于自动化发现与诊断设计及切片过程中的问题。

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [4] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: 本文提出了一种支持常用字符串序列操作的新理论和求解器ostrichseq，并通过大量实验验证了其实用性和有效性，推动了SMT求解器在处理字符串相关程序中的能力。


<details>
  <summary>Details</summary>
Motivation: 字符串和序列在许多程序，特别是涉及字符串操作的程序中密切相关，但现有SMT求解器通常只支持通用的序列理论，对于实际常用的字符串序列操作支持不足，影响了对程序的建模和验证能力。

Method: 提出了一种字符串序列的新理论，并研究其可满足性问题。证明了该理论整体不可判定，但限制在straight-line片段时可判定。具体方法为：将字符串序列编码为字符串，相应的操作编码为字符串操作；结合自动机进行原像计算，并集成进OSTRICH字符串约束求解框架。最后实现为工具ostrichseq。

Result: 通过在真实JavaScript程序、手工模板和单元测试生成的基准约束上进行实验，结果验证了新方法的有效性。

Conclusion: 提出的字符串序列理论及其决策程序能够高效支持包含常用字符串序列操作的约束求解，推动了这类问题在实际程序中的建模和验证能力进步。

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [5] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 本文提出将精化类型的覆盖推理与不正确性逻辑结合，提升属性测试及程序分析工具的能力，并为程序验证与分析带来革新。


<details>
  <summary>Details</summary>
Motivation: 现有的函数式语言中的精化类型通常支持必须类型的近似推理，但需要更好地支持诸如完整性和安全性验证的场景，特别是在属性测试框架中，这类能力尤其重要。本文动机是更深入地探讨覆盖类型与近年来提出的不正确性逻辑推理框架之间的联系。

Method: 将覆盖类型的近似推理与不正确性逻辑推理框架相结合，探索其在表达性精化类型系统中的系统性集成机制，并分析对相关工具和编程实践的影响。

Result: 识别并论证了系统集成不正确性推理到精化类型系统的机制，为函数式编程、程序验证、程序分析及相关工具提供了新的理论基础和机会。

Conclusion: 将不正确性推理与覆盖类型结合，为程序分析、验证及属性测试等领域提供更强大、灵活的支持，推动相关理论与工具发展。

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [6] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 提出了一种类型理论框架，利用符号正则表达式和有限自动机，系统化地对效应型库API的不正确行为进行可组合分析和类型推断。


<details>
  <summary>Details</summary>
Motivation: 在函数式编程中，程序和带副作用的库API进行交互时，如果库API实现不透明，开发者很难对其不正确的行为进行系统化分析，因此亟需一种框架可以对其错误行为进行类型层面的推理。

Method: 作者提出了一种基于类型理论的框架，通过追踪和分析库API的调用序列（即trace），并用符号正则表达式（SREs）进行表示，能够形式化描述跨函数边界的不正确抽象数据类型（ADT）行为。核心技术是引入一种新的类型推断算法，该算法能够根据指定的不正确性性质并利用符号有限自动机（SFAs）对trace进行组合推理。

Result: 这种类型推断算法在成功时，可以推断出ADT实现会表现出一些特定的不正确行为类型，从而实现了对trace基础的不正确性规格的系统化欠近似推理。

Conclusion: 本工作首次系统性地实现了针对trace基础的不正确性规格的欠近似类型推断，提供了一种新的可组合的trace引导分析方法。

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards](https://arxiv.org/abs/2509.00140)
*Songhui Yue*

Main category: cs.SE

TL;DR: 本文提出一种结合大语言模型的本体自动生成方法，在软件工程标准这一复杂场景中实现了优于传统方法的三元组抽取与本体构建效果。


<details>
  <summary>Details</summary>
Motivation: 本论文关注自动本体生成（AOG）在知识表达及推理中的关键作用，尤其是针对软件工程标准这类包含大量非结构化、含噪声的专业文本，现有自动处理方法存在精准与扩展性不足的问题。

Method: 提出一种以开源大语言模型（LLM）为辅助的关系三元组抽取（RTE）方法，结合文档分段、候选术语挖掘、基于LLM的关系推理、术语归一化以及跨章节对齐的完整AOG流程，不仅依赖提示工程，还充分利用LLM在本体构建过程中的能力。

Result: 构建了三个粒度的黄金标准基准集，并用其评估所生成本体，结果显示该方法的本体生成效果与OpenIE三元组抽取法相当，甚至在某些方面优于OpenIE。

Conclusion: 基于LLM的本体自动生成流程在处理软件工程标准等复杂专业文档时表现出很大潜力，为领域本体构建和知识表达提供了更优方案。

Abstract: Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.

</details>


### [8] [LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers](https://arxiv.org/abs/2509.00256)
*Yutong Wang,Cindy Rubio-González*

Main category: cs.SE

TL;DR: LLM4FP通过LLM自动生成浮点程序，有效提升了跨编译器数值不一致性的发现率和范围，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 编译器间的浮点数值不一致会影响数值软件的可靠性，现有工具检测能力有限，需要新的自动化方法增强不一致性检测。

Method: 采用基于语法的生成和基于反馈的变异方法，利用大型语言模型自动生成多样且有效的浮点程序，以激发编译器间的不一致性。

Result: LLM4FP检测到的不一致性数量超过当前最先进工具Varity的两倍，并且覆盖更多优化级别，发现主机和设备编译器间最多的不一致情况，大多数检测结果为实值差异而非极端值。

Conclusion: LLM4FP框架通过LLM生成浮点程序，能显著提升编译器之间数值不一致性的检测效果。

Abstract: Floating-point inconsistencies across compilers can undermine the reliability
of numerical software. We present LLM4FP, the first framework that uses Large
Language Models (LLMs) to generate floating-point programs specifically
designed to trigger such inconsistencies. LLM4FP combines Grammar-Based
Generation and Feedback-Based Mutation to produce diverse and valid programs.
We evaluate LLM4FP across multiple compilers and optimization levels, measuring
inconsistency rate, time cost, and program diversity. LLM4FP detects over twice
as many inconsistencies compared to the state-of-the-art tool, Varity. Notably,
most of the inconsistencies involve real-valued differences, rather than
extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies
across a wider range of optimization levels, and finds the most mismatches
between host and device compilers. These results show that LLM-guided program
generation improves the detection of numerical inconsistencies.

</details>


### [9] [JS-TOD: Detecting Order-Dependent Flaky Tests in Jest](https://arxiv.org/abs/2509.00466)
*Negar Hashemi,Amjed Tahir,Shawn Rasheed,August Shi,Rachel Blagojevic*

Main category: cs.SE

TL;DR: JS-TOD可以自动检测Jest测试中的顺序依赖型flakiness，并指出主要成因（共享文件和模拟状态），有助于提升测试可靠性。


<details>
  <summary>Details</summary>
Motivation: 测试中的顺序依赖会导致测试不稳定（flakiness），而现实中大量测试并不能保证顺序无关。因此需要一个工具来检测JavaScript环境下的测试顺序依赖问题。

Method: 开发了JS-TOD工具，可以提取、重排、并重新运行Jest测试，有系统地随机化测试、测试套件及describe块的执行顺序，实现对测试顺序依赖的检测。工具还支持自定义重排和重跑的次数。

Result: 通过实验，JS-TOD发现导致顺序依赖型测试不稳定的两个主要原因：测试间共享文件和共享模拟（mocking）状态。

Conclusion: JS-TOD能够有效检测Jest测试中的顺序依赖问题，为改进测试稳定性、识别flaky tests提供关键手段。

Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that
can extract, reorder, and rerun Jest tests to reveal possible order-dependent
test flakiness. Test order dependency is one of the leading causes of test
flakiness. Ideally, each test should operate in isolation and yield consistent
results no matter the sequence in which tests are run. However, in practice,
test outcomes can vary depending on their execution order. JS-TOD employed a
systematic approach to randomising tests, test suites, and describe blocks. The
tool is highly customisable, as one can set the number of orders and reruns
required (the default setting is 10 reorder and 10 reruns for each test and
test suite). Our evaluation using JS-TOD reveals two main causes of test order
dependency flakiness: shared files and shared mocking state between tests.

</details>


### [10] [Bug Whispering: Towards Audio Bug Reporting](https://arxiv.org/abs/2509.00785)
*Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 论文提出了通过让终端用户录音报告bug的方法，这有望增加bug收集数量，但音频报告在分析与重现方面存在特殊挑战，呼吁进一步研究音频bug报告相关技术。


<details>
  <summary>Details</summary>
Motivation: 目前移动应用依赖用户提交bug报告以获取未覆盖的故障信息，但现有文本报告形式可能限制了用户的提交积极性数量。该论文提出通过录音的方式，让用户能更便捷地报告遇到的问题，从而有望提升bug报告数量和质量。

Method: 论文进行了初步实验，探索了基于音频消息的bug报告在收集和分析过程中的特点以及面临的挑战，并讨论了这些挑战。

Result: 音频bug报告虽然易于实现并可能提升报告数量，但因其特殊性，现有的bug重现和分析技术难以有效处理音频报告，带来了新的技术挑战。

Conclusion: 音频报告为移动应用bug收集带来新机遇，但也暴露出当前自动重现和分析工具的局限性。论文呼吁对音频报告的收集与分析开展进一步研究。

Abstract: Bug reporting is a key feature of mobile applications, as it enables
developers to collect information about faults that escaped testing and thus
affected end-users. This paper explores the idea of allowing end-users to
immediately report the problems that they experience by recording and
submitting audio messages. Audio recording is simple to implement and has the
potential to increase the number of bug reports that development teams can
gather, thus potentially improving the rate at which bugs are identified and
fixed. However, audio bug reports exhibit specific characteristics that
challenge existing techniques for reproducing bugs. This paper discusses these
challenges based on a preliminary experiment, and motivates further research on
the collection and analysis of audio-based bug reports

</details>


### [11] [REConnect: Participatory RE that Matters](https://arxiv.org/abs/2509.01006)
*Daniela Damian,Bachan Ghimire,Ze Shi Li*

Main category: cs.SE

TL;DR: REConnect框架强调了把人与人的联系作为需求工程核心，通过信任、协同设计和赋权，确保系统需求切实反映用户真实经验和价值观，并在生成式AI时代继续维护和强化人的主导地位。


<details>
  <summary>Details</summary>
Motivation: 现有如CrowdRE和AI辅助等需求工程方法，容易脱离实际用户的文化、社会和政治背景，导致需求结果不符合真实人性和多样化社会需求。

Method: 提出REConnect框架，以三个案例（尼泊尔BloodSync、加拿大Herluma、加拿大北极BridgingRoots）为支撑，提炼三大原则（建立信任、协同设计、用户赋权），并提出一系列可操作的实践方法嵌入需求工程全过程。

Result: 基于REConnect方法开发的系统，其需求更能体现文化基础、社会合法性和交付后的可持续性；涉及的三项案例均展示了以人为本的参与式需求工程实践可实现更高影响力。

Conclusion: REConnect方法能够更好地将需求工程和人类价值观、文化、社会以及政治背景相结合，强化需求工程的社会合法性和可持续性。要让AI辅助的需求开发真正有益，必须坚持以人为本和持续实时的利益相关方参与。

Abstract: Software increasingly shapes the infrastructures of daily life, making
requirements engineering (RE) central to ensuring that systems align with human
values and lived experiences. Yet, current popular practices such as CrowdRE
and AI-assisted elicitation strategies risk detaching requirements work from
the cultural, social, and political contexts that shape lived experiences,
human values, and real user needs. In this paper, we introduce REConnect that
re-centers RE on the human connection as central to the understanding of lived
experiences where impact is sought. REConnect advocates for a human-centered
participatory approach "that matters" to the communities and beneficiaries
involved, ensuring alignment with their values and aspirations. Drawing on
three case studies of societal impact: BloodSync in rural Nepal, Herluma
supporting women at risk of homelessness in Canada, and BridgingRoots to
revitalize Indigenous languages in the Canadian Arctic. REConnect argues that
three key principles and enablers: building trusting relationships,
co-designing with and alongside stakeholders, and empowering users as agents of
change, can yield requirements that are culturally grounded, socially
legitimate, and sustainable beyond system delivery. REConnect also proposes a
set of actionable practices (REActions) that embed relationality and ongoing
stakeholder engagement throughout requirements elicitation, analysis, and
validation of solution development. Finally, we situate REConnect in the era of
Generative AI. While AI can accelerate and scale certain RE tasks, its
integration must be guided by participatory practices that not only preserve
human agency but also empower humans' roles to become guardians of values and
ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in
iterative review cycles.

</details>


### [12] [Generative Goal Modeling](https://arxiv.org/abs/2509.01048)
*Ateeq Sharfuddin,Travis Breaux*

Main category: cs.SE

TL;DR: 本研究提出利用GPT-4o自动从访谈记录抽取并建模需求目标。在多领域测试中，目标识别率62%，原文追溯率98.7%，目标关系建模准确率72.2%。显示大模型可提升需求分析自动化，但目标关系建模还有提升空间。


<details>
  <summary>Details</summary>
Motivation: 软件工程中，基于访谈等方式收集需求后，分析师需要从访谈记录中识别和建模需求目标。现有方法对目标抽取与建模依赖人工识别，效率较低，且主观性较强。本研究希望通过自动化手段提升目标抽取与建模的效率和客观性。

Method: 提出了一种利用文本蕴涵技术，从访谈记录自动抽取目标并构建目标模型的方法。具体评估了GPT-4o对访谈记录的处理能力，包括目标识别与追溯原文、以及目标间关系建模准确性。

Result: GPT-4o可自动识别62.0%的人工抽取目标，目标追溯原文准确率达98.7%，抽取目标间细化关系准确率为72.2%。算法在29个应用领域的15份访谈记录中进行了测试。

Conclusion: GPT-4o等大模型可以较为可靠地从访谈记录自动抽取目标并构建目标模型，显著提升了需求分析自动化和标准化水平。目标与原文关联追溯能力极强，但目标关系建模仍有改进空间。适于广泛应用于需求分析流程。

Abstract: In software engineering, requirements may be acquired from stakeholders
through elicitation methods, such as interviews, observational studies, and
focus groups. When supporting acquisition from interviews, business analysts
must review transcripts to identify and document requirements. Goal modeling is
a popular technique for representing early stakeholder requirements as it lends
itself to various analyses, including refinement to map high-level goals into
software operations, and conflict and obstacle analysis. In this paper, we
describe an approach to use textual entailment to reliably extract goals from
interview transcripts and to construct goal models. The approach has been
evaluated on 15 interview transcripts across 29 application domains. The
findings show that GPT-4o can reliably extract goals from interview
transcripts, matching 62.0% of goals acquired by humans from the same
transcripts, and that GPT-4o can trace goals to originating text in the
transcript with 98.7% accuracy. In addition, when evaluated by human
annotators, GPT-4o generates goal model refinement relationships among
extracted goals with 72.2% accuracy.

</details>


### [13] [A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps](https://arxiv.org/abs/2509.01068)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: 本研究系统梳理了移动应用自动化需求获取与分析领域的主流技术和工具，发现半自动化、开源和非自主开发的工具应用最广，主要支持分析、挖掘、分类等需求工程任务。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用软件大量涌现，需求获取与分析成为软件工程中的核心环节。自动化需求获取与分析技术不断发展，但相关技术、工具及它们支持的需求工程任务等方面的知识尚不清晰。为此，需要对该领域当前的研究进展进行全面梳理和归纳。

Method: 本研究采用了Kitchenham等人提出的系统性映射研究方法，对自动化移动应用需求获取与分析相关文献进行了系统梳理，并基于筛选出的73篇论文，分析了常用技术、工具及所支持的需求工程任务。

Result: 结果显示：常用技术以半自动技术为主，常见工具具有开源和非自主开发的特点，主要应用于需求分析和文本预处理任务。此外，需求分析、挖掘与分类是获得最广泛技术支持的三大需求工程任务。

Conclusion: （1）自动化需求获取与分析技术和工具在移动应用领域呈增长趋势；（2）相关文献以半自动化技术应用居多；（3）需求分析、挖掘和分类是自动化技术支持最多的任务；（4）主流工具多为开源且非自研，主要用于需求分析和文本处理。

Abstract: [Background:] Research on automated requirements elicitation and analysis of
mobile apps employed lots of techniques and tools proposed by RE researchers
and practitioners. However, little is known about the characteristics of these
techniques and tools as well as the RE tasks in requirements elicitation and
analysis that got supported with the help of respective techniques and tools.
[Aims:] The goal of this paper is to investigate the state-of-the-art of the
techniques and tools used in automated requirements elicitation and analysis of
mobile apps. [Method:] We carried out a systematic mapping study by following
the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we
found the most frequently used techniques - semi-automatic techniques, and the
main characteristics of the tools - open-sourced and non-self-developed tools
for requirements analysis and text pre-processing. Plus, the most three
investigated RE tasks are requirements analysis, mining and classification.
[Conclusions:] Our most important conclusions are: (1) there is a growth in the
use of techniques and tools in automated requirements elicitation and analysis
of mobile apps, (2) semi-automatic techniques are mainly used in the
publications on this research topic, (3) requirements analysis, mining and
classification are the top three RE tasks with the support of automatic
techniques and tools, and (4) the most popular tools are open-sourced and
non-self-developed, and they are mainly used in requirements analysis and text
processing.

</details>


### [14] [Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound](https://arxiv.org/abs/2509.01149)
*Hui Zeng,Zhihao Xu,Hui Li,Siwen Wang,Qian Ma*

Main category: cs.SE

TL;DR: Lin-Hunter通过智能多样化测试生成和动态优化策略，实现了更强的FPGA综合工具质量保障，并实际挖掘到新缺陷，优于现有测试方法。


<details>
  <summary>Details</summary>
Motivation: 现有的FPGA逻辑综合工具测试方法大多依赖随机选择策略，导致生成的HDL测试用例结构单一，无法充分覆盖工具的功能空间，进而影响缺陷挖掘效率。

Method: 提出Lin-Hunter框架：1）引入系统化的变形规则，自动生成功能等价但结构多样的HDL测试用例；2）集成基于LinUCB的自适应策略选择机制，根据历史测试反馈动态优先采用更可能触发bug的变换策略。

Result: Lin-Hunter在三个月时间内发现了18个独特bug，包括10个官方未公开的缺陷，经开发者确认。此外，在测试用例多样性和bug发现效率上均优于现有主流工具。

Conclusion: Lin-Hunter通过结构多样性生成和自适应策略，有效提升了FPGA逻辑综合工具的测试质量和缺陷发现效率，证明了其实用价值。

Abstract: Field-Programmable Gate Arrays (FPGAs) play an indispensable role in
Electronic Design Automation (EDA), translating Register-Transfer Level (RTL)
designs into gate-level netlists. The correctness and reliability of FPGA logic
synthesis tools are critically important, as unnoticed bugs in these tools may
infect the final hardware implementations. However, recent approaches often
rely heavily on random selection strategies, limiting the structural diversity
of the generated HDL test cases and resulting in inadequate exploration of the
tool's feature space. To address this limitation, we propose Lin-Hunter, a
novel testing framework designed to systematically enhance the diversity of HDL
test cases and the efficiency of FPGA logic synthesis tool validation.
Specifically, Lin-Hunter introduces a principled set of metamorphic
transformation rules to generate functionally equivalent yet structurally
diverse HDL test case variants, effectively addressing the limited diversity of
existing test inputs. To further enhance bug discovery efficiency, Lin-Hunter
integrates an adaptive strategy selection mechanism based on the Linear Upper
Confidence Bound (LinUCB) method. This method leverages feedback from synthesis
logs of previously executed test cases to dynamically prioritize transformation
strategies that have empirically demonstrated a higher likelihood of triggering
synthesis bugs. Comprehensive experiments conducted over a three-month period
demonstrate the practical effectiveness of Lin-Hunter. Our method has
discovered 18 unique bugs, including 10 previously unreported defects, which
have been confirmed by official developers. Moreover, our method outperforms
state-of-the-art testing methods in both test-case diversity and bug-discovery
efficiency.

</details>


### [15] [Policy-driven Software Bill of Materials on GitHub: An Empirical Study](https://arxiv.org/abs/2509.01255)
*Oleksii Novikov,Davide Fucci,Oleksandr Adamov,Daniel Mendez*

Main category: cs.SE

TL;DR: 开源项目中实际按安全合规目标生成和应用的SBOM极其稀少，SBOM展示了依赖存在的广泛漏洞和许可信息缺失，现状难以充分支持安全与合规。


<details>
  <summary>Details</summary>
Motivation: 尽管政府出台了SBOM相关规定以加强软件供应链安全，并推动其在业界使用，但目前关于SBOM的学术研究尚不充分。特别是针对为达成安全目标（如提升透明度和合规性）而实际使用的SBOM的应用现状及其问题需要深入研究。

Method: 采用了软件仓库挖掘方法，从GitHub平台收集并筛选出与政策相关的SBOM文件，并用描述性统计分析这些SBOM所申报的依赖信息及其相关的安全漏洞和许可情况。

Result: 在流行的GitHub开源项目中，只有极少数（0.56%）包含政策驱动的SBOM。这些SBOM申报的依赖共涉及2202个独特漏洞，其中22%的依赖未报告许可证信息，反映出SBOM在安全和合规用途中存在不足。

Conclusion: 本研究发现，尽管政策推动和法规要求，实际在流行的GitHub开源项目中，真正为安全政策目的生成并应用的SBOM文件占比非常低（仅0.56%）。SBOM中的依赖项存在大量已知漏洞，并且有相当比例缺失许可信息，表明当前SBOM在支持安全评估和合规方面还有显著不足。

Abstract: Background. The Software Bill of Materials (SBOM) is a machine-readable list
of all the software dependencies included in a software. SBOM emerged as way to
assist securing the software supply chain. However, despite mandates from
governments to use SBOM, research on this artifact is still in its early
stages. Aims. We want to understand the current state of SBOM in open-source
projects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to
achieve security goals, such as enhancing project transparency and ensuring
compliance, rather than being used as fixtures for tools or artificially
generated for benchmarking or academic research purposes. Method. We performed
a mining software repository study to collect and carefully select SBOM files
hosted on GitHub. We analyzed the information reported in policy-driven SBOMs
and the vulnerabilities associated with the declared dependencies by means of
descriptive statistics. Results. We show that only 0.56% of popular GitHub
repositories contain policy-driven SBOM. The declared dependencies contain
2,202 unique vulnerabilities, while 22% of them do not report licensing
information. Conclusion. Our findings provide insights for SBOM usage to
support security assessment and licensing.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Undecidability of Linear Logics without Weakening](https://arxiv.org/abs/2509.00644)
*Jun Suzuki,Katsuhiko Sano*

Main category: cs.LO

TL;DR: 本文研究了线性逻辑的两个变种系统，CLLR和CLLRR，均通过去除某些规则（weakening和单位元）来探讨系统的可判定性。结果证明这两种系统的推理序列可证性依然是不可判定的，为相关逻辑系统的复杂性和不可判定性提供了新证据。


<details>
  <summary>Details</summary>
Motivation: 线性逻辑的判定性问题一直是逻辑和计算理论的重要研究内容，其中经典命题线性逻辑（CLL）已被证明为不可判定。已有变体通过削弱或限制外延规则（比如weakening）试图探索系统是否变得可判定，然而针对移除指数模态的weakening规则后的系统（如CLLR和CLLRR），其判定性仍未有定论，因此作者试图解决此疑问。

Method: 作者首先定义了两个新的逻辑系统CLLR和CLLRR，分别通过在CLL中去除指数模态的weakening规则，以及进一步去除单位元（1和⊥）。对于CLLR，作者通过巧妙利用单位元来模拟weakening，并将不可判定性归约到原CLL的不可判定性上。对于CLLRR，作者则通过证明该系统能模拟任意Minsky两计数器机，建立其不可判定性。

Result: 作者证明了在不带weakening的CLLR系统中，判定一个推理序列是否可证依然是不可判定的。进一步地，在移除单位元后的CLLRR系统中，也同样不可判定。

Conclusion: 无论是仅移除指数模态的weakening规则，还是进一步剥去单位元，两个变体系统CLLR和CLLRR的可判定性问题均仍不可判定。这说明weakening和单位元并非不可判定性的唯一来源，线性逻辑本身机制导致其复杂性仍然很高。

Abstract: The goal of this paper is to establish that it remains undecidable whether a
sequent is provable in two systems in which a weakening rule for an exponential
modality is completely omitted from classical propositional linear logic
$\mathbf{CLL}$ introduced by Girard (1987), which is shown to be undecidable by
Lincoln et al. (1992). We introduce two logical systems, $\mathbf{CLLR}$ and
$\mathbf{CLLRR}$. The first system, $\mathbf{CLLR}$, is obtained by omitting
the weakening rule for the exponential modality of $\mathbf{CLL}$. The system
$\mathbf{CLLR}$ has been studied by several authors, including
Meli\`es-Tabareau (2010), but its undecidability was unknown. This paper shows
the undecidability of $\mathbf{CLLR}$ by reducing it to the undecidability of
$\mathbf{CLL}$, where the units $\mathbf{1}$ and $\bot$ play a crucial role in
simulating the weakening rule. We also omit these units from the syntax and
inference rules of $\mathbf{CLLR}$ in order to define the second system,
$\mathbf{CLLRR}$. The undecidability of $\mathbf{CLLRR}$ is established by
showing that the system can simulate any two-counter machine proposed by Minsky
(1961).

</details>


### [17] [Formal Verification of Isothermal Chemical Reactors](https://arxiv.org/abs/2509.01130)
*Parivash Feyzishendi,Sophia Hamer,Jinyu Huang,Tyler R. Josephson*

Main category: cs.LO

TL;DR: 本论文提出利用微分动态逻辑与KeYmaera X对化学反应器状态可达性进行自动定理证明，能为部分动力学体系提供符号化安全保证，但在复杂系统下应用仍有限。


<details>
  <summary>Details</summary>
Motivation: 化学反应器作为动态系统，其安全性、合规性和经济性依赖于能否到达特定状态，传统方法主要采用数值模拟。该论文希望通过符号方法提供更严格的数学保证。

Method: 采用微分动态逻辑（dL）并利用自动化定理证明器KeYmaera X，对化学反应器的可达性进行符号化分析，首先针对可解析求解闭式解系统，再扩展至更复杂的动力学体系。

Result: 对于简单的反应体系，如一阶反应批式反应器，作者证明出输出浓度不会超过相关阈值。对于更复杂如米氏动力学体系，同样能够利用dL方法给出证明，但CSTR等复杂网络由于难以找到有用不变量，证明能力受限。同时，dL得出的界限较宽泛，不如数值方法精细。

Conclusion: 微分动态逻辑和KeYmaera X工具能为化学反应器可达性问题提供符号化的数学保证，尤其适合于某些具备可辨识不变量的系统。但方法目前在处理复杂网络和收敛性方面有限，其实用性暂不能全面替代数值分析。

Abstract: Chemical reactors are dynamic systems that can be described by systems of
ordinary differential equations (ODEs). Reactor safety, regulatory compliance,
and economics depend on whether certain states are reachable by the reactor,
and are generally assessed using numerical simulation. In this work, we show
how differential dynamic logic (dL), as implemented in the automated theorem
prover KeYmaera X, can be used to symbolically determine reachability in
isothermal chemical reactors, providing mathematical guarantees that certain
conditions are satisfied (for example, that an outlet concentration never
exceeds a regulatory threshold). First, we apply dL to systems whose dynamics
can be solved in closed form, such as first-order reactions in batch reactors,
proving that such reactors cannot exceed specified concentration limits. We
extend this method to reaction models as complex as Michaelis-Menten kinetics,
whose dynamics require approximations or numerical solutions. In all cases,
proofs are facilitated by identification of invariants; we find that
conservation of mass is both a principle proved from the ODEs describing mass
action kinetics as well as a useful relationship for proving other properties.
Useful invariants for continuous stirred tank reactors (CSTRs) were not found,
which limited the complexity of reaction networks that could be proved with dL.
While dL provides an interesting symbolic logic approach for reachability in
chemical reactions, the bounds we obtained are quite broad relative to those
typically achieved via numerical reachability analyses.

</details>


### [18] [Quantum Petri Nets with Event Structure semantics](https://arxiv.org/abs/2509.01423)
*Julien Saan Joachim,Marc de Visme,Stefan Haar*

Main category: cs.LO

TL;DR: 本文提出了具有严密语义和展开理论的量子Petri网（QPNs）模型，为量子并发建模提供了理论框架，并建立了与量子编程的联系，弥补了量子并发理论的空白。


<details>
  <summary>Details</summary>
Motivation: 古典Petri网为并发建模提供了标准工具，但在量子并发领域缺乏相应的理论框架，现有的“量子Petri网”缺乏严谨的并发和量子语义、分析工具及展开理论。因此，需要开发一个有语义基础的量子并发建模框架。

Method: 首先，提出了与量子事件结构兼容的量子发生网（LQONs）的局部定义。其次，构建了具有良定义展开语义的QPNs。最后，建立了QPNs的组合性框架，实现了Petri网理论与量子程序设计之间的桥梁。

Result: 论文提出了量子Petri网（QPNs），并给出了局部量子发生网、具有良好展开语义的QPNs构建方法和组合性框架，完善了量子并发的理论基础，推动了量子Petri网理论与量子编程的结合。

Conclusion: 本论文建立了量子Petri网（QPNs）的理论基础，为量子并发建模提供了严谨的语义、分析工具以及展开理论，成功将Petri网理论扩展到量子计算领域。

Abstract: Classical Petri nets provide a canonical model of concurrency, with unfolding
semantics linking nets, occurrence nets, and event structures. No comparable
framework exists for quantum concurrency: existing ''quantum Petri nets'' lack
rigorous concurrent and sound quantum semantics, analysis tools, and unfolding
theory. We introduce Quantum Petri Nets (QPNs), Petri nets equipped with a
quantum valuation compatible with the quantum event structure semantics of
Clairambault, De Visme, and Winskel (2019). Our contributions are: (i) a local
definition of Quantum Occurrence Nets (LQONs) compatible with quantum event
structures, (ii) a construction of QPNs with a well-defined unfolding
semantics, (iii) a compositional framework for QPNs. This establishes a
semantically well grounded model of quantum concurrency, bridging Petri net
theory and quantum programming.

</details>


### [19] [TREBL -- A Relative Complete Temporal Event-B Logic. Part I: Theory](https://arxiv.org/abs/2509.01462)
*Klaus-Dieter Schewe,Flavio Ferrarotti,Peter Rivière,Neeraj Kumar Singh,Guillaume Dupont,Yamine Aït Ameur*

Main category: cs.LO

TL;DR: 本文扩展了 Event-B 逻辑，提出片段 TREBL 及对应推理体系，用于表达和验证活性条件，并证明了推理体系的相对完备性和实际可行性，为安全等领域的活性验证提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 验证活性条件（系统是否能持续进展）在 Event-B 等基于状态的形式方法中是关键问题，但现有逻辑难以直接表达和验证轨迹上的属性，因此需要扩展表达能力。

Method: 将 Event-B 逻辑扩展为能在状态之上表达轨迹属性的新逻辑，并限定出片段TREBL，提出针对 TREBL 的推理规则；证明这些规则具有相对完备性，即对于每个有效蕴含，都存在导出证明（前提是机器充分细化且可定义变体项）。通过案例（主要为安全领域）阐述方法。

Result: 提出的 TREBL 逻辑片段能表达所有相关活性条件，为其给出了可靠的推理规则。这些规则被证明是相对完备的，并论证了所需的机器细化总能实现。理论通过安全领域的例子得到了说明。

Conclusion: 本文扩展了 Event-B 逻辑，提出了专门用于活性条件（如安全领域问题）表达和验证的方法，并证明了其实用性和理论可行性，为形式化方法的活性验证提供了完整的工具链。

Abstract: The verification of liveness conditions is an important aspect of state-based
rigorous methods. This article addresses the extension of the logic of Event-B
to a powerful logic, in which properties of traces of an Event-B machine can be
expressed. However, all formulae of this logic are still interpreted over
states of an Event-B machine rather than traces. The logic exploits that for an
Event-B machine $M$ a state $S$ determines all traces of $M$ starting in $S$.
We identify a fragment called TREBL of this logic, in which all liveness
conditions of interest can be expressed, and define a set of sound derivation
rules for the fragment. We further show relative completeness of these
derivation rules in the sense that for every valid entailment of a formula
$\varphi$ one can find a derivation, provided the machine $M$ is sufficiently
refined. The decisive property is that certain variant terms must be definable
in the refined machine. We show that such refinements always exist. Throughout
the article several examples from the field of security are used to illustrate
the theory.

</details>


### [20] [An Information-Flow Perspective on Explainability Requirements: Specification and Verification](https://arxiv.org/abs/2509.01479)
*Bernd Finkbeiner,Hadar Frenkel,Julian Siber*

Main category: cs.LO

TL;DR: 作者提出一种基于扩展认知时序逻辑的新方法，能够统一形式化建模和验证系统的可解释性与隐私需求，通过算法和原型验证展示其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和自动化系统的普及，系统的可解释性和隐私保护成为重要需求。作者指出可解释性本质上是一种正向信息流，需要与可能导致隐私泄露的负向信息流进行平衡。现有方法难以同时对可解释性与隐私进行形式化建模和验证。

Method: 提出将具备计数反事实因果量化的认知时序逻辑应用于多智能体系统，能形式化地指定系统需暴露哪些信息以让代理获得为何某些效果发生的知识。作者还提出了一种算法，用于验证有限状态模型是否满足这些可解释性规范，并开发了原型实现。

Result: 作者通过多个基准示例，展示了该方法如何区分可解释和不可解释的系统，并可灵活添加隐私保护需求，有效实现了可解释性与隐私的统一形式化分析。

Conclusion: 作者证明了基于扩展认知时序逻辑的方法能够统一指定和验证系统的可解释性与隐私需求，为多智能体系统的现实部署提供了形式化保障。该方法能区分可解释和不可解释系统，并灵活支持隐私增强。

Abstract: Explainable systems expose information about why certain observed effects are
happening to the agents interacting with them. We argue that this constitutes a
positive flow of information that needs to be specified, verified, and balanced
against negative information flow that may, e.g., violate privacy guarantees.
Since both explainability and privacy require reasoning about knowledge, we
tackle these tasks with epistemic temporal logic extended with quantification
over counterfactual causes. This allows us to specify that a multi-agent system
exposes enough information such that agents acquire knowledge on why some
effect occurred. We show how this principle can be used to specify
explainability as a system-level requirement and provide an algorithm for
checking finite-state models against such specifications. We present a
prototype implementation of the algorithm and evaluate it on several
benchmarks, illustrating how our approach distinguishes between explainable and
unexplainable systems, and how it allows to pose additional privacy
requirements.

</details>


### [21] [Derivation and Verification of Array Sorting by Merging, and its Certification in Dafny](https://arxiv.org/abs/2509.01758)
*Juan Pablo Carbonell,José E. Solsona,Nora Szasz,Álvaro Tasistro*

Main category: cs.LO

TL;DR: 本文提出一种基于Dafny的分治算法验证模式，完整证明了递归与迭代归并排序的正确性，并可推广到快排等其他算法。


<details>
  <summary>Details</summary>
Motivation: 归并排序和类似分治算法广泛应用，但其正确性证明和验证较为复杂。该研究旨在构建通用分治算法模式，通过形式化手段降低复杂性，并在自动化验证工具Dafny上实现完整证明流程。

Method: 首先，论文提出了基于数组前置与后置条件（pre/post-condition）的分治式算法模式(schema)，然后将归并排序和归并过程实例化为该模式，得到了递归实现；随后分析分治问题树，设计循环不变式，将递归算法转化为无栈的迭代算法；最后，利用这些模式在Dafny里进行自动化形式化验证。

Result: 成功在Dafny上形式化并认证了递归和迭代两种归并排序实现，并为类似分治算法提供了通用的设计和验证模式框架。为快排等算法的形式化验证提供了可行路径。

Conclusion: 论文为两种归并排序算法在Dafny语言下实现了全过程的正确性证明，并展示了一个可推广的方法论。该方法同样适用于快排等分治算法的形式化验证。

Abstract: We provide full certifications of two versions of merge sort of arrays in the
verification-aware programming language Dafny. We start by considering schemas
for applying the divide-and-conquer or partition method of solution to
specifications given by pre- and post-conditions involving linear arrays. We
then derive the merge sort and merging algorithms as instances of these
schemas, thereby arriving at a fully recursive formulation. Further, the
analysis of the tree of subproblems arising from the partition facilitates the
design of loop invariants that allow to derive a fully iterative version
(sometimes called bottom-up merge sort) that does not employ a stack. We show
how the use of the provided schemas conveniently conducts the formalization and
actual verification in Dafny. The whole method is also applicable to deriving
variants of quicksort, which we sketch.

</details>


### [22] [Probabilistically stable revision and comparative probability: a representation theorem and applications](https://arxiv.org/abs/2509.02495)
*Krzysztof Mierzewski*

Main category: cs.LO

TL;DR: 本文提出并刻画了基于概率性稳定性的信念修正逻辑，获得了表示定理及多个一般理论结果，拓展了信念修正方法及其在逻辑、博弈和经济偏好分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 本文动机是针对现有的信念修正理论，尤其是AGM信念修正理论在刻画概率性稳定信念（即对命题持续保持高度信任）方面的不足，发展一种新的概率性稳定信念修正框架。作者希望更好地解释理性体在Bayesian更新和全有或全无式信念采纳间的关系。

Method: 作者首先利用可比概率序理论，提出并刻画一类概率性稳定信念修正算子。这些算子基于理性体对命题概率稳定度的要求，并通过数学证明给出了这些算子的刻画表征。论文还提出了逻辑选择函数语义，并提供了比、严格比和非严格比的序的联合表征条件。

Result: 作者得到一组概率性稳定信念修正算子的完整刻画，给出了概率空间内最强稳定集算子的判据；获得了新的逻辑选择函数语义；证明了与可比概率序相关的两个一般定理，并将这些结果应用到简单投票博弈与偏好理论。所得到的概率性稳定信念修正逻辑表现出强单调性，但不满足AGM修正公理，只满足非常弱的案例推理。

Conclusion: 本文丰富了信念修正理论，提出了一种区别于传统AGM理论的概率性稳定信念修正方式，在比较概率、理性选择与度量理论等方面提供了新工具，并具有实际应用前景。

Abstract: The stability rule for belief, advocated by Leitgeb [Annals of Pure and
Applied Logic 164, 2013], is a rule for rational acceptance that captures
categorical belief in terms of $\textit{probabilistically stable
propositions}$: propositions to which the agent assigns resiliently high
credence. The stability rule generates a class of $\textit{probabilistically
stable belief revision}$ operators, which capture the dynamics of belief that
result from an agent updating their credences through Bayesian conditioning
while complying with the stability rule for their all-or-nothing beliefs. In
this paper, we prove a representation theorem that yields a complete
characterisation of such probabilistically stable revision operators and
provides a `qualitative' selection function semantics for the (non-monotonic)
logic of probabilistically stable belief revision. Drawing on the theory of
comparative probability orders, this result gives necessary and sufficient
conditions for a selection function to be representable as a
strongest-stable-set operator on a finite probability space. The resulting
logic of probabilistically stable belief revision exhibits strong monotonicity
properties while failing the AGM belief revision postulates and satisfying only
very weak forms of case reasoning. In showing the main theorem, we prove two
results of independent interest to the theory of comparative probability: the
first provides necessary and sufficient conditions for the joint representation
of a pair of (respectively, strict and non-strict) comparative probability
orders. The second result provides a method for axiomatising the logic of ratio
comparisons of the form ``event $A$ is at least $k$ times more likely than
event $B$''. In addition to these measurement-theoretic applications, we point
out two applications of our main result to the theory of simple voting games
and to revealed preference theory.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation](https://arxiv.org/abs/2509.00030)
*Marshall Thomas,Edward Fish,Richard Bowden*

Main category: cs.CL

TL;DR: 提出的MultiStream-LLM多模态手语翻译框架，通过分别处理不同子任务（连贯手语、手指拼写、唇读）显著提升了翻译准确率，在多个基准测试上表现优异，证明了分工协作的识别策略优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译端到端模型在手语中的两个关键部分（高速手指拼写和异步面部非手动线索整合）上表现不佳，特别是在人名、地名和术语等重要信息的翻译任务中存在严重性能瓶颈。

Method: 提出了MultiStream-LLM，一个模块化框架，通过分别为连贯手语、手指拼写和唇读设计专用识别器，各自将其模态解码为令牌序列，然后由轻量级Transformer融合以解决时间不同步，最后再交由大语言模型（LLM）生成完整句子。

Result: 在How2Sign基准上获得了新的最优BLEU-4分数23.5，并在挑战性的ChicagoFSWildPlus手指拼写数据集上取得了73.2%的字母准确率。

Conclusion: 将识别任务按模态拆分再融合，有效提升了手语翻译的质量和鲁棒性，验证了多专家模型优于单一端到端模型。

Abstract: Despite progress in gloss-free Sign Language Translation (SLT), monolithic
end-to-end models consistently fail on two critical components of natural
signing: the precise recognition of high-speed fingerspelling and the
integration of asynchronous non-manual cues from the face. Recent progress in
Automated Sign Language Translation with Large Language Models has side stepped
this challenge, forcing a single network to learn these simultaneously
resulting in poor performance when tasked with translating crucial information
such as names,places, and technical terms. We introduce MultiStream-LLM, a
modular framework designed to overcome these limitations. Our approach employs
separate, specialized predictors for continuous signing, fingerspelling, and
lipreading. Each expert network first decodes its specific modality into a
sequence of tokens. These parallel streams are then fused by a lightweight
transformer that resolves temporal misalignments before passing the combined
representation to a Large Language Model (LLM) for final sentence generation.
Our method establishes a new state-of-the-art on the How2Sign benchmark with a
BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging
ChicagoFSWildPlus fingerspelling dataset. These results validate our core
hypothesis: by isolating and solving distinct recogni tion tasks before fusion,
our multi-expert approach provides a more powerful and effective pathway to
robust, high-fidelity sign language translation.

</details>


### [24] [Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis](https://arxiv.org/abs/2509.00038)
*Teo Susnjak*

Main category: cs.CL

TL;DR: 本文提出将声明式提示优化方法应用于文献综述自动化流程，提升了可靠性和可重复性，是该领域的创新实践。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在系统性文献综述（SLR）方面有潜力，但现有方法依赖手工设计的提示，易碎且难以重现，影响科学可信度。

Method: 将用于通用LLM的声明式提示优化方法引入SLR领域，提出包含任务声明、测试集和自动提示调优的结构化领域专用框架，并附代码示例。

Result: 验证声明式提示优化在SLR自动化中的适用性，实现了可验证的LLM流程，提高了透明度与严格性的符合程度。

Conclusion: 首次将通用声明式提示优化方法系统应用于SLR流程，为自动化和可重复性提供了新范式。

Abstract: Large language models (LLMs) offer significant potential to accelerate
systematic literature reviews (SLRs), yet current approaches often rely on
brittle, manually crafted prompts that compromise reliability and
reproducibility. This fragility undermines scientific confidence in
LLM-assisted evidence synthesis. In response, this work adapts recent advances
in declarative prompt optimisation, developed for general-purpose LLM
applications, and demonstrates their applicability to the domain of SLR
automation. This research proposes a structured, domain-specific framework that
embeds task declarations, test suites, and automated prompt tuning into a
reproducible SLR workflow. These emerging methods are translated into a
concrete blueprint with working code examples, enabling researchers to
construct verifiable LLM pipelines that align with established principles of
transparency and rigour in evidence synthesis. This is a novel application of
such approaches to SLR pipelines.

</details>


### [25] [What Are Research Hypotheses?](https://arxiv.org/abs/2509.00185)
*Jian Wu,Sarah Rajtmajer*

Main category: cs.CL

TL;DR: 该文系统综述了自然语言理解领域不同任务中对“假设”概念的定义差异，强调构建机器可理解的学术记录时明确定义假设的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前在自然语言理解领域对“假设”概念的理解和定义并不统一，且与传统科学领域存在偏移，因此有必要系统梳理和澄清其含义。

Method: 综述与分析文献、对比近年自然语言理解任务中的假设定义，并归纳差异。

Result: 论文梳理了当前文献中“假设”定义的不同形式，尤其分析了近期自然语言理解任务中的具体表现和差异，揭示了定义上的细微差别。

Conclusion: 各类自然语言理解任务中对“假设”的理解存在明显差异，推进统一和结构化的假设定义对于自动化学术工具和机器可解释的研究记录具有重要意义。

Abstract: Over the past decades, alongside advancements in natural language processing,
significant attention has been paid to training models to automatically
extract, understand, test, and generate hypotheses in open and scientific
domains. However, interpretations of the term \emph{hypothesis} for various
natural language understanding (NLU) tasks have migrated from traditional
definitions in the natural, social, and formal sciences. Even within NLU, we
observe differences defining hypotheses across literature. In this paper, we
overview and delineate various definitions of hypothesis. Especially, we
discern the nuances of definitions across recently published NLU tasks. We
highlight the importance of well-structured and well-defined hypotheses,
particularly as we move toward a machine-interpretable scholarly record.

</details>


### [26] [Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics](https://arxiv.org/abs/2509.00190)
*Sheldon Yu,Yuxin Xiong,Junda Wu,Xintong Li,Tong Yu,Xiang Chen,Ritwik Sinha,Jingbo Shang,Julian McAuley*

Main category: cs.CL

TL;DR: 本文提出用谱分析与马尔可夫链建模的方式，对CoT推理过程进行结构化建模，提升了推理可解释性，支持多种推理动态分析应用。


<details>
  <summary>Details</summary>
Motivation: 链式思考（CoT）提升了大语言模型进行多步推理的能力，但目前在推理过程的可解释性方面仍有限，缺乏对高层语义和推理步骤转化机制的深入探索。

Method: 提出了一种状态感知转化框架，将CoT推理轨迹抽象为结构化的潜在动态：首先通过对每一步推理的token级嵌入进行谱分析，聚类为语义一致的潜在状态；接着用马尔可夫链建模其演化，展现推理的整体结构。

Result: 该抽象方法能够帮助理解和解释推理过程，包括语义角色的识别、时序模式的可视化及推理一致性的评估。

Conclusion: 该方法为分析和解释大语言模型的多步推理过程提供了新的结构化视角，丰富了对高层语义和全局推理动态的理解。

Abstract: Recent advances in chain-of-thought (CoT) prompting have enabled large
language models (LLMs) to perform multi-step reasoning. However, the
explainability of such reasoning remains limited, with prior work primarily
focusing on local token-level attribution, such that the high-level semantic
roles of reasoning steps and their transitions remain underexplored. In this
paper, we introduce a state-aware transition framework that abstracts CoT
trajectories into structured latent dynamics. Specifically, to capture the
evolving semantics of CoT reasoning, each reasoning step is represented via
spectral analysis of token-level embeddings and clustered into semantically
coherent latent states. To characterize the global structure of reasoning, we
model their progression as a Markov chain, yielding a structured and
interpretable view of the reasoning process. This abstraction supports a range
of analyses, including semantic role identification, temporal pattern
visualization, and consistency evaluation.

</details>


### [27] [The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs](https://arxiv.org/abs/2509.00245)
*Seiji Maekawa,Hayate Iso,Nikita Bhutani*

Main category: cs.CL

TL;DR: 本文提出并系统评测了“独特特征挖掘”能力，发现主流LLM在识别文档集合中罕见特征方面存在重大短板，尤其在文档规模扩大时表现退化，显示当前模型在统计推理与稀有性检测能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）评测主要关注信息检索或摘要，但缺乏衡量模型在一组文档中挖掘全球特征稀有性的能力。现实场景如候选人遴选或产品差异化，常常需要识别整体中罕见的特征，这类任务依靠统计推理而非单纯检索。

Method: 提出Distinctive Feature Mining（DFM）新任务，要求模型在10-40份文档中发现全球背景下稀有的特征（如在全部文档中出现率低于10%）。开发DiFBench评测框架，可调节文档集大小和稀有性阈值，对十个主流LLM进行大规模评测，比较其在挖掘独特特征任务上的表现。

Result: 一般用途的模型和增强推理的模型之间在任务表现上存在明显差距。所有模型在文档数量和任务复杂度增加时性能均明显下降，常见的失败模式是把常见特征误判为独特特征。

Conclusion: 主流LLM在进行细粒度统计推理与罕见性检测方面存在核心局限，当前模型难以胜任挖掘文档集合中独特特征的实际应用需求。

Abstract: Effective decision-making often relies on identifying what makes each
candidate distinctive. While existing benchmarks for LLMs emphasize retrieving
or summarizing information relevant to a given query, they do not evaluate a
model's ability to identify globally distinctive features across a set of
documents. We introduce Distinctive Feature Mining (DFM), a new task that
challenges models to analyze a small-to-medium collection (10-40 documents) and
surface features that are rare in the global context (e.g., appearing in less
than 10% of documents). This setting mirrors real-world scenarios such as
candidate selection or product differentiation, where statistical reasoning,
not retrieval, is key. To enable systematic evaluation of this capability, we
present DiFBench, a configurable benchmark creation framework with controllable
parameters such as document set size and distinctiveness thresholds. Using
DiFBench, we perform a large-scale assessment of distinctive feature mining
across ten state-of-the-art LLMs. Our findings reveal a significant performance
gap between general-purpose and reasoning-enhanced models. All models, however,
substantially degrade as the task complexity and document count increase. We
also find that a common failure mode is misidentifying frequent features as
distinctive. These insights reveal core limitations in contemporary LLMs'
abilities to perform fine-grained, statistical reasoning and rarity detection.

</details>


### [28] [The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions](https://arxiv.org/abs/2509.00248)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 本文提出了一个基于Peirce符号学的理论框架，统一分析和比较意义建模方法，将模型理解为符号及其决策。展现了该框架的实证用例，并讨论了理论基础和未来展望，有助于推动复杂符号系统模型的理论发展。


<details>
  <summary>Details</summary>
Motivation: 目前人类意义建构建模方法繁多，但缺乏一个可以统一描述各种模型实践的理论框架。亟需一种能在不同模型类型间进行公平比较的通用理论。

Method: 提出一种以C. S. Peirce的符号学理论为基础的统一框架，用于理解和比较不同的意义建模方法。理论上，将模型看作是在度量潜在符号几何结构，并通过模型间的对比来显现模型解释的视角。提出模型语义理论，并通过简要实例说明框架的应用。

Result: 成功构建了一个符号学理论的模型比较框架，并通过实例展示其实证效用。提出模型及其决策本身可被视作符号，探讨了由此引发的基础性问题与未来研究方向。

Conclusion: 文中提出的框架为意义建模方法的理论化分析提供了新工具，使不同模型能在符号学视角下被比较与理解，并启发了对于模型本体及语义的新认识。

Abstract: The proliferation of methods for modeling of human meaning-making constitutes
a powerful class of instruments for the analysis of complex semiotic systems.
However, the field lacks a general theoretical framework for describing these
modeling practices across various model types in an apples-to-apples way. In
this paper, we propose such a framework grounded in the semiotic theory of C.
S. Peirce. We argue that such models measure latent symbol geometries, which
can be understood as hypotheses about the complex of semiotic agencies
underlying a symbolic dataset. Further, we argue that in contexts where a
model's value cannot be straightforwardly captured by proxy measures of
performance, models can instead be understood relationally, so that the
particular interpretive lens of a model becomes visible through its contrast
with other models. This forms the basis of a theory of model semantics in which
models, and the modeling decisions that constitute them, are themselves treated
as signs. In addition to proposing the framework, we illustrate its empirical
use with a few brief examples and consider foundational questions and future
directions enabled by the framework.

</details>


### [29] [The Temporal Game: A New Perspective on Temporal Relation Extraction](https://arxiv.org/abs/2509.00250)
*Hugo Sousa,Ricardo Campos,Alípio Jorge*

Main category: cs.CL

TL;DR: 本文提出一种创新的时间关系注释方法，将注释任务转化为游戏，有效提高了注释的细粒度与灵活性，同时支持研究与实际应用，且工具已开源公开。


<details>
  <summary>Details</summary>
Motivation: 现有的时间关系注释方法在粒度和灵活性上存在局限，急需一种更细致且易用的注释工具，同时希望能为相关领域的强化学习提供数据和平台支持。

Method: 将时间关系抽取任务转化为交互式游戏。具体做法是将区间关系分解为时点间的比较，让玩家逐步判别各个时间点的关系，并利用时间闭包推理更多关系、保证一致性。此外，系统设计了“游戏模式”和“注释模式”，满足研究与实际注释的需求。

Result: 提出的Temporal Game支持区间与点实体的灵活时间注释，能够提升注释质量，同时具有训练RL智能体的潜力。工具已开源，公开在线，支持学术研究和社区发展。

Conclusion: 该方法不仅提供了传统的时间关系注释工具，还通过交互式游戏机制提升了注释的灵活性与细粒度，同时也为强化学习智能体的训练提供了基础设施，有助于推动时间推理领域的进一步发展。

Abstract: In this paper we demo the Temporal Game, a novel approach to temporal
relation extraction that casts the task as an interactive game. Instead of
directly annotating interval-level relations, our approach decomposes them into
point-wise comparisons between the start and end points of temporal entities.
At each step, players classify a single point relation, and the system applies
temporal closure to infer additional relations and enforce consistency. This
point-based strategy naturally supports both interval and instant entities,
enabling more fine-grained and flexible annotation than any previous approach.
The Temporal Game also lays the groundwork for training reinforcement learning
agents, by treating temporal annotation as a sequential decision-making task.
To showcase this potential, the demo presented in this paper includes a Game
mode, in which users annotate texts from the TempEval-3 dataset and receive
feedback based on a scoring system, and an Annotation mode, that allows custom
documents to be annotated and resulting timeline to be exported. Therefore,
this demo serves both as a research tool and an annotation interface. The demo
is publicly available at https://temporal-game.inesctec.pt, and the source code
is open-sourced to foster further research and community-driven development in
temporal reasoning and annotation.

</details>


### [30] [Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval](https://arxiv.org/abs/2509.00276)
*Yuxiang Liu,Tian Wang,Gourab Kundu,Tianyu Cao,Guang Cheng,Zhen Ge,Jianshu Chen,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 论文提出了一种称为RITE的新方法，能将推理能力融入文本嵌入，有效提升基于大语言模型的检索效果。实验验证了其在推理密集型任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型虽然能较好捕捉上下文信息，但在需要复杂推理的检索任务中表现有限。大语言模型拥有强大的推理能力，但其嵌入方法未能充分利用这一点，因此急需提升基于推理的文本表示能力。

Method: 作者提出RITE方法，利用生成式大语言模型（LLM）在进行文本嵌入前，先在token空间生成中间推理文本，再计算最终嵌入向量，从而在表示中引入推理能力。

Result: 实验结果表明，RITE（Reasoning-Infused Text Embedding）在BRIGHT这一对推理需求较高的检索基准测试中，能够显著提升零样本检索的表现，且适用于不同领域。

Conclusion: 将推理过程整合到生成模型的文本嵌入之中，能够极大地增强模型在需要复杂推理的检索场景中的表现，并优于仅依赖上下文表示的传统方法。

Abstract: Transformer-based models such as BERT and E5 have significantly advanced text
embedding by capturing rich contextual representations. However, many complex
real-world queries require sophisticated reasoning to retrieve relevant
documents beyond surface-level lexical matching, where encoder-only retrievers
often fall short. Decoder-only large language models (LLMs), known for their
strong reasoning capabilities, offer a promising alternative. Despite this
potential, existing LLM-based embedding methods primarily focus on contextual
representation and do not fully exploit the reasoning strength of LLMs. To
bridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple
but effective approach that integrates logical reasoning into the text
embedding process using generative LLMs. RITE builds upon existing language
model embedding techniques by generating intermediate reasoning texts in the
token space before computing embeddings, thereby enriching representations with
inferential depth. Experimental results on BRIGHT, a reasoning-intensive
retrieval benchmark, demonstrate that RITE significantly enhances zero-shot
retrieval performance across diverse domains, underscoring the effectiveness of
incorporating reasoning into the embedding process.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [31] [Distance-based (and path-based) covering problems for graphs of given cyclomatic number](https://arxiv.org/abs/2509.00383)
*Dibyayan Chakraborty,Florent Foucaud,Anni Hakanen*

Main category: cs.DM

TL;DR: 本文针对有界圈指标图的距离相关覆盖问题，提出新的理论上界并给出线性或多项式时间算法，解决了若干领域猜想与难题。


<details>
  <summary>Details</summary>
Motivation: 图覆盖问题（如度量维数、测地集、路径覆盖等）在图论中有重要应用，但其最优解与图的结构参数（特别是圈指标和度为1的节点）之间的关系尚未充分研究。此前相关问题的上界多为粗略或特殊情形，存在诸多猜想和未解问题。作者旨在推广和完善现有理论，解决这些开放问题，并提升算法效率。

Method: 作者在现有技术基础上（特别是对2022年Lu等人算法的改进），发展了一种基于广度优先搜索的算法化方法。通过系统性地分析并构建解集，推导出覆盖数与圈指标和度为1节点数线性相关的严密界，并使所有构造可线性时间内完成。

Result: 论文给出上述三类图覆盖问题对于有界圈指标的图的最优值线性上界，并在多个情形下取得近最优界，部分或完全解决相关领域的猜想和公开问题。此外，部分问题可在有界圈指标图上多项式时间求解。所有结果均通过新算法实现，且构造效率高。

Conclusion: 作者推广了有界圈指标图的覆盖问题最优解上界理论，引入高效广搜类算法，实现在多个具体问题上的理论和算法突破，对相关猜想与算法复杂性作出重要贡献。

Abstract: We study a large family of graph covering problems, whose definitions rely on
distances, for graphs of bounded cyclomatic number (that is, the minimum number
of edges that need to be removed from the graph to destroy all cycles). These
problems include (but are not restricted to) three families of problems: (i)
variants of metric dimension, where one wants to choose a small set $S$ of
vertices of the graph such that every vertex is uniquely determined by its
ordered vector of distances to the vertices of $S$; (ii) variants of geodetic
sets, where one wants to select a small set $S$ of vertices such that any
vertex lies on some shortest path between two vertices of $S$; (iii) variants
of path covers, where one wants to select a small set of paths such that every
vertex or edge belongs to one of the paths. We generalize and/or improve
previous results in the area which show that the optimal values for these
problems can be upper-bounded by a linear function of the cyclomatic number and
the degree~1-vertices of the graph. To this end, we develop and enhance a
technique recently introduced in [C. Lu, Q. Ye, C. Zhu. Algorithmic aspect on
the minimum (weighted) doubly resolving set problem of graphs, Journal of
Combinatorial Optimization 44:2029--2039, 2022] and give near-optimal bounds in
several cases. This solves (in some cases fully, in some cases partially) some
conjectures and open questions from the literature. The method, based on
breadth-first search, is of algorithmic nature and thus, all the constructions
can be computed in linear time. Our results also imply an algorithmic
consequence for the computation of the optimal solutions: for some of the
problems, they can be computed in polynomial time for graphs of bounded
cyclomatic number.

</details>


### [32] [Morse sequences on stacks and flooding sequences](https://arxiv.org/abs/2509.01384)
*Gilles Bertrand*

Main category: cs.DM

TL;DR: 本文深化了离散Morse理论，将Morse序列拓展到加权单纯复形和stack，提出了洪泛序列的理论与算法，证明了相关拓扑性质和算法效果，为复杂加权结构的拓扑分析提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 论文旨在通过扩展Morse序列的理论，提升离散Morse理论在加权单纯复形及其相关算法中的应用能力。作者关注如何捕捉加权结构下的拓扑特征，并寻求与经典分水岭方法的结合。

Method: 将Morse序列的定义拓展到所谓的“stack”（在单纯复形上的单调函数），提出并研究了在stack上的Morse序列，并进一步界定了其中一类具有特殊排序行为的“洪泛序列”。同时，通过余单纯复形给出了洪泛序列的算法实现方案。

Result: 作者证明了stack上的Morse序列扩张保持所有子水平集的同伦类型，从而推广了基本的塌缩定理到加权单纯复形。还证明了任意Morse序列关联的梯度向量场可以用洪泛序列恢复。

Conclusion: 该工作推广了Morse理论至更广泛的加权单纯复形，并通过洪泛序列连接了拓扑特征提取与经典算法。给出了新的算法框架，为处理加权结构的拓扑分析问题提供了有效工具。

Abstract: This paper builds upon the framework of \emph{Morse sequences}, a simple and
effective approach to discrete Morse theory. A Morse sequence on a simplicial
complex consists of a sequence of nested subcomplexes generated by expansions
and fillings-two operations originally introduced by Whitehead. Expansions
preserve homotopy, while fillings introduce critical simplexes that capture
essential topological features. We extend the notion of Morse sequences to
\emph{stacks}, which are monotonic functions defined on simplicial complexes,
and define \emph{Morse sequences on stacks} as those whose expansions preserve
the homotopy of all sublevel sets. This extension leads to a generalization of
the fundamental collapse theorem to weighted simplicial complexes. Within this
framework, we focus on a refined class of sequences called \emph{flooding
sequences}, which exhibit an ordering behavior similar to that of classical
watershed algorithms. Although not every Morse sequence on a stack is a
flooding sequence, we show that the gradient vector field associated with any
Morse sequence can be recovered through a flooding sequence. Finally, we
present algorithmic schemes for computing flooding sequences using cosimplicial
complexes.

</details>


### [33] [Continuous Petri Nets for Fast Yield Computation: Polynomial-Time and MILP Approaches](https://arxiv.org/abs/2509.02371)
*Addie Jordon,Juri Kolčák,Daniel Merkle*

Main category: cs.DM

TL;DR: 论文提出利用连续Petri网和混合整数线性规划算法高效计算化学反应网络中分子最大产额，显著降低原本复杂计算的难度，实验显示新算法在多种数据上具实用优势。


<details>
  <summary>Details</summary>
Motivation: 研究Petri网在化学反应网络的建模优势，但受限于其高计算复杂性（如可达性问题为PSpace-hard）。希望提升化学反应网络分析的可行性和效率。

Method: 提出基于连续Petri网的多项式时间最大产额算法，并设计一种基于混合整数线性规划的替代算法，两种方法在理论和实践中比较。

Result: 连续Petri网算法能在多项式时间内计算分子最大产额；混合整数线性规划算法理论复杂度较高但实际运行速度较快。分别在合成和真实化学数据上验证。

Conclusion: 连续Petri网能有效提升化学系统最大分子产额的计算效率，实现多项式时间解法；混合整数线性规划法虽理论性能较差，但实践效果更佳。为化学反应网络分析提供高效工具。

Abstract: Petri nets provide accurate analogues to chemical reaction networks, with
places representing individual molecules (the resources of the system) and
transitions representing chemical reactions which convert educt molecules into
product molecules. Their natural affinity for modeling chemical reaction
networks is, however, impeded by their computational complexity, which is at
least PSpace-hard for most interesting questions, including reachability.
Continuous Petri nets offer the same structure and discrete time as discrete
Petri nets, but use continuous state-space, which allows them to answer the
reachability question in polynomial time. We exploit this property to introduce
a polynomial time algorithm for computing the maximal yield of a molecule in a
chemical system. Additionally, we provide an alternative algorithm based on
mixed-integer linear programming with worse theoretical complexity, but better
runtime in practice, as demonstrated on both synthetic and chemical data.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [34] [Generalised Möbius Categories and Convolution Kleene Algebras](https://arxiv.org/abs/2509.00168)
*James Cranch,Georg Struth,Jana Wagemaker*

Main category: cs.FL

TL;DR: 本文提出了利用广义Möbius范畴和Kleene星推广，在更广结构上系统性地构造卷积Kleene代数的方法，拓展了其在程序验证及高维代数推理等领域的应用，为未来研究提供新工具。


<details>
  <summary>Details</summary>
Motivation: 构造卷积Kleene代数在许多数学与科学应用中的重要性，尤其在计算领域，但广泛结构上的Kleene星操作定义一直存在障碍。论文旨在突破这一瓶颈。

Method: 将广义Möbius范畴与形式幂级数Kleene星的经典定义推广相结合，提出了一种适用于更广泛结构的卷积Kleene代数构造方法，并探讨了其在多种特定结构上的应用，包括带测试、模态、并发与高阶卷积Kleene代数。同时将该方法适配到Conway半环，并与卷积quantale的先前方法进行对比。

Result: 提出了新的卷积Kleene代数构造框架，适用于广义Möbius范畴，并给出在不同结构上的具体实例；适配了Conway半环，展示了多种具体应用场景，并呈现了与已知构造的比较。

Conclusion: 通过对Möbius范畴与Kleene星的推广，成功实现了在广泛结构（包括严格高阶范畴等）上的卷积Kleene代数构造，这为加权、概率性程序的验证及高维重写的代数推理等提供了理论基础，并为后续应用打下了基础。

Abstract: Convolution algebras on maps from structures such as monoids, groups or
categories into semirings, rings or fields abound in mathematics and the
sciences. Of special interest in computing are convolution algebras based on
variants of Kleene algebras, which are additively idempotent semirings equipped
with a Kleene star. Yet an obstacle to the construction of convolution Kleene
algebras on a wide class of structures has so far been the definition of a
suitable star. We show that a generalisation of M\"obius categories combined
with a generalisation of a classical definition of a star for formal power
series allow such a construction. We discuss several instances of this
construction on generalised M\"obius categories: convolution Kleene algebras
with tests, modal convolution Kleene algebras, concurrent convolution Kleene
algebras and higher convolution Kleene algebras (e.g. on strict higher
categories and higher relational monoids). These are relevant to the
verification of weighted and probabilistic sequential and concurrent programs,
using quantitative Hoare logics or predicate transformer algebras, as well as
for algebraic reasoning in higher-dimensional rewriting. We also adapt the
convolution Kleene algebra construction to Conway semirings, which is widely
studied in the context of weighted automata. Finally, we compare the
convolution Kleene algebra construction with a previous construction of
convolution quantales and present concrete example structures in preparation
for future applications.

</details>


### [35] [Computational Exploration of Finite Semigroupoids](https://arxiv.org/abs/2509.00837)
*Attila Egri-Nagy,Chrystopher L. Nehaniv*

Main category: cs.FL

TL;DR: 本文利用关系编程系统性分析了自动机理论中的半群体结构，实现了模型的枚举、同态与表示构建，深入区分了数学属性及类型结构，加深了对有类型运算的理解。


<details>
  <summary>Details</summary>
Motivation: 近年自动机理论的算法进展使半群体（半范畴）作为有类型的计算过程模型受到关注，但其在自动机中的系统性研究仍显不足。本文旨在借助关系式编程更深入地理解与探索半群体模型。

Method: 采用关系式（声明式）编程，具体实现了：抽象半群体的枚举（部分合成表），同态的寻找，以及（最小）变换表示的构建。

Result: 提出了数学方法用于枚举和构建有限半群体，并展示了结合性与类型一致性的区别，区分了严格与宽松的同态，并系统性地枚举了箭头类型（类型显式化）半群体结构。

Conclusion: 研究展示了关联性编程可以在自动机理论下有效探索有限半群体模型，并区分了不同的数学属性和结构类型，扩展了对有类型计算过程的理解。

Abstract: Recent algorithmic advances in algebraic automata theory drew attention to
semigroupoids (semicategories). These are mathematical descriptions of typed
computational processes, but they have not been studied systematically in the
context of automata. Here, we use relational programming to explore finite
semigroupoids to improve our mathematical intuition about these models of
computation. We implement declarative solutions for enumerating abstract
semigroupoids (partial composition tables), finding homomorphisms, and
constructing (minimal) transformation representations. We show that
associativity and consistent typing are different properties, distinguish
between strict and more permissive homomorphisms, and systematically enumerate
arrow-type semigroupoids (reified type structures).

</details>


### [36] [A substitution lemma for multiple context-free languages](https://arxiv.org/abs/2509.02117)
*Andrew Duncan,Murray Elder,Lisa Frenkel,Mengfan Lyu*

Main category: cs.FL

TL;DR: 提出了一个名为替换引理的新方法，用于证明语言不属于多重上下文无关语言，并以$F_2\times F_2$的词问题为例进行了应用。同时，对这种群的有理子集问题与交问题进行了理论分析。


<details>
  <summary>Details</summary>
Motivation: 由于多重上下文无关语言的泵引理无法像常规上下文无关语言那样被有效推广（已有文献证明相关引理不适用），因此需要新的工具来分析和判断一类语言是否属于多重上下文无关语言。

Method: 作者提出并应用了一种替换引理作为判据，用以分析和证明某些语言不属于多重上下文无关语言。通过具体语言（如$F_2\times F_2$的词问题）进行举例，并对该类语言的群论属性进行了进一步分析。

Result: 证明了一些重要语言（包括$F_2\times F_2$的词问题）不是多重上下文无关语言，并发现相关群的有理子集成员和交问题具有较好的算法性质。

Conclusion: 本文提出的新判据——替换引理（Substitution Lemma）能够证明某些语言不是多重上下文无关语言（multiple context-free language）。通过应用该方法，作者证明了一些具体例子的语言（包括$F_2\times F_2$的词问题）不属于多重上下文无关语言。此外，作者还证明多重上下文无关词问题群具有有理子集成员和交问题的良好属性。

Abstract: We present a new criterion for proving that a language is not multiple
context-free, which we call a Substitution Lemma. We apply it to show a sample
selection of languages are not multiple context-free, including the word
problem of $F_2\times F_2$.
  Our result is in contrast to Kanazawa et al. [2014, Theory Comput. Syst.] who
proved that it was not possible to generalise the standard pumping lemma for
context-free languages to multiple context-free languages, and Kanazawa [2019,
Inform. and Comput.] who showed a weak variant of generalised Ogden's lemma
does not apply to multiple context-free languages.
  We also show that groups with multiple context-free word problem have
rational subset membership and intersection problems.

</details>


### [37] [DTMC Model Checking by Path Abstraction Revisited (extended version)](https://arxiv.org/abs/2509.02393)
*Arnd Hartmanns,Robert Modderman*

Main category: cs.FL

TL;DR: 本文系统扩展了路径抽象用于DTMC达成概率计算的理论与实现，证明其正确且灵活，并给出高效实现，为概率模型检测提供了更通用的方法。


<details>
  <summary>Details</summary>
Motivation: 概率模型检测中，精准计算或细分目标集合达成概率是核心问题，尤其对于反例细化、分析概率质量分布等实际需求，现有方法存在需求更灵活路径抽象与高效实现的难点。

Method: 作者采用将DTMC解释为其状态空间上的自由幺半群结构的新颖方式，对路径抽象等价性和分割条件进行严密证明，并提供了PARI/GP上的紧凑代码实现。

Result: 证明了路径抽象分割的普适性和正确性，无需依赖特定结构划分，并通过实现在实际工具中验证。

Conclusion: 本文证明了在离散时间马尔可夫链（DTMC）中，将概率计算拆分为路径抽象的方法与直接整体计算的结果是一致的，并且路径抽象的分割方式不必依据强连通分量（SCC）结构，可以沿任意有限的非目标状态集序列进行。此外，给出了相应的理论证明和实用实现。

Abstract: Computing the probability of reaching a set of goal states G in a
discrete-time Markov chain (DTMC) is a core task of probabilistic model
checking. We can do so by directly computing the probability mass of the set of
all finite paths from the initial state to G; however, when refining
counterexamples, it is also interesting to compute the probability mass of
subsets of paths. This can be achieved by splitting the computation into path
abstractions that calculate "local" reachability probabilities as shown by
\'Abrah\'am et al. in 2010. In this paper, we complete and extend their work:
We prove that splitting the computation into path abstractions indeed yields
the same result as the direct approach, and that the splitting does not need to
follow the SCC structure. In particular, we prove that path abstraction can be
performed along any finite sequence of sets of non-goal states. Our proofs
proceed in a novel way by interpreting the DTMC as a structure on the free
monoid on its state space, which makes them clean and concise. Additionally, we
provide a compact reference implementation of path abstraction in PARI/GP.

</details>
