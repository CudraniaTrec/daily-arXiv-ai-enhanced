<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.DM](#cs.DM) [Total: 4]
- [cs.FL](#cs.FL) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Latency Based Tiling](https://arxiv.org/abs/2510.15912)
*Jack Cashman*

Main category: cs.PL

TL;DR: 提出了基于延迟观测的循环分块方法，能在编译时高效判定缓存容量并据此优化循环局部性，编译开销小，通用且适合 Rust 环境。


<details>
  <summary>Details</summary>
Motivation: 现有自动调优方法虽然能提升性能，但编译和调优所耗时间过长，不适合大规模/快速开发。希望设计一种系统级、硬件无关、开销极小、同时能高效提升局部性的循环分块策略。

Method: 利用三角循环检测系统中的缓存延迟突增点，从而推断 L1/L2/L3 缓存容量，并据此对循环进行基于内存层次结构和数据访问特征的分块优化。该方法在 Rust 语言下实现，结合缓存计时技术，提供通用、内存安全的优化。

Result: 验证了该方法能够较准确地推断系统缓存大小，实现了获得低误差、高效率局部性优化，并避免了非预取器带来的失真问题，整个过程编译负担极小，不依赖特定硬件。

Conclusion: Latency Based Tiling 实现了近似的最佳循环分块，提升了数据局部性，同时保持了极快的编译速度，且适用于不同硬件环境。

Abstract: Latency Based Tiling provides a systems based approach to deriving
approximate tiling solution that maximizes locality while maintaining a fast
compile time. The method uses triangular loops to characterize miss ratio
scaling of a machine avoiding prefetcher distortion. Miss ratio scaling
captures the relationship between data access latency and working set size with
sharp increases in latency indicating the data footprint exceeds capacity from
a cache level. Through these noticeable increases in latency we can determine
an approximate location for L1, L2, and L3 memory sizes. These sizes are
expected to be under approximations of a systems true memory sizes which is in
line with our expectations given the shared nature of cache in a multi process
system as described in defensive loop tiling. Unlike auto tuning, which can be
effective but prohibitively slow, Latency Based Tiling achieves negligible
compile time overhead. The implementation in Rust enables a hardware agnostic
approach which combined with a cache timing based techniques, yields a
portable, memory safe system running wherever Rust is supported. The tiling
strategy is applied to a subset of the polyhedral model, where loop nestings
are tiled based on both the derived memory hierarchy and the observed data
footprint per iteration.

</details>


### [2] [Typing Strictness (Extended Version)](https://arxiv.org/abs/2510.16133)
*Daniel Sainati,Joseph W. Cutler,Benjamin C. Pierce,Stephanie Weirich*

Main category: cs.PL

TL;DR: 本文提出一种更精细的严格性定义和对应类型系统，有效提升非严格编程语言中严格性分析的准确性，并在理论与工具中均得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有严格性分析的传统定义不够精确且对程序员来说难以理解和使用，因而希望从类型论角度提出更直观、更细致的新定义，提升非严格语言实现的效率，并让严格性分析结果更能反映运行时实际。

Method: 采用了类型系统理论基础，分别针对按名调用和按推值调用两种计算模型，结合效果和共效果类型系统的相关文献，提出新的严格性定义并建立了类型系统。在技术上使用逻辑关系证明类型系统对变量运行时使用的刻画准确，还实现了从按名调用到按推值调用的严格性注释保持型翻译。所有工作均被机械化验证。

Result: 提出了比传统定义更精细的严格性分析新定义，建立了支持新定义的类型系统，理论上证明了其准确性，并证明两种计算模型间可严格性保持地转换。所有理论成果都用Rocq工具机械化实现。

Conclusion: 作者提出的严格性定义和类型系统能更精确地表示和追踪非严格计算语言中变量的实际使用情况，其类型系统推导出的严格性属性与运行时变量使用吻合，并在系统（Rocq）中形式化和验证。

Abstract: Strictness analysis is critical to efficient implementation of languages with
non-strict evaluation, mitigating much of the performance overhead of laziness.
However, reasoning about strictness at the source level can be challenging and
unintuitive. We propose a new definition of strictness that refines the
traditional one by describing variable usage more precisely. We lay
type-theoretic foundations for this definition in both call-by-name and
call-by-push-value settings, drawing inspiration from the literature on type
systems tracking effects and coeffects. We prove via a logical relation that
the strictness attributes computed by our type systems accurately describe the
use of variables at runtime, and we offer a strictness-annotation-preserving
translation from the call-by-name system to the call-by-push-value one. All our
results are mechanized in Rocq.

</details>


### [3] [SimpliPy: A Source-Tracking Notional Machine for Simplified Python](https://arxiv.org/abs/2510.16594)
*Moida Praneeth Jain,Venkatesh Choppella*

Main category: cs.PL

TL;DR: SimpliPy通过结合形式语义、程序分析和可视化，帮助初学者更好地掌握Python控制流和作用域，是程序理解教育和实际演示的有力工具。


<details>
  <summary>Details</summary>
Motivation: 许多初学者对程序执行存在误解，尤其在控制流和作用域等核心概念上，为此作者提出帮助理解的工具。

Method: 提出了SimpliPy——基于Python子集的想象机模型。该模型明确追踪每一步执行的源代码行号，结合静态分析生成控制流图（CFG）和词法作用域标识，同时还开发了一个交互式可视化调试器，将动态运行状态和静态结构直观呈现。

Result: SimpliPy将形式语义、程序分析与可视化集成，为学习者提供理论与实践结合的程序理解新方法，并有效促进初学者对程序执行核心概念的把握。

Conclusion: SimpliPy不仅是一种教学途径，也展示了形式化方法在程序理解中的实际应用价值，通过其工具提升初学者对控制流和作用域的理解。

Abstract: Misconceptions about program execution hinder many novice programmers. We
introduce SimpliPy, a notional machine designed around a carefully chosen
Python subset to clarify core control flow and scoping concepts. Its foundation
is a precise operational semantics that explicitly tracks source code line
numbers for each execution step, making the link between code and behavior
unambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis
to generate Control Flow Graphs (CFGs) and identify lexical scopes, helping
students build a structural understanding before tracing. We also present an
interactive web-based debugger built on these principles. This tool embodies
the formal techniques, visualizing the operational state (environments, stack)
and using the static CFG to animate control flow directly on the graph during
step-by-step execution. SimpliPy thus integrates formal semantics, program
analysis, and visualization to offer both a pedagogical approach and a
practical demonstration of applying formal methods to program understanding.

</details>


### [4] [JAX Autodiff from a Linear Logic Perspective (Extended Version)](https://arxiv.org/abs/2510.16883)
*Giulia Giusti,Michele Pagani*

Main category: cs.PL

TL;DR: 本文将自动微分（Autodiff）编译到标准线性λ-演算，实现与Girard线性逻辑的理论统一，并证明其语义和成本的正确性，同时指出Autodiff中的unzipping操作其实可以省略。


<details>
  <summary>Details</summary>
Motivation: 现有的Autodiff线性类型演算虽能表达Autodiff的主要程序转换，但其专用性强，且该类型系统是否有更广泛逻辑意义尚不明确。作者希望将Autodiff置于更通用、具有理论意义的线性逻辑框架下。

Method: 作者提出将Autodiff编码进与Girard线性逻辑存在Curry-Howard对应的线性λ-演算，并从定性（扩展等价）与定量（保留工作量成本）两个方面严格论证该编码的正确性。

Result: 作者证明了Autodiff到线性λ-演算的编码既在行为等价性上成立，也能保留原算法的工作量成本。此外，还意外发现Autodiff中用于反向传播实现的“unzipping”操作其实是可选的。

Conclusion: 作者提供了Autodiff的线性逻辑化编码，为其专用型类型系统赋予一般理论意义，并优化了自动微分的实现细节。

Abstract: Autodiff refers to the core of the automatic differentiation systems
developed in projects like JAX and Dex. Autodiff has recently been formalised
in a linear typed calculus by Radul et al in arXiv:2204.10923. Although this
formalisation suffices to express the main program transformations of Autodiff,
the calculus is very specific to this task, and it is not clear whether the
type system yields a substructural logic that has interest on its own.
  We propose an encoding of Autodiff into a linear $\lambda$-calculus that
enjoys a Curry-Howard correspondence with Girard's linear logic. We prove that
the encoding is sound both qualitatively (the encoded terms are extensionally
equivalent to the original ones) and quantitatively (the encoding preserves the
original work cost as described in arXiv:2204.10923). As a byproduct, we show
that unzipping, one of the transformations used to implement backpropagation in
Autodiff, is, in fact, optional.

</details>


### [5] [Introducing Linear Implication Types to $λ_{GT}$ for Computing With Incomplete Graphs](https://arxiv.org/abs/2510.17429)
*Jin Sano,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: 该论文提出在λ_{GT}语言类型系统中引入线性蕴含和新约束，解决了对不完整图的支持及对静态类型检查的依赖，从而提升了复杂图结构的安全声明式操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有的编程语言在安全、直观操作复杂指针数据结构时面临重大挑战。传统的指针操作复杂且易错，现有类型系统虽然有所改进，但高层声明式语言操作高复杂性的指针结构问题仍未完全解决。

Method: 提升了λ_{GT}纯函数语言的类型系统，引入了线性蕴含以及新约束，用以支持不完整图结构，并强化静态类型检查，确保操作安全性和系统健壮性。

Result: 新的类型系统解决了原有λ_{GT}在处理不完整图和依赖动态类型检查时的不足，提升了静态安全保障。

Conclusion: 本研究通过扩展λ_{GT}类型系统，实现了对复杂指针结构的高层次声明式安全操作，有效克服了之前两项主要难题。

Abstract: Designing programming languages that enable intuitive and safe manipulation
of data structures is a critical research challenge. Conventional destructive
memory operations using pointers are complex and prone to errors. Existing type
systems, such as affine types and shape types, address this problem towards
safe manipulation of heaps and pointers, but design of high-level declarative
languages that allow us to manipulate complex pointer data structures at a
higher level of abstraction is largely an open problem. The $\lambda_{GT}$
language, a purely functional programming language that treats hypergraphs
(hereafter referred to as graphs) as primary data structures, addresses some of
these challenges. By abstracting data with shared references and cycles as
graphs, it enables declarative operations through pattern matching and
leverages its type system to guarantee safety of these operations.
Nevertheless, the previously proposed type system of $\lambda_{GT}$ leaves two
significant open challenges. First, the type system does not support
\emph{incomplete graphs}, that is, graphs in which some elements are missing
from the graphs of user-defined types. Second, the type system relies on
dynamic type checking during pattern matching. This study addresses these two
challenges by incorporating linear implication into the $\lambda_{GT}$ type
system, while introducing new constraints to ensure its soundness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: 本文提出了结合快慢思维机制的自适应程序修复方法SIADAFIX，能根据任务复杂度灵活调整修复流程，实验结果表现出色，达到了当前最优。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在程序修复等复杂任务中表现虽有进步，但在解决不同复杂度问题时效率与准确性难以兼顾，因此迫切需要更自适应的方法来提升修复效果。

Method: 提出了一种基于大语言模型的自适应程序修复方法SIADAFIX。该方法结合“快-慢”思维，利用快思维组件优化与分类任务描述，并用慢思维agent完成复杂修复，同时根据问题难度自适应选择三种修复模式，分别使用通用泛化与测试时扩展技术。

Result: 在SWE-bench Lite数据集上，使用Claude-4 Sonnet模型，SIADAFIX方法实现了60.67%的pass@1表现，达到了当前开源方法中的最优水平。

Conclusion: SIADAFIX兼顾了程序修复的效率与准确性，在自动化程序修复领域提供了新的思路。

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [7] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 本研究发现科学软件开发者在学术署名和影响力评价上常被忽视，频繁编码与h指数呈负相关，涉及代码贡献的评估与激励机制有待改进。


<details>
  <summary>Details</summary>
Motivation: 虽然软件开发对科学研究至关重要，但其对学术署名和影响力的实际作用尚不清楚，亟需量化分析两者之间的关系，揭示评价体系的潜在不公。

Method: 作者建立了包含约140,000对研究论文与代码仓库的数据集，并通过预测模型将论文作者与开发者账号进行匹配，进而分析了代码开发活动与学术署名和影响力指标之间的关系。

Result: 约30%的论文涉及了未署名的代码贡献者；代码贡献者的文章被引次数略有提升（约4.2%），但在控制学科、类型与开放获取后该影响不显著；一作更可能是代码贡献者；编码频率越高，其h指数往往越低，说明编码贡献与传统学术声誉评价之间存在负相关。

Conclusion: 软件开发在科学研究中的贡献往往未能通过传统学术署名获得正式认可，频繁进行编码的作者通常学术影响力指标（如h指数）较低，这反映出学术奖励机制与实际研究贡献之间存在脱节。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [8] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 该论文推出MLCPD数据集，以统一AST结构覆盖十种主流编程语言的七百多万源码文件，实现了跨语言结构的标准化和可对齐性，填补了现有数据集的不足，并已在Hugging Face等平台开源，助推跨语言程序分析与表征学习研究。


<details>
  <summary>Details</summary>
Motivation: 目前公开的代码语料库多局限于单一编程语言、分词级表示或各自为政的解析器，缺乏统一的结构化表示，无法方便进行跨语言结构分析与表示学习。因此，需要一个新的、统一的、多语言代码数据集。

Method: 作者提出了一个通用AST（抽象语法树）架构，并基于该架构，将十种主流编程语言的七百多万个源码文件进行解析与标准化，生成包含丰富元数据的新数据集MLCPD。所有数据以高效Parquet格式存储，配套提供完整处理流程、可视化工具和源代码。

Result: MLCPD数据展示了不同语言（如Python、Java、Go）在统一AST模式下的结构相似性，实验证明不同语言的语法图可以对齐，为跨语言结构分析和表示学习提供了实证基础。数据集和工具均已开放。

Conclusion: MLCPD为跨语言代码结构分析和表示学习奠定了统一、可复现的开源基础，将支持该领域的进一步研究和应用。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [9] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt通过静态分析和LLM相结合，显著提升了代码优化的成功率和实际性能，解决了传统检索方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的自动化代码优化方法难以捕捉语义等价但语法不同的优化示例，导致优化效果有限。需要更高效、更准确地发现和应用优化策略。

Method: 提出SemOpt框架，整合静态程序分析和LLM驱动组件，包括策略库构建、规则生成与代码优化，能够更精确地识别和改进优化点。

Result: 在151项优化任务的基准测试中，SemOpt比传统方法优化成功数提升1.38倍至28倍。在大型C/C++开源项目中，相关性能指标提升5.04%至218.07%。

Conclusion: SemOpt显著提升了自动化代码优化的有效性和实用性，在多个基准测试和实际开源工程中均取得了明显的性能提升。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [10] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 提出Code Digital Twin框架，通过融合AI与结构化知识，解决企业软件开发中隐性知识保存与持续演化难题，使LLM更有效适配超复杂系统开发。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程任务中表现出色，但企业软件开发涉及大量隐性知识与持续演化，常规LLM应用难以满足实际需求，需要对AI能力与软件开发现实进行深度对齐。

Method: 系统梳理LLM和企业软件开发面临的挑战，提炼AI与结构化知识结合的机会，提出以混合知识表示、多阶段抽取、增量更新、人机协作为核心的Code Digital Twin框架。

Result: Code Digital Twin框架能够保留并演进软件开发中的隐性与显性知识，将知识碎片化转化为可用资源，助推企业级AI辅助开发，实现复杂系统的智能、可持续演进。

Conclusion: 提出Code Digital Twin框架，将AI与结构化知识结合，实现对企业级软件开发复杂性的支持。该框架可增强AI辅助决策、知识保留与演进能力，有助于企业应对复杂系统演化。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [Six Proofs of Interpolation for the Modal Logic K](https://arxiv.org/abs/2510.16398)
*Nick Bezhanishvili,Balder ten Cate,Rosalie Iemhoff*

Main category: cs.LO

TL;DR: 本文系统地用六种不同技术证明了模态逻辑K的Craig插值定理，并详细比较了这些方法的优劣。


<details>
  <summary>Details</summary>
Motivation: Craig插值性质是逻辑学重要的元性质，对不同证明方法进行归纳总结有助于深入理解其本质。该工作针对模态逻辑K，比较六种不同插值定理的证明技术，从多角度评估其优劣。

Method: 作者对模态逻辑K的Craig插值定理，分别应用了六种不同的证明技术：模型论、证明论、句法法、自动机理论法、准模型法和代数法。对于每种方法，都给出了具体的证明过程。

Result: 通过对比分析，明确展示了六种证明技术在应用到模态逻辑K的Craig插值定理时的优缺点，总结了各种方法的适用场景和局限性。

Conclusion: 多样化的证明方法帮助更全面认识Craig插值定理在模态逻辑K中的成立原因，并为相关逻辑理论的发展提供多元工具。该对比为研究者选择适当的技术路径提供了借鉴。

Abstract: In this chapter, we present six different proofs of Craig interpolation for
the modal logic K, each using a different set of techniques (model-theoretic,
proof-theoretic, syntactic, automata-theoretic, using quasi-models, and
algebraic). We compare the pros and cons of each proof technique.

</details>


### [12] [Explainability Requirements as Hyperproperties](https://arxiv.org/abs/2510.16402)
*Bernd Finkbeiner,Julian Siber*

Main category: cs.LO

TL;DR: 本文提出了一种结合多种模态逻辑的新逻辑体系，可用于在多智能体自治系统中形式化和验证可解释性，且模型检测问题可判定，为自动化实现可解释性检测提供了理论支撑。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注于什么是有效的解释，较少将可解释性形式化为自治系统的系统性质。该工作着眼于如何将可解释性作为系统级属性在多智能体系统中进行规范和验证。

Method: 将三种模态逻辑（路易斯反事实、线性时序逻辑、知识模态）结合，提出用于建模和验证多智能体系统中的反事实可解释性的新逻辑体系，并进一步将其嵌入至超逻辑中进行分析。

Result: 提出的逻辑体系能够形式化系统级的多种可解释性定义，并能被嵌入至超逻辑中进行进一步分析，且模型检测是可判定的，为可解释性需求的自动化校验提供了理论保障。

Conclusion: 本文证明了提出的逻辑体系在模型检测方面是可判定的，这为自动化验证自治系统的可解释性要求奠定了基础。

Abstract: Explainability is emerging as a key requirement for autonomous systems. While
many works have focused on what constitutes a valid explanation, few have
considered formalizing explainability as a system property. In this work, we
approach this problem from the perspective of hyperproperties. We start with a
combination of three prominent flavors of modal logic and show how they can be
used for specifying and verifying counterfactual explainability in multi-agent
systems: With Lewis' counterfactuals, linear-time temporal logic, and a
knowledge modality, we can reason about whether agents know why a specific
observation occurs, i.e., whether that observation is explainable to them. We
use this logic to formalize multiple notions of explainability on the system
level. We then show how this logic can be embedded into a hyperlogic. Notably,
from this analysis we conclude that the model-checking problem of our logic is
decidable, which paves the way for the automated verification of explainability
requirements.

</details>


### [13] [Bilateralist base-extension semantics with incompatible proofs and refutations](https://arxiv.org/abs/2510.16763)
*Victor Barroso-Nascimento,Maria Osório Costa,Elaine Pimentel*

Main category: cs.LO

TL;DR: 本文提出了断言和否定独立又对立的双边自然演绎系统，避免可证可驳并存，扩展和强化了直觉主义逻辑，适合建构性认识论推理。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑往往只强调断言（assertion），而忽视否定（denial）的独立地位。逻辑双边主义（bilateralism）理论挑战了这一点，认为断言与否定是独立但对立的行为，并探讨这些行为在直觉主义逻辑（intuitionistic logic）中的解释。本文旨在构建一种能避免自相矛盾的双边系统，从而更好地建模如数学证明和反驳等具有认识论意义的实体。

Method: 论文提出了一种双边自然演绎系统（bilateral natural deduction system），并具备范式化（normalisation）等良好的证明论性质。同时提出了一种基础-扩展（base-extension）语义，该语义要求对每个公式必须分别显式构造证明和反驳，且保证它们不能共存。通过证明该语义与演算的完备性和正确性，论证了该系统的严格性。

Result: 提出的系统使得逻辑中任何公式不能既可证又可驳，保障了逻辑系统内在的一致性。反驳的概念与Nelson的“建构性伪”（constructive falsity）相符，系统既扩展了直觉主义逻辑，又强化了其在建构性认识论推理中的适用性。

Conclusion: 该系统为断言与否定各自独立但互斥的形式化，增强了逻辑系统表达建构性认识论对象（如证明和反驳）的能力，也为将Nelson的建构性否定纳入更广泛范畴提供了理论基础。

Abstract: Logical bilateralism challenges traditional concepts of logic by treating
assertion and denial as independent yet opposed acts. While initially devised
to justify classical logic, its constructive variants show that both acts admit
intuitionistic interpretations. This paper presents a bilateral system where a
formula cannot be both provable and refutable without contradiction, offering a
framework for modelling epistemic entities, such as mathematical proofs and
refutations, that exclude inconsistency.
  The logic is formalised through a bilateral natural deduction system with
desirable proof-theoretic properties, including normalisation. We also
introduce a base-extension semantics requiring explicit constructions of proofs
and refutations while preventing them from being established for the same
formula. The semantics is proven sound and complete with respect to the
calculus. Finally, we show that our notion of refutation corresponds to David
Nelson's constructive falsity, extending rather than revising intuitionistic
logic and reinforcing the system's suitability for representing constructive
epistemic reasoning.

</details>


### [14] [ATL*AS: An Automata-Theoretic Approach and Tool for the Verification of Strategic Abilities in Multi-Agent Systems](https://arxiv.org/abs/2510.17306)
*Sofia Garcia de Blas Garcia-Alcalde,Francesco Belardinelli*

Main category: cs.LO

TL;DR: 本文提出并实现了两种针对ATL*模型检测的创新符号算法，显著提升了验证效率和规模，尤其是在无限轨迹情形，实验和工具验证其优越性。


<details>
  <summary>Details</summary>
Motivation: ATL*是一种重要的时序逻辑，用于多智能体系统规范验证。但传统的显式状态方法在规模和效率上存在瓶颈，尤其是在处理无限轨迹语义时，现有工具的可伸缩性与效率亟待提升。

Method: 提出了两种关于ATL*模型检测的创新符号算法，分别适用于无限轨迹和有限轨迹语义。对于无限轨迹，通过符号化归约到奇偶博弈(parity games)实现。算法在ATL*AS模型检测工具中实现，并通过合成基准与网络安全场景进行了实验评测。

Result: 实验结果表明，基于符号方法的实现显著优于显式状态方法，奇偶博弈算法可扩展性和性能更优；有限轨迹语义下的模型检测性能优势显著。ATL*AS工具能有效支持ATL*规范的自动化验证，提升了多智能体系统的实际应用价值。

Conclusion: 我们提出的符号化方法在ATL*模型检测上极大优于现有的显式状态方法，尤其是在处理无限轨迹验证时表现更高效、更具扩展性，并通过ATL*AS工具集成，为多智能体系统的ATL*规范验证提供了完整工具链。

Abstract: We present two novel symbolic algorithms for model checking the
Alternating-time Temporal Logic ATL*, over both the infinite-trace and the
finite-trace semantics. In particular, for infinite traces we design a novel
symbolic reduction to parity games. We implement both methods in the ATL*AS
model checker and evaluate it using synthetic benchmarks as well as a
cybersecurity scenario. Our results demonstrate that the symbolic approach
significantly outperforms the explicit-state representation and we find that
our parity-game-based algorithm offers a more scalable and efficient solution
for infinite-trace verification, outperforming previously available tools. Our
results also confirm that finite-trace model checking yields substantial
performance benefits over infinite-trace verification. As such, we provide a
comprehensive toolset for verifying multiagent systems against specifications
in ATL*.

</details>


### [15] [A Judgmental Construction of Directed Type Theory](https://arxiv.org/abs/2510.17494)
*Jacob Neumann*

Main category: cs.LO

TL;DR: 本文通过多上下文区域的逻辑演算重新表述了有向类型理论的进展，引入中性和极性变量，提供了重写理论的新逻辑描述，并提出了dual CwF，拓展了该类逻辑的范畴模型基础。


<details>
  <summary>Details</summary>
Motivation: 旨在用更结构化、更灵活的逻辑体系来表达和扩展有向类型理论，使其能够更好地刻画和处理范畴结构、重写理论，并统一模型理论基础。

Method: 本研究采用了Pfenning和Davies的多上下文'区域'框架，引入了两类变量（'中性'与'极性'），并重点分析最低维度情形（类型为合成预序时）。进一步发展了双上下文系统的范畴语义，提出dual CwF。

Result: 论文展示了多区域逻辑语言在重写理论表达中的应用，并提出了dual CwF的概念，丰富了相关逻辑的范畴模型基础。

Conclusion: 该论文将有向类型理论的最新进展重新表述为多上下文区域的逻辑演算，并发展了双上下文系统的范畴语义学，提出了dual CwF的新概念，为此类逻辑的模型理论提供了结构基础。

Abstract: We reformulate recent advances in directed type theory--a type theory where
the types have the structure of synthetic (higher) categories--as a logical
calculus with multiple context 'zones', following the example of Pfenning and
Davies. This allows us to have two kinds of variables--'neutral' and
'polar'--with different functoriality requirements. We focus on the
lowest-dimension version of this theory (where types are synthetic preorders)
and apply the logical language to articulate concepts from the theory of
rewriting. We also take the occasion to develop the categorical semantics of
dual-context systems, proposing a notion of dual CwF to serve as a common
structural base for the model theories of such logics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文将QNLP方法用于自然语言推理并与经典模型对比，结果表明在参数极少的情况下，量子模型可达到类似精度且每参数学习效率极高，同时提出的聚类参数结构增强了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言推理与语义建模在低资源和结构敏感场景下面临挑战，传统深度学习模型参数众多，效率不高。量子自然语言处理（QNLP）因其天然的组合结构和潜在高效性而被认为有望改善现状，亟需实证比较其与经典模型的表现及优势。

Method: 利用lambeq库和DisCoCat框架，搭建针对句子对的可参数化量子电路，并在语义相关性和推理分类任务上进行训练。引入信息增益每参数（IGPP）作为新的信息论评估指标，并研究了随机初始化transformer、标准transformer、混合模型与量子模型在少样本设定下的表现。

Result: 量子模型在测试集上的错误率较低，并显著优于随即初始化的transformer。量子模型在信息增益每参数上表现出远高于经典模型（最高超过5个数量级）的学习效率。基于聚类的参数共享机制提升了泛化性能。

Conclusion: 量子模型在参数远少于经典模型的情况下，能够实现与经典基线模型相当的效果，并且量子模型的每参数学习效率远优于经典模型。提出的基于聚类的量子电路结构进一步提升了泛化能力。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [17] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 通过融合ChatGPT和Claude两个大语言模型，并加入临床文本，采用输出共识策略，显著提升了AI对胸部X光片的诊断准确率，为AI医学影像诊断增强提供了低成本且高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 胸部X光的自动解读有助于提高诊断效率和准确性，但单一模型的稳定性和可靠性有限，存在误诊风险。因此，如何利用多种AI模型和多模态信息提升诊断性能，是该研究关注的问题。

Method: 采用了多模型融合框架，结合了两种大型语言模型（ChatGPT和Claude），并通过输出相似性阈值（95%）实现共识决策。在二维实验组中，分别只输入影像数据进行测试；在多模态组，则生成模拟临床笔记，与影像一同输入模型，比较各自及融合方式的诊断表现。

Result: 仅影像输入时，ChatGPT准确率为62.8%，Claude为76.9%，共识后为77.6%；影像+文本输入时，ChatGPT提升至84%，Claude为76%，共识提升至91.3%。每种实验条件下，基于共识的融合均高于任一单独模型。

Conclusion: 多模态信息和模型输出级共识结合，可有效提升AI辅助胸部X光诊断的可靠性和实用性，可降低诊断错误，且成本不高。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [18] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: CorrectBench系统性评测了多种大模型自我纠错方法，验证了自我纠错对推理准确率有提升作用，但效率有待优化，CoT方法表现突出。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）被广泛应用于推理任务，模型自我纠错能力成为提升其推理性能的关键。但目前针对不同自我纠错方法的系统性评估尚缺乏，且LLM是否真的能够有效自我纠正仍值得关注。

Method: 提出并构建了一个名为CorrectBench的评测基准，系统性地评估了多种自我纠错方法（内在、外部以及微调），并在常识推理、数学推理和代码生成三项任务上进行测试。研究同时比较不同纠错方法的组合效果。

Result: 自我纠错方法能提升模型准确率，特别是在复杂推理任务中；混合不同方法进一步提升效果但效率降低；某些推理型LLM（如DeepSeek-R1）通过自我纠错提升有限且耗时较高；简单的chain-of-thought (CoT) 基线虽然方法简单，却在准确率和效率上展现竞争力。

Conclusion: 自我纠错有助于提升LLM推理能力，但效率提升依然是挑战。未来研究应进一步优化性能与效率之间的平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [19] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 本文提出EvolveR框架，通过离线自蒸馏和在线强化学习，赋予大语言模型持续自我优化的能力，在多跳问答任务上超越现有基线，为打造更自主可进化智能体奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）虽在工具使用方面表现优异，但缺乏从自身经验中系统性学习和自我改进的能力。目前的框架主要关注于弥补外部知识差距，未能根本解决模型无法迭代优化解题策略的问题。

Method: 提出了EvolveR框架，包括两个核心阶段：（1）离线自蒸馏，将模型实际交互过程提炼成结构化、可复用的策略原则；（2）在线交互，模型在任务中实时调用这些策略原则，并通过多样化行为轨迹自我积累经验，同时借助强化学习机制，基于表现持续更新模型。

Result: 在复杂多跳问答任务基准上，EvolveR框架在性能上明显优于现有强力基准方法。

Conclusion: EvolveR为智能体提供了完整的从自我经验中闭环自学习的蓝图，不仅能利用外部数据，还能迭代吸收自身行为后果，实现更高水平的自主性和持续改进。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [20] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 针对系统综述文献筛选，系统评估了LLM与提示的交互，发现不同组合性能和成本差异明显，推荐先用低成本模型初筛、疑难再交高性能模型处理，有助提升自动化筛文效率及成本控制。


<details>
  <summary>Details</summary>
Motivation: 当前系统性文献综述（SLR）的筛选阶段人工成本高，而大语言模型（LLMs）有潜力实现自动化，但不同模型与提示方式如何影响表现缺乏系统分析。

Method: 对六个主流大语言模型与五种提示方法进行组合实验，涵盖相关性分类及六个二级任务，用准确率、精度、召回率和F1值等多指标考察表现；并以每千篇摘要为单位比较成本—性能。

Result: CoT-few-shot提示整体精度-召回表现最好，zero-shot适合高召回需求；self-reflection不稳定且包容性过高。GPT-4o与DeepSeek综合表现最佳，GPT-4o-mini性价比高。不同模型-提示组合成本与表现差异大。提出低成本模型+结构化提示优先，边界情况再升级至高性能模型的流程建议。

Conclusion: 模型和提示方式对自动化文献筛选效果影响显著，应采用分阶段流程提升效益。本研究为基于任务调优的LLM部署提供实证对比与实用建议。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [21] [Forward-Backward Binarization](https://arxiv.org/abs/2510.16183)
*Ismail Belgacem,Franck Delaplace*

Main category: cs.DM

TL;DR: 提出了一种基于调控网络补全的基因表达数据二值化方法，有效提升数据二值化准确性和生物学一致性，为实现可靠的布尔GRN建模奠定基础。


<details>
  <summary>Details</summary>
Motivation: Boolean基因调控网络（GRN）的建模需要将基因表达数据进行二值化，但传统的阈值法易于过度简化生物学本质，并难以应对基因特异性、测量不确定性和快照式数据等问题。为提升二值化的准确性和生物学一致性，亟需一种更科学的二值化方法。

Method: 提出了一种基于调控关系的二值化新方法，结合常规阈值法和调控图结构，通过在调控者与靶基因间传播布尔调控规则，实现功能性的二值化补全。该方法能够推断缺失或不确定的表达值，并保持数据与生物调控网络及布尔建模的科学性和一致性。

Result: 通过人工和已有布尔GRN的ODE仿真验证，所提方法在二值化准确性和鲁棒性方面优于传统阈值法，能够提升布尔网络建模的可靠性。

Conclusion: 调控网络引导下的二值化算法有效克服了阈值法在快照数据处理中的局限性，提升了布尔GRN模型的生物学可信度与后续分析的可靠性。

Abstract: Binarization of gene expression data is a \textbf{critical prerequisite} for
the synthesis of Boolean gene regulatory network (GRN) models from omics
datasets. Because Boolean networks encode gene activity as binary variables,
the accuracy of binarization directly conditions whether the inferred models
can faithfully reproduce biological experiments, capture regulatory dynamics,
and support downstream analyses such as controllability and therapeutic
strategy design. In practice, binarization is most often performed using
thresholding methods that partition expression values into two discrete levels,
representing the absence or presence of gene expression. However, such
approaches oversimplify the underlying biology: gene-specific functional roles,
measurement uncertainty, and the scarcity of time-resolved experimental data
render thresholding alone insufficient. To overcome these limitations, we
propose a novel \textbf{regulation-based binarization method} tailored to
snapshot data. Our approach combines thresholding with functional binary value
completion guided by the regulatory graph, propagating values between
regulators and targets according to Boolean regulation rules. This strategy
enables the inference of missing or uncertain values and ensures that
binarization remains biologically consistent with both regulatory interactions
and Boolean modeling principles of the gene regulation. Validation against ODE
simulations of artificial and established Boolean GRNs demonstrates that the
method achieves accurate and robust binarization, thereby strengthening the
reliability of Boolean network synthesis.

</details>


### [22] [Random generation of universal cycles and de Bruijn sequences](https://arxiv.org/abs/2510.16545)
*Joe Sawada,Daniel Gabrić*

Main category: cs.DM

TL;DR: 本文提出了实用的通用循环随机生成算法，适用多种组合对象，利用 de Bruijn 图和随机游走原理，理论与实验结果均显示生成效率高且分布均匀。


<details>
  <summary>Details</summary>
Motivation: 生成均匀分布的通用循环（universal cycles）在组合学等领域有广泛应用，但缺乏高效、实用的随机生成算法。本论文旨在提出实际可用的生成算法，并扩展覆盖多种组合对象。

Method: 针对多种组合对象（如简写排列、子集、多重集排列、弱序、可定向序列、de Bruijn 序列、带权 de Bruijn 序列以及带限制的 de Bruijn 序列），作者提出基于随机游走的算法：首先从给定集合中随机选取一个元素，使用它作为起点在底层欧拉 de Bruijn 图上执行随机游走，生成随机生成树（arborescence），进而在常数时间内生成对应的随机通用循环。文中还采用 Las Vegas 算法测算生成任意对象所需的平均覆盖时间（cover time），并给出实验结果。

Result: 作者提出的算法可以针对多种组合对象高效、均匀地生成通用循环，每个符号生成仅需常数时间。实验表明，利用随机游走的 Las Vegas 算法求解随机生成树的平均覆盖时间表现良好，验证了算法的实用性和高效性。

Conclusion: 论文提供了一套实用、高效的生成通用循环的随机算法，适用于多种常见的组合对象，并通过实验展示了其平均性能和广泛适用性。创新点在于结合随机游走与 de Bruijn 图，极大提升了生成通用循环的效率和可操作性。

Abstract: We present practical algorithms for generating universal cycles uniformly at
random. In particular, we consider universal cycles for shorthand permutations,
subsets and multiset permutations, weak orders, and orientable sequences.
Additionally, we consider de Bruijn sequences, weight-range de Bruin sequences,
and de Bruijn sequences, with forbidden $0^z$ substring. Each algorithm, seeded
with a random element from the given set, applies a random walk of an
underlying Eulerian de Bruijn graph to obtain a random arborescence (spanning
in-tree). Given the random arborescence and the de Bruijn graph, a
corresponding random universal cycle can be generated in constant time per
symbol. We present experimental results on the average cover time needed to
compute a random arborescence for each object using a Las Vegas algorithm.

</details>


### [23] [New results on $B_α$-eigenvalues of a graph](https://arxiv.org/abs/2510.16812)
*Germain Pastén,Carla Silva Oliveira,João Domingos G. da Silva Junior,Claudia M. Justel*

Main category: cs.DM

TL;DR: 本文研究了图的邻接矩阵和拉普拉斯矩阵凸组合$B_\alpha(G)$在某些顶点集满足条件下的特征值性质和正半定性。


<details>
  <summary>Details</summary>
Motivation: 受2024年Samanta等人提出$A(G)$和$L(G)$的凸线性组合$B_\alpha(G)$启发，进一步研究其特征值性质及正半定性问题。

Method: 通过研究图的邻接矩阵$A(G)$和拉普拉斯矩阵$L(G)$的凸线性组合$B_\alpha(G)$，分析其在特定顶点条件下的特征值分布以及正半定性。

Result: 给出了在特殊顶点集满足某些条件时$B_\alpha(G)$的特征值及其重数的若干结果，并对$B_\alpha(G)$正半定性进行了系统讨论。

Conclusion: 本文得出结论：在某些顶点满足特定条件时，$B_\alpha(G)$的特征值及其重数具有一定规律，并且讨论了$B_\alpha(G)$的正半定性问题。

Abstract: Let $G$ be a graph with adjacency matrix $A(G)$ and Laplacian matrix $L(G)$.
In 2024, Samanta \textit{et} \textit{al.} defined the convex linear combination
of $A(G)$ and $L(G)$ as $B_\alpha(G) = \alpha A(G) + (1-\alpha)L(G)$, for
$\alpha \in [0,1]$. This paper presents some results on the eigenvalues of
$B_{\alpha}(G)$ and their multiplicity when some sets of vertices satisfy
certain conditions. Moreover, the positive semidefiniteness problem of
$B_{\alpha}(G)$ is studied.

</details>


### [24] [Efficient recognition algorithms for $(1,2)$-, $(2,1)$- and $(2,2)$-graphs](https://arxiv.org/abs/2510.17665)
*Flavia Bonomo-Braberman,Min Chih Lin,Ignacio Maqueda*

Main category: cs.DM

TL;DR: 本文针对部分(k,ℓ)-图识别问题，提出了更高效的新算法，把关键类型的识别时间复杂度显著降低，推动图识别效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有(k,ℓ)-图的识别算法在某些情况下复杂度很高，特别是对于(2,1)、(1,2)和(2,2)三类，限制了其应用。作者希望通过优化算法提升识别效率。

Method: 作者通过改进算法设计，利用已有图结构理论，优化了对(2,1)、(1,2)和(2,2)类图的识别过程。具体通过分析图及其补图的结构特性，简化算法运行步骤。

Result: 对(2,1)-图，时间复杂度由O((n+m)^2)降至O(n^2+nm)；对(1,2)-图，由O((n+\overline{m})^2)降至O(n^2+n\overline{m})；对(2,2)-图，由O(n^{10}(n+m))大幅优化至O(n^4(n+min{m,\overline{m}})^3)。

Conclusion: 本文提出了针对(2,1)、(1,2)和(2,2)类型图的新识别算法，大幅降低了时间复杂度，提高了识别效率。

Abstract: A graph $G$ is said to be a $(k,\ell)$-graph if its vertex set can be
partitioned into $k$ independent sets and $\ell$ cliques. It is well
established that the recognition problem for $(k,\ell)$-graphs is NP-complete
whenever $k \geq 3$ or $\ell \geq 3$, while it is solvable in polynomial time
otherwise. In particular, for the case $k+\ell \leq 2$, recognition can be
carried out in linear time, since split graphs coincide with the class of
$(1,1)$-graphs, bipartite graphs correspond precisely to $(2,0)$-graphs, and
$(\ell,k)$-graphs are the complements of $(k,\ell)$-graphs. Recognition
algorithms for $(2,1)$- and $(1,2)$-graphs were provided by Brandst\"adt, Le
and Szymczak in 1998, while the case of $(2,2)$-graphs was addressed by Feder,
Hell, Klein, and Motwani in 1999. In this work, we refine these results by
presenting improved recognition algorithms with lower time complexity.
Specifically, we reduce the running time from $O((n+m)^2)$ to $O(n^2+nm)$ for
$(2,1)$-graphs, from $O((n+\overline{m})^2)$ to $O(n^2+n\overline{m})$ for
$(1,2)$-graphs, and from $O(n^{10}(n+m))$ to $O(n^4
(n+\min\{m,\overline{m}\})^3)$ for $(2,2)$-graphs. Here, $n$ and $m$ denote the
number of vertices and edges of the input graph $G$, respectively, and
$\overline{m}$ denotes the number of edges in the complement of $G$.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [25] [Inference of Deterministic Finite Automata via Q-Learning](https://arxiv.org/abs/2510.17386)
*Elaheh Hosseinkhani,Martin Leucker*

Main category: cs.FL

TL;DR: 本文创新性地将强化学习Q-learning方法用于被动DFA推断，将Q函数转化为自动机的转移函数，实验证明方法有效，为机器学习与符号推理结合提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有DFA（确定性有限自动机）推断方法大多源自符号AI，包括主动学习（如Angluin的L*算法）和被动学习方法；而子符号AI尤其是机器学习提供了新的从数据学习范式。本研究的动机是探索将强化学习（Q-learning）应用于DFA被动推断的可能性，并尝试建立子符号学习与符号推断之间的桥梁。

Method: 本研究提出使用Q-learning（强化学习中的知名算法）进行DFA的被动推断。作者将Q函数（状态-动作对到奖励的映射）重新解释为DFA的状态转移函数，并对此进行了算法适配与实现。

Result: 通过实验证明Q-learning在若干DFA推断示例上可以有效工作，验证了该方法的可行性和创新性。

Conclusion: Q-learning可应用于DFA的被动推断，并能自然地将学习到的Q函数转化为DFA的转移函数，实现子符号方法与符号知识表征的结合。

Abstract: Traditional approaches to inference of deterministic finite-state automata
(DFA) stem from symbolic AI, including both active learning methods (e.g.,
Angluin's L* algorithm and its variants) and passive techniques (e.g., Biermann
and Feldman's method, RPNI). Meanwhile, sub-symbolic AI, particularly machine
learning, offers alternative paradigms for learning from data, such as
supervised, unsupervised, and reinforcement learning (RL). This paper
investigates the use of Q-learning, a well-known reinforcement learning
algorithm, for the passive inference of deterministic finite automata. It
builds on the core insight that the learned Q-function, which maps state-action
pairs to rewards, can be reinterpreted as the transition function of a DFA over
a finite domain. This provides a novel bridge between sub-symbolic learning and
symbolic representations. The paper demonstrates how Q-learning can be adapted
for automaton inference and provides an evaluation on several examples.

</details>


### [26] [Castor Ministerialis](https://arxiv.org/abs/2510.17438)
*Christian Hercher*

Main category: cs.FL

TL;DR: 本文提出了修正版busy beaver问题，即从空白带起始且在空白带终止的图灵机，全面解决了5状态内的最大步数问题，分析了6状态候选，并部分讨论了多符号情形。


<details>
  <summary>Details</summary>
Motivation: 原始的busy beaver问题仅要求空白带开始并停机，本文提出了进一步收紧的约束——不仅要从空白带开始，还要必须在空白带停机，探索其极限行为，并系统给出若干状态下的答案。

Method: 针对限制条件（必须从空白带开始且在空白带终止）的busy beaver问题，通过理论分析与穷举，给出了小状态数（最多五状态）的详尽答案；六状态通过候选举例进行分析，推广到多符号则讨论部分性质。

Result: 获得了5状态下此variant的精确busy beaver值，对6状态的候选进行了分析，并初探了m符号情景的行为。

Conclusion: 作者对“从空白带子开始并最终停机（在空白带上停机）”的图灵机，在状态数至五的情形下给出了精确最大运行步数；还分析了六状态候选机，并对m符号字母表的情形作了部分研究。

Abstract: The famous problem of Busy Beavers can be stated as the question on how long
a $n$-state Turing machine (using a 2-symbol alphabet or -- in a generalization
-- a $m$-symbol alphabet) can run if it is started on the blank tape before it
holds. Thus, not halting Turing machines are excluded. For up to four states
the answer to this question is well-known. Recently, it could be verified that
the widely assumed candidate for five states is in fact the champion. And there
is progress in searching for good candidates with six or more states.
  We investigate a variant of this problem: Additionally to the requirement
that the Turing machines have to start from the blank tape we only consider
such Turing machines that hold on the blank tape, too. For this variant we give
definitive answers on how long such a Turing machine with up to five states can
run, analyze the behavior of a six-states candidate and give some findings on
the generalization of Turing-machines with $m$-symbol alphabet.

</details>


### [27] [Non-interference analysis of bounded labeled Petri nets](https://arxiv.org/abs/2510.17582)
*Ning Ran,Zhengguang Wu,Shaokang Zhang,Zhou He,Carla Seatzu*

Main category: cs.FL

TL;DR: 本文针对有界标注Petri网系统的信息安全，研究了一种更严格的非干扰性（SNNI），提出了以SNNI Verifier自动机为基础的判定方法和完整的理论判据，有助于信息安全系统的分析和验证。


<details>
  <summary>Details</summary>
Motivation: 在分层控制系统中，不同层级用户能观测到的信息不同。低层用户不应通过自己可以观测的信息推断出高层事件的发生，因此需要对系统进行非干扰性分析以保障信息安全。

Method: 通过引入一种特殊的自动机（SNNI Verifier）来判定系统是否满足强非确定性非干扰性（SNNI）属性。

Result: 研究了强非确定性非干扰性（SNNI）属性，并利用SNNI Verifier给出了SNNI的必要和充分条件。

Conclusion: 本文提出了针对有界标注Petri网信息安全中的非干扰性分析问题的方法，提出了一种必要且充分的SNNI判定条件。

Abstract: This paper focuses on a fundamental problem on information security of
bounded labeled Petri nets: non-interference analysis. As in hierarchical
control, we assume that a system is observed by users at different levels,
namely high-level users and low-level users. The output events produced by the
firing of transitions are also partitioned into high-level output events and
low-level output events. In general, high-level users can observe the
occurrence of all the output events, while low-level users can only observe the
occurrence of low-level output events. A system is said to be non-interferent
if low-level users cannot infer the firing of transitions labeled with
high-level output events by looking at low-level outputs. In this paper, we
study a particular non-interference property, namely strong non-deterministic
non-interference (SNNI), using a special automaton called SNNI Verifier, and
propose a necessary and sufficient condition for SNNI.

</details>
