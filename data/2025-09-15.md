<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 52]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出SWE-Effi新指标体系，填补AI系统实际效能评估空白，发现准确性和资源消耗需综合考量，有助于更具现实约束的AI选型与部署。


<details>
  <summary>Details</summary>
Motivation: 当前SWE AI排行榜仅关注结果准确性，忽视实际部署中的资源消耗与效率，而现实世界往往资源有限，AI不仅要正确，还要高效。

Method: 提出新的评估指标SWE-Effi，衡量AI系统准确性与资源消耗的整体效能；并在SWE-bench基准子集上，使用多维度指标，对主流AI系统进行重新排序和评估。

Result: 发现AI效能显著受实现细节影响，并揭示了“token雪崩”“昂贵失败”两大资源浪费现象。实验证明，必须在token和时间预算之间做权衡。

Conclusion: AI系统的效率不只是准确率，还需要资源消耗的平衡。系统的效能不仅取决于其本身，还与其和基础模型的结合方式有关。资源浪费（如“token雪崩”和“昂贵失败”）是主要挑战。此外，存在token限制和时间限制下效能的权衡。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [2] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 开放源代码AI在模型授权应用时存在大量许可冲突，造成法律风险。作者大规模审计Hugging Face及其下游应用，发现超三分之一模型重授权消除限制条款，原型工具能解决大部分冲突。研究为合规模型应用提供了数据与自动化工具，对AI合规治理具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 在开放源代码AI生态系统中，隐藏的许可协议冲突带来法律和伦理风险，但目前缺乏对这些冲突发生频率、来源及影响社区的系统性数据驱动研究。

Method: 首次对Hugging Face上的数据集和模型，以及它们在开源软件应用中的下游集成进行端到端的许可审计，分析了364,000个数据集、160万个模型和140,000个GitHub项目，并原型开发了一个可扩展的规则引擎，编码近200条SPDX和模型相关条款，用于检测许可冲突。

Result: 实证分析发现，35.5%的模型到应用的转化过程中通过重新许可消除了限制性条款，且原型规则引擎能够解决86.4%的软件应用中的许可冲突。

Conclusion: 许可合规是开放源代码AI治理的核心挑战。研究为自动化、AI相关的大规模合规性提供了数据和工具支持，并发布了相关数据集和引擎以推动后续研究。

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [3] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec通过程序切片和逻辑删除两新阶段，显著提升了复杂循环程序规范生成的质量和验证性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的自动化规范生成方法在处理包含复杂循环结构的程序时效果有限，生成的规范常常不相关、不完整且含糊；同时，验证工具的严格证明义务和设计约束也加剧了这些问题。

Method: 提出了SLD-Spec方法，在传统规范生成流程中引入两个新阶段：（1）切片阶段，将函数分解成包含独立循环结构的代码片段，简化规范生成的复杂度；（2）逻辑删除阶段，利用LLM推理能力过滤掉验证工具难以识别的不正确规范，保留有效规范。

Result: 在简单数据集上，SLD-Spec比最先进的AutoSpec多成功验证5个程序，运行时间减少23.73%；在人工构建的四类复杂循环程序数据集上，生成规范的正确性、相关性与完整性大幅提升，95.1%的断言和90.91%的程序通过验证。消融实验显示逻辑删除对于提升规范正确性与相关性至关重要，而程序切片则显著提升规范的完整性。代码和数据已公开。

Conclusion: SLD-Spec能够提升复杂循环程序规范自动生成的质量和验证通过率，有效解决了现有方法在相关性和完整性上的不足，实现了更高效的端到端自动化软件验证。

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [4] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 提出WALL工具，实现代码问题检测、修正和评估的自动化流程，有效提升效率和质量，未来可实现完全自动化代码管理。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目日益复杂，代码文件中的问题数量和类型大幅增加，传统人工检测和修复方式费时费力。因此，亟需高效的自动化工具来实现代码问题的检测、修正与评估。

Method: 提出并实现了名为WALL的Web应用，将SonarQube与大模型（GPT-3.5 Turbo，GPT-4o）结合，自动化完成代码问题提取、自动修复及修复效果对比评估。包括三大模块：问题提取工具、自动修正工具、代码比对工具。

Result: 在563份文件、7599个问题上的实验证明，WALL可以有效减少人工投入，同时保持高质量的代码修正。混合使用高性价比与先进大模型能进一步降低成本并提升修正率。

Conclusion: WALL整合静态分析和大语言模型，为自动化代码质量管理提供了高效方案。未来将集成开源大模型并实现全自动流程，进一步消除人工干预。

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [5] [Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://arxiv.org/abs/2509.10236)
*Mingyi Li,Junmin Xiao,Siyan Chen,Hui Ma,Xi Chen,Peihua Bao,Liang Yuan,Guangming Tan*

Main category: cs.SE

TL;DR: Stencil-Lifting通过新的分层递归方法和数据依赖建模，大幅提升了底层stencil代码到领域特定语言的自动化迁移效率，实测速度比最优系统快5.8到31倍，且保证了功能等价。


<details>
  <summary>Details</summary>
Motivation: 现有的代码转换系统（verified lifting systems）在将底层语言写成的stencil模式代码迁移到高层领域特定语言（DSL）时存在效率瓶颈。提升老旧优化技术与现代DSL范式之间的高效衔接，成为重要需求。

Method: 提出Stencil-Lifting系统，采用分层递归提升理论和分层递归提升算法，将底层stencil代码转化为等价的DSL实现。该方法利用定制数据依赖图（invariant subgraphs）对循环结构进行建模，通过谓词化摘要表示计算语义，避免外部验证，并通过递归算法保证过程收敛和终止。

Result: 在多个基准测试和实际应用中，Stencil-Lifting相比于当前领先系统（STNG、Dexter），分别获得了31.62倍和5.8倍的加速，同时保持等价语义，显著提升了自动化迁移效率。

Conclusion: Stencil-Lifting极大提高了底层stencil代码向DSL自动迁移的效率与准确性，缩小了遗留优化技术与现代编程范式之间的差距。

Abstract: We introduce Stencil-Lifting, a novel system for automatically converting
stencil kernels written in low-level languages in legacy code into semantically
equivalent Domain-Specific Language (DSL) implementations. Targeting the
efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting
achieves scalable stencil kernel abstraction through two key innovations.
First, we propose a hierarchical recursive lifting theory that represents
stencil kernels, structured as nested loops, using invariant subgraphs, which
are customized data dependency graphs that capture loop-carried computation and
structural invariants. Each vertex in the invariant subgraph is associated with
a predicate-based summary, encoding its computational semantics. By enforcing
self-consistency across these summaries, Stencil-Lifting ensures the derivation
of correct loop invariants and postconditions for nested loops, eliminating the
need for external verification. Second, we develop a hierarchical recursive
lifting algorithm that guarantees termination through a convergent recursive
process, avoiding the inefficiencies of search-based synthesis. The algorithm
efficiently derives the valid summaries of stencil kernels, and its
completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil
benchmarks from two different suites and on four real-world applications.
Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$
and 5.8$\times$ speedups compared to the state-of-the-art verified lifting
systems STNG and Dexter, respectively, while maintaining full semantic
equivalence. Our work significantly enhances the translation efficiency of
low-level stencil kernels to DSL implementations, effectively bridging the gap
between legacy optimization techniques and modern DSL-based paradigms.

</details>


### [6] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 针对小型语言模型在代码生成中的能耗问题，本研究发现巧妙设计提示（如链式思考）能在部分模型上明显降低能耗，提示效能具模型相关性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在软件开发中的高能耗和碳排放引发了环境关注，作者希望寻找更可持续的替代方案，关注小型语言模型（SLM）的节能表现，探讨提示工程在提升SLM能效上的作用。

Method: 评估四款开源小型语言模型（StableCode-Instruct-3B、Qwen2.5-Coder-3B-Instruct、CodeLlama-7B-Instruct、Phi-3-Mini-4K-Instruct），在150道不同难度的LeetCode Python题目上测试四种提示策略（角色、零样本、少样本、链式思考），并对每个生成解决方案记录运行时间、内存占用和能耗，与人工编写的基线进行比较。

Result: 链式思考（CoT）提示在Qwen2.5-Coder和StableCode-3B模型上稳定带来能耗节省；而CodeLlama-7B和Phi-3-Mini-4K未能在任何提示策略下超过人工基线。模型间提示效能差异显著。

Conclusion: 提示工程的节能效益依赖于所选模型，合理设计提示能够引导SLM实现更绿色的软件开发，但具体效益需针对模型进行定制化探索。

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [7] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: 本文提出利用LLM自动审核软件设计文档，提出11个审核视角，并改进LLM理解复杂文档的能力。实验证明LLMs可有效识别文档不一致性，具备实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 自动化软件设计文档审核过程，提高审核效率，降低人工参与，探索LLM在此领域的实际应用潜力。

Method: 分析设计文档的审核方法，组织了11个审核视角。研究了LLM在这些视角下的应用，并针对可由LLM审核的视角开发了新技术，提升其理解包含表格数据的复杂设计文档的能力。实验选用GPT进行设计项和不同文档描述的一致性审核。

Result: 结果显示，LLM能够在文档审核过程中发现设计文档间的不一致性，证实了其自动化审核的有效性。

Conclusion: LLM可以用于自动审核软件设计文档，能识别文档中存在的不一致性。

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [8] [Sustaining Research Software: A Fitness Function Approach](https://arxiv.org/abs/2509.10085)
*Philipp Zech,Irdin Pekaric*

Main category: cs.SE

TL;DR: 这篇论文提出用适应函数自动化量化和管理研究软件的FAIR属性，实现了更好的可持续性和科学影响。


<details>
  <summary>Details</summary>
Motivation: 研究软件常因维护性差、适应性弱，最终变得过时，长期可持续性面临重大挑战。

Method: 借鉴演化架构中的适应函数理念，提出为研究软件量身定制的自动化评估指标体系，涵盖可发现性、可获取性、互操作性和可重用性（FAIR），并将这些指标嵌入开发生命周期。

Result: 案例研究和实验结果显示，本文方法显著提升了研究软件的长期FAIR属性，推动了其可持续发展。

Conclusion: 通过适应函数自动化管理，促进了研究软件的模块化、文档完善、版本控制和技术兼容，帮助建立可持续的研发文化，实现了科学影响的长期延续。

Abstract: The long-term sustainability of research software is a critical challenge, as
it usually suffers from poor maintainability, lack of adaptability, and
eventual obsolescence. This paper proposes a novel approach to addressing this
issue by leveraging the concept of fitness functions from evolutionary
architecture. Fitness functions are automated, continuously evaluated metrics
designed to ensure that software systems meet desired non-functional,
architectural qualities over time. We define a set of fitness functions
tailored to the unique requirements of research software, focusing on
findability, accessibility, interoperability and reusability (FAIR). These
fitness functions act as proactive safeguards, promoting practices such as
modular design, comprehensive documentation, version control, and compatibility
with evolving technological ecosystems. By integrating these metrics into the
development life cycle, we aim to foster a culture of sustainability within the
research community. Case studies and experimental results demonstrate the
potential of this approach to enhance the long-term FAIR of research software,
bridging the gap between ephemeral project-based development and enduring
scientific impact.

</details>


### [9] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: 本文实证评估了LLM生成的Python代码在能效方面的表现。结果发现，LLM代码虽然有一定能效，但整体仍低于人类开发者，尤其是绿色软件专家。提示工程对能效提升效果不明显，因此在追求绿色计算时，人类专业经验仍不可替代。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的兴起使其被广泛应用于软件开发流程。随着节能和能效越来越受到关注，研究者希望了解LLM生成的Python代码在能效方面与人类开发者及绿色软件专家编写的代码相比，表现如何。

Method: 作者选择了EvoEval基准中的9个编程问题，使用6种主流LLM和4种提示工程技术，生成363份代码解答，并与人类开发者及绿色软件专家的解决方案对比。在服务器、个人计算机和树莓派三种不同硬件上，累计测试881小时，测量各自的能耗表现。

Result: 总体上，人类开发者编写的代码在服务器和树莓派上分别比LLM生成的代码能效高16%和3%；但在个人电脑上，LLM的代码能效比人类开发者高25%。不同提示工程对能耗没有一致的改善作用，在不同硬件上的最优提示也不相同。绿色软件专家编写的代码在所有设备上都比LLM生成的代码能效高17%至30%。

Conclusion: 尽管LLM展现出了较强的代码生成能力，但其代码能效尚未超过有经验的绿色软件开发者。目前，开发高能效的Python代码仍需依靠人类专家的经验。

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


### [10] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: 本文提出一套机器学习驱动的工业测试选择方法T-TS，通过创新的数据表示与特征，无需覆盖图即可在实际生产环境显著提升测试效率和故障检测率。该方法平均仅选择15%测试，检测率超95%，实现已公开，具备较高行业价值。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发规模和测试套件的扩大，日常频繁代码提交使得高效管理测试过程变得十分困难，尤其在工业环境下更为突出。提升测试选择效率并保证故障检测率成为亟需解决的问题。

Method: 提出了T-TS（Targeted Test Selection）方法，通过机器学习对每次提交进行测试选择。创新地将提交表示为变更文件的词袋（Bags-of-Words），并结合跨文件特征及其他预测特征，显著地规避了传统覆盖图的使用。

Result: T-TS在工业生产环境中部署并通过内部和公开数据集进行评估。结果显示，T-TS平均只选择15%的测试用例，测试执行时间减少5.9倍，流水线加速5.6倍，能检测超过95%的测试失败。

Conclusion: T-TS方法在提升测试效率的同时保障了高故障检测率，具备显著的工业实际应用效果，并公开其实现以促进后续研究与应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [11] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: 基于真实对话大数据，分析LLM代码助手在多语言下频发的代码质量问题，并验证通过有效提示纠错可逐步提升代码输出质量。


<details>
  <summary>Details</summary>
Motivation: 尽管开发者在编程中广泛使用大型语言模型（LLM），但我们对开发者与LLM实际交互的方式、这种对话动态如何影响任务结果、代码质量和软件工程流程的理解还很有限。

Method: 基于CodeChat数据集，分析82,845个开发者与LLM的真实对话，涵盖368,506段代码片段，涉及20多种编程语言，对对话长度、代码质量、问题类型及多轮对话下错误修复的情况进行统计与分析。

Result: LLM响应远长于开发者输入，多轮对话占比高（68%），话题多集中于网页设计和神经网络训练。不同语言生成代码存在各自高发的问题，如Python/Javascript常有未定义变量，Java代码缺少注释，C++遗漏头文件，C#命名空间未解析。多轮对话中某些错误持续，但部分质量指标（如Java文档、Python导入）会逐步提升。指出并要求修复错误的提示最有效。

Conclusion: 开发者与LLM的多轮交互既带来较高的出错率（且存有语言特异性），也为代码质量优化提供了动力。有效的错误指令能促进快速修复，但持续关注错误类型和优化对话提示是提升开发工作流的关键。

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [A Note on Constructive Canonical Splitter Strategies in Nowhere Dense Graph Classes](https://arxiv.org/abs/2509.10062)
*Janne Fuchser,Nikolas Mählmann,Sebastian Siebertz*

Main category: cs.LO

TL;DR: 本文分析了Splitter游戏在刻画稀疏图类中的作用，提出并构造性证明了Splitter在有限回合内获胜的行动步骤数量的具体上界，推动了该领域的精细理论发展。


<details>
  <summary>Details</summary>
Motivation: Splitter游戏用于图论中刻画稀疏图类（nowhere dense classes），前人已证明进取子（Splitter）的胜利轮数与图的稀疏性有关。近期虽证明了进取子的有效行动步数有限，但该界有限制，没有明确构造上界。本文旨在提出构造性的上界证明。

Method: 作者通过对Splitter胜利策略进行分析，构造性地推导Splitter在半径r splitter游戏中k步获胜时，最多有多少步进取行动。他没有依赖于紧致性定理，而是直接构建边界。

Result: 证明了如果Splitter能在半径r的游戏中k步获胜，则进取行动（使得胜利回合减少一轮的行动）最多不超过$(2r+1)^{2^{k-1}-1}$步。

Conclusion: 本文首次给出了Splitter进取行动数的显式可计算上界，填补了先前仅存在不可构造性证明的空白，对相关图类研究和方法设计具有理论价值。

Abstract: The radius-$r$ splitter game is played on a graph $G$ between two players:
Splitter and Connector. In each round, Connector selects a vertex $v$, and the
current game arena is restricted to the radius-$r$ neighborhood of $v$. Then
Splitter removes a vertex from this restricted subgraph. The game ends, and
Splitter wins, when the arena becomes empty. Splitter aims to end the game as
quickly as possible, while Connector tries to prolong it for as long as
possible. The splitter game was introduced by Grohe, Kreutzer and Siebertz to
characterize nowhere dense graph classes. They showed that a class
$\mathscr{C}$ of graphs is nowhere dense if and only if for every radius $r$
there exists a number $\ell$ such that Splitter has a strategy on every $G\in
\mathscr{C}$ to win the radius-$r$ splitter game in at most $\ell$ rounds. It
was recently proved by Ohlmann et al. that there are only a bounded number of
possible Splitter moves that are progressing, that is, moves that lead to an
arena where Splitter can win in one less round. The proof of Ohlmann et al. is
based on the compactness theorem and does not give a constructive bound on the
number of progressing moves. In this work, we give a simple constructive proof,
showing that if Splitter can force a win in the radius-$r$ game in $k$ rounds,
then there are at most $(2r+1)^{\,2^{k-1}-1}$ progressing moves.

</details>


### [13] [On Syntactical Simplification of Temporal Operators in Negation-free MTL](https://arxiv.org/abs/2509.10146)
*Mathijs van Noort,Femke Ongenae,Pieter Bonte*

Main category: cs.LO

TL;DR: 本文揭示无需否定即可表达关键时序约束，精简了时序逻辑工具箱，有助于面向开放复杂环境的推理系统设计与实现。


<details>
  <summary>Details</summary>
Motivation: 在动态且数据密集的环境下进行时序推理通常需兼顾表达能力与可行性。传统上依赖否定（如失败否定）来表达“缺失”或“矛盾”信息，但在物联网、语义网等开放分布式系统中数据不完整，否定操作带来问题。因此，研究无否定的时序逻辑以保证推理可扩展性和单调性变得重要。

Method: 作者对无否定的时序逻辑（negation-free MTL）进行了表达能力分析，着重研究了常用的“总是(always)”操作符，并提出其可通过“曾经(once)”、“自(since)”和“直到(until)”等操作符组合实现，甚至可以仅用“until”和“since”操作符表达所有模式。

Result: 证明了无否定时序逻辑片段（仅含until和since）有足够表达能力，不依赖否定亦可刻画普遍性时序约束和存在性时序模式。这简化了MTL的语法，为后续理论和实现带来潜在好处。

Conclusion: 否定并非表达强时序模式或约束的必要条件，仅凭until和since操作符即可支撑基本时序推理需求，为构建可扩展可靠推理系统提供理论基础。

Abstract: Temporal reasoning in dynamic, data-intensive environments increasingly
demands expressive yet tractable logical frameworks. Traditional approaches
often rely on negation to express absence or contradiction. In such contexts,
Negation-as-Failure is commonly used to infer negative information from the
lack of positive evidence. However, open and distributed systems such as IoT
networks or the Semantic Web Negation-as-Failure semantics become unreliable
due to incomplete and asynchronous data. This has led to a growing interest in
negation-free fragments of temporal rule-based systems, which preserve
monotonicity and enable scalable reasoning.
  This paper investigates the expressive power of negation-free MTL, a temporal
logic framework designed for rule-based reasoning over time. We show that the
"always" operators of MTL, often treated as syntactic sugar for combinations of
other temporal constructs, can be eliminated using "once", "since" and "until"
operators. Remarkably, even the "once" operators can be removed, yielding a
fragment based solely on "until" and "since". These results challenge the
assumption that negation is necessary for expressing universal temporal
constraints, and reveal a robust fragment capable of capturing both existential
and invariant temporal patterns. Furthermore, the results induce a reduction in
the syntax of MTL, which in turn can provide benefits for both theoretical
study as well as implementation efforts.

</details>


### [14] [Initial Algebras of Domains via Quotient Inductive-Inductive Types](https://arxiv.org/abs/2509.10187)
*Simcha van Collem,Niels van der Weide,Herman Geuvers*

Main category: cs.LO

TL;DR: 本文在领域理论框架下提出了一种统一建模代数效应的方法，利用DCPO代数和QIITs实现理论上的推广和具体实例验证，并通过Cubical Agda进行了形式化证明，为语义理论和代数效应领域提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在为程序语言和计算模型中的各种代数效应（如非确定性、部分函数、副作用等）提供统一建模框架，弥补现有领域理论在处理复杂代数结构时的不足。

Method: 作者提出在领域理论中，用定向完备偏序（DCPO）及其代数结构描述代数效应，利用签名定义操作及其满足的不等式理论，并通过等价归纳-归纳类型（QIITs）构建初始DCPO代数，用以同时定义归纳类型、关系和方程，理论借助同伦类型论并在Cubical Agda中形式化。

Result: 证明了初始DCPO代数的存在性，并通过具体模型（如并和、smash积、自由DCPO、partiality和power domains）展示该一般框架的广泛适用性。

Conclusion: 该研究为领域理论中代数效应的建模提供了统一、形式化和可扩展的新方法，丰富了程序语言语义理论，并加强了同伦类型论与实际计算模型的联系。

Abstract: Domain theory has been developed as a mathematical theory of computation and
to give a denotational semantics to programming languages. It helps us to fix
the meaning of language concepts, to understand how programs behave and to
reason about programs. At the same time it serves as a great theory to model
various algebraic effects such as non-determinism, partial functions, side
effects and numerous other forms of computation.
  In the present paper, we present a general framework to construct algebraic
effects in domain theory, where our domains are DCPOs: directed complete
partial orders. We first describe so called DCPO algebras for a signature,
where the signature specifies the operations on the DCPO and the inequational
theory they obey. This provides a method to represent various algebraic
effects, like partiality. We then show that initial DCPO algebras exist by
defining them as so called Quotient Inductive-Inductive Types (QIITs), known
from homotopy type theory. A quotient inductive-inductive type allows one to
simultaneously define an inductive type and an inductive relation on that type,
together with equations on the type. We illustrate our approach by showing that
several well-known constructions of DCPOs fit our framework: coalesced sums,
smash products and free DCPOs (partiality and power domains). Our work makes
use of various features of homotopy type theory and is formalized in Cubical
Agda.

</details>


### [15] [Effects of the Strict-Tolerant Approach on Intuitionistic and Minimal Logic](https://arxiv.org/abs/2509.10322)
*Victor Barroso-Nascimento,German Mejia*

Main category: cs.LO

TL;DR: 本文将严格-宽容逻辑方法应用于直觉主义和极小逻辑，发现其在推理层面和元推理层面表现出不同的性质，丰富了对严格-宽容方法作用范围的理解。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，严格-宽容逻辑方法在某些情况下会导致非经典逻辑的推理规则坍缩为经典逻辑，但尚不清楚这一现象的普遍性和在不同逻辑体系中的表现。本文旨在进一步刻画其影响范围。

Method: 将严格-宽容逻辑方法推广应用于直觉主义和极小逻辑，分析了在推理和元推理层面的结果，并比较与经典逻辑的关系。

Result: 证明了直觉主义下的严格-宽容推理会坍缩为经典推理，而极小逻辑则不会；极小严格-宽容逻辑下没有有效推理。并且在元推理层面上，三种逻辑体系得到的逻辑彼此不同。

Conclusion: 直觉主义严格-宽容推理会坍缩为经典推理，但极小逻辑（minimal logic）则不会。同时，极小严格-宽容逻辑没有有效推理（但此特性不适用于元推理层面），而在元推理层面上，由直觉主义、极小和经典逻辑得到的逻辑各不相同。

Abstract: This paper extends the literature on the strict-tolerant logical approach by
applying its methods to intuitionistic and minimal logic. In short, the
strict-tolerant approach modifies the usual notion of logical consequence by
stipulating that, in order for an inference to be valid, from the truth of the
premises must follow the non-falsity of the conclusion. This notion can also be
generalized to define strict-tolerant metainferences, metametainferences and so
on, which may or may not generate logics distinct from those obtained on the
inferential level. It is already known that strict-tolerant definitions can
make the notion of inference for non-classical logics collapse into the
classical notion, but the strength of this effect is not yet fully known. This
paper shows that intuitionistic strict-tolerant inferences also collapse into
classical ones, but minimal ones do not. However, minimal strict-tolerant logic
has the property that no inferences are valid (which is not carried over to the
metainferential level). Additionally, it is shown that the logics obtained from
intuitionistic, minimal and classical logic at the metainferential level are
distinct from each other.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文通过文档级知识图谱结构化临床文档，并整合到PLM-ICD自动编码框架，实现ICD-9编码准确性与效率的提升。知识图谱使得仅用原文少量信息即可高效表达病患情况，在主流测试集上Macro-F1提升3.2%，并优化可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床文档标准化编码能够支持信息检索和分析，对于临床研究、医院管理和提升病患护理都至关重要。人工编码耗时费力、难以大规模应用，因此自动编码成为亟需解决的问题。

Method: 本文提出利用文档级知识图谱（KG）对输入文档进行结构化表示，仅用原文23%的文本表达出90%的信息，通过将此知识图谱整合进最先进的ICD自动编码架构PLM-ICD，提升编码表现。

Result: 引入知识图谱后，自动ICD-9编码在主流基准上Macro-F1最高提升3.2%，且训练效率提高。知识图谱中的不同实体与关系改善了模型的泛化能力和解释性。

Conclusion: 利用知识图谱结构化输入文档并结合现有模型，能够有效提升临床文档标准化编码的准确性和效率，同时增强模型的可解释性。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [17] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 本文提出的CLAP技术提升了LLM幻觉检测效果，实现了更可靠的幻觉缓解策略，总体提高了模型在多任务与分布外场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各类应用中的广泛应用，其生成不准确文本（即幻觉）的倾向引发了可靠性担忧。

Method: 提出了一种新的激活探测技术——跨层注意力探测（CLAP），通过联合处理LLM整个残差流的激活序列，实现对幻觉的检测。

Result: 在五种LLM和三项任务上的实证评估显示，CLAP在贪婪解码和高温采样响应中均优于现有基线方法，能够更细粒度地区分同一提示下不同响应中的幻觉与非幻觉。

Conclusion: CLAP能够实现检测—再缓解的策略，有效减少幻觉并提升LLM可靠性，且在分布外应用时也能保持高可靠性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [18] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文从正则化角度优化语音到文本端到端翻译多任务学习，通过跨模态和同模态正则化及MT损失调节，提出正则化地平线，在MuST-C数据集上获得了接近最优的结果。


<details>
  <summary>Details</summary>
Motivation: 语音到文本的端到端翻译因配对语料稀缺受限，研究利用机器翻译中的双语数据通过多任务学习提升任务表现。

Method: 从正则化视角分析多任务学习，采用一致性正则化（跨模态）、R-drop（同模态）和MT损失系数作为三种正则化来源，提出“正则化地平线”以优化超参数。

Result: 超参数在正则化“地平线”内调优后，在MuST-C数据集上实现了近乎最优的翻译性能。

Conclusion: 通过调整多任务学习中的不同正则化因素，可以获得接近最先进的语音到文本翻译表现。

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [19] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 本文提出创意基准评估LLMs在营销领域创造力。678位创意专家对模型输出进行大规模比较，发现模型间表现差异小，顶与底模型胜率仅61%。自动化评判相关性弱，易有偏见，传统测试难完全适用，突出专家人工评价及多样性工作流的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于大语言模型（LLMs）在市场营销领域创造力评估的标准化基准。虽然LLMs在很多任务上表现优秀，但其在创造性任务中的真实能力及自动评估方法需求进一步研究。

Method: 提出了Creativity Benchmark基准，涵盖100个品牌（12类）和三类创意提示（洞察、创意、异想天开）。收集678位专业创意人员对11,012对模型输出进行匿名两两偏好比较，用Bradley-Terry模型分析结果，并检查模型间表现的多样性（余弦距离、提示重构敏感性），比较自动化LLM判别与人工评分相关性和偏差，评估传统创意测试在品牌受限任务中的适用性。

Result: 不同LLMs在品牌和提示类型下表现集群，分布紧密，最佳与最差模型的胜率只约61%。模型输出多样性通过余弦距离有所揭示，对提示方式有敏感性。自动化LLM评审与人工判断相关性弱且受特定偏差影响，传统创造力测试方法不能完全迁移至品牌创意任务。

Conclusion: LLMs在品牌相关创意任务中的优势有限，评估需依赖专家人工评价及关注输出多样性，自动化判别目前无法替代人工。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [20] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出了一种新型的上下文相关规则驱动模型指纹方案CTCC，可在提升隐蔽性和鲁棒性的同时实现黑盒下的所有权验证，有望成为现实应用中的有效工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛部署，模型被盗和未经授权的分发变得更容易，知识产权保护的需求愈发紧迫。如何在模型中嵌入能验证所有权的“指纹”，成为重要研究问题。

Method: 本文提出了一种新颖的基于规则的指纹框架——CTCC，通过跨多轮对话的上下文关联（如反事实推理）进行编码，而非传统的单个token或回合级触发。这样既能黑盒验证指纹，又降低假阳性概率和“指纹”泄漏风险，且在部分指纹暴露情况下仍可持续构造。

Result: 多种LLM架构上的大量实验证明，CTCC相较于以往方法，在隐蔽性和鲁棒性方面均有更优表现。

Conclusion: CTCC方法为实际LLM部署中的所有权验证提供了可靠且实用的解决方案。相关代码与数据已开源。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [21] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 本论文分析语言模型在跨时期选择中的时间倾向，并提出了Manipulability of Time Orientation（MTO）度量指标，发现部分模型对未来或现在偏好可被提示系统性改变，为AI助手设计与个性化校准提出了建议。


<details>
  <summary>Details</summary>
Motivation: 人类在跨时期决策（即时与延迟利益取舍）中的时间倾向已广泛研究，但语言模型作为AI助手，是否也有类似未来或现在偏向，其偏好能否被有系统地操控，尚未被充分探讨。研究旨在填补这一领域空白，并为AI伦理与设计提供参考。

Method: 采用人类实验协议改编后的时间权衡任务，评估多种语言模型在未来与现在提示下的选择行为，并与人类样本对比。通过引入MTO指标，量化模型在未来导向与现在导向提示下的时间偏好变化，检验模型个性化决策能力及地理身份适应性。

Result: 推理能力较强的模型（如DeepSeek-Reasoner与grok-3-mini）在未来导向提示下更倾向选择推迟方案，但其在不同身份或地域上的个性化决策表现有限。能正确理解时间导向的模型亦会倾向于认为AI本身宜采未来导向，表现出内化的时间偏好。

Conclusion: 语言模型在时间权衡任务中表现出可操控的时间倾向，部分模型受提示影响呈现未来偏好，但个性化和地域调优能力仍有限。研究结果对构建符合多元、长期目标的AI助手具有重要启示，未来需关注个性化校准和社会性部署。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [22] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 本文系统分析了小型与中型开源LLM在多次重复提问时的答案一致性。发现小模型一致性通常在50%-80%之间，中型模型明显更优，一致答案的准确率与总体准确率相关。相关分析有助于模型选型和实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型（2B-8B参数）在实际应用中被广泛使用，但它们对同一问题多次回答时的一致性还不清楚。因此，研究它们在全部重复提问时表现出的答案一致性具有重要价值，有助于选择更合适的模型及调整推理参数。

Method: 本文对开源小型语言模型，在不同推理温度、模型规模（小型与中型）、是否微调等条件下，针对MMLU-Redux和MedQA多选题基准，进行了10次重复提问的一致性分析。同时提出了新的分析和可视化工具，具体对比分析了答案一致性和准确率的关系及权衡。

Result: 研究发现，小型模型在低推理温度下，一致性回答问题的比例通常在50%-80%之间，并且答案一致性的准确率与整体准确率呈现较好相关。相比之下，中型模型的一致性水平显著更高。

Conclusion: 不同模型在多次回答同一问题时表现出明显不同的一致性，推理参数与模型大小对一致性与准确性存在权衡。多轮一致性分析有助于评估模型表现，并为实际应用中的模型选择提供参考。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [23] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 本文通过在大语言模型中利用稀疏自动编码器和因子分解机，精细识别和操控促成有害提示拒绝行为的关键特征，揭示了其中的非线性交互和冗余性，为模型安全提升提供了机制性新见解。


<details>
  <summary>Details</summary>
Motivation: 虽然指令微调的大语言模型在检测有害提示时具备安全的“拒绝”行为，但目前对其内部机理知之甚少；本研究旨在揭示模型内部控制拒绝与顺从行为的机制，从而提升安全性审计与干预能力。

Method: 使用稀疏自动编码器（SAE）对模型残差流激活进行训练，然后在SAE潜空间中搜索和剪枝与拒绝相关的特征，并用因子分解机（FM）发现特征间的非线性交互，逐步锁定导致模型拒绝或顺从的关键特征集。

Result: 成功找到了多个影响拒绝行为的关键特征，并发现了在前置特征被抑制时才会激活的冗余特征，这为模型安全性行为的精细审计和有针对性的干预提供了新思路。

Conclusion: 该研究表明，通过在大语言模型的可解释潜空间中干预，可识别和操控驱动模型拒绝有害提示的关键特征，实现了模型从拒绝到顺从的翻转，并揭示了特征间的冗余和非线性交互机制。

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [24] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 文章针对大语言模型在学术写作中的内容质量与引用虚假问题，提出量化评估指标与迭代优化方法，经实验证明可显著提升写作质量并减少引用造假。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（如ChatGPT）在学术写作中应用广泛，但存在引用错误或虚假等伦理问题。此外，内容质量评估依赖主观人工判断，缺乏客观性和一致性。

Method: 提出两项评价指标：内容质量和引用真实性，并基于这两个指标的评分设计迭代提示方法，从而量化评估模型能力并提升写作表现。

Result: 所提出的指标能够客观、量化评估ChatGPT学术写作表现；迭代提示方法显著提升内容质量，并减少引用错误与虚假，缓解了学术伦理困境。

Conclusion: 建立了内容质量与引用真实性的量化评估框架，并验证了通过迭代提示能提高大语言模型学术写作质量和伦理合规性。

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [25] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出用大型语言模型结合开放数据自动生成个人出行日记，不仅真实性接近真实数据，还优于经典方法在部分指标表现，展示了LLM在交通微观建模的新潜力。


<details>
  <summary>Details</summary>
Motivation: 当前基于代理人的交通模型依赖大量专有的家庭出行调查数据，获取难度大、开放性差，因此亟需利用开放数据和新方法自动生成个体出行日记，以降低成本并提升可扩展性。

Method: 该研究提出使用大型语言模型（LLM）方案，通过开放的美国社区调查（ACS）和智能位置数据库（SLD）数据，随机生成用户画像，并直接用提示生成个体出行日记。日记真实性通过四项指标（出行次数、出行时间间隔、出行目的、出行方式）构建创新的“one-to-cohort realism score”，并用Jensen-Shannon Divergence衡量生成日记与真实数据（CTST调查）的分布相似性，并与经典统计模型进行对比。

Result: LLM生成的出行日记在整体真实性上与经典方法相当（均值0.485 vs. 0.455），在出行目的和一致性上表现更佳（分布更窄），但经典模型在出行次数和活动时长估算上更准。聚合验证显示LLM的统计代表性更强（均值0.612 vs. 0.435），展示了LLM在零样本生成场景的可行性和未来出行日记真实性评测的量化方法。

Conclusion: 利用LLM可在无需专有调查数据的情况下自动合成高真实性的个人出行日记，并以创新的多维度分数体系量化其真实性，对交通模拟和分析具有重要意义。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [26] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: 作者构建了精神病学领域权威评测基准PsychiatryBench，评测大语言模型时发现其在临床一致性和安全性方面尚有明显不足，为模型专门训练和改进评测体系奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）在精神科临床应用中有巨大潜力，但现有评测资源多依赖于小型访谈语料、社交媒体等材料，缺乏专业权威性，无法全面反映精神病学复杂推理过程，因此亟需更权威、更专业的评测基准。

Method: 作者提出并构建了一个全新基准PsychiatryBench，基于权威精神科教材和案例集，涵盖11种不同的问答任务（如诊断推理、治疗方案、跟踪管理等），共包含5300多个专家注释条目。对多种前沿大模型和医学模型进行了评测，采用传统指标和“LLM-as-judge”相似度评分框架。

Result: 评测结果显示，现有模型在临床一致性和安全性上存在明显不足，尤其在多轮跟进和管理任务中表现不佳，反映出当前模型需要专门化调优和更严密的评估体系。

Conclusion: PsychiatryBench提供了一个模块化、可扩展的平台，用于高风险精神健康领域内LLM性能的权威评测和提升。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [27] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 本研究表明通过偏好对齐的策略优化方法（ORPO）可以显著提升小型开放权重大语言模型在ACT心理治疗方面的忠实度和同理心，并指出链式思考推理对模仿型训练模型意义重大，但对策略优化模型无明显帮助。


<details>
  <summary>Details</summary>
Motivation: 探索后训练方法（监督微调SFT与偏好对齐策略优化ORPO）以及显式推理步骤（chain-of-thought, COT）对小型开放权重大语言模型（LLM）执行接受与承诺疗法（ACT）能力的影响。

Method: 使用Mistral-Large生成的50组合成ACT会话记录，对Llama-3.2-3b-Instruct进行SFT和ORPO两种后训练方式训练，并各自配合或不配合COT推理步骤。采用ACT-FM和TES量表，通过经过微调的LLM评判模型对模拟治疗场景下四种变体模型及基础模型进行量化评估。

Result: ORPO模型在ACT忠实性和治疗师同理心评分上显著优于SFT及基础模型。COT对SFT模型有显著增益（ACT-FM提升2.68分），但对性能更优的ORPO及Instruct变体无明显效果。

Conclusion: 以偏好为导向的策略优化（ORPO）能更好培养小型LLM的ACT治疗能力，COT推理的价值依赖于具体训练范式，对仿内容的模型尤为关键。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [28] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 针对RAG在多跳查询中的噪声与效率问题，作者提出HANRAG框架，通过启发式查询分解与噪声过滤，显著提升了问答准确率和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG技术在应对多跳查询问题时存在显著挑战，如过度依赖迭代检索导致检索步骤浪费，以及复杂查询用于检索时不能有效获取与子查询相关内容，容易产生检索噪声并引发噪声积累。

Method: 提出了HANRAG，一种基于启发式方法的检索增强生成框架。HANRAG由强大的revelator驱动，能够合理路由查询、将复杂查询分解为子查询，并对检索到的文档进行噪声过滤。

Result: 在多项基准测试上，HANRAG在单跳和多跳问答任务中均优于主流工业方法，展现出更强的适应性与抗噪能力。

Conclusion: HANRAG框架有效解决了现有RAG方法在多跳查询中的噪声积累及检索效率问题，显著提升了多类查询的应答能力。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [29] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 本研究系统比较了18种文本和代码语义相似性度量方法，发现传统嵌入方法易误判反义关系，LLM方法区分语义更准确，建议优化度量技术以更好支持软件工程应用。


<details>
  <summary>Details</summary>
Motivation: 语义相似性度量对于软件工程中的代码搜索、API推荐、自动代码审查和重构工具等应用非常重要。然而，随着大语言模型的广泛应用，人们质疑它们是否真的理解语义关系还是仅仅识别表面模式。

Method: 研究测试了18种不同的语义相似性测量方法，包括基于词的方法、嵌入技术、基于LLM的系统和结构感知算法，并通过系统化测试框架对文本和代码进行了受控变更，以评估这些方法在处理不同语义关系上的表现。

Result: 常用度量方法存在严重问题：部分嵌入方法会错误地将语义相反的内容判定为高度相似（误差高达99.9%），有的Transformer模型甚至将反义关系判定为比同义关系更相似。嵌入方法性能差主要是距离计算方式导致，更换为余弦相似度，结果提升24~66%。LLM方法在辨别语义差异方面更好，对真正不同含义赋予较低分（0.00~0.29），而嵌入方法赋予不相似内容较高分（0.82~0.99）。

Conclusion: 主流语义相似性度量工具在判定语义关系上存在可靠性隐患，嵌入方法需调整距离函数，LLM方法能更精准区分语义差异。未来评估与改进应关注方法的语义理解能力而不仅是模式识别。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [30] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 论文系统分析了导致大语言模型幻觉的关键属性，发现符号性元素（修饰词、命名实体）极易引发幻觉现象，并且此弱点随模型规模增加仍然存在，提示该方向是未来提升LLMs可靠性的重点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）被广泛关注其幻觉（Hallucination）问题，但导致LLMs本质上易受幻觉影响的内部属性尚未被详细识别和研究。该研究希望通过系统性分析，发现并归纳导致幻觉的关键属性，从而更精准地定位模型的脆弱性。

Method: 研究采用了HaluEval和TruthfulQA两个公开数据集，将原有问答格式转化为多种新格式，从不同角度考察模型产生幻觉的属性。同时，比较了不同规模Gemma模型在各属性上的表现。

Result: 在符号属性上，Gemma-2-2B模型的幻觉率高达79.0%，Gemma-2-9B为73.6%，Gemma-2-27B为63.9%，规模提升会降低幻觉率，但并不能完全消除属性相关的幻觉。对于修饰词和命名实体，所有模型在两个数据集上的幻觉率仍然极高（修饰词84.76%~94.98%，命名实体83.87%~93.96%）。

Conclusion: 符号元素（如修饰词和命名实体）持续让LLMs产生幻觉，即使模型规模提升也无法根除，反映出当前模型在处理符号性输入时存在固有弱点。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [31] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: 本文提出了ALIGNS，一个基于大模型的测量验证工具，生成大规模名义网络，有效拓展了心理测量的理论验证手段，实验验证其准确性和实用价值。


<details>
  <summary>Details</summary>
Motivation: 心理测量在许多学科中至关重要，但建立理论性的名义网络以验证量表有效性依然是个挑战。这一缺陷会导致临床试验无法检测治疗效果，公共政策目标错误等实际后果。

Method: 提出了基于大型语言模型的ALIGNS系统，使用已验证的问卷测量训练模型。系统提供了包含心理学、医学、社会政策等领域的55万多个指标的三个全面名义网络，并通过分类准确性测试及三个评估方法进行验证。

Result: 第一项评估显示NIH PROMIS焦虑与抑郁量表可归为单一的情绪困扰维度；第二项评估发现儿童气质测量有四个现有框架未覆盖的新维度，并质疑一个维度的合理性；第三项评估中专家心理测量师肯定了系统的重要性、可访问性和适用性。

Conclusion: ALIGNS为解决测量验证的基础问题提供了新方法，通过大规模名义网络分析补充传统验证手段，且系统对各领域开放免费使用。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [32] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 本文提出了利用时间关系和大语言模型分析专利数据、发现新兴科技机会的方法，并在人工智能领域专利验证了其实用性和前景。


<details>
  <summary>Details</summary>
Motivation: 科技机会对于推动技术、产业和创新进步至关重要，但如何高效地识别新兴科技机会仍面临挑战。

Method: 提出了一个基于技术之间时间关系的框架，利用专利数据文本分析，通过主题挖掘发现技术关系，并追踪主题随时间变化来识别新兴技术机会。同时，框架使用大语言模型提取主题，并通过对话式语言模型辅助科技机会发现。

Result: 在美国专利局提供的人工智能专利数据集上进行了评估，实验表明人工智能技术正发展为更易于日常使用的形式。

Conclusion: 该框架能够有效识别未来科技机会，为技术发展和创新方向提供指导。

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [33] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出一种专注于多语言嵌套生物医学实体链接的轻量级pipeline，通过检索-排序解耦、实体边界标签和自动数据增强三大技术创新，在BioNNE 2025赛道上排名第三。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学文本的实体链接（EL）研究主要集中在英文和非嵌套实体提及，现实中存在多语言和嵌套实体提及的复杂需求却缺乏有效探索。因此，团队致力于提升多语言嵌套实体的链接能力，填补该领域空白。

Method: 提出一种轻量级pipeline，保持原有EL模型不变，仅对三方面进行针对性改进：1）两阶段检索-排序，在排序阶段引入领域特定微调，2）在排序时用可学习的特定标记[Ms]/[Me]包裹mention以增强实体边界识别能力，3）通过自动数据增强扩展训练集，无需额外人工标注。

Result: 在BioNNE 2025多语言赛道排行榜上，该两阶段系统BIBERT-Pipe取得了第三名的成绩，验证了上述方法在多语言嵌套实体链接上的有效性与竞争力。

Conclusion: 通过三项有针对性的最小修改，BIBERT-Pipe在多语言、生物医学嵌套实体链接任务中取得优异表现，证明该轻量级pipeline具有实际推广应用价值。

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [34] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 提出了一种结合大语言模型口语化和摘要能力，将形式化证明自动翻译为高质量、易读自然语言证明的方法，并通过教材案例和Lean库验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 将机器可验证的形式化证明翻译为自然语言证明，使其更易于理解和传播，并验证大语言模型（LLM）在这一任务中的能力。

Method: 利用大语言模型对形式化证明步骤进行口语化处理和摘要化生成，实现形式化语言到自然语言的自动转换。通过对照本科教材中的自然语言证明以及现有Lean证明库，分析生成结果的质量。

Result: 该方法能够生成可读性高且内容准确的自然语言证明，实验表明生成的自然语言证明质量较高，并能够与原始自然语言证明进行有效对比。

Conclusion: 大语言模型具备将形式化证明自动转化为高质量自然语言的能力，为形式化证明更广泛的应用和理解提供了桥梁。

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [35] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 本文提出了结合多智能体和检索增强的金融问答系统，利用角色分工和专家点评机制，提高了答案准确率，并实现了主流大模型的性能跃升，为金融领域智能问答提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在金融教育中的问答任务表现有限，无法充分捕捉金融领域所需的复杂推理、专业术语以及实际场景理解。金融问答需要多步骤的定量推理和领域知识，因此有必要提出新的方法提升其准确性。

Method: 提出了一种多智能体框架，包括基础生成器、证据检索器和专家审查员三种角色，采用单轮迭代的方式协同工作。结合基于检索增强生成（RAG）的方法，从6本金融教材中获取上下文证据，并利用领域专家审查的提示策略对答案进行完善。

Result: 在3,532道专家设计的金融教育题目上进行评估，批判式完善策略让答案准确率较零样本链式思维（CoT）基线提升了6.6-8.3%。Gemini-2.0-Flash模型表现最佳。同时，GPT-4o-mini的表现与金融领域调优的FinGPT-mt_Llama3-8B_LoRA持平。

Conclusion: 多智能体结构结合检索增强和专家审查机制能够以低成本显著提升金融问答的准确率，并为多智能体金融大模型系统的后续研究提供了新思路。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [36] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 本研究通过严格的元分析方法，发现推特情感分析中ML模型的平均准确率为0.80，但指出准确率指标常存在局限，呼吁性能报告标准化以实现更可靠的比较。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在系统评估机器学习在推特情感分析中的表现，并分析不同研究之间及内部的差异，以及研究特征对模型性能的影响。

Method: 采用PRISMA指南检索学术数据库，筛选出来自20项研究的195个实验，收集包含12个研究特征的数据。通过双弧正弦变换与三级随机效应模型，对主要性能指标——整体准确率进行元分析。

Result: 经过元分析，AIC优化模型的平均整体准确率为0.80 [0.76, 0.84]。

Conclusion: 1）整体准确率虽然广泛应用，但易受类别不均衡和情感类别数量影响，存在误导性，应进行归一化处理；2）标准化模型性能报告（包括独立测试集的混淆矩阵）对于机器学习分类器的横向比较至关重要，但目前尚未成为通行实践。

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [37] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 手语处理研究因缺乏合适工具难以复现和比较，本文提出MultimodalHugs框架，增强多模态数据与任务的统一处理能力，提升了实验的灵活性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 手语处理（SLP）在自然语言处理领域日益重要，但现有的研究受限于不规范的代码和实验流程，导致结果难以复现且难以公平比较。现有工具无法灵活支持多种手语实验需求，SLP研究者对此表达了需求。

Method: 提出了MultimodalHugs，这是一个基于Hugging Face的框架，增强了对多模态数据和任务的支持，并保留了Hugging Face生态的优点。该框架通过增加抽象层，支持更广泛的应用场景。

Result: 通过定量实验展示MultimodalHugs对不同数据模态（如手语的姿态估计数据和文字像素数据）的支持能力，验证了其实用性和通用性。

Conclusion: MultimodalHugs缓解了手语处理领域的复现难题，促进了多模态研究的规范和发展，对更广泛的多模态任务也具有适用性。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [38] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: 本文首次推出中文古代文献基准AncientDoc，涵盖OCR、翻译及深度问答等五大任务，并实测现有主流视觉语言模型，为古文数字化和智能理解提供重要评测平台，推动相关技术发展。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）难以处理古代中文文献的视觉与语言复杂性，而以往文献基准主要集中于英文印刷文本或简体中文，缺乏针对古中文文献的专用评测工具。古代文献在数字化和理解上面临两大挑战，急需有效解决。

Method: 本文提出AncientDoc，这是首个针对中文古代文献设计的评测基准。AncientDoc包含五大任务（页面层面OCR、白话翻译、推理型问答、知识型问答、语言变体问答），涵盖14类文献类型、100余部古籍、约3000页文献，并使用主流视觉语言模型进行评测，利用多种指标和与人对齐的大型语言模型评分。

Result: AncientDoc基准能够系统评估VLMs在古代中文文献处理中的五种核心能力，并提供详细性能对比。通过多种指标和人类对齐模型评分，揭示主流VLMs在各项任务中的表现与不足。

Conclusion: AncientDoc填补了古代中文文献数字化与理解领域的评测空白，为日后模型优化和相关研究提供了重要工具和参考。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [39] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 本文提出了专为MCP协议设计的智能体工具交互基准测试——MCP-AgentBench，结合综合测试平台、结构化任务与创新评测方法，系统分析了主流智能体真实任务表现，为智能体工具生态发展和评测提供了新标准。


<details>
  <summary>Details</summary>
Motivation: 由于MCP协议逐渐成为智能体工具整合和互操作的核心标准，但现有评测方法无法准确反映其在真实场景中的性能，评价结果失真、难以区分智能体能力，因此亟需开发适用MCP的新型基准测试工具。

Method: 建立MCP测试平台，涵盖33个服务器和188种工具；设计600个查询，分布于6类不同交互复杂度场景；提出MCP-Eval以结果为导向的评测方法；对主流语言智能体进行系统性实证评测。

Result: 开发出MCP-AgentBench，包括标准测试平台、多元评测任务和新颖评测方法，成功评估主流智能体在MCP环境下的性能，为社区提供了通用、可复现、面向实际的评测框架。

Conclusion: MCP-AgentBench为评估支持MCP协议的语言智能体工具交互能力提供了标准化、可靠的基准测试框架，并揭示了当前主流智能体在实际任务中的差异和能力水平，为MCP生态建设和研究提供了坚实基础。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [40] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 本文通过大规模实验（30多万条任务）在GPT-3.5和GPT-4o上检测了性别、年龄和背景偏见，发现决策任务偏见明显但提示词可减轻，尤其在新版GPT-4o上更有效。跨英语和荷兰语偏见模式接近，强调部署LLMs时需偏见检测和缓解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各领域快速应用，对社会不平等和信息偏见的担忧愈发重要。本文旨在系统分析LLMs在背景、性别和年龄等方面的偏见，以及这些偏见在决策和摘要任务中的影响。还关注了偏见随语言跨域传播，及利用提示词降低偏见的可能性。

Method: 研究采用Tamkin等人（2023）数据集，翻译为荷兰语并设计了大量独特决策与摘要任务提示（决策任务151,200条，摘要任务176,400条）。在GPT-3.5和GPT-4o上测试不同人口变量、指令、突出性和语言设置，评估模型的决策和摘要偏见及相应缓解方案效果。

Result: 分析发现，两种模型在决策任务中均表现出明显偏见，尤其偏向女性、年轻群体和某些背景（如非裔美国人）；摘要任务偏见较小，仅GPT-3.5在英语下有年龄相关差异。横跨英语与荷兰语的偏见模式大体相似，但在特定人口类别有区别。新提出的缓解指令未能完全消除偏见，但其中最有效的指令实现了27%的偏见差距平均减小。GPT-4o提示偏见整体较少，显示新型号更易通过提示缓解偏见。

Conclusion: 研究表明，LLMs在关键任务中存在不可忽视的偏见，且部分可跨语言传播。提示工程可部分缓解此类偏见，新模型对缓解措施更敏感。强调在实际应用前要谨慎检测偏见，并持续改进缓解方案，以确保AI负责任部署。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [41] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: 该文提出并实验证明了一种分层高效微调策略（HEFT），创新性地结合了权重和表征两种微调方法，能在大幅减少算力消耗的同时，显著提升大模型在高难推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专门推理任务上的适应受到算力资源的严重限制，目前流行的参数高效微调（PEFT）方法虽然有效，但方法众多且分布于权重空间和表征空间，如何进一步提升性能和效率仍是难题。

Method: 提出一种新颖的分层高效微调策略（HEFT），结合了两种不同的PEFT方法。先在权重空间用LoRA进行广泛基础适应，再用ReFT对内部表征做精细调整。最后在Llama-2-7B模型上通过BoolQ推理能力基准数据集进行测试。

Result: HEFT方法仅经过三轮微调即可获得85.17%的准确率，优于LoRA（20轮，85.05%）和ReFT（20轮，83.36%）单独应用的效果。

Conclusion: 合理组合PEFT方法可极大提升微调大语言模型的效率与推理能力，用更少的算力资源实现更优表现，为模型适应复杂认知任务提供了有效路线。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [42] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 本研究将语用框架中的手势标注引入真实场景多模态数据集，揭示交流者如何用手势组织对话轮次，并发现新的手势变化，有助于深入理解语言与认知的多模态关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏关于多模态面对面对话中交流者组织谈话轮次时的具体策略，特别是使用手势的系统性数据。现有数据集未包括可以用于机器学习的相关手势标注。

Method: 提出了通过分析语言与互动手势间关联，基于语用框架的概念化和唤起进行多模态会话轮次组织建模框架。开发了注释方法，将语义框架标注的多模态数据集（Frame2）扩展为包含语用框架和轮次组织相关手势的标注。Frame2数据来自巴西电视剧实际场景视频和文本。

Result: 通过分析扩展后的真实场景数据，证实面对面交流者广泛使用手势进行谈话的轮次传递、接收和保持，并发现一些未被文献记载过的新手势变体。

Conclusion: 轮次组织手势的使用源于语用框架的心智空间、融合和概念隐喻的认知过程，语用框架的标注能加深我们对人类认知和语言的理解。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [43] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出了结合主题标签和GRPO强化学习的新方法，显著提升了多文档摘要的质量，实验验证优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多文档摘要（MDS）面临如何有效整合多源信息并保持连贯性和主题相关性的关键挑战。大型语言模型在单文档摘要上表现优异，但在MDS上还有提升空间。

Method: 提出了一种基于主题引导的强化学习方法，通过在Group Relative Policy Optimization（GRPO）框架中设计主题奖励，衡量生成摘要与原文档之间的主题一致性，并结合显式的主题标签提示。

Result: 在Multi-News和Multi-XScience两个数据集上的实验结果表明，该方法在内容选择和摘要质量方面都优于强基线方法。

Conclusion: 利用主题线索（如主题标签和主题奖励）能显著提升多文档摘要的内容选择和专题对齐效果。

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [44] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型在模拟公众调查中的可靠性，显示在部分题项（如信任）上表现优异，但不同题项和人群的异质性明显。当前LLMs合成回应虽可近似真实调查结果，但实际应用仍需针对社会偏见及准确性进行深入校正。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）作为合成受访者有望在调查研究中创新方法与应用，可以模拟人类回答和行为，可能有助于减少测量与代表性误差。然而，目前尚不清楚LLMs在恢复聚合题项分布方面的能力，并且存在再现训练数据中的社会偏见和刻板印象的风险。

Method: 作者将LLMs生成的合成调查回应与智利公共意见概率调查中的真实人类回应进行对比。具体做法为：基准测试128个提示-模型-问题三元组，生成189,696个合成样本，并在128个问题-子样本对上进行元分析，汇总表现指标（准确率、精度、召回率、F1分数），分析关键社会人口学维度上的偏见。评估了OpenAI GPT系列、o-series推理模型、Llama和Qwen的多种版本。

Result: 1）在信任相关题项上，合成回应表现优异（F1和准确率>0.90）；2）GPT-4o、GPT-4o-mini与Llama 4 Maverick在该任务上的表现不相上下；3）合成—真实回答的一致性在人群中以45-59岁群体最高。整体而言，LLMs合成样本在总体采样上接近期望，但在题项层面存在显著异质性。

Conclusion: LLM驱动的合成样本能较好地模拟概率样本调查回应，但要充分捕捉公众意见的多样性，还需精细校准和额外分布测试，以确保模型的准确性并降低错误。

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [45] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文综述了法律领域的大语言模型发展，包括主要模型、框架、基准和数据集，并分析了挑战与未来趋势；为法律AI领域研究者和初学者提供了系统参考。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术在法律领域的应用日益广泛，推动该领域的系统化研究和应用发展既有理论意义也有实际价值。

Method: 对16个法律领域LLM系列和47个相关框架进行综述，收集了15个基准和29个数据集进行能力评估。

Result: 为初学者和研究者全面介绍了法律人工智能中的LLM应用现状，提供了丰富的资源和评估工具，并提出了面临的问题及未来方向。

Conclusion: 本文系统梳理了法律领域基于大语言模型（LLM）的最新研究进展，并指出目前的挑战及未来的发展方向。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [46] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 本论文构建了藏语、维吾尔语和蒙古语的大规模标题生成数据集（CMHG），并附有母语者标注的高质量测试集，为少数民族语言NLP研究提供了基础资源和基准。


<details>
  <summary>Details</summary>
Motivation: 中国的一些少数民族语言（如藏语、维吾尔语和传统蒙古语）由于拥有与国际标准不同的独特书写体系，导致相关语料库（特别是用于监督学习的任务如标题生成）极为稀缺。

Method: 作者新构建了一个中文少数民族标题生成数据集（CMHG），包含10万条藏语数据、各5万条维吾尔语和蒙古语数据，并配备由母语者标注的高质量测试集，旨在为该领域提供基准。

Result: 大规模少数民族语种标题生成数据集成功构建，并伴有高质量、可用于基准测试的测试集。

Conclusion: 本工作填补了中国少数民族语言标题生成领域的资源空白，为相关研究和基准制定提供了重要支撑。

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [47] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 本论文提出了一种创新的无监督幻觉检测方法IRIS，通过利用LLM的内在表示与不确定性，大幅提升了检测性能，为实时、低成本、高泛化性的幻觉内容检测提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 无监督幻觉检测旨在识别大型语言模型生成的虚假内容，但现有方法常依赖与事实无关的代理信号，导致检测能力受限且泛化性差。

Method: 提出了IRIS无监督幻觉检测框架，通过引导LLM仔细验证陈述的真实性，并获取其上下文嵌入作为训练特征，同时将模型对回答的不确定度作为真实性的软伪标签。

Result: 实验结果表明，IRIS在无监督幻觉检测任务上明显优于现有方法，并且完全无监督、计算成本低、少量训练数据下效果依然优异，适合实时检测。

Conclusion: IRIS框架有效提升了LLM幻觉内容的无监督检测准确性，具备高效、低成本和广泛适用性，有望推动实际应用落地。

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [48] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 三款小型开源LLM在多标签意图分类任务上表现有差异，Mistral-7B-v0.1在few-shot下优于其他LLM，但整体不及BERT的监督模型；研究为对话系统NLU应用小型开源模型提供了参考。


<details>
  <summary>Details</summary>
Motivation: 探索可在消费级硬件上运行的小型开源LLM，评估其在多标签意图分类任务中的性能，并验证其作为高效NLP工具落地对话系统的可行性。

Method: 对MultiWOZ 2.1数据集中的多标签意图分类进行few-shot学习实验，分别使用Llama2-7B-hf、Mistral-7B-v0.1与Yi-6B等三种开源LLM，并以BERT监督学习模型作为对比，综合采用准确率、精确率、召回率、F1分等多项评估指标，另对推理时间和显存需求等进行报告。

Result: Mistral-7B-v0.1在14类意图中11类F-Score表现最佳（加权平均0.50），并在Humming Loss与Jaccard相似度指标上表现突出，是few-shot场景下最优生成模型。BERT的监督分类模型总体表现优于最好的few-shot LLM。

Conclusion: 小型开源LLM在多意图对话中的检测表现有潜力，Mistral-7B-v0.1在few-shot任务中优于其他LLM，但BERT的监督模型整体表现更佳。该研究为任务型聊天机器人中的自然语言理解提供了有效的分析框架。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [49] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 通过分析社交媒体上双相情感障碍患者的语言，发现其在诊断及之后长期内有明显周期性和多方面的语言变化，验证了社交媒体用于大规模精神健康监测的可行性。


<details>
  <summary>Details</summary>
Motivation: 临床对情感障碍如双相情感障碍（BD）的评估受限于规模，难以实现大范围、高频次的监测。社交媒体语言分析由于高时间分辨率与可追踪性，成为近年研究焦点。作者提出利用社交媒体数据，以补充传统医学评估的不足。

Method: 本文提出一种新颖的方法以确定用户自述诊断时间，通过分析社交媒体发文内容，追踪BD、单相抑郁症（UD）、健康人（HC）三类用户从诊断前3年到诊断后21年的语言特征轨迹，并进行比较。

Result: 研究发现，BD诊断伴随着表征情绪障碍、精神共病、药物滥用、住院、共存疾病、异常思维内容和思维紊乱的广泛语言变化。诊断后二十多年中，情绪相关语言周期性变化尤为突出，具有12个月的明显周期性，提示情绪发作与季节相关。趋势数据还显示，女性用户中此类周期性更显著。

Conclusion: 本研究证明：BD急性与慢性阶段均存在显著语言变化，利用社交媒体数据进行情绪障碍监测具有可行性和价值，拓展了利用SM监测精神健康的研究领域。

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [50] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: 本论文提出多transformer集成和多样化增广方法，在阿拉伯语可读性评估竞赛六项任务全部夺冠，显著提升评估精度。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语可读性评估数据稀缺且类别不平衡，迫切需要有效识别不同可读性水平的强大方法，尤其是在实际应用中对高精细度预测的需求不断增长。

Method: 提出了由四个互补transformer模型（AraBERTv2、AraELECTRA、MARBERT和CAMeLBERT）组成的置信度加权集成方法，每个模型使用不同损失函数进行微调，结合权重训练、高级预处理、利用最强模型对SAMER语料库重新标注，以及通过Gemini 2.5 Flash合成1万条稀有等级样本的方法来补足数据，并采用后处理修正预测分布偏斜。

Result: 在BAREC 2025语句层面和文档层面的阿拉伯语可读性评估所有六个子任务中均取得第一，QWK指标分别达到87.5%和87.4%；后处理增加了6.3%的QWK分数。

Conclusion: 融合多模型、多损失和置信度信息的集成方法，辅以数据增强和后处理，可以显著提升阿拉伯语可读性细粒度评估的稳健性与精确度。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [51] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 目前常用的心理测量问卷不适合直接评估大型语言模型的个性或价值观，作者通过对比分析指出传统问卷与生态问卷的评测差异及潜在误导，呼吁慎用人类设计的量表来评价LLM。


<details>
  <summary>Details</summary>
Motivation: 目前研究者常用心理测量问卷（如BFI、PVQ）来评估大型语言模型（LLMs）的个性特质和价值观。但这些问卷是为人类设计的，其适用于LLM存在疑问，尤其在于问卷的生态效度，即是否真实反映LLM在实际用户查询环境下的表现。

Method: 本文对传统心理测量问卷与生态效度问卷在LLM评测中的效果进行了系统对比分析。

Result: 分析发现：（1）传统问卷与生态效度问卷在LLM评估结果上存在显著差异，且传统问卷无法反映LLM在用户查询语境中展现出的心理特征；（2）传统问卷题量不足，测量稳定性较差；（3）传统问卷可能误导人们认为LLM拥有稳定人格特质；（4）在“人格提示”下，传统问卷会导致LLM人格特征被夸大。

Conclusion: 不建议将传统心理学问卷直接用于LLM的心理特质评价，应警惕其局限性。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [52] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 本文提出了气候科学领域的知识图谱，可实现结构化语义查询，结合大语言模型优化问答质量，有助于气候研究者高效获取有用信息。


<details>
  <summary>Details</summary>
Motivation: 随着气候科学文献的数量和复杂性不断增加，研究人员在模型、数据集、区域和变量间查找相关信息变得愈发困难。

Method: 本研究构建了一个领域特定的知识图谱（KG），涵盖气候学出版物及更广泛的科学文本，允许结构化、语义化的查询。通过Cypher查询对KG进行演示，并结合大语言模型与RAG系统，提升问答的透明度和可靠性。

Result: 知识图谱能支持复杂查询（如模型验证区域、特定模式对应数据集等），并能与大语言模型集成，改善气候相关问答系统的效果。显示出KG在帮助气候研究人员和开发者获取准确、语境相关科学信息上的实际应用价值。

Conclusion: 领域专用知识图谱及其与语言模型的结合显著提升了气候科学知识的获取方式，为相关领域研究人员和开发者提供了有力的信息支持。

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [53] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本研究针对阿拉伯语医疗场景，采集社交媒体真实对话，针对多方言进行数据清洗，并微调了多种大语言模型。结果表明，Mistral-7B微调模型在生成准确、相关的医疗文本上表现最优，展示了生成式AI在提升医院管理和医疗服务中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 当前医院管理系统在应对拥挤、资源有限和紧急医疗服务可用性差等全球性挑战方面存在不足，尤其在不规则输入和小语种（如阿拉伯语）环境下，无法实时、准确地提供医疗建议。

Method: 本研究收集了来自社交媒体的阿拉伯语患者与医生真实交流数据，进行清洗和多方言预处理。然后，采用Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium等前沿大语言模型进行微调，优化其阿拉伯语医疗文本生成能力。最后，通过BERT Score等指标对模型产出进行了量化与质性评估。

Result: 微调后的Mistral-7B模型在BERT Score的精确率、召回率和F1分数上分别达到了68.5%、69.08%和68.5%，优于其它对比模型。系统在非正式输入下能生成连贯、相关且可靠的医疗回复，证明了其实用性。

Conclusion: 生成式人工智能具有提升医院管理系统能力的巨大潜力，尤其能为语言和文化多样的环境提供可扩展、可适配的医疗助理解决方案。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [54] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 人工智能合成高质量医疗对话数据，有效提升阿拉伯语医疗聊天机器人的性能，解决数据资源不足问题。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语医疗聊天机器人发展受限于高质量标注数据集稀缺，现有数据集规模小，模型泛化能力不足。

Method: 提出一种可扩展的合成数据增强策略，利用ChatGPT-4o和Gemini 2.5 Pro生成符合医学语境的问答对，并通过语义筛选与人工验证整合入训练集。对五种大语言模型进行微调和评估，包括BERTScore指标与专家定性分析。还进行了消融实验，比较不同生成模型的数据效果。

Result: 合成数据将训练集扩展至10万条记录，通过消融研究发现ChatGPT-4o生成的数据能显著提升F1分数并减少幻觉现象，为所有模型带来更佳表现。

Conclusion: 合成数据增强是一种可行且高效的方法，可显著提升低资源医疗NLP领域阿拉伯语聊天机器人的性能，为更包容、可扩展和精准的医疗聊天系统提供基础。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [55] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本文提出将韵律显著性检测与ASR结合的方法，基于wav2vec2模型实现词级显著性判别，并训练能同时识别词和显著性的新型ASR系统。实验结果显示韵律信息的集成未显著提升ASR性能，但韵律检测准确率较高，表明transformer模型对韵律特征有良好编码能力，对相关领域具有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于提升自动语音识别（ASR）系统对奥地利德语会话中的韵律显著性（prominence）信息的检测和利用能力，弥补现有系统在识别韵律特征方面的不足，同时推动语音识别与韵律信息结合的相关领域研究与应用。

Method: 本研究首先通过微调wav2vec2模型，实现词级韵律显著性分类，从而形成显著性检测器。接着，用该检测器自动标注大量语音语料的韵律显著性。基于这些自动化标注结果，训练出可同时转录词和显著性等级的新型韵律感知ASR系统。

Result: 集成韵律显著性信息后，ASR系统的识别性能与基线系统无显著差异。同时，在正确识别词序的语句中，显著性检测器实现了85.53%的准确率。

Conclusion: 基于transformer的模型能够有效编码和利用韵律信息。本研究为韵律增强型ASR系统做出新颖贡献，未来可用于语言学研究及韵律感知对话系统等应用。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [56] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出了一种自动生成、评估并分布对齐的人格集方法，能够大幅减少LLM社会模拟中的群体偏差，提升模拟的代表性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 目前利用大型语言模型（LLM）进行社会模拟时，缺乏能够真实反映现实人群多样性和分布的人格设定，且现有研究多忽视这一难题，导致模拟结果可能存在偏差。

Method: 提出了一套系统的方法，利用LLM从社交媒体长期数据中自动生成“叙事型人格”。采用质量评估手段筛选低质量人格特征，并通过重要性采样使人格集与真实心理测量分布（如大五人格）在整体上对齐。同时，引入任务特定模块，根据不同模拟需求对人格集进行适应性调整。

Result: 实验结果显示，该方法显著降低了人口级别的偏差，使社会模拟更加准确灵活，适用于多种研究和政策制定场景。

Conclusion: 所提方法能高效合成高质量、与真实人群分布相符的人格集，有效提升了LLM驱动社会模拟的可靠性和多样性，有助于推动计算社会科学发展。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [57] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: 本论文提出一种可插拔的空间定位模块DocExplainerV0，对VLM正确识别文本但定位不准的问题进行系统分析，并建立标准评测框架，推动模型的可解释性和实际应用。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视觉-语言模型（VLMs）在理解文档和提取文本信息方面表现强大，但在文档中准确定位答案仍然是一个主要难题，限制了解释性和实际应用价值。

Method: 提出DocExplainerV0，这是一种即插即用的边界框预测模块，将答案生成与空间定位进行解耦，能够应用于包括无法微调的专有系统在内的现有VLMs。

Result: 系统性评估显示，文本准确性和空间定位之间存在差距，即使模型生成了正确答案，往往也无法可靠地进行空间定位。提出的标准化评测框架可用于分析这些缺陷。

Conclusion: DocExplainerV0为VLM中的答案空间定位问题提供了通用插件方案，并建立了统一评测标准，有助于提升文档信息抽取VLM模型的可解释性和稳健性。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [58] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本文系统比较了人类与AI生成文本在语域变异上的不同，发现LLMs与人类在某些语言维度上存在共性差异，并据此建立了可解释的评估基准，为模型优化和文本质量提升提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究人类与大型语言模型（LLMs）所写文本在语域变异上的差异，尤其关注在哪些变异维度上，LLMs与人类文本最显著和系统性地不同。

Method: 采用Biber多维分析（MDA），对人类文本与对应的AI生成文本进行维度对比分析。以AI-Brown语料库作为LLM生成文本样本，并与代表当代英式英语的BE-21语料库进行比较。针对捷克语也进行了类似分析，使用AI-Koditex语料库与捷克多维模型，考察了16种前沿LLMs在不同设置和提示下的表现。注重基准模型与指令微调模型的差异。

Result: 发现LLMs与人类文本在部分变异维度上存在显著且系统性的差异。建立了一个可解释的基准指标，可以在这些维度上对模型进行比较和排名。

Conclusion: 多维分析揭示了人类与LLMs在语域变异方面的主要差异，并提出了用于评估和排名模型的新方法，为未来改进模型生成的文本质量提供了方向。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [59] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 本研究揭示了大语言模型在网上情感支持对话中容易出现“积极不符”问题，尤其在高风险情境下表现更明显。通过分级分析对话内容和开发检测工具，作者强调未来应兼顾情感承认与积极表达，以提升LLM辅助对话的可信度和效果。


<details>
  <summary>Details</summary>
Motivation: 在情感支持的对话中，积极回应有时会让人感觉被忽视或贬低，尤其是当回应与语境不符时。作者希望深入分析人类及大语言模型（LLM）在不同情绪强度场景下表达积极支持时出现的“积极不符”（incongruent positivity）现象。

Method: 收集了Reddit平台用户与助手的真实对话，并用大语言模型生成了相同语境下的额外回应。根据情绪强度分为“轻度”（如关系紧张、一般建议）和“严重”（如悲伤、焦虑）两类。对话被分类后，对人类及LLM的支持性回应进行比较分析，还训练了LLMs用于强烈和弱情感反应的数据集，并开发了基于DeBERTa和MentalBERT的弱监督多标签分类器，用于检测不符类型的积极回应。

Result: 发现LLMs在高风险情境下更容易表现出不切实际的积极回应，具有忽视、贬低情感的倾向。分类器在检测不同类型的不符积极回应（轻度及严重关切）方面表现提升。

Conclusion: 单纯生成积极回应并不能满足情感支持的需要，需平衡积极情感和情感承认，推动LLMs调整生成内容以更好对齐人类情感期望；从而有助于构建更可信、更具情境感知能力的在线对话系统。

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [60] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 本文比较了多种大语言模型在多语言、多类别长文本政策分类任务中的表现，发现Longformer及GPT类模型未明显优于最佳开源模型，类别内容重叠影响分类效果。


<details>
  <summary>Details</summary>
Motivation: 当前主流的语言模型如BERT及其变体存在输入文本长度受限的问题，尤其在处理法律文本等超长内容的分类任务时，这一缺陷尤为突出。作者希望评估多模型在处理超长文本分类任务时的性能差异。

Method: 作者在五种语言环境下，采用XLM-RoBERTa、Longformer、GPT-3.5、GPT-4等模型，针对Comparative Agendas Project的多类别政策主题分类任务（包含21个政策标签），进行一系列实验对比。

Result: 实验结果显示，为长文本输入预训练的Longformer并未表现出明显优势。GPT类模型与最佳开源模型相比也未能取得明显领先。进一步分析发现，各类别间的支持度与内容重叠对长文本分类性能影响较大。

Conclusion: 针对超长文本分类问题，专门为长文本设计的Longformer不一定具备显著优势，开源模型在某些情况下甚至优于主流大模型。类别间的相关性和重叠为模型表现的关键因素。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [61] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 本文提出了一种能自动生成对比学习数据并提升模型上下文忠实度的新方法，在多个任务上明显优于现有方法，是大语言模型领域增强信任度的重要进展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理知识密集任务时，经常因为依赖自身参数化知识而非提供的上下文，导致生成不忠实的回答。

Method: 作者提出了一个自我提升的忠实度对比调优（Self Improving Faithfulness Aware Contrastive Tuning）框架，通过自我指令机制让基础LLM自动生成高质量结构化对比学习数据（包括锚点样本、等价正样本和虚假负样本），之后采用对比学习方法训练模型，将忠实回答与不忠实回答在表征空间上区分开。

Result: 在知识冲突评测基准ECARE KRE和COSE KRE上的实验结果表明，基于Llama3 8B Instruct的SI FACT模型比最优基准方法提升了6.2%的上下文召回率，同时显著降低了对内部记忆的依赖。

Conclusion: SI FACT框架有效提升了大模型对上下文的忠实度，并以高数据效率为构建更可靠的语言模型提供了实用途径。

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [62] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: 提出的DERN框架能细致地重组和精简SMoE模型专家，显著提升推理性能和内存效率，对大模型部署具有现实意义。


<details>
  <summary>Details</summary>
Motivation: SMoE架构虽然计算高效，但每次推理仍需加载所有专家参数，导致内存使用高且部署困难，且现有方法只关注专家级别的操作，忽略了更细粒度的神经元结构优化。

Method: 提出了一种名为DERN（Dropping Experts, Recombining Neurons)的新型框架，通过三步骤：利用路由统计信息剪枝冗余专家，将专家细分为神经元段并分配到最兼容的保留专家，最后合并同一专家下的神经元段，形成更紧凑的模型，无需再训练且适用于任意任务。

Result: 在Mixtral、Qwen、DeepSeek等SMoE模型上，在50%专家稀疏率下，DERN在常识推理和MMLU基准上性能提升超过5%，显著减少了专家数量和内存占用。

Conclusion: DERN方法使SMoE语言模型更加高效、易于部署，在不增加训练成本的前提下提升了模型性能和资源利用效率。

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [63] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文理论及实证分析ICL，发现其是有限的学习机制，对新任务泛化受限，并非真正实现了通用的“学习”。


<details>
  <summary>Details</summary>
Motivation: 该论文关注于自回归模型的in-context learning（ICL）能力，探究其是否真的具备通过少量样本无监督地学习和泛化新任务的能力，回应业界对ICL“学习”本质的讨论。

Method: 作者从理论上论证ICL是否属于学习，并进行了大规模实证分析，排除了记忆和预训练效应，考察了分布偏移、提示风格和措辞等多种影响因素。

Result: ICL在特定条件下是一种有效的学习范式，但其泛化到新任务的能力有限。随着例子增多，准确性对例子分布、模型、提示风格和输入语言特征不敏感，但更依赖于提示中的模式，这导致在某些提示风格上分布敏感。

Conclusion: 自回归模型的ICL并非一种稳健、通用的编码机制，其泛化能力有限，尤其是在面对分布改变和不同提示风格时表现不佳。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [64] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 传统Transformer模型受长度限制，截断长文会影响作文评分有效性。本文比较了多种支持长文本的Transformer模型在自动作文评分上的表现，结果显示长文本支持对评分任务十分关键。


<details>
  <summary>Details</summary>
Motivation: 主流Transformer模型受限于输入文本最大长度，无法完全处理高年级学生常写的长篇作文，常用的截断方式影响模型对写作结构等需长距离依赖信息的评价准确性。

Method: 评估和对比多种在结构上调整、以克服长度限制的Transformer变体，包括XLNet、Longformer、ModernBERT、Mamba和Llama，并在Kaggle ASAP 2.0数据集上进行微调实验。

Result: 通过对几种结构改进后的模型进行实验，研究其对自动作文评分任务中超长文本处理能力的提升情况。

Conclusion: 结构上支持长文本输入的改良版Transformer模型，更适合应用于自动作文评分任务，能更全面地考察写作组织结构等需要整体性理解的要素。

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [65] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 本文提出了云-边端多智能体协同架构，通过GuideLLM、SolverLLM和JudgeLLM三模块配合，结合RefactorCoderQA多领域新基准，全面提升了大语言模型在编码任务中的能力，实验证明性能领先主流模型，并在系统性能上获得良好结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在推理和问题解决能力方面仍有提升空间，且现有的编码任务基准数据集覆盖面有限，不能全面反映真实应用场景的需求。

Method: 提出了一种云-边端协同的多智能体提示框架，包含GuideLLM（边缘轻量引导模型）、SolverLLM（云端解题模型）和JudgeLLM（自动评测模型）三部分。同时，构建了新的编码评测基准RefactorCoderQA，覆盖软件工程、数据科学、机器学习和自然语言处理等领域。最后通过大规模实验和系统级评价，分析性能及权衡。

Result: RefactorCoder-MoE微调模型在多领域编码任务中取得了76.84%的最优准确率，显著超过开源和商用基线模型。同时通过人工评估验证了解决方案的可解释性、准确性和实际相关性，并从系统层面对吞吐量和延迟等指标进行了评估。

Conclusion: 所提云-边多智能体协作架构及RefactorCoderQA基准显著提升了LLM的推理、解题能力和现实应用适配性，为未来相关系统的设计和优化提供了有效参考。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [66] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive通过自动问题生成与多轮强化学习，显著提升开源大模型的深度检索和长程推理能力，实现了新一代开源竞品最佳结果。


<details>
  <summary>Details</summary>
Motivation: 当前开源大语言模型（LLMs）在结合浏览工具以处理复杂、现实任务时，表现依然不佳，主要原因在于长程推理能力有限和缺乏高难度的监督数据。

Method: 提出DeepDive方法：一是自动合成复杂难找的问题用于训练，二是利用端到端多轮强化学习（RL）提升LLMs的长程推理与深度检索能力。

Result: 实验结果表明，DeepDive-32B在BrowseComp上取得了新的开源竞品最佳结果，超越了WebSailor、DeepSeek-R1-Browse和Search-o1，并且多轮RL训练大幅提升了深度搜索能力和多项基准的表现。该方法还实现了测试时工具调用和并行采样的可扩展性。

Conclusion: 多轮强化学习结合自动合成难题，有效提升了LLMs在复杂检索与推理任务中的表现，推动了深度搜索智能体的发展。

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [67] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE是一种只用文本提升语音识别模型在新领域表现的方法，通过VAE建模与联合微调，无需额外语音数据，显著降低识别错误率。


<details>
  <summary>Details</summary>
Motivation: 虽然像Whisper这样的预训练ASR（自动语音识别）模型表现良好，但在面对新领域词汇和表达方式时仍需适配。实际场景中很难收集语音数据，因此只能用文本进行适配。

Method: 提出了WhisTLE，这是一种只需文本的深度监督适配方法。具体做法是用变分自编码器（VAE）建模预训练ASR模型encoder端的输出，并用这个文本到潜变量的encoder联合微调decoder，此外还可以结合文本转语音（TTS）进行适配。推理时，恢复原encoder，不增加额外推理开销。

Result: WhisTLE结合TTS情况下，相较于只用TTS适配方法使字错误率（WER）相对下降12.3%，并在32种适配场景中的27种都优于其他非WhisTLE基线方法。

Conclusion: WhisTLE可以在无需额外语音数据的情况下，有效提升ASR模型对新领域的适配能力，且在多个数据集和模型上验证了其优越性能。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [68] [On arc-density of pushably $3$-critical oriented graphs](https://arxiv.org/abs/2509.10182)
*Tapas Das,Pavan P D,Sagnik Sen,S Taruni*

Main category: cs.DM

TL;DR: 作者系统刻画了pushably $3$-critical有向图的最小弧数、相关平均度与girth界限，提升了对有向图色数与同态理论的认识和具体界限，实现了对经典结果的拓展与改进。


<details>
  <summary>Details</summary>
Motivation: 研究oriented graph（有向图的一个特定类型）在一种名为pushable coloring下的临界结构，推动相关图论参数（如平均度数、girth）上的新界限，以获得更优的染色和同态结果。这个研究是对Borodin等人1998年结果的改进和推广。

Method: 作者建立了pushably $k$-critical有向图的结构特性，通过精确计数推导了$n$阶pushably $3$-critical有向图的最小弧数下界。此外，研究了在最大平均度（maximum average degree）和girth上的临界图，通过穷举与构造例子的方式证明界的紧性，同时将这些结论应用于图的同态映射和相关色数问题上。

Result: 证明了除了4个特例外，任何$n$阶pushably $3$-critical有向图至少有$\frac{15n+2}{13}$条弧，且该下界最优。进一步推出：最大平均度小于$\frac{30}{13}$且girth至少为5的有向图，pushable chromatic number不超过3，包括所有girth至少15的平面/射影平面有向图。此外，给出所有平均度等于$\frac{30}{13}$的pushably $3$-critical 图及界的紧性例子，所有这些类的图都能同态映射到6点有向平面图（$K_{2,2,2}$的有向版）。$2$-dipath $L(p, q)$以及有向$L(p, q)$的spans对于$q\leq p$都至多为$2p+3q$。

Conclusion: 本文在pushably $3$-critical有向图的结构、色数、同态等方面取得了系列最优新结论，显著改进了已有相关理论，为有向图的色彩、稠密性及结构理论提供了新视角和工具。

Abstract: An oriented graph $\overrightarrow{G}$ is pushably $k$-critical if it is not
pushably $k$-colorable, but every proper subgraph of $\overrightarrow{G}$ is.
The main result of this article is that every pushably $3$-critical oriented
graph on $n$ vertices, but for four exceptions, has at least $\frac{15n+2}{13}$
arcs, and that this bound is tight. As an application of this result, we show
that the class of oriented graphs with maximum average degree strictly less
than $\frac{30}{13}$ and girth at least $5$, which includes all oriented planar
and projective planar graphs with girth at least $15$, have pushable chromatic
number at most $3$. Moreover, we provide an exhaustive list of pushably
$3$-critical graphs with maximum average degree equal to $\frac{30}{13}$ and a
pushably $3$-critical orientation of a $4$-cycle to prove the tightness of our
bound with respect to both maximum average degree and girth. We also show that
these classes of oriented graphs admit a homomorphism to an oriented planar
graph on six vertices (an orientation of $K_{2,2,2}$) which (tightly) improves
a result due to Borodin \textit{et al.} [Discrete Mathematics 1998].
Furthermore, for these classes of oriented graphs, we prove that the $2$-dipath
$L(p,q)$ and the oriented $L(p,q)$ spans are upper bounded by $2p+3q$ for all
$q \leq p$. All these implications improve previously known results.

</details>
