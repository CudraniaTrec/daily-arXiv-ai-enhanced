<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 58]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Fair intersection of seekable iterators](https://arxiv.org/abs/2510.26016)
*Michael Arntzenius*

Main category: cs.PL

TL;DR: 本文将miniKanren的公平搜索与有界工作量思想，应用到关系型数据的最优连接实现，提出了函数式语言可嵌入的优雅解法，实现高效且组合性良好的查询。


<details>
  <summary>Details</summary>
Motivation: miniKanren相比Prolog的主要语义进展在于其实现了高效且完全的搜索策略，实现了在选择分支之间的公平交错执行。本文的动机是将这种通过有界工作量实现公平性的思想，应用到关系数据库中的最优连接算法实现上。

Method: 作者利用有界工作量实现分支间公平切换的思想，提出了一种使用可查找迭代器接口的方法，并将其用于在函数式语言中实现浅嵌入式的最坏情况最优连接算法。

Result: 本文展示了通过这种组合的、有界工作量驱动的迭代器接口，可以在函数式语言中优雅地实现最坏情况最优连接算法，同时保持良好的可组合性和嵌入友好性。

Conclusion: miniKanren的公平搜索和有界工作量理念可成功应用于关系型连接问题，实现高效、可组合和嵌入友好的最优连接算法。

Abstract: miniKanren's key semantic advance over Prolog is to implement a complete yet
efficient search strategy, fairly interleaving execution between disjuncts.
This fairness is accomplished by bounding how much work is done exploring one
disjunct before switching to the next. We show that the same idea -- fairness
via bounded work -- underlies an elegant compositional approach to implementing
worst-case optimal joins using a seekable iterator interface, suitable for
shallow embedding in functional languages.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 本文针对企业在开源深度整合中面临的系统性风险，提出了一个以“目标-威胁-脆弱性-应对措施”(OTVM)为核心的全面风险治理框架，通过专家访谈与实际案例验证，帮助企业实现从局部被动应对到全局主动治理的转变。


<details>
  <summary>Details</summary>
Motivation: 企业对开源的参与度已经从战术层面的采用提升到战略层面的深度整合，但随之出现了更为复杂的风险环境，仅依赖传统、以技术为中心的风险管理方式，难以应对如上游“静默修复”、社区冲突、突然的许可证变更等系统性威胁，导致治理盲点。作者希望解决企业在开源治理中的风险管理不足问题。

Method: 作者采用了扎根理论研究方法，对15位业界实践者进行了访谈，以开发一个全面的风险治理框架。

Result: 作者提出了一个基于“目标-威胁-脆弱性-应对措施”（OTVM）的逻辑链风险治理框架，并提出了三项具体贡献：（1）明确目标的“战略目标矩阵”；（2）外部威胁与内部脆弱性的双向系统分类；（3）将能力建设与脆弱性对应的可操作性应对框架。框架通过真实案例的回顾研究，获得三位业界专家的实证验证。

Conclusion: 该研究提供了企业从被动“救火”到主动构建组织“免疫系统”的系统化路径和诊断工具，为开源环境下的企业风险治理带来了新的理论与实践基础。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [3] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM系统结合大语言模型与模型驱动工程，通过多层次约束和验证，实现可合规、可验证的自动工件生成，在安全和合规领域大幅提高效率并减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 在安全和合规关键领域，如汽车软件工程和跨境法律管辖等，自动生成可合规、可验证的工件极具挑战，且人工修正代价高昂。当前大语言模型易忽略结构和语义约束，难以直接满足监管要求。

Method: PRISM提出三大支柱：一是统一元模型(UMM)将异构结构和法规文本融合到同一语义空间；二是综合约束模型(ICM)将结构和语义要求转化为可执行工件如GBNF、DFA自动机和后置校验器如SHACL、SMT；三是约束引导的可验证生成（CVG），实现生成时结构约束和生成后语义验证。违例时，自动修复并留下合规追踪记录。

Result: PRISM在AUTOSAR（汽车软件）和Brussels I bis（法律域）应用，生成的工件结构正确、可审计、能集成现有工具链，显著减少人工修正。

Conclusion: PRISM为高安全和合规领域的自动工件生成提供了可行方案，为自动化且内建保障的合规工件生成奠定基础，促进自动化合规推进。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [4] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: 本文提出CodeSight系统，通过GitHub数据采集、流程挖掘和LSTM模型，实现了对PR截止期合规性的高精度预测，有助于主动监控开发流程并提升管理效率。


<details>
  <summary>Details</summary>
Motivation: 软件开发中PR难以精准预测能否按时完成，影响项目管理效率。希望通过自动化、数据驱动手段提前发现风险。

Method: 直接从GitHub抓取开发与部署数据，经流程挖掘转化为日志，提取特征后采用LSTM模型进行PR剩余时间预测。

Result: 系统在预测PR按时完成问题上取得了高精度和高F1分数，验证了方法的可行性和有效性。

Conclusion: 将流程挖掘与机器学习结合，能够有效提升软件项目管理的主动性，准确预测PR是否能按时完成。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [5] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: 论文提出并评估了CHCVERIF，它利用现有软件验证工具解决CHC问题，对位向量基准表现较好，说明软件验证工具可作为CHC求解后台。


<details>
  <summary>Details</summary>
Motivation: 为了更好地解决安全性验证、程序不变量合成、跨过程分析等任务，希望利用已有的软件验证工具，提升对CHC特别是复杂语义（如位向量）的支持能力。

Method: 提出了一种组合型CHC求解器CHCVERIF，利用软件验证的方法与现有成熟工具结合，专门处理涉及位向量与底层语义的CHC基准问题。

Result: 该方法在线性整数算术问题上表现一般，在位向量相关的CHC基准上取得了一定的成功，展示了组合方案的实际效果。

Conclusion: 证明了将软件验证工具用于CHC求解具有可行性和潜力，特别是在精心构建的组合和应对特定语义时效果较好。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [6] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 当前大模型在真实项目类级代码生成上表现远逊于理想，检索增强有一定帮助但并非万能。未来亟需关注上下文建模、文档策略与检索集成，以改善生产环境中的代码自动生成工具。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在单函数生成方面已取得显著进步，但其在复杂、真实软件工程场景下的能力尚未充分评估和理解。类级别代码不仅更贴近实际开发需求，也是智能编码助手服务落地的关键环节。

Method: 本文构建了源自开源代码库的新颖基准，涵盖真实世界的类级别代码生成任务，并将任务划分为见过和未见过两类以测试LLMs泛化能力。对不同模型、输入规格、检索增强配置和文档完整性多因素进行性能评估与误差分析。

Result: 在合成基准上，主流LLMs正确率达84%-89%，但在真实类任务仅有25%-34%；见过与未见代码库间差异不大。充分文档最多提升1%-3%正确率，统计意义不显著。检索增强在文档不完备时能提升4%-7%的正确率。主要错误类型为AttributeError、TypeError和AssertionError（占84%），检索增强可降低逻辑错误但可能引发依赖冲突。基准分析揭示LLMs在类级工程中的显著短板，并对未来优化提出方向。

Conclusion: 当前大型语言模型（LLMs）在函数级别代码生成上表现卓越，但在真实项目的类级别代码实现上存在巨大不足，表现显著低于合成基准测试。同时，类级别泛化能力差，现有LLMs对见过和未见过代码库的任务表现接近。补充文档有微弱提升，检索增强方法有一定帮助，但也可能引入新问题。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [7] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 针对因顺序依赖导致自动化测试效率低下的问题，论文提出基于静态字段分析的优先级识别方法，在多项目实验中有效减少了测试执行和重跑次数，显著提升检测效率。


<details>
  <summary>Details</summary>
Motivation: 自动化测试由于flaky tests（不稳定测试）出现不可靠性，尤其是顺序依赖型（OD）测试，经常导致CI流程失败。现有检测与修复方法需多次重复运行测试，效率低下，因此有必要优先识别可能的OD测试，以减少重复运行。

Method: 该论文提出了一种基于测试类共享静态字段的分析方法，用于优先识别可能的顺序依赖型测试，从而降低无效测试重跑的数量。

Result: 在27个项目模块的实验中，该方法在23个模块成功优先识别所有OD测试，将测试执行次数平均减少了65.92%，无必要重跑次数减少了72.19%。

Conclusion: 该方法能显著提升顺序依赖型测试检测的效率，降低执行成本。

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [8] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 本文综合分析国内外软件供应链安全框架，发现针对关键基础设施领域的框架不足，归纳了安全实践核心类别并设计多层问题清单，建议采取更综合和情境化的方法应对关键基础设施的软件供应链安全挑战。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击频率和复杂性不断增加，对关键基础设施领域构成严重威胁，目前的安全框架存在碎片化和针对性不足的问题，特别是未能满足关键基础设施的特殊需求。

Method: 综合国际框架、澳大利亚监管资料及学术研究，采用多声道文献综述和“4W+1H”分析方法，系统梳理并归纳软件供应链安全实践。

Result: 发现现有安全框架很少针对关键基础设施领域。归纳出十大核心类别的安全实践，并将其与生命周期、参与者角色和实施层级进行映射，最终制定出多层次80项问题清单，帮助相关利益方评估和提升安全性。

Conclusion: 现有框架与关键基础设施的具体需求之间存在显著差距，亟需更具整合性和针对性的安全措施，以应对不断演化的软件供应链风险。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [9] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文通过设计科学研究方法，整合多种证据，构建了生成式AI增强软件工程的未来发展路线图，识别研究挑战与方向，并提出2030年十大预测，助力SE领域应对GenAI快速变革。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）正在快速改变软件工程（SE）的实践，对SE流程及软件系统的开发、运维和演进方式产生深远影响。当前需要系统性梳理GenAI对SE带来的变革及其研究挑战。

Method: 采用设计科学方法，结合FSE 2025“Software Engineering 2030”研讨会协作讨论、快速文献综述、外部同行反馈，以三轮递进融合多种证据。应用McLuhan四象限框架系统性梳理GenAI对SE流程和产品的影响。

Result: 构建了GenAI增强SE的路线图，识别出四种基本的增强形式，并系统性揭示了各自面临的研究挑战与机会。将见解整合为未来研究方向，并提出了2030年的十项SE预测。

Conclusion: 本研究构建了透明且可复现的分析基础，帮助业界和学界系统理解GenAI影响下的软件工程流程、方法和工具，同时为未来的研究提供了明确路线和参考预测。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [10] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: 本文提出基于MCTS的仓库级软件工程强化学习框架RepoSearch-R1，显著提升答案完整性与训练效率，解决数据合规难题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理代码仓库相关的软件工程任务时，基于上下文的学习方法在工具使用和通过环境反馈进行决策方面存在引导不足的问题，而训练型方法依赖于昂贵的大模型蒸馏，且带来企业级的数据合规风险。

Method: 提出RepoSearch-R1，一个基于蒙特卡洛树搜索（MCTS）的智能体强化学习框架，允许模型通过自我训练生成高质量、多样化的推理轨迹，无需蒸馏或外部监督。以此为基础构建了用于仓库问答的RepoQA-Agent。

Result: 在仓库问答任务中，RepoSearch-R1在答案完整性上较无检索方法提升16.0%，较迭代检索方法提升19.5%，训练效率相比一般智能体强化学习方法提升33%。

Conclusion: RepoSearch-R1框架有效提升了代码仓库级推理任务中答案的完整性和训练效率，并通过冷启动训练方法规避了数据合规风险，保持了探索多样性和答案质量。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [11] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: 本文首次大规模测量了GitHub Actions的碳足迹和水足迹，发现其环境影响显著。建议优化地区部署、调度和资源利用，以减缓CI/CD带来的环境负担。


<details>
  <summary>Details</summary>
Motivation: 随着云计算带来的环境影响日益增长，软件开发领域中广泛使用的CI/CD流水线（如GitHub Actions）所产生的碳足迹和水足迹（CWF）却很少为开发者所知。CI服务商通常不公开相关数据，缺乏对这一隐性环境成本的了解。理解这些服务的环境足迹对于推动可持续软件开发变得非常重要。

Method: 本研究以Cloud Carbon Footprint框架为基础，首次使用了文献中最大规模的数据集，涵盖超过2.2百万次GitHub Actions工作流运行、涉及18,000多个开源代码仓库。通过对这些数据的定量分析，估算GitHub Actions生态系统的碳和水足迹。

Result: 分析显示，GitHub Actions产生了可观的环境影响：2024年碳足迹的估算值在最乐观情景为150.5 MTCO2e，最悲观情景为994.9 MTCO2e，最可能情景为456.9 MTCO2e。水足迹在1,989.6到37,664.5千升之间，最可能情景为5,738.2千升。研究还提出了减缓环境影响的建议，包括选择低碳能源地区部署runner、减少无效资源消耗、优化任务调度时间和缩小仓库体积。

Conclusion: GitHub Actions等CI/CD服务的环境影响不容忽视，通过合理部署和策略优化可显著减缓其碳足迹和水足迹。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [12] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: Nexus提出了一种多智能体协作框架，用于自动生成高质量测试预言子，经实验证明其准确性和下游应用效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非回归测试中的测试预言子生成一直是软件工程中的难题，目的是自动产生能准确判断被测函数（FUT）行为是否符合预期的预言子。

Method: 提出Nexus，一个多智能体框架。通过四种不同测试理念的特化智能体进行协作评议，初步生成测试预言子后，系统在沙箱环境中用候选函数实现运行验证，并通过自动化自我修正环路，利用运行时错误调试和完善失败的预言子。

Result: 在七个基准测试上的广泛评估表明，Nexus在测试预言子的准确率明显优于现有方法。例如，在LiveCodeBench基准下，GPT-4.1-Mini的准确率从46.30%提升到57.73%。在HumanEval上，预言子生成的缺陷检测率从90.91%提升至95.45%；自动化程序修复成功率从35.23%提升至69.32%。

Conclusion: Nexus通过多智能体协作、迭代验证和自动优化，有效提升了测试预言子生成的准确率和下游程序分析任务的效果，超过了当前主流基线方法。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [13] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 提出基于大语言模型的安全代码自动审查方法SecureReviewer，结合安全领域知识与专用评价指标，相比已有方法在安全检测与审查效果上有明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动化代码审查方法主要针对通用场景，对安全相关问题的识别和改进效果尚未充分研究，且在数据集和评价指标方面存在挑战。这增加了安全隐患，影响代码的长期安全性。

Method: 作者提出了SecureReviewer，专为提升大模型识别和修复安全问题能力的自动化代码审查方法。具体方法包括：构建专用于安全审查的数据集，基于该数据集对大模型进行安全感知微调，结合RAG技术嵌入安全领域知识减少幻觉，并提出用于衡量安全审查效果的新指标SecureBLEU。

Result: 实验表明，SecureReviewer在安全问题检测准确率、代码审查评论质量及实用性方面均优于现有最新方法。

Conclusion: SecureReviewer提升了大模型在安全代码审查中的有效性和可靠性，为安全性问题的自动检测与修复提供了新方案。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [14] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本文系统评估了五款开源LLM在Python代码自动提取重构任务上的能力，发现递归批评改进型提示效果显著优于单次提示，尤其是Deepseek-Coder-RCI和Qwen2.5-Coder-RCI模型，不仅测试通过率高且代码质量优，获得开发者高度认可。研究强调结合自动化和人工评估，助力LLM自动重构技术发展，并提供开源基准促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有代码提取重构依赖大量人工操作，效率低且难以规模化。随着高效开源LLM的发展，自动化该任务有望提升代码可读性和可维护性，但需验证模型效果和最佳提示策略。

Method: 系统评估了五个主流开源LLM（参数规模3B~8B）在Python代码自动提取方法重构任务的表现。采用自动化指标（功能正确性和代码质量）和开发者调查，并比较了单次提示与RCI递归批评改进提示策略。

Result: RCI提示方法在测试通过率和重构质量方面均优于单次提示，Deepseek-Coder-RCI和Qwen2.5-Coder-RCI分别在TPP、LOC、CC等指标上表现突出，并有超过70%的开发者接受率。人类评分与传统自动化指标（如CC和LOC）有偏差，强调人工参与评估的重要性。提出了开源基准数据集，推动后续研究。

Conclusion: RCI驱动的提示策略显著优于单次提示，在测试通过率和重构质量方面均表现更佳。Deepseek-Coder-RCI和Qwen2.5-Coder-RCI模型在测试通过率和代码质量上优于其他模型，并获得开发者高度认可。这表明高质量提示可有效提升自动化重构效果，但需要结合人工评估以确保实际价值。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [15] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 提出自动化LLM生成网页编辑数据集（Instruct4Edit），实现高效准确网页代码编辑，开源所有结果，小型开源模型微调后性能强劲，推动自然语言网页编辑进展。


<details>
  <summary>Details</summary>
Motivation: 现有网页应用的代码迭代和编辑过程依赖人工手动，十分耗时。虽然大语言模型能生成代码，但对已有代码的精确编辑还面临挑战，原因在于缺乏高质量、大规模调优数据来满足人类期望。

Method: 提出一种自动化数据生成流程，利用LLM自动合成网页编辑任务的高质量微调数据集（Instruct4Edit），方法包括生成多样化指令、相应代码修改，以及视觉验证确保正确性。

Result: 通过在Instruct4Edit上微调模型，实现了将人类意图转化为结构合理、视觉准确代码编辑的持续改进。小型开源模型微调后性能可媲美闭源系统。所有数据、代码与模型已开源。

Conclusion: 该工作为自然语言驱动的网页编辑提供了可扩展、透明的新基础，验证了自动化微调数据生成和小型模型微调的有效性，推动了相关技术的开放与发展。

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [16] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 针对LLM在软件工程应用中带来的新挑战，作者梳理现状、分析问题，并提出提升研究标准和可持续性的建议。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在软件工程研究中的广泛应用，诸如基准测试严谨性、数据污染、可复现性和可持续性等新挑战也随之出现。作者希望推动研究社区反思并解决这些挑战。

Method: 该论文通过对当前在ICSE会议发表的基于LLM的软件工程研究进行结构化梳理和分析，识别积极实践与存在问题，并提出改进建议。

Result: 分析表明，部分LLM-SE研究在基准测试和可复现性等方面已展现出积极做法，但诸多短板依然存在，例如验证不严谨、可持续性考量不足等。

Conclusion: 论文强调需要加强LLM-SE研究的基准测试严谨性、提升可复现性，并关注相关研究的财务及环境成本，助力社区形成更健康的研究生态。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [17] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文提出并验证了ZKMLOps框架，通过集成零知识证明，实现机器学习模型合规性证明过程的自动化，兼顾审核需求与模型/数据隐私，有望解决AI合规验证难题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键领域的广泛应用，合规性与可信赖性（如可验证责任追踪）成为重要议题，法规要求透明性，但与保护专有资产（如机密数据、模型）存在矛盾，现有手段如流程审查、形式化方法等成本高且不适应AI黑盒特性，影响AI应用的合规与可验证性。

Method: 提出了ZKMLOps框架，将零知识证明（ZKP）集成进MLOps（机器学习运维）流程，与现有的软件工程模式结合，提供可模块化、可复现的加密合规证明生成过程，并通过金融风险审计的合规性案例及主流ZKP协议的性能实证分析对其进行了评估。

Result: ZKMLOps 能实现合规性加密证明，在有效保护数据及模型隐私的同时，满足监管的审核、可验证性要求。通过金融风险合规案例和不同复杂度ML模型的ZKP协议性能评估，验证了其实用性与可行性，并讨论了性能权衡。

Conclusion: ZKMLOps为AI系统合规验证提供了一种兼顾隐私保护和可验证性的创新方案，降低了审核门槛，有望推动AI在更多受监管领域的应用。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [18] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 该论文提出并评估了一种集成于开发环境的新型贝叶斯推断调试工具，有效降低了调试难度和所需时间。


<details>
  <summary>Details</summary>
Motivation: 概率编程可以让用户将贝叶斯模型变为程序，并自动执行后验推断，使更多领域的从业者能应用这些技术。然而，调试贝叶斯推断过程既耗时又需要很深的相关知识，因此提高调试效率和降低门槛十分重要。

Method: 提出一种新型的贝叶斯推断调试方法，并开发了一个能直接集成到开发环境中的调试工具。同时，进行了包含18名有经验参与者的用户研究来评估方法效果。

Result: 实验表明，所提出的在线和交互式贝叶斯推断调试方法，显著减少了调试任务所需的时间和难度。

Conclusion: 该工作展示了一种可在开发环境中应用的贝叶斯推断调试工具，有效提升了推断调试的效率和易用性，对实际开发有积极意义。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [19] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: 块状编程教学中，“一键看答案”容易削弱学习效果。Stitch系统以分步互动方式，让学生理解并逐步修正错误，经实验比自动反馈工具与直接答案法更能提升编程学习成果。


<details>
  <summary>Details</summary>
Motivation: 虽然块状编程环境（如Scratch）在编程教育中日益流行，其简化了语法错误，但初学者仍常常面临难以解决的语义错误。现有调试流程通常直接向学生展示正确答案，这有助于纠错但不利于培养解决问题的能力。

Method: 提出Stitch互动辅导系统，通过Diff-Analyze模块对比学生项目与参考实现，找出关键差异，并利用大型语言模型说明这些差异的意义。学生通过定制渲染引擎逐步查看高亮代码块、理解解释、选择性地修复部分错误，迭代直到功能正确。实验研究将Stitch与现有自动化反馈工具进行对比评估。

Result: 研究发现，仅展示正确答案教学效果不佳。Stitch的互动式分步指导系统可显著提升学习效果，比直接答案法和目前自动反馈工具更有优势。

Conclusion: 分步引导与逐步反馈能有效提高初学者在块状编程中的学习成效。该研究证明了互动式分步辅导系统在编程教学中的优势。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [20] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 本研究通过分析ImageMagick中漏洞再引入与过程度量的关联，发现团队问题管理短板往往导致安全漏洞再次出现，说明过程度量有助于实现安全风险预测与治理。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究通过源代码度量识别漏洞，但鲜有工作关注过程度量能否揭示易引入风险的开发活动。该研究动机在于探索过程度量如何揭示漏洞的再引入，从而为漏洞预测与软件安全防护提供新思路。

Method: 本研究在ImageMagick项目上开展案例研究，通过关联纵向过程度量（如bus factor、问题密度和问题陈旧度）与漏洞再引入活动，涵盖76个漏洞再引入实例。研究不仅分析安全修复提交是否引入新漏洞，还关注漏洞在长序列变更中的演化与重现过程。

Result: 结果发现，漏洞再引入常与问题陈旧度上升和问题密度波动有关，反映出短期内问题管理效率低、团队响应性差等现象。

Conclusion: 过程度量（不仅仅是源代码度量）对于理解和预防漏洞再引入至关重要，可作为预测高风险修复和提升软件安全能力的重要依据。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [21] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: AI自动迁移Python库API使用点精准，但应用功能可靠性不足，自动化代码维护仍需改进。


<details>
  <summary>Details</summary>
Motivation: 软件系统需要保持更新，以减少技术负债、解决安全漏洞以及避免老旧系统的僵化，但库和框架的更新过程繁琐且易错。近年来大型语言模型（LLMs）和智能编码系统为自动化维护任务带来了新机会。

Method: 评估Github的Copilot Agent Mode——一种自主AI系统——在自动迁移SQLAlchemy库时的表现。数据集包含十个Python应用，提出了“Migraton Coverage”这一衡量API迁移正确性的指标，以量化自动迁移效果。

Result: LLM Agent能够完成SQLAlchemy不同版本间的功能和API迁移，迁移覆盖率达到100%（中位数），但无法保证应用功能的完整性，自动迁移后的测试通过率仅为39.75%（中位数）。

Conclusion: 虽然AI代理可以完全迁移API使用点，但现阶段仍难以保证整体应用功能，自动化库迁移还需进一步完善以提升可靠性。

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [22] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 论文系统比较了不同日志解析技术与架构，发现语义方法模板识别更优，语法方法效率分组优势显著。双阶段架构有效提升准确率。创新提出的SynLog+模块，在不增加运行成本下，大幅提升主流方法的解析准确率。


<details>
  <summary>Details</summary>
Motivation: 日志在系统运行过程中提供重要信息，但随着日志体量和复杂性增加，自动化分析成为必要。日志解析技术种类繁多，需系统评估其性能和特点。

Method: 对现有的语法及语义日志解析方法、单阶段与双阶段架构进行实证对比分析，并提出SynLog+模块用于双阶段架构的模板识别。

Result: 实验发现，语义方法模板识别能力较强，语法方法速度快且分组准确但模板识别不足。双阶段架构提升解析准确性。SynLog+模块提升语法方法236%和语义方法20%的解析准确率，且几乎不增加运行时间。

Conclusion: 该研究明确了不同日志解析方法和架构的优缺点，提出的SynLog+显著提升了解析准确性并保证效率，对自动日志分析实践具有指导意义。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Finding Regular Herbrand Models for CHCs using Answer Set Programming](https://arxiv.org/abs/2510.26428)
*Gregoire Maire,Thomas Genet*

Main category: cs.LO

TL;DR: 本论文提出利用Clingo工具，通过将带有代数数据类型的受约束Horn子句转化为SAT问题，从而自动化地构建树自动机进行可满足性检验，提升了此类逻辑系统的分析能力。


<details>
  <summary>Details</summary>
Motivation: 该论文关注于带有代数数据类型（ADT）的受约束Horn子句（CHCs）的可满足性证明。由于现有方法虽有进展，但面对复杂数据结构时仍存在挑战，因此有必要探索新的有效自动化可满足性检验方法。

Method: 作者提出通过构建识别CHC的Herbrand模型的树自动机来证明CHC的可满足性。具体方法是，将CHC与ADT编码成SAT问题，并利用Clingo（一个答案集编程工具）进行求解。此方法不同于Kostyukov等基于模型查找的自动机构造方法。

Result: 作者实现了CHC到ASP问题的自动转换，并与Clingo结合，得到了一个半完备的可满足性检查器：若存在正则的Herbrand模型，则能找到相应的树自动机；若问题不可满足，则能找到反例。

Conclusion: 该方法为基于自动机技术的CHC可满足性检验提供了一种新的有效途径，扩展了正则模型验证的能力，并在理论和工具实现方面取得了进展。

Abstract: We are interested in proving satisfiability of Constrained Horn Clauses
(CHCs) over Algebraic Data Types (ADTs). We propose to prove satisfiability by
building a tree automaton recognizing the Herbrand model of the CHCs. If such
an automaton exists then the model is said to be regular, i.e., the Herbrand
model is a regular set of atoms. Kostyukov et al. have shown how to derive an
automaton when CVC4 finds a finite model of the CHCs. We propose an alternative
way to build the automaton using an encoding into a SAT problem using Clingo,
an Answer Set Programming (ASP) tool. We implemented a translation of CHCs with
ADTs into an ASP problem. Combined with Clingo, we obtain a semi-complete
satisfiability checker: it finds a tree automaton if a regular Herbrand model
exists or finds a counter-example if the problem is unsatisfiable.

</details>


### [24] [Semantic Properties of Computations Defined by Elementary Inference Systems](https://arxiv.org/abs/2510.26429)
*Salvador Lucas*

Main category: cs.LO

TL;DR: 论文提出了一种通过任意模型可满足性检验的方法，无需典型模型，即可分析由元素推理系统定义的集合、关系或计算的语义属性，尤其适用于编程语言、重写系统等的属性证明与分析。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决如何证明由Elementary Inference Systems（元素推理系统）定义的集合、关系或计算的性质，因为这些性质通常需要在不可计算的典型模型中进行检验。作者希望找到更实际的方法来分析如编程语言等复杂系统的语义属性。

Method: 论文将Smullyan的元素形式系统与Gentzen的推理规则记号结合，利用证明树方法描述谓词。通过赋予系统一阶理论（实际上是一组Horn子句），并用一阶句子表达对象属性，将这些属性的真值判断转为语义属性的可满足性问题。为克服典型模型不可计算，作者提出在任意模型中检验满足性的方案。

Result: 作者证明了可以通过在任意模型中检验满足性来（证伪）元素推理系统的语义属性，从而避免了典型模型不可计算的难题。这一结论能够应用于用推理系统描述的编程语言和重写系统属性分析。

Conclusion: 对于由元素推理系统定义的属性，只需在任意模型中进行可满足性检验即可推断属性的真假，无需依赖不可计算的典型模型。该方法进一步扩展到了编程语言及重写系统的语义分析。

Abstract: We consider sets/relations/computations defined by *Elementary Inference
Systems* I, which are obtained from Smullyan's *elementary formal systems*
using Gentzen's notation for inference rules, and proof trees for atoms
P(t_1,...,t_n), where predicate P represents the considered
set/relation/computation. A first-order theory Th(I), actually a set of
definite Horn clauses, is given to I. Properties of objects defined by I are
expressed as first-order sentences F, which are proved true or false by
*satisfaction* M |= F of F in a *canonical* model M of Th(I). For this reason,
we call F a *semantic property* of I. Since canonical models are, in general,
incomputable, we show how to (dis)prove semantic properties by satisfiability
in an *arbitrary* model A of Th(I). We apply these ideas to the analysis of
properties of programming languages and systems whose computations can be
described by means of an elementary inference system. In particular,
rewriting-based systems.

</details>


### [25] [Theta as a Horn Solver](https://arxiv.org/abs/2510.26430)
*Levente Bajczi,Milán Mondok,Vince Molnár*

Main category: cs.LO

TL;DR: 本文详细解析了验证框架Theta的算法和设计特色，尤其针对2025年CHC-COMP因配置问题造成的性能偏差，重新评估并展示了其真实能力，同时总结了工具的优势与局限性，为后续发展提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: Theta作为一个参加CHC-COMP竞赛的验证框架，其核心方法（将受约束的Horn子句转化为控制流自动机进行分析）在相关领域已保持不变，但其具体验证技术、设计权衡及局限性尚未深入探讨。本文旨在系统剖析Theta工具，弥补该方面的空白。

Method: 本文详细介绍了Theta所采用的算法，突出该工具相较其他CHC求解器的独特性。对CHC-COMP基准下Theta的优缺点进行了分析，并针对2025年竞赛中因配置问题导致性能下降的情况，重新在正确设置下运行并报告其性能。

Result: 通过修正配置并重新执行后，展现了Theta在CHC-COMP基准下的真实性能表现，弥补了先前因配置错误带来的结果偏差，同时提供了对工具实现层面和算法优劣的深入理解。

Conclusion: Theta在受约束Horn子句分析领域具备独特优势，尽管此前受配置问题影响，经过修正后展示出更具竞争力的实际验证能力。本文对其设计以及适用性进行了系统性评估，为后续工具开发和竞赛实践提供了参考。

Abstract: Theta is a verification framework that has participated in the CHC-COMP
competition since 2023. While its core approach -- based on transforming
constrained Horn clauses (CHCs) into control-flow automata (CFAs) for analysis
-- has remained mostly unchanged, Theta's verification techniques, design
trade-offs, and limitations have remained mostly unexplored in the context of
CHCs. This paper fills that gap: we provide a detailed description of the
algorithms employed by Theta, highlighting the unique features that distinguish
it from other CHC solvers. We also analyze the strengths and weaknesses of the
tool in the context of CHC-COMP benchmarks. Notably, in the 2025 edition of the
competition, Theta's performance was impacted by a configuration issue, leading
to suboptimal results. To provide a clearer picture of Theta's actual
capabilities, we re-execute the tool on the competition benchmarks under
corrected settings and report on the resulting performance.

</details>


### [26] [Bridge and Bound: A Logic-Based Framework for Abstracting (Preliminary Report)](https://arxiv.org/abs/2510.26654)
*Andrzej Szalas*

Main category: cs.LO

TL;DR: 论文提出了一种新的逻辑框架，不仅支持必要和充要条件，还能进行层级抽象，提升了抽象和模型简化的能力，同时分析了推理任务的复杂性。


<details>
  <summary>Details</summary>
Motivation: 抽象在科学知识和日常认知结构中至关重要。面对不完全或不完善的信息时，现有方法局限于必要条件，忽略了充要等丰富逻辑关系。该论文旨在通过更广泛的逻辑框架，提高抽象过程的表达能力及处理复杂系统的能力。

Method: 本论文提出了一种基于逻辑的新框架，用于建模抽象过程。该方法不仅涵盖了传统的必要条件，还引入了充要条件。同时，作者定义了近似抽象，并研究了其最紧和最精确的形式，还将方法扩展至分层抽象以实现复杂系统的层级简化。最后，论文探讨了相关推理任务的计算复杂性。

Result: 论文提出的逻辑框架有效处理了近似抽象及分层抽象，并讨论了其计算复杂性，展现出方法在模型简化和推理任务上的潜力。

Conclusion: 该论文为复杂系统的抽象建模提供了更强大的逻辑工具，通过引入近似与分层抽象，在简化模型和推理任务中展现了理论和实际应用价值。

Abstract: At its core, abstraction is the process of generalizing from specific
instances to broader concepts or models, with the primary objective of reducing
complexity while preserving properties essential to the intended purpose. It is
a fundamental, often implicit, principle that structures the understanding,
communication, and development of both scientific knowledge and everyday
beliefs. Studies on abstraction have evolved from its origins in Ancient Greek
philosophy through methodological approaches in psychological and philosophical
theories to computational frameworks.
  Formally, abstraction can be understood as the transformation of a source
representation into an abstract representation that discards certain details
while retaining desirable features. In real-world modeling and reasoning,
abstraction is crucial, particularly when managing imperfect or incomplete
information that calls for approximate representations. This paper introduces a
novel logic-based framework for modeling abstraction processes that goes beyond
the traditional entailment of necessary conditions to encompass sufficient
conditions as well. We define approximate abstractions, study their tightest
and exact forms, and extend the approach to layered abstractions, enabling
hierarchical simplification of complex systems and models. The computational
complexity of the related reasoning tasks is also discussed.
  For clarity, our framework is developed within classical logic, chosen for
its simplicity, expressiveness, and computational friendliness.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 本文针对LLMs近似数学推理能力进行基准评测和模型机制分析，发现模型不具备人类在街头数学场景下的认知吝啬特性，并揭示精确与近似运算依赖不同神经组件。开源了StreetMath数据集和分析工具。


<details>
  <summary>Details</summary>
Motivation: 大多数学术研究关注大型语言模型（LLMs）在精确算术操作上的数学推理能力，尤其是自回归架构下的表现，但在非自回归解码器模型中，针对快速且非正式的近似数学推理能力的研究较少。本文旨在填补这一研究空白。

Method: 提出了StreetMath基准测试，用真实世界近似场景评估LLMs进行近似推理的能力，涵盖多种模型架构，并采用机制解释技术深入分析模型内部计算过程。还进行了对精确与近似算术操作神经组件差异的实验。

Result: LLMs通常倾向求精确答案或调用外部工具，即使任务要求近似推理。在某些情况下，模型早期层能得到正确答案，但解决近似任务时消耗更多token。实验表明精确与近似算术依赖不同的神经组件。

Conclusion: 当前LLMs并不具备与人类在近似数学推理场景下相同的认知吝啬特性。论文开源了相关基准和实验结果。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [28] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文提出了一种基于模糊逻辑和句法依赖的评论情感强度细分方法，不仅考虑情感极性，还对强度进行分级，实现了实体的更精细颗粒度排序。


<details>
  <summary>Details</summary>
Motivation: 传统词典方法未考虑意见强度，导致无法细致反映用户情感（如非常强烈或非常弱烈的情感色彩）。因此，作者希望提出一种能对评论强度细分的方法，以提高实体的情感评分精度。

Method: 借助模糊逻辑算法将评论词（形容词、副词、名词、动词）按照强度层级（非常弱、弱、中等、强、非常强）分类，并利用句法依赖解析算法找出与产品相关的方面词之间的依赖关系，最后计算实体在某一方面的分数。

Result: 通过把评论中的意见词按照强度分级，并与产品相关的方面词关联，能够为各方面实体评分，进而实现基于评论强度和倾向性的排序，为用户查找产品时提供更精准的情感参考。

Conclusion: 本文提出的方法能够有效地对实体进行排序，考虑了评论和查询的倾向性以及强度，实现了更细粒度的情感分析。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [29] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 本文提出了韩语立场检测领域的大规模目标无关数据集LASTIST（共563,299标注句子），促进了低资源语言下的立场检测研究，并支持多任务模式。数据集已开放可访问。


<details>
  <summary>Details</summary>
Motivation: 现有立场检测研究主要集中在面向特定目标的检测任务，且大多数基准数据集为英文，这对开发低资源语言（如韩语）的模型造成了困难，尤其是在立场检测领域。

Method: 提出并构建了一个大规模、目标无关的韩语立场检测数据集LASTIST。该数据集从韩国政党新闻稿中收集，包含563,299个经过标注的韩语句子，并详细描述了数据收集、构建及使用先进深度学习和立场检测模型进行训练的过程。

Result: 提供了面向韩语的、大规模目标无关立场检测数据集，可用于多种立场检测任务，包括目标无关和历时演化的立场检测。并部署了数据集以供公开访问使用。

Conclusion: LASTIST数据集弥补了韩国语立场检测领域数据资源的不足，并支持多样化的立场检测研究任务，促进了该领域在低资源语言下的发展。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [30] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文提出了无延迟的低秩适配器zFLoRA，在多个平台和任务实验证明其几乎不增加推理延迟的同时，性能优于主流微调方法，解决了适配器推理效率瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）常和任务专用的适配器一起部署用于多种下游应用，但这些适配器参数虽然数量少（通常<1%），推理时计算量却大幅提升（最高达2.5倍基础模型），影响效率。

Method: 提出了一种新的零延迟融合低秩适配器zFLoRA，设计上减少或消除适配器对推理延迟的影响。

Result: 在1B、3B、7B参数规模LLM基准测试中，zFLoRA在18个不同任务（包括常识推理、数学推理和摘要-对话）上表现优异，优于传统低秩适配器LoRA和全量微调（FFT），并且在NPU与GPU平台实测几乎不增加延迟。

Conclusion: zFLoRA能在几乎零延迟下显著提升多领域任务性能，为LLM适配器效率问题提供有效解决方案。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [31] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文针对神经网络机理可解释性的回路发现问题，提出三项改进，分别为自助采样、一种新选择策略和整数线性规划，大大提升了发现回路的忠实性和性能。


<details>
  <summary>Details</summary>
Motivation: 机理可解释性中的一个主要难题是发现模型内部执行特定任务的回路，即确定模型哪些部分在完成特定任务。

Method: 在现有机理可解释性基准（MIB）基础上，提出三项关键改进：1）利用自助采样法识别归因分数一致的边；2）引入基于比值的边选择策略，优先选择得分强且为正的边，以平衡性能与忠实性；3）将标准的贪心选择替换为整数线性规划（ILP）方法。

Result: 所提出的方法在多个MIB任务和模型上获得更高忠实性，并优于之前的方法。

Conclusion: 改进后的方法能有效发现更真实的模型回路，提升了机理可解释性回路发现的效果。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [32] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: SymCode通过使用代码生成结合符号库，对大语言模型的数学推理能力实现大幅提升，使推理过程更透明、更易于验证。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在复杂数学推理方面表现不佳，主要由于自然语言生成过程缺乏可验证性，且易出现算术错误。现有的“思维链”等提示策略仍无法彻底解决这一问题。

Method: 提出SymCode神经符号框架，将数学问题求解转化为可验证的代码生成任务，并基于SymPy库实现。这样使得推理过程可以通过代码逻辑进行确定性验证。

Result: 在MATH-500和OlympiadBench等高难度基准测试上的表现优于以往方法，准确率提升最高达到13.6个百分点。分析表明，SymCode减少了隐晦的逻辑错误，将错误转变为易于理解和调试的程序性错误。

Conclusion: 通过引入基于确定性符号工具（SymPy）的代码生成方法，SymCode显著提升了大模型在数学等正式领域的推理准确率和可靠性，推动了AI推理的透明性和可验证性。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [33] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: LISTEN利用大语言模型，通过自然语言简化专家偏好获取，并用两种算法提升多目标决策效率和准确性，为复杂选择任务提供了智能化的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决专家在多目标优化问题中难以明确表达复杂隐性偏好，以及以自然语言方式引导决策、降低认知负担的挑战。

Method: 提出LISTEN框架，包括LISTEN-U（基于参数化效用函数迭代优化）和LISTEN-T（非参数化小批量锦标赛选择），在多样化任务上进行实证评估，并开发一致性新指标衡量偏好匹配程度。

Result: LISTEN-U在偏好能参数化时效果最佳，LISTEN-T在各种场景下表现更稳定，体现了用大模型和自然语言引导多目标选择的可行性。

Conclusion: LISTEN框架能有效地利用大语言模型（LLM）进行复杂多目标决策，并能通过自然语言简化专家偏好获取过程。LISTEN-U在参数化偏好明确时表现更好，而LISTEN-T则更具鲁棒性。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [34] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 该论文提出QCoder Benchmark，用于评估LLMs在量子编程代码生成上的能力，结合量子模拟器和人类竞赛代码，实验结果显示现有模型准确率较低，推理型模型表现突出，公开资源促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自动编程代码生成领域的应用越来越广泛，但在需要与硬件设备交互的领域（如量子编程）仍未被充分研究。人类编写的Python代码在量子计算机上执行时对代码生成提出了更高要求。为此，作者提出了新的评测框架以填补这一空白。

Method: 作者提出了QCoder Benchmark，该评测框架通过模拟硬件环境（量子模拟器）反馈专属领域的评估指标，如电路深度、执行时间、错误分类等。此外，框架还收集了来自真实编程竞赛的人类代码，支持量化和质化分析LLM与人类代码的对比。

Result: 实验显示，诸如GPT-4o等先进模型在该任务上准确率仅约18.97%，而基于推理的模型o3准确率可达78%，显著优于人类代码的平均成功率（39.98%）。

Conclusion: QCoder Benchmark揭示了量子编程代码生成任务的挑战性，也表明推理能力对LLM在此领域尤为重要。作者公开了数据集和评测API，推动相关研究进展。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [35] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: 长文本数据大多只需局部信息，现有训练方法效率低。LongFilter框架通过筛选真正需要长距离依赖的数据，显著提升长上下文大模型在多项长文本任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 长文本语言模型能够更好地实现推理、代码生成和文档摘要等高级能力，但大多数可用的长文本数据其实只有局部依赖，缺乏有意义的长距离依赖，导致训练效率低下。因此，迫切需要更精细的数据选择方法，提升对真实长距离依赖数据的利用率。

Method: 提出了LongFilter框架，通过比较模型在长上下文和短上下文下的预测差异，衡量长距离依赖的信息增益，从而筛选出那些确实需要长距离依赖的信息样本，适合用于长上下文预训练。

Result: 在扩展LLaMA-3-8B的上下文长度从8K到64K的实验中，LongFilter能够高效筛选高质量数据，在HELMET、LongBench和RULER等基准测试上有明显性能提升。

Conclusion: 通过LongFilter对训练数据进行精细筛选，能大幅提升长上下文模型对长距离依赖任务的能力，提高训练效率和模型表现。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [36] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 该研究发现，给LLM设置不同人格会导致模型在有害内容的判定上表现出意识形态偏见，尤其在政治相关任务中，模型更偏向自身人格立场，从而危及AI内容审核的中立性和公平性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在内容审核系统中的应用越来越广，然而如何保证其公平性和中立性成为一项关键挑战。研究动机在于探究LLMs在采用不同“人格”设定后，对于有害内容的判别是否会受到影响，并分析这种影响是否会引入意识形态偏见。

Method: 本研究比较了不同LLM架构、模型规模及内容形式（文本和视觉）下，模型采纳各类人格对有害内容判定的一致性和公平性影响。研究通过分析模型采用不同政治立场人格后的行为表现，以及模型与不同人格间的一致性，特别是在政治偏向性任务下进行了补充实验。

Result: 一般表面表现（如准确率等）显示人格对总体判别影响较小，但深入分析发现，不同意识形态人格判别有害内容的倾向性明显不同。尤其大模型在与自身意识形态相符的人格条件下判别更一致，而不同意识形态之间分歧加大。例如政治相关任务下，不同人格不仅在本方观点下表现更一致，还倾向于弱化对对立观点的有害性判定。

Conclusion: LLM采用人格设定时会引入细微但实际存在的意识形态偏见，可能导致内容判定结果强化特定党派视角，而表面上呈现中立。此现象对AI内容审核的公平性和中立性提出警示。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [37] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: CLEAR方法通过引入实体感知检索，有效提升了医疗问答的准确率与效率，尤其对长文本表现优异，远超传统方法，是临床NLP的有前景方案。


<details>
  <summary>Details</summary>
Motivation: 电子病历（EHR）通常以base64编码的形式存储在FHIR DocumentReference资源中，这为实现基于语义的问题回答造成了困难，且传统的向量数据库方法难以捕捉细致的临床关系。本文旨在解决这一问题，提升医疗文本语义问答的精度和效率。

Method: 提出并实现了CLEAR（Clinical Entity Augmented Retrieval）方法，采用实体感知检索技术以改善临床关系的捕捉效果，并开发了一个临床笔记问答评估平台，与大上下文推断和传统分块增强检索方法进行对比。实验涉及12份10,000至65,000 tokens的真实临床笔记。

Result: CLEAR方法在F1分数上达到0.90（传统嵌入检索为0.86），同时减少了70%以上的token消耗。在真实数据测试中，CLEAR获得了58.3%的胜率、平均语义相似度0.878，并比宽上下文处理节省了78%的tokens。文档长度越长优势越明显，超过65,000 tokens文档的胜率为75%。

Conclusion: 实体感知检索方法（CLEAR）在临床自然语言处理中的效率和准确率均优于传统方法，特别适合长篇医疗文档的问答场景。所提出的评估平台为临床语义问答系统提供了透明、可重复的基准测试方式。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [38] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 该论文系统综述了大型语言模型后训练中的数据高效方法，包括数据选择、质量提升、合成与蒸馏压缩等，并梳理未来发展问题与研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的后训练面临数据获取、人工标注成本高、数据规模递减等挑战。促使研究者探索如何在后训练过程中高效利用数据，提升模型的任务泛化和领域适应能力。

Method: 本文首次系统性梳理了提升LLM后训练数据效率的方法，提出了数据选择、数据质量提升、合成数据生成、数据蒸馏与压缩、自进化数据生态等方法类别，并对各类代表性方法进行了归纳与总结。

Result: 总结了现有数据高效后训练方式，标明各类别代表性方法，指出了现存挑战和未来研究方向。通过系统分类和归纳，为后续研究提供思路，促进数据效用最大化。

Conclusion: 系统性地分析并分类了数据高效LLM后训练的重要技术路线，指明了开放性问题和新研究方向，期望激励更多关于大规模模型数据利用的探索。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [39] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本研究系统评估了LLM参与创建语义标注数据集的三种方式。结果显示，半自动（人工+LLM辅助）方式既维持人工标注质量，也丰富了多样性，而纯自动化目前仍难以达成人类水平，仅在效率上有优势。


<details>
  <summary>Details</summary>
Motivation: LLM已经能够加速或替代部分语言资源和数据集的人工创建。但针对其在框架语义标注领域，缺乏系统、全面的性能和影响评估。作者旨在填补该领域的评价空白。

Method: 评估不同标注方式（人工、自动、半自动）在FrameNet式语义标注上的表现，比较标注时间、覆盖率和多样性。

Result: 半自动注方式下，标注的框架多样性提升，标注覆盖率与人工保持相近，而全自动方式虽标注更快，但质量与多样性均明显下降。

Conclusion: 半自动化（人工+ LLM）语义标注能够提升框架多样性且保持与人工标注相似的覆盖率；但全自动化标注虽然节省时间，在质量和多样性上显著逊色。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [40] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: RECAP系统通过循环纠错与越狱模块，能更好地引导LLM重现训练数据内容，在实证中大幅提升了抽取准确率。


<details>
  <summary>Details</summary>
Motivation: 在无法检查大语言模型训练数据的情况下，研究者希望找到有效的手段推断模型曾经见过什么内容，尤其关注模型对于训练数据内容的再现能力。

Method: 提出了RECAP管道，通过反馈驱动循环和辅助手段不断引导LLM生成更忠实于训练数据的内容。具体包括：初次抽取、辅助模型对比和纠错、最小纠正提示反馈，以及应对模型因对齐拒绝输出的“越狱”模块。

Result: 在EchoTrace基准（涵盖30本完整书籍）上验证了RECAP的有效性。以GPT-4.1为例，使用RECAP的平均ROUGE-L分数从单步方法的0.38提升到0.47，提升约24%。

Conclusion: RECAP能够更高效地从LLM输出中抽取记忆化训练数据，并显著优于传统单步抽取方法。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [41] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 本文实证分析了大模型多语言预训练的数据配比影响，发现只要语料平衡得当，包含多语种并不会拖累表现，也未显现所谓“多语言性诅咒”，尤其英语为枢轴语时多语种泛化得到提升。


<details>
  <summary>Details</summary>
Motivation: 当前关于多语言大模型预训练中多语言数据混合的影响存在争议，尤其关注语言覆盖与模型性能的潜在权衡问题。本文旨在系统性探究这些假设。

Method: 训练参数规模为1.1B和3B的大语言模型，预训练语料包含25至400种不同语言，细致分析不同语种数量和数据配比对模型表现的影响。

Result: 1）英语与多语言数据结合不会必然削弱各自的语内表现，前提是各语言在语料中有足够token数；2）英语作为“枢轴语”，对多语种泛化有积极作用，而选用同一语系中枢轴语并未带来特别提升；3）在该模型规模下，多语言数量增多并未出现显著性能下降（即‘多语言性诅咒’未被观测到）。

Conclusion: 合理平衡的多语言数据可在不损失模型性能的前提下增强大模型的多语种能力，哪怕在低资源语境下亦然。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [42] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 机器翻译在处理不同文化背景语言时，会因文化差异导致标签语义漂移，现代LLMs虽具文化知识但更易加剧漂移。文化因素尤为关键，如被忽视可能造成严重误解和冲突。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译（MT）常用于低资源语言的数据生成，但在翻译过程中，情感标签的保持受文化异同的影响尚少被深入探讨。论文假设文化差异会导致标签语义在翻译中发生偏移。

Method: 通过一系列实验证明，分别在文化敏感和中性领域进行标签转移分析，比较传统MT系统与现代大型语言模型（LLMs）的表现，研究文化因素对语义标签漂移的影响。

Result: （1）现代MT系统和LLMs在翻译过程中均会引起标签漂移，敏感领域尤显；（2）LLMs具备文化知识，其作用反而加剧标签漂移；（3）源语言与目标语言之间的文化相似性或差异性，是标签保持的关键决定因素。

Conclusion: 机器翻译若忽视文化因素，不仅降低标签保真度，还可能造成下游应用的误读和文化冲突。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [43] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文针对AWS Trainium AI加速器提出了专用矩阵乘法优化方案，采用内核融合和缓存技术，实验显示在LLM推理任务上性能远超现有官方实现，最高可达2.49倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着大模型（LLM）训练和推理任务计算需求不断增加，专用的AI加速器（如AWS的Trainium）成为提升性能和性价比的关键。但Trainium的异构架构、阵列结构和特殊数据布局要求使其高效利用变得复杂，亟需定制高性能内核以充分释放硬件潜力。

Method: 针对Trainium架构，本文设计了高性能矩阵乘法（matmul）内核，采用内核融合和创新缓存策略，减少数据在软件管理内存层级中的移动，最大化SRAM带宽使用，避免了耗时的矩阵转置操作。

Result: 在九个数据集和四个主流大模型上的评测表明，本文方法在matmul内核层面平均提升1.35倍性能（最高2.22倍），整体LLM推理端到端加速平均提升1.66倍（最高2.49倍），优于当前AWS官方实现。

Conclusion: 本文有效利用了Trainium架构特点，通过定制内核与优化内存访问策略，显著提升了LLM推理矩阵乘法与整体推理的性能，为AI加速器的深入定制化优化提供了参考。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [44] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 该论文提出AttnCache，通过相似注意力映射缓存和检索技术，有效加速LLM在prefill阶段的推理，能显著减少计算量并保持准确率，在CPU和GPU上均取得明显加速效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在prefill阶段（如分类、问答、推荐等任务）自注意力计算复杂度高，严重影响推理速度。观察发现不同语义句子的注意力分布很相似，因此有复用潜力。

Method: 提出AttnCache框架，通过建立注意力映射数据库、缓存及相似性搜索技术，在推理过程中检索并复用历史的注意力映射，减少自注意力计算量。

Result: 实验表明，AttnCache在CPU上端到端加速平均1.2倍，自注意力加速2倍，在GPU上端到端加速1.6倍，自注意力加速3倍，且准确率损失极小。

Conclusion: AttnCache能通过记忆和检索相似注意力映射，有效加速LLM的prefill阶段推理，且几乎不影响准确率。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [45] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 本研究提出监督强化学习（SRL）框架，通过细粒度步骤监督和奖励，极大提升了小型大语言模型在难题推理与智能代理任务上的能力，优于传统SFT和RLVR。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在需要多步推理的问题上表现不佳，现有的小型开源模型中，基于可验证奖励的强化学习（RLVR）偶遇正确解的概率极低，而有监督微调（SFT）又容易因逐令牌模仿导致过拟合。

Method: 提出了一种名为Supervised Reinforcement Learning（SRL）的新方法，把问题求解重构为一系列“动作”生成，模型在每个动作前生成内部推理自述。步进式比较模型动作与专家动作的相似度，作为更细致的奖励信号，实现更灵活与丰富的监督。

Result: SRL方法让小型模型能够学习之前SFT和RLVR无法解决的复杂问题。在训练时先用SRL预训练，再结合RLVR微调，模型性能达到最优。SRL还可以推广到面向推理的软件工程智能代理等新任务。

Conclusion: SRL是一种稳健且高泛化性的新框架，可以显著提升小型LLM在复杂推理和多步决策场景下的学习能力。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [46] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: 本文提出了PORTool，一种通过强化学习指导工具型大模型探索多条正确的工具调用路径的方法，实现了更高准确率与更精简的工具调用流程，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用型大模型仅模仿固定的工具调用路径，难以适应动态工具环境，缺乏对多种解决方案的探索能力，导致性能受限。

Method: 提出了基于强化学习的PORTool方法，通过生成多条工具调用轨迹并对轨迹中的每一步进行奖励分配，然后结合分叉相关优势和轨迹相关优势训练模型。

Result: 实验使用了17种工具，涉及时效性和时不变主题，在多个训练方案对比下，PORTool在最终准确率及工具调用步数上均有显著提升。

Conclusion: PORTool显著提升了工具使用型大模型在动态环境下的准确率和工具调用步骤效率。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [47] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 该工作发现现有跨语言对齐技术虽提升多语知识迁移，但会损害文化本地化（文化抹除）。提出全新评估框架和针对模型内部层次激活的推理方法，有效兼顾知识迁移与文化保留，在多语言场景下取得更优效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言对齐方法提升了语言间的知识迁移，但往往牺牲各语言文化本地化，造成“文化抹除”。因此亟需能够兼顾事实知识共享与文化特异性的新技术和评估标准。

Method: 构建了“transfer-localization plane”评估框架，定量分析模型知识迁移与文化抹除间的权衡。进一步通过对模型内部表征层次的研究，提出Surgical Steering推理方法，在特定层分别实现目标激活，引导模型在迁移与本地化间取得最优平衡。

Result: 实验结果表明：六种语言测试中，典型CLA方法确实实现了知识迁移，但同时导致文化本地化能力下降。采用Surgical Steering后，模型在知识迁移和文化本地化之间获得了更优平衡，能够有效缓解文化抹除问题。

Conclusion: 提出了一种能够在模型推理时平衡知识迁移与文化本地化的新方法，通过在不同层进行有针对性激活，引导模型同时保留事实转移和文化特异性，从而克服了现有CLA方法中二者互斥的局限。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [48] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 利用NLP技术分析大量放射报告发现，影像检查中的偶发甲状腺发现很常见。这些发现导致甲状腺癌（多数为低风险乳头状癌）的过度诊断，呼吁规范报告和优化随访流程。


<details>
  <summary>Details</summary>
Motivation: 偶发甲状腺发现（ITFs）在为非甲状腺疾病进行影像检查时越来越常见，但其流行率、特征和临床后果尚未明确定义。研究旨在明确这些现象和影响。

Method: 采用回顾性队列研究，利用变换器架构的自然语言处理（NLP）管道分析梅奥诊所多地点的大量放射报告，识别ITF并提取结节特征。纳入没甲状腺病史的成人，通过多种影像方式检测。主要分析ITF流行率及下游检查和诊断情况。

Result: 115,683名患者中，7.8%有ITF，其中92.9%为结节。ITF更易在女性、老年人、高BMI者及肿瘤科/内科开单时发现。与胸部CT比，颈部CT、PET及核医学扫描发现率更高。影像中结节特征记录不全，尺寸仅44%记录。ITF群体后续甲状腺结节诊断、穿刺、切除及癌症诊断概率明显增加。大多数癌症为乳头状，且ITF后发现的肿瘤更大。

Conclusion: 偶发甲状腺发现普遍存在，易导致小型、低风险甲状腺癌的过度诊断。需标准化影像报告及更谨慎的后续管理，以避免过度诊治。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [49] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 该论文提出通过‘一题多解’训练和RPD衡量推理路径差异，提升大语言模型多样性和推理表现，实验上表现优于传统训练模式。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理效果虽然因Test-Time Scaling (TTS)得到提升，但输出缺乏多样性，主要原因在于常见的‘一题一解’（1P1S）训练模式使模型仅接触单一标准答案，限制了其推理路径。

Method: 提出‘一题多解’（1PNS）训练范式，让模型学习多种合理推理路径；并设计了Reasoning Path Divergence（RPD）指标，以衡量和选择在多步链式思考过程中具有语义差异的解答方案。

Result: 用RPD筛选的多样化训练集对Qwen3-4B-Base模型进行微调，实验结果显示模型输出明显更具多样性，pass@k指标提升显著，其中pass@16平均提升2.80%，在AIME24上提升4.99%。

Conclusion: ‘一题多解’范式结合RPD指标能有效提升输出多样性和模型推理能力，增强了TTS方法的效果。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [50] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 该文利用大型语言模型，通过对现有说服技术数据集自动标注话语关系，发现六种话语关系在说服性文本中极为重要，有助于检测网络信息误导和理解有效沟通。


<details>
  <summary>Details</summary>
Motivation: 当前尚无同时标注说服技术和话语关系的数据集，因此研究人员需探索两者之间的关系并自动化标注过程，以加深对有效说服性沟通的理解。

Method: 利用大型语言模型（LLM）和提示工程，基于SemEval 2023第3题19类说服技术数据集，使用22类话语关系（PDTB 3.0）进行分类，构建集成模型，通过不同的投票策略生成5个带银标注的数据集，并进行统计分析。

Result: 分析发现Cause、Purpose、Contrast、Cause+Belief、Concession和Condition六类话语关系在Loaded Language、Exaggeration/Minimisation、Repetition等说服技术的表达中至关重要，有助于识别网络宣传和错误信息。

Conclusion: 六种话语关系在说服性文本中非常关键，这些发现有助于理解宣传、误导信息和有效沟通。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [51] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet通过混合专家机制实现多头注意力，有效提升了SSM/GRM模型的表现力，实验与实际部署均表现出优越性，是递归LLM架构的有力新选择。


<details>
  <summary>Details</summary>
Motivation: 现有的SSM/GRM架构通常只模拟单一注意力头，表现力有限，当前NLP生成系统需要更高效且表现更好的模型架构。

Method: 提出一种新的混合状态空间专家架构MossNet，通过混合专家机制在MLP块与SSM核实现多个“注意力头”，用以模拟线性的多头注意力。

Result: 广泛的语言建模和下游任务实验证明，MossNet在相同模型规模和数据量下性能优于主流的Transformer和SSM架构。更大规模的MossNet在处理数万亿tokens时也展现出良好的可扩展性与性能。此外，实际设备测评（Samsung Galaxy S24 Ultra和Nvidia A100 GPU）显示其运行速度和资源利用率均优于同类模型。

Conclusion: MossNet为高效且高性能的递归LLM架构提供了新方向，有望成为有竞争力的选择。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [52] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 提出SDM语言模型结构，通过特定训练方式显著减少模型拒答率，在高置信度输出区域提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在需要高置信度输出或准确性时，常常存在拒答率高或统计效率低的问题，作者希望通过结构改进和训练机制提升模型输出的置信度与效率。

Method: 采用SDM语言模型结构，将现有Decoder-Only Transformer模型通过有监督微调转换为SDM模型，在训练时利用最终层的SDM激活进行二分类，并采用对比编码与在线生成的难负样本，优化下一步词预测损失。

Result: SDM模型比强有监督基线在减少拒答（提高统计效率）方面表现更优。

Conclusion: 通过引入SDM激活层，能够提升模型在高置信度区域生成答案的能力，并且有效减少模型拒答次数。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [53] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 本文提出多维度评估框架RCScore，用于分析LLM对不同指令风格的敏感性，发现风格转变显著影响模型准确率，且风格自洽性与可靠性密切相关，为衡量和提升LLM实用性提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）评估方法通常只使用单一的指令模板，忽略了模型对不同指令风格的敏感性，而这在实际部署中非常重要。

Method: 提出了RCScore框架，通过系统地将基准测试问题转化为多种指令风格，定量分析指令表达方式对模型回应的影响，并引入Cross-Response Similarity（CRS）方法来衡量风格一致性。

Result: 在对10个LLM、4个推理基准的实验中，发现指令风格可导致准确率最多变化16.7个百分点。CRS与任务准确度存在强相关性，指令一致性可作为模型可靠性的有效指示器。此外，确定性解码产生更稳定的风格输出，模型规模与跨风格一致性呈正相关。

Conclusion: RCScore为评估LLM对指令多样化的鲁棒性提供了系统方法，强调了风格敏感性对模型实际表现和可靠性的影响。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [54] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 文章分析扩散语言模型控制力不足的核心原因——更新遗忘，提出按token时间步分配的TTA策略提升文本控制和流畅度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型尽管支持细致文本编辑，但实际控制能力薄弱，尤其是扩散更新过程中忘记先前编辑（更新遗忘），导致生成文本流畅性和连贯性下降。因此亟需新的方法提升编辑可控性。

Method: 提出Token Timestep Allocation（TTA）方法，通过对每个token设定不同的refinement时间步，关键token提前冻结，不确定token继续优化，可按固定策略或自适应任务信号运作。该方法仅在推理阶段应用，适用于多种DLM及各类监督。

Result: TTA方法在情感控制任务中准确率提升20%以上，困惑度减少一半且步数仅为五分之一；在文本“去毒”任务中最大毒性值、困惑度均显著降低，展现出优异的可控性和生成质量。

Conclusion: 通过逐步分配时间步进行软性语义排序，成功缓解了扩散语言模型中的更新遗忘问题，增强了文本生成的稳定性和可控性。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [55] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: 本文提出WIMHF方法用稀疏自编码器自动分析人类反馈数据，无需预先假设就能提取关键偏好特征。实验证明，该方法不仅揭示了偏好的多样性和潜在风险，还能用于安全改进和个性化模型优化。


<details>
  <summary>Details</summary>
Motivation: 人类反馈对语言模型的调整过程可能引发难以预测或不理想的效应，因为当前实践者并不了解反馈数据具体编码了哪些偏好。此前工作一般只能针对部分属性（如篇幅、奉承性）进行研究，且自动提取相关特征而无需预设假说依然具有挑战性。

Method: 提出了一种名为 WIMHF（What's In My Human Feedback?）的方法，用稀疏自编码器分析和解释反馈数据。该方法能够描述：1）数据集可衡量的偏好；2）标注者实际表达的偏好。通过自动化来提取人类可解释的特征，无需人工预定义。

Result: 在7个数据集实证，WIMHF筛选出了少量可解释特征，这些特征大幅度解释了黑箱模型在偏好预测上的性能。研究揭示了人类偏好的差异性和数据集上下文的影响，如 Reddit 用户偏好非正式与玩笑内容，而其他数据集则相反。WIMHF还发现了潜在的安全风险（如用户投票反对拒绝，进而可能支持有害内容）。利用学习到的特征进行数据再标记，可提升安全性37%且不损害模型的整体表现；此外，能定制化个体标注者的偏好权重，提高预测准确率。

Conclusion: WIMHF提供了以人为中心的分析工具，使实践者更好地理解和利用人类偏好数据，用于安全性提升和定制化优化。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [56] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文首创用于评测全局RAG任务的GlobalQA基准，指出现有方法表现有限，并提出GlobalRAG多工具框架，大幅提升全局信息汇聚与推理能力，F1分数提升超4倍。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估主要关注局部信息检索，无法满足实际中的全局信息聚合需求。为应对需要汇聚并分析整个文档集合获取语料级洞见的实际应用，迫切需要评估和提升全局RAG能力。

Method: 提出了一种多工具协作的GlobalRAG框架，包括块级检索、LLM驱动的智能过滤器和聚合模块，从而实现高准确率的符号级计算。采用Qwen2.5-14B模型进行验证，并与现有基线方法进行系统对比评测。

Result: 新提出的GlobalRAG框架在GlobalQA基准上的F1分数达6.63，远高于现有最优基线1.51，展现出明显优势。

Conclusion: 本文提出GlobalRAG框架，有效提升了全局RAG任务中的表现，相较于现有方法，F1分数显著提升。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [57] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 将语用学理论引入语言模型的提示语，能有效提升模型理解隐含语义的表现，并且即使只简单提及理论名称也能对大型模型有所帮助。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效提升语言模型在理解言外之意上的能力，特别关注将人类语用学理论应用于模型推理过程中的效果。

Method: 通过在提示中概述语用学理论（如Grice语用学和关联理论），引导模型进行逐步推理解释隐含意义，并与不使用理论提示的0-shot Chain-of-Thought方法进行对比实验。

Result: 在实验中，含语用学理论提示的方法相较于只进行中间推理的基线方法，模型在语用推理任务上表现提升最高达9.6%。而仅在提示中提及理论名称也能带来约1-3%的提升（大模型）。

Conclusion: 在语言模型推理过程中，提示语中引入语用学理论能够显著提升模型理解隐含意义的能力。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [58] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 预训练语言模型难以像人类一样辨认借词，且表现出偏向借词的倾向，对少数民族语言的保护和相关NLP工具开发有重要影响。


<details>
  <summary>Details</summary>
Motivation: 随着语言演变，借词成为多数语言的重要组成部分，尤其在双语社群中显著。研究动机是检验大型预训练语言模型是否具备和人类类似的识别借词能力，对少数民族语言及语言保护工具设计具有现实意义。

Method: 本文对多种预训练语言模型（包括大型语言模型）在10种不同语言上的借词识别能力进行评估。实验设计包括为模型提供明确的任务指令和上下文信息，使其从词汇中区分借词与本土词。

Result: 实验结果显示，无论给予明确指令或语境信息，模型在区分借词和本土词汇方面表现很差。这一现象在多种模型和语言中均被观察到。

Conclusion: 现代NLP系统在词汇处理上对借词存在偏向性，难以有效辨别本土词和借词。这为设计支持少数语言、促进语言保护的技术工具提出了挑战和发展方向。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [59] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文系统比较了五种知识蒸馏方法在视觉-语言模型压缩及多语言任务下的表现，发现部分方法可有效提升小模型的多语稳定性，但需警惕蒸馏策略对跨任务性能的敏感影响，仅靠准确率无法完全表征模型的实际效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在多语言环境下表现不均衡，尤其在模型压缩后更明显，而应用知识蒸馏于多语言模型领域尚未被充分研究，亟需探明其对多语言能力与模型性能稳定性的影响。

Method: 对五种不同的知识蒸馏方法进行了对照实证研究，分别在CLIP和SigLIP2两个视觉-语言模型上，考察跨语言表示一致性和下游任务在模型压缩下的性能稳定性，并在领域内检索与领域外视觉问答两个任务上进行了评估。

Result: 特定的蒸馏设置能够在缩小模型规模的同时，保留或增强多语种检索的稳定性；但也发现某些蒸馏方法难以维持跨任务性能的一致性，表明蒸馏设计存在精度之外的潜在权衡因素。

Conclusion: 部分知识蒸馏配置能够在减半模型规模的情况下，仍然保持甚至提升多语言检索的鲁棒性，但部分配置无法保持跨任务稳定性，揭示了仅依赖总精度无法暴露的设计敏感性权衡。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [60] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 本文发现正确信号在大语言模型内部激活上具有特异性，引入无监督的NAD方法，利用内部信号筛选候选答案，大幅节约计算资源并提升推理无标签解码性能，与多数票甚至超越传统方法，在数学、科学和编程任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在推理任务中多采用“采样-评估-集成”解码方式，虽然无需真实标签就能提升性能，但通常只依赖外部信号（如token概率、自评等），而这些信号在训练后容易失准。作者希望探索模型内部信号，并提升解码质量及效率。

Method: 作者分析了神经元激活内部行为，发现正确信号与错误信号在激活模式上的显著不同。基于此，提出了一种无监督的“神经元一致性解码（NAD）”方法，通过激活稀疏性和跨样本神经元一致性，选择最佳候选，无需外部文本输出。

Result: NAD方法能够在生成前32个token就预测正确性，并支持激进的提前终止。在数学、科学等有可验证答案的基准上，NAD与多数票一致；在开放式编程任务中，NAD优于常规平均方法。NAD能以几乎不损失生成质量的情况下，减少99%的token使用。

Conclusion: 内部神经元信号能够有效指导无标签集成解码，提升推理效率与质量。NAD具备高可靠性、可扩展性和计算效率，在无需真实标签或外部输出的场景下尤其有价值。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [61] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 虽然大型语言模型输出数字时常犯错，但它们内部对数字的表征高度一致且准确。通过分析这些表征和追踪模型层级，可提示未来优化语言模型的方法，减少数字相关错误。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现大型语言模型对数字输入有准确的表征，但在涉及数字信息输出时却经常出错。这一矛盾促进作者深入探究语言模型内部如何处理和操控数字，并解释这种冲突。

Method: 作者通过探测（probe）技术分析不同大型语言模型的隐藏层，并量化模型操控数字表征的准确性下限，进一步追踪输出错误的根源。

Result: 研究发现，尽管存在输出错误，不同语言模型对数字的隐藏层表征是高度一致、精确且可互换的，且表征跨输入上下文类型普遍有效。作者由此开发了通用探测器，可定位错误原因至具体层级。

Conclusion: 本研究为理解预训练语言模型如何处理数字提供了基础，并指出通过更精确的探测技术可以优化模型架构和减少数字相关的输出错误。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [62] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: 本论文系统评估了ChatGPT Atlas在网页互动中的表现：逻辑推理类任务表现优越，实时互动与操作类任务明显不足，揭示了其实际应用的能力边界。


<details>
  <summary>Details</summary>
Motivation: 针对目前ChatGPT Atlas在信息检索上的能力已有验证，但其在动态和交互性强的环境中的表现尚未充分研究，因此该论文希望通过真实的网页互动场景进一步评估Atlas的能力。

Method: 选择在线浏览器游戏（T-Rex Runner、Sudoku、Flappy Bird、Stein.world）作为实验场景，利用游戏得分作为量化指标，通过不同类型任务测试Atlas的实际表现。

Result: Atlas在需要逻辑推理（如Sudoku）上表现优异，解题速度远超真人基线，但在需要实时反应和精密操作的游戏（如Flappy Bird、T-Rex Runner）中表现不佳，常常难以通过初始障碍。

Conclusion: Atlas具备较强的分析和逻辑处理能力，但在动态、需要高实时性的网页环境下，仍存在显著的局限性。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [63] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE通过多跳推理和工具增强，打造适用于本地、隐私友好型教育反馈的高效小模型，在实际应用和用户体验上优于更多大模型，推动教育AI小型化和实用化。


<details>
  <summary>Details</summary>
Motivation: 在教育环境中，通过语言模型为学生提供个性化、互动性的反馈已成为趋势，但现有解决方案存在隐私、算力和教学有效性等三大挑战。因此，亟需开发能够本地运行并确保输出可靠的小型开源模型。

Method: 提出SCRIBE框架，采用多跳、工具增强推理，结合领域特定工具和自反推理流水线，实现反馈报告的有效回复。通过两阶段LoRA微调（在GPT-4o生成的合成数据上），将这些能力融入3B和8B规模模型。

Result: 在由人类对齐的GPT-Judge评测和108名学生参与的用户研究中，8B-SCRIBE模型在相关性、可行性等核心维度上表现达到或超越更大模型，且与GPT-4o、Llama-3.3 70B同水平的用户体验。

Conclusion: SCRIBE小型模型在低资源和重视隐私的教育场景中具备实用性，能够在保证质量的前提下满足实际需求。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [64] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER通过自动生成、系统化的课程内容与递进训练方案，让大模型在专业领域显著进步，同时不损失其通用能力，还促进了领域间的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在经济学、心理学等特定领域表现不佳，这些领域需要更深层次的理解和系统化学习。有必要探索如何让大模型在不丧失广泛能力的前提下,成为专业领域专家。

Method: 提出了ACER（自动化课程增强训练），先自动生成某学科的教科书式课程（大纲及基于Bloom分类法的问题-答案对），保证系统性与递进难度。基于该合成语料，采用交错课程训练方案进行持续预训练，强化模型对专业内容与认知层次的掌握。

Result: 在Llama 3.2（1B和3B）实验中，经济学等专业任务MMLU子集精度提升了5个百分点，所有目标领域宏平均增长3个百分点。ACER还缓解了灾难性遗忘，促进了领域间知识迁移，非目标领域也有0.7个百分点提升。同时，在ARC和GPQA等知识密集型任务上性能提高2个百分点，且在一般推理任务上性能稳定。

Conclusion: ACER方法可扩展、有效地提升LLM在专业领域的表现，缩小关键领域的性能差距，且全面增强模型能力。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [65] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 该论文提出了利用生成的合成谬误数据对大语言模型进行微调，显著提升了模型识别科学错误信息的能力，有效解决了真实标注数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 健康相关错误信息普遍且有害，尤其是当主张歪曲或误解科学发现时，识别十分困难，需要更有效的方法帮助识别。

Method: 提出了MisSynth管道，利用检索增强生成（RAG）技术生成合成谬误样本，并用这些样本对大语言模型（LLM）进行轻量微调。使用MISSCI数据集和框架进行评估。

Result: 微调后的模型对识别科学谬误的能力显著提升。例如，LLaMA 3.1 8B微调模型在MISSCI测试集的F1分数相比原始模型提高了超过35%。

Conclusion: 通过合成谬误数据来扩充有限的标注资源，可大幅提升大语言模型在科学错误信息识别任务中的零样本分类性能，即使计算资源有限也同样有效。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [66] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 本文提出通过语义会话关系和社区检测自动组队，无需知道模型细节。新方法发现性能优异的协同团队，效果媲美专家人工组队，为多智能体大模型团队自动化设计提供新方案。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型（LLM）团队有望超越单一模型的能力，但有效团队组建受限于模型内部特性不可见性，因此很难自动组建最佳团队。

Method: 提出一种以交互为中心的自动团队组建框架，无需事先了解模型内部结构、训练数据或任务性能。该方法通过构建“语言模型图”，以语义对话的连贯性映射模型间关系，并利用社区发现算法识别协同模型群。

Result: 实验表明，该方法能自动发现表现出潜在专长的功能性团队。基于特定主题引导的对话能组建出性能超越随机团队的协同团队，与基于专家知识手动选取的团队在准确率上相当。

Conclusion: 提出的自动团队组建方法无需先验知识，能有效发现具有协同潜力的模型组合，为多智能体LLM团队设计提供新思路。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [67] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本论文探索了在科学论文中利用PLM和LLM进行话语关系分类，发现引入话语结构相关的上下文普遍提升分类效果，并分析了具体哪些话语关系类别最能受益。这为未来AI生成科学论据时如何自动寻找支持证据提供了有益参考。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在科学工作流程中的广泛应用，作者关注如何利用话语层级信息来为AI生成的科学结论寻找支持证据。首要目标是研究科学写作中的话语结构推断任务。

Method: 作者对预训练语言模型（PLM）和大语言模型（LLM）在科学论文中进行话语关系分类（DRC）的初步研究，特别关注科学出版物这一领域。作者通过实验考察了话语结构定义的上下文在DRC任务中的作用，并分析哪些科学话语关系类型最受益于上下文。

Result: 发现上下文（由话语结构定义）对话语关系分类任务普遍有帮助。还分析了不同科学话语关系类型在引入上下文时的受益情况。

Conclusion: 通过引入话语层级结构的上下文，可以提升科学著作中的话语关系分类效果，为后续AI生成科学论述的证据寻找提供方法论基础。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [68] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: OmniEduBench是一个全面的中文教育大模型评测基准，涵盖知识与能力培养两大维度。测试结果显示现有模型在教育能力培养上与人类有显著差距，亟待改进。


<details>
  <summary>Details</summary>
Motivation: 当前大模型及其基准普遍只注重知识层面，缺乏对实际教育场景所需的能力培养评估，且学科和题型单一，尤其在中文领域更为突出。

Method: 构建了OmniEduBench中文教育基准，包括两大核心维度（知识、能力培养）和多学科、多类型题目，并在多个主流大模型上进行评测分析。

Result: OmniEduBench数据集共包含24,602对高质量问答，其中知识维度18,121对，培养维度6,481对，覆盖61个科目、11种常见题型。评测显示，大模型在知识维度表现较好，但在能力培养维度与人类差距大，最佳模型仍落后约30%。

Conclusion: 现有主流大模型在教育领域的表现，特别是在能力培养维度，与人类水平仍存在明显差距，有较大提升空间。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [69] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 提出了协同稀疏与低秩压缩（SSLC）方法，首次统一剪枝与低秩分解以压缩大型语言模型，在主流模型上实现了性能无损的50%压缩和显著加速，优于现有单独方法，极大促进LLM实际应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在语言理解和生成方面表现突出，但由于带宽和计算资源需求高，实际应用受到限制。单独使用剪枝和低秩分解各自有成效，但两者结合的应用还鲜有深入研究。

Method: 提出了一种协同稀疏与低秩压缩（SSLC）方法，将低秩分解和稀疏优化统一建模，通过迭代优化算法实现。低秩分解减少信息损失地压缩模型结构，稀疏优化则去除非关键权重以增强泛化能力。

Result: 在LLaMA和Qwen2.5等主流模型（7B-70B）上的实验显示，即使不进行额外训练，SSLC方法明显优于单独剪枝或低秩分解，达到最新压缩性能。具体包括Qwen2.5模型压缩率达50%，且性能无损，速度提升至少1.63倍。

Conclusion: SSLC方法能够高效协同提升模型压缩率和执行速度，不损失性能，为大型语言模型的实际部署提供了新的有效方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [70] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: 本文提出BNLF框架，通过贝叶斯网络融合多个LLM，实现情感分析。在金融文本中，准确率提高约6%，增强了模型的稳健性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在不同领域任务中表现优异，但存在可解释性差、微调成本高、需要复杂提词、结果不一致以及计算资源消耗大等问题。为了解决这些挑战，本文提出了一种新的融合框架。

Method: 提出了Bayesian network LLM fusion (BNLF) 框架，将FinBERT、RoBERTa和BERTweet三个LLM的情感预测，以概率形式作为贝叶斯网络中的节点，实现晚融合（late fusion）。

Result: 在三个经过人工标注且具有不同语言、语境特征的金融语料库上，BNLF在准确率上比各基线LLM提升约6%。

Conclusion: BNLF框架在数据集多样性下表现出较强的稳健性，并且通过概率融合提升了情感分类的准确性和可解释性。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [71] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 集成多个大型语言模型比单独使用一个模型更能一致和可靠地评估临床AI分诊工具的性能，特别是在生成回顾性黄金标准时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 此研究旨在探索使用多个大型语言模型（LLM）集成（ensemble）是否能比单一模型更可靠地评估基于像素的AI分诊工具在临床场景中的表现。

Method: 研究收集了来自14家医院的29,766例无增强CT头部检查，并用商业化的脑内出血（ICH）AI检测工具进行处理。随后，利用8个开源LLM模型和一个内部GPT-4o模型，通过统一的多轮提示，对放射报告是否存在ICH进行分析，并人工复核了1,726例。比较了各模型及集成模型的性能，并测试了三种理想的LLM集成方案来评估分诊工具。

Result: Llama3.3:70b和GPT-4o单模型的AUC最高（0.78）；Llama3.3:70b的F1、召回率、精度、特异性和MCC均表现突出。LLM集成（如Full-9、Top-3、Consensus）在MCC上优于单一GPT-4o，但其间差异无统计学意义（p > 0.05）。

Conclusion: 多模型集成（尤其是中大型开源LLM）比单一LLM在临床AI分诊工具的回顾性评估中表现更为一致和可靠，可更有效地获得客观“黄金标准”。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [72] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: CORE-KG框架在法律知识图谱构建中，通过指代消解与结构化提示，有效减少节点重复与噪声。消融实验证明两者缺一不可，对提升LLM法律抽取效果有重要意义。


<details>
  <summary>Details</summary>
Motivation: 人工智能自动构建法律领域知识图谱在分析人口走私网络中面临挑战，主要由于法律文件非结构化、词汇密集且指代模糊，导致知识图谱生成质量低下。现有方法如基于LLM，虽然优于模板法，但仍有重复节点和噪声信息严重的问题。作者提出新的CORE-KG框架，希望解决上述痛点。

Method: 提出CORE-KG框架，包含类型感知的指代模块与领域指导的结构化提示。论文通过系统性消融实验，分别去除核心组件，评估各部分对减少节点重复和噪声的作用。

Result: 去除指代消解模块后，节点重复增加28.32%、噪声节点增加4.32%；去除结构化提示后，节点重复增加4.34%、噪声节点大幅增加73.33%。两大组件均显著提升自动抽取法律知识图谱的有效性。

Conclusion: 本研究定量揭示核心组件对法律知识图谱自动构建的贡献，指导未来设计更鲁棒的LLM法律信息抽取方案。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [73] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文提出了新的希伯来语视觉化还原符号方法DIVRIT，无需复杂语言分析即可高效和准确地实现符号恢复，展现出视觉表示在语言处理任务中的广阔前景。


<details>
  <summary>Details</summary>
Motivation: 希伯来语在未经符号标注（unvocalized）时存在高度歧义，影响词语发音和文本意义的准确性。恢复希伯来语中的字母符号是解决这些问题的关键任务。尽管最近机器学习方法取得了进展，仍需提升其准确性和泛化能力。

Method: 提出了DIVRIT系统，将希伯来语元音符号恢复任务建模为零样本分类问题。该方法在词级别选择最合适的符号模式，并基于上下文动态生成候选集。系统采用了希伯来文视觉语言模型，把未加符号的文本作为图像处理，使符号信息能嵌入输入向量。

Result: 在全面评估下，DIVRIT能在不借助复杂语言分析的前提下有效实现希伯来语加符号。在“oracle”设置下，确保正确答案在候选集内时，系统准确率非常高。通过架构优化和训练方法改进，系统泛化能力显著提升。

Conclusion: 视觉表示方法在自动、精准实现希伯来语符号恢复方面具有巨大潜力。DIVRIT不仅准确率高，且无需复杂的语言学分析，在泛化能力上也有明显优势。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [74] [The Structure of Relation Decoding Linear Operators in Large Language Models](https://arxiv.org/abs/2510.26543)
*Miranda Anna Christ,Adrián Csiszárik,Gergely Becsó,Dániel Varga*

Main category: cs.CL

TL;DR: 本文发现transformer模型解码多种关系时，线性算子高度复用，只需少量参数即可提取属性—而非单一具体关系—信息。因此算子易压缩，但对语义不相关的新关系泛化性有限。


<details>
  <summary>Details</summary>
Motivation: 探究transformer语言模型中线性算子如何解码特定关系事实，并扩展此前研究对单一关系的观点，系统性分析多个关系下算子的结构组织。旨在理解这种机制背后的共性与压缩潜力。

Method: 将多个关系的解码算子用三阶张量网络进行压缩，并提出cross-evaluation协议，将每个线性解码算子应用于其他关系的主体，分析算子之间的语义重叠。

Result: 发现多个关系的解码算子可以通过简单的三阶张量网络高度压缩，且解码准确率损失很小。交叉评估显示这些线性算子提取的是复用、粗粒度属性而非具体关系，导致可压缩性强且只能泛化到语义相近的新关系。

Conclusion: transformer模型中的线性关系解码器本质上是基于属性（如country-of-X），而不是针对单一具体关系（如capital-of、food-of）。这解释了其压缩性强和泛化能力局限于语义相近关系。

Abstract: This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.

</details>


### [75] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 为解决深度搜索强化学习的奖励稀疏问题，作者提出InfoFlow框架，通过子问题分解、失败提示和双代理结构三管齐下，提升奖励密度和效率，在基准测试中让轻量LLM也能表现媲美高端模型。


<details>
  <summary>Details</summary>
Motivation: 深度搜索中的强化学习由于奖励稀疏，导致探索成本高、最终奖励稀少，对实际应用造成障碍。论文提出，要解决如何让每单位探索成本获得更多奖励的问题。

Method: 提出InfoFlow框架，从三方面解决问题：1) 子问题分解，将长任务拆为多阶段分配奖励，提供更密集的学习信号；2) 失败导向的提示，对卡住的探索轨迹注入纠正信息，提高成功率；3) 双代理精炼结构，一个精炼代理总览并压缩搜索轨迹，减少探索成本，提升整体奖励密度。

Result: 在多种智能搜索基准测试上，InfoFlow大幅超过现有强基线方法，使轻量LLM也能达到接近高级专有LLM的性能。

Conclusion: InfoFlow框架有效优化了奖励密度，提升了智能体在深度搜索场景中的强化学习表现，降低了探索成本，提高了轻量模型性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


### [76] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: 本文提出CAST，针对LLM推理加速问题，结合GPU和batch size动态优化解码树结构，在多任务和多模型实验证明显著优于现有方法，加速效果突出。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理延迟高，尤其因为自回归设计和模型体量大。现有speculative decoding方法虽有效，但往往忽视了GPU设备和batch size等系统变量对加速效果的影响。

Method: 提出了一种动态树解码方法CAST，能够结合GPU配置和batch size等系统变量，动态优化推理树结构，并通过实验进行评估。

Result: 在六种任务和六个不同LLM上测试，CAST获得最高达5.2倍的推理加速，相较主流方法普遍提升5%~20%。

Conclusion: 提出的CAST方法可以有效提升大语言模型的推理速度，且在多项任务和多种模型下表现优异，优于现有主流技术。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [77] [SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding](https://arxiv.org/abs/2510.26615)
*Yiqiao Jin,Rachneet Kaur,Zhen Zeng,Sumitra Ganesh,Srijan Kumar*

Main category: cs.CL

TL;DR: 本文提出了针对多页、多模态视觉文档（如幻灯片）的SlideAgent智能体框架，通过多层次推理显著提升了文档理解表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在处理多页视觉文档（如演示文稿、手册等）时，难以进行细粒度推理和跨页面理解。因此，提出新的方法以提升模型在复杂文档理解方面的能力。

Method: 提出SlideAgent框架，设计专门的智能体，将多页、多模态文档的推理任务分为全局、页面和元素三个层次，并构建结构化的、与查询无关的文档表示。推理过程按需激活专用智能体，融合各层次结果，生成连贯且有上下文的答案。

Result: 经大量实验验证，SlideAgent在文档理解任务上相较于专有模型提升7.9分、相较于开源模型提升9.8分。

Conclusion: SlideAgent有效提升了多页、多模态视觉文档的理解和推理能力，优于现有专有和开源模型。

Abstract: Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).

</details>


### [78] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: 本文系统比较了编码器-解码器（RedLLM）和仅解码器（DecLLM）大模型，发现改进的RedLLM具备强扩展性和推理效率，下游任务表现不输主流模型，建议对该结构重新关注和发掘。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLM）从编码器-解码器结构迅速转向现在主流的仅解码器结构，相关的对比分析特别是在可扩展性方面较为缺失，可能导致编码器-解码器模型的潜力被忽视。

Method: 作者复现并改进了编码器-解码器LLM（RedLLM），借鉴了仅解码器LLM（DecLLM）的最新优化方法，并用RedPajama V1进行预训练，使用FLAN进行指令微调，比较了不同规模下RedLLM和DecLLM的性能。

Result: RedLLM展现出良好的可扩展性和强劲的性能，推理效率更高。在预训练阶段，DecLLM在计算最优性上占优势；但RedLLM表现出与之相当的扩展性和上下文范围外插能力。在多项下游任务中，RedLLM结果可比甚至更优。

Conclusion: RedLLM作为编码器-解码器结构的大模型值得重新审视，其在推理效率和下游任务性能上表现突出，有潜力用于开发更高效且强大的大语言模型。

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [79] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: Evontree通过本体规则提取和微调，有效增强LLM在医疗领域的知识，无需大规模数据，实验显示准确率提升，适用于低资源情境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗等数据敏感领域的适应性受到高质量领域数据不足的影响，而领域专家的本体规则蕴含着重要的专业知识。

Method: 提出Evontree框架，通过少量高质量本体规则，从LLM中系统性地提取、验证和增强领域知识，而无需大量外部数据。方法包括从原始模型中抽取本体、利用两条本体规则检测一致性，并通过自蒸馏微调强化知识。

Result: 在医疗问答基准上的实验表明，Evontree在Llama3-8B-Instruct和Med42-v2模型上均优于原始模型和领先的有监督基线，准确率提升最高达3.7%。

Conclusion: Evontree无需大量数据即可高效增强LLM在数据敏感领域的知识，提升模型性能，实现低资源领域的鲁棒适应。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [80] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: Kimi Linear提出了创新的线性注意力结构，首次在多场景下超越全注意力机制，表现更强且效率更高，并实现了大幅硬件加速。相关代码和模型公开，可广泛应用及二次开发。


<details>
  <summary>Details</summary>
Motivation: 线性注意力机制近年来因其高效性而备受关注，但相比于全注意力机制，其表现常常受限，特别是在短/长上下文和强化学习扩展等多种任务场景下。本文旨在解决线性注意力在多任务中的性能瓶颈，实现能完全替代全注意力机制的高性能架构。

Method: 提出Kimi Delta Attention（KDA），作为一种扩展Gated DeltaNet的线性注意力模块，并使用细粒度门控机制提升有限RNN内存的利用效率；设计定制化chunkwise算法，采用特殊的Diagonal-Plus-Low-Rank(DPLR)过渡矩阵变体，在保证算法一致性的前提下，大幅提升硬件计算效率与减小计算量。此外，预训练了Kimi Linear模型，并采用KDA与多头潜在注意力(MLA)的层次混合结构进行大规模实验。

Result: Kimi Linear模型在短上下文、长上下文和强化学习等所有评测任务上，均显著超越了全MLA注意力模型，并在1M上下文长度下，将KV缓存开销降低至25%，解码吞吐提升高达6倍。

Conclusion: Kimi Linear架构能够作为全注意力机制的直接替代方案，具备更优的性能和效率，尤其适用于输入输出长度较长的任务场景。相关实现和模型参数已开源，助力后续研究与落地应用。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [81] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: AutoDeco通过模型自学温度与top-p参数，实现真正端到端可控的生成和基于指令的解码调节，优于现有手动调整方案。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM称为“端到端”模型其实依赖手动调节的、不可微分的解码流程，严重影响效率和自动化，因此需要让模型自我学习控制解码策略。

Method: 在标准Transformer基础上增加轻量化的头部，动态预测每步的temperature和top-p参数，使解码过程成为可学习的、token级的参数化过程。

Result: 在八个基准任务上，AutoDeco在性能上优于默认解码策略，同时表现与经过最优调整（oracle-tuned baseline）的静态方法相当；并涌现出对自然语言指令进行解码控制的新能力。

Conclusion: AutoDeco模型能够显著优于常规的解码策略，达到接近于oracle调整的基线水平，并实现根据自然语言指令实时调节解码参数的新能力。

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [82] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 本文揭示了大模型价值对齐的训练动态：SFT阶段确定了模型价值观，后续的偏好优化影响有限，但算法选择仍能左右最终结果。研究为数据和算法选择提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）日益重要，其不仅需展现通用知识，还需符合人类价值观。此前研究多评估训练完成后的模型，忽略了模型在训练过程中如何学习这些价值观。为此，本研究关注模型在人类价值对齐方面的训练动态。

Method: 分析后训练阶段中不同算法和数据集对模型价值观漂移的影响，测量其变化规模与时间点。实验对象包括Llama-3和Qwen-3等不同规模模型，并采用主流的SFT（监督微调）和偏好优化算法与数据集，以及合成偏好数据集以进行可控实验。

Result: SFT阶段通常决定了模型的主要价值观，后续偏好优化很少能重新调整这些价值观。即使偏好数据集一致，不同的优化算法也会导致不同的价值对齐结果。

Conclusion: 价值观的学习主要发生在SFT阶段，不同偏好优化算法对对齐效果有显著影响。这些发现有助于指导数据设计与算法选择，提升模型的人类价值对齐。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [83] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个新推出的高难度数学推理基准，可更有效评估大型语言模型的极限能力。26个模型测试结果显示，现有模型远未达到人类顶级水平，有极大提升空间。该基准将有助于推动LLM在数学推理领域的进步。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在数学推理基准测试中表现已经达到饱和，主要使用高中数学竞赛题目，难以进一步评估顶尖模型的推理能力。因此需要更难的、能更有效测试模型数学推理能力的新基准。

Method: 作者提出了AMO-Bench，一个包含50道人类精心设计的数学难题的新测试集。这些题目都由专家验证至少达到国际数学奥林匹克（IMO）难度标准，并且均为原创，避免模型因数据记忆泄漏而获得不公平优势。只需要最终答案，无需完整证明，便于自动化评估。

Result: 在AMO-Bench上对26款LLM进行测试，最优秀的模型准确率仅为52.4%，大多数模型低于40%。此外，更高算力测试显示性能有提升趋势。

Conclusion: 当前LLM在高级数学推理方面远未达到人类竞赛水平，AMO-Bench揭示了模型在此领域的巨大改进空间。AMO-Bench已公开，助力推动LLM数学推理能力的发展。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


### [84] [Gistify! Codebase-Level Understanding via Runtime Execution](https://arxiv.org/abs/2510.26790)
*Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia*

Main category: cs.CL

TL;DR: 论文提出了Gistify任务来精准评估编码大模型在大型代码库理解与重构上的能力，结果显示现有模型表现有限，揭示出该领域尚有大量提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着编码智能体在大型代码库中的部署日益增多，自动化设计具有挑战性的代码库级评测变得十分重要。该论文旨在提出一种新的评测任务，以更全面地测试当前主流编码大模型的实际能力。

Method: 提出了Gistify任务：要求编码大模型读取整个代码库并指定入口点后，生成一个精简、自包含的文件，其功能可以复现代码库入口点输出，同时仅包含执行该命令所需的关键组件。

Result: 实验表明，目前最先进的编码大模型在Gistify任务上难以稳定取得成功，特别是在需要长执行路径的任务上表现不佳。

Conclusion: 目前的主流代码生成大模型尚未具备完整、准确理解大规模代码库结构和执行流程的能力，在Gistify任务中面临重大挑战。

Abstract: As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [85] [On the number of non-degenerate canalizing Boolean functions](https://arxiv.org/abs/2510.26556)
*Claus Kadelka*

Main category: cs.DM

TL;DR: 本论文推导了精确统计具备指定本质变量及canalizing特性的布尔函数的公式，为之后生物网络中canalization现象的分析和功能分布提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 理解复杂系统（尤其是基因调控网络）中canalization（主导性控制）和degeneracy（冗余性变量）对系统稳态与动态的影响，并分析它们的组合学基础尚未明晰。

Method: 推导出递归公式，用于统计具备指定数量本质变量和特定canalizing特性的布尔函数，尤其关注所有变量都是本质且至少一个变量具备canalizing属性的非degenerate canalizing布尔函数数量。

Result: 得到具备指定数量本质变量和canalizing特性的布尔函数计数递归公式，特别是首次给出非degenerate canalizing布尔函数的精确数量。

Conclusion: 本研究完善了对canalizing及递归canalizing布尔函数的枚举方法，为分析随机布尔函数中canalization出现频率以及其在生物网络模型中的显著高频现象提供了理论基础。同时，揭示了canalization在系统稳健性和调控分工中的重要作用。

Abstract: Canalization is a key organizing principle in complex systems, particularly
in gene regulatory networks. It describes how certain input variables exert
dominant control over a function's output, thereby imposing hierarchical
structure and conferring robustness to perturbations. Degeneracy, in contrast,
captures redundancy among input variables and reflects the complete dominance
of some variables by others. Both properties influence the stability and
dynamics of discrete dynamical systems, yet their combinatorial underpinnings
remain incompletely understood. Here, we derive recursive formulas for counting
Boolean functions with prescribed numbers of essential variables and given
canalizing properties. In particular, we determine the number of non-degenerate
canalizing Boolean functions -- that is, functions for which all variables are
essential and at least one variable is canalizing. Our approach extends earlier
enumeration results on canalizing and nested canalizing functions. It provides
a rigorous foundation for quantifying how frequently canalization occurs among
random Boolean functions and for assessing its pronounced over-representation
in biological network models, where it contributes to both robustness and to
the emergence of distinct regulatory roles.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [86] [Unambiguous Acceptance of Thin Coalgebras](https://arxiv.org/abs/2510.26371)
*Anton Chernev,Corina Cîrstea,Helle Hvid Hansen,Clemens Kupke*

Main category: cs.FL

TL;DR: 本文将无歧义自动机构造推广到更广泛的结构（薄余代数），并通过coherent algebra建立语言识别与自动机理论的桥梁，扩展了自动机适用领域并提升了理论深度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于无歧义自动机具备确定性自动机的一些优良性质，且应用广泛，因此希望将经典的无歧义自动机构造从薄树（thin trees）推广到更一般的薄余代数（thin coalgebras）结构。

Method: 作者将无歧义自动机的经典构造从薄树一般化到分析函子下的薄余代数，并在余代数框架下进行形式化，增强了结构和构造的清晰性及参数化能力。同时，将自动机对薄余代语的接受与所谓的coherent algebra（先前为研究薄余代数引入的结构）下的语言识别联系起来。

Result: 作者不仅扩展了无歧义自动机构造的适用范围，还通过构造性地将自动机与coherent algebra的语言识别相联系，从而刻画了有限coherent algebra可识别语言的自动机理论特性。

Conclusion: 本研究将无歧义自动机的构造方法推广至更广泛的结构，并建立了与coherent algebra识别能力的联系，丰富了自动机理论及其在形式验证中的应用基础。

Abstract: Automata admitting at most one accepting run per structure, known as
unambiguous automata, find applications in verification of reactive systems as
they extend the class of deterministic automata whilst maintaining some of
their desirable properties. In this paper, we generalise a classical
construction of unambiguous automata from thin trees to thin coalgebras for
analytic functors. This achieves two goals: extending the existing construction
to a larger class of structures, and providing conceptual clarity and
parametricity to the construction by formalising it in the coalgebraic
framework. As part of the construction, we link automaton acceptance of
languages of thin coalgebras to language recognition via so-called coherent
algebras, which were previously introduced for studying thin coalgebras. This
link also allows us to establish an automata-theoretic characterisation of
languages recognised by finite coherent algebras.

</details>
