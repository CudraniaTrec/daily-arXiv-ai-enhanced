<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [HITrees: Higher-Order Interaction Trees](https://arxiv.org/abs/2510.14558)
*Amir Mohammad Fadaei Ayyam,Michael Sammler*

Main category: cs.PL

TL;DR: 本文提出了支持高阶效应的Higher-Order Interaction Trees（HITrees），用以更强表达复杂系统的组合语义，并已在Lean中实现，效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有的交互树语义虽然支持组合性，但不支持高阶效应（即能传递或返回单子计算的效应），而高阶效应对于建模并行组合、call/cc等复杂语义特性是必需的。

Method: 提出了Higher-Order Interaction Trees（HITrees），通过重新设计效应的定义使其能在无保护型类型论中表达具有高阶输入的效应的不动点，并用去函数化（defunctionalization）将高阶输出编码为一阶表示。并在Lean证明助理中实现了HITrees和配套效应库。

Result: HITrees被实现于Lean，并且包括支持并发、递归、call/cc等效应的综合库；提出了HITrees的两种解释（状态转移系统和单子程序），用以定义具有并行组合和call/cc的语言语义，展示了其表达能力。

Conclusion: HITrees首次在无保护型类型论中为交互树提供了对高阶效应的支持，极大拓展了交互树的表达力和实际应用场景。

Abstract: Recent years have witnessed the rise of compositional semantics as a
foundation for formal verification of complex systems. In particular,
interaction trees have emerged as a popular denotational semantics. Interaction
trees achieve compositionality by providing a reusable library of effects.
However, their notion of effects does not support higher-order effects, i.e.,
effects that take or return monadic computations. Such effects are essential to
model complex semantic features like parallel composition and call/cc.
  We introduce Higher-Order Interaction Trees (HITrees), the first variant of
interaction trees to support higher-order effects in a non-guarded type theory.
HITrees accomplish this through two key techniques: first, by designing the
notion of effects such that the fixpoints of effects with higher-order input
can be expressed as inductive types inside the type theory; and second, using
defunctionalization to encode higher-order outputs into a first-order
representation. We implement HITrees in the Lean proof assistant, accompanied
by a comprehensive library of effects including concurrency, recursion, and
call/cc. Furthermore, we provide two interpretations of HITrees, as state
transition systems and as monadic programs. To demonstrate the expressiveness
of HITrees, we apply them to define the semantics of a language with parallel
composition and call/cc.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca 实现了基于大语言模型和动态执行的命令自动规范挖掘，基本消除了人工规范编写，提升了相关系统效率。


<details>
  <summary>Details</summary>
Motivation: 目前许多先进系统在处理由不透明组件（如 Unix shell 命令）组成的程序时取得了性能、安全性和可靠性上的显著提升，但这些系统依赖部分规范，而规范的编写依然是手动、繁琐且易错的，严重影响了其实用性。

Method: 本文提出了 Caruca 系统，自动为不透明命令挖掘规范。Caruca 首先借助大语言模型，将命令的用户文档翻译成结构化的调用语法，再基于此探索命令调用及环境空间，通过实际执行并拦截系统调用与文件系统操作，自动提取并导出命令的关键属性（如可并行性、文件系统前后条件等）。

Result: Caruca 支持多种规范格式，可直接被现有系统利用。在 60 个 GNU Coreutils、POSIX 及第三方命令上的实验表明，除了一个极端案例，Caruca 能为全部命令自动生成正确规范，完全消除了人工规范编写，实现了零人工介入。目前还为一个先进静态分析工具提供了完整规范支持。

Conclusion: Caruca 有效自动化了命令规范挖掘，大大提高了规范获取效率，减少了人工成本，对依赖规范的系统具备极大实用价值。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [3] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 本文批判传统软件工程方法在智能体开发中的不适用，提出治理优先的新范式，并通过ArbiterOS架构改善了大模型驱动智能体的可用性和可信性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的智能体虽有强大能力，但在关键任务应用中表现脆弱、不可信，原因在于开发范式的根本不适配。

Method: 提出并设计了一种正式治理优先架构ArbiterOS，以实现智能体系统的可控、可预测和可信赖。

Result: 通过引入ArbiterOS治理架构，智能体在稳定性、可预测性和可信度方面显著提升。

Conclusion: 论文认为当前智能体开发的“工艺危机”源自用传统确定性软件工程思维来驾驭本质上概率性的LLMs，提出以治理优先的范式重塑智能体开发。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [4] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: 该论文提出了MT-Sec基准填补多轮编码场景下AI代码助手的正确性与安全性评测空白，揭示单轮转多轮任务时主流模型性能大幅下降，强调评估标准需紧贴实际开发流程。


<details>
  <summary>Details</summary>
Motivation: 目前AI编程助手多采用单轮任务来评测语言模型生成代码的正确性与安全性，但现实中的开发往往是多轮交互、迭代进行的，现有基准难以反映复杂实际场景。缺乏针对多轮环境下正确性和安全性联合评测的基准，有局限性。

Method: 提出了MT-Sec，这是首个系统性评估多轮编码场景中代码正确性与安全性的基准。通过合成数据流程，将已有的单轮任务转化为语义一致的多轮交互序列，并保留原始测试用例，实现了可以复用的复杂多轮任务评测。对32个模型及三种代理引导方法在该基准上进行了系统测试。

Result: 实验发现，从单轮到多轮场景，各类模型“正确且安全”输出比例下降20-27%，即使是最先进的模型也如此。在多轮代码diff生成（更贴近真实需求）上，表现更差，错误和不安全概率上升。代理引导能提升单轮表现，但在多轮任务下助益有限。

Conclusion: 需强调真实开发流程下的多轮编码基准，只有同时关注正确性与安全性，才能有效推动模型与工具的实用性提升。目前主流评测方案低估了多轮环境下的挑战，需改进。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [5] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn通过对大模型训练引入可访问性违例惩罚，显著提升了模型生成Web界面的无障碍合规性，生成效果好于传统方式。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在依据文本指令生成Web界面时会复制训练数据中的可访问性缺陷，导致生成界面对各类用户不友好。作者希望让LLM生成更加符合可访问性规范的Web UI，提升包容性。

Method: 提出了A11yn方法，基于对WCAG可访问性标准违例的惩罚性奖励函数训练代码生成LLM，并构建了6,800条UI生成指令数据集(UIReq-6.8K)和300条真实需求的评测基准(RealUIReq-300)。

Result: A11yn方法在实验证明下，显著优于现有主流基线模型，将不可访问率降低60%，且生成质量保持不变，验证了可通过训练使代码生成模型具备可访问性对齐能力。

Conclusion: A11yn方法可以显著提升大语言模型生成Web界面代码的可访问性，将不可访问率降低60%，且保持语义和视觉质量，是将可访问性系统性优化进LLMs的有效方式。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [6] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 本文系统性分析了Spectral Signature在代码模型后门检测中的效果，发现其默认设置常常不佳，并提出优化方案和新评估指标，无需重新训练即可更好检测后门。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）越来越多地被集成到软件开发流程中，针对它们的攻击威胁也在提升。后门攻击是一类重要威胁，它允许攻击者通过在训练数据中植入隐蔽触发器，操控模型输出。然而，检测此类后门依然是个难题，其中一种有前景的方案是通过光谱特征（Spectral Signature）分析来识别被污染的数据。此前研究在神经网络中初步验证了该方法的有效性，但最近有工作发现该方法在代码生成类模型中可能效果不佳。本文旨在系统性地评估光谱特征方法在代码模型后门检测中的适用性。

Method: 系统性地评估光谱特征防御方法在不同攻击场景和防御配置下的效果，分析其优缺点，并尝试优化配置，探寻关键参数的设置，并提出一种无需再次训练模型即可准确评估光谱特征防御性能的新代理指标。

Result: 结果发现，目前常用的Spectral Signature设置在代码模型后门检测场景中往往不是最优。通过调整设置参数，作者发现了一种新的代理指标，可以无需重新训练模型情况下，更准确地估计Spectral Signature防御的实际性能。

Conclusion: Spectral Signature在代码模型后门检测中存在局限性，默认设置常常不足以应对各种攻击场景。优化参数设置和采用新代理指标能提升检测效果，并为未来高效、实用的代码模型后门防御方法提供实证基础。

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [7] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: 本文提出BugStone系统，自动挖掘并定位程序中的重复漏洞模式（RPBs），在大规模软件（如Linux内核）中发现大量新漏洞，显示其极高的实际效果和精度。


<details>
  <summary>Details</summary>
Motivation: 在大型程序中，修复每一个重复出现的、源于同一根本原因的代码缺陷（RPBs）十分耗时且困难。传统缺陷报告流程容易漏掉程序其他部分的同类缺陷，同时还可能无意间扩大攻击面，使攻击者能够利用类似但尚未修复的缺陷。

Method: 本文提出了BugStone，一种结合LLVM与大语言模型（LLM）的程序分析系统。BugStone利用已修复的缺陷实例提取错误模式，并在程序全局自动搜索和定位具有同一模式的易受攻击代码段。系统通过一批已知的RPBs和相应修复，训练模型自动挖掘类似问题。研究团队还手动建立了安全漏洞数据集，并进行了详细注释与匹配。

Result: BugStone在Linux内核上，以135类已知RPB为起点，自动发现了超过22,000个潜在新问题。随后的人工分析确认了其中400个样本中的246个为真实缺陷。同时，数据集层面，模型取得了高达92.2%的精度和79.1%的成对准确率。

Conclusion: RPB广泛存在且严重威胁程序安全，通过基于样例学习的BugStone系统，可以有效发现并定位同类隐藏漏洞，显著提升大规模代码基础设施安全性。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [8] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: 本文提出了针对自然语言到Scenic代码生成的新数据集与评测框架NL2Scenic，并综合评测了主流商业与开源大模型，发现中型开源模型即具高性价比和实用价值。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的测试离不开场景模拟，但基于自然语言生成Scenic脚本（用于CARLA的DSL）受限于数据稀缺、可重现性差以及评测标准不一致。为了解决上述问题，需要一个公开数据集和统一评测框架。

Method: 本文提出了NL2Scenic数据集与框架，包含146对自然语言/Scenic用例，细分难度的30例测试集，一个示例检索器，和14种提示变体。评测了13个大语言模型，包括知名商业与开源模型，并采用文本与代码执行等多维度指标以及专家对比研究。提出新的EDIT-COMP指标提升评测可靠性。

Result: GPT-4o整体表现最佳，Qwen2.5Coder-14B在本地设备上能达到其88%的专家评分。检索增强式提示对小模型提升显著，模型规模扩大到中等程度收益递减。Qwen2.5Coder整体优于CodeLlama。EDIT-COMP指标提升数据集层面排序的一致性。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成提供了标准化、可复现的基线，显示中型开源模型在自动驾驶场景编程上具备实际、经济优势。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [9] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出基于大规模知识库的混合进化编译器pass自动调优框架，显著超越LLVM已有优化基线11.0%，高效提升程序性能。


<details>
  <summary>Details</summary>
Motivation: 编译器中优化passes顺序自动调整对于提升软件性能至关重要，但找到特定程序的最优passes序列是NP难问题。传统的通用优化flag（如-O3、-Oz）为“一刀切”策略，通常无法释放程序的全部性能潜力。

Method: 提出了一个混合、知识引导的进化框架。先通过大规模离线分析构建知识库（包括优化行为向量、聚类得出的Pass Groups、Pass协同图、典型Pass序列库），再结合定制的遗传算法与知识注入的操作符进行线上个性化搜索。

Result: 在七个公开数据集上，框架平均比已高度优化的LLVM opt -Oz基线进一步减少了11.0%的IR指令数。

Conclusion: 该知识引导的混合进化框架能发现个性化、高性能的优化pass序列，性能优于当前前沿方法。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [10] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本论文首次大规模分析线上编程平台TLE错误来源，发现成因多样，并提出高效自动修复工具Nettle和评测框架Nettle-Eval，大幅提高TLE修复成功率。


<details>
  <summary>Details</summary>
Motivation: 线上编程平台如Codeforces和LeetCode拥有数百万用户，他们面临的主要挑战之一是“时间限制超出”（TLE）错误。TLE错误信息缺乏诊断指引，平台和现有工具支持有限，导致用户屡试屡败后放弃提交。

Method: 大规模实证研究：手动分析1000个带有TLE错误的Codeforces提交，归类其根本原因并追踪用户修复尝试。此外，提出Nettle自动修复工具及配套评测框架Nettle-Eval，将LLM与编译器和测试用例反馈整合，实现针对TLE的自动修复。

Result: TLE错误不仅源自低效算法，还包括死循环、数据结构错误和低效I/O。Nettle工具在1000个真实案例上修复率达98.5%，远超LLM基线，所有修复均通过Nettle-Eval和平台检查。

Conclusion: TLE错误成因复杂，传统性能视角有局限。Nettle工具针对TLE自动修复高效可靠，配套评测框架验证其修复有效性，为线上编程平台用户解决TLE问题提供了创新方案。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [11] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: 本文提出基于路径敏感约束和大语言模型的自动程序修复方法PathFix，显著改善了补丁质量和修复能力，尤其在复杂结构下表现优越。


<details>
  <summary>Details</summary>
Motivation: 自动化修复程序（APR）能提高软件质量，但当前方法面临两个主要问题：生成过多备选修复补丁，以及容易对测试样例过拟合。主要原因在于很难精确刻画缺陷修复规范。作者为解决这些挑战展开研究。

Method: 提出PathFix方法，结合执行路径敏感约束与大语言模型。PathFix包含四步：1）追踪代码中的故障路径；2）根据控制流图分析和推导期望路径（理想修复后的正确输出路径）；3）基于期望路径及路径状态约束生成、评估修复补丁；4）验证补丁正确性。为提升修复效果和可扩展性，还引入了大语言模型。

Result: 实验证明PathFix在处理复杂程序结构（如循环与递归）时效果优于现有修复方法。

Conclusion: PathFix能有效减少无效补丁生成和过拟合现象，通过路径敏感分析和LLM的结合，提升了自动化程序修复的准确性与效率。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [12] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 论文针对软件开发协作主体多样化（含AI代理）及治理政策不清，提出了用于政策制定与执行的DSL，有助于开源项目实现自动化与高效治理。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发协作主体愈加多元化，尤其开源项目治理缺乏明确且具体的政策，面对人类与AI代理协作的新挑战，需要更灵活的政策表达和自动化治理手段。

Method: 设计并阐述了一种领域特定语言（DSL），用于表达和实施涉及多元主体的软件治理策略。

Result: 该DSL能够定义与强制执行复杂的治理政策，提高OSS等项目的协作效率和治理自动化水平。

Conclusion: 提出并介绍了一种专门用于协作治理的新型领域特定语言（DSL），为未来软件开发中多元主体（包括人和AI代理）提供更高效、更自动化的治理路径。

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [13] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个开源端到端需求测试数据集及自动化评测框架，提出新标注方法并发现在该领域任务难度依然较高。


<details>
  <summary>Details</summary>
Motivation: 端到端需求自动化测试任务尚缺高质量数据集与统一基准，且传统标注方式成本高昂、自动化方案效果不佳。

Method: 1. 构建E2EDev，包括用户需求、BDD测试案例及步骤实现、Behave自动测试流程。
2. 提出人类-多智能体协同标注框架优化数据标注。
3. 利用E2EDev评测多种E2ESD和大模型骨干表现。

Result: E2EDev是一个端到端需求任务的数据集和评测基线工具集，包含细粒度的用户需求说明、多套BDD测试场景和基于Behave的自动化测试流程。为优化标注质量和降低人工成本，作者提出了人类-多智能体协同标注框架（HITL-MAA）。通过对不同E2ESD框架和大模型骨干进行评测，发现当前方案难以高效解决该类任务，表明更优、更经济的E2ESD解决方案的迫切性。相关代码和基准已开源。

Conclusion: 当前E2ESD解决方案在E2EDev上的表现均不理想，需要更有效、高性价比的方法。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [14] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 该论文揭示软件测试教育与行业实际需求的脱节，尤其在AI、安全测试及软技能方面。指出需要优化培训与课程，以缩小知识差距。


<details>
  <summary>Details</summary>
Motivation: 软件开发环境不断变化，促使测试人员需要不断学习新工具和技能。该研究旨在确定行业对软件测试能力的需求、当前测试教育中的知识空白，并指出学术文献未解决的问题。

Method: 采用两次焦点小组会、跨行业专业人员访谈（涉及铁路、医疗、软件咨询等领域）以及小规模文献综述方法。研究工具由项目成员协作多次迭代开发，确保涵盖行业需求与教育空白。通过主题定性分析总结发现及观察。

Result: 研究发现了关于专业培训方法、行业培训挑战、培训质量评估方式、学术与行业间知识差距、未来教育趋势及企业知识转移方法等方面的见解。文献综述确认了在AI测试、安全测试和软技能等领域存在知识空白。

Conclusion: 软件测试领域存在显著的教育与实际需求之间的差距，特别是在AI测试、安全测试和软技能培训方面。需加强课程设置和职业培训以满足产业发展。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [15] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen框架利用对抗性强化学习动态地生成更有效的测试用例，不仅提升了代码漏洞检测能力，还为代码生成模型提供了更高质量的训练反馈，优于现有静态方法，极大提高LLM代码的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在代码生成方面表现出色，但生成的代码常常包含隐蔽的错误。测试用例的生成成为提升代码可靠性的关键瓶颈，而当前的测试用例生成方法均基于静态数据集，难以发现更复杂或新的错误。

Method: 提出ATGen框架，通过对抗性强化学习训练测试用例生成器。ATGen将测试用例生成器与对抗性代码生成器对抗，后者不断制造更难检测的错误，从而动态提升难度。测试用例生成器的优化目标是同时最大化“输出准确率”和“攻击成功率”。

Result: ATGen在广泛实验中明显优于现有的测试生成方法，并能更有效地筛选最优推理结果，作为更高质量的奖励信号促进代码生成模型训练。

Conclusion: ATGen打破了静态训练的难度上限，引入动态对抗和强化学习方法，大幅提升LLM代码生成的可靠性，开创了新的动态范式。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [16] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 本论文提出一种结构化方法，根据研究目标分阶段识别交通仿真需求，可显著提升仿真的逼真度与实验有效性，对汽车研发测试非常有帮助。


<details>
  <summary>Details</summary>
Motivation: 研究交通仿真时，如何确保模拟出的交通环境尽可能真实，是实验有效性和参与者投入度的重要挑战。当前缺乏系统性的仿真需求识别方法。

Method: 提出一种系统性方法，通过分阶段的子目标结构化识别仿真需求，细致考虑微观层面、智能体模型以及视觉表现的技术细节。该方法将研究目标和交通仿真设计紧密联系起来。

Result: 能够细致、系统地衍生各阶段对应的仿真需求，保障交通仿真的高逼真度与实验相关性。

Conclusion: 采用结构化、目标导向的方法识别交通仿真需求，有助于提升实验的真实性和参与度，为汽车研发与测试提供更强支撑。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [17] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 论文系统评估了现有LLM代理在网页漏洞自动复现任务中的能力。发现代理在简单场景下有效，但在复杂、多组件环境中表现不佳，且强依赖完善的输入信息。指出环境适应和自主问题解决是亟需突破的关键。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在软件工程和网络安全领域表现突出，尤其在代码生成、漏洞发现和自动化测试等方面。然而，将LLM应用于自动化网页漏洞复现仍面临诸多挑战，尤其是在将漏洞报告转换为可工作的漏洞利用代码时。该方向是网络安全自动化的重要环节，但缺乏系统深入的实证评测。因此，作者旨在全面评估并分析当前主流LLM代理在网页漏洞自动复现中的能力与不足。

Method: 作者首先对来自软件工程、网络安全和通用领域的20个LLM代理进行了系统评估，考察其在技术能力、环境适应性和用户体验等16个维度上的表现，并在3个代表性网页漏洞场景中进行测试。随后，作者精选表现最佳的3个代理（OpenHands、SWE-agent、CAI），并在涵盖7种漏洞类型和6种网页技术的80个真实世界CVE漏洞数据集上进行了深度评测。

Result: 结果显示，LLM代理在处理简单的库型漏洞时表现尚可，但对于服务型、涉及多组件环境的复杂漏洞则普遍失败。复杂的环境配置和认证机制导致代理虽能执行漏洞利用代码，却难以成功触发实际漏洞。此外，代理对输入的引导信息高度敏感，如果认证信息不完整，性能会下降超过33%。

Conclusion: 当前LLM代理与可靠自动漏洞复现的实际需求之间存在显著差距，突出表现为环境自适应能力和自主问题求解能力的不足。后续需在这两个方向进行突破，提升代理在复杂场景下的表现。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [18] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 本文提出一种基于代码内聚度、无监督检测供应链攻击代码注入的自动化方法，实验证明该方法在极端不平衡情境下有一定检测准确性，对提高源码安全具有实际意义。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击虽罕见但危害巨大，现有工具检测难度高，亟需自动化且有效的代码注入识别方法。

Method: 提出一种无监督方法，通过量化源码的内聚度变化来识别恶意代码注入，引入NPC指标，分析函数级别代码注入与自然更新间的内聚度差异，并在极度不平衡样本情况进行评估。

Result: 在369个C++开源仓库的54,707个函数实验，注入代码会降低代码内聚度且命名模式变得简短、描述性弱。NPC指标在极端不平衡测试集下检测的Precision@100可达36.41%（1:1000比例）和12.47%（1:10000比例）。

Conclusion: 自动化的代码内聚度测量，特别是基于命名预测的内聚度（NPC），对于识别供应链攻击有潜力，提高源码完整性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [19] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 学界以往关注ISA迁移的二进制翻译问题，但Google大规模x86到Arm案例显示，自动化重编译和AI应用成为新的迁移挑战核心。论文梳理了迁移过程中的任务分类，展示了自动化成果与未解难题，指明了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有学术界多关注于二进制翻译技术，未充分研究随着开源生态壮大带来的软件重编译和自动化迁移新挑战。云巨头开始采用多架构（Arm与x86），使大规模ISA迁移成为亟需解决的工程问题。

Method: 通过分析Google规模庞大的从x86到Arm的ISA迁移案例（涉及约4万个代码提交），归纳ISA迁移过程中的任务类型，并探讨了自动化和AI技术在各环节中的作用。

Result: Google成功实现了大量自动化迁移流程，AI在其中发挥了关键作用。论文提出的任务分类体系揭示了哪些迁移任务已经自动化，哪些仍充满挑战，并提出需要进一步研究的核心难题。

Conclusion: 现代指令集架构（ISA）迁移的主要挑战已经不再是二进制翻译，而是如何充分利用开源生态系统，实现软件的自动化重编译。AI技术在自动化迁移流程中扮演了重要角色，但仍存在尚未解决的任务和研究难题。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [20] [T-BAT semantics and its logics](https://arxiv.org/abs/2510.14361)
*Pawel Pawlowski*

Main category: cs.LO

TL;DR: 该论文提出T-BAT四值逻辑系统，用于表达非正式可证性，证明其完备性，并用Kripke语义给予模态分析与直观公理化，为数学实践中的可证性问题提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 数学实践中的证明有非正式与正式之分，现有逻辑系统难以表达非正式证明，因此需要新的逻辑系统来刻画这一概念。

Method: 提出了一种名为T-BAT的四值非确定性逻辑，设计语义以表达真假及“可证性”状态，并研究语义与诱发公理的交互；利用Kripke语义从模态角度分析这些公理及框架条件，同时提供了T-BAT的直观公理化。

Result: 证明了该语义下可定义所有逻辑的完备性，并用对象语言表达具体的真值转换；给出了相关公理的模态语义框架条件，还直观地刻画了T-BAT逻辑的公理体系。

Conclusion: T-BAT逻辑成功融合非正式可证性与形式逻辑语义，通过四值系统拓展了可证性逻辑表达，同时在理论上已证明完备性及相关语义框架。

Abstract: \textbf{T-BAT} logic is a formal system designed to express the notion of
informal provability. This type of provability is closely related to
mathematical practice and is quite often contrasted with formal provability,
understood as a formal derivation in an appropriate formal system.
\textbf{T-BAT} is a non-deterministic four-valued logic. The logical values in
\textbf{T-BAT} semantics convey not only the information whether a given
formula is true but also about its provability status.
  The primary aim of our paper is to study the proposed four-valued
non-deterministic semantics. We look into the intricacies of the interactions
between various weakenings and strengthenings of the semantics with axioms that
they induce. We prove the completeness of all the logics that are definable in
this semantics by transforming truth values into specific expressions
formulated within the object language of the semantics. Additionally, we
utilize Kripke semantics to examine these axioms from a modal perspective by
providing a frame condition that they induce. The secondary aim of this paper
is to provide an intuitive axiomatization of \textbf{T-BAT} logic.

</details>


### [21] [Optimization Modulo Integer Linear-Exponential Programs](https://arxiv.org/abs/2510.14550)
*S Hitarth,Alessio Mansutti,Guruprerana Shabadi*

Main category: cs.LO

TL;DR: 本文首次系统研究了带指数和模运算的整数线性规划的最优化问题，构建了可简洁表示最优解的计算模型，并给出了有效的判定与比较算法，将其归入复杂性理论中的新扩展类。


<details>
  <summary>Details</summary>
Motivation: 过去仅有关于带指数和模运算的整数线性规划可行性（判定有无解）为NP完全，而其最优化版的复杂性和可解性未明，该文旨在填补这一理论空白。

Method: 提出并分析了整数线性-指数直线程序（ILESLP）表示最优解的方法，以及在整数因数分解oracle辅助下的多项式时间验证和比较算法。

Result: 1）最优解若存在可由整数线性-指数直线程序简洁表示；2）存在oracle辅助下的多项式时间算法可验证解及比较目标函数值；3）将问题定义在新的复杂性类中。

Conclusion: 本文将整数线性-指数规划（IL-exponential programs, ILEP）的最优化问题放入了一个扩展的最优化类（介于NPO和FNP^NP之间），该扩展类不需通过二分搜索来确定最优解。

Abstract: This paper presents the first study of the complexity of the optimization
problem for integer linear-exponential programs which extend classical integer
linear programs with the exponential function $x \mapsto 2^x$ and the remainder
function ${(x,y) \mapsto (x \bmod 2^y)}$. The problem of deciding if such a
program has a solution was recently shown to be NP-complete in [Chistikov et
al., ICALP'24]. The optimization problem instead asks for a solution that
maximizes (or minimizes) a linear-exponential objective function, subject to
the constraints of an integer linear-exponential program. We establish the
following results:
  1. If an optimal solution exists, then one of them can be succinctly
represented as an integer linear-exponential straight-line program (ILESLP): an
arithmetic circuit whose gates always output an integer value (by construction)
and implement the operations of addition, exponentiation, and multiplication by
rational numbers.
  2. There is an algorithm that runs in polynomial time, given access to an
integer factoring oracle, which determines whether an ILESLP encodes a solution
to an integer linear-exponential program. This algorithm can also be used to
compare the values taken by the objective function on two given solutions.
  Building on these results, we place the optimization problem for integer
linear-exponential programs within an extension of the optimization class
$\text{NPO}$ that lies within $\text{FNP}^{\text{NP}}$. In essence, this
extension forgoes determining the optimal solution via binary search.

</details>


### [22] [Problems and Consequences of Bilateral Notions of (Meta-)Derivability](https://arxiv.org/abs/2510.14619)
*Sara Ayhan*

Main category: cs.LO

TL;DR: 本文探讨在证明-理论语义下，如何在自然演绎和序列演算中均衡表达证明与反驳。尤其发现序列演算在形式上存在障碍，并提出了可能解决方案。


<details>
  <summary>Details</summary>
Motivation: 推动对证明-理论语义的双边解读，既要表达联结词的可证性条件，也要表达其可反驳性条件，从而更全面反映逻辑系统的性质。

Method: 作者通过分析自然演绎和序列演算系统，比较了两者在表达双边语义（证明与反驳规则）时的方式，并详细探究了序列演算中对横线的对偶化难题。

Result: 指出自然演绎容易表达双边框架，而序列演算仅对序列符号的对偶化较直接，但对横线的对偶化导致深层的不平衡，提出了分析根源和调整方案。

Conclusion: 文章认为在序列演算中实现真正的双边推理（证明与反驳）存在概念上的障碍，具体表现在证明与反驳间的不平衡，并讨论了其根源和可行的解决方法。

Abstract: A bilateralist take on proof-theoretic semantics can be understood as
demanding of a proof system to display not only rules giving the connectives'
provability conditions but also their refutability conditions. On such a view,
then, a system with two derivability relations is obtained, which can be quite
naturally expressed in a proof system of natural deduction but which faces
obstacles in a sequent calculus representation. Since in a sequent calculus
there are two derivability relations inherent, one expressed by the sequent
sign and one by the horizontal lines holding between sequents, in a truly
bilateral calculus both need to be dualized. While dualizing the sequent sign
is rather straightforwardly corresponding to dualizing the horizontal lines in
natural deduction, dualizing the horizontal lines in sequent calculus, uncovers
problems that, as will be argued in this paper, shed light on deeper conceptual
issues concerning an imbalance between the notions of proof vs. refutation. The
roots of this problem will be further analyzed and possible solutions on how to
retain a bilaterally desired balance in our system are presented.

</details>


### [23] [Admissibility of Substitution Rule in Cyclic-Proof Systems](https://arxiv.org/abs/2510.14749)
*Kenji Saotome,Koji Nakazawa*

Main category: cs.LO

TL;DR: 本文提出循环证明系统中替换规则的可容许性新证明方法，实现理论简化并有助于相关证明系统的高效实现。


<details>
  <summary>Details</summary>
Motivation: 替换规则让理论分析变复杂且增加证明搜索的计算成本，因此若能证明替换规则的可容许性，将简化理论与实践应用。而以往方法不适用循环证明系统，存在可容许性未明的问题。

Method: 将循环证明展开为无穷形式，提升替换规则，并通过连接回边重构无须替换规则的循环证明，完成可容许性证明。

Result: 证明在包含cut规则的CLKID^omega循环证明系统中，替换规则是可容许的，并可在排除函数符号的情况下推广到另外一类系统如cut-free CLKID^omega与分离逻辑的循环证明系统。

Conclusion: 本文证明了在存在cut规则的情况下，替换规则在CLKID^omega循环证明系统中是可容许的。若限制替换不包含函数符号，该结果可推广到更广泛的系统。

Abstract: This paper investigates the admissibility of the substitution rule in
cyclic-proof systems. The substitution rule complicates theoretical case
analysis and increases computational cost in proof search since every sequent
can be a conclusion of an instance of the substitution rule; hence,
admissibility is desirable on both fronts. While admissibility is often shown
by local proof transformations in non-cyclic systems, such transformations may
disrupt cyclic structure and do not readily apply. Prior remarks suggested that
the substitution rule is likely nonadmissible in the cyclic-proof system
CLKID^omega for first-order logic with inductive predicates. In this paper, we
prove admissibility in CLKID^omega, assuming the presence of the cut rule. Our
approach unfolds a cyclic proof into an infinitary form, lifts the substitution
rules, and places back edges to construct a cyclic proof without the
substitution rule. If we restrict substitutions to exclude function symbols,
the result extends to a broader class of systems, including cut-free
CLKID^omega and cyclic-proof systems for the separation logic.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](https://arxiv.org/abs/2510.14972)
*Yinxi Li,Yuntian Deng,Pengyu Nie*

Main category: cs.CL

TL;DR: 代码大模型的分词器因对代码语法不敏感，会导致格式或命名的细微变化影响模型行为，TokDrift实验证实并分析了该问题，呼吁未来采用语法感知分词。


<details>
  <summary>Details</summary>
Motivation: 当前分词器大多基于统计而非语法，对代码的分词对齐不佳，导致相同语义代码因格式或命名差异而分词不同，进而影响模型输出一致性及可靠性，需量化其影响。

Method: 提出TokDrift框架，通过语义保持的重写规则，生成只在分词上不同的代码变体，分析不同格式差异对模型行为的影响，并做层级分析追踪问题来源。

Result: 不同代码格式即使语义完全一致，也会导致9个主流代码LLM（包括30B参数级别的大模型）行为差异显著。定位问题根源在于分词器无法正确反映语法边界，早期词嵌入环节即发生错配。

Conclusion: 当前代码大语言模型（LLM）的子词分词器由于缺乏语法感知，导致分词与代码语法边界不一致，从而妨碍了模型可靠理解和生成代码。

Abstract: Large language models (LLMs) for code rely on subword tokenizers, such as
byte-pair encoding (BPE), learned from mixed natural language text and
programming language code but driven by statistics rather than grammar. As a
result, semantically identical code snippets can be tokenized differently
depending on superficial factors such as whitespace or identifier naming. To
measure the impact of this misalignment, we introduce TokDrift, a framework
that applies semantic-preserving rewrite rules to create code variants
differing only in tokenization. Across nine code LLMs, including large ones
with over 30B parameters, even minor formatting changes can cause substantial
shifts in model behavior. Layer-wise analysis shows that the issue originates
in early embeddings, where subword segmentation fails to capture grammar token
boundaries. Our findings identify misaligned tokenization as a hidden obstacle
to reliable code understanding and generation, highlighting the need for
grammar-aware tokenization for future code LLMs.

</details>


### [25] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 本文提出结合GRPO与多语言对比奖励的新方法，大幅提升了跨语言Text-to-SQL任务的执行和语义准确率，充分展现了使用小规模训练样本也能取得优异甚至超越大模型零样本表现的能力。


<details>
  <summary>Details</summary>
Motivation: 目前Text-to-SQL方法在评估时主要关注可执行性，忽视了语义对齐问题，尤其是在多语言（跨语言）场景下的性能急剧下降。因此需要提升系统在多语言下的语义和执行一致性与准确性。

Method: 提出了一种结合Group Relative Policy Optimization（GRPO）和多语言对比奖励信号的新框架，在多语言数据集上针对LLaMA-3-3B模型进行微调，并用语义相似度奖励信号提升模型的语义和执行准确率，采用3000条强化学习示例验证有效性。

Result: 在七语言MultiSpider数据集上，LLaMA-3-3B模型结合新方法后，执行准确率提升至87.4%（比零样本提升26个百分点）、语义准确率提升至52.29%（提升32.86个百分点）；引入对比奖励后，平均语义准确率进一步提升至59.14%；小模型超越大模型的零样本结果，且3B模型几乎能与8B模型接近，所有提升仅需3000条训练数据。

Conclusion: 通过引入多语言对比奖励信号并结合GRPO，我们能有效提升Text-to-SQL系统在跨语言场景下的执行准确率和语义准确率，极大缩小不同模型规模间的差距，并显著提升非英语语言下的表现。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [26] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: 提出用大语言模型将XAI技术解释与临床知识融合，生成可操作性医学解释，为精神健康AI落地提供一体化解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前XAI在精神健康筛查领域与实际临床采纳存在鸿沟，主要体现在技术透明度与临床可用性的脱节，缺少能为医生和患者理解及应用的解释性成果。

Method: 构建系统体系结构，采用大语言模型（LLM）作为“翻译引擎”，结合循环式检索增强（RAG），将XAI工具的技术输出同临床指南相结合，生成临床易读的、基于证据的叙述，并系统性分析各组成部分及其演化路径。

Result: 该框架能有效将XAI技术输出转化为临床相关、易于理解与应用的解释，改善流程整合、偏见缓解和多方沟通问题，为可集成、可操作及可信赖的AI临床应用提供战略路线图。

Conclusion: 提出了一种新框架——生成式操作框架，通过大语言模型将XAI技术产出的技术解释转化为面向临床的可操作性解释，推动AI在精神健康筛查中的落地应用。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [27] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: STELA 利用词性 n-gram 不确定性动态水印嵌入，无需模型 logits 即可进行公开水印检测，实验证明其在多语言场景下优于现有方法，兼顾文本质量与检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的水印技术存在检测需模型 logits、影响文本质量等问题，普遍难以兼顾可检测性与文本自然性，且极大限制了水印公开验证的可行性。

Method: 提出 STELA 框架，基于词性（POS）n-gram 语言不确定性动态调节水印信号强度，并针对不同语种（分析型英语、孤立型汉语和黏着型韩语）进行实验验证。检测器不需要访问模型 logits，便于公开验证。

Result: STELA 方法在英、中、韩三种不同类型语言实验中，检测鲁棒性超越了之前的水印方案，同时最大程度保留了文本质量，并可公开验证。

Conclusion: STELA 模型能在保持文本质量的情况下，实现鲁棒且公开可验证的水印检测，无需依赖模型内部 logit 信息，优于现有方法。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [28] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于用户日常交互和模型不对称响应的新型偏好数据收集和过滤方法，通过用户行为建模与EM算法，有效提升了大模型对齐过程中用户偏好数据的质量。


<details>
  <summary>Details</summary>
Motivation: 专业标注的pairwise偏好数据虽高质但成本高，随着LLM大规模应用，用户日常交互作为天然数据源变得丰富，但这些偏好多有噪声或缺乏质量控制，急需对用户标注数据质量进行甄别和提升。

Method: 提出了通过不同模型/模型版本生成的不对称响应收集用户偏好数据，并基于用户行为模型，利用期望最大化（EM）算法估算用户的潜在标注质量因素，从而过滤低质量标注。

Result: 通过上述算法实现了对用户标注质量的有效甄别和过滤，在LLM对齐下游任务中显著提升了行为建模和数据筛选的效果。

Conclusion: 本文提出的新方法能有效评估和筛选用户偏好标注数据，提升LLM对齐任务的数据质量和最终效果。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [29] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了‘informed routing’机制，通过对token可恢复性的预估，实现了更高效的大模型推理和训练。无需全量微调即可取得优越或同等效果，并大幅节约了计算资源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在实际应用中的推理成本很高，限制了它们的部署。尽管存在通过动态分配token级别计算的高效推理方法，但主流方案依赖于贪心路由，只做‘执行或跳过’的决策，容易导致信息不可逆丢失、token选择不理想。

Method: 提出一种新范式‘informed routing’，在决策token是否执行时，不仅考虑token的重要性，还引入其‘可恢复性’（即，输出能否被近似）。为此设计了Lightweight Feature Forecaster（LFF）小型预测模块，能够在路由决策前预测单元的输出，使机制可以灵活采用‘执行或近似’策略，从而保持模型性能并显著降低计算量。

Result: 大量实验证明‘informed routing’在语言建模及推理任务中，在多个稀疏度水平下达成了最优效率-性能权衡。无需最后LoRA微调时，该方法依然能够追平甚至超过需要全量微调的现有强基线，并将训练时间减少50%以上。

Conclusion: ‘informed routing’为大模型推理提出了一种兼顾效率和准确性的新策略，能在显著降低计算与训练成本的同时，保持甚至提升模型性能。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [30] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 本文提出结合头重要性与注意力熵的新剪枝标准HIES，明显优于现有HIS方法，实现更高效的Transformer模型压缩且无准确性与稳定性损失。


<details>
  <summary>Details</summary>
Motivation: 现有的头剪枝方法尤其是基于梯度的HIS方法虽然流行，但仅衡量了梯度贡献，忽略了注意力模式的多样性，存在一定局限性。

Method: 提出一种新颖的头剪枝标准HIES，将Head Importance Scores（头重要性得分）与注意力熵进行整合，并在Transformer模型上进行实验评估。

Result: HIES剪枝相比单独HIS剪枝，模型质量最大提升15.2%，稳定性提升2.04倍，可以大幅度压缩模型同时保证模型准确性与稳定性。

Conclusion: 提出的HIES剪枝标准通过结合Head Importance Scores与注意力熵，有效提升了模型质量与稳定性，在模型压缩方面取得优异效果，并无明显精度损失。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [31] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: 该文提出了ConDABench——一个生成互动式数据分析基准与评测工具的框架，真实模拟用户需求和数据复杂性。实验发现，当前大模型仍难以完成需要多轮交互和深度协作的数据分析任务。


<details>
  <summary>Details</summary>
Motivation: 当前面向真实数据分析的LLM评测都不能反映任务的真实复杂性和用户互动需求，因此需要一个兼容互动和更贴近实际场景的新型基准。

Method: 提出了ConDABench框架，包括：多智能体协作生成基准、1420个由公共数据集分析文章抽取的问题、以及支持系统性评价外部工具的测评平台。通过这些构筑物，测试并分析了主流大模型的表现。

Result: 建立了ConDABench工具套件，成功生成了1420个互动式数据分析问题与测评框架。实验发现，最新LLM对持续交互、复杂任务的处理能力依然有限。

Conclusion: 通过ConDABench基准的实验表明，最新的大模型在简单任务上表现有所提升，但在需要长时、互动的复杂任务中仍表现不足。ConDABench为进一步开发能够持续协同工作的模型指明了方向。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [32] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文提出了基于生成结果一致性的新型黑盒置信度估计方法，无需访问语言模型内部，有效提升了生成型任务中不确定性的评估准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成输出时，并不总是知道自己何时“不知道”，即存在不确定性。对于可信AI系统来说，衡量和校准模型输出的不确定性可以提升可靠性，但很多现有方法需访问内部模型信息，使用受限、成本高。因此，需要一种无需依赖模型内部信息，且高效、通用的不确定性量化（UQ）方法。

Method: 论文提出了一种基于相似性聚合的高层黑盒不确定性量化（UQ）框架。该方法通过比较LLM生成输出与其它采样生成之间的一致性，作为输出正确性的置信度 proxy。同时，提出了从少量训练集中训练置信度估计模型的具体新技术。这些方法大多不依赖模型内部结构，便于实际落地。

Result: 通过在问答、总结、Text-to-SQL等多样化任务数据集上的实证研究，所提出的基于相似性的方法在置信度校准上优于传统基线方法。

Conclusion: 论文的方法在无需访问LLM内部信息的条件下，对复杂生成任务提供了更可靠的不确定性评价手段，有助于提升AI系统在实际环境中的可信度和适用性。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [33] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 提出文化敏感的仇恨言论检测框架，综合利用文化属性和标签传播，克服标签偏见与文化歧义，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法忽视了标签偏见和不同文化背景下对仇恨的理解差异，以及数据稀疏、文化纠缠和标签歧义等现实挑战。

Method: 构建个体化仇恨言论子空间，并通过文化属性组合建模缓解数据稀疏，利用标签传播应对文化纠缠和标签歧义，从而挖掘每种文化组合的特性并增强分类性能。

Result: 该方法在多项指标上比当前先进方法平均提升了1.05%。

Conclusion: 本文提出的文化敏感仇恨言论检测框架，有效提升了检测准确率，且能更好地应对标签偏见和文化差异带来的挑战。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [34] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 本文提出用LLMs自动化抽取产品本体，并在评测中优于原有BERT方法，为用LLMs做本体抽取铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 人工构建本体既费时又昂贵，特别是在电商等海量产品信息场景下，迫切需要自动化、高效的本体抽取技术。

Method: 提出了一种基于LLMs的自动化方法，从原始评论文本中抽取产品本体（部分-整体关系meronymies），并用LLM作为判别者对此方法与BERT基线进行效果比较。

Result: 所提出方法自动提取的本体在实验中效果优于BERT基线，通过LLM判别获得更好评价。

Conclusion: 利用大型语言模型（LLMs）可以实现产品本体的全自动抽取，且效果优于传统的BERT基线方法。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [35] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 提出了无需访问目标模型的高级知识投毒方法ADMIT，可以高效欺骗RAG事实核查系统，跨多模型和检索器有效，显示该类系统存在严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: RAG系统中的知识库可能被恶意注入虚假内容，使大模型输出攻击者希望的错误信息。虽然现有研究关注了该风险，但实际事实核查场景中，可靠证据占主导，使防护更具挑战性。本文欲揭示并深入探讨在有强证据参与下的投毒攻击威胁。

Method: 提出了ADMIT（Adversarial Multi-Injection Technique）方法，通过几-shot、有语义对齐的投毒策略，无需访问目标大模型、检索器或令牌级控制，即可制造事实翻转和误导性论据。该方法在多种检索器、多个大模型和多个领域基准任务中进行了大规模实验验证。

Result: ADMIT能够在极低的投毒率（0.93×10^-6）下，实现平均86%的攻击成功率，比现有方法提升了11.2%，且在有强力反证时仍保持高效攻击能力，具备强泛化和鲁棒性。

Conclusion: ADMIT方法能够在事实核查场景下有效进行知识投毒攻击，显著提升攻击成功率，并且跨检索器和大模型具有良好迁移性，揭示了RAG系统在现实应用中的重要安全隐患。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [36] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: 提出SerialBEHRT模型，通过EHR序列结构化预训练，提升患者表征效果，在抗生素敏感性预测任务上优于现有方法，强调序列化对医疗基础模型预训练至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基础模型难以同时处理表格式和事件型EHR数据，继承自然语言模型的顺序性假设，难以捕捉跨就诊的纵向依赖。因此提出更适合医疗EHR结构的模型以提升表现。

Method: 提出SerialBEHRT模型，基于SciBERT，并进行结构化电子健康记录序列额外预训练；通过丰富临床事件的时间与上下文关系来优化患者表征。模型在抗生素敏感性预测任务上进行效果评估，并与多种EHR表征方法进行基准对比。

Result: SerialBEHRT在抗生素敏感性预测任务中表现优于现有EHR表征方法，且结果更具一致性，证明临床事件序列化在模型预训练中的重要性。

Conclusion: SerialBEHRT通过结构化EHR序列的自适应预训练，在抗生素敏感性预测上表现优越，其对序列化和临床事件上下文的编码对医疗基础模型的研究至关重要。

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [37] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: 论文提出了DynaSpec动态候选词表方法，通过上下文自适应选取token簇，大幅加速LLM推理，并在多任务场景下优于传统固定词表方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理加速方法，多采用静态、高频词的固定候选词表，导致计算受限且难以覆盖低频或领域特定的词，且泛化性弱。因此需要一种更动态、更鲁棒的候选词表机制来提升推理速度与准确性。

Method: 引入上下文相关的动态候选词表机制。具体做法是利用轻量级的元分类器，将不同上下文路由到少量的token簇，最终联合这些簇得到drafter的候选词表。同时，元分类器与drafter的编码过程并行计算，提高整体效率。

Result: 在标准的speculative-decoding基准上，DynaSpec在平均单次验证接受长度上持续优于固定候选词表基线方案，且能够用更小的候选词表保持高接受率。

Conclusion: DynaSpec能够在不影响验收率的情况下，有效缩减候选词表，加速推理，并在多任务场景表现出更强的泛化能力。

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [38] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 本文针对LLM的组合多任务场景，提出在适配器基础上加投影层的新方法，有效实现摘要和翻译任务并提升效率，实测在安卓和云端表现优秀，适合高效及资源有限的实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前的参数高效微调方法（如LoRA）在处理复杂组合任务时效率有限，特别是需要同时进行摘要和翻译等任务。

Method: 在组合摘要和翻译适配器的基础上，新增一个可学习投影层，提升任务集成效果并减少计算开销。

Result: 实验证明该方法在安卓设备和云端均能实现高效、快速的组合任务处理，优于需要大量重训练或顺序执行的传统方法。

Conclusion: 提出的方法可以高效地在设备端和云端执行组合型多任务，如摘要和翻译，且速度快，适合资源受限的真实应用场景。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [39] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 该论文提出了一种高效推理期语言控制方法，通过PCA分析少量多语种数据，调整LLM隐空间方向，显著提升语言一致性且无损语义，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 多语种大模型往往会在生成任务中无意混合不同语言，影响下游任务的可控性与可靠性。因此需要轻量且通用的方法在推理时实现语言控制。

Method: 提出了隐空间语言引导：通过对多语种的平行翻译结果做主成分分析（PCA），找到与语言相关的隐空间方向，并在推理阶段沿这些方向调整token嵌入来影响语言属性。这种方法只需少量平行数据即可校准，且只在推理时采用，计算开销非常小。

Result: 用单一主成分即可达到95-99%的语言分类准确率，并且在多个语言对（Qwen2.5和Llama-3.2模型）最小化下一个词分布偏差，最高降低达42%。还发现语言特征主要集中在模型最后几层，且线性可分性极高。

Conclusion: 通过对LLM的隐空间进行语言引导，可以有效缓解模型在多语言任务中的无意混合（code-switching）现象，同时保持语义不变，实现高效的语言控制。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [40] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: 作者用信息密度分析LLMs推理过程，发现其正确答案的推理流信息密度波动剧烈，与人类沟通习惯截然不同，提示应重新思考模型解释性与推理设计。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的链式推理步骤不总是忠实或易于解释，在理解和改进其推理过程上尚存在挑战。人类交流常常维持稳定信息流，作者尝试用信息密度新视角分析机器推理过程。

Method: 受到心理语言学中的均匀信息密度假说启发，作者提出基于熵的信息流度量，分析LLMs推理过程中的信息流动。通过在三个数学基准上实证，比较模型正确推理与人类沟通的信息密度特征。

Result: 在三个高难数学任务基准上，LLMs正确推理的过程中信息密度分布明显不均，呈现大幅起伏，与人类交流形成鲜明对比。该发现对解释性和模型设计提出新方向。

Conclusion: 与人类交流保持稳定信息流不同，大型语言模型成功推理过程中的信息密度呈现不均匀波动，挑战了关于机器推理过程的既有假设。

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [41] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: 本文提出了EvoEdit编辑策略，有效缓解了顺序知识编辑中的灾难性干扰，实现更高效及稳定的大模型知识更新，在实际基准上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）需要不断更新以纠正过时或错误的知识，现有方法利用局部编辑但在多次顺序编辑时出现知识干扰问题。

Method: 提出了一种新型编辑策略EvoEdit，通过顺序零空间对齐方法，在每次新编辑时保持原有和已修改知识的表征，将干扰降到最小。

Result: 在真实世界知识连续编辑基准数据集上，EvoEdit实现了比现有先进编辑技术更优或相当的效果，并带来最多3.53倍的速度提升。

Conclusion: EvoEdit为动态信息更新环境下LLM的高效和稳定编辑提供了一种简单有效且具理论保证的方案，突显了原理性方法的重要性。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [42] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: 该研究提出新基准衡量大型语言模型对不同用户是否给出一致事实，通过大规模实验量化一致性差异，并公开代码工具。


<details>
  <summary>Details</summary>
Motivation: 检测大模型针对不同人口背景用户是否提供一致事实，推动模型客观性和公平性评估，避免模型因不同人群背景出现回答不一致现象。

Method: 构建 ConsistencyAI 基准，对 19 个 LLM 以 15 个主题、100 种不同人口背景进行重复提问，利用句子嵌入和余弦相似度计算跨人格事实一致性分数。

Result: 100 人格实验中一致性分数平均为 0.8656，Grok-3 得分最高，多数轻量模型一致性较差；不同话题一致性差异显著，社会敏感话题分歧较大。

Conclusion: 不同人口群体背景下，LLMs 在事实一致性方面表现存在差异，且差异受模型提供商和话题影响。Grok-3 一致性最高，轻量模型较低；部分社会热点话题一致性较差。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [43] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: 本文提出BenchPress系统，用于加速企业私有SQL日志的text-to-SQL基准数据集构建。通过LLM自动生成描述+人工校正，大幅提升了数据集制作效率与质量，对模型评估更可靠，系统已开源发布。


<details>
  <summary>Details</summary>
Motivation: 现有text-to-SQL基准主要针对公开数据，LLM在企业私有数据仓库上的效果较差。人工注释企业SQL日志以生成基准数据既耗时又昂贵，急需自动化自动辅助工具来提升效率和质量。

Method: BenchPress采用人类-大模型协作流程：首先利用RAG和LLM为SQL查询自动生成多种自然语言描述，随后由专家选择、排序或编辑这些描述以确保准确和领域相关性。

Result: 结合LLM建议和人工审核，BenchPress显著提高了注释效率与准确率，使定制领域的SQL基准能够快速建立，并增强了text-to-SQL模型评测的实用性和可靠性。

Conclusion: BenchPress系统结合人类专家和大模型辅助，大幅降低了高质量私有领域text-to-SQL基准数据集的制作成本与难度，提升了数据注释的准确性和评测的可靠性。

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [44] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 提出了结合语言规则的R2T训练框架，在无监督和小样本条件下表现显著优越，能高效处理OOV词并提升低资源下的NLP任务效果。


<details>
  <summary>Details</summary>
Motivation: 目前神经网络受限于仅依赖标注数据，难以用显式任务约束提升表现，尤其在资源匮乏的语言场景下，传统方法难以处理OOV词和小样本任务。该文旨在探索原则性学习（PrL），即用显式规则指导模型而非仅依赖标注数据。

Method: 提出Rule-to-Tag（R2T）框架，将多层语言规则直接整合入神经网络的训练目标，通过自适应损失函数增加正则项，引导模型以更合理的不确定性应对OOV词。采用无标签文本进行训练，并通过案例研究验证其效果。

Result: R2T-BiLSTM在Zarma词性标注任务上仅用无标签文本达到98.2%准确率，优于用300标注句微调的AfriBERTa；在命名实体识别任务上，仅用R2T预训练加50标注句微调即可超越300标注句的传统训练基线。

Conclusion: R2T框架在无监督和小样本条件下表现出色，能有效处理OOV词，并通过结合规则提升下游任务效果。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [45] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: 本文分析了大语言模型集成的常见失败原因——Token和模型层的不一致，提出CoRE方法利用一致性提升集成鲁棒性和性能，经广泛实验验证，有效且通用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型集成虽提升了质量，但对模型异质化带来的错误信号（如Token化差异、模型专长不同）缺乏鲁棒性，易导致集成失效。需要提升集成方法抵御错误和不一致的能力。

Method: 针对大模型集成失败的原因，提出结合Token级和模型级一致性的方法（CoRE）：Token级用低通滤波减弱不一致性高的Token；模型级通过提升高自信、输出一致的模型权重。实验在多种基准、模型组合和集成策略验证了方法有效性。

Result: CoRE方法在各类集成基准上均显著提升了集成的性能和鲁棒性，实验证明其适用性广泛，效果优越。

Conclusion: CoRE技术能显著提升大模型集成的鲁棒性和性能，无需改变现有集成方法即可无缝植入。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [46] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 本研究提出把通用LLM和轻量级RAG组件结合应用于医疗VQA任务，在无需训练或繁杂配置的情况下实现优异性能，为多模态临床NLP问题提供了简单有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 提升医学视觉问答（MedVQA）在创伤护理等实际临床应用中的自由文本与结构化响应能力，简化系统实现，并提升准确性和临床可用性。

Method: 将经过指令微调的通用大语言模型（LLM）与RAG框架结合，通过检索和融合来自领域内的文本与视觉示例，增强模型推理、结构遵循和生成质量。

Result: 在MEDIQA-WV 2025创伤护理VQA任务中，提出的系统在19支参赛队伍、51份提交中综合排名第三，平均分为41.37%，在多个评价指标（dBLEU、ROUGE、BERTScore和LLM评分）上表现优异。

Conclusion: 轻量级的检索增强生成（RAG）方法结合通用大语言模型，不需要额外训练或复杂排序，通过简单的索引和融合相关示例，在医学视觉问答任务中取得了有效性能，成为多模态临床NLP任务的简单且有效基线。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [47] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 本文提出ShishuLM，通过结构优化显著减少Transformer模型参数和内存需求，在中等上下文下能用MLP近似Transformer块，实验表明训练和推理性能均大幅提升，为高效SLM设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽表现优异，但存在大量结构冗余，导致巨大内存和算力消耗。提升小型语言模型在Agentic AI系统中的部署效率需求迫切。

Method: 结合AI可解释性和推理时层剪枝研究，提出对Transformer架构的优化，将部分Transformer模块用MLPs近似，从而减少参数和KV缓存占用，并对不同规模的SLM进行评估和分析。

Result: ShishuLM在训练和推理阶段，内存需求降低最多25%，延迟提升最高40%，尤其在中等上下文场景下效果突出。

Conclusion: ShishuLM模型在保持性能的情况下，显著降低了内存和计算负担，为更高效的小型语言模型架构设计提供了重要理论和实践依据。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [48] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 该研究提出大模型集成方法，检测AI辅导对话中学生实时情绪。发现学生情绪多变且正面情绪较多，负面能快速化解，中性时刻常成转机，AI有望精准把控干预时机以提升学习体验。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在教育应用的情绪动态研究不足，该工作旨在探讨AI辅导下学生情感变化，以推动生成式AI在教育中负责任融合。

Method: 提出了首个集合式大模型框架，用于在大规模对话中感知学生的情绪状态。利用Gemini、GPT-4o、Claude三种前沿大语言模型对16,986轮师生对话进行零样本情感标注（包括数值评分与文本标签），采用模型间加权融合和多数共识机制提取稳健的情感特征。

Result: 分析显示学生在与AI交流中体验到轻微积极和适中唤醒的情绪，问题解决过程中常伴有混淆与好奇，偶尔出现挫折但易于恢复，情绪瞬息即变，中性情绪易成为积极转折点，为AI辅导系统精准干预提供了契机。

Conclusion: 学生在与AI辅导系统互动时，情绪主要偏积极和适中唤醒，但出现混淆、好奇和挫败感，且这些情绪状态持续时间较短，积极情绪较为脆弱，负面情绪较快反弹回积极状态，中性时刻则常为情绪转折点，可供系统干预优化学习体验。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [49] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 本文提出了模板填充和动态片段分配方法用于扩散语言模型，在数学和代码任务上获得明显提升，尤其在多token生成场景下兼顾速度和质量，拓展了DLMs应用前景。


<details>
  <summary>Details</summary>
Motivation: 扩展扩散语言模型（DLMs）推理方法，克服其局限于自回归前缀提示的不足，提升在数学推理和代码生成等任务中的结构控制和灵活性。

Method: 提出了模板填充（Template Infilling, TI）方法：先生成目标响应的结构模板，后填充掩码片段；引入动态片段分配（Dynamic Segment Allocation, DSA），根据生成置信度自适应调整片段长度。

Result: 在数学推理和代码生成基准测试中，所提方法相较于基线模型取得了平均17.01个百分点的显著提升。同时，TI在多token生成场景下提升了生成速度且保持了质量。

Conclusion: TI和DSA有效提升了DLMs在结构控制和灵活性上的能力，显著优化了模型性能并加快了推理过程，展示了DLMs超越自回归语言模型的潜力。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [50] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: 本文分析了如何通过Common Voice平台为Quechua等资源匮乏语言建立社区驱动的语音数据集。以Puno Quechua为例，收集和验证了大量语音数据。结果显示，该平台有助于少数语言的语音技术发展，并提出了未来研究重点，包括技术与伦理问题。


<details>
  <summary>Details</summary>
Motivation: Quechua等资源匮乏语言在语音技术发展中受限，数据和资源的稀缺阻碍了其进步。本文旨在通过Common Voice开源平台促进社区参与语音数据集建设，以缓解资源不足问题。

Method: 分析Quechua语言（涵盖17种）在Common Voice平台的接入流程，并以Puno Quechua为案例，具体展示语料库收集，包括朗读和自发语音数据采集。

Result: Common Voice现已收录191.1小时的Quechua语音数据（86%已验证），其中Puno Quechua贡献12小时（77%已验证），展现了平台在少数语言语音数据收集方面的潜力。

Conclusion: 平台促进了资源匮乏语言的语音数据开放获取，有助于提升包容性语音技术发展并推动社区数字赋权。同时，提出了技术挑战、伦理和本土数据主权等后续研究议题。

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [51] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: 本文介绍了一套法语肿瘤学专家标注的高质量临床语料库FRACCO，对临床文本进行了实体识别和ICD-O规范化处理，全人工审核，填补了法语肿瘤自然语言处理领域标注语料缺乏的空白。


<details>
  <summary>Details</summary>
Motivation: 针对法语肿瘤学领域缺乏已标注数据这一局限，促进法语临床信息抽取和概念标准化工具的开发与研究。

Method: 以西班牙语CANTEMIST语料库为基础，翻译生成1301篇合成法语病例，并按照ICD-O标准对形态学、部位及组织分化等实体进行深度标注，采用自动化匹配和人工校验相结合，所有文本由专家人工二次校验。

Result: 构建出包含399个独特形态编码、272个独特部位编码及2043个独特复合表达的高质量数据集，总计约7.1万条ICD-O标准化标注，为法语命名实体识别和规范化研究奠定了基础。

Conclusion: 本文构建了FRACCO，一个专家标注的法语肿瘤学临床文本语料库，专用于实体识别和概念规范化，为法语医学自然语言处理研究提供了标准参考资源。

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [52] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出GateSkip机制，实现了token-wise层跳跃，有效降低计算资源消耗，性能稳定，精度损失小，且易于与其它优化方法结合。


<details>
  <summary>Details</summary>
Motivation: 当前Mixture-of-Depths及早退出等模型在层跳跃方面存在不稳定和需重训练等问题，因此需要一种平滑、可微分且稳健的机制，以在节省计算的同时保持模型性能。

Method: 提出了GateSkip机制，通过sigmoid-linear门控方式，对每个Attention/MLP分支进行token-wise的层跳跃控制，在推理时依据门值对token排序并按预算跳过重要性低的token。该方法可在预训练模型上稳定微调。

Result: 在长文本推理任务中节省最多15%的计算量仍保留90%以上的基线精度；在指令微调模型中，在完整计算下有精度提升，在约50%计算量下仍能保持与基线相当的质量；门控学习还揭示了Transformer中的信息流等现象。

Conclusion: GateSkip方法能有效提升decoder-only语言模型的推理效率，在节省计算量的同时仍保持高精度，并可与量化、剪枝等其他技术结合。

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [53] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: 提出只用语言提示的多臂老虎机测试，发现部分LLM（如Qwen3-4B）能通过语言表现出强概率推理和决策能力，超越传统算法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM逐渐展现出推理能力，其仅通过自然语言进行不确定性条件下的连续决策能力尚不清楚。希望探索LLM在无数值信息、仅靠文本提示情况下的决策和概率推理能力。

Method: 设计纯文本反馈的多臂老虎机测试，LLM只能获得语言提示如“你获得一个代币”，不提供数字化信息。评估四个开源LLM，并与Thompson Sampling、Epsilon Greedy、UCB和随机选择等标准算法对比其性能。

Result: Qwen3-4B模型的最佳臂选择率达到89.2%，高于其他大模型和所有传统算法。其他LLM整体表现不如传统基线方法。该实验验证了在非数字化的自然语境下也可涌现概率推理能力。

Conclusion: 大多数开源大语言模型在基于文本反馈的多臂老虎机决策任务中表现不及传统算法，但Qwen3-4B模型显著优于其他模型和传统算法，成功选中最佳臂的比例高达89.2%。表明语言模型有可能仅通过语言实现概率性推理和决策。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [54] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 本工作提出动态分配计算步数的损失函数，通过使语言模型能自主请求更多推理步骤，显著减少训练数据需求，并提升对复杂语境和令牌的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对所有输入令牌分配固定的计算步数，无法根据令牌复杂性动态调整计算资源。这导致计算资源浪费或准确率下降。为提升模型效率与表现，需要更智能的计算步数分配机制。

Method: 提出Catch Your Breath(CYB)损失函数类，允许模型通过<don't know>输出请求更多计算步骤，<pause>令牌则为其分配额外计算。包括三种方法：CYB-AP（随时可预测，准确率随时间折扣）、CYB-VA（变分方法，优化预测精度与停止时间分布）、CYB-DP（依据计算预算施加惩罚）。通过微调实验比较不同损失效果。

Result: CYB模型在达到与基线模型相同性能时，所需训练数据仅为其三分之一，比结合暂停和交叉熵损失的模型少一半数据。CYB模型能针对复杂或不确定输入智能请求更多计算步骤，并在不同语境下自适应调整处理时间。

Conclusion: Catch Your Breath方法能显著提升语言模型的数据使用效率与预测表现，使模型自适应分配计算资源，处理复杂或不确定输入效果更佳。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [55] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: PAGE框架通过简单的辅助模块丰富生成模型输入，提升文本生成质量与可控性，无需复杂结构，易于适配，在软件需求生成等领域初步验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言生成模型在特定任务或需求下表现不佳，若要适配需大量额外数据，现有方法通常依赖复杂模型，缺乏灵活性。

Method: 提出PAGE框架，使用轻量级辅助模块（如分类器或抽取器），在生成之前对输入做推理，将其输出与原输入结合形成更丰富的生成提示，提升文本生成质量和可控性。

Result: 在需求工程领域的概念验证实验显示，PAGE借助分类器模块，有效提升了软件需求生成的质量。

Conclusion: PAGE框架通过辅助模块提升了自然语言生成模型在特殊任务中的表现，无需使用复杂的辅助生成模型，结构简洁易于迁移。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [56] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文强调用开放式文本来发挥大语言模型用于社会模拟时的价值，可提高社会模拟的真实度和方法学效果，建议开发支持开放表达的新实践与评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍采用选择题或简答题这种封闭式设计，便于评分和对比，但忽略了大语言模型本质上的生成能力和开放表达特点。作者希望引入开放式文本，提高社会模拟的真实性和效能。

Method: 立场论文，结合调查方法学和自然语言处理领域的最新成果，理论分析开放式文本优势，并讨论其对社会模拟设计的影响。

Result: 开放设计不仅提升了测量和实验设计的表现，还能发掘意外观点、减少研究者偏见、增强表达个性，以及便于预测试和增加方法学效用。作者呼吁建立新的评估框架，充分利用LLM的生成性为社会科学和自然语言处理带来协同进展。

Conclusion: 开放式文本比封闭式选择更适合用于大语言模型进行社会模拟；只有给予模型自由表达的空间，才能更真实反映社会观点和推理过程。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [57] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 现有大语言模型在无结构文本分类领域表现一般，类别膨胀和幻觉问题突出。集成多模型专家协作可大幅提升准确率并消除幻觉，有望成为突破人类水平的有效路径。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在无结构文本分类任务上的表现有限，尤其是需要依据复杂层级分类体系（如IAB 2.2），而单一扩展规模或架构无法根本提升准确率，因此亟需新方法提升模型在人类标注任务上的表现。

Method: 作者对十种最先进的大语言模型进行了统一的对比实验，所有模型在相同的8,660个人类标注样本和一致的zero-shot提示下运行，用四个传统指标（准确率、精确率、召回率、F1分数），以及三个LLM特定指标（幻觉比率、膨胀比率、分类成本）进行评价。并进一步引入一种集成范式：多个模型像独立专家协同分类。

Result: 主流LLM的经典指标表现平庸（平均准确率仅34%），且模型易出现类别膨胀和吠幻觉，最佳性价比模型为Gemini 1.5/2.0 Flash与GPT 20B/120B，GPT 120B的幻觉比率最低。集成方法显著改善所有指标，尤其能完全消除幻觉问题。

Conclusion: LLM的扩容和架构升级不足以显著提高无结构文本的分类准确率，协调多模型协作（集成方法）比单点规模提升更有望与人类专家表现看齐或超越。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [58] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: 本文填补了LLM生成数学证明自动评价的细粒度评分空白，提出评价与开发流程，发布专家标注数据集，用于开发并验证ProofGrader，其准确度显著优于传统方法，有望促进数学证明的自动生成应用。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在数学推理任务中，特别是自然语言数学证明的自动生成评价，缺乏可靠细粒度的评分标准和工具。填补这一评估环节的空白，有助于推进数学证明自动生成的应用。

Method: 提出系统化的方法开发和验证0-7分细粒度评分的自动评价器，并构建专家标注的数据集ProofBench以测试其效果。分析了多种评价器设计因素，如骨干模型、输入上下文、流程等，最终集成出最优方案ProofGrader。

Result: ProofGrader在与专家成绩的均值绝对误差为0.926，远低于基线方法。同时在“best-of-n”选优任务中，证明其可提升实际下游应用效果，平均得分接近于人类专家。

Conclusion: ProofGrader评价器能准确细粒度评分LLM生成的数学证明，与专家评分高度匹配，优于传统评价方法。

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [59] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 本文系统梳理SLM（小型语言模型）与LLM（大型语言模型）的协同框架，分类总结其提升性能、降低成本、增强隐私与信任的目标及方法，并展望未来的挑战与发展方向。


<details>
  <summary>Details</summary>
Motivation: LLM虽然强大，但在微调成本、推理延迟、边缘部署和可靠性方面存在限制，需要充分利用SLM的高效性和适应性，与LLM进行协同以弥补各自不足。

Method: 系统性综述，基于协同目标分类，提出四大目标，并回顾代表性方法及设计模式。

Result: 提出SLM-LLM协同的目标分类体系（性能提升、成本效益、隐私保护与可信性），总结现有方法与设计范式，并指明未来研究方向与挑战。

Conclusion: 推动高效、安全和可扩展的SLM-LLM协同发展，提出未来需要解决的挑战。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [60] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: THTB框架结合认知科学方法，通过筛选高难度、高质量指令，有效提升少量训练数据模型的性能和泛化能力，远超传统和LLM内部知识自选方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在特定领域的适应依赖于高质量的监督微调数据，但数据筛选方法存在过度依赖模型自有知识、解释性弱和泛化能力有限的问题。

Method: THTB采用质量过滤结合内在、外在难度评分机制，优先筛选高层次认知指令，并为注释流程提供定量解释标准，指导高效数据微调。

Result: 通过提出THTB框架，结合质量过滤和任务难度评分，实现更高效、更具解释性的指令数据筛选与注释指导，只需5%数据即可超越全量训练模型，并在泛化性上优于仅依赖LLM选择的数据，垂直领域2%数据训练亦可超越大样本模型。

Conclusion: THTB在数据选择与注释方面提供了高效、可解释的指导，使得少量数据情况下模型表现超越全数据与高数据量模型，在领域适应性上表现出显著优势。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [61] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型的jailbreak攻击技术，提出更细致全面的分类体系，用意大利语新数据集和结构化挑战检验了各类攻击的成功率和检测方法，为LLM安全防护和未来研究奠定了坚实基础。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs防护手段多聚焦于单轮攻击、语言覆盖有限、且攻击分类缺乏全面细致，难以对抗多样化jailbreak技术，因此需要系统性理解和防御。

Method: 作者开展了结构化的red-teaming挑战，对不同jailbreak策略进行测试和分析；提出了新的分层分类法；对jailbreak策略进行统计与效果分析；用分类提示引导LLM自动检测对抗样本，并标注了意大利语多轮对话新数据集。

Result: （1）提出全面、分层的jailbreak策略taxonomy（含7大类50个策略）；（2）统计各类攻击方式的流行度和成功率，发现具体的模型漏洞及规避规律；（3）基于分类提示优化LLM自动检测能力；（4）发布意大利语多轮对话攻防新数据集，支持更复杂场景分析。

Conclusion: 论文通过系统性的red-teaming挑战，总结和分析了LLMs被jailbreak的典型策略类型及其效果，并提出了新的分类体系和检测方法，以及多轮对话数据集，为评估和抵御jailbreak攻击提供了研究基础和资源。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [62] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 论文比较了两种作者归属方法—风格嵌入与指令微调LLM评判器（GPT-4o）—在多领域数据集上的表现，发现两者在不同文本类型下各有优势，推荐混合策略提升归属准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，机器生成文本与人类写作愈发接近，作者归属判别变得复杂；因而需系统性评估不同归属机制以提升鉴别能力。

Method: 采用基准实验，分别利用固定风格嵌入和指令微调的LLM（GPT-4o）对比归属准确性，在含六个领域的公开数据集上进行评估，并分析不同方法在具体领域的表现差异。

Result: 风格嵌入对GPT系续写准确度高（82%），LLM评判方法在LLaMA续写稍优且更擅长识别学术/小说风格，而嵌入方法更擅长口语/剧本场景；各方法优劣互补。

Conclusion: 将固定风格嵌入和指令微调的LLM评判方法用于作者归属任务，各自展现出不同优势，需要混合策略以应对多维归属问题。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [63] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 通过分析微调前后LLM激活差异，发现窄领域微调产生明显训练目标痕迹并影响模型解释性与安全性，建议更慎重使用及改进微调流程。


<details>
  <summary>Details</summary>
Motivation: 微调已成为适应LLM到特定任务和构建特殊性质模型的重要方法，但微调可能带来模型偏见和安全、可解释性隐患，亟需分析和理解其影响机制。

Method: 通过模型微分（model diffing）分析微调前后模型激活的差异，尤其是随机文本前几token的激活差异，并以此差异引导生成相似格式和内容的文本，同时构建了解释性智能体用于理解微调领域。

Result: 发现窄领域微调会造成显著偏差，通过混合预训练数据可部分消除这种偏差；解释性智能体利用这些偏差能更好理解微调领域，优于传统提示法。分析覆盖多架构和规模的模型，并展示了微调的潜在过拟合和残留风险。

Conclusion: 窄领域的微调会在LLM模型激活中留下强烈的偏差，这些偏差能清晰反映训练目标，同时提醒安全和可解释性研究者应谨慎使用窄领域微调模型作为广泛微调的代理，并建议对微调方式进行改进。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [64] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: 论文提出了一种新型的、基于嵌入空间优化的攻防框架RAID，用以系统性检测和攻击LLM安全防线，实验证明其攻击效果和效率都优于现有技术，并指明嵌入空间正则化对于模型安全有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在众多任务中表现优异，但仍容易受到绕过安全机制的“jailbreak”攻击。现有攻击方式或防御手段存在效率与效果上的不足，亟需更系统化的攻防方法来检验和提升模型安全性。

Method: 提出RAID（Refusal-Aware and Integrated Decoding）框架，通过将离散词元松弛到连续嵌入空间，并以联合目标优化嵌入：（1）促使模型生成受限内容；（2）引入拒绝感知正则化项以偏离拒绝方向；（3）使用连贯性项确保语义合理和不冗余。优化完成后，采用critic-guided解码将嵌入映射回词元，平衡语义亲和度和语言模型概率，生成既自然又有效的攻击后缀。

Result: RAID针对多个开源LLM进行了实验，结果表明其比最新的白盒和黑盒基线方法攻击成功率更高，查询次数和计算成本更低。

Conclusion: RAID技术能更系统、有效地发现大型语言模型的安全弱点，并证明嵌入空间中的正则化设计对于理解和缓解LLM越狱脆弱性至关重要。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [65] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 本文用道德基础理论分析LLM在道德与政治问题上的倾向，发现其生成内容受政治和人口学因素影响，回应存在偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛应用于医疗、关系、法律等领域，其在复杂政治和道德话题中的回应偏差成为关键问题，亟需系统量化和分析其潜在意识形态倾向。

Method: 采用道德基础理论（MFT），对LLM在多个实验条件下的回应进行对比分析，并与已有人类数据做直接比较，结合角色扮演和直接提示等方式检验LLM回应的意识形态倾向。

Result: LLM在回应中展现出与某些政治意识形态更为一致的倾向，不同提示和角色设定下，其对意识形态的表征准确性也有所不同，受政治和人口学影响明显。

Conclusion: 通过系统性分析，本文证明了LLM生成的回应存在一定政治和人口学倾向，揭示了AI在道德和政治立场上的偏向性问题。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [66] [An efficient algorithm for \textsc{$\mathcal{F}$-subgraph-free Edge Deletion} on graphs having a product structure](https://arxiv.org/abs/2510.14674)
*Shinwoo An,Seonghyuk Im,Seokbeom Kim,Myounghwan Lee*

Main category: cs.DM

TL;DR: 本文提出了高效的固定参数算法，判定平面图是否可通过有限次删除变为特定子图自由图，并论证了参数不可删减的最优性，算法泛化到部分更广泛的图类。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用中需要通过少量顶点或边的操作，使平面图满足特定的子图禁用性质，但这一问题在参数化计算中仍有挑战性。现有一般方法如基于有界twins宽的图的本阶模型检测，效率不高，因而需要更高效的专用算法。

Method: 本文提出了一个统一的算法设计框架，针对带有“乘积结构”的图，开发了固定参数线性时间算法，用于判定平面图是否可通过至多k个顶点或边的删除，使之成为$\mathcal{F}$-subgraph-free，并推广到局部半径有界的disk图和有界属的图。

Result: 算法在参数k、$\mathcal{F}$的规模与$\mathcal{F}$中最大顶点数的双指数时间下工作，优于基于有界twins宽的本阶模型检测的一般方法。进一步拓展表明在更广泛图类下问题仍有高效算法。

Conclusion: 文中提出的算法在上述参数范围内达到了最优参数化复杂度，在只对参数$k$和$|V(F)|$做参数化、且仅限制于平面图的情况下，如果再减少参数，则除非指数时间假设失败，否则无法保持固定参数可解性。

Abstract: Given a family $\mathcal{F}$ of graphs, a graph is
\emph{$\mathcal{F}$-subgraph-free} if it has no subgraph isomorphic to a member
of $\mathcal{F}$. We present a fixed-parameter linear-time algorithm that
decides whether a planar graph can be made $\mathcal{F}$-subgraph-free by
deleting at most $k$ vertices or $k$ edges, where the parameters are $k$,
$\lvert \mathcal{F} \rvert$, and the maximum number of vertices in a member of
$\mathcal{F}$. The running time of our algorithm is double-exponential in the
parameters, which is faster than the algorithm obtained by applying the
first-order model checking result for graphs of bounded twin-width.
  To obtain this result, we develop a unified framework for designing
algorithms for this problem on graphs with a ``product structure.'' Using this
framework, we also design algorithms for other graph classes that generalize
planar graphs. Specifically, the problem admits a fixed-parameter linear time
algorithm on disk graphs of bounded local radius, and a fixed-parameter
almost-linear time algorithm on graphs of bounded genus.
  Finally, we show that our result gives a tight fixed-parameter algorithm in
the following sense: Even when $\mathcal{F}$ consists of a single graph $F$ and
the input is restricted to planar graphs, it is unlikely to drop any parameters
$k$ and $\lvert V(F) \rvert$ while preserving fixed-parameter tractability,
unless the Exponential-Time Hypothesis fails.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [67] [Decidability and Characterization of Expansivity for Group Cellular Automata](https://arxiv.org/abs/2510.14568)
*Niccolo' Castronuovo,Alberto Dennunzio,Luciano Margara*

Main category: cs.FL

TL;DR: 本文系统研究了群元胞自动机的膨胀性，给出判断膨胀性的简单刻画，证明其可判定，厘清了膨胀性与拓扑传递、单射性质间的从属关系。


<details>
  <summary>Details</summary>
Motivation: 群元胞自动机是动力系统与计算理论中的重要模型，但关于其膨胀性的判别及与其他动力属性的关系缺乏系统描述。本文旨在明确膨胀性的判别方式及其在更一般群结构下的可判定性。

Method: 作者通过对群元胞自动机在不同群结构下的膨胀性进行理论分析，提出易于判断的刻画方法，并利用判定性理论证明了在非阿贝尔群情形下膨胀性为可判定性质。还通过严格的集合关系证明不同自动机类别间的包含关系。

Result: 得到了一种简单的方法判断阿贝尔群上群元胞自动机的膨胀性，证明了膨胀性在非阿贝尔群上是可判定的，并揭示了膨胀性自动机与拓扑传递、单射自动机类间的严格包含关系。

Conclusion: 文章给出了群元胞自动机在阿贝尔群上的膨胀性的易检查刻画，并证明了膨胀性对一般（非阿贝尔）群来说是可判定的。同时，证明了膨胀性群元胞自动机的集合严格包含于拓扑传递且单射的群元胞自动机集合之内。

Abstract: Group cellular automata are continuous, shift-commuting endomorphisms of
$G^\mathbb{Z}$, where $G$ is a finite group. We provide an easy-to-check
characterization of expansivity for group cellular automata on abelian groups
and we prove that expansivity is a decidable property for general (non-abelian)
groups. Moreover, we show that the class of expansive group cellular automata
is strictly contained in that of topologically transitive injective group
cellular automata.

</details>


### [68] [Efficient Verification of Metric Temporal Properties with Past in Pointwise Semantics](https://arxiv.org/abs/2510.14699)
*S. Akshay,Prerak Contractor,Paul Gastin,R. Govind,B. Srivathsan*

Main category: cs.FL

TL;DR: 这篇论文提出了一种创新的MITL模型检测方法，支持高效处理含过去及未来模态的公式。通过自动机结构优化和新算法实现，大幅提升了检测效率，在标准和实际基准上均取得优越实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有MITL模型检测对过去和未来混合模态的处理效率不高，且对自动机确定性的保障有限，因此目标是提升自动机确定性和模型检测效率。

Method: 将MITL（含过去模态的片段）线性时间翻译为同步确定性定时自动机网络；对含更复杂模态的MITL，翻译为带有未来时钟的广义定时自动机网络，并提出了SCC-based活性分析算法，开发了原型工具实现并评估。

Result: 新方法支持处理同时含过去和未来模态的MITL公式，且在72条基准公式上性能显著优于主流工具，实现了点式语义下的端到端模型检测算法，对两个著名基准测试展现出有效性。

Conclusion: 开发了一种新的MITL点式语义模型检测方法，可高效支持包含过去及未来模态的检查。实验表明其性能优于现有方法，在标准测试集上展现出显著提升，并在实际基准测试中验证了其实用性。

Abstract: Model checking for real-timed systems is a rich and diverse topic. Among the
different logics considered, Metric Interval Temporal Logic (MITL) is a
powerful and commonly used logic, which can succinctly encode many interesting
timed properties especially when past and future modalities are used together.
In this work, we develop a new approach for MITL model checking in the
pointwise semantics, where our focus is on integrating past and maximizing
determinism in the translated automata.
  Towards this goal, we define synchronous networks of timed automata with
shared variables and show that the past fragment of MITL can be translated in
linear time to synchronous networks of deterministic timed automata. Moreover
determinism can be preserved even when the logic is extended with future
modalities at the top-level of the formula. We further extend this approach to
the full MITL with past, translating it into networks of generalized timed
automata (GTA) with future clocks (which extend timed automata and event clock
automata). We present an SCC-based liveness algorithm to analyse GTA. We
implement our translation in a prototype tool which handles both finite and
infinite timed words and supports past modalities. Our experimental evaluation
demonstrates that our approach significantly outperforms the state-of-the-art
in MITL satisfiability checking in pointwise semantics on a benchmark suite of
72 formulas. Finally, we implement an end-to-end model checking algorithm for
pointwise semantics and demonstrate its effectiveness on two well-known
benchmarks.

</details>


### [69] [On the order of lazy cellular automata](https://arxiv.org/abs/2510.14841)
*Edgar Alcalá-Arroyo,Alonso Castillo-Ramirez*

Main category: cs.FL

TL;DR: 文章系统研究了在群上定义的lazy cellular automata关于阶的动力学性质，给出了阶的统一上界，并展示当活动模式为quasi-constant时其阶达此界。


<details>
  <summary>Details</summary>
Motivation: lazy cellular automata结构简单但行为仍带有微妙复杂性，研究其动力学性质（特别是阶的界）有助于理解基础自同步系统的理论结构。

Method: 采用理论推导的方法，分析lazy cellular automata在不同活动转移p和输出符号a的情况下的阶。通过构造和泛化，建立阶的上界并证明达界情形。

Result: 推导出lazy cellular automata的阶的普遍上界，且当活动转移模式p为quasi-constant时严格达到该上界。

Conclusion: 本文提出了关于lazy cellular automata的阶的上界，并证明当特定条件（p为quasi-constant pattern）下可达到此上界。

Abstract: We study the most elementary family of cellular automata defined over an
arbitrary group universe $G$ and an alphabet $A$: the lazy cellular automata,
which act as the identity on configurations in $A^G$, except when they read a
unique active transition $p \in A^S$, in which case they write a fixed symbol
$a \in A$. As expected, the dynamical behavior of lazy cellular automata is
relatively simple, yet subtle questions arise since they completely depend on
the choice of $p$ and $a$. In this paper, we investigate the order of a lazy
cellular automaton $\tau : A^G \to A^G$, defined as the cardinality of the set
$\{ \tau^k : k \in \mathbb{N} \}$. In particular, we establish a general upper
bound for the order of $\tau$ in terms of $p$ and $a$, and we prove that this
bound is attained when $p$ is a quasi-constant pattern.

</details>
