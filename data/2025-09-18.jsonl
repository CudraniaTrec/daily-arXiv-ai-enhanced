{"id": "2509.13436", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13436", "abs": "https://arxiv.org/abs/2509.13436", "authors": ["Evan Eisinger", "Michael A. Heroux"], "title": "Is Research Software Science a Metascience?", "comment": "5 pages", "summary": "As research increasingly relies on computational methods, the reliability of\nscientific results depends on the quality, reproducibility, and transparency of\nresearch software. Ensuring these qualities is critical for scientific\nintegrity and discovery. This paper asks whether Research Software Science\n(RSS)--the empirical study of how research software is developed and\nused--should be considered a form of metascience, the science of science.\nClassification matters because it could affect recognition, funding, and\nintegration of RSS into research improvement. We define metascience and RSS,\ncompare their principles and objectives, and examine their overlaps. Arguments\nfor classification highlight shared commitments to reproducibility,\ntransparency, and empirical study of research processes. Arguments against\nportraying RSS as a specialized domain focused on a tool rather than the\nbroader scientific enterprise. Our analysis finds RSS advances core goals of\nmetascience, especially in computational reproducibility, and bridges\ntechnical, social, and cognitive aspects of research. Its classification\ndepends on whether one adopts a broad definition of metascience--any empirical\neffort to improve science--or a narrow one focused on systemic and\nepistemological structures. We argue RSS is best understood as a distinct\ninterdisciplinary domain that aligns with, and in some definitions fits within,\nmetascience. Recognizing it as such can strengthen its role in improving\nreliability, justify funding, and elevate software development in research\ninstitutions. Regardless of classification, applying scientific rigor to\nresearch software ensures the tools of discovery meet the standards of the\ndiscoveries themselves."}
{"id": "2509.13471", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13471", "abs": "https://arxiv.org/abs/2509.13471", "authors": ["Sina Gogani-Khiabani", "Ashutosh Trivedi", "Diptikalyan Saha", "Saeid Tizpaz-Niari"], "title": "An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software", "comment": "To appear at ICSE 26. 12 pages", "summary": "Large language models (LLMs) show promise for translating natural-language\nstatutes into executable logic, but reliability in legally critical settings\nremains challenging due to ambiguity and hallucinations. We present an agentic\napproach for developing legal-critical software, using U.S. federal tax\npreparation as a case study. The key challenge is test-case generation under\nthe oracle problem, where correct outputs require interpreting law. Building on\nmetamorphic testing, we introduce higher-order metamorphic relations that\ncompare system outputs across structured shifts among similar individuals.\nBecause authoring such relations is tedious and error-prone, we use an\nLLM-driven, role-based framework to automate test generation and code\nsynthesis. We implement a multi-agent system that translates tax code into\nexecutable software and incorporates a metamorphic-testing agent that searches\nfor counterexamples. In experiments, our framework using a smaller model\n(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier\nmodels (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results\nsupport agentic LLM methodologies as a path to robust, trustworthy\nlegal-critical software from natural-language specifications."}
{"id": "2509.13487", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13487", "abs": "https://arxiv.org/abs/2509.13487", "authors": ["Abubakari Alidu", "Michele Ciavotta", "Flavio DePaoli"], "title": "Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation", "comment": null, "summary": "Developing reliable data enrichment pipelines demands significant engineering\nexpertise. We present Prompt2DAG, a methodology that transforms natural\nlanguage descriptions into executable Apache Airflow DAGs. We evaluate four\ngeneration approaches -- Direct, LLM-only, Hybrid, and Template-based -- across\n260 experiments using thirteen LLMs and five case studies to identify optimal\nstrategies for production-grade automation. Performance is measured using a\npenalized scoring framework that combines reliability with code quality (SAT),\nstructural integrity (DST), and executability (PCT). The Hybrid approach\nemerges as the optimal generative method, achieving a 78.5% success rate with\nrobust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly\noutperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.\nOur findings show that reliability, not intrinsic code quality, is the primary\ndifferentiator. Cost-effectiveness analysis reveals the Hybrid method is over\ntwice as efficient as Direct prompting per successful DAG. We conclude that a\nstructured, hybrid approach is essential for balancing flexibility and\nreliability in automated workflow generation, offering a viable path to\ndemocratize data pipeline development."}
{"id": "2509.13535", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13535", "abs": "https://arxiv.org/abs/2509.13535", "authors": ["S M Farah Al Fahim", "Md Nakhla Rafi", "Zeyang Ma", "Dong Jae Kim", "Tse-Hsun", "Chen"], "title": "Crash Report Enhancement with Large Language Models: An Empirical Study", "comment": null, "summary": "Crash reports are central to software maintenance, yet many lack the\ndiagnostic detail developers need to debug efficiently. We examine whether\nlarge language models can enhance crash reports by adding fault locations,\nroot-cause explanations, and repair suggestions. We study two enhancement\nstrategies: Direct-LLM, a single-shot approach that uses stack-trace context,\nand Agentic-LLM, an iterative approach that explores the repository for\nadditional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced\nreports improve Top-1 problem-localization accuracy from 10.6% (original\nreports) to 40.2-43.1%, and produce suggested fixes that closely resemble\ndeveloper patches (CodeBLEU around 56-57%). Both our manual evaluations and\nLLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause\nexplanations and more actionable repair guidance. A user study with 16\nparticipants further confirms that enhanced reports make crashes easier to\nunderstand and resolve, with the largest improvement in repair guidance. These\nresults indicate that supplying LLMs with stack traces and repository code\nyields enhanced crash reports that are substantially more useful for debugging."}
{"id": "2509.13429", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13429", "abs": "https://arxiv.org/abs/2509.13429", "authors": ["Anthony Arnold", "Mark Marron"], "title": "Catalpa: GC for a Low-Variance Software Stack", "comment": null, "summary": "The performance of an application/runtime is usually conceptualized as a\ncontinuous function where, the lower the amount of memory/time used on a given\nworkload, then the better the compiler/runtime is. However, in practice, good\nperformance of an application is viewed as more of a binary function - either\nthe application responds in under, say 100 ms, and is fast enough for a user to\nbarely notice, or it takes a noticeable amount of time, leaving the user\nwaiting and potentially abandoning the task. Thus, performance really means how\noften the application is fast enough to be usable, leading industrial\ndevelopers to focus on the 95th and 99th percentile tail-latencies as heavily,\nor moreso, than average response time. Our vision is to create a software stack\nthat actively supports these needs via programming language and runtime system\ndesign. In this paper we present a novel garbage-collector design, the Catalpa\ncollector, for the Bosque programming language and runtime. This allocator is\ndesigned to minimize latency and variability while maintaining high-throughput\nand incurring small memory overheads. To achieve these goals we leverage\nvarious features of the Bosque language, including immutability and\nreference-cycle freedom, to construct a collector that has bounded collection\npauses, incurs fixed-constant memory overheads, and does not require any\nbarriers or synchronization with application code."}
{"id": "2509.13699", "categories": ["cs.LO", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13699", "abs": "https://arxiv.org/abs/2509.13699", "authors": ["Max Barth", "Marie-Christine Jakobs"], "title": "Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement", "comment": null, "summary": "Automatic software verification is a valuable means for software quality\nassurance. However, automatic verification and in particular software model\nchecking can be time-consuming, which hinders their practical applicability\ne.g., the use in continuous integration. One solution to address the issue is\nto reduce the response time of the verification procedure by leveraging today's\nmulti-core CPUs.\n  In this paper, we propose a solution to parallelize trace abstraction, an\nabstraction-based approach to software model checking. The underlying idea of\nour approach is to parallelize the abstraction refinement. More concretely, our\napproach analyzes different traces (syntactic program paths) that could violate\nthe safety property in parallel. We realize our parallelized version of trace\nabstraction in the verification tool Ulti mate Automizer and perform a thorough\nevaluation. Our evaluation shows that our parallelization is more effective\nthan sequential trace abstraction and can provide results significantly faster\non many time-consuming tasks. Also, our approach is more effective than DSS, a\nrecent parallel approach to abstraction-based software model checking."}
{"id": "2509.14087", "categories": ["cs.FL", "cs.LO", "F.1.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2509.14087", "abs": "https://arxiv.org/abs/2509.14087", "authors": ["Rüdiger Ehlers"], "title": "How Concise are Chains of co-Büchi Automata?", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "Chains of co-B\\\"uchi automata (COCOA) have recently been introduced as a new\ncanonical model for representing arbitrary omega-regular languages. They can be\nminimized in polynomial time and are hence an attractive language\nrepresentation for applications in which normally, deterministic omega-automata\nare used. While it is known how to build COCOA from deterministic parity\nautomata, little is currently known about their relationship to automaton\nmodels introduced earlier than COCOA.\n  In this paper, we analyze the conciseness of chains of co-B\\\"uchi automata.\nWe show that even in the case that all automata in the chain are deterministic,\nchains of co-B\\\"uchi automata can be exponentially more concise than\ndeterministic parity automata. We then answer the question if this conciseness\nis retained when performing Boolean operations (such as disjunction and\nconjunction) over COCOA by showing that there exist families of languages for\nwhich these operations lead to an exponential growth of the sizes of the\nautomata. The families have the property that when representing them using\ndeterministic parity automata, taking the disjunction or conjunction of them\nonly requires a polynomial blow-up, which shows that Boolean operations over\nCOCOA do not retain their conciseness in general."}
{"id": "2509.13787", "categories": ["cs.DM"], "pdf": "https://arxiv.org/pdf/2509.13787", "abs": "https://arxiv.org/abs/2509.13787", "authors": ["Abdulkafi Sanad"], "title": "Hyper-Zagreb Indices of Hypergraphs with Application in Drug Design", "comment": null, "summary": "Let $\\mathcal{H}$ be a hypergraph on the non-empty finite vertex set\n$V(\\mathcal{H})$ with the hyperedge set $E(\\mathcal{H})$, where each hyperedge\n$e \\in E(\\mathcal{H})$ is a subset of $V(\\mathcal{H})$ with at least two\nvertices. This paper introduces the first and second Hyper-Zagreb indices for\nhypergraphs, extending these well-known graph indices to hypergraphs. We\ndiscuss bounds on these indices for general hypergraphs, weak bipartite\nhypergraphs, hypertrees, $k$-uniform hypergraphs, $k$-uniform weak bipartite\nhypergraphs, and $k$-uniform hypertrees, characterizing the extremal\nhypergraphs that achieve these bounds. Additionally, we present a novel\napplication of these indices in drug design and bioactivity prediction,\ndemonstrating their utility in quantitative structure-activity relationship\n(QSAR) modeling."}
{"id": "2509.13480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation."}
{"id": "2509.13650", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13650", "abs": "https://arxiv.org/abs/2509.13650", "authors": ["Amena Amro", "Manar H. Alalfi"], "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?", "comment": null, "summary": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security."}
{"id": "2509.13489", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.13489", "abs": "https://arxiv.org/abs/2509.13489", "authors": ["Chester J. F. Gould", "William J. Bowman"], "title": "Extended Abstract: Towards a Performance Comparison of Syntax and Type-Directed NbE", "comment": "Submitted to TyDe 2025", "summary": "A key part of any dependent type-checker is the method for checking whether\ntwo types are equal. A common claim is that syntax-directed equality is more\nperformant, although type-directed equality is more expressive. However, this\nclaim is difficult to make precise, since implementations choose only one or\nthe other approach, making a direct comparison impossible. We present some\nwork-in-progress developing a realistic platform for direct, apples-to-apples,\ncomparison of the two approaches, quantifying how much slower type-directed\nequality checking is, and analyzing why and how it can be improved."}
{"id": "2509.13871", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.13871", "abs": "https://arxiv.org/abs/2509.13871", "authors": ["Dror Fried", "Etay Segal", "Gad E. Yaron"], "title": "Algorithmic Perspective on Toda's Theorem", "comment": null, "summary": "Toda's Theorem is a fundamental result in computational complexity theory,\nwhose proof relies on a reduction from a QBF problem with a constant number of\nquantifiers to a model counting problem. While this reduction, henceforth\ncalled Toda's reduction, is of a purely theoretical nature, the recent progress\nof model counting tools raises the question of whether the reduction can be\nutilized to an efficient algorithm for solving QBF. In this work, we address\nthis question by looking at Toda's reduction from an algorithmic perspective.\nWe first convert the reduction into a concrete algorithm that given a QBF\nformula and a probability measure, produces the correct result with a\nconfidence level corresponding to the given measure. Beyond obtaining a naive\nprototype, our algorithm and the analysis that follows shed light on the fine\ndetails of the reduction that are so far left elusive. Then, we improve this\nprototype through various theoretical and algorithmic refinements. While our\nresults show a significant progress over the naive prototype, they also provide\na clearer understanding of the remaining challenges in turning Toda's reduction\ninto a competitive solver."}
{"id": "2509.13819", "categories": ["cs.DM", "cs.CC", "math.CO"], "pdf": "https://arxiv.org/pdf/2509.13819", "abs": "https://arxiv.org/abs/2509.13819", "authors": ["Florian Galliot"], "title": "4-uniform Maker-Breaker and Maker-Maker games are PSPACE-complete", "comment": null, "summary": "We study two positional games where two players take turns picking a\npreviously unpicked vertex of a hypergraph $H$. We say a player fills an edge\nof $H$ if that player has picked all the vertices of that edge. In the\nMaker-Maker game, whoever first fills an edge wins, or we get a draw if no edge\nis filled. In the Maker-Breaker game, the first player aims at filling an edge\nwhile the second player aims at preventing the first player from filling an\nedge. We show that, for both games, deciding whether the first player has a\nwinning strategy is a PSPACE-complete problem even when restricted to 4-uniform\nhypergraphs. For the Maker-Maker game, this improves on a previous result for\nhypergraphs of rank 4. For the Maker-Breaker game, this improves on a previous\nresult for 5-uniform hypergraphs, and closes the complexity gap as the problem\nfor hypergraphs of rank 3 is known to be solvable in polynomial time."}
{"id": "2509.13539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts."}
{"id": "2509.13656", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13656", "abs": "https://arxiv.org/abs/2509.13656", "authors": ["Yingao Elaine Yao", "Vedant Nimje", "Varun Viswanath", "Saikat Dutta"], "title": "A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks", "comment": "22 pages, 2 figures, 6 tables", "summary": "Notebooks have become the de-facto choice for data scientists and machine\nlearning engineers for prototyping and experimenting with machine learning (ML)\npipelines. Notebooks provide an interactive interface for code, data, and\nvisualization. However, notebooks provide very limited support for testing.\nThus, during continuous development, many subtle bugs that do not lead to\ncrashes often go unnoticed and cause silent errors that manifest as performance\nregressions.\n  To address this, we introduce NBTest - the first regression testing framework\nthat allows developers to write cell-level assertions in notebooks and run such\nnotebooks in pytest or in continuous integration (CI) pipelines. NBTest offers\na library of assertion APIs, and a JupyterLab plugin that enables executing\nassertions. We also develop the first automated approach for generating\ncell-level assertions for key components in ML notebooks, such as data\nprocessing, model building, and model evaluation. NBTest aims to improve the\nreliability and maintainability of ML notebooks without adding developer\nburden.\n  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163\nassertions (35.75 on average per notebook). The generated assertions obtain a\nmutation score of 0.57 in killing ML-specific mutations. NBTest can catch\nregression bugs in previous versions of the Kaggle notebooks using assertions\ngenerated for the latest versions. Because ML pipelines involve non\ndeterministic computations, the assertions can be flaky. Hence, we also show\nhow NBTest leverages statistical techniques to minimize flakiness while\nretaining high fault-detection effectiveness. NBTest has been adopted in the CI\nof a popular ML library. Further, we perform a user study with 17 participants\nthat shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful\nin writing assertions and testing notebooks (Rating 4.24/5)."}
{"id": "2509.13982", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.13982", "abs": "https://arxiv.org/abs/2509.13982", "authors": ["Boyu Zhang", "Ping He", "Tianyu Du", "Xuhong Zhang", "Lei Yun", "Kingsum Chow", "Jianwei Yin"], "title": "CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing", "comment": null, "summary": "With the widespread adoption of open-source code language models (code LMs),\nintellectual property (IP) protection has become an increasingly critical\nconcern. While current watermarking techniques have the potential to identify\nthe code LM to protect its IP, they have limitations when facing the more\npractical and complex demand, i.e., offering the individual user-level tracing\nin the black-box setting. This work presents CLMTracing, a black-box code LM\nwatermarking framework employing the rule-based watermarks and\nutility-preserving injection method for user-level model tracing. CLMTracing\nfurther incorporates a parameter selection algorithm sensitive to the robust\nwatermark and adversarial training to enhance the robustness against watermark\nremoval attacks. Comprehensive evaluations demonstrate CLMTracing is effective\nacross multiple state-of-the-art (SOTA) code LMs, showing significant harmless\nimprovements compared to existing SOTA baselines and strong robustness against\nvarious removal attacks."}
{"id": "2509.14089", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.14089", "abs": "https://arxiv.org/abs/2509.14089", "authors": ["Luca Aceto", "Antonis Achilleos", "Aggeliki Chalki", "Anna Ingólfsdóttir"], "title": "The Complexity of Deciding Characteristic Formulae Modulo Nested Simulation (extended abstract)", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258. A full version of this\n  paper, containing all proofs, appears at arXiv:2505.22277", "summary": "This paper studies the complexity of determining whether a formula in the\nmodal logics characterizing the nested-simulation semantics is characteristic\nfor some process, which is equivalent to determining whether the formula is\nsatisfiable and prime. The main results are that the problem of determining\nwhether a formula is prime in the modal logic characterizing the\n2-nested-simulation preorder is coNP-complete and is PSPACE-complete in the\ncase of the n-nested-simulation preorder, when n>=3. This establishes that\ndeciding characteristic formulae for the n-nested simulation semantics is\nPSPACE-complete, when n>=3. In the case of the 2-nested simulation semantics,\nthat problem lies in the complexity class DP, which consists of languages that\ncan be expressed as the intersection of one language in NP and of one in coNP."}
{"id": "2509.14193", "categories": ["cs.DM", "math.CO", "physics.soc-ph", "G.2; I.5"], "pdf": "https://arxiv.org/pdf/2509.14193", "abs": "https://arxiv.org/abs/2509.14193", "authors": ["Fernando Diaz-Diaz", "Karel Devriendt", "Renaud Lambiotte"], "title": "Gremban Expansion for Signed Networks: Algebraic and Combinatorial Foundations for Community-Faction Detection", "comment": null, "summary": "This article deals with the characterization and detection of community and\nfaction structures in signed networks. We approach the study of these mesoscale\nstructures through the lens of the Gremban expansion. This graph operation\nlifts a signed graph to a larger unsigned graph, and allows the extension of\nstandard techniques from unsigned to signed graphs. We develop the\ncombinatorial and algebraic properties of the Gremban expansion, with a focus\non its inherent involutive symmetry. The main technical result is a bijective\ncorrespondence between symmetry-respecting cut-sets in the Gremban expansion,\nand regular cut-sets and frustration sets in the signed graph (i.e., the\ncombinatorial structures that underlie communities and factions respectively).\nThis result forms the basis for our new approach to community-faction detection\nin signed networks, which makes use of spectral clustering techniques that\nnaturally respect the required symmetries. We demonstrate how this approach\ndistinguishes the two mesoscale structures, how to generalize the approach to\nmulti-way clustering and discuss connections to network dynamical systems."}
{"id": "2509.13569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendonça", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "João Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks."}
{"id": "2509.13680", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13680", "abs": "https://arxiv.org/abs/2509.13680", "authors": ["Wei Ma", "Yixiao Yang", "Jingquan Ge", "Xiaofei Xie", "Lingxiao Jiang"], "title": "Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations", "comment": null, "summary": "Code generation models are widely used in software development, yet their\nsensitivity to prompt phrasing remains under-examined. Identical requirements\nexpressed with different emotions or communication styles can yield divergent\noutputs, while most benchmarks emphasize only peak performance. We present\nPromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically\nequivalent prompt variants with emotion and personality templates, and that\nevaluates stability using probability aware continuous scoring or using binary\npass rates when logits are unavailable. The results are aggregated into a\nproposed area under curve metric (AUC-E) for cross model comparison. Across 14\nmodels from three families (Llama, Qwen, and DeepSeek), our study shows that\nperformance and stability behave as largely decoupled optimization objectives,\nand it reveals architectural and scale related patterns that challenge common\nassumptions about model robustness. The framework supports rapid screening for\nclosed-source models as well as detailed stability analysis in research\nsettings. PromptSE enables practitioners to quantify performance stability\ntrade offs for deployment and model selection, positioning prompt stability as\na complementary evaluation dimension alongside performance and fairness, and\ncontributing to more trustworthy AI-assisted software development tools."}
{"id": "2509.14092", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.14092", "abs": "https://arxiv.org/abs/2509.14092", "authors": ["Michele Boreale", "Luisa Collodi"], "title": "Parallelizable Feynman-Kac Models for Universal Probabilistic Programming", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "We study provably correct and efficient instantiations of Sequential Monte\nCarlo (SMC) inference in the context of formal operational semantics of\nProbabilistic Programs (PPs). We focus on universal PPs featuring sampling from\narbitrary measures and conditioning/reweighting in unbounded loops. We first\nequip Probabilistic Program Graphs (PPGs), an automata-theoretic description\nformat of PPs, with an expectation-based semantics over infinite execution\ntraces, which also incorporates trace weights. We then prove a finite\napproximation theorem that provides bounds to this semantics based on\nexpectations taken over finite, fixed-length traces. This enables us to frame\nour semantics within a Feynman-Kac (FK) model, and ensures the consistency of\nthe Particle Filtering (PF) algorithm, an instance of SMC, with respect to our\nsemantics. Building on these results, we introduce VPF, a vectorized version of\nthe PF algorithm tailored to PPGs and our semantics. Experiments conducted with\na proof-of-concept implementation of VPF show very promising results compared\nto state-of-the-art PP inference tools."}
{"id": "2509.14090", "categories": ["cs.LO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.14090", "abs": "https://arxiv.org/abs/2509.14090", "authors": ["Massimo Benerecetti", "Dario Della Monica", "Angelo Matteo", "Fabio Mogavero", "Gabriele Puppis"], "title": "An Automaton-based Characterisation of First-Order Logic over Infinite Trees", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "In this paper, we study First Order Logic (FO) over (unordered) infinite\ntrees and its connection with branching-time temporal logics. More\nspecifically, we provide an automata-theoretic characterisation of FO\ninterpreted over infinite trees. To this end, two different classes of hesitant\ntree automata are introduced and proved to capture precisely the expressive\npower of two branching time temporal logics, denoted polcCTLp and cCTL*[f],\nwhich are, respectively, a restricted version of counting CTL with past and\ncounting CTL* over finite paths, both of which have been previously shown\nequivalent to FO over infinite trees. The two automata characterisations\nnaturally lead to normal forms for the two temporal logics, and highlight the\nfact that FO can only express properties of the tree branches which are either\nsafety or co-safety in nature."}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation."}
{"id": "2509.13755", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.13755", "abs": "https://arxiv.org/abs/2509.13755", "authors": ["Zhaoyang Chu", "Yao Wan", "Zhikun Zhang", "Di Wang", "Zhou Yang", "Hongyu Zhang", "Pan Zhou", "Xuanhua Shi", "Hai Jin", "David Lo"], "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "While Code Language Models (CLMs) have demonstrated superior performance in\nsoftware engineering tasks such as code generation and summarization, recent\nempirical studies reveal a critical privacy vulnerability: these models exhibit\nunintended memorization of sensitive training data, enabling verbatim\nreproduction of confidential information when specifically prompted. To address\nthis issue, several approaches, including training data de-duplication and\ndifferential privacy augmentation, have been proposed. However, these methods\nrequire full-model retraining for deployed CLMs, which incurs substantial\ncomputational costs. In this paper, we aim to answer the following research\nquestion: Can sensitive information memorized by CLMs be erased effectively and\nefficiently?\n  We conduct a pioneering investigation into erasing sensitive memorization in\nCLMs through machine unlearning - a post-hoc modification method that removes\nspecific information from trained models without requiring full retraining.\nSpecifically, we first quantify the memorization risks of sensitive data within\nCLM training datasets and curate a high-risk dataset of 50,000 sensitive\nmemorized samples as unlearning targets. We study two widely used gradient\nascent-based unlearning approaches: the vanilla and constraint-based methods,\nand introduce CodeEraser, an advanced variant that selectively unlearns\nsensitive memorized segments in code while preserving the structural integrity\nand functional correctness of the surrounding code. Extensive experiments on\nthree families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,\nvalidate the effectiveness and efficiency of CodeEraser in erasing targeted\nsensitive memorization while maintaining model utility."}
{"id": "2509.14094", "categories": ["cs.LO", "F.4.1;I.2.3"], "pdf": "https://arxiv.org/pdf/2509.14094", "abs": "https://arxiv.org/abs/2509.14094", "authors": ["Radu Mardare", "Neil Ghani", "Eigil Rischel"], "title": "Metric Equational Theories", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "This paper proposes appropriate sound and complete proof systems for\nalgebraic structures over metric spaces by combining the development of\nQuantitative Equational Theories (QET) with the Enriched Lawvere Theories. We\nextend QETs to Metric Equational Theories (METs) where operations no longer\nhave finite sets as arities (as in QETs and the general theory of universal\nalgebras), but arities are now drawn from countable metric spaces. This\nextension is inspired by the theory of Enriched Lawvere Theories, which\nsuggests that the arities of operations should be the lambda-presentable\nobjects of the underlying lambda-accessible category. In this setting, the\nvalidity of terms in METs can no longer be guaranteed independently of the\nvalidity of equations, as is the case with QET. We solve this problem, and\nadapt the sound and complete proof system for QETs to these more general METs,\ntaking advantage of the specific structure of metric spaces."}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior."}
{"id": "2509.13758", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13758", "abs": "https://arxiv.org/abs/2509.13758", "authors": ["Kevin Halim", "Sin G. Teo", "Ruitao Feng", "Zhenpeng Chen", "Yang Gu", "Chong Wang", "Yang Liu"], "title": "A Study on Thinking Patterns of Large Reasoning Models in Code Generation", "comment": null, "summary": "Currently, many large language models (LLMs) are utilized for software\nengineering tasks such as code generation. The emergence of more advanced\nmodels known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek\nR1, and Qwen3. They have demonstrated the capability of performing multi-step\nreasoning. Despite the advancement in LRMs, little attention has been paid to\nsystematically analyzing the reasoning patterns these models exhibit and how\nsuch patterns influence the generated code. This paper presents a comprehensive\nstudy aimed at investigating and uncovering the reasoning behavior of LRMs\nduring code generation. We prompted several state-of-the-art LRMs of varying\nsizes with code generation tasks and applied open coding to manually annotate\nthe reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning\nbehaviors, encompassing 15 reasoning actions across four phases.\n  Our empirical study based on the taxonomy reveals a series of findings.\nFirst, we identify common reasoning patterns, showing that LRMs generally\nfollow a human-like coding workflow, with more complex tasks eliciting\nadditional actions such as scaffolding, flaw detection, and style checks.\nSecond, we compare reasoning across models, finding that Qwen3 exhibits\niterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like\napproach. Third, we analyze the relationship between reasoning and code\ncorrectness, showing that actions such as unit test creation and scaffold\ngeneration strongly support functional outcomes, with LRMs adapting strategies\nbased on task context. Finally, we evaluate lightweight prompting strategies\ninformed by these findings, demonstrating the potential of context- and\nreasoning-oriented prompts to improve LRM-generated code. Our results offer\ninsights and practical implications for advancing automatic code generation."}
{"id": "2509.14095", "categories": ["cs.LO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.14095", "abs": "https://arxiv.org/abs/2509.14095", "authors": ["Gaëtan Regaud", "Martin Zimmermann"], "title": "The Complexity of Generalized HyperLTL with Stuttering and Contexts", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "We settle the complexity of satisfiability and model-checking for generalized\nHyperLTL with stuttering and contexts, an expressive logic for the\nspecification of asynchronous hyperproperties. Such properties cannot be\nspecified in HyperLTL, as it is restricted to synchronous hyperproperties.\nNevertheless, we prove that satisfiability is $\\Sigma_1^1$-complete and thus\nnot harder than for HyperLTL. On the other hand, we prove that model-checking\nis equivalent to truth in second-order arithmetic, and thus much harder than\nthe decidable HyperLTL model-checking problem. The lower bounds for the\nmodel-checking problem hold even when only allowing stuttering or only allowing\ncontexts."}
{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains."}
{"id": "2509.13782", "categories": ["cs.SE", "cs.AI", "cs.MA", "D.2.2; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.13782", "abs": "https://arxiv.org/abs/2509.13782", "authors": ["Yu Ge", "Linna Xie", "Zhong Li", "Yu Pei", "Tian Zhang"], "title": "Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis", "comment": "20 pages, 6 figures", "summary": "Large Language Model Powered Multi-Agent Systems (MASs) are increasingly\nemployed to automate complex real-world problems, such as programming and\nscientific discovery. Despite their promising, MASs are not without their\nflaws. However, failure attribution in MASs - pinpointing the specific agent\nactions responsible for failures - remains underexplored and labor-intensive,\nposing significant challenges for debugging and system improvement. To bridge\nthis gap, we propose FAMAS, the first spectrum-based failure attribution\napproach for MASs, which operates through systematic trajectory replay and\nabstraction, followed by spectrum analysis.The core idea of FAMAS is to\nestimate, from variations across repeated MAS executions, the likelihood that\neach agent action is responsible for the failure. In particular, we propose a\nnovel suspiciousness formula tailored to MASs, which integrates two key factor\ngroups, namely the agent behavior group and the action behavior group, to\naccount for the agent activation patterns and the action activation patterns\nwithin the execution trajectories of MASs. Through expensive evaluations\nagainst 12 baselines on the Who and When benchmark, FAMAS demonstrates superior\nperformance by outperforming all the methods in comparison."}
{"id": "2509.14087", "categories": ["cs.FL", "cs.LO", "F.1.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2509.14087", "abs": "https://arxiv.org/abs/2509.14087", "authors": ["Rüdiger Ehlers"], "title": "How Concise are Chains of co-Büchi Automata?", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "Chains of co-B\\\"uchi automata (COCOA) have recently been introduced as a new\ncanonical model for representing arbitrary omega-regular languages. They can be\nminimized in polynomial time and are hence an attractive language\nrepresentation for applications in which normally, deterministic omega-automata\nare used. While it is known how to build COCOA from deterministic parity\nautomata, little is currently known about their relationship to automaton\nmodels introduced earlier than COCOA.\n  In this paper, we analyze the conciseness of chains of co-B\\\"uchi automata.\nWe show that even in the case that all automata in the chain are deterministic,\nchains of co-B\\\"uchi automata can be exponentially more concise than\ndeterministic parity automata. We then answer the question if this conciseness\nis retained when performing Boolean operations (such as disjunction and\nconjunction) over COCOA by showing that there exist families of languages for\nwhich these operations lead to an exponential growth of the sizes of the\nautomata. The families have the property that when representing them using\ndeterministic parity automata, taking the disjunction or conjunction of them\nonly requires a polynomial blow-up, which shows that Boolean operations over\nCOCOA do not retain their conciseness in general."}
{"id": "2509.13677", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement."}
{"id": "2509.13852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13852", "abs": "https://arxiv.org/abs/2509.13852", "authors": ["Yulun Wu", "Guangba Yu", "Zhihan Jiang", "Yichen Li", "Michael R. Lyu"], "title": "Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing", "comment": null, "summary": "Distributed tracing is an essential diagnostic tool in microservice systems,\nbut the sheer volume of traces places a significant burden on backend storage.\nA common approach to mitigating this issue is trace sampling, which selectively\nretains traces based on specific criteria, often preserving only anomalous\nones. However, this method frequently discards valuable information, including\nnormal traces that are essential for comparative analysis. To address this\nlimitation, we introduce Trace Sampling 2.0, which operates at the span level\nwhile maintaining trace structure consistency. This approach allows for the\nretention of all traces while significantly reducing storage overhead. Based on\nthis concept, we design and implement Autoscope, a span-level sampling method\nthat leverages static analysis to extract execution logic, ensuring that\ncritical spans are preserved without compromising structural integrity. We\nevaluated Autoscope on two open-source microservices. Our results show that it\nreduces trace size by 81.2% while maintaining 98.1% faulty span coverage,\noutperforming existing trace-level sampling methods. Furthermore, we\ndemonstrate its effectiveness in root cause analysis, achieving an average\nimprovement of 8.3%. These findings indicate that Autoscope can significantly\nenhance observability and storage efficiency in microservices, offering a\nrobust solution for performance monitoring."}
{"id": "2509.13683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks."}
{"id": "2509.13868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13868", "abs": "https://arxiv.org/abs/2509.13868", "authors": ["Manal Binkhonain", "Reem Alfayaz"], "title": "Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification", "comment": "33 pages, 12 figures", "summary": "Requirements classification assigns natural language requirements to\npredefined classes, such as functional and non functional. Accurate\nclassification reduces risk and improves software quality. Most existing models\nrely on supervised learning, which needs large labeled data that are costly,\nslow to create, and domain dependent; they also generalize poorly and often\nrequire retraining for each task. This study tests whether prompt based large\nlanguage models can reduce data needs. We benchmark several models and\nprompting styles (zero shot, few shot, persona, and chain of thought) across\nmultiple tasks on two English datasets, PROMISE and SecReq. For each task we\ncompare model prompt configurations and then compare the best LLM setups with a\nstrong fine tuned transformer baseline. Results show that prompt based LLMs,\nespecially with few shot prompts, can match or exceed the baseline. Adding a\npersona, or persona plus chain of thought, can yield further gains. We conclude\nthat prompt based LLMs are a practical and scalable option that reduces\ndependence on large annotations and can improve generalizability across tasks."}
{"id": "2509.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples."}
{"id": "2509.13896", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13896", "abs": "https://arxiv.org/abs/2509.13896", "authors": ["Shalini Chakraborty", "Lola Burgueño", "Nathalie Moreno", "Javier Troya", "Paula Muñoz"], "title": "Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education", "comment": "8 pages, Educators Symposium at MODELS 2025", "summary": "Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in\nsoftware modeling education, embraced by both students and educators. As GenAI\nassists with interpreting requirements, formalizing models, and translating\nstudents' mental models into structured notations, it increasingly shapes core\nlearning outcomes such as domain comprehension, diagrammatic thinking, and\nmodeling fluency without clear ethical oversight or pedagogical guidelines.\nYet, the ethical implications of this integration remain underexplored.\n  In this paper, we conduct a systematic literature review across six major\ndigital libraries in computer science (ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to\nidentify studies discussing the ethical aspects of GenAI in software modeling\neducation, including responsibility, fairness, transparency, diversity, and\ninclusion among others.\n  Out of 1,386 unique papers initially retrieved, only three explicitly\naddressed ethical considerations. This scarcity highlights the critical absence\nof ethical discourse surrounding GenAI in modeling education and raises urgent\nquestions about the responsible integration of AI in modeling curricula, as\nwell as it evinces the pressing need for structured ethical frameworks in this\nemerging educational landscape. We examine these three studies and explore the\nemerging research opportunities as well as the challenges that have arisen in\nthis field."}
{"id": "2509.13696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks."}
{"id": "2509.13941", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13941", "abs": "https://arxiv.org/abs/2509.13941", "authors": ["Simiao Liu", "Fang Liu", "Liehao Li", "Xin Tan", "Yinghao Zhu", "Xiaoli Lian", "Li Zhang"], "title": "An Empirical Study on Failures in Automated Issue Solving", "comment": null, "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign."}
{"id": "2509.13702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality."}
{"id": "2509.13942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13942", "abs": "https://arxiv.org/abs/2509.13942", "authors": ["Duc Minh Ha", "Phu Trac Kien", "Tho Quan", "Anh Nguyen-Duc"], "title": "Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation", "comment": null, "summary": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation."}
{"id": "2509.13706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset."}
{"id": "2509.14093", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14093", "abs": "https://arxiv.org/abs/2509.14093", "authors": ["Kerui Huang", "Shuhan Liu", "Xing Hu", "Tongtong Xu", "Lingfeng Bao", "Xin Xia"], "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."}
{"id": "2509.13723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76."}
{"id": "2509.13429", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13429", "abs": "https://arxiv.org/abs/2509.13429", "authors": ["Anthony Arnold", "Mark Marron"], "title": "Catalpa: GC for a Low-Variance Software Stack", "comment": null, "summary": "The performance of an application/runtime is usually conceptualized as a\ncontinuous function where, the lower the amount of memory/time used on a given\nworkload, then the better the compiler/runtime is. However, in practice, good\nperformance of an application is viewed as more of a binary function - either\nthe application responds in under, say 100 ms, and is fast enough for a user to\nbarely notice, or it takes a noticeable amount of time, leaving the user\nwaiting and potentially abandoning the task. Thus, performance really means how\noften the application is fast enough to be usable, leading industrial\ndevelopers to focus on the 95th and 99th percentile tail-latencies as heavily,\nor moreso, than average response time. Our vision is to create a software stack\nthat actively supports these needs via programming language and runtime system\ndesign. In this paper we present a novel garbage-collector design, the Catalpa\ncollector, for the Bosque programming language and runtime. This allocator is\ndesigned to minimize latency and variability while maintaining high-throughput\nand incurring small memory overheads. To achieve these goals we leverage\nvarious features of the Bosque language, including immutability and\nreference-cycle freedom, to construct a collector that has bounded collection\npauses, incurs fixed-constant memory overheads, and does not require any\nbarriers or synchronization with application code."}
{"id": "2509.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs."}
{"id": "2509.13699", "categories": ["cs.LO", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13699", "abs": "https://arxiv.org/abs/2509.13699", "authors": ["Max Barth", "Marie-Christine Jakobs"], "title": "Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement", "comment": null, "summary": "Automatic software verification is a valuable means for software quality\nassurance. However, automatic verification and in particular software model\nchecking can be time-consuming, which hinders their practical applicability\ne.g., the use in continuous integration. One solution to address the issue is\nto reduce the response time of the verification procedure by leveraging today's\nmulti-core CPUs.\n  In this paper, we propose a solution to parallelize trace abstraction, an\nabstraction-based approach to software model checking. The underlying idea of\nour approach is to parallelize the abstraction refinement. More concretely, our\napproach analyzes different traces (syntactic program paths) that could violate\nthe safety property in parallel. We realize our parallelized version of trace\nabstraction in the verification tool Ulti mate Automizer and perform a thorough\nevaluation. Our evaluation shows that our parallelization is more effective\nthan sequential trace abstraction and can provide results significantly faster\non many time-consuming tasks. Also, our approach is more effective than DSS, a\nrecent parallel approach to abstraction-based software model checking."}
{"id": "2509.13775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning."}
{"id": "2509.13790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning."}
{"id": "2509.13803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura García-Sardiña", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias."}
{"id": "2509.13813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy."}
{"id": "2509.13814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks."}
{"id": "2509.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage."}
{"id": "2509.13869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability."}
{"id": "2509.13879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER."}
{"id": "2509.13888", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER"}
{"id": "2509.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities."}
{"id": "2509.13930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior."}
{"id": "2509.13980", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments."}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC."}
{"id": "2509.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning."}
{"id": "2509.14008", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14008", "abs": "https://arxiv.org/abs/2509.14008", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP."}
{"id": "2509.14023", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14023", "abs": "https://arxiv.org/abs/2509.14023", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks."}
{"id": "2509.14031", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14031", "abs": "https://arxiv.org/abs/2509.14031", "authors": ["Paweł Mąka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively."}
{"id": "2509.14034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14034", "abs": "https://arxiv.org/abs/2509.14034", "authors": ["Zijie Lin", "Bryan Hooi"], "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems."}
{"id": "2509.14036", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14036", "abs": "https://arxiv.org/abs/2509.14036", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality."}
{"id": "2509.14128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14128", "abs": "https://arxiv.org/abs/2509.14128", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters."}
{"id": "2509.14161", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14161", "abs": "https://arxiv.org/abs/2509.14161", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs."}
{"id": "2509.14171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14171", "abs": "https://arxiv.org/abs/2509.14171", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes."}
{"id": "2509.14180", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts."}
{"id": "2509.14197", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14197", "abs": "https://arxiv.org/abs/2509.14197", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts."}
{"id": "2509.14233", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14233", "abs": "https://arxiv.org/abs/2509.14233", "authors": ["Alejandro Hernández-Cano", "Alexander Hägele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank Ďurech", "Ido Hakimi", "Juan García Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabolčec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian Bösch", "Maximilian Böther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "María Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike Lübeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendoncça", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "Léo Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tramèr", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension."}
{"id": "2509.13941", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13941", "abs": "https://arxiv.org/abs/2509.13941", "authors": ["Simiao Liu", "Fang Liu", "Liehao Li", "Xin Tan", "Yinghao Zhu", "Xiaoli Lian", "Li Zhang"], "title": "An Empirical Study on Failures in Automated Issue Solving", "comment": null, "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign."}
{"id": "2509.14093", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14093", "abs": "https://arxiv.org/abs/2509.14093", "authors": ["Kerui Huang", "Shuhan Liu", "Xing Hu", "Tongtong Xu", "Lingfeng Bao", "Xin Xia"], "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."}
