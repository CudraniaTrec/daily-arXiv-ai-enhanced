{"id": "2510.09128", "categories": ["cs.DM", "cs.CC", "math.CO", "05C75, 05C60, 05C63", "F.2.2"], "pdf": "https://arxiv.org/pdf/2510.09128", "abs": "https://arxiv.org/abs/2510.09128", "authors": ["Manuel Bodirsky", "Santiago Guzm\u00e1n-Pro"], "title": "A CSP approach to Graph Sandwich Problems", "comment": "31 pages; accepted for publication in the proceedings of SODA 2026", "summary": "The \\emph{Sandwich Problem} (SP) for a graph class $\\calC$ is the following\ncomputational problem. The input is a pair of graphs $(V,E_1)$ and $(V,E_2)$\nwhere $E_1\\subseteq E_2$, and the task is to decide whether there is an edge\nset $E$ where $E_1\\subseteq E \\subseteq E_2$ such that the graph $(V,E)$\nbelongs to $\\calC$. In this paper we show that many SPs correspond to the\nconstraint satisfaction problem (CSP) of an infinite $2$-edge-coloured graph\n$H$. We then notice that several known complexity results for SPs also follow\nfrom general complexity classifications of infinite-domain CSPs, suggesting a\nfruitful application of the theory of CSPs to complexity classifications of\nSPs. We strengthen this evidence by using basic tools from constraint\nsatisfaction theory to propose new complexity results of the SP for several\ngraph classes including line graphs of multigraphs, line graphs of bipartite\nmultigraphs, $K_k$-free perfect graphs, and classes described by forbidding\nfinitely many induced subgraphs, such as $\\{I_4,P_4\\}$-free graphs, settling an\nopen problem of Alvarado, Dantas, and Rautenbach (2019). We also construct a\ngraph sandwich problem which is in coNP, but neither in P nor coNP-complete\n(unless P = coNP).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86Sandwich Problem\u5728\u56fe\u8bba\u4e2d\u7684\u590d\u6742\u6027\u5f52\u7c7b\uff0c\u63d0\u51fa\u5c06\u5176\u4e0e\u65e0\u9650\u57dfCSP\u7406\u8bba\u5173\u8054\uff0c\u5e76\u5229\u7528\u6b64\u5173\u7cfb\u89e3\u51b3\u4e86\u6570\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u4f5c\u8005\u8fd8\u53d1\u73b0\u4e86\u4e00\u4e2a\u7279\u6b8a\u7684\u56fe\u4e09\u660e\u6cbb\u95ee\u9898\u5c5e\u4e8ecoNP\u4f46\u4f3c\u4e4e\u4e0d\u5c5e\u4e8eP\u6216coNP\u5b8c\u5168\uff0c\u62d3\u5c55\u4e86\u76f8\u5173\u7406\u8bba\u7684\u5e94\u7528\u548c\u7406\u89e3\u3002", "motivation": "Sandwich Problem (SP) \u662f\u4e00\u79cd\u5173\u4e8e\u56fe\u7c7b\u7684\u8ba1\u7b97\u95ee\u9898\u3002\u9488\u5bf9\u5df2\u77e5\u5b50\u56fe\u548c\u8d85\u56fe\uff0c\u5224\u5b9a\u662f\u5426\u5b58\u5728\u8be5\u7c7b\u7684\u4e2d\u95f4\u56fe\u3002\u5df2\u6709\u7814\u7a76\u5bf9\u90e8\u5206SP\u7684\u590d\u6742\u6027\u8fdb\u884c\u4e86\u63a2\u8ba8\uff0c\u4f46\u672a\u7cfb\u7edf\u5730\u5f52\u7eb3\u5176\u4e0e\u9650\u5236\u6ee1\u8db3\u95ee\u9898\uff08CSP\uff09\u7406\u8bba\u7684\u5173\u7cfb\uff0c\u4e5f\u4ecd\u6709\u5f00\u653e\u95ee\u9898\u5f85\u89e3\u51b3\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528CSP\u7406\u8bba\u6765\u63a8\u8fdb\u5bf9SP\u590d\u6742\u6027\u8d28\u7684\u5f52\u7c7b\u548c\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5c06\u591a\u79cdSP\u8f6c\u5316\u4e3a\u65e0\u9650\u57df\u7684CSP\uff0c\u5e76\u5e94\u7528CSP\u7406\u8bba\u4e2d\u7684\u57fa\u672c\u5de5\u5177\uff0c\u7ee7\u800c\u63a8\u5bfc\u51faSP\u5728\u4e0d\u540c\u56fe\u7c7b\u522b\u4e2d\u7684\u590d\u6742\u6027\u5f52\u7c7b\uff08\u5982\u591a\u91cd\u56fe\u7684\u7ebf\u56fe\u3001\u53cc\u90e8\u591a\u91cd\u56fe\u7ebf\u56fe\u3001Kk-\u81ea\u7531\u5b8c\u7f8e\u56fe\u7b49\uff09\uff0c\u5e76\u89e3\u51b3\u7279\u5b9a\u5b50\u56fe\u7c7b\u7684\u5f52\u7c7b\u95ee\u9898\u3002", "result": "\u4f5c\u8005\u8bc1\u660e\u4e86\u8bb8\u591aSP\u53ef\u4ee5\u5f52\u7ea6\u4e3a\u65e0\u9650\u4e8c\u8fb9\u7740\u8272\u56fe\u7684CSP\uff1b\u5229\u7528CSP\u590d\u6742\u6027\u5f52\u7c7b\uff0c\u7ed9\u51fa\u4e86SP\u5728\u82e5\u5e72\u65b0\u7684\u56fe\u7c7b\u522b\u4e2d\u7684\u590d\u6742\u6027\u7ed3\u679c\uff0c\u5e76\u89e3\u51b3\u4e86\u7531Alvarado\u7b49\u4eba\u63d0\u51fa\u7684\u5f00\u653e\u95ee\u9898\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u8fd8\u6784\u9020\u4e86\u4e00\u4e2a\u5c5e\u4e8ecoNP\u4f46\u4e0d\u662fP\u6216coNP\u5b8c\u5168\u7684\u56fe\u4e09\u660e\u6cbb\u95ee\u9898\uff08\u9664\u975eP=coNP\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c06SP\u7eb3\u5165CSP\u6846\u67b6\uff0c\u4f5c\u8005\u6df1\u5316\u4e86\u5bf9Sandwich Problem\u590d\u6742\u6027\u5f52\u7c7b\u7684\u7406\u89e3\uff0c\u5bf9\u82e5\u5e72\u56fe\u7c7b\u522bSP\u590d\u6742\u6027\u4f5c\u51fa\u65b0\u5224\u5b9a\uff0c\u5e76\u89e3\u51b3\u4e86\u76f8\u5173\u5f00\u653e\u95ee\u9898\uff0c\u663e\u793a\u4e86CSP\u7406\u8bba\u5728SP\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.08988", "categories": ["cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.08988", "abs": "https://arxiv.org/abs/2510.08988", "authors": ["Lan Zhang", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "MASA: LLM-Driven Multi-Agent Systems for Autoformalization", "comment": "EMNLP 2025 Demo camera-ready. Code and data are available at:\n  https://github.com/lanzhang128/multi_agent_autoformalization", "summary": "Autoformalization serves a crucial role in connecting natural language and\nformal reasoning. This paper presents MASA, a novel framework for building\nmulti-agent systems for autoformalization driven by Large Language Models\n(LLMs). MASA leverages collaborative agents to convert natural language\nstatements into their formal representations. The architecture of MASA is\ndesigned with a strong emphasis on modularity, flexibility, and extensibility,\nallowing seamless integration of new agents and tools to adapt to a\nfast-evolving field. We showcase the effectiveness of MASA through use cases on\nreal-world mathematical definitions and experiments on formal mathematics\ndatasets. This work highlights the potential of multi-agent systems powered by\nthe interaction of LLMs and theorem provers in enhancing the efficiency and\nreliability of autoformalization, providing valuable insights and support for\nresearchers and practitioners in the field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MASA\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e14\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u7cfb\u7edf\u3002MASA\u652f\u6301\u7075\u6d3b\u6269\u5c55\u548c\u5de5\u5177\u96c6\u6210\uff0c\u5728\u6570\u5b66\u5b9a\u4e49\u548c\u5f62\u5f0f\u5316\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u7814\u7a76\u8868\u660e\uff0cMASA\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u5e26\u6765\u65b0\u65b9\u6cd5\u4e0e\u542f\u793a\u3002", "motivation": "\u81ea\u52a8\u5f62\u5f0f\u5316\u65e8\u5728\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u4e0e\u5f62\u5f0f\u5316\u63a8\u7406\uff0c\u5b9e\u73b0\u5c06\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\u81ea\u52a8\u8f6c\u5316\u4e3a\u53ef\u8fdb\u884c\u903b\u8f91\u63a8\u7406\u7684\u5f62\u5f0f\u5316\u8868\u793a\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u53ca\u63a8\u7406\u9886\u57df\u7684\u663e\u8457\u80fd\u529b\u63d0\u5347\uff0c\u5982\u4f55\u5c06LLM\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7ed3\u5408\u4ee5\u63d0\u9ad8\u81ea\u52a8\u5f62\u5f0f\u5316\u6548\u7387\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MASA\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u667a\u80fd\u4f53\u5c06\u81ea\u7136\u8bed\u8a00\u9648\u8ff0\u8f6c\u4e3a\u5176\u5f62\u5f0f\u5316\u8868\u793a\u3002MASA\u6846\u67b6\u5177\u6709\u9ad8\u6a21\u5757\u5316\u3001\u7075\u6d3b\u6027\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u65b9\u4fbf\u5730\u96c6\u6210\u65b0\u667a\u80fd\u4f53\u548c\u5de5\u5177\uff0c\u9002\u5e94\u81ea\u52a8\u5f62\u5f0f\u5316\u9886\u57df\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u6570\u5b66\u5b9a\u4e49\u548c\u5f62\u5f0f\u5316\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9645\u6848\u4f8b\u548c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86MASA\u5728\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u3001LLM\u4e0e\u5b9a\u7406\u8bc1\u660e\u5668\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0eLLM\u53ca\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u5e26\u6765\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u5e76\u63ed\u793a\u4e86\u672a\u6765\u7684\u53d1\u5c55\u6f5c\u529b\u3002"}}
{"id": "2510.09292", "categories": ["cs.LO", "F.3.1"], "pdf": "https://arxiv.org/pdf/2510.09292", "abs": "https://arxiv.org/abs/2510.09292", "authors": ["Flavio Ascari", "Roberto Bruni", "Roberta Gori", "Azalea Raad"], "title": "U-Turn: Enhancing Incorrectness Analysis by Reversing Direction", "comment": "35 pages, 13 figures. Conditionally accepted at POPL26", "summary": "O'Hearn's Incorrectness Logic (IL) has sparked renewed interest in static\nanalyses that aim to detect program errors rather than prove their absence,\nthereby avoiding false alarms -- a critical factor for practical adoption in\nindustrial settings. As new incorrectness logics emerge to capture diverse\nerror-related properties, a key question arises: can the combination of\n(in)correctness techniques enhance precision, expressiveness, automation, or\nscalability? Notable frameworks, such as outcome logic, UNTer, local\ncompleteness logic, and exact separation logic, unify multiple analyses within\na single proof system. In this work, we adopt a complementary strategy. Rather\nthan designing a unified logic, we combine IL, which identifies reachable error\nstates, with Sufficient Incorrectness Logic (SIL), which finds input states\npotentially leading to those errors. As a result, we get a more informative and\neffective analysis than either logic in isolation. Rather than naively\nsequencing them, our key innovation is reusing heuristic choices from the first\nanalysis to steer the second. In fact, both IL and SIL rely on\nunder-approximation and thus their automation legitimates heuristics that avoid\nexhaustive path enumeration (e.g., selective disjunct pruning, loop unrolling).\nConcretely, we instrument the second logic's proof rules with derivations\ncoming from the first to inductively guide rule selection and application. To\nour knowledge, this is the first rule format enabling such inter-analysis\ninstrumentation. This combined analysis aids debugging and testing by revealing\nboth reachable errors and their causes, and opens new avenues for embedding\nincorrectness insights into (a new kind of) scalable, expressive, automated\ncode contracts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7ed3\u5408\u4e24\u79cd\u9519\u8bef\u903b\u8f91\uff08IL\u548cSIL\uff09\uff0c\u521b\u65b0\u6027\u5730\u7528\u4e00\u79cd\u5206\u6790\u4ea7\u751f\u7684\u542f\u53d1\u4fe1\u606f\u6307\u5bfc\u53e6\u4e00\u79cd\u5206\u6790\uff0c\u4ece\u800c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u53d1\u73b0\u7a0b\u5e8f\u9519\u8bef\u53ca\u5176\u539f\u56e0\uff0c\u51cf\u5c11\u8bef\u62a5\uff0c\u5e76\u652f\u6301\u66f4\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u7684\u4ee3\u7801\u5408\u7ea6\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u5173\u6ce8\u8bc1\u660e\u7a0b\u5e8f\u65e0\u8bef\uff0c\u8fc7\u591a\u8bef\u62a5\u5236\u7ea6\u5b9e\u9645\u4f7f\u7528\u3002IL\u63a8\u52a8\u91cd\u89c6\u5b9e\u9645\u9519\u8bef\u68c0\u6d4b\u3002\u968f\u7740\u591a\u6837\u9519\u8bef\u903b\u8f91\u51fa\u73b0\uff0c\u5982\u4f55\u7ed3\u5408\u4e0d\u540c\u65b9\u6cd5\u4ee5\u63d0\u5347\u5206\u6790\u80fd\u529b\uff0c\u662f\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u4e0d\u662f\u8bbe\u8ba1\u7edf\u4e00\u903b\u8f91\uff0c\u800c\u662f\u5c06IL\uff08\u8bc6\u522b\u53ef\u8fbe\u9519\u8bef\u72b6\u6001\uff09\u4e0eSIL\uff08\u786e\u5b9a\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7684\u8f93\u5165\u72b6\u6001\uff09\u7ed3\u5408\uff0c\u5e76\u7528\u524d\u8005\u5206\u6790\u4ea7\u751f\u7684\u63a8\u5bfc\u6307\u5bfc\u540e\u8005\u89c4\u5219\u9009\u62e9\u548c\u5e94\u7528\uff0c\u5b9e\u73b0\u4e92\u8865\u63d0\u5347\u3002\u5177\u4f53\u5b9e\u73b0\u901a\u8fc7\u89c4\u5219\u683c\u5f0f\u521b\u65b0\uff0c\u5b9e\u73b0\u5206\u6790\u95f4\u7684\u4fe1\u606f\u5171\u4eab\u3002", "result": "\u63d0\u51fa\u9996\u4e2a\u80fd\u591f\u901a\u8fc7\u89c4\u5219\u683c\u5f0f\u5b9e\u73b0\u4e92\u8865\u4eea\u5668\u5316\u7684\u5206\u6790\u6846\u67b6\u3002\u5b9e\u9a8c\u8868\u660e\u7ed3\u5408IL\u4e0eSIL\u5e76\u4f20\u9012\u5206\u6790\u4fe1\u606f\uff0c\u53ef\u63d0\u5347\u5206\u6790\u7684\u8868\u8fbe\u529b\u548c\u81ea\u52a8\u5316\uff0c\u5bf9\u5de5\u4e1a\u5b9e\u8df5\u6709\u4fc3\u8fdb\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408IL\u548cSIL\uff0c\u5e76\u4e14\u7528\u9996\u8f6e\u5206\u6790\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u6765\u6307\u5bfc\u7b2c\u4e8c\u8f6e\u5206\u6790\uff0c\u80fd\u5f97\u5230\u66f4\u6709\u4fe1\u606f\u91cf\u4e14\u6709\u6548\u7684\u9519\u8bef\u5206\u6790\u65b9\u6cd5\u3002\u6b64\u7ec4\u5408\u5206\u6790\u4e0d\u4ec5\u63ed\u793a\u53ef\u8fbe\u9519\u8bef\uff0c\u8fd8\u63ed\u793a\u5bfc\u81f4\u9519\u8bef\u7684\u8f93\u5165\u72b6\u6001\uff0c\u5bf9\u8c03\u8bd5\u3001\u6d4b\u8bd5\u548c\u81ea\u52a8\u5316\u5408\u7ea6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.08726", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08726", "abs": "https://arxiv.org/abs/2510.08726", "authors": ["Yifan Zhao", "Egan Johnson", "Prasanth Chatarasi", "Vikram Adve", "Sasa Misailovic"], "title": "Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs", "comment": null, "summary": "Operator fusion has become a key optimization for deep learning, which\ncombines multiple deep learning operators to improve data reuse and reduce\nglobal memory transfers. However, existing tensor compilers struggle to fuse\ncomplex reduction computations involving loop-carried dependencies, such as\nattention mechanisms.\n  The paper introduces Neptune, a tensor compiler for advanced operator fusion\nfor sequences of reduction operators. Neptune presents a new approach for\nadvanced operator fusion, which intentionally breaks some existing dependencies\nand compensates by constructing algebraic correction expressions that allow the\nkernel to produce the correct result.\n  On ten attention-based benchmarks, Neptune, starting from simple attention\ncode and a high-level scheduling template, outperforms existing compilers like\nTriton, TVM, and FlexAttention, including Triton-based implementations of\nFlashAttention. Across four different GPU architectures from NVIDIA and AMD,\nNeptune-generated kernels have average speedup of $1.35\\times$ over the next\nbest alternative, demonstrating its effectiveness for deep learning workloads.", "AI": {"tldr": "Neptune\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u5b50\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63\u4f9d\u8d56\u9ad8\u6548\u878d\u5408\u590d\u6742\u5f52\u7ea6\u7b97\u5b50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6ce8\u610f\u529b\u673a\u5236\u76f8\u5173\u5de5\u4f5c\u8d1f\u8f7d\u5728GPU\u4e0a\u7684\u6267\u884c\u6548\u7387\uff0c\u4f18\u4e8e\u4e3b\u6d41\u5f20\u91cf\u7f16\u8bd1\u5668\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u7f16\u8bd1\u5668\u96be\u4ee5\u878d\u5408\u6d89\u53ca\u5faa\u73af\u4f9d\u8d56\u7684\u590d\u6742\u5f52\u7ea6\u8ba1\u7b97\uff0c\u7279\u522b\u662f\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5f71\u54cd\u6027\u80fd\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u6253\u7834\u90e8\u5206\u4f9d\u8d56\u5173\u7cfb\u5e76\u6784\u9020\u4ee3\u6570\u4fee\u6b63\u8868\u8fbe\u5f0f\uff0c\u5b9e\u73b0\u590d\u6742\u5f52\u7ea6\u7b97\u5b50\u7684\u878d\u5408\u3002", "result": "\u572810\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0d\u540cGPU\u67b6\u6784\u4e0b\uff0cNeptune\u751f\u6210\u7684\u5185\u6838\u76f8\u8f83\u4e8e\u4e3b\u6d41\u7f16\u8bd1\u5668\uff08\u5982Triton\u3001TVM\u7b49\uff09\u5b9e\u73b0\u4e86\u5e73\u57471.35\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "Neptune\u5728\u9ad8\u7ea7\u7b97\u5b50\u878d\u5408\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6ce8\u610f\u529b\u673a\u5236\u7b97\u5b50\u7684\u6027\u80fd\u3002"}}
{"id": "2510.08576", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08576", "abs": "https://arxiv.org/abs/2510.08576", "authors": ["Justus Flerlage", "Alexander Acker", "Odej Kao"], "title": "Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions", "comment": null, "summary": "Large Language Models (LLMs) have emerged as transformative tools for natural\nlanguage understanding and user intent resolution, enabling tasks such as\ntranslation, summarization, and, increasingly, the orchestration of complex\nworkflows. This development signifies a paradigm shift from conventional,\nGUI-driven user interfaces toward intuitive, language-first interaction\nparadigms. Rather than manually navigating applications, users can articulate\ntheir objectives in natural language, enabling LLMs to orchestrate actions\nacross multiple applications in a dynamic and contextual manner. However,\nextant implementations frequently rely on cloud-based proprietary models, which\nintroduce limitations in terms of privacy, autonomy, and scalability. For\nlanguage-first interaction to become a truly robust and trusted interface\nparadigm, local deployment is not merely a convenience; it is an imperative.\nThis limitation underscores the importance of evaluating the feasibility of\nlocally deployable, open-source, and open-access LLMs as foundational\ncomponents for future intent-based operating systems. In this study, we examine\nthe capabilities of several open-source and open-access models in facilitating\nuser intention resolution through machine assistance. A comparative analysis is\nconducted against OpenAI's proprietary GPT-4-based systems to assess\nperformance in generating workflows for various user intentions. The present\nstudy offers empirical insights into the practical viability, performance\ntrade-offs, and potential of open LLMs as autonomous, locally operable\ncomponents in next-generation operating systems. The results of this study\ninform the broader discussion on the decentralization and democratization of AI\ninfrastructure and point toward a future where user-device interaction becomes\nmore seamless, adaptive, and privacy-conscious through locally embedded\nintelligence.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u672c\u5730\u53ef\u90e8\u7f72\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u610f\u56fe\u89e3\u6790\u53ca\u5de5\u4f5c\u6d41\u7f16\u6392\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u64cd\u4f5c\u7cfb\u7edf\u6838\u5fc3\u7ec4\u4ef6\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI\u57fa\u7840\u8bbe\u65bd\u7684\u53bb\u4e2d\u5fc3\u5316\u4e0e\u9690\u79c1\u53cb\u597d\u5316\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u591a\u4f9d\u8d56\u4e8e\u4e91\u7aef\u53ca\u5546\u4e1a\u95ed\u6e90\u65b9\u6848\uff0c\u5e26\u6765\u9690\u79c1\u3001\u81ea\u6cbb\u548c\u53ef\u6269\u5c55\u6027\u7b49\u9650\u5236\u3002\u4e3a\u4e86\u63a8\u52a8\u4ee5\u81ea\u7136\u8bed\u8a00\u4e3a\u4e3b\u5bfc\u7684\u4eba\u673a\u4ea4\u4e92\u8d70\u5411\u666e\u53ca\u548c\u53ef\u4fe1\uff0c\u9700\u8981\u9a8c\u8bc1\u80fd\u5426\u672c\u5730\u90e8\u7f72\u5f00\u6e90\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u4e3b\u4e0e\u9690\u79c1\u53cb\u597d\u7684\u64cd\u4f5c\u7cfb\u7edf\u3002", "method": "\u5bf9\u591a\u4e2a\u5f00\u6e90\u3001\u672c\u5730\u90e8\u7f72\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0eOpenAI\u7684GPT-4\u4e3a\u4ee3\u8868\u7684\u4e91\u7aef\u95ed\u6e90\u6a21\u578b\u8fdb\u884c\u4e86\u610f\u56fe\u89e3\u6790\u548c\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u80fd\u529b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u548c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5f00\u6e90\u672c\u5730LLM\u5728\u5b9e\u9645\u7528\u6237\u610f\u56fe\u89e3\u6790\u548c\u64cd\u4f5c\u7cfb\u7edf\u5de5\u4f5c\u6d41\u7f16\u6392\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u548c\u6c11\u4e3b\u5316AI\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u6709\u529b\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "\u5f00\u653e\u6e90\u4ee3\u7801\u548c\u5f00\u653e\u8bbf\u95ee\u7684\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80dc\u4efb\u7528\u6237\u610f\u56fe\u89e3\u6790\u548c\u5de5\u4f5c\u6d41\u81ea\u52a8\u7f16\u6392\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u5b9e\u7528\u53ef\u884c\u6027\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u6027\u80fd\u6307\u6807\u4e0a\u4e0e\u5546\u4e1a\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4\uff09\u4ecd\u5b58\u5dee\u8ddd\u3002"}}
{"id": "2510.08588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08588", "abs": "https://arxiv.org/abs/2510.08588", "authors": ["Ritesh Mehta"], "title": "Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6", "comment": "Paper published to CLEF 2025 CEUR-WS", "summary": "Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in\nlarge-scale biomedical semantic indexing and question answering), is crucial\nfor extracting information from scientific literature but faces hurdles such as\ndistinguishing between similar entity types like genes and chemicals. This\nstudy evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a\ntargeted dictionary-based post-processing strategy to address common\nmisclassifications. While this post-processing approach demonstrated notable\nimprovement on our development set, increasing the micro F1-score from a\nbaseline of 0.79 to 0.83, this enhancement did not generalize to the blind test\nset, where the post-processed model achieved a micro F1-score of 0.77 compared\nto the baselines 0.79. We also discuss insights gained from exploring\nalternative methodologies, including Conditional Random Fields. This work\nhighlights the potential of dictionary-based refinement for pre-trained BioNER\nmodels but underscores the critical challenge of overfitting to development\ndata and the necessity of ensuring robust generalization for real-world\napplicability.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u5178\u7684\u540e\u5904\u7406\u7b56\u7565\u6765\u63d0\u5347BiomNER\u6a21\u578b\u8bc6\u522b\u8868\u73b0\uff0c\u4ec5\u5728\u5f00\u53d1\u96c6\u4e0a\u6548\u679c\u663e\u8457\uff0c\u800c\u5728\u76f2\u6d4b\u96c6\u4e0a\u672a\u80fd\u6cdb\u5316\uff0c\u6307\u51fa\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u4ecd\u662f\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\uff0c\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u4fe1\u606f\uff08\u5982\u57fa\u56e0\u548c\u5316\u5b66\u7269\u8d28\u5b9e\u4f53\uff09\u5bf9\u4e0b\u6e38\u4efb\u52a1\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e0d\u540c\u5b9e\u4f53\u7c7b\u578b\u4e4b\u95f4\u5f80\u5f80\u96be\u4ee5\u533a\u5206\uff0c\u5bb9\u6613\u51fa\u73b0\u8bc6\u522b\u9519\u8bef\uff0c\u8fd9\u5bf9\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7cfb\u7edf\u63d0\u51fa\u6311\u6218\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86GLiNER-BioMed\u6a21\u578b\u5728BioASQ\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u5178\u7684\u540e\u5904\u7406\u7b56\u7565\uff0c\u65e8\u5728\u7ea0\u6b63\u6a21\u578b\u5e38\u89c1\u7684\u5b9e\u4f53\u5206\u7c7b\u9519\u8bef\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u6761\u4ef6\u968f\u673a\u573a\u7b49\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u5b57\u5178\u7684\u540e\u5904\u7406\u65b9\u6cd5\u5728\u5f00\u53d1\u96c6\u4e0a\u5c06micro F1\u5206\u6570\u4ece0.79\u63d0\u5347\u81f30.83\uff0c\u4f46\u5728\u76f2\u6d4b\u8bd5\u96c6\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u5206\u6570\u4ece0.79\u4e0b\u964d\u52300.77\uff0c\u672a\u80fd\u5b9e\u73b0\u6cdb\u5316\u3002", "conclusion": "\u5b57\u5178\u589e\u5f3a\u7684\u540e\u5904\u7406\u7b56\u7565\u80fd\u63d0\u5347BiomNER\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u5b58\u5728\u8fc7\u62df\u5408\u5f00\u53d1\u96c6\u3001\u96be\u4ee5\u6cdb\u5316\u81f3\u5b9e\u9645\u5e94\u7528\u7684\u98ce\u9669\u3002\u5b9e\u9645\u90e8\u7f72\u65f6\u9700\u5173\u6ce8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.09472", "categories": ["cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.09472", "abs": "https://arxiv.org/abs/2510.09472", "authors": ["Manuel Vargas Guzm\u00e1n", "Jakub Szymanik", "Maciej Malicki"], "title": "Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic", "comment": null, "summary": "Despite the remarkable progress in neural models, their ability to\ngeneralize, a cornerstone for applications like logical reasoning, remains a\ncritical challenge. We delineate two fundamental aspects of this ability:\ncompositionality, the capacity to abstract atomic logical rules underlying\ncomplex inferences, and recursiveness, the aptitude to build intricate\nrepresentations through iterative application of inference rules. In the\nliterature, these two aspects are often confounded together under the umbrella\nterm of generalization. To sharpen this distinction, we investigated the\nlogical generalization capabilities of pre-trained large language models (LLMs)\nusing the syllogistic fragment as a benchmark for natural language reasoning.\nThough simple, this fragment provides a foundational yet expressive subset of\nformal logic that supports controlled evaluation of essential reasoning\nabilities. Our findings reveal a significant disparity: while LLMs demonstrate\nreasonable proficiency in recursiveness, they struggle with compositionality.\nTo overcome these limitations and establish a reliable logical prover, we\npropose a hybrid architecture integrating symbolic reasoning with neural\ncomputation. This synergistic interaction enables robust and efficient\ninference, neural components accelerate processing, while symbolic reasoning\nensures completeness. Our experiments show that high efficiency is preserved\neven with relatively small neural components. As part of our proposed\nmethodology, this analysis gives a rationale and highlights the potential of\nhybrid models to effectively address key generalization barriers in neural\nreasoning systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u9012\u5f52\u6027\u4f18\u4e8e\u7ec4\u5408\u6027\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u63a8\u7406\u7684\u6df7\u5408\u6a21\u578b\uff0c\u53ef\u6709\u6548\u63d0\u5347\u63a8\u7406\u6548\u679c\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u795e\u7ecf\u63a8\u7406\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u903b\u8f91\u63a8\u7406\u573a\u666f\u4e2d\u3002\u4f5c\u8005\u5e0c\u671b\u5398\u6e05\u903b\u8f91\u6cdb\u5316\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u2014\u2014\u7ec4\u5408\u6027\u4e0e\u9012\u5f52\u6027\uff0c\u5e76\u63d0\u51fa\u65b9\u6cd5\u63d0\u5347\u795e\u7ecf\u63a8\u7406\u7cfb\u7edf\u7684\u6cdb\u5316\u8868\u73b0\u3002", "method": "\u5b9e\u9a8c\u9009\u7528syllogistic fragment\u4f5c\u4e3a\u63a8\u7406\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u5206\u6790LLMs\u7684\u903b\u8f91\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u7ed3\u5408\u795e\u7ecf\u63a8\u7406\u548c\u7b26\u53f7\u63a8\u7406\u7684\u6df7\u5408\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLLMs\u5728\u9012\u5f52\u6027\u63a8\u7406\u65b9\u9762\u6709\u8f83\u597d\u8868\u73b0\uff0c\u4f46\u5728\u7ec4\u5408\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002\u6240\u63d0\u6df7\u5408\u67b6\u6784\u80fd\u5728\u7528\u8f83\u5c0f\u795e\u7ecf\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u9ad8\u63a8\u7406\u6548\u7387\u548c\u5b8c\u5907\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5bf9\u4e8e\u9012\u5f52\u6027\u63a8\u7406\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5bf9\u7ec4\u5408\u6027\u63a8\u7406\u80fd\u529b\u8f83\u5f31\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u7684\u6df7\u5408\u67b6\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u5b8c\u5907\u6027\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u6cdb\u5316\u4e0a\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.08889", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08889", "abs": "https://arxiv.org/abs/2510.08889", "authors": ["Songlin Jia", "Craig Liu", "Siyuan He", "Haotian Deng", "Yuyan Bao", "Tiark Rompf"], "title": "Typestate via Revocable Capabilities", "comment": null, "summary": "Managing stateful resources safely and expressively is a longstanding\nchallenge in programming languages, especially in the presence of aliasing.\nWhile scope-based constructs such as Java's synchronized blocks offer ease of\nreasoning, they restrict expressiveness and parallelism. Conversely,\nimperative, flow-sensitive management enables fine-grained control but demands\nsophisticated typestate analyses and often burdens programmers with explicit\nstate tracking.\n  In this work, we present a novel approach that unifies the strengths of both\nparadigms by extending flow-insensitive capability mechanisms into\nflow-sensitive typestate tracking. Our system decouples capability lifetimes\nfrom lexical scopes, allowing functions to provide, revoke, and return\ncapabilities in a flow-sensitive manner, based on the existing mechanisms\nexplored for the safety and ergonomics of scoped capability programming.\n  We implement our approach as an extension to the Scala 3 compiler, leveraging\npath-dependent types and implicit resolution to enable concise, statically\nsafe, and expressive typestate programming. Our prototype generically supports\na wide range of stateful patterns, including file operations, advanced locking\nprotocols, DOM construction, and session types. This work demonstrates that\nexpressive and safe typestate management can be achieved with minimal\nextensions to existing capability-based languages, paving the way for more\nrobust and ergonomic stateful programming.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4f5c\u7528\u57df\u80fd\u529b\u548c\u6d41\u654f\u611f\u7c7b\u578b\u72b6\u6001\u673a\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u5728Scala 3\u7f16\u8bd1\u5668\u5b9e\u73b0\uff0c\u652f\u6301\u591a\u79cd\u72b6\u6001\u8d44\u6e90\u7ba1\u7406\uff0c\u517c\u987e\u5b89\u5168\u3001\u8868\u8fbe\u529b\u548c\u6613\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4f5c\u7528\u57df\u7684\u8d44\u6e90\u7ba1\u7406\uff08\u5982Java\u7684\u540c\u6b65\u5757\uff09\u5bb9\u6613\u63a8\u7406\u4f46\u4e0d\u591f\u7075\u6d3b\u4e14\u9650\u5236\u5e76\u884c\u6027\uff1b\u800c\u547d\u4ee4\u5f0f\u3001\u6d41\u654f\u611f\u7684\u7ba1\u7406\u867d\u66f4\u7075\u6d3b\u5374\u5b9e\u73b0\u590d\u6742\uff0c\u6613\u589e\u52a0\u7a0b\u5e8f\u5458\u8d1f\u62c5\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u5b89\u5168\u53c8\u8868\u8fbe\u529b\u5f3a\u7684\u72b6\u6001\u7ba1\u7406\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u5c06\u6d41\u4e0d\u654f\u611f\u7684\u80fd\u529b\u673a\u5236\u6269\u5c55\u5230\u6d41\u654f\u611f\u7684\u7c7b\u578b\u72b6\u6001\u8ddf\u8e2a\uff0c\u5e76\u5728Scala 3\u7f16\u8bd1\u5668\u4e2d\u901a\u8fc7\u8def\u5f84\u4f9d\u8d56\u7c7b\u578b\u548c\u9690\u5f0f\u89e3\u6790\u52a0\u4ee5\u5b9e\u73b0\u3002", "result": "\u5b9e\u73b0\u4e86\u539f\u578b\uff0c\u652f\u6301\u6587\u4ef6\u64cd\u4f5c\u3001\u9ad8\u7ea7\u9501\u534f\u8bae\u3001DOM\u6784\u5efa\u548c\u4f1a\u8bdd\u7c7b\u578b\u7b49\u591a\u79cd\u72b6\u6001\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u80fd\u529b\u673a\u5236\u53ef\u8f7b\u91cf\u6269\u5c55\u7528\u4e8e\u5b89\u5168\u3001\u8868\u8fbe\u529b\u5f3a\u7684\u72b6\u6001\u7ba1\u7406\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u80fd\u529b\u673a\u5236\u7684\u6269\u5c55\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u8868\u8fbe\u529b\u548c\u5b89\u5168\u6027\u7684\u72b6\u6001\u7ba1\u7406\uff0c\u65e0\u9700\u5bf9\u73b0\u6709\u8bed\u8a00\u8fdb\u884c\u5927\u89c4\u6a21\u66f4\u6539\u3002"}}
{"id": "2510.08609", "categories": ["cs.SE", "cs.CR", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08609", "abs": "https://arxiv.org/abs/2510.08609", "authors": ["Imranur Rahman", "Jill Marley", "William Enck", "Laurie Williams"], "title": "Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?", "comment": "Accepted to ASE 2025", "summary": "Developers consistently use version constraints to specify acceptable\nversions of the dependencies for their project. \\emph{Pinning} dependencies can\nreduce the likelihood of breaking changes, but comes with a cost of manually\nmanaging the replacement of outdated and vulnerable dependencies. On the other\nhand, \\emph{floating} can be used to automatically get bug fixes and security\nfixes, but comes with the risk of breaking changes. Security practitioners\nadvocate \\emph{pinning} dependencies to prevent against software supply chain\nattacks, e.g., malicious package updates. However, since \\emph{pinning} is the\ntightest version constraint, \\emph{pinning} is the most likely to result in\noutdated dependencies. Nevertheless, how the likelihood of becoming outdated or\nvulnerable dependencies changes across version constraint types is unknown. The\ngoal of this study is to aid developers in making an informed dependency\nversion constraint choice by empirically evaluating the likelihood of\ndependencies becoming outdated or vulnerable across version constraint types at\nscale. In this study, we first identify the trends in dependency version\nconstraint usage and the patterns of version constraint type changes made by\ndevelopers in the npm, PyPI, and Cargo ecosystems. We then modeled the\ndependency state transitions using survival analysis and estimated how the\nlikelihood of becoming outdated or vulnerable changes when using \\emph{pinning}\nas opposed to the rest of the version constraint types. We observe that among\noutdated and vulnerable dependencies, the most commonly used version constraint\ntype is \\emph{floating-minor}, with \\emph{pinning} being the next most common.\nWe also find that \\emph{floating-major} is the least likely to result in\noutdated and \\emph{floating-minor} is the least likely to result in vulnerable\ndependencies.", "AI": {"tldr": "\u5206\u6790\u4e86\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4f9d\u8d56\u7248\u672c\u7ea6\u675f\u7c7b\u578b\u5bf9\u4f9d\u8d56\u8fc7\u65f6\u4e0e\u5b89\u5168\u6f0f\u6d1e\u6982\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0floating-minor\u80fd\u517c\u987e\u66f4\u65b0\u4e0e\u5b89\u5168\uff0cpinning\u6613\u81f4\u4f9d\u8d56\u8fc7\u65f6\u3002\u5efa\u8bae\u5f00\u53d1\u8005\u7ed3\u5408\u9879\u76ee\u9700\u6c42\uff0c\u79d1\u5b66\u9009\u62e9\u4f9d\u8d56\u7248\u672c\u7ea6\u675f\u7c7b\u578b\u4ee5\u6700\u4f18\u5316\u5b89\u5168\u4e0e\u7ef4\u62a4\u6210\u672c\u3002", "motivation": "\u5f00\u53d1\u8005\u9700\u8981\u6743\u8861\u4f9d\u8d56\u7248\u672c\u7ea6\u675f\u7c7b\u578b\uff1a\u9501\u5b9a\uff08pinning\uff09\u53ef\u4ee5\u907f\u514d\u7834\u574f\u6027\u53d8\u66f4\u548c\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u5bb9\u6613\u5bfc\u81f4\u4f9d\u8d56\u8fc7\u65f6\uff0c\u589e\u52a0\u624b\u52a8\u66f4\u65b0\u8d1f\u62c5\uff1b\u6d6e\u52a8\uff08floating\uff09\u5219\u80fd\u81ea\u52a8\u83b7\u5f97\u4fee\u590d\uff0c\u5374\u6709\u7834\u574f\u6027\u53d8\u66f4\u7684\u98ce\u9669\u3002\u5982\u4f55\u9009\u62e9\u6700\u5408\u9002\u7684\u7ea6\u675f\u7c7b\u578b\u4ee5\u5e73\u8861\u5b89\u5168\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u662f\u5f00\u53d1\u4e2d\u5e38\u89c1\u96be\u9898\u3002\u73b0\u6709\u5b89\u5168\u5b9e\u8df5\u63a8\u5e7f\u9501\u5b9a\u4ee5\u9632\u4f9b\u5e94\u94fe\u653b\u51fb\uff0c\u4f46\u7248\u672c\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u4f9d\u8d56\u8fc7\u65f6\u6216\u51fa\u73b0\u5b89\u5168\u6f0f\u6d1e\u7684\u6982\u7387\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728npm\u3001PyPI\u548cCargo\u4e09\u4e2a\u4e3b\u6d41\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u7edf\u8ba1\u5206\u6790\u4f9d\u8d56\u7248\u672c\u7ea6\u675f\u7684\u4f7f\u7528\u8d8b\u52bf\u53ca\u5176\u53d8\u5316\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u751f\u5b58\u5206\u6790\u6a21\u578b\uff0c\u63a8\u7b97\u4e0d\u540c\u7ea6\u675f\u7c7b\u578b\u4e0b\u4f9d\u8d56\u53d8\u4e3a\u8fc7\u65f6\u6216\u6613\u53d7\u653b\u51fb\u7684\u6982\u7387\u3002", "result": "\u5728\u8fc7\u65f6\u6216\u6709\u5b89\u5168\u6f0f\u6d1e\u7684\u4f9d\u8d56\u4e2d\uff0c\u6700\u5e38\u89c1\u7684\u7ea6\u675f\u7c7b\u578b\u662ffloating-minor\uff0c\u5176\u6b21\u4e3apinning\uff1b\u800cfloating-major\u6700\u4e0d\u5bb9\u6613\u5bfc\u81f4\u4f9d\u8d56\u8fc7\u65f6\uff0cfloating-minor\u6700\u4e0d\u5bb9\u6613\u5bfc\u81f4\u4f9d\u8d56\u6709\u5b89\u5168\u6f0f\u6d1e\u3002", "conclusion": "\u4e0d\u540c\u7248\u672c\u7ea6\u675f\u7c7b\u578b\u5f71\u54cd\u4f9d\u8d56\u51fa\u73b0\u8fc7\u65f6\u548c\u5b89\u5168\u6f0f\u6d1e\u7684\u6982\u7387\uff0c\u5404\u7c7b\u578b\u6709\u5404\u81ea\u7684\u4f18\u52bf\u548c\u98ce\u9669\u3002\u5f00\u53d1\u8005\u53ef\u4ee5\u4f9d\u636e\u4e0d\u540c\u7ea6\u675f\u7c7b\u578b\u7684\u5b9e\u8bc1\u98ce\u9669\uff0c\u505a\u51fa\u66f4\u667a\u80fd\u7684\u4f9d\u8d56\u7ba1\u7406\u51b3\u7b56\uff0c\u4ece\u800c\u5728\u81ea\u52a8\u5316\u66f4\u65b0\u4e0e\u5b89\u5168\u9632\u62a4\u4e4b\u95f4\u53d6\u5f97\u66f4\u4f73\u5e73\u8861\u3002"}}
{"id": "2510.08592", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08592", "abs": "https://arxiv.org/abs/2510.08592", "authors": ["Shahriar Kabir Nahin", "Hadi Askari", "Muhao Chen", "Anshuman Chhabra"], "title": "Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models", "comment": null, "summary": "Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple\ncandidate responses and then operating over this set to find the best output. A\ntacit premise behind TTS is that sufficiently diverse candidate pools enhance\nreliability. In this work, we show that this assumption in TTS introduces a\npreviously unrecognized failure mode. When candidate diversity is curtailed,\neven by a modest amount, TTS becomes much more likely to produce unsafe\noutputs. We present a reference-guided diversity reduction protocol (RefDiv)\nthat serves as a diagnostic attack to stress test TTS pipelines. Through\nextensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,\nGemma3) and two widely used TTS strategies (Monte Carlo Tree Search and\nBest-of-N), constraining diversity consistently signifies the rate at which TTS\nproduces unsafe results. The effect is often stronger than that produced by\nprompts directly with high adversarial intent scores. This observed phenomenon\nalso transfers across TTS strategies and to closed-source models (e.g. OpenAI\no3 and Gemini-2.5-Pro), thus indicating that this is a general and extant\nproperty of TTS rather than a model-specific artifact. Additionally, we find\nthat numerous widely used safety guardrail classifiers (e.g. Llama-Guard and\nOpenAI Moderation API), are unable to flag the adversarial input prompts\ngenerated by RefDiv, demonstrating that existing defenses offer limited\nprotection against this diversity-driven failure mode. Through this work, we\nhope to motivate future research on designing robust TTS strategies that are\nboth effective and secure against diversity-targeted stress tests as\nillustrated by RefDiv.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\uff0c\u9650\u5236TTS\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5019\u9009\u591a\u6837\u6027\u4f1a\u663e\u8457\u63d0\u9ad8\u751f\u6210\u4e0d\u5b89\u5168\u8f93\u51fa\u7684\u6982\u7387\uff0c\u63d0\u51faRefDiv\u534f\u8bae\u63ed\u793a\u8be5\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u5b89\u5168\u673a\u5236\u96be\u4ee5\u9632\u8303\u8be5\u98ce\u9669\uff0c\u4fc3\u8bf7\u672a\u6765\u5f00\u53d1\u66f4\u52a0\u5b89\u5168\u9c81\u68d2\u7684TTS\u7b56\u7565\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u65b9\u6cd5Test-Time Scaling\uff08TTS\uff09\u4f9d\u8d56\u591a\u6837\u5316\u5019\u9009\u54cd\u5e94\uff0c\u4ee5\u63d0\u5347\u8f93\u51fa\u7684\u53ef\u9760\u6027\uff0c\u4f46\u5bf9\u5019\u9009\u96c6\u591a\u6837\u6027\u7684\u9690\u542b\u5047\u8bbe\u672a\u88ab\u5145\u5206\u9a8c\u8bc1\u53ca\u5206\u6790\u3002\u4f5c\u8005\u5e0c\u671b\u63ed\u793a\u9650\u5236\u591a\u6837\u6027\u5e26\u6765\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faRefDiv\u53c2\u8003\u5f15\u5bfc\u591a\u6837\u6027\u7ea6\u51cf\u534f\u8bae\uff0c\u4f5c\u4e3aTTS\u7ba1\u9053\u7684\u8bca\u65ad\u653b\u51fb\u5de5\u5177\u3002\u5e76\u5728\u591a\u79cd\u4e3b\u6d41\u5f00\u6e90\u6a21\u578b\uff08Qwen3\u3001Mistral\u3001Llama3.1\u3001Gemma3\uff09\u548c\u4e3b\u6d41TTS\u7b56\u7565\uff08MCTS\u3001Best-of-N\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1b\u540c\u65f6\u6269\u5c55\u5230\u95ed\u6e90\u6a21\u578b\u548c\u5b89\u5168\u62a4\u680f\u5de5\u5177\u3002", "result": "\u5f53\u5019\u9009\u96c6\u7684\u591a\u6837\u6027\u88ab\u6536\u7f29\u65f6\uff0cTTS\u663e\u8457\u66f4\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\uff1b\u8fd9\u4e00\u73b0\u8c61\u5728\u4e0d\u540cTTS\u7b56\u7565\u548c\u6a21\u578b\u4e4b\u95f4\u8fc1\u79fb\uff0c\u8bf4\u660e\u662fTTS\u7684\u666e\u904d\u95ee\u9898\u3002\u540c\u65f6\u73b0\u6709\u5b89\u5168\u5de5\u5177\uff08\u5982Llama-Guard\u548cOpenAI Moderation API\uff09\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u8fd9\u4e00\u7c7b\u578b\u7684\u4e0d\u5b89\u5168\u8f93\u5165\u3002", "conclusion": "TTS\u7684\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5019\u9009\u591a\u6837\u6027\u3002\u5f53\u524d\u4e3b\u6d41\u5b89\u5168\u5de5\u5177\u96be\u4ee5\u9632\u5fa1\u9488\u5bf9\u591a\u6837\u6027\u6536\u7f29\u7684\u653b\u51fb\uff0c\u9700\u53d1\u5c55\u66f4\u9c81\u68d2\u7684TTS\u673a\u5236\u6765\u589e\u5f3a\u5b89\u5168\u6027\u3002"}}
{"id": "2510.08939", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08939", "abs": "https://arxiv.org/abs/2510.08939", "authors": ["Haotian Deng", "Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer", "comment": null, "summary": "We present a flow-sensitive effect system for reachability types that\nsupports explicit memory management, including Rust-style move semantics, in\nhigher-order impure functional languages. Our system refines the existing\nreachability qualifier with polymorphic \\emph{use} and \\emph{kill} effects that\nrecord how references are read, written, transferred, and deallocated. The\neffect discipline tracks operations performed on each resource using\nqualifiers, enabling the type system to express ownership transfer, contextual\nfreshness, and destructive updates without regions or linearity. We formalize\nthe calculus, its typing and effect rules, and a compositional operational\nsemantics that validates use-after-free safety. All metatheoretic results,\nincluding preservation, progress, and effect soundness, are mechanized. The\nsystem models idioms such as reference deallocation, move semantics, reference\nswapping, while exposing precise safety guarantee. Together, these\ncontributions integrate reachability-based reasoning with explicit resource\ncontrol, advancing the state of the art in safe manual memory management for\nhigher-order functional languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u654f\u611f\u7684\u53ef\u8fbe\u6027effect\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u9636\u4e0d\u7eaf\u51fd\u6570\u8bed\u8a00\u7684\u5b89\u5168\u624b\u52a8\u5185\u5b58\u7ba1\u7406\uff0c\u5305\u62ec\u7cbe\u786e\u8ffd\u8e2a\u5f15\u7528\u64cd\u4f5c\u548c\u4fdd\u8bc1\u7528\u540e\u91ca\u653e\u5b89\u5168\uff0c\u63a8\u52a8\u4e86\u7c7b\u578b\u7cfb\u7edf\u5bf9\u8d44\u6e90\u7ba1\u7406\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9ad8\u9636\u51fd\u6570\u8bed\u8a00\u4e2d\u624b\u52a8\u5185\u5b58\u7ba1\u7406\u5b89\u5168\u6027\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u652f\u6301Rust\u98ce\u683cmove\u8bed\u4e49\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u7c7b\u578b\u5206\u6790\u6765\u4fdd\u8bc1\u8d44\u6e90\u7ba1\u7406\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u5907\u6d41\u654f\u611f\u6027\u7684effect\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u591a\u6001use\u4e0ekill effects\uff0c\u5f62\u5f0f\u5316\u4e86\u6f14\u7b97\u4f53\u7cfb\u53ca\u5176\u7c7b\u578b\u4e0eeffect\u89c4\u5219\uff0c\u5e76\u5b9e\u73b0\u4e86\u64cd\u4f5c\u8bed\u4e49\u4e0e\u5143\u7406\u8bba\u7ed3\u679c\u7684\u673a\u68b0\u5316\u8bc1\u660e\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u7cbe\u51c6\u8ffd\u8e2a\u8d44\u6e90\u7684\u6240\u6709\u64cd\u4f5c\uff08\u5305\u62ec\u8bfb\u53d6\u3001\u5199\u5165\u3001\u8f6c\u79fb\u53ca\u91ca\u653e\uff09\uff0c\u63d0\u4f9b\u5982\u6240\u6709\u6743\u8f6c\u79fb\u3001\u5f15\u7528\u65b0\u9c9c\u6027\u3001\u9500\u6bc1\u6027\u66f4\u65b0\u7b49\u8bed\u4e49\uff0c\u65e0\u9700\u533a\u57df\u6216\u7ebf\u6027\u7c7b\u578b\u4fdd\u62a4\u7528\u540e\u91ca\u653e\u5b89\u5168\uff0c\u5e76\u652f\u6301\u4e30\u5bcc\u5185\u5b58\u7ba1\u7406\u60ef\u7528\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c06\u53ef\u8fbe\u6027\u7c7b\u578b\u7684\u63a8\u7406\u4e0e\u663e\u5f0f\u8d44\u6e90\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86\u9ad8\u9636\u51fd\u6570\u8bed\u8a00\u4e2d\u5b89\u5168\u624b\u52a8\u5185\u5b58\u7ba1\u7406\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2510.08610", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08610", "abs": "https://arxiv.org/abs/2510.08610", "authors": ["Imranur Rahman", "Md Rayhanur Rahman"], "title": "Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model", "comment": "Accepted to Context Collection Workshop co-located with ASE 2025", "summary": "Code completion can help developers improve efficiency and ease the\ndevelopment lifecycle. Although code completion is available in modern\nintegrated development environments (IDEs), research lacks in determining what\nmakes a good context for code completion based on the information available to\nthe IDEs for the large language models (LLMs) to perform better. In this paper,\nwe describe an effective context collection strategy to assist the LLMs in\nperforming better at code completion tasks. The key idea of our strategy is to\npreprocess the repository into smaller code chunks and later use syntactic and\nsemantic similarity-based code chunk retrieval with relative positioning. We\nfound that code chunking and relative positioning of the chunks in the final\ncontext improve the performance of code completion tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u4ee3\u7801\u5757\u5212\u5206\u548c\u76f8\u5bf9\u5b9a\u4f4d\u7684\u4e0a\u4e0b\u6587\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u4ee3\u7801\u8865\u5168\u53ef\u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u5f53\u524dIDE\u4e2d\u4f7f\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u8865\u5168\u65f6\uff0c\u7f3a\u4e4f\u5bf9\u6700\u4f73\u4e0a\u4e0b\u6587\u9009\u62e9\u7684\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u4e3aLLMs\u63d0\u4f9b\u66f4\u4f18\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63d0\u5347\u4ee3\u7801\u8865\u5168\u6548\u679c\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u7b56\u7565\u3002\u9996\u5148\uff0c\u5c06\u4ee3\u7801\u4ed3\u5e93\u9884\u5904\u7406\u4e3a\u8f83\u5c0f\u7684\u4ee3\u7801\u5757\uff0c\u7136\u540e\u901a\u8fc7\u8bed\u6cd5\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8fdb\u884c\u4ee3\u7801\u5757\u68c0\u7d22\uff0c\u5e76\u91c7\u7528\u76f8\u5bf9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u6700\u7ec8\u7684\u8865\u5168\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4ee3\u7801\u5757\u5316\u5904\u7406\u548c\u4ee3\u7801\u5757\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u76f8\u5bf9\u5b9a\u4f4d\u80fd\u591f\u63d0\u5347\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2dLLMs\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4ee3\u7801\u4ed3\u5e93\u7684\u9884\u5904\u7406\u548c\u4e0a\u4e0b\u6587\u6784\u5efa\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLMs\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.08593", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08593", "abs": "https://arxiv.org/abs/2510.08593", "authors": ["Yuxin Li", "Eng Siong Chng", "Cuntai Guan"], "title": "Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech", "comment": null, "summary": "Speech-based depression detection (SDD) is a promising, non-invasive\nalternative to traditional clinical assessments. However, it remains limited by\nthe difficulty of extracting meaningful features and capturing sparse,\nheterogeneous depressive cues over time. Pretrained self-supervised learning\n(SSL) models such as WavLM provide rich, multi-layer speech representations,\nyet most existing SDD methods rely only on the final layer or search for a\nsingle best-performing one. These approaches often overfit to specific datasets\nand fail to leverage the full hierarchical structure needed to detect subtle\nand persistent depression signals.\n  To address this challenge, we propose HAREN-CTC, a novel architecture that\nintegrates multi-layer SSL features using cross-attention within a multitask\nlearning framework, combined with Connectionist Temporal Classification loss to\nhandle sparse temporal supervision. HAREN-CTC comprises two key modules: a\nHierarchical Adaptive Clustering module that reorganizes SSL features into\ncomplementary embeddings, and a Cross-Modal Fusion module that models\ninter-layer dependencies through cross-attention. The CTC objective enables\nalignment-aware training, allowing the model to track irregular temporal\npatterns of depressive speech cues.\n  We evaluate HAREN-CTC under both an upper-bound setting with standard data\nsplits and a generalization setting using five-fold cross-validation. The model\nachieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on\nMODMA, outperforming prior methods across both evaluation scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u5c42SSL\u7279\u5f81\u878d\u5408\u548cCTC\u635f\u5931\u7684HAREN-CTC\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u6291\u90c1\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u8bed\u97f3\u6291\u90c1\u68c0\u6d4b\uff08SDD\uff09\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u3001\u975e\u4fb5\u5165\u6027\u7684\u66ff\u4ee3\u4f20\u7edf\u4e34\u5e8a\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u53d7\u9650\u4e8e\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u7279\u5f81\uff0c\u4ee5\u53ca\u96be\u4ee5\u6355\u83b7\u7a00\u758f\u3001\u5f02\u8d28\u6027\u6291\u90c1\u7ebf\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u53ea\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u5982WavLM\u7684\u67d0\u4e00\u5c42\u6216\u6700\u7ec8\u5c42\u8bed\u97f3\u7279\u5f81\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u672a\u5145\u5206\u5229\u7528\u5c42\u6b21\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51faHAREN-CTC\u65b0\u67b6\u6784\uff0c\u5c06\u591a\u5c42SSL\u7279\u5f81\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u5230\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u7ed3\u5408CTC\u635f\u5931\u4ee5\u5e94\u5bf9\u7a00\u758f\u65f6\u95f4\u76d1\u7763\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a\u5c42\u6b21\u81ea\u9002\u5e94\u805a\u7c7b\u6a21\u5757\uff08\u91cd\u65b0\u7ec4\u7ec7SSL\u7279\u5f81\u4e3a\u4e92\u8865\u5d4c\u5165\uff09\u548c\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff08\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u5efa\u6a21\u5c42\u95f4\u4f9d\u8d56\uff09\uff1bCTC\u76ee\u6807\u5b9e\u73b0\u5bf9\u4e0d\u89c4\u5219\u6291\u90c1\u8bed\u97f3\u65f6\u5e8f\u7ebf\u7d22\u7684\u5bf9\u9f50\u8bad\u7ec3\u3002", "result": "\u5728DAIC-WOZ\u548cMODMA\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u6807\u51c6\u5206\u5272\u548c\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684Macro F1\u5206\u6570\uff1a\u5206\u522b\u4e3a0.81\u548c0.82\uff0c\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "HAREN-CTC\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5229\u7528\u591a\u5c42\u8bed\u97f3\u7279\u5f81\u5e76\u5bf9\u7a00\u758f\u3001\u5f02\u8d28\u6027\u6291\u90c1\u7ebf\u7d22\u8fdb\u884c\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u8bed\u97f3\u6291\u90c1\u68c0\u6d4b\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\uff0c\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.08969", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08969", "abs": "https://arxiv.org/abs/2510.08969", "authors": ["Bjarne Stroustrup"], "title": "Concept-Based Generic Programming in C++", "comment": null, "summary": "We present programming techniques to illustrate the facilities and principles\nof C++ generic programming using concepts. Concepts are C++'s way to express\nconstraints on generic code. As an initial example, we provide a simple type\nsystem that eliminates narrowing conversions and provides range checking\nwithout unnecessary notational or run-time overheads. Concepts are used\nthroughout to provide user-defined extensions to the type system. The aim is to\nshow their utility and the fundamental ideas behind them, rather than to\nprovide a detailed or complete explanation of C++'s language support for\ngeneric programming or the extensive support provided by the standard library.\nGeneric programming is an integral part of C++, rather than an isolated\nsub-language. In particular, key facilities support general programming as well\nas generic programming (e.g., uniform notation for types, lambdas, variadic\ntemplates, and C++26 static reflection). Finally, we give design rationales and\norigins for key parts of the concept design, including use patterns, the\nrelationship to Object-Oriented Programming, value arguments, notation, concept\ntype-matching, and definition checking.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u793a\u4f8b\u6df1\u5165\u4ecb\u7ecd\u4e86C++ concepts\u5bf9\u6cdb\u578b\u7f16\u7a0b\u7684\u652f\u6301\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6d88\u9664\u7c7b\u578b\u8f6c\u6362\u98ce\u9669\u548c\u8fd0\u884c\u8d1f\u62c5\u3001\u63d0\u5347\u5b89\u5168\u6027\u7684\u663e\u8457\u4f5c\u7528\uff0c\u5e76\u5256\u6790\u4e86concepts\u7684\u8bbe\u8ba1\u5408\u7406\u6027\u548c\u4e0e\u5176\u4ed6\u8303\u5f0f\u7684\u5173\u7cfb\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5c55\u793aC++\u4e2d\u6cdb\u578b\u7f16\u7a0b\u7684\u57fa\u672c\u539f\u7406\u548c\u8bbe\u65bd\uff0c\u5c24\u5176\u662fconcepts\u7684\u5e94\u7528\u548c\u4f18\u52bf\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5177\u4f53\u7f16\u7a0b\u6280\u672f\u4ee5\u53ca\u793a\u4f8b\uff0c\u8bf4\u660econcepts\u5982\u4f55\u589e\u5f3aC++\u6cdb\u578b\u7f16\u7a0b\u7684\u8868\u8fbe\u529b\u548c\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u7f16\u7a0b\u6848\u4f8b\u8bf4\u660econcepts\u5728\u7ea6\u675f\u6cdb\u578b\u4ee3\u7801\u4e2d\u7684\u4f5c\u7528\uff1b\u8bbe\u8ba1\u4e00\u4e2a\u7b80\u5355\u7684\u7c7b\u578b\u7cfb\u7edf\uff0c\u907f\u514d\u7f29\u7a84\u8f6c\u6362\u5e76\u5b9e\u73b0\u8303\u56f4\u68c0\u67e5\uff1b\u901a\u8fc7\u8d2f\u7a7f\u5168\u6587\u7684concepts\u5e94\u7528\uff0c\u5ef6\u5c55\u7c7b\u578b\u7cfb\u7edf\uff1b\u5206\u6790concepts\u7684\u8bbe\u8ba1\u7406\u7531\u53ca\u4e0e\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u7b49\u8303\u5f0f\u7684\u5173\u7cfb\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86C++ concepts\u7684\u5b9e\u7528\u6027\u4e0e\u8bbe\u8ba1\u601d\u60f3\uff0c\u5e76\u901a\u8fc7\u7b80\u5316\u7c7b\u578b\u7cfb\u7edf\u3001\u6d88\u9664\u8fd0\u884c\u65f6\u548c\u8bed\u6cd5\u8d1f\u62c5\uff0c\u63d0\u5347\u4e86\u6cdb\u578b\u7f16\u7a0b\u7684\u5b89\u5168\u6027\u548c\u6269\u5c55\u6027\u3002\u540c\u65f6\u9610\u91ca\u4e86\u6982\u5ff5\u8bbe\u8ba1\u7684\u8bf8\u591a\u5408\u7406\u6027\u53ca\u5176\u4e0e\u5176\u5b83\u7f16\u7a0b\u8303\u5f0f\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "C++ concepts\u6781\u5927\u4e30\u5bcc\u4e86\u6cdb\u578b\u7f16\u7a0b\u7684\u8868\u8fbe\u529b\uff0c\u4f7f\u5176\u6210\u4e3aC++\u7684\u57fa\u672c\u7ec4\u6210\u90e8\u5206\u3002\u76f8\u5173\u8bbe\u65bd\u4e0d\u4ec5\u6709\u52a9\u4e8e\u6cdb\u578b\u7f16\u7a0b\uff0c\u4e5f\u6539\u5584\u4e86\u4e00\u822c\u7f16\u7a0b\u4e60\u60ef\u3002\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u548c\u8bbe\u8ba1\u5206\u6790\uff0c\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86concepts\u5728\u7c7b\u578b\u7ea6\u675f\u3001\u7528\u6237\u6269\u5c55\u4ee5\u53ca\u7c7b\u578b\u7cfb\u7edf\u5b89\u5168\u7b49\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.08612", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08612", "abs": "https://arxiv.org/abs/2510.08612", "authors": ["Devang Dhanuka"], "title": "Impact of LLMs on Team Collaboration in Software Development", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being integrated into software\ndevelopment processes, with the potential to transform team workflows and\nproductivity. This paper investigates how LLMs affect team collaboration\nthroughout the Software Development Life Cycle (SDLC). We reframe and update a\nprior study with recent developments as of 2025, incorporating new literature\nand case studies. We outline the problem of collaboration hurdles in SDLC and\nexplore how LLMs can enhance productivity, communication, and decision-making\nin a team context. Through literature review, industry examples, a team survey,\nand two case studies, we assess the impact of LLM-assisted tools (such as code\ngeneration assistants and AI-powered project management agents) on\ncollaborative software engineering practices. Our findings indicate that LLMs\ncan significantly improve efficiency (by automating repetitive tasks and\ndocumentation), enhance communication clarity, and aid cross-functional\ncollaboration, while also introducing new challenges like model limitations and\nprivacy concerns. We discuss these benefits and challenges, present research\nquestions guiding the investigation, evaluate threats to validity, and suggest\nfuture research directions including domain-specific model customization,\nimproved integration into development tools, and robust strategies for ensuring\ntrust and security.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u56e2\u961f\u534f\u4f5c\u4e2d\u7684\u4f5c\u7528\u3002\u7ed3\u679c\u663e\u793a\uff0cLLM\u80fd\u63d0\u5347\u6548\u7387\u4e0e\u6c9f\u901a\uff0c\u4f46\u5f15\u5165\u6a21\u578b\u548c\u9690\u79c1\u7684\u65b0\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5b9a\u5236\u53ca\u5b89\u5168\u96c6\u6210\u65b9\u6848\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4e9f\u9700\u63a2\u7a76\u5176\u5bf9\u56e2\u961f\u534f\u4f5c\u6d41\u7a0b\u548c\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5bf9SDLC\u56e2\u961f\u534f\u4f5c\u8fdb\u884c\u6587\u732e\u7efc\u8ff0\u3001\u884c\u4e1a\u6848\u4f8b\u3001\u56e2\u961f\u95ee\u5377\u8c03\u67e5\u53ca\u6848\u4f8b\u5206\u6790\u3002", "result": "LLM\u8f85\u52a9\u5de5\u5177\u80fd\u591f\u81ea\u52a8\u5316\u91cd\u590d\u4efb\u52a1\u3001\u63d0\u5347\u6587\u6863\u548c\u6c9f\u901a\u8d28\u91cf\u3001\u4fc3\u8fdb\u8de8\u804c\u80fd\u5408\u4f5c\uff0c\u4f46\u9700\u8981\u5173\u6ce8\u6a21\u578b\u5c40\u9650\u53ca\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u6539\u8fdb\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LLM\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u56e2\u961f\u6548\u7387\u3001\u6c9f\u901a\u548c\u534f\u4f5c\uff0c\u4f46\u4e5f\u5e26\u6765\u6a21\u578b\u5c40\u9650\u548c\u9690\u79c1\u7b49\u65b0\u6311\u6218\u3002"}}
{"id": "2510.08595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08595", "abs": "https://arxiv.org/abs/2510.08595", "authors": ["V. S. Raghu Parupudi"], "title": "Systematic Diagnosis of Brittle Reasoning in Large Language Models", "comment": "Submitted to NEURIPS-2025 MATHAI workshop", "summary": "A central question in artificial intelligence is the extent to which machine\nlearning models comprehend mathematics. To address this, we propose a novel\nframework for measuring mathematical reasoning that moves beyond standard\nbenchmarks to diagnose specific failure points. Our method first generates\nstructured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We\nthen use a more capable analyst model, gpt-4o-mini, to categorize errors and,\ncrucially, perform an unsupervised clustering of every reasoning sentence to\nidentify emergent \"reasoning modes.\" This analysis reveals a cognitive profile\nwith a stark, nonhuman-like brittleness: while the model achieves near-perfect\naccuracy on procedural modes like sequential calculation, its performance on\nmodes requiring combinatorial reasoning with restrictions plummets. By\nidentifying and quantifying the reliability of these distinct reasoning skills,\nour work provides a more granular method to evaluate mathematical comprehension\nand offers a precise roadmap for developing new capabilities and more reliable\nfuture applications.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u66f4\u7ec6\u81f4\u5730\u5206\u6790\u548c\u91cf\u5316\u4e86\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e0d\u540c\u73af\u8282\u7684\u8868\u73b0\uff0c\u6307\u51fa\u5176\u5728\u7a0b\u5e8f\u6027\u63a8\u7406\u51e0\u4e4e\u65e0\u8bef\uff0c\u4f46\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u6781\u5f31\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u5177\u4f53\u7ebf\u7d22\u3002", "motivation": "\u5f53\u524d\u6807\u51c6\u57fa\u51c6\u96be\u4ee5\u7ec6\u81f4\u8bca\u65ad\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u5177\u4f53\u8584\u5f31\u73af\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6d4b\u8bc4\u6846\u67b6\u6765\u66f4\u7ec6\u7c92\u5ea6\u5730\u63ed\u793a\u5176\u63a8\u7406\u80fd\u529b\u7684\u7ec4\u6210\u548c\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u5229\u7528gpt-3.5-turbo\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u751f\u6210\u7ed3\u6784\u5316\u3001\u9010\u6b65\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u968f\u540e\u7531\u66f4\u5f3a\u5927\u7684\u5206\u6790\u6a21\u578bgpt-4o-mini\u5bf9\u9519\u8bef\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5bf9\u6bcf\u4e00\u53e5\u63a8\u7406\u53e5\u8fdb\u884c\u65e0\u76d1\u7763\u805a\u7c7b\u4ee5\u8bc6\u522b\u51fa\u201c\u63a8\u7406\u6a21\u5f0f\u201d\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8bc6\u522b\u548c\u91cf\u5316\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u6280\u80fd\uff0c\u8fd8\u9996\u6b21\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7ec4\u5408\u6027\u7ea6\u675f\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u77ed\u677f\uff0c\u4e3a\u672a\u6765\u80fd\u529b\u63d0\u5347\u548c\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6539\u8fdb\u8def\u5f84\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6570\u5b66\u63a8\u7406\u65f6\u5b58\u5728\u663e\u8457\u7684\u975e\u4eba\u7c7b\u5f0f\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7ec4\u5408\u63a8\u7406\u548c\u7ea6\u675f\u65f6\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u5728\u987a\u5e8f\u8ba1\u7b97\u7b49\u7a0b\u5e8f\u5316\u6a21\u5f0f\u4e0b\u51e0\u4e4e\u5b8c\u7f8e\u3002"}}
{"id": "2510.09591", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09591", "abs": "https://arxiv.org/abs/2510.09591", "authors": ["Saad Ahmed Bazaz", "Mirza Omer Beg"], "title": "A Multilingual Python Programming Language", "comment": "For project homepage, see https://universalpython.github.io/", "summary": "All widely used and useful programming languages have a common problem. They\nrestrict entry on the basis of knowledge of the English language. The lack of\nknowledge of English poses a major hurdle to many newcomers who do not have the\nresources, in terms of time and money, to learn the English language. Studies\nshow that people learn better in their own language. Therefore, we propose a\nlanguage transpiler built on top of the Python programming language, called\nUniversalPython, which allows one to write Python in their own human language.\nWe demonstrate the ability to create an \"Urdu Python\" with this transpiler. In\nthe future, we aim to scale the language to encapsulate more human languages to\nincrease the availability of programming. The source code for this transpiler\nis open-source, and available at\nhttps://github.com/universalpython/universalpython", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86UniversalPython\u5de5\u5177\uff0c\u8ba9\u7528\u6237\u80fd\u4ee5\u6bcd\u8bed\u7f16\u5199Python\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f16\u7a0b\u8bed\u8a00\u53d7\u9650\u4e8e\u82f1\u8bed\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u4e4c\u5c14\u90fd\u8bed\u5b9e\u4f8b\uff0c\u6e90\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u76ee\u6807\u662f\u8ba9\u66f4\u591a\u4eba\u80fd\u7528\u672c\u5730\u8bed\u8a00\u7f16\u7a0b\u3002", "motivation": "\u7f16\u7a0b\u8bed\u8a00\u901a\u5e38\u8981\u6c42\u4f7f\u7528\u8005\u5177\u5907\u4e00\u5b9a\u7684\u82f1\u8bed\u77e5\u8bc6\uff0c\u8fd9\u5bf9\u4e0d\u61c2\u82f1\u8bed\u7684\u65b0\u624b\u6765\u8bf4\u662f\u4e00\u5927\u969c\u788d\u3002\u4eba\u4eec\u5728\u6bcd\u8bed\u73af\u5883\u4e0b\u5b66\u4e60\u6548\u679c\u66f4\u597d\uff0c\u4f46\u73b0\u6709\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00\u672a\u80fd\u5f88\u597d\u5730\u652f\u6301\u591a\u8bed\u8a00\u7f16\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u8bed\u8a00\u8f6c\u8bd1\u5668UniversalPython\uff0c\u4f7f\u7528\u6237\u53ef\u4ee5\u7528\u81ea\u5df1\u7684\u6bcd\u8bed\u4e66\u5199Python\u4ee3\u7801\u3002\u4f5c\u8005\u4ee5\u4e4c\u5c14\u90fd\u8bed\u5b9e\u73b0\u4e3a\u4f8b\u5c55\u793a\u4e86\u8be5\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86UniversalPython\uff0c\u4f7fPython\u652f\u6301\u4e4c\u5c14\u90fd\u8bed\u7f16\u7a0b\uff0c\u5e76\u5f00\u6e90\u4e86\u5176\u4ee3\u7801\uff0c\u4e3a\u672a\u6765\u652f\u6301\u66f4\u591a\u4eba\u7c7b\u8bed\u8a00\u7f16\u7a0b\u6253\u4e0b\u4e86\u57fa\u7840\u3002", "conclusion": "UniversalPython\u80fd\u591f\u964d\u4f4e\u7f16\u7a0b\u5165\u95e8\u95e8\u69db\uff0c\u6709\u52a9\u4e8e\u66f4\u591a\u975e\u82f1\u8bed\u7528\u6237\u5b66\u4e60\u7f16\u7a0b\uff0c\u63a8\u52a8\u7f16\u7a0b\u8bed\u8a00\u591a\u8bed\u8a00\u5316\u53d1\u5c55\u3002\u672a\u6765\u8ba1\u5212\u652f\u6301\u66f4\u591a\u8bed\u8a00\uff0c\u6269\u5927\u5f71\u54cd\u529b\u3002"}}
{"id": "2510.08640", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08640", "abs": "https://arxiv.org/abs/2510.08640", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "comment": null, "summary": "Android is the largest mobile platform, yet automatically building\napplications remains a practical challenge. While Large Language Models (LLMs)\nshow promise for code repair, their use for fixing Android build errors remains\nunderexplored. To address this gap, we first introduce AndroidBuildBench, a\nbenchmark of 1,019 build failures curated from the commit histories of 43\nopen-source Android projects. Each problem is paired with a verified solution\nfrom a subsequent commit, ensuring that fixes are feasible. Second, we propose\nGradleFixer, an LLM agent with domain-specific tools for inspecting and\nmanipulating the Gradle build environment. GradleFixer achieves a resolve rate\nof 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent\nthat relies on a general-purpose shell. GradleFixer's success suggests that\nwhile LLMs possess the high-level knowledge to solve these failures, they\nstruggle to translate this knowledge into effective low-level actions using a\ngeneral-purpose shell. We demonstrate the effectiveness of a strategy we term\nTool Bridging, which replaces general-purpose shell commands with domain-aware\nabstractions. We hypothesize this approach works through two mechanisms: 1) it\nprovides tools in an API-like format that LLMs use more reliably, and 2) it\nconstrains the action space to relevant operations. This approach bridges the\ngap between the model's high-level reasoning and effective low-level execution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Android\u6784\u5efa\u5931\u8d25\u4fee\u590d\u57fa\u51c6\u548c\u914d\u5957LLM\u667a\u80fd\u4f53GradleFixer\u3002\u901a\u8fc7\u4e3a\u6a21\u578b\u914d\u5907\u9886\u57df\u4e13\u7528\u5de5\u5177\uff0c\u5e76\u7528API\u6765\u7ea6\u675f\u548c\u5f15\u5bfc\u64cd\u4f5c\uff0c\u4ece\u800c\u5927\u5e45\u63d0\u5347\u4e86\u4fee\u590d\u6548\u7387\uff0c\u663e\u793a\u51fa\u201c\u5de5\u5177\u6865\u63a5\u201d\u7b56\u7565\u5728\u81ea\u52a8\u5316\u6784\u5efa\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u81ea\u52a8\u5316\u6784\u5efaAndroid\u5e94\u7528\u4ecd\u5177\u6311\u6218\u6027\uff0c\u800c\u5927\u6a21\u578b\u5728\u4ee3\u7801\u4fee\u590d\u4e0a\u7684\u5e94\u7528\u5df2\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9488\u5bf9Android\u6784\u5efa\u9519\u8bef\u7684\u4fee\u590d\u7814\u7a76\u76f8\u5bf9\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86AndroidBuildBench\u57fa\u51c6\uff0c\u75311019\u4e2a\u771f\u5b9e\u6784\u5efa\u5931\u8d25\u6848\u4f8b\u548c\u5bf9\u5e94\u4fee\u590d\u7ec4\u6210\uff0c\u5e76\u5f00\u53d1\u4e86GradleFixer\u2014\u2014\u7ed3\u5408\u9886\u57df\u4e13\u7528\u5de5\u5177\uff08\u7528\u4e8e\u68c0\u67e5\u548c\u64cd\u4f5cGradle\u73af\u5883\uff09\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u201cTool Bridging\u201d\u7b56\u7565\u8ba9LLM\u4f7f\u7528API\u5f0f\u62bd\u8c61\u66ff\u4ee3\u901a\u7528\u547d\u4ee4\u3002", "result": "GradleFixer\u7684\u4fee\u590d\u6210\u529f\u7387\u8fbe\u523081.4%\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u901a\u7528shell\u4e3a\u57fa\u7840\u7684\u6700\u65b0\u4ee3\u7801\u667a\u80fd\u4f53\uff0c\u8bf4\u660e\u9886\u57df\u4e13\u7528\u5de5\u5177\u548cAPI\u62bd\u8c61\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u4f4e\u9636\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u4e3aLLM\u63d0\u4f9b\u9886\u57df\u611f\u77e5\u5de5\u5177\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5176\u89e3\u51b3Android\u6784\u5efa\u9519\u8bef\u7684\u80fd\u529b\uff0c\u4e5f\u8bc1\u660e\u4e86\u9ad8\u5c42\u77e5\u8bc6\u4e0e\u4f4e\u5c42\u64cd\u4f5c\u4e4b\u95f4\u201c\u5de5\u5177\u6865\u63a5\u201d\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.08596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08596", "abs": "https://arxiv.org/abs/2510.08596", "authors": ["V. S. Raghu Parupudi"], "title": "Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs", "comment": "Submitted to AACL-IJCNLP 2025 (Eval4NLP)", "summary": "Reference-free metrics like self-perplexity are strongly biased against\ncreative text generation. We propose the Confidence Score (CS), derived from a\nmodel's output probability distribution, as a less biased alternative.\nExperiments on gpt-4o-mini show that while fluency-based metrics prefer novel\nresponses in 0\\% of cases on 99 creative prompts, our CS does so 19% of the\ntime, a statistically significant difference (95% CI for difference: [11.1%,\n27.3%]). We also show that CS effectively distinguishes between easy, medium,\nand hard tasks, confirmed by non-overlapping confidence intervals. The\nConfidence Score thus mitigates the creativity bias of traditional metrics\nwhile retaining their core evaluative strengths, offering a more balanced\nassessment for modern LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Confidence Score\uff0c\u6709\u6548\u5f25\u8865\u4e86\u4f20\u7edf\u8bc4\u4ef7\u6307\u6807\u5728\u521b\u9020\u6027\u6587\u672c\u751f\u6210\u9886\u57df\u7684\u663e\u8457\u504f\u89c1\uff0c\u80fd\u66f4\u79d1\u5b66\u5730\u8bc4\u4f30\u5927\u6a21\u578b\u751f\u6210\u7684\u521b\u65b0\u6027\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u53c2\u8003\u8bc4\u4ef7\u6307\u6807\uff08\u5982self-perplexity\uff09\u5bf9\u521b\u9020\u6027\u6587\u672c\u751f\u6210\u5b58\u5728\u4e25\u91cd\u504f\u89c1\uff0c\u96be\u4ee5\u516c\u6b63\u8bc4\u4ef7\u751f\u6210\u6a21\u578b\u7684\u521b\u9020\u6027\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u6982\u7387\u5206\u5e03\u7684\u65b0\u8861\u91cf\u6807\u51c6\uff1aConfidence Score\uff08CS\uff09\uff0c\u5e76\u5728gpt-4o-mini\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u5176\u533a\u5206\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\u548c\u5bf9\u521b\u9020\u6027\u6587\u672c\u7684\u503e\u5411\u3002", "result": "CS\u6307\u6807\u572899\u4e2a\u521b\u9020\u6027\u6587\u672c\u63d0\u793a\u4e2d\uff0c\u6bd4fluency-based\u6307\u6807\u5bf9\u65b0\u9896\u56de\u590d\u670919%\u7684\u504f\u597d\uff08fluency-based\u6307\u6807\u4e3a0%\uff09\uff0c\u5dee\u5f02\u572895%\u7f6e\u4fe1\u533a\u95f4\u5185\u660e\u663e\uff08[11.1%, 27.3%]\uff09\uff1bCS\u8fd8\u80fd\u6709\u6548\u533a\u5206\u5bb9\u6613\u3001\u4e2d\u7b49\u548c\u56f0\u96be\u4efb\u52a1\u3002", "conclusion": "Confidence Score\u80fd\u591f\u51cf\u8f7b\u4f20\u7edf\u65e0\u53c2\u8003\u6307\u6807\u5bf9\u521b\u9020\u6027\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u6838\u5fc3\u7684\u8bc4\u4ef7\u80fd\u529b\uff0c\u4e3a\u73b0\u4ee3\u5927\u6a21\u578b\u5e26\u6765\u66f4\u5e73\u8861\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002"}}
{"id": "2510.08664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08664", "abs": "https://arxiv.org/abs/2510.08664", "authors": ["Jianan Mu", "Mingyu Shi", "Yining Wang", "Tianmeng Yang", "Bin Sun", "Xing Hu", "Jing Ye", "Huawei Li"], "title": "Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware", "comment": null, "summary": "LLM-based RTL generation is an interesting research direction, as it holds\nthe potential to liberate the least automated stage in the current chip design.\nHowever, due to the substantial semantic gap between high-level specifications\nand RTL, coupled with limited training data, existing models struggle with\ngeneration accuracy. Drawing on human experience, design with verification\nhelps improving accuracy. However, as the RTL testbench data are even more\nscarce, it is not friendly for LLMs. Although LLMs excel at higher-level\nlanguages like Python/C, they have a huge semantic gap from RTL. When\nimplementing the same functionality, Python/C code and hardware code differ\nsignificantly in the spatiotemporal granularity, requiring the LLM not only to\nconsider high-level functional semantics but also to ensure the low-level\ndetails align with the circuit code. It is not an easy task. In this paper, we\npropose a function abstracted verifiable middleware (Faver) that streamlines\nRTL verification in LLM-based workflows. By mixing LLM-friendly code structures\nwith a rule-based template, Faver decouples the details of circuit\nverification, allowing the LLM to focus on the functionality itself. In our\nexperiments on the SFT model and open-source models, Faver improved the model's\ngeneration accuracy by up to 14%.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9LLM\u751f\u6210RTL\u4ee3\u7801\u7684\u51c6\u786e\u7387\u96be\u9898\uff0c\u63d0\u51fa\u4e86Faver\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7\u51fd\u6570\u62bd\u8c61\u548c\u6a21\u677f\u89c4\u5219\u7b80\u5316\u9a8c\u8bc1\u73af\u8282\uff0c\u4f7fLLM\u4e13\u6ce8\u4e8e\u529f\u80fd\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u63d0\u5347\u751f\u6210\u51c6\u786e\u7387\u81f314%\u3002", "motivation": "\u5f53\u524d\u82af\u7247\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u5728RTL\u751f\u6210\u73af\u8282\u8f83\u4f4e\uff0c\u4e14\u9ad8\u5c42\u6b21\u89c4\u683c\u4e0eRTL\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u8bed\u4e49\u9e3f\u6c9f\uff0c\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u5bfc\u81f4\u73b0\u6709LLM\u6a21\u578b\u751f\u6210\u51c6\u786e\u7387\u4e0d\u9ad8\u3002\u8981\u63d0\u5347\u51c6\u786e\u6027\uff0c\u501f\u9274\u4eba\u7c7b\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u201c\u8bbe\u8ba1-\u9a8c\u8bc1\u201d\u6a21\u5f0f\uff0c\u4f46\u7531\u4e8eRTL\u9a8c\u8bc1\u76f8\u5173\u6570\u636e\u66f4\u4e3a\u532e\u4e4f\uff0c\u5bf9LLM\u4e0d\u53cb\u597d\u3002\u9ad8\u9636\u8bed\u8a00\u4e0eRTL\u5728\u8bed\u4e49\u548c\u5b9e\u73b0\u65b9\u5f0f\u4e0a\u5dee\u5f02\u5de8\u5927\uff0c\u7ed9\u751f\u6210\u4efb\u52a1\u5e26\u6765\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u62bd\u8c61\u7684\u53ef\u9a8c\u8bc1\u4e2d\u95f4\u4ef6Faver\uff0c\u901a\u8fc7\u878d\u5408LLM\u53cb\u597d\u7684\u4ee3\u7801\u7ed3\u6784\u548c\u6a21\u677f\u89c4\u5219\uff0c\u89e3\u8026\u7535\u8def\u9a8c\u8bc1\u7ec6\u8282\uff0c\u4f7fLLM\u80fd\u591f\u4e13\u6ce8\u4e8e\u751f\u6210\u529f\u80fd\u4ee3\u7801\u800c\u975e\u4f4e\u5c42\u6b21\u5b9e\u73b0\u7ec6\u8282\uff0c\u63d0\u9ad8\u6574\u4f53\u6d41\u7a0b\u7684\u51c6\u786e\u7387\u3002", "result": "\u5728SFT\u6a21\u578b\u53ca\u5f00\u6e90\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cFaver\u53ef\u63d0\u5347LLM\u751f\u6210\u7684RTL\u4ee3\u7801\u51c6\u786e\u7387\u6700\u9ad8\u8fbe14%\u3002", "conclusion": "\u5229\u7528Faver\u4e2d\u95f4\u4ef6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u9ad8\u9636\u4ee3\u7801\u4e0eRTL\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u5e76\u63d0\u5347\u4e86LLM\u5728RTL\u751f\u6210\u9886\u57df\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u8fdb\u4e00\u6b65\u63a8\u52a8\u82af\u7247\u81ea\u52a8\u5316\u8bbe\u8ba1\u6d41\u7a0b\u4e2dRTL\u73af\u8282\u7684\u667a\u80fd\u5316\u53d1\u5c55\u3002"}}
{"id": "2510.08600", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08600", "abs": "https://arxiv.org/abs/2510.08600", "authors": ["Devleena Das", "Rajeev Patwari", "Ashish Sirasao"], "title": "Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation", "comment": "Accepted to EMNLP 2025 Industry Track", "summary": "Inference optimizations such as quantization, pruning, format and datatype\nconversion, model export, and serialization can lead to functional degradations\nin language model task performance. While most efforts on performance recovery\nfor deployment focus on robust quantization techniques, we focus on recovering\nmodel accuracies from any sources that degrade model weights, such as improper\nmodel serialization. In this work, we propose Recover-LoRA, a lightweight and\ndataset agnostic method to recover accuracy in degraded models. Recover-LoRA\nuses synthetic data and logit distillation to learn LoRA adapters on selective\nlayers that facilitate aligning the degraded model to its full precision model.\nWe investigate the utility of Recover-LoRA across a diverse set of small\nlanguage models (SLMs), including models with varying attention architectures,\nmulti-head attention (MHA) and group-query attention (GQA), as well as several\nevaluation datasets. Our results show that Recover-LoRA recovers model\naccuracies by 5-17% on MHA and GQA SLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Recover-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684LoRA\u9002\u914d\u5668\u4e0elogit\u84b8\u998f\uff0c\u53ef\u9488\u5bf9\u5404\u79cd\u6743\u91cd\u9000\u5316\u6709\u6548\u6062\u590d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7cbe\u5ea6\uff0c\u5728\u4e0d\u540c\u6ce8\u610f\u529b\u7ed3\u6784\u548c\u6570\u636e\u96c6\u4e0a\u80fd\u63d0\u53475-17%\u51c6\u786e\u7387\u3002", "motivation": "\u63a8\u7406\u4f18\u5316\u5982\u91cf\u5316\u3001\u526a\u679d\u3001\u683c\u5f0f\u548c\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u3001\u6a21\u578b\u5bfc\u51fa\u4e0e\u5e8f\u5217\u5316\uff0c\u5e38\u5e38\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u3002\u76ee\u524d\uff0c\u5927\u591a\u6570\u9488\u5bf9\u90e8\u7f72\u540e\u7684\u6027\u80fd\u6062\u590d\u5de5\u4f5c\u96c6\u4e2d\u5728\u66f4\u9c81\u68d2\u7684\u91cf\u5316\u6280\u672f\u4e0a\u3002\u672c\u5de5\u4f5c\u5173\u6ce8\u4e8e\u6062\u590d\u7531\u5404\u79cd\u6743\u91cd\u9000\u5316\uff08\u5982\u6a21\u578b\u5e8f\u5217\u5316\u4e0d\u5f53\uff09\u5f15\u8d77\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faRecover-LoRA\u65b9\u6cd5\u3002\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u96c6\u65e0\u5173\u7684\u7cbe\u5ea6\u6062\u590d\u65b9\u6cd5\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u548clogit\u84b8\u998f\uff0c\u5bf9\u9009\u62e9\u6027\u5c42\u8fdb\u884cLoRA\u9002\u914d\u5668\u5b66\u4e60\uff0c\u4f7f\u9000\u5316\u6a21\u578b\u4e0e\u5176\u5168\u7cbe\u5ea6\u6a21\u578b\u5bf9\u9f50\u3002", "result": "Recover-LoRA\u53ef\u5728\u591a\u79cd\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u3001\u5982\u591a\u5934\u6ce8\u610f\u529b\u548c\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff09\u53ca\u591a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u53ef\u6062\u590d\u6a21\u578b5-17%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Recover-LoRA\u80fd\u6709\u6548\u6062\u590d\u56e0\u591a\u79cd\u6743\u91cd\u9000\u5316\uff08\u5305\u62ec\u975e\u9c81\u68d2\u5e8f\u5217\u5316\u7b49\uff09\u5bfc\u81f4\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u635f\u5931\uff0c\u9002\u7528\u8303\u56f4\u5e7f\uff0c\u4e14\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u3002"}}
{"id": "2510.09073", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09073", "abs": "https://arxiv.org/abs/2510.09073", "authors": ["Matthew Sotoudeh"], "title": "Literate Tracing", "comment": "examples at https://lair.masot.net/trex . SPLASH Onward 2025", "summary": "As computer systems grow ever larger and more complex, a crucial task in\nsoftware development is for one person (the system expert) to communicate to\nanother (the system novice) how a certain program works. This paper reports on\nthe author's experiences with a paradigm for program documentation that we call\nliterate tracing. A literate trace explains a software system using annotated,\nconcrete execution traces of the system. Literate traces complement both\nin-code comments (which often lack global context) and out-of-band design docs\n(which often lack a concrete connection to the code). We also describe TReX,\nour tool for making literate traces that are interactive, visual, and\nguaranteed by construction to be faithful to the program semantics. We have\nused TReX to write literate traces explaining components of large systems\nsoftware including the Linux kernel, Git source control system, and GCC\ncompiler.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4ee3\u7801\u5177\u4f53\u6267\u884c\u8f68\u8ff9\u548c\u6ce8\u91ca\u7684\u65b0\u578b\u7a0b\u5e8f\u6587\u6863\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7TReX\u5de5\u5177\u5728\u5927\u578b\u5f00\u6e90\u9879\u76ee\u4e2d\u5b9e\u73b0\uff0c\u63d0\u5347\u4e86\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53ef\u7406\u89e3\u6027\u548c\u6587\u6863\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u673a\u7cfb\u7edf\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u4e0d\u65ad\u589e\u52a0\uff0c\u7cfb\u7edf\u4e13\u5bb6\u5411\u65b0\u624b\u89e3\u91ca\u7a0b\u5e8f\u7684\u5de5\u4f5c\u539f\u7406\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u4f20\u7edf\u7684\u4ee3\u7801\u6ce8\u91ca\u548c\u8bbe\u8ba1\u6587\u6863\u5404\u6709\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u4f20\u9012\u5177\u4f53\u548c\u5168\u5c40\u7684\u7a0b\u5e8f\u8fd0\u884c\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cliterate tracing\u201d\u7684\u7a0b\u5e8f\u6587\u6863\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5e26\u6ce8\u91ca\u7684\u3001\u5177\u4f53\u7684\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u6765\u89e3\u91ca\u7cfb\u7edf\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86TReX\u5de5\u5177\uff0c\u53ef\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u53ef\u89c6\u5316\u4e14\u8bed\u4e49\u4e0a\u4e25\u683c\u5bf9\u9f50\u7684literate trace\u6587\u6863\u3002", "result": "\u5229\u7528TReX\u5de5\u5177\uff0c\u4f5c\u8005\u6210\u529f\u4e3a\u5927\u578b\u7cfb\u7edf\u8f6f\u4ef6\uff08\u5982Linux\u5185\u6838\u3001Git\u3001GCC\u7f16\u8bd1\u5668\uff09\u7684\u90e8\u5206\u7ec4\u4ef6\u7f16\u5199\u4e86literate trace\u6587\u6863\u3002", "conclusion": "literate tracing\u7ed3\u5408\u4e86\u4ee3\u7801\u6ce8\u91ca\u548c\u8bbe\u8ba1\u6587\u6863\u7684\u4f18\u70b9\uff0c\u63d0\u4f9b\u4e86\u5bf9\u8f6f\u4ef6\u7cfb\u7edf\u66f4\u6e05\u6670\u3001\u5177\u4f53\u548c\u5fe0\u5b9e\u7684\u7406\u89e3\u65b9\u5f0f\u3002TReX\u5de5\u5177\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u590d\u6742\u8f6f\u4ef6\u9879\u76ee\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.08665", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08665", "abs": "https://arxiv.org/abs/2510.08665", "authors": ["Aofan Liu", "Haoxuan Li", "Bin Wang", "Ao Yang", "Hui Li"], "title": "RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution", "comment": null, "summary": "Code generation models based on large language models (LLMs) have gained wide\nadoption, but challenges remain in ensuring safety, accuracy, and\ncontrollability, especially for complex tasks. Existing methods often lack\ndynamic integration of external tools, transparent reasoning, and user control\nover safety. To address these issues, we propose a controllable code generation\nframework utilizing the ReAct paradigm for multi-agent task execution. This\nframework is a multi-agent system designed to enable efficient, precise, and\ninterpretable code generation through dynamic interactions between LLMs and\nexternal resources. The framework adopts a collaborative architecture\ncomprising four specialized agents: a Planner for task decomposition, a\nSearcher that leverages the ReAct framework for reasoning and tool integration,\na CodeGen agent for accurate code generation, and an Extractor for structured\ndata retrieval. The ReAct-based Searcher alternates between generating\nreasoning traces and executing actions, facilitating seamless integration of\ninternal knowledge with external tools (such as search engines) to enhance\naccuracy and user control. Experimental results show the framework's\neffectiveness across multiple languages, achieving a 94.8% security rate on the\nSVEN dataset with CodeQL, outperforming existing approaches. Its transparent\nreasoning process fosters user trust and improves controllability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eReAct\u8303\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001\u63a8\u7406-\u884c\u52a8\u4ea4\u66ff\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u636e\u63d0\u53d6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u53ef\u63a7\u4e0e\u900f\u660e\u7684\u751f\u6210\u6d41\u7a0b\uff0c\u5728\u5b89\u5168\u6d4b\u8bd5\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u51c6\u786e\u6027\u548c\u53ef\u63a7\u6027\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u7f3a\u4e4f\u52a8\u6001\u5de5\u5177\u96c6\u6210\u3001\u900f\u660e\u63a8\u7406\u8fc7\u7a0b\u4ee5\u53ca\u7528\u6237\u5bf9\u5b89\u5168\u7684\u63a7\u5236\u6743\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u4e86ReAct\u8303\u5f0f\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u6267\u884c\u3002\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u4e13\u4e1a\u5316\u7684\u667a\u80fd\u4f53\uff1a\u89c4\u5212\u8005\uff08Planner\uff09\u3001\u641c\u7d22\u8005\uff08Searcher\uff0c\u5229\u7528ReAct\u5b9e\u73b0\u63a8\u7406\u548c\u5de5\u5177\u96c6\u6210\uff09\u3001\u4ee3\u7801\u751f\u6210\u8005\uff08CodeGen\uff09\u548c\u6570\u636e\u63d0\u53d6\u8005\uff08Extractor\uff09\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u4efb\u52a1\u5206\u89e3\u3001\u63a8\u7406\u3001\u7cbe\u786e\u4ee3\u7801\u751f\u6210\u548c\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u641c\u7d22\u8005\u52a8\u6001\u7ed3\u5408\u5185\u5916\u90e8\u8d44\u6e90\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ef\u63a7\u6027\u3002", "result": "\u6846\u67b6\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u7528CodeQL\u5de5\u5177\u5728SVEN\u6570\u636e\u96c6\u4e0a\u5b89\u5168\u7387\u8fbe\u523094.8%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5e76\u7ed3\u5408ReAct\u8303\u5f0f\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u8fd8\u6709\u8f83\u5f3a\u7684\u53ef\u63a7\u6027\u548c\u900f\u660e\u6027\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u4fe1\u4efb\u3002"}}
{"id": "2510.08601", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08601", "abs": "https://arxiv.org/abs/2510.08601", "authors": ["Aneesh Jonelagadda", "Christina Hahn", "Haoze Zheng", "Salvatore Penachio"], "title": "Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs", "comment": "12 pages, 4 figures", "summary": "Long-term memory is essential for natural, realistic dialogue. However,\ncurrent large language model (LLM) memory systems rely on either brute-force\ncontext expansion or static retrieval pipelines that fail on edge-constrained\ndevices. We introduce Mnemosyne, an unsupervised, human-inspired long-term\nmemory architecture designed for edge-based LLMs. Our approach uses\ngraph-structured storage, modular substance and redundancy filters, memory\ncommitting and pruning mechanisms, and probabilistic recall with temporal decay\nand refresh processes modeled after human memory. Mnemosyne also introduces a\nconcentrated \"core summary\" efficiently derived from a fixed-length subset of\nthe memory graph to capture the user's personality and other domain-specific\nlong-term details such as, using healthcare application as an example,\npost-recovery ambitions and attitude towards care. Unlike existing\nretrieval-augmented methods, Mnemosyne is designed for use in longitudinal\nhealthcare assistants, where repetitive and semantically similar but temporally\ndistinct conversations are limited by naive retrieval. In experiments with\nlongitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate\nof 65.8% in blind human evaluations of realism and long-term memory capability\ncompared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current\nhighest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval\ncompared to other same-backboned techniques. Further, the average overall score\nof 54.6% was second highest across all methods, beating commonly used Mem0 and\nOpenAI baselines among others. This demonstrates that improved factual recall,\nenhanced temporal reasoning, and much more natural user-facing responses can be\nfeasible with an edge-compatible and easily transferable unsupervised memory\narchitecture.", "AI": {"tldr": "Mnemosyne\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u65e0\u76d1\u7763\u3001\u7c7b\u4eba\u957f\u671f\u8bb0\u5fc6\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u6a21\u578b\u7684\u4e8b\u5b9e\u56de\u5fc6\u3001\u65f6\u95f4\u63a8\u7406\u548c\u81ea\u7136\u6027\uff0c\u5c24\u5176\u5728\u533b\u7597\u957f\u671f\u52a9\u624b\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u8bb0\u5fc6\u7cfb\u7edf\u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0d\u53cb\u597d\uff0c\u57fa\u4e8e\u66b4\u529b\u4e0a\u4e0b\u6587\u6269\u5c55\u6216\u9759\u6001\u68c0\u7d22\uff0c\u5bf9\u4e8e\u957f\u671f\u5bf9\u8bdd\uff08\u5982\u533b\u7597\u52a9\u624b\uff09\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u4eba\u6027\u5316\u3001\u66f4\u9ad8\u6548\u7684\u957f\u671f\u8bb0\u5fc6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7c7b\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u7684\u65e0\u76d1\u7763\u957f\u65f6\u8bb0\u5fc6\u67b6\u6784Mnemosyne\uff0c\u4f7f\u7528\u56fe\u7ed3\u6784\u5b58\u50a8\u3001\u5197\u4f59\u8fc7\u6ee4\u3001\u8bb0\u5fc6\u63d0\u4ea4\u4e0e\u4fee\u526a\u3001\u6982\u7387\u53ec\u56de\u548c\u8bb0\u5fc6\u66f4\u65b0\u7b49\u673a\u5236\uff0c\u8fd8\u63d0\u51fa\u96c6\u4e2d\u5f0f\u6838\u5fc3\u6458\u8981\uff0c\u63d0\u5347\u5bf9\u7528\u6237\u957f\u671f\u7ec6\u8282\u7684\u6355\u83b7\u80fd\u529b\u3002", "result": "\u5728\u957f\u671f\u533b\u7597\u5bf9\u8bdd\u5b9e\u9a8c\u4e2d\uff0cMnemosyne\u5728\u4eba\u7c7b\u76f2\u6d4b\u8bc4\u4f30\u4e2d\u5b9e\u73b065.8%\u80dc\u7387\uff08\u5bf9\u7167RAG\u57fa\u7ebf31.1%\uff09\uff0c\u5728LoCoMo\u57fa\u51c6\u4e0a\u8868\u73b0\u9886\u5148\uff0c\u540c\u65f6\u7efc\u5408\u5f97\u5206\u4e3a54.6%\uff0c\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u4eba\u6027\u5316\u5bf9\u8bdd\u80fd\u529b\u3002", "conclusion": "Mnemosyne\u5728\u957f\u671f\u533b\u7597\u5bf9\u8bdd\u573a\u666f\u4e2d\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u4e8b\u5b9e\u56de\u5fc6\u3001\u65f6\u95f4\u63a8\u7406\u548c\u66f4\u81ea\u7136\u7684\u4eba\u673a\u5bf9\u8bdd\u6548\u679c\uff0c\u4e14\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2510.08667", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08667", "abs": "https://arxiv.org/abs/2510.08667", "authors": ["Mohammad Baqar"], "title": "RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data", "comment": "13 Pages", "summary": "Modern software teams frequently encounter delays in resolving recurring or\nrelated issues due to fragmented knowledge scattered across JIRA tickets,\ndeveloper discussions, and GitHub pull requests (PRs). To address this\nchallenge, we propose a Retrieval-Augmented Generation (RAG) framework that\nintegrates Sentence-Transformers for semantic embeddings with FAISS-based\nvector search to deliver context-aware ticket resolution recommendations. The\napproach embeds historical JIRA tickets, user comments, and linked PR metadata\nto retrieve semantically similar past cases, which are then synthesized by a\nLarge Language Model (LLM) into grounded and explainable resolution\nsuggestions. The framework contributes a unified pipeline linking JIRA and\nGitHub data, an embedding and FAISS indexing strategy for heterogeneous\nsoftware artifacts, and a resolution generation module guided by retrieved\nevidence. Experimental evaluation using precision, recall, resolution time\nreduction, and developer acceptance metrics shows that the proposed system\nsignificantly improves resolution accuracy, fix quality, and knowledge reuse in\nmodern DevOps environments.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8bed\u4e49\u5d4c\u5165\u548c\u5927\u6a21\u578b\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u81ea\u52a8\u63a8\u8350\u9488\u5bf9JIRA\u8f6f\u4ef6\u7f3a\u9677\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347DevOps\u56e2\u961f\u7684\u6548\u7387\u4e0e\u77e5\u8bc6\u590d\u7528\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u56e2\u961f\u9762\u4e34\u77e5\u8bc6\u788e\u7247\u5316\uff0c\u5bfc\u81f4\u7ecf\u5e38\u6027\u6216\u76f8\u5173\u95ee\u9898\u96be\u4ee5\u53ca\u65f6\u89e3\u51b3\uff0c\u963b\u788d\u4e86\u77e5\u8bc6\u590d\u7528\u548c\u5f00\u53d1\u6548\u7387\u3002", "method": "\u7ed3\u5408Sentence-Transformers\u7528\u4e8e\u8bed\u4e49\u5d4c\u5165\uff0cFAISS\u5b9e\u73b0\u5411\u91cf\u5316\u68c0\u7d22\uff0c\u518d\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u57fa\u4e8e\u68c0\u7d22\u7ed3\u679c\u751f\u6210\u5177\u4f53\u7684\u89e3\u51b3\u5efa\u8bae\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86JIRA\u548cGitHub\u6570\u636e\uff0c\u7edf\u4e00\u5904\u7406\u5f02\u6784\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4ef6\u5e76\u6839\u636e\u5386\u53f2\u6848\u4f8b\u751f\u6210\u89e3\u91ca\u6027\u5efa\u8bae\u3002", "result": "\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u89e3\u51b3\u65f6\u95f4\u7f29\u77ed\u548c\u5f00\u53d1\u8005\u91c7\u7eb3\u5ea6\u7b49\u591a\u9879\u6307\u6807\u4e0a\uff0c\u8be5\u7cfb\u7edf\u8868\u73b0\u4f18\u8d8a\uff0c\u63d0\u5347\u4e86\u95ee\u9898\u89e3\u51b3\u6548\u7387\u548c\u8d28\u91cf\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684RAG\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u95ee\u9898\u89e3\u51b3\u7684\u51c6\u786e\u7387\u3001\u4fee\u590d\u8d28\u91cf\u4ee5\u53ca\u77e5\u8bc6\u590d\u7528\u80fd\u529b\u3002"}}
{"id": "2510.08602", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08602", "abs": "https://arxiv.org/abs/2510.08602", "authors": ["Cong Zeng", "Shengkun Tang", "Yuanzhou Chen", "Zhiqiang Shen", "Wenchao Yu", "Xujiang Zhao", "Haifeng Chen", "Wei Cheng", "Zhiqiang Xu"], "title": "Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection", "comment": null, "summary": "The rapid advancement of large language models (LLMs) such as ChatGPT,\nDeepSeek, and Claude has significantly increased the presence of AI-generated\ntext in digital communication. This trend has heightened the need for reliable\ndetection methods to distinguish between human-authored and machine-generated\ncontent. Existing approaches both zero-shot methods and supervised classifiers\nlargely conceptualize this task as a binary classification problem, often\nleading to poor generalization across domains and models. In this paper, we\nargue that such a binary formulation fundamentally mischaracterizes the\ndetection task by assuming a coherent representation of human-written texts. In\nreality, human texts do not constitute a unified distribution, and their\ndiversity cannot be effectively captured through limited sampling. This causes\nprevious classifiers to memorize observed OOD characteristics rather than learn\nthe essence of `non-ID' behavior, limiting generalization to unseen\nhuman-authored inputs. Based on this observation, we propose reframing the\ndetection task as an out-of-distribution (OOD) detection problem, treating\nhuman-written texts as distributional outliers while machine-generated texts\nare in-distribution (ID) samples. To this end, we develop a detection framework\nusing one-class learning method including DeepSVDD and HRN, and score-based\nlearning techniques such as energy-based method, enabling robust and\ngeneralizable performance. Extensive experiments across multiple datasets\nvalidate the effectiveness of our OOD-based approach. Specifically, the\nOOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake\ndataset. Moreover, we test our detection framework on multilingual, attacked,\nand unseen-model and -domain text settings, demonstrating the robustness and\ngeneralizability of our framework. Code, pretrained weights, and demo will be\nreleased.", "AI": {"tldr": "\u4ee5\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\u53d6\u4ee3\u4f20\u7edf\u4e8c\u5206\u7c7b\uff0c\u6781\u5927\u63d0\u5347\u4e86AI\u6587\u672c\u68c0\u6d4b\u7684\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u3001DeepSeek\u3001Claude\uff09\u7684\u5e94\u7528\u5e7f\u6cdb\uff0c\u63a8\u52a8\u4e86AI\u751f\u6210\u6587\u672c\u7684\u5927\u89c4\u6a21\u51fa\u73b0\uff0c\u5bfc\u81f4\u4eba\u4eec\u8feb\u5207\u9700\u8981\u533a\u5206\u4eba\u7c7b\u4e0e\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e0d\u540c\u9886\u57df\u53ca\u6a21\u578b\u7684\u65b0\u8f93\u5165\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u5c06\u68c0\u6d4b\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\uff08OOD\uff09\u95ee\u9898\uff0c\u5c06\u4eba\u7c7b\u6587\u672c\u89c6\u4e3a\u5206\u5e03\u5916\u6837\u672c\uff0c\u673a\u5668\u751f\u6210\u6587\u672c\u89c6\u4e3a\u5206\u5e03\u5185\uff08ID\uff09\u6837\u672c\uff0c\u5e76\u5f15\u5165\u4e00\u7c7b\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DeepSVDD\u3001HRN\uff09\u548c\u57fa\u4e8e\u5206\u6570\u7684\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u80fd\u91cf\u7684\u65b9\u6cd5\uff09\u6784\u5efa\u68c0\u6d4b\u6846\u67b6\u3002", "result": "\u5728DeepFake\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0OOD\u65b9\u6cd5\u5b9e\u73b0\u4e8698.3%\u7684AUROC\u548cAUPR\uff0cFPR95\u4ec5\u4e3a8.9%\u3002\u5728\u591a\u8bed\u79cd\u3001\u5bf9\u6297\u653b\u51fb\u3001\u672a\u77e5\u6a21\u578b\u548c\u65b0\u9886\u57df\u573a\u666f\u4e2d\u4ea6\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u5c06AI\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u4ece\u4e8c\u5206\u7c7b\u91cd\u65b0\u5b9a\u4f4d\u4e3aOOD\u68c0\u6d4b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.08697", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08697", "abs": "https://arxiv.org/abs/2510.08697", "authors": ["Terry Yue Zhuo", "Xiaolong Jin", "Hange Liu", "Juyong Jiang", "Tianyang Liu", "Chen Gong", "Bhupesh Bishnoi", "Vaisakhi Mishra", "Marek Suppa", "Noah Ziems", "Saiteja Utpala", "Ming Xu", "Guangyu Song", "Kaixin Li", "Yuhan Cao", "Bo Liu", "Zheng Liu", "Sabina Abdurakhmanova", "Wenhao Yu", "Mengzhao Jia", "Jihan Yao", "Kenneth Hamilton", "Kumar Shridhar", "Minh Chien Vu", "Dingmin Wang", "Jiawei Liu", "Zijian Wang", "Qian Liu", "Binyuan Hui", "Meg Risdal", "Ahsen Khaliq", "Atin Sood", "Zhenchang Xing", "Wasi Uddin Ahmad", "John Grundy", "David Lo", "Banghua Zhu", "Xiaoning Du", "Torsten Scholak", "Leandro von Werra"], "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution", "comment": "Built with love by the BigCode community :)", "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5f00\u653e\u7684\u4ee3\u7801\u751f\u6210\u8bc4\u6d4b\u5e73\u53f0BigCodeArena\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7801\u8bc4\u4f30\u7684\u4eba\u5de5\u9ad8\u96be\u5ea6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u4f53\u7cfb\u91cf\u5316\u4e3b\u6d41\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u4ecd\u5177\u5907\u9886\u5148\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u8bc4\u4f30\u96be\u9898\uff0c\u5c24\u5176\u662f\u4eba\u5de5\u5ba1\u67e5\u4ee3\u7801\u8d28\u91cf\u7684\u9ad8\u96be\u5ea6\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u4ee3\u7801\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u8bc4\u6d4b\u3002", "method": "\u63d0\u51faBigCodeArena\u5e73\u53f0\uff0c\u652f\u6301\u5b9e\u65f6\u4eba\u7c7b\u8bc4\u4f30\u4e0e\u4ee3\u7801\u81ea\u52a8\u6267\u884c\uff0c\u6536\u96c6\u5e76\u5206\u6790\u591a\u8bed\u79cd\u3001\u4e0d\u540c\u6267\u884c\u73af\u5883\u7684\u4ee3\u7801\u751f\u6210\u5bf9\u8bdd\uff0c\u5efa\u7acbBigCodeReward\u548cAutoCodeArena\u4e24\u4e2a\u57fa\u51c6\u8bc4\u4ef7\u4f53\u7cfb\uff0c\u5206\u522b\u7528\u4e8e\u4eba\u7c7b\u504f\u597d\u4e0e\u81ea\u52a8\u8bc4\u5206\u3002", "result": "\u6536\u96c6\u8d85\u8fc714,000\u4e2a\u4ee3\u7801\u5bf9\u8bdd\uff0c\u6db5\u76d610\u79cd\u8bed\u8a00\u548c8\u7c7b\u6267\u884c\u73af\u5883\uff0c\u6316\u6398\u4e864,700\u591a\u7ec4\u542b\u6709\u660e\u786e\u4eba\u7c7b\u504f\u597d\u7684\u591a\u8f6e\u5bf9\u8bdd\u6837\u672c\u3002\u5b9e\u8bc1\u663e\u793a\u5728\u6709\u6267\u884c\u7ed3\u679c\u8f85\u52a9\u65f6\uff0cLLM\u5bf9\u4ee3\u7801\u504f\u597d\u5224\u65ad\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002\u81ea\u52a8\u8bc4\u5206\u4f53\u7cfb\u4e0b\uff0c\u4e13\u6709\u6a21\u578b\u8868\u73b0\u6700\u597d\u3002", "conclusion": "\u4e13\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-5\u3001Claude-Sonnet-4\u548cClaude-Opus-4\uff09\u5728\u4ee3\u7801\u751f\u6210\u6027\u80fd\u4e0a\u4ecd\u7136\u9886\u5148\u4e8e\u65b0\u5174\u6a21\u578b\u3002"}}
{"id": "2510.08603", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08603", "abs": "https://arxiv.org/abs/2510.08603", "authors": ["Deshui Yu", "Yizhi Wang", "Saihui Jin", "Taojie Zhu", "Fanyi Zeng", "Wen Qian", "Zirui Huang", "Jingli Ouyang", "Jiameng Li", "Zhen Song", "Tian Guan", "Yonghong He"], "title": "YpathRAG:A Retrieval-Augmented Generation Framework and Benchmark for Pathology", "comment": null, "summary": "Large language models (LLMs) excel on general tasks yet still hallucinate in\nhigh-barrier domains such as pathology. Prior work often relies on domain\nfine-tuning, which neither expands the knowledge boundary nor enforces\nevidence-grounded constraints. We therefore build a pathology vector database\ncovering 28 subfields and 1.53 million paragraphs, and present YpathRAG, a\npathology-oriented RAG framework with dual-channel hybrid retrieval (BGE-M3\ndense retrieval coupled with vocabulary-guided sparse retrieval) and an\nLLM-based supportive-evidence judgment module that closes the\nretrieval-judgment-generation loop. We also release two evaluation benchmarks,\nYpathR and YpathQA-M. On YpathR, YpathRAG attains Recall@5 of 98.64%, a gain of\n23 percentage points over the baseline; on YpathQA-M, a set of the 300 most\nchallenging questions, it increases the accuracies of both general and medical\nLLMs by 9.0% on average and up to 15.6%. These results demonstrate improved\nretrieval quality and factual reliability, providing a scalable construction\nparadigm and interpretable evaluation for pathology-oriented RAG.", "AI": {"tldr": "YpathRAG\u662f\u4e00\u79cd\u4e3a\u75c5\u7406\u9886\u57df\u8bbe\u8ba1\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7ed3\u5408\u5bc6\u96c6\u4e0e\u7a00\u758f\u68c0\u7d22\u53caLLM\u8bc1\u636e\u5224\u65ad\uff0c\u5728\u75c5\u7406\u5b66\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u56de\u7b54\u51c6\u786e\u7387\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u75c5\u7406\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e3a\u9886\u57df\u77e5\u8bc6\u6269\u5c55\u548c\u8bc1\u636e\u5f3a\u7ea6\u675f\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u75c5\u7406\u7b49\u9ad8\u95e8\u69db\u9886\u57df\u4f9d\u7136\u5b58\u5728\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e14\u4ee5\u5f80\u901a\u8fc7\u9886\u57df\u5fae\u8c03\u7684\u65b9\u5f0f\u65e0\u6cd5\u62d3\u5c55\u77e5\u8bc6\u8fb9\u754c\u6216\u4fdd\u8bc1\u57fa\u4e8e\u8bc1\u636e\u7684\u7ea6\u675f\u3002\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u75c5\u7406\u8f85\u52a9\u63a8\u7406\u6d41\u7a0b\u3002", "method": "\u6784\u5efa\u5305\u542b28\u4e2a\u5b50\u9886\u57df\u548c153\u4e07\u6bb5\u843d\u7684\u75c5\u7406\u5b66\u5411\u91cf\u6570\u636e\u5e93\uff0c\u63d0\u51faYpathRAG\u6846\u67b6\uff0c\u91c7\u7528\u5bc6\u96c6\u68c0\u7d22\u548c\u7a00\u758f\u68c0\u7d22\u7ed3\u5408\u7684\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\uff0c\u4ee5\u53caLLM\u9a71\u52a8\u7684\u8bc1\u636e\u5224\u65ad\u6a21\u5757\uff0c\u5f62\u6210\u68c0\u7d22-\u5224\u65ad-\u751f\u6210\u95ed\u73af\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86\u4e24\u4e2a\u75c5\u7406\u5b66\u8bc4\u6d4b\u57fa\u51c6\uff08YpathR\u548cYpathQA-M\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5YpathR\u4e0a\uff0cYpathRAG\u83b7\u5f97Recall@5\u4e3a98.64%\uff0c\u6bd4\u57fa\u7ebf\u9ad823\u4e2a\u767e\u5206\u70b9\uff1b\u5728YpathQA-M\uff08\u6700\u5177\u6311\u6218\u7684300\u4e2a\u95ee\u9898\uff09\u4e0a\uff0c\u63d0\u5347\u4e00\u822c\u4e0e\u533b\u5b66LLM\u7684\u51c6\u786e\u7387\u5e73\u57479.0%\uff0c\u6700\u9ad8\u8fbe15.6%\u3002", "conclusion": "YpathRAG\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u9886\u57df\u7684\u68c0\u7d22\u8d28\u91cf\u548c\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u8bc1\u660e\u5176\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u8bc4\u4f30\u4ef7\u503c\uff0c\u4e3a\u75c5\u7406\u9886\u57dfRAG\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.08716", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08716", "abs": "https://arxiv.org/abs/2510.08716", "authors": ["Stephan Lukasczyk", "Gordon Fraser"], "title": "Search-based Hyperparameter Tuning for Python Unit Test Generation", "comment": "Accepted to the 17th Symposium on Search-Based Software Engineering\n  2025 (SSBSE 2025)", "summary": "Search-based test-generation algorithms have countless configuration options.\nUsers rarely adjust these options and usually stick to the default values,\nwhich may not lead to the best possible results. Tuning an algorithm's\nhyperparameters is a method to find better hyperparameter values, but it\ntypically comes with a high demand of resources. Meta-heuristic search\nalgorithms -- that effectively solve the test-generation problem -- have been\nproposed as a solution to also efficiently tune parameters. In this work we\nexplore the use of differential evolution as a means for tuning the\nhyperparameters of the DynaMOSA and MIO many-objective search algorithms as\nimplemented in the Pynguin framework. Our results show that significant\nimprovement of the resulting test suite's coverage is possible with the tuned\nDynaMOSA algorithm and that differential evolution is more efficient than basic\ngrid search.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u5dee\u5206\u8fdb\u5316\u81ea\u52a8\u8c03\u4f18\u641c\u7d22\u6d4b\u8bd5\u751f\u6210\u7b97\u6cd5\u8d85\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\uff0c\u4e14\u6bd4\u7f51\u683c\u641c\u7d22\u66f4\u9ad8\u6548\u3002", "motivation": "\u641c\u7d22\u57fa\u7840\u7684\u6d4b\u8bd5\u751f\u6210\u7b97\u6cd5\u5177\u6709\u5927\u91cf\u7684\u914d\u7f6e\u9009\u9879\uff0c\u4f46\u7528\u6237\u5f88\u5c11\u8c03\u6574\u8fd9\u4e9b\u53c2\u6570\uff0c\u5f80\u5f80\u91c7\u7528\u9ed8\u8ba4\u503c\uff0c\u53ef\u80fd\u65e0\u6cd5\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002\u53c2\u6570\u8c03\u4f18\u53ef\u63d0\u5347\u6548\u679c\uff0c\u4f46\u8d44\u6e90\u6d88\u8017\u5f88\u9ad8\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u4f5c\u4e3a\u5143\u542f\u53d1\u5f0f\u641c\u7d22\u65b9\u6cd5\uff0c\u5bf9Pynguin\u6846\u67b6\u4e2d\u7684DynaMOSA\u548cMIO\u591a\u76ee\u6807\u641c\u7d22\u7b97\u6cd5\u7684\u8d85\u53c2\u6570\u8fdb\u884c\u81ea\u52a8\u8c03\u4f18\u3002\u5e76\u4e0e\u57fa\u7840\u7684\u7f51\u683c\u641c\u7d22\u65b9\u6cd5\u8fdb\u884c\u4e86\u6548\u7387\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u8c03\u4f18\u7684DynaMOSA\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u5957\u4ef6\u7684\u8986\u76d6\u7387\uff0c\u540c\u65f6\u5dee\u5206\u8fdb\u5316\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u7f51\u683c\u641c\u7d22\u3002", "conclusion": "\u81ea\u52a8\u8c03\u4f18\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7b97\u6cd5\u8868\u73b0\uff0c\u5dee\u5206\u8fdb\u5316\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u4f18\u4e8e\u5e38\u89c4\u7684\u7f51\u683c\u641c\u7d22\u3002"}}
{"id": "2510.08604", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08604", "abs": "https://arxiv.org/abs/2510.08604", "authors": ["Raffaele Mura", "Giorgio Piras", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Maura Pintor", "Amin Karbasi", "Battista Biggio"], "title": "LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback", "comment": null, "summary": "Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLatentBreak\uff0c\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u7b49\u4ef7\u66ff\u6362\u964d\u4f4e\u56f0\u60d1\u5ea6\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u7ed5\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u9632\u62a4\u673a\u5236\uff0c\u5728\u591a\u79cd\u5b89\u5168\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9LLM\u5b89\u5168\u9632\u62a4\u6709\u6311\u6218\u610f\u4e49\u3002", "motivation": "\u76ee\u524d\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8d8a\u72f1\u653b\u51fb\uff08jailbreak\uff09\u591a\u901a\u8fc7\u9644\u52a0\u9ad8\u56f0\u60d1\u5ea6\u7684\u653b\u51fb\u6027\u540e\u7f00\u6216\u957f\u63d0\u793a\u6a21\u677f\u5b9e\u73b0\uff0c\u4f7f\u6a21\u578b\u751f\u6210\u53d7\u9650\u5236\u6216\u6709\u5bb3\u5185\u5bb9\uff0c\u4f46\u8fd9\u7c7b\u653b\u51fb\u5bb9\u6613\u88ab\u57fa\u4e8e\u56f0\u60d1\u5ea6\uff08perplexity\uff09\u7684\u8fc7\u6ee4\u673a\u5236\u68c0\u6d4b\u51fa\u6765\u3002\u4f5c\u8005\u610f\u5728\u7a81\u7834\u73b0\u6709\u8d8a\u72f1\u624b\u6cd5\u5728\u56f0\u60d1\u5ea6\u9632\u5fa1\u4e0b\u7684\u5c40\u9650\u3002", "method": "\u63d0\u51faLatentBreak\uff0c\u4e00\u79cd\u767d\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u3002LatentBreak\u901a\u8fc7\u5728\u8f93\u5165\u63d0\u793a\u4e2d\u7528\u8bed\u4e49\u7b49\u4ef7\u8bcd\u66ff\u6362\u539f\u6709\u5355\u8bcd\uff0c\u907f\u514d\u6dfb\u52a0\u9ad8\u56f0\u60d1\u5ea6\u7684\u653b\u51fb\u6027\u540e\u7f00\u6216\u957f\u6a21\u677f\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u610f\u56fe\u7684\u524d\u63d0\u4e0b\u751f\u6210\u81ea\u7136\u3001\u4f4e\u56f0\u60d1\u5ea6\u7684\u5bf9\u6297\u6027\u63d0\u793a\u8bcd\u3002\u5177\u4f53\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u6297\u6027\u63d0\u793a\u8bed\u4e0e\u65e0\u5bb3\u8bf7\u6c42\u5728\u6f5c\u5728\u7a7a\u95f4\u7684\u8ddd\u79bb\u6765\u9009\u62e9\u66ff\u6362\u5355\u8bcd\u3002", "result": "LatentBreak\u4ea7\u751f\u7684\u8d8a\u72f1\u63d0\u793a\u76f8\u6bd4\u4e8e\u73b0\u6709\u65b9\u6cd5\u66f4\u77ed\u3001\u56f0\u60d1\u5ea6\u66f4\u4f4e\uff0c\u80fd\u6709\u6548\u7ed5\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u8fc7\u6ee4\u5668\uff0c\u5bf9\u591a\u79cd\u5b89\u5168\u5bf9\u9f50\u6a21\u578b\u7684\u653b\u51fb\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u8d8a\u72f1\u7b97\u6cd5\u3002", "conclusion": "LatentBreak\u7a81\u7834\u4e86\u56f0\u60d1\u5ea6\u9632\u5fa1\u7684\u9650\u5236\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u8d8a\u72f1\u653b\u51fb\u80fd\u529b\u3002\u4f20\u7edf\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u9632\u5fa1\u5bb9\u6613\u5bf9\u5f53\u524d\u81ea\u52a8\u5316\u8d8a\u72f1\u529e\u6cd5\u594f\u6548\uff0c\u4f46\u65b0\u65b9\u6cd5LatentBreak\u53ef\u6709\u6548\u89c4\u907f\u6b64\u7c7b\u68c0\u6d4b\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u6a21\u578b\u5b89\u5168\u9632\u62a4\u7814\u7a76\u3002"}}
{"id": "2510.08810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08810", "abs": "https://arxiv.org/abs/2510.08810", "authors": ["Mohayeminul Islam", "Ajay Kumar Jha", "May Mahmoud", "Sarah Nadi"], "title": "PyMigTool: a tool for end-to-end Python library migration", "comment": "arXiv admin note: text overlap with arXiv:2504.13272", "summary": "Library migration is the process of replacing a library with a similar one in\na software project. Manual library migration is time consuming and error prone,\nas it requires developers to understand the Application Programming Interfaces\n(API) of both libraries, map equivalent APIs, and perform the necessary code\ntransformations. Due to the difficulty of the library migration process, most\nof the existing automated techniques and tooling stop at the API mapping stage\nor support a limited set of libraries and code transformations. In this paper,\nwe develop an end-to-end solution that can automatically migrate code between\nany arbitrary pair of Python libraries that provide similar functionality. Due\nto the promising capabilities of Large Language Models (LLMs) in code\ngeneration and transformation, we use LLMs as the primary engine for migration.\nBefore building the tool, we first study the capabilities of LLMs for library\nmigration on a benchmark of 321 real-world library migrations. We find that\nLLMs can effectively perform library migration, but some post-processing steps\ncan further improve the performance. Based on this, we develop PyMigTool, a\ncommand line application that combines the power of LLMs, static analysis, and\ndynamic analysis to provide accurate library migration. We evaluate PyMigTool\non 717 real-world Python applications that are not from our benchmark. We find\nthat PyMigTool can migrate 32% of the migrations with complete correctness. Of\nthe remaining migrations, only 14% of the migration-related changes are left\nfor developers to fix for more than half of the projects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u91cd\u5206\u6790\u65b9\u6cd5\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5e93\u8fc1\u79fb\u5de5\u5177PyMigTool\uff0c\u5b9e\u73b0\u4e86\u4efb\u610fPython\u5e93\u4e4b\u95f4\u7684\u4ee3\u7801\u8fc1\u79fb\u81ea\u52a8\u5316\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u8fc1\u79fb\u51c6\u786e\u7387\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u624b\u5de5\u8fc1\u79fb\u7684\u96be\u5ea6\u3002", "motivation": "\u5e93\u8fc1\u79fb\u662f\u4e00\u9879\u8017\u65f6\u4e14\u6613\u51fa\u9519\u7684\u5de5\u4f5c\uff0c\u9700\u8981\u5f00\u53d1\u8005\u6df1\u5165\u7406\u89e3\u591a\u4e2a\u5e93\u7684API\uff0c\u5e76\u5b8c\u6210\u590d\u6742\u7684\u4ee3\u7801\u8f6c\u6362\u3002\u76ee\u524d\u591a\u6570\u81ea\u52a8\u5316\u5de5\u5177\u4ec5\u505c\u7559\u5728API\u6620\u5c04\u9636\u6bb5\uff0c\u5b9e\u9645\u652f\u6301\u7684\u5e93\u548c\u8f6c\u6362\u573a\u666f\u8f83\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u66f4\u9ad8\u7ea7\u7684\u81ea\u52a8\u5316\u624b\u6bb5\uff0c\u63d0\u9ad8\u8fc1\u79fb\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u9996\u5148\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5b9e\u9645321\u4e2a\u771f\u5b9e\u5e93\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u5176\u7ed3\u679c\u8fdb\u884c\u5206\u6790\u4e0e\u4f18\u5316\u3002\u968f\u540e\uff0c\u63d0\u51fa\u57fa\u4e8eLLM\u3001\u9759\u6001\u5206\u6790\u548c\u52a8\u6001\u5206\u6790\u7684\u7aef\u5230\u7aef\u5e93\u8fc1\u79fb\u5de5\u5177PyMigTool\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u8fc1\u79fb\u4ee3\u7801\u3001\u8f85\u52a9\u5f00\u53d1\u8005\u7b80\u5316\u8fc1\u79fb\u8fc7\u7a0b\u3002", "result": "PyMigTool\u5728717\u4e2a\u771f\u5b9ePython\u9879\u76ee\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u670932%\u7684\u5e93\u8fc1\u79fb\u4efb\u52a1\u53ef\u5b8c\u5168\u6b63\u786e\u5b8c\u6210\uff0c\u5269\u4f59\u4efb\u52a1\u4e2d\u6709\u4e00\u534a\u4ee5\u4e0a\u9879\u76ee\u7684\u8fc1\u79fb\u76f8\u5173\u4ee3\u7801\u53d8\u66f4\u53ea\u670914%\u9700\u5f00\u53d1\u8005\u624b\u52a8\u4fee\u6b63\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u9759\u6001\u4e0e\u52a8\u6001\u5206\u6790\uff0c\u53ef\u4ee5\u81ea\u52a8\u5316\u5b9e\u73b0\u4efb\u610fPython\u5e93\u95f4\u7684\u8fc1\u79fb\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u8005\u7684\u8d1f\u62c5\u548c\u9519\u8bef\uff0c\u63d0\u9ad8\u8fc1\u79fb\u6548\u7387\u3002PyMigTool\u5de5\u5177\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6709\u671b\u6210\u4e3a\u5e7f\u6cdb\u5e94\u7528\u7684\u5e93\u8fc1\u79fb\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2510.08605", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08605", "abs": "https://arxiv.org/abs/2510.08605", "authors": ["Nouar Aldahoul", "Yasir Zaki"], "title": "Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks", "comment": null, "summary": "The rapid spread of misinformation on digital platforms threatens public\ndiscourse, emotional stability, and decision-making. While prior work has\nexplored various adversarial attacks in misinformation detection, the specific\ntransformations examined in this paper have not been systematically studied. In\nparticular, we investigate language-switching across English, French, Spanish,\nArabic, Hindi, and Chinese, followed by translation. We also study query length\ninflation preceding summarization and structural reformatting into\nmultiple-choice questions. In this paper, we present a multilingual,\nmulti-agent large language model framework with retrieval-augmented generation\nthat can be deployed as a web plugin into online platforms. Our work\nunderscores the importance of AI-driven misinformation detection in\nsafeguarding online factual integrity against diverse attacks, while showcasing\nthe feasibility of plugin-based deployment for real-world web applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53ef\u63d2\u4ef6\u5316\u7684\u591a\u8bed\u8a00\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff0c\u7cfb\u7edf\u7814\u7a76\u8bed\u8a00\u5207\u6362\u3001\u5185\u5bb9\u6269\u5c55\u4e0e\u7ed3\u6784\u53d8\u6362\u7b49\u591a\u79cd\u5bf9\u6297\u65b9\u5f0f\uff0c\u5bf9\u5b9e\u7528\u5e73\u53f0\u5177\u6709\u826f\u597d\u63a8\u5e7f\u4ef7\u503c\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5e73\u53f0\u4e0a\u9519\u8bef\u4fe1\u606f\u7684\u8fc5\u901f\u4f20\u64ad\uff0c\u5bf9\u516c\u5171\u8bdd\u8bed\u3001\u60c5\u611f\u548c\u51b3\u7b56\u9020\u6210\u5a01\u80c1\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u5f0f\uff0c\u4f46\u672c\u6587\u6240\u7814\u7a76\u7684\u7279\u5b9a\u53d8\u6362\uff08\u5982\u591a\u8bed\u8a00\u5207\u6362\u548c\u683c\u5f0f\u91cd\u7ec4\uff09\u5c1a\u672a\u88ab\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u591a\u667a\u80fd\u4f53\u7684\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5e76\u53ef\u4ee5\u4f5c\u4e3a\u7f51\u9875\u63d2\u4ef6\u90e8\u7f72\u5230\u5728\u7ebf\u5e73\u53f0\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u5207\u6362\u3001\u7ffb\u8bd1\u3001\u67e5\u8be2\u957f\u5ea6\u81a8\u80c0\u4ee5\u53ca\u7ed3\u6784\u91cd\u7ec4\uff08\u5982\u8f6c\u5316\u4e3a\u591a\u9879\u9009\u62e9\u9898\uff09\u7b49\u591a\u79cd\u653b\u51fb\u624b\u6bb5\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u8868\u660eAI\u9a71\u52a8\u7684\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u5bf9\u4e8e\u4fdd\u62a4\u5728\u7ebf\u4e8b\u5b9e\u5b8c\u6574\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u63d2\u4ef6\u5316\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u8bed\u8a00\u548c\u591a\u79cd\u653b\u51fb\u5f62\u5f0f\u7684\u7efc\u5408\u68c0\u6d4b\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u5728\u7ebf\u5e73\u53f0\u5bf9\u6297\u591a\u6837\u5316\u9519\u8bef\u4fe1\u606f\u653b\u51fb\u7684\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2510.08827", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.08827", "abs": "https://arxiv.org/abs/2510.08827", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "McMining: Automated Discovery of Misconceptions in Student Code", "comment": "16 pages, 8 figures", "summary": "When learning to code, students often develop misconceptions about various\nprogramming language concepts. These can not only lead to bugs or inefficient\ncode, but also slow down the learning of related concepts. In this paper, we\nintroduce McMining, the task of mining programming misconceptions from samples\nof code from a student. To enable the training and evaluation of McMining\nsystems, we develop an extensible benchmark dataset of misconceptions together\nwith a large set of code samples where these misconceptions are manifested. We\nthen introduce two LLM-based McMiner approaches and through extensive\nevaluations show that models from the Gemini, Claude, and GPT families are\neffective at discovering misconceptions in student code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4ece\u5b66\u751f\u4ee3\u7801\u4e2d\u53d1\u73b0\u7f16\u7a0b\u8bef\u89e3\uff0c\u6784\u5efa\u4e86\u76f8\u5173\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u4e86 McMiner \u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u6d41 LLM \u80fd\u6709\u6548\u8bc6\u522b\u8bef\u89e3\uff0c\u5bf9\u7f16\u7a0b\u6559\u80b2\u6709\u79ef\u6781\u610f\u4e49\u3002", "motivation": "\u5b66\u751f\u5728\u5b66\u4e60\u7f16\u7a0b\u65f6\u5bb9\u6613\u4ea7\u751f\u5404\u79cd\u8bed\u8a00\u6982\u5ff5\u4e0a\u7684\u8bef\u89e3\uff0c\u8fd9\u4e0d\u4ec5\u4f1a\u5bfc\u81f4\u7a0b\u5e8f\u51fa\u73b0 bug \u6216\u6548\u7387\u4f4e\u4e0b\uff0c\u8fd8\u4f1a\u5f71\u54cd\u540e\u7eed\u76f8\u5173\u77e5\u8bc6\u7684\u5b66\u4e60\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u53d1\u73b0\u5b66\u751f\u7684\u7f16\u7a0b\u8bef\u89e3\u3002", "method": "\u63d0\u51fa\u4e86 McMining \u4efb\u52a1\uff0c\u5373\u4ece\u5b66\u751f\u4ee3\u7801\u4e2d\u6316\u6398\u7f16\u7a0b\u8bef\u89e3\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u7f16\u7a0b\u8bef\u89e3\u53ca\u5176\u5177\u4f53\u4ee3\u7801\u8868\u73b0\u7684\u5927\u578b\u53ef\u6269\u5c55\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u968f\u540e\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bbe\u8ba1\u4e86\u4e24\u79cd McMiner \u65b9\u6cd5\u7528\u4e8e\u53d1\u73b0\u7f16\u7a0b\u8bef\u89e3\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u53d1\u73b0 Gemini\u3001Claude \u548c GPT \u7b49\u7cfb\u5217\u7684\u5927\u6a21\u578b\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5b66\u751f\u4ee3\u7801\u4e2d\u7684\u8bef\u89e3\u3002", "conclusion": "\u81ea\u52a8\u5316\u6316\u6398\u5b66\u751f\u4ee3\u7801\u8bef\u89e3\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\uff0c\u91c7\u7528 LLM \u80fd\u4e3a\u7f16\u7a0b\u6559\u80b2\u548c\u76f8\u5173\u5de5\u5177\u5e26\u6765\u65b0\u7684\u5e2e\u52a9\u3002"}}
{"id": "2510.08606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08606", "abs": "https://arxiv.org/abs/2510.08606", "authors": ["Yu Liu", "Hanlei Shi", "Haoxun Li", "Yuqing Sun", "Yuxuan Ding", "Linlin Gong", "Leyuan Qu", "Taihao Li"], "title": "Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations", "comment": "Under review for ICASSP 2026", "summary": "Emotion Recognition in Conversations (ERC) is hard because discriminative\nevidence is sparse, localized, and often asynchronous across modalities. We\ncenter ERC on emotion hotspots and present a unified model that detects\nper-utterance hotspots in text, audio, and video, fuses them with global\nfeatures via Hotspot-Gated Fusion, and aligns modalities using a routed\nMixture-of-Aligners; a cross-modal graph encodes conversational structure. This\ndesign focuses modeling on salient spans, mitigates misalignment, and preserves\ncontext. Experiments on standard ERC benchmarks show consistent gains over\nstrong baselines, with ablations confirming the contributions of HGF and MoA.\nOur results point to a hotspot-centric view that can inform future multimodal\nlearning, offering a new perspective on modality fusion in ERC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u60c5\u611f\u70ed\u70b9\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u68c0\u6d4b\u8bed\u53e5\u7ea7\u70ed\u70b9\u5e76\u8fdb\u884c\u6709\u6548\u878d\u5408\u548c\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u8f83\u5927\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "motivation": "\u60c5\u611f\u8bc6\u522b\u4e2d\uff0c\u5224\u522b\u6027\u8bc1\u636e\u7a00\u758f\u4e14\u901a\u5e38\u5728\u4e0d\u540c\u6a21\u6001\u95f4\u5f02\u6b65\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u6781\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\u4e9f\u9700\u521b\u65b0\u65b9\u6cd5\u6316\u6398\u548c\u878d\u5408\u591a\u6a21\u6001\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u89e3\u51b3\u6a21\u6001\u4e4b\u95f4\u7684\u9519\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6a21\u578b\uff0c\u68c0\u6d4b\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u4e2d\u7684\u6bcf\u53e5\u8bdd\u60c5\u611f\u70ed\u70b9\uff0c\u5c06\u5176\u901a\u8fc7Hotspot-Gated Fusion\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\uff0c\u5e76\u7528\u5206\u8def\u6df7\u5408\u5bf9\u9f50\u5668\uff08Mixture-of-Aligners\uff09\u5b9e\u73b0\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u540c\u65f6\u5229\u7528\u8de8\u6a21\u6001\u56fe\u5efa\u6a21\u5bf9\u8bdd\u7ed3\u6784\u3002", "result": "\u5728\u6807\u51c6ERC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u6d88\u878d\u5b9e\u9a8c\u4e5f\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0HGF\u548cMoA\u6a21\u5757\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u70ed\u70b9\u4e2d\u5fc3\u6a21\u578b\u5728\u6807\u51c6ERC\u57fa\u51c6\u4e0a\u76f8\u8f83\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u6709\u4e00\u81f4\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86Hotspot-Gated Fusion (HGF) \u548c Mixture-of-Aligners (MoA) \u7684\u6709\u6548\u8d21\u732e\u3002\u8be5\u7ed3\u679c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u5b66\u4e60\u548c\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.08834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08834", "abs": "https://arxiv.org/abs/2510.08834", "authors": ["Carlos Pinto Gomez", "Fabio Petrillo"], "title": "Identifying Video Game Debugging Bottlenecks: An Industry Perspective", "comment": "8 pages, 3 figures, 4 tables, gas 2026 conference submission", "summary": "Conventional debugging techniques used in traditional software are similarly\nused when debugging video games. However, the reality of video games require\nits own set of unique debugging techniques such as On-Screen Console, Debug\nDraws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this\narticle, we provide insights from a video game studio on how 20 seasoned\nindustry game developers debug during the production of a game. Our experiments\nrely on the recordings of debugging sessions for the most critical bugs\ncategorized as Crashes, Object Behaviors, and Object Persistence. In this\npaper, we focus on identifying the debugging activities that bottleneck bug\nresolution. We also identify the debugging tools used to perform debugging\ntechniques. Lastly, we present how different disciplines collaborate during\ndebugging and how technical roles are at the core of debugging. Our thematic\nanalysis has identified game developers spend 36.6\\% of their time inspecting\ngame artifacts and 35.1\\% of their time reproducing the bug locally.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u884c\u4e1a\u5f00\u53d1\u8005\u8c03\u8bd5\u884c\u4e3a\u7684\u5b9e\u9645\u89c2\u6d4b\uff0c\u6587\u7ae0\u63ed\u793a\u4e86\u6e38\u620f\u5f00\u53d1\u4e2d\u74f6\u9888\u6d3b\u52a8\u3001\u534f\u4f5c\u6a21\u5f0f\u53ca\u5de5\u5177\u4f7f\u7528\uff0c\u4e3a\u4f18\u5316\u8c03\u8bd5\u6d41\u7a0b\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6491\u3002", "motivation": "\u867d\u7136\u4f20\u7edf\u8f6f\u4ef6\u8c03\u8bd5\u6280\u672f\u540c\u6837\u9002\u7528\u4e8e\u6e38\u620f\u5f00\u53d1\uff0c\u4f46\u6e38\u620f\u7531\u4e8e\u5176\u72ec\u7279\u6027\uff0c\u9700\u8981\u7279\u6709\u7684\u8c03\u8bd5\u65b9\u6cd5\u3002\u9488\u5bf9\u884c\u4e1a\u4e2d\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u5e2e\u52a9\u4f18\u5316\u6e38\u620f\u8c03\u8bd5\u6d41\u7a0b\u3002", "method": "\u901a\u8fc7\u5bf9\u4e00\u5bb6\u6e38\u620f\u5de5\u4f5c\u5ba420\u4f4d\u8d44\u6df1\u5f00\u53d1\u8005\u8fdb\u884c\u8c03\u8bd5\u4f1a\u8bdd\u5f55\u97f3\uff0c\u5206\u6790\u4ed6\u4eec\u5728\u5904\u7406\u4e09\u7c7b\u5173\u952ebug\uff08\u5d29\u6e83\u3001\u5bf9\u8c61\u884c\u4e3a\u3001\u5bf9\u8c61\u6301\u4e45\u6027\uff09\u65f6\u7684\u8c03\u8bd5\u6d3b\u52a8\uff0c\u501f\u52a9\u4e3b\u9898\u5206\u6790\u6cd5\u5bf9\u6d41\u7a0b\u3001\u5de5\u5177\u53ca\u8de8\u90e8\u95e8\u534f\u4f5c\u60c5\u51b5\u8fdb\u884c\u5f52\u7eb3\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u6e38\u620f\u5f00\u53d1\u8005\u8c03\u8bd5\u65f6\u670936.6%\u65f6\u95f4\u7528\u4e8e\u68c0\u67e5\u6e38\u620f\u5de5\u4ef6\uff0c35.1%\u65f6\u95f4\u7528\u4e8e\u672c\u5730\u590d\u73b0bug\uff1b\u8bc6\u522b\u51fa\u5f71\u54cdbug\u5b9a\u4f4d\u6548\u7387\u7684\u4e3b\u8981\u8c03\u8bd5\u6d3b\u52a8\u53ca\u76f8\u5173\u8c03\u8bd5\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u6280\u672f\u89d2\u8272\u5728\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u7684\u6838\u5fc3\u5730\u4f4d\u3002", "conclusion": "\u6e38\u620f\u5f00\u53d1\u9700\u8981\u7ed3\u5408\u4f20\u7edf\u4e0e\u4e13\u6709\u8c03\u8bd5\u624b\u6bb5\uff0c\u9ad8\u6548\u534f\u4f5c\u4e0e\u5408\u9002\u5de5\u5177\u53ef\u663e\u8457\u74f6\u9888\u8c03\u8bd5\u6d41\u7a0b\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2510.08608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08608", "abs": "https://arxiv.org/abs/2510.08608", "authors": ["Weihua Zheng", "Zhengyuan Liu", "Tanmoy Chakraborty", "Weiwen Xu", "Xiaoxue Gao", "Bryan Chen Zhengyu Tan", "Bowei Zou", "Chang Liu", "Yujia Hu", "Xing Xie", "Xiaoyuan Yi", "Jing Yao", "Chaojun Wang", "Long Li", "Rui Liu", "Huiyao Liu", "Koji Inoue", "Ryuichi Sumida", "Tatsuya Kawahara", "Fan Xu", "Lingyu Ye", "Wei Tian", "Dongjun Kim", "Jimin Jung", "Jaehyung Seo", "Nadya Yuki Wangsajaya", "Pham Minh Duc", "Ojasva Saxena", "Palash Nandi", "Xiyan Tao", "Wiwik Karlina", "Tuan Luong", "Keertana Arun Vasan", "Roy Ka-Wei Lee", "Nancy F. Chen"], "title": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation", "comment": null, "summary": "Large language models (LLMs) are now used worldwide, yet their multimodal\nunderstanding and reasoning often degrade outside Western, high-resource\nsettings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'\ncultural awareness with a focus on Asian contexts. MMA-ASIA centers on a\nhuman-curated, multilingual, and multimodally aligned multiple-choice benchmark\ncovering 8 Asian countries and 10 languages, comprising 27,000 questions; over\n79 percent require multi-step reasoning grounded in cultural context, moving\nbeyond simple memorization. To our knowledge, this is the first dataset aligned\nat the input level across three modalities: text, image (visual question\nanswering), and speech. This enables direct tests of cross-modal transfer.\nBuilding on this benchmark, we propose a five-dimensional evaluation protocol\nthat measures: (i) cultural-awareness disparities across countries, (ii)\ncross-lingual consistency, (iii) cross-modal consistency, (iv) cultural\nknowledge generalization, and (v) grounding validity. To ensure rigorous\nassessment, a Cultural Awareness Grounding Validation Module detects \"shortcut\nlearning\" by checking whether the requisite cultural knowledge supports correct\nanswers. Finally, through comparative model analysis, attention tracing, and an\ninnovative Vision-ablated Prefix Replay (VPR) method, we probe why models\ndiverge across languages and modalities, offering actionable insights for\nbuilding culturally reliable multimodal LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMMA-ASIA\u8bc4\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u4e9a\u6d32\u8bed\u5883\u4e0b\u6587\u5316\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u72ec\u521b\u4e09\u6a21\u6001\u4e25\u683c\u5bf9\u9f50\u57fa\u51c6\u548c\u4e94\u7ef4\u8bc4\u4ef7\u4f53\u7cfb\uff0c\u6df1\u5165\u5206\u6790\u4e86\u6a21\u578b\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u6587\u5316\u53ef\u9760\u7684\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6301\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e9a\u6d32\u7b49\u975e\u897f\u65b9\u3001\u4f4e\u8d44\u6e90\u8bed\u5883\u4e0b\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff0c\u4e9f\u9700\u7cfb\u7edf\u4e14\u6587\u5316\u654f\u611f\u7684\u8bc4\u6d4b\u65b9\u5f0f\uff0c\u4ee5\u63a8\u52a8\u6a21\u578b\u6cdb\u5316\u548c\u9002\u5e94\u672c\u5730\u6587\u5316\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86MMA-ASIA\u57fa\u51c6\uff0c\u5305\u62ec\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u8bed\u97f3\uff09\u4e25\u683c\u5bf9\u9f50\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u8986\u76d68\u4e2a\u4e9a\u6d32\u56fd\u5bb610\u79cd\u8bed\u8a00\u3002\u540c\u65f6\u521b\u65b0\u6027\u5f15\u5165\u4e94\u7ef4\u8bc4\u4ef7\u534f\u8bae\u548c\u201c\u6587\u5316\u8ba4\u77e5\u624e\u6839\u9a8c\u8bc1\u6a21\u5757\u201d\uff0c\u7ed3\u5408\u6a21\u578b\u5bf9\u6bd4\u3001\u6ce8\u610f\u529b\u8ffd\u8e2a\u548cVPR\u65b9\u6cd5\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "MMA-ASIA\u9996\u6b21\u5b9e\u73b0\u4e09\u6a21\u6001\u8f93\u5165\u4e25\u683c\u5bf9\u9f50\uff0c\u57fa\u51c6\u5305\u542b2.7\u4e07\u9898\u4e14\u5927\u591a\u6570\u9700\u591a\u6b65\u6587\u5316\u63a8\u7406\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u6a21\u578b\u5728\u8de8\u56fd\u5bb6\u3001\u8de8\u8bed\u8a00\u3001\u8de8\u6a21\u6001\u65b9\u9762\u7684\u5dee\u5f02\u53ca\u5176\u6587\u5316\u624e\u6839\u6027\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u6587\u5316\u9002\u5e94\u6027\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86MMA-ASIA\u6846\u67b6\uff0c\u5e76\u8bc1\u5b9e\u73b0\u6709\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u4e9a\u6d32\u8bed\u5883\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u540c\u65f6\u63d0\u51fa\u591a\u7ef4\u8bc4\u4ef7\u4e0e\u5206\u6790\u65b9\u6cd5\u4e3a\u63d0\u5347\u6a21\u578b\u7684\u6587\u5316\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.08850", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08850", "abs": "https://arxiv.org/abs/2510.08850", "authors": ["Vasudha Yanuganti", "Ishaan Puri", "Swapnil Chhatre", "Mantinder Singh", "Ashok Jallepalli", "Hritvik Shrivastava", "Pradeep Kumar Sharma"], "title": "Repository-Aware File Path Retrieval via Fine-Tuned LLMs", "comment": null, "summary": "Modern codebases make it hard for developers and AI coding assistants to find\nthe right source files when answering questions like \"How does this feature\nwork?\" or \"Where was the bug introduced?\" Traditional code search (keyword or\nIR based) often misses semantic context and cross file links, while large\nlanguage models (LLMs) understand natural language but lack repository specific\ndetail. We present a method for file path retrieval that fine tunes a strong\nLLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file\npaths directly from a natural language query. To build training data, we\nintroduce six code aware strategies that use abstract syntax tree (AST)\nstructure and repository content to generate realistic question-answer pairs,\nwhere answers are sets of file paths. The strategies range from single file\nprompts to hierarchical repository summaries, providing broad coverage. We fine\ntune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,\nand obtain high retrieval accuracy: up to 91\\% exact match and 93\\% recall on\nheld out queries, clearly beating single strategy training. On a large codebase\nlike PyTorch (about 4,000 Python files), the model reaches 59\\% recall, showing\nscalability. We analyze how multi level code signals help the LLM reason over\ncross file context and discuss dataset design, limits (for example, context\nlength in very large repos), and future integration of retrieval with LLM based\ncode intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c42\u6b21\u4ee3\u7801\u4fe1\u606f\u3001\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u6587\u4ef6\u8def\u5f84\u68c0\u7d22\u65b9\u6cd5\uff0c\u5728\u591a\u4e2aPython\u9879\u76ee\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u6e90\u6587\u4ef6\u7684\u68c0\u7d22\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u4e2d\u8868\u73b0\u51fa\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5c55\u793a\u4e86\u591a\u7b56\u7565\u6570\u636e\u96c6\u5728\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u53ec\u56de\u4e0a\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u5e93\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u5f00\u53d1\u8005\u548cAI\u52a9\u624b\u5728\u56de\u7b54\u5982\u201c\u8fd9\u4e2a\u529f\u80fd\u5982\u4f55\u5b9e\u73b0\uff1f\u201d\u6216\u201c\u6f0f\u6d1e\u662f\u5982\u4f55\u5f15\u5165\u7684\uff1f\u201d\u7b49\u95ee\u9898\u65f6\uff0c\u5b9a\u4f4d\u76f8\u5173\u6e90\u6587\u4ef6\u53d8\u5f97\u6108\u52a0\u56f0\u96be\u3002\u4f20\u7edf\u7684\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5230\u4ee3\u7801\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u53ca\u8de8\u6587\u4ef6\u7684\u8054\u7cfb\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5584\u4e8e\u7406\u89e3\u81ea\u7136\u8bed\u8a00\uff0c\u5374\u7f3a\u4e4f\u4e0e\u7279\u5b9a\u4ee3\u7801\u5e93\u76f8\u5173\u7684\u8be6\u7ec6\u4fe1\u606f\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u4ef6\u8def\u5f84\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7QLoRA\u548cUnsloth\u4f18\u5316\uff0c\u5bf9\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578bQwen3-8B\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u53ef\u4ee5\u6839\u636e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u76f4\u63a5\u9884\u6d4b\u76f8\u5173\u7684\u6587\u4ef6\u8def\u5f84\u3002\u4e3a\u4e86\u6784\u5efa\u8bad\u7ec3\u6570\u636e\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u516d\u79cd\u7ed3\u5408\u4ee3\u7801\u7ed3\u6784\u3001\u5185\u5bb9\u7684\u7b56\u7565\uff0c\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u548c\u4ed3\u5e93\u4fe1\u606f\u751f\u6210\u73b0\u5b9e\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u7b54\u6848\u4e3a\u6587\u4ef6\u8def\u5f84\u96c6\u5408\u3002\u8fd9\u4e9b\u7b56\u7565\u8986\u76d6\u9762\u5e7f\uff0c\u4ece\u5355\u6587\u4ef6\u5230\u4ed3\u5e93\u7684\u5206\u5c42\u6458\u8981\u90fd\u6709\u3002\u6a21\u578b\u5728\u5305\u62ecFlask\u3001Click\u3001Jinja\u3001FastAPI\u53caPyTorch\u7b49Python\u9879\u76ee\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad891%\u7684\u7cbe\u786e\u5339\u914d\u7387\uff08exact match\uff09\u548c93%\u7684\u53ec\u56de\u7387\uff08recall\uff09\uff0c\u660e\u663e\u4f18\u4e8e\u53ea\u7528\u5355\u4e00\u7b56\u7565\u8bad\u7ec3\u3002\u5728PyTorch\u8fd9\u6837\u7684\u5927\u578b\u4ee3\u7801\u5e93\uff08\u7ea64,000\u4e2aPython\u6587\u4ef6\uff09\u4e0a\uff0c\u6a21\u578b\u53ec\u56de\u7387\u8fbe\u5230\u4e8659%\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u591a\u5c42\u6b21\u4ee3\u7801\u4fe1\u53f7\u6709\u52a9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8de8\u6587\u4ef6\u8bed\u5883\u63a8\u7406\uff0c\u4ece\u800c\u63d0\u5347\u4ee3\u7801\u8def\u5f84\u68c0\u7d22\u80fd\u529b\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u6570\u636e\u96c6\u8bbe\u8ba1\u3001\u6a21\u578b\u5728\u8d85\u5927\u4ed3\u5e93\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u53ca\u672a\u6765\u4e0e\u4ee3\u7801\u667a\u80fd\u878d\u5408\u7684\u5c55\u671b\u3002"}}
{"id": "2510.08613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08613", "abs": "https://arxiv.org/abs/2510.08613", "authors": ["Xinnan Dai", "Kai Guo", "Chung-Hsiang Lo", "Shenglai Zeng", "Jiayuan Ding", "Dongsheng Luo", "Subhabrata Mukherjee", "Jiliang Tang"], "title": "GraphGhost: Tracing Structures Behind Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable reasoning capabilities,\nyet the structural mechanisms underlying these abilities remain under explored.\nIn this work, we introduce GraphGhost, a unified framework that represents\nneuron activations and their signal propagation as graphs, explaining how LLMs\ncapture structural semantics from sequential inputs and generate outputs\nthrough structurally consistent mechanisms. This graph-based perspective\nenables us to employ graph algorithms such as PageRank to characterize the\nproperties of LLMs, revealing both shared and model-specific reasoning\nbehaviors across diverse datasets. We further identify the activated neurons\nwithin GraphGhost and evaluate them through structural interventions, showing\nthat edits to key neuron nodes can trigger reasoning collapse, altering both\nlogical flow and semantic understanding. Together, these contributions position\nGraphGhost as a powerful tool for analyzing, intervening in, and ultimately\nunderstanding the structural foundations of reasoning in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGraphGhost\uff0c\u7528\u56fe\u7ed3\u6784\u65b9\u6cd5\u89e3\u91ca\u548c\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5173\u952e\u795e\u7ecf\u5143\u8282\u70b9\u5bf9\u6a21\u578b\u63a8\u7406\u548c\u8bed\u4e49\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5176\u80cc\u540e\u7684\u7ed3\u6784\u6027\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7a76\u3002\u672c\u6587\u5e0c\u671b\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86GraphGhost\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u5143\u6fc0\u6d3b\u53ca\u5176\u4fe1\u53f7\u4f20\u64ad\u7528\u56fe\u7684\u65b9\u5f0f\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u56fe\u7b97\u6cd5\uff08\u4f8b\u5982PageRank\uff09\u5206\u6790LLM\u7684\u7ed3\u6784\u7279\u6027\u3002\u540c\u65f6\u901a\u8fc7\u7ed3\u6784\u6027\u5e72\u9884\uff0c\u5bf9\u5173\u952e\u795e\u7ecf\u5143\u8282\u70b9\u8fdb\u884c\u7f16\u8f91\uff0c\u8003\u5bdf\u5bf9\u6a21\u578b\u63a8\u7406\u548c\u8bed\u4e49\u7684\u5f71\u54cd\u3002", "result": "GraphGhost\u80fd\u63ed\u793aLLM\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5171\u4eab\u548c\u7279\u5b9a\u63a8\u7406\u884c\u4e3a\u3002\u901a\u8fc7\u5e72\u9884\u5173\u952e\u8282\u70b9\u53d1\u73b0\uff0c\u5bf9\u8fd9\u4e9b\u795e\u7ecf\u5143\u7684\u7f16\u8f91\u4f1a\u5f15\u53d1\u63a8\u7406\u5d29\u6e83\u3001\u6539\u53d8\u903b\u8f91\u6d41\u7a0b\u548c\u8bed\u4e49\u7406\u89e3\u3002", "conclusion": "GraphGhost\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5206\u6790\u3001\u5e72\u9884\u4e0e\u7406\u89e3LLM\u63a8\u7406\u7ed3\u6784\u57fa\u7840\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3LLM\u5185\u90e8\u7684\u7ed3\u6784\u6027\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.08876", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08876", "abs": "https://arxiv.org/abs/2510.08876", "authors": ["Kostiantyn Bevziuk", "Andrii Fatula", "Svetozar Lashin Yaroslav Opanasenko", "Anna Tukhtarova", "Ashok Jallepalli Pradeepkumar Sharma", "Hritvik Shrivastava"], "title": "Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval", "comment": null, "summary": "We present a repository decomposition system that converts large software\nrepositories into a vectorized knowledge graph which mirrors project\narchitectural and semantic structure, capturing semantic relationships and\nallowing a significant level of automatization of further repository\ndevelopment. The graph encodes syntactic relations such as containment,\nimplementation, references, calls, and inheritance, and augments nodes with\nLLM-derived summaries and vector embeddings. A hybrid retrieval pipeline\ncombines semantic retrieval with graph-aware expansion, and an LLM-based\nassistant formulates constrained, read-only graph requests and produces\nhuman-oriented explanations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u52a8\u5c06\u4ee3\u7801\u5e93\u8f6c\u5316\u4e3a\u77e5\u8bc6\u56fe\u8c31\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u4e49\u68c0\u7d22\u4e0e\u81ea\u52a8\u5316\u5f00\u53d1\u652f\u6301\u3002", "motivation": "\u9762\u5bf9\u5927\u578b\u4ee3\u7801\u5e93\u7ed3\u6784\u590d\u6742\u3001\u5f00\u53d1\u4e0e\u7ef4\u62a4\u96be\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u5de5\u5177\u8f85\u52a9\u7406\u89e3\u4ee3\u7801\u8bed\u4e49\u4e0e\u67b6\u6784\uff0c\u8fdb\u800c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u5bf9\u5927\u578b\u8f6f\u4ef6\u4ed3\u5e93\u8fdb\u884c\u8f6c\u6362\uff0c\u7f16\u7801\u5982\u5305\u542b\u3001\u5b9e\u73b0\u3001\u5f15\u7528\u3001\u8c03\u7528\u4e0e\u7ee7\u627f\u7b49\u8bed\u6cd5\u5173\u7cfb\uff0c\u540c\u65f6\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8282\u70b9\u6458\u8981\u548c\u5411\u91cf\u5d4c\u5165\u3002\u68c0\u7d22\u6d41\u7a0b\u4e0a\uff0c\u878d\u5408\u4e86\u8bed\u4e49\u68c0\u7d22\u4e0e\u56fe\u7ed3\u6784\u6269\u5c55\uff0c\u5e76\u914d\u5907\u4e86\u80fd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u63a8\u7406\u548c\u89e3\u91ca\u7684LLM\u52a9\u624b\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u81ea\u52a8\u6784\u5efa\u7ec6\u7c92\u5ea6\u7684\u4ee3\u7801\u77e5\u8bc6\u56fe\u8c31\uff0c\u6781\u5927\u63d0\u5347\u4e86\u4ee3\u7801\u5e93\u8bed\u4e49\u7406\u89e3\u4e0e\u68c0\u7d22\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u5316\u548c\u667a\u80fd\u5316\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4ee3\u7801\u5e93\u5206\u89e3\u7cfb\u7edf\uff0c\u80fd\u591f\u6784\u5efa\u53cd\u6620\u9879\u76ee\u67b6\u6784\u4e0e\u8bed\u4e49\u7ed3\u6784\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u4fc3\u8fdb\u4ee3\u7801\u5e93\u540e\u7eed\u5f00\u53d1\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.08614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08614", "abs": "https://arxiv.org/abs/2510.08614", "authors": ["Mingxuan Liu", "Yuhe Ke", "Wentao Zhu", "Mayli Mertens", "Yilin Ning", "Jingchi Liao", "Chuan Hong", "Daniel Shu Wei Ting", "Yifan Peng", "Danielle S. Bitterman", "Marcus Eng Hock Ong", "Nan Liu"], "title": "Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications", "comment": null, "summary": "The integration of large language models (LLMs) into healthcare holds promise\nto enhance clinical decision-making, yet their susceptibility to biases remains\na critical concern. Gender has long influenced physician behaviors and patient\noutcomes, raising concerns that LLMs assuming human-like roles, such as\nclinicians or medical educators, may replicate or amplify gender-related\nbiases. Using case studies from the New England Journal of Medicine Challenge\n(NEJM), we assigned genders (female, male, or unspecified) to multiple\nopen-source and proprietary LLMs. We evaluated their response consistency\nacross LLM-gender assignments regarding both LLM-based diagnosis and models'\njudgments on the clinical relevance or necessity of patient gender. In our\nfindings, diagnoses were relatively consistent across LLM genders for most\nmodels. However, for patient gender's relevance and necessity in LLM-based\ndiagnosis, all models demonstrated substantial inconsistency across LLM\ngenders, particularly for relevance judgements. Some models even displayed a\nsystematic female-male disparity in their interpretation of patient gender.\nThese findings present an underexplored bias that could undermine the\nreliability of LLMs in clinical practice, underscoring the need for routine\nchecks of identity-assignment consistency when interacting with LLMs to ensure\nreliable and equitable AI-supported clinical care.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u533b\u7597\u9886\u57dfLLM\u5728\u60a3\u8005\u6027\u522b\u76f8\u5173\u6027\u5224\u65ad\u4e0a\u7684\u4e0d\u4e00\u81f4\u53ca\u6027\u522b\u504f\u89c1\uff0c\u5f3a\u8c03\u5728\u5b9e\u9645\u5e94\u7528\u524d\u9700\u5e38\u89c4\u68c0\u67e5\u5176\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4ee5\u4fdd\u8bc1AI\u4e34\u5e8a\u8f85\u52a9\u7684\u53ef\u9760\u6027\u4e0e\u516c\u5e73\u6027\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u6a21\u62df\u533b\u7597\u89d2\u8272\u65f6\u662f\u5426\u4f1a\u91cd\u590d\u6216\u653e\u5927\u6027\u522b\u76f8\u5173\u504f\u89c1\uff0c\u5c24\u5176\u5728\u4e34\u5e8a\u51b3\u7b56\u573a\u666f\u4e2d\uff0c\u8fd9\u79cd\u504f\u89c1\u53ef\u80fd\u5f71\u54cd\u60a3\u8005\u6cbb\u7597\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u5bf9\u591a\u79cd\u5f00\u6e90\u53ca\u5546\u4e1aLLM\u5728NEJM\u75c5\u4f8b\u6311\u6218\u4e2d\u7684\u6027\u522b\u8eab\u4efd\u8bbe\u5b9a\uff08\u5973\u6027\u3001\u7537\u6027\u6216\u672a\u6307\u5b9a\uff09\uff0c\u5206\u6790\u5176\u8bca\u65ad\u4e00\u81f4\u6027\u53ca\u5bf9\u60a3\u8005\u6027\u522b\u76f8\u5173\u6027\u7684\u5224\u65ad\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u8bca\u65ad\u4e0a\u6027\u522b\u4e00\u81f4\u6027\u8f83\u597d\uff0c\u4f46\u5728\u5224\u65ad\u60a3\u8005\u6027\u522b\u91cd\u8981\u6027\u548c\u76f8\u5173\u6027\u65f6\u8868\u73b0\u51fa\u8f83\u5927\u4e0d\u4e00\u81f4\uff0c\u4e14\u90e8\u5206\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u7537\u5973\u5dee\u5f02\uff0c\u663e\u793a\u51fa\u5ffd\u89c6\u6216\u52a0\u5267\u6027\u522b\u504f\u89c1\u7684\u98ce\u9669\u3002", "conclusion": "\u672c\u6587\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u8bca\u65ad\u65f6\u5bf9\u60a3\u8005\u6027\u522b\u76f8\u5173\u6027\u7684\u5224\u65ad\u5b58\u5728\u663e\u8457\u7684\u4e0d\u4e00\u81f4\u548c\u6027\u522b\u5dee\u5f02\uff0c\u8fd9\u79cd\u504f\u89c1\u53ef\u80fd\u5f71\u54cd\u5176\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u9760\u6027\u4e0e\u516c\u5e73\u6027\u3002"}}
{"id": "2510.08981", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08981", "abs": "https://arxiv.org/abs/2510.08981", "authors": ["Mandira Roy", "Novarun Deb", "Nabendu Chaki", "Agostino Cortesi"], "title": "SEER: Sustainability Enhanced Engineering of Software Requirements", "comment": "Main Paper: 32 pages, References: 3 pages, Appendix: 13 pages.\n  Submitted to the Journal of Systems and Software, Elsevier", "summary": "The rapid expansion of software development has significant environmental,\ntechnical, social, and economic impacts. Achieving the United Nations\nSustainable Development Goals by 2030 compels developers to adopt sustainable\npractices. Existing methods mostly offer high-level guidelines, which are\ntime-consuming to implement and rely on team adaptability. Moreover, they focus\non design or implementation, while sustainability assessment should start at\nthe requirements engineering phase. In this paper, we introduce SEER, a\nframework which addresses sustainability concerns in the early software\ndevelopment phase. The framework operates in three stages: (i) it identifies\nsustainability requirements (SRs) relevant to a specific software product from\na general taxonomy; (ii) it evaluates how sustainable system requirements are\nbased on the identified SRs; and (iii) it optimizes system requirements that\nfail to satisfy any SR. The framework is implemented using the reasoning\ncapabilities of large language models and the agentic RAG (Retrieval Augmented\nGeneration) approach. SEER has been experimented on four software projects from\ndifferent domains. Results generated using Gemini 2.5 reasoning model\ndemonstrate the effectiveness of the proposed approach in accurately\nidentifying a broad range of sustainability concerns across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEER\u6846\u67b6\uff0c\u4f9d\u6258\u5927\u8bed\u8a00\u6a21\u578b\u4e0eRAG\u65b9\u6cd5\uff0c\u80fd\u5728\u8f6f\u4ef6\u9700\u6c42\u9636\u6bb5\u9ad8\u6548\u8bc6\u522b\u548c\u4f18\u5316\u53ef\u6301\u7eed\u6027\u9700\u6c42\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u884c\u4e1a\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8f6f\u4ef6\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "motivation": "\u4e3a\u4e86\u63a8\u52a8\u5b9e\u73b0\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u5fc5\u987b\u91c7\u7528\u66f4\u53ef\u6301\u7eed\u7684\u53d1\u5c55\u5b9e\u8df5\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u9ad8\u5c42\u6307\u5bfc\uff0c\u5b9e\u65bd\u8017\u65f6\u4e14\u4f9d\u8d56\u56e2\u961f\u9002\u5e94\u6027\uff0c\u5e76\u4e14\u53ea\u5173\u6ce8\u8bbe\u8ba1\u6216\u5b9e\u73b0\u9636\u6bb5\uff0c\u5ffd\u89c6\u4e86\u9700\u6c42\u5de5\u7a0b\u9636\u6bb5\u7684\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SEER\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u8f6f\u4ef6\u5f00\u53d1\u521d\u671f\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\u5206\u6790\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e09\u6b65\uff1a1\uff09\u4ece\u901a\u7528\u5206\u7c7b\u4e2d\u8bc6\u522b\u7279\u5b9a\u8f6f\u4ef6\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff08SRs\uff09\uff1b2\uff09\u57fa\u4e8eSRs\u8bc4\u4f30\u7cfb\u7edf\u9700\u6c42\u7684\u53ef\u6301\u7eed\u6027\uff1b3\uff09\u9488\u5bf9\u4e0d\u6ee1\u8db3SRs\u7684\u7cfb\u7edf\u9700\u6c42\u8fdb\u884c\u4f18\u5316\u3002SEER\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u667a\u80fdRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u6280\u672f\u5b9e\u73b0\u3002", "result": "SEER\u6846\u67b6\u5728\u4e0d\u540c\u884c\u4e1a\u7684\u56db\u4e2a\u8f6f\u4ef6\u9879\u76ee\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7Gemini 2.5\u6a21\u578b\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u5404\u79cd\u53ef\u6301\u7eed\u6027\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u6837\u9886\u57df\u3002", "conclusion": "SEER\u6846\u67b6\u53ef\u5728\u8f6f\u4ef6\u5f00\u53d1\u65e9\u671f\u51c6\u786e\u8bc6\u522b\u548c\u4f18\u5316\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff0c\u5c06\u53ef\u6301\u7eed\u53d1\u5c55\u66f4\u597d\u5730\u878d\u5165\u8f6f\u4ef6\u9700\u6c42\u5de5\u7a0b\u6d41\u7a0b\u3002\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e0eRAG\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u8de8\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.08615", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08615", "abs": "https://arxiv.org/abs/2510.08615", "authors": ["Kaiqi Yang", "Hang Li", "Yucheng Chu", "Zitao Liu", "Mi Tian", "Hui Liu"], "title": "Iterative LLM-Based Generation and Refinement of Distracting Conditions in Math Word Problems", "comment": null, "summary": "Mathematical reasoning serves as a crucial testbed for evaluating the\nintelligence of large language models (LLMs), and math word problems (MWPs)\nrepresent one of the most widely used formats. Most existing MWP datasets\ncontain only the necessary information, while problems with distracting or\nexcessive conditions are often overlooked. Prior studies have shown that\npopular LLMs experience a dramatic performance drop when such distracting\nconditions are introduced. However, available datasets of MWPs with distracting\nconditions remain limited, and most exhibit low difficulty and out-of-context\nexpressions. These shortcomings make the distracting conditions easy to detect\nand disregard, thereby reducing the credibility of benchmarking on these\ndatasets. Moreover, when distracting conditions are added, the reasoning\nprocess and answers may change, requiring intensive manual effort to check and\nrewrite solutions.\n  To address these issues, we design an iterative framework that leverages LLMs\nto generate distracting conditions automatically. We develop a set of prompts\nto revise MWPs from multiple perspectives and cognitive levels, encouraging the\ncreation of meaningful distracting conditions as well as suggestions for\nfurther refinement. A key advantage of our framework is the preservation of\nshared solutions between the original and revised problems: the LLMs are\nexplicitly guided to generate distractions that do not alter the original\nsolution, thus eliminating the need to produce new answers. This framework is\nefficient and easy to deploy, substantially reducing the effort required to\ngenerate MWPs with distracting conditions while maintaining high data quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4e3a\u6570\u5b66\u6587\u5b57\u9898\u751f\u6210\u5e72\u6270\u6761\u4ef6\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u96c6\u4e30\u5bcc\u6027\u548c\u4f7f\u7528\u6548\u7387\uff0c\u65e0\u9700\u4eba\u5de5\u91cd\u65b0\u6821\u5bf9\u7b54\u6848\uff0c\u9002\u5408\u5927\u89c4\u6a21\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u6587\u5b57\u9898\u6570\u636e\u96c6\u901a\u5e38\u4ec5\u5305\u542b\u5fc5\u9700\u4fe1\u606f\uff0c\u7f3a\u5c11\u5e72\u6270\u6216\u5197\u4f59\u6761\u4ef6\u3002\u73b0\u6709\u5c11\u6570\u5e72\u6270\u6761\u4ef6\u9898\u76ee\u7684\u6570\u636e\u96c6\u5b58\u5728\u96be\u5ea6\u4f4e\u3001\u5bb9\u6613\u8bc6\u522b\u7b49\u95ee\u9898\uff0c\u4e14\u6dfb\u52a0\u5e72\u6270\u540e\u9700\u5927\u91cf\u4eba\u5de5\u6821\u5bf9\u7b54\u6848\uff0c\u5f71\u54cd\u6d4b\u8bc4\u6548\u679c\u548c\u6570\u636e\u96c6\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fed\u4ee3\u5f0f\u7684\u6846\u67b6\uff0c\u501f\u52a9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u89d2\u5ea6\u3001\u591a\u8ba4\u77e5\u5c42\u6b21\u7684\u63d0\u793a\u8bcd\uff0c\u5f15\u5bfcLLMs\u81ea\u52a8\u751f\u6210\u6709\u610f\u4e49\u4e14\u4e0d\u5f71\u54cd\u539f\u7b54\u6848\u7684\u5e72\u6270\u6761\u4ef6\uff0c\u5e76\u4fdd\u7559\u539f\u9898\u548c\u4fee\u8ba2\u9898\u76ee\u6807\u51c6\u89e3\u3002", "result": "\u8be5\u6846\u67b6\u9ad8\u6548\u6613\u7528\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u5e26\u5e72\u6270\u6761\u4ef6\u7684\u6570\u5b66\u9898\u751f\u6210\u548c\u6821\u5bf9\u7684\u4eba\u5de5\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u8bc1\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u548c\u95ee\u9898\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u751f\u6210\u6570\u5b66\u6587\u5b57\u9898\u5e72\u6270\u6761\u4ef6\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u539f\u9898\u7b54\u6848\u7684\u524d\u63d0\u4e0b\uff0c\u5feb\u901f\u9ad8\u8d28\u91cf\u5730\u6269\u5145\u9898\u76ee\u7c7b\u578b\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u4fee\u8ba2\u3002"}}
{"id": "2510.08990", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08990", "abs": "https://arxiv.org/abs/2510.08990", "authors": ["Mandira Roy", "Novarun Deb", "Nabendu Chaki", "Agostino Cortesi"], "title": "Towards a Taxonomy of Sustainability Requirements for Software Design", "comment": "Paper: 7 pages", "summary": "Software systems are a significant contributor to global sustainability\nconcerns, demanding that environmental, social, technical, and economic factors\nbe systematically addressed from the initial requirements engineering phase.\nAlthough existing research provides various sustainability requirements (SRs),\nthese contributions are often fragmented, specific to certain dimensions, or\nlimited to particular application domains, resulting in a critical lack of a\nunified, comprehensive taxonomy for the software engineering community. To\naddress this gap, this research conducts a Systematic Literature Review (SLR)\nto extract and organize sustainability requirements from the state-of-the-art.\nThe primary contribution is a comprehensive taxonomy of SRs across the four\ndimensions of sustainability (environmental, technical, social, and economic).\nFor each identified category, we provide clear definitions, associated metrics,\nand measures. Furthermore, we depict a correlation matrix that projects the\npositive and negative influences (synergies and conflicts) among categories\nacross different dimensions. This systematized reference assists both software\ndevelopers and researchers in effectively formulating, managing, and\nreconciling trade-offs within sustainable software development.", "AI": {"tldr": "\u672c\u7814\u7a76\u57fa\u4e8e\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u5efa\u7acb\u4e86\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u9700\u6c42\u7684\u5168\u9762\u5206\u7c7b\u4f53\u7cfb\u53ca\u5176\u5b9a\u4e49\u3001\u6307\u6807\u548c\u5ea6\u91cf\uff0c\u5e76\u5206\u6790\u4e86\u5404\u7ef4\u5ea6\u9700\u6c42\u95f4\u7684\u534f\u540c\u4e0e\u51b2\u7a81\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u8005\u9ad8\u6548\u5b9e\u73b0\u53ef\u6301\u7eed\u5f00\u53d1\u76ee\u6807\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u53c2\u8003\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u7cfb\u7edf\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u95ee\u9898\u8d21\u732e\u663e\u8457\uff0c\u9700\u8981\u5728\u9700\u6c42\u5de5\u7a0b\u9636\u6bb5\u7cfb\u7edf\u6027\u8003\u8651\u73af\u5883\u3001\u793e\u4f1a\u3001\u6280\u672f\u548c\u7ecf\u6d4e\u56e0\u7d20\u3002\u7136\u800c\uff0c\u5df2\u6709\u5173\u4e8e\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u9700\u6c42\u7684\u7814\u7a76\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u3001\u5168\u9762\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u9650\u5236\u4e86\u5de5\u7a0b\u5b9e\u8df5\u4e0e\u7814\u7a76\u7684\u63a8\u8fdb\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\uff08Systematic Literature Review, SLR\uff09\u63d0\u53d6\u4e0e\u7ec4\u7ec7\u73b0\u6709\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff0c\u5efa\u7acb\u7edf\u4e00\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u5176\u6307\u6807\u4e0e\u5ea6\u91cf\u65b9\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u8986\u76d6\u73af\u5883\u3001\u6280\u672f\u3001\u793e\u4f1a\u548c\u7ecf\u6d4e\u56db\u5927\u7ef4\u5ea6\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff08SRs\uff09\u7efc\u5408\u5206\u7c7b\uff0c\u5e76\u660e\u786e\u4e86\u5404\u7c7b\u522b\u5b9a\u4e49\u3001\u76f8\u5173\u6307\u6807\u548c\u5ea6\u91cf\u3002\u6b64\u5916\uff0c\u7ed8\u5236\u4e86\u7c7b\u522b\u95f4\u53ca\u4e0d\u540c\u7ef4\u5ea6\u95f4\u7684\u6b63\u8d1f\u5f71\u54cd\u77e9\u9635\uff0c\u63ed\u793a\u5176\u534f\u540c\u4e0e\u51b2\u7a81\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u53ef\u6301\u7eed\u6027\u9700\u6c42\u5206\u7c7b\u4f53\u7cfb\u7684\u7f3a\u53e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u8005\u548c\u7814\u7a76\u8005\u66f4\u6709\u6548\u5730\u5236\u5b9a\u3001\u7ba1\u7406\u5e76\u6743\u8861\u53ef\u6301\u7eed\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u591a\u7ef4\u9700\u6c42\u548c\u51b2\u7a81\u3002"}}
{"id": "2510.08616", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08616", "abs": "https://arxiv.org/abs/2510.08616", "authors": ["Juan Miguel Navarro Carranza"], "title": "LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests", "comment": "NeurIPS 2025 Workshop. Evaluating the Evolving LLM Lifecycle:\n  Benchmarks, Emergent Abilities, and Scaling. Selected for contributed talk", "summary": "Benchmark scores for Large Language Models (LLMs) can be inflated by\nmemorization of test items or near duplicates. We present a simple, protocol\nthat probes generalization by re-evaluating models on paraphrased versions of\nbenchmark questions. Using Mistral-7B-Instruct and Qwen2.5-7B-Instruct, we\nmeasure the accuracy gap between original and paraphrased items on ARC-Easy and\nARC-Challenge. Our pipeline controls decoding, enforces multiple-choice output\nformat, and includes a robust paraphrase-cleaning step to preserve semantics.\nWe find that paraphrasing induces a non-trivial accuracy drop (original vs.\nparaphrased), consistent with prior concerns about contamination and brittle\nsurface-form shortcuts.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u9898\u8fdb\u884c\u8bed\u4e49\u590d\u8ff0\u518d\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u5927\u6a21\u578b\u51c6\u786e\u7387\u663e\u8457\u4e0b\u6ed1\uff0c\u8bf4\u660e\u539f\u59cb\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLM\u80fd\u529b\u53ef\u80fd\u9ad8\u4f30\uff0c\u771f\u5b9e\u6cdb\u5316\u80fd\u529b\u5c1a\u6709\u9650\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5f97\u5206\u53ef\u80fd\u88ab\u8bb0\u5fc6\u6d4b\u8bd5\u9898\u76ee\u6216\u5176\u8fd1\u4f3c\u91cd\u590d\u9879\u6240\u5938\u5927\u3002\u4e3a\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u89c4\u907f\u201c\u9898\u76ee\u8bb0\u5fc6\u201d\u73b0\u8c61\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u7b80\u5355\u534f\u8bae\uff0c\u901a\u8fc7\u5bf9\u6d4b\u8bd5\u57fa\u51c6\u9898\u76ee\u8fdb\u884c\u590d\u8ff0\uff08paraphrase\uff09\uff0c\u7528Mistral-7B-Instruct\u548cQwen2.5-7B-Instruct\u4e24\u4e2a\u6a21\u578b\u5728ARC-Easy\u548cARC-Challenge\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5bf9\u539f\u9898\u548c\u590d\u8ff0\u9898\u8fdb\u884c\u51c6\u786e\u7387\u6d4b\u91cf\u3002\u6d41\u7a0b\u5305\u62ec\u53d7\u63a7\u89e3\u7801\u3001\u5f3a\u5236\u9009\u62e9\u9898\u8f93\u51fa\u683c\u5f0f\u4ee5\u53ca\u7a33\u5065\u7684\u8bed\u4e49\u4fdd\u6301\u590d\u8ff0\u6e05\u6d17\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u590d\u8ff0\u9898\u76ee\u4e0a\u7684\u51c6\u786e\u7387\u76f8\u6bd4\u539f\u9898\u6709\u660e\u663e\u4e0b\u964d\uff0c\u8fd9\u5370\u8bc1\u4e86\u6b64\u524d\u5173\u4e8e\u6570\u636e\u6cc4\u6f0f\u548c\u6a21\u578b\u5bf9\u8868\u5c42\u7279\u5f81\u53d6\u5de7\u7684\u62c5\u5fe7\u3002", "conclusion": "\u4ec5\u4f9d\u9760\u539f\u59cb\u57fa\u51c6\u9898\u76ee\u8bc4\u4f30LLM\u80fd\u529b\u7684\u505a\u6cd5\u6613\u5bfc\u81f4\u8fc7\u9ad8\u4f30\u8ba1\uff0c\u800c\u901a\u8fc7\u9898\u76ee\u590d\u8ff0\u53ef\u66f4\u771f\u5b9e\u8003\u5bdf\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63ed\u793a\u5176\u6613\u53d7\u9898\u578b\u8868\u9762\u5f62\u5f0f\u5f71\u54cd\u7684\u95ee\u9898\u3002"}}
{"id": "2510.08996", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08996", "abs": "https://arxiv.org/abs/2510.08996", "authors": ["Spandan Garg", "Ben Steenhoek", "Yufan Huang"], "title": "Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation", "comment": null, "summary": "Current benchmarks for evaluating software engineering agents, such as\nSWE-Bench Verified, are predominantly derived from GitHub issues and fail to\naccurately reflect how developers interact with chat-based coding assistants in\nintegrated development environments (IDEs). We posit that this mismatch leads\nto a systematic overestimation of agent's capabilities in real-world scenarios,\nespecially bug fixing. We introduce a novel benchmarking framework that\ntransforms existing formal benchmarks into realistic user queries through\nsystematic analysis of developer interaction patterns with chat-based agents.\nOur methodology is flexible and can be easily extended to existing benchmarks.\nIn this paper, we apply our testing framework to SWE-Bench Verified, the\nTypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and\ntransform formal GitHub issue descriptions into realistic user-style queries\nbased on telemetry analysis of a popular chat-based agent interactions. Our\nfindings reveal that existing benchmarks significantly overestimate agent\ncapabilities for some models by >50% over baseline performance for public\nbenchmarks and ~10-16% for our internal benchmark. This work establishes a new\nparadigm for evaluating interactive chat-based software engineering agents\nthrough benchmark mutation techniques.", "AI": {"tldr": "\u4ee5GitHub issue\u4e3a\u57fa\u7840\u7684\u73b0\u6709\u667a\u80fd\u4f53\u8bc4\u6d4b\u65b9\u5f0f\u9ad8\u4f30\u4e86\u771f\u5b9e\u80fd\u529b\uff0c\u4f5c\u8005\u63d0\u51fa\u7528\u5f00\u53d1\u8005\u5b9e\u9645\u4ea4\u4e92\u4e60\u60ef\u8f6c\u5316\u57fa\u51c6\u7684\u65b0\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u73b0\u6709\u8bc4\u6d4b\u9ad8\u4f30\u4e86\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u5e76\u4e3a\u540e\u7eed\u66f4\u771f\u5b9e\u7684\u8bc4\u6d4b\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u8bc4\u6d4b\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u4e8eGitHub issue\uff0c\u4f46\u672a\u80fd\u771f\u5b9e\u53cd\u6620\u5f00\u53d1\u8005\u5728IDE\u4e2d\u4e0e\u804a\u5929\u5f0f\u7f16\u7a0b\u52a9\u624b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5bfc\u81f4\u4e86\u5bf9\u667a\u80fd\u4f53\u5b9e\u9645\u80fd\u529b\u7684\u9ad8\u4f30\uff0c\u5c24\u5176\u662f\u5728\u4fee\u590dbug\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5c06\u73b0\u6709\u6b63\u5f0f\u57fa\u51c6\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5f00\u53d1\u8005\u4e0e\u804a\u5929\u5f0f\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u8f6c\u5316\u4e3a\u66f4\u771f\u5b9e\u7684\u7528\u6237\u67e5\u8be2\u3002\u8be5\u65b9\u6cd5\u7075\u6d3b\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u57fa\u51c6\u4e0a\u3002\u5b9e\u9a8c\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8eSWE-Bench Verified\u3001Multi-SWE-Bench\uff08TypeScript\u5b50\u96c6\uff09\u53ca\u5185\u90e8\u57fa\u51c6\uff08SWE-Bench C#\uff09\uff0c\u901a\u8fc7\u9065\u6d4b\u5206\u6790\u628a\u6b63\u5f0fGitHub issue\u63cf\u8ff0\u8f6c\u5316\u4e3a\u7528\u6237\u98ce\u683c\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u57fa\u51c6\u5728\u67d0\u4e9b\u6a21\u578b\u4e0a\u7684\u80fd\u529b\u8bc4\u4f30\u6bd4\u771f\u5b9e\u60c5\u51b5\u9ad8\u51fa\u4e8650%\u4ee5\u4e0a\uff08\u516c\u5171\u57fa\u51c6\uff09\uff0c\u800c\u5728\u5185\u90e8\u57fa\u51c6\u4e0a\u9ad8\u4f30\u4e5f\u670910-16%\u3002", "conclusion": "\u6587\u7ae0\u786e\u7acb\u4e86\u4e00\u79cd\u901a\u8fc7\u57fa\u51c6\u53d8\u5f02\u6280\u672f\uff0c\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u804a\u5929\u578b\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.08620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08620", "abs": "https://arxiv.org/abs/2510.08620", "authors": ["Attapol T. Rutherford", "Jullajak Karnjanaekarin", "Narongkorn Panitsrisit", "Pontakorn Trakuekul", "Sumana Sumanakul", "Natchanon Pollertlam"], "title": "JAI-1: A Thai-Centric Large Language Model", "comment": null, "summary": "This technical report introduces JAI-1, a Thai-centric language model with\n75B parameters. Recent Thai models have primarily relied on existing\nopen-source models, applying additional training without structural\nmodifications to specialize in Thai. However, this approach risks eroding\npre-existing knowledge in the model's parameter space during the injection of\nThai-specific information, as optimized parameters for general tasks may\nconflict with new linguistic requirements. In contrast, JAI-1 adopts an\nupscaling strategy: starting from a smaller, high-performing English\nopen-source LLM, we expanded its parameter space and utilized the newly\nallocated capacity to systematically integrate Thai-language knowledge. This\nmethodology not only preserves the original model's general intelligence but\nalso establishes a unique architecture distinct from other open-source models,\nenabling scalable future enhancements. During pre-training, JAI-1 was exposed\nto 1.5T tokens, including over 300B Thai language tokens. This was followed by\npost-training stages -- supervised fine-tuning and alignment tuning -- using\nmore than 600K instruction-based examples. The final model demonstrated\nsuperior performance compared to Typhoon2-70B on Thai-centric benchmarks\n(IFEval-TH, MT-Bench-TH, and JAI-Hall-Bench), validating the efficacy of its\nupscaling and knowledge-integration framework.", "AI": {"tldr": "JAI-1\u662f\u4e00\u6b3e\u53c2\u6570\u89c4\u6a2175B\u7684\u6cf0\u8bed\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u5148\u6269\u5bb9\u518d\u7cfb\u7edf\u6ce8\u5165\u6cf0\u8bed\u77e5\u8bc6\u7684\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cf0\u8bed\u5904\u7406\u80fd\u529b\u5e76\u4f18\u4e8e\u4e3b\u6d41\u6a21\u578b\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u5fae\u8c03\u5e26\u6765\u7684\u77e5\u8bc6\u9057\u5931\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6cf0\u8bed\u5927\u6a21\u578b\u4e00\u822c\u662f\u5728\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u57fa\u7840\u4e0a\u505a\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u76f4\u63a5\u6ce8\u5165\u6cf0\u8bed\u77e5\u8bc6\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u539f\u6709\u77e5\u8bc6\u6d41\u5931\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u77e5\u8bc6\u6ce8\u5165\u7684\u51b2\u7a81\uff0c\u4fdd\u6301\u901a\u7528\u80fd\u529b\u5e76\u63d0\u5347\u6cf0\u8bed\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5148\u57fa\u4e8e\u8f83\u5c0f\u4f46\u6027\u80fd\u4f18\u5f02\u7684\u82f1\u6587\u5f00\u6e90\u6a21\u578b\uff0c\u5c06\u5176\u53c2\u6570\u7a7a\u95f4\u6269\u5c55\u523075B\uff0c\u518d\u5728\u65b0\u6269\u5c55\u7684\u53c2\u6570\u5bb9\u91cf\u4e2d\u7cfb\u7edf\u6ce8\u5165\u6cf0\u8bed\u77e5\u8bc6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6cf0\u8bed\u53ca\u82f1\u6587\u9884\u8bad\u7ec3\u548c\u5206\u9636\u6bb5\u7684\u6307\u4ee4\u5fae\u8c03\u5b9e\u73b0\u3002", "result": "JAI-1\u8fdb\u884c\u4e861.5\u4e07\u4ebftoken\u7684\u9884\u8bad\u7ec3\uff08\u5176\u4e2d\u5305\u542b\u8d853000\u4ebf\u6cf0\u8bedtoken\uff09\uff0c\u4e4b\u540e\u53c8\u901a\u8fc760\u4e07\u6761\u4ee5\u4e0a\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u8bad\u7ec3\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5176\u5728\u591a\u4e2a\u6cf0\u8bed\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u5747\u4f18\u4e8e\u4e3b\u6d41\u7684Typhoon2-70B\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u7684\u201c\u6269\u5bb9\u6ce8\u5165\u201d\u7b56\u7565\u65e2\u4fdd\u7559\u4e86\u539f\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u4e5f\u6709\u6548\u96c6\u6210\u4e86\u6cf0\u8bed\u77e5\u8bc6\uff0c\u662f\u6cf0\u8bed\u5927\u6a21\u578b\u65b9\u5411\u7684\u91cd\u8981\u521b\u65b0\u3002"}}
{"id": "2510.09045", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09045", "abs": "https://arxiv.org/abs/2510.09045", "authors": ["Manojit Chakraborty", "Madhusudan Ghosh", "Rishabh Gupta"], "title": "Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements", "comment": null, "summary": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6807\u8bc6\u7b26\u66ff\u6362\u7684\u96f6\u6837\u672c\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86token\u6d88\u8017\u5e76\u63d0\u5347\u5927\u6a21\u578b\u957f\u4ee3\u7801\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6e90\u4ee3\u7801\u65f6\uff0c\u5e38\u51fa\u73b0\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u5bfc\u81f4\u7ffb\u8bd1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u65b9\u6cd5\u63d0\u5347\u957f\u4ee3\u7801\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5728\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u7528\u901a\u7528\u5360\u4f4d\u7b26\u66ff\u6362\u7528\u6237\u7ed9\u5b9a\u7684\u957f\u6807\u8bc6\u7b26\uff0c\u4ece\u800c\u51cf\u5c11\u5927\u6a21\u578b\u5904\u7406\u65f6\u7684token\u6570\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u51cf\u5c11token\u6570\u91cf\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4ee3\u7801\u7684\u8bed\u6cd5\u548c\u5c42\u6b21\u7ed3\u6784\uff0c\u63d0\u5347\u4e86\u957f\u4ee3\u7801\u7684\u7ffb\u8bd1\u6548\u7387\u4e0e\u6027\u4ef7\u6bd4\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u5728\u4ee3\u7801\u7ffb\u8bd1\u524d\u66ff\u6362\u957f\u6807\u8bc6\u7b26\u4e3a\u5360\u4f4d\u7b26\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u4ee3\u7801\u957f\u5ea6\uff0c\u4f7f\u5f97\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u51c6\u786e\u5730\u7ffb\u8bd1\u957f\u4ee3\u7801\uff0c\u5e76\u4fdd\u6301\u8bed\u6cd5\u548c\u7ed3\u6784\u6b63\u786e\u3002"}}
{"id": "2510.08621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08621", "abs": "https://arxiv.org/abs/2510.08621", "authors": ["Wen-Yu Chang", "Tzu-Hung Huang", "Chih-Ho Chen", "Yun-Nung Chen"], "title": "From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents", "comment": null, "summary": "Amid the rapid rise of agentic dialogue models, realistic user-simulator\nstudies are essential for tuning effective conversation strategies. This work\ninvestigates a sales-oriented agent that adapts its dialogue based on user\nprofiles spanning age, gender, and occupation. While age and gender influence\noverall performance, occupation produces the most pronounced differences in\nconversational intent. Leveraging this insight, we introduce a lightweight,\noccupation-conditioned strategy that guides the agent to prioritize intents\naligned with user preferences, resulting in shorter and more successful\ndialogues. Our findings highlight the importance of rich simulator profiles and\ndemonstrate how simple persona-informed strategies can enhance the\neffectiveness of sales-oriented dialogue systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u804c\u4e1a\u4fe1\u606f\u5b9a\u5236\u9500\u552e\u5bf9\u8bdd\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u4e30\u5bcc\u7528\u6237\u6a21\u62df\u8d44\u6599\u63d0\u5347\u4ee3\u7406\u7cfb\u7edf\u6548\u7387\uff0c\u7b80\u4fbf\u7684\u4eba\u683c\u5316\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5b9e\u9645\u8868\u73b0\u3002", "motivation": "\u5728\u667a\u80fd\u5bf9\u8bdd\u6a21\u578b\u5feb\u901f\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\uff0c\u5f00\u53d1\u80fd\u66f4\u6709\u6548\u8fdb\u884c\u9500\u552e\u5bf9\u8bdd\u7684\u4ee3\u7406\u65f6\uff0c\u9700\u8981\u4f9d\u8d56\u771f\u5b9e\u7684\u7528\u6237\u6a21\u62df\u7814\u7a76\uff0c\u4ee5\u4f18\u5316\u5bf9\u8bdd\u7b56\u7565\u3002", "method": "\u7814\u7a76\u8ba9\u9500\u552e\u4ee3\u7406\u6839\u636e\u7528\u6237\u7684\u5e74\u9f84\u3001\u6027\u522b\u548c\u804c\u4e1a\u4e09\u7c7b\u7279\u5f81\u8c03\u6574\u5bf9\u8bdd\u7b56\u7565\uff0c\u91cd\u70b9\u5206\u6790\u804c\u4e1a\u5bf9\u610f\u56fe\u8868\u8fbe\u7684\u5f71\u54cd\uff0c\u636e\u6b64\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u804c\u4e1a\u7684\u8f7b\u91cf\u7ea7\u5bf9\u8bdd\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u5e74\u9f84\u548c\u6027\u522b\u5f71\u54cd\u6574\u4f53\u8868\u73b0\uff0c\u800c\u804c\u4e1a\u5bf9\u5bf9\u8bdd\u610f\u56fe\u5dee\u5f02\u5f71\u54cd\u6700\u5927\u3002\u57fa\u4e8e\u804c\u4e1a\u5b9a\u5236\u7b56\u7565\u540e\uff0c\u4ee3\u7406\u80fd\u66f4\u5feb\u3001\u66f4\u6709\u6548\u5730\u8fbe\u6210\u5bf9\u8bdd\u76ee\u6807\u3002", "conclusion": "\u4e30\u5bcc\u7684\u7528\u6237\u6a21\u62df\u89d2\u8272\u6781\u5176\u91cd\u8981\uff0c\u4e14\u7b80\u5355\u7684\u4e2a\u6027\u5316\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u9500\u552e\u578b\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6548\u679c\u3002"}}
{"id": "2510.09058", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09058", "abs": "https://arxiv.org/abs/2510.09058", "authors": ["Italo Santos", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding", "comment": null, "summary": "Large Language Models have quickly become a central component of modern\nsoftware development workflows, and software practitioners are increasingly\nintegrating LLMs into various stages of the software development lifecycle.\nDespite the growing presence of LLMs, there is still a limited understanding of\nhow these tools are actually used in practice and how professionals perceive\ntheir benefits and limitations. This paper presents preliminary findings from a\nglobal survey of 131 software practitioners. Our results reveal how LLMs are\nutilized for various coding-specific tasks. Software professionals report\nbenefits such as increased productivity, reduced cognitive load, and faster\nlearning, but also raise concerns about LLMs' inaccurate outputs, limited\ncontext awareness, and associated ethical risks. Most developers treat LLMs as\nassistive tools rather than standalone solutions, reflecting a cautious yet\npractical approach to their integration. Our findings provide an early,\npractitioner-focused perspective on LLM adoption, highlighting key\nconsiderations for future research and responsible use in software engineering.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\uff0c\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u4ece\u4e1a\u8005\u5728\u5b9e\u9645\u5de5\u4f5c\u4e2d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f7f\u7528\u73b0\u72b6\u3001\u4f18\u52bf\u4e0e\u98ce\u9669\u8ba4\u77e5\u3002\u591a\u6570\u4eba\u5c06\u5176\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u65e2\u80af\u5b9a\u4e86\u751f\u4ea7\u529b\u63d0\u5347\uff0c\u4e5f\u5173\u6ce8\u4e0d\u51c6\u786e\u6027\u548c\u4f26\u7406\u95ee\u9898\uff0c\u4e3a\u540e\u7eedLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8d1f\u8d23\u4efb\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u53c2\u8003\u548c\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u867d\u7136LLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5bf9\u4e8e\u5176\u5b9e\u9645\u4f7f\u7528\u65b9\u5f0f\u3001\u4e13\u4e1a\u4eba\u58eb\u5bf9\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u8ba4\u77e5\u4ecd\u7f3a\u4e4f\u6df1\u5165\u4e86\u89e3\u3002", "method": "\u901a\u8fc7\u5168\u7403\u8303\u56f4\u5185\u9488\u5bf9131\u4f4d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u7684\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c6\u548c\u5206\u6790\u4ed6\u4eec\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u4f7f\u7528LLMs\u7684\u7ecf\u9a8c\u548c\u770b\u6cd5\u3002", "result": "LLMs\u88ab\u7528\u4e8e\u591a\u79cd\u7f16\u7801\u76f8\u5173\u4efb\u52a1\uff0c\u5e26\u6765\u63d0\u5347\u751f\u4ea7\u529b\u3001\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u548c\u52a0\u901f\u5b66\u4e60\u7b49\u76ca\u5904\u3002\u4f46\u540c\u65f6\u5b58\u5728\u5bf9\u4e8e\u8f93\u51fa\u51c6\u786e\u6027\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u53ca\u4f26\u7406\u98ce\u9669\u7b49\u65b9\u9762\u7684\u62c5\u5fe7\u3002", "conclusion": "\u5927\u591a\u6570\u5f00\u53d1\u4eba\u5458\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c6\u4e3a\u8f85\u52a9\u5de5\u5177\u800c\u975e\u72ec\u7acb\u89e3\u51b3\u65b9\u6848\uff0c\u8868\u73b0\u51fa\u5bf9\u5176\u96c6\u6210\u7684\u8c28\u614e\u4e14\u52a1\u5b9e\u6001\u5ea6\u3002"}}
{"id": "2510.08622", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08622", "abs": "https://arxiv.org/abs/2510.08622", "authors": ["Francesco Dente", "Fabiano Dalpiaz", "Paolo Papotti"], "title": "Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories", "comment": "8 pages", "summary": "Large language models (LLMs) can be employed for automating the generation of\nsoftware requirements from natural language inputs such as the transcripts of\nelicitation interviews. However, evaluating whether those derived requirements\nfaithfully reflect the stakeholders' needs remains a largely manual task. We\nintroduce Text2Stories, a task and metrics for text-to-story alignment that\nallow quantifying the extent to which requirements (in the form of user\nstories) match the actual needs expressed by the elicitation session\nparticipants. Given an interview transcript and a set of user stories, our\nmetric quantifies (i) correctness: the proportion of stories supported by the\ntranscript, and (ii) completeness: the proportion of transcript supported by at\nleast one story. We segment the transcript into text chunks and instantiate the\nalignment as a matching problem between chunks and stories. Experiments over\nfour datasets show that an LLM-based matcher achieves 0.86 macro-F1 on held-out\nannotations, while embedding models alone remain behind but enable effective\nblocking. Finally, we show how our metrics enable the comparison across sets of\nstories (e.g., human vs. generated), positioning Text2Stories as a scalable,\nsource-faithful complement to existing user-story quality criteria.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faText2Stories\u4efb\u52a1\u548c\u5ea6\u91cf\uff0c\u7ed3\u5408LLM\u81ea\u52a8\u8bc4\u4f30\u7531\u8bbf\u8c08\u751f\u6210\u7684\u8f6f\u4ef6\u7528\u6237\u6545\u4e8b\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u4f18\u5f02\uff0c\u53ef\u4f5c\u4e3a\u8d34\u5408\u539f\u59cb\u9700\u6c42\u7684\u81ea\u52a8\u5316\u8865\u5145\u8bc4\u5224\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u4ece\u8bbf\u8c08\u8bb0\u5f55\u7b49\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u4e2d\u751f\u6210\u8f6f\u4ef6\u9700\u6c42\uff0c\u4f46\u5982\u4f55\u81ea\u52a8\u8bc4\u4f30\u8fd9\u4e9b\u9700\u6c42\u662f\u5426\u771f\u5b9e\u53cd\u6620\u4e86\u5229\u76ca\u76f8\u5173\u65b9\u9700\u6c42\uff0c\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u9879\u65b0\u4efb\u52a1\u548c\u5ea6\u91cfText2Stories\uff0c\u7528\u4e8e\u8bc4\u4f30\u7531LLM\u751f\u6210\u7684\u7528\u6237\u6545\u4e8b\u4e0e\u539f\u59cb\u8bbf\u8c08\u5185\u5bb9\u7684\u4e00\u81f4\u6027\u3002\u5c06\u8bbf\u8c08\u8f6c\u5f55\u6587\u672c\u5206\u5272\u4e3a\u5757\uff0c\u5c06\u5176\u4e0e\u7528\u6237\u6545\u4e8b\u8fdb\u884c\u5339\u914d\uff0c\u901a\u8fc7\u8861\u91cf\u6b63\u786e\u6027\uff08\u6545\u4e8b\u88ab\u8bbf\u8c08\u652f\u6301\u7684\u6bd4\u4f8b\uff09\u548c\u5b8c\u6574\u6027\uff08\u8bbf\u8c08\u88ab\u6545\u4e8b\u8986\u76d6\u7684\u6bd4\u4f8b\uff09\uff0c\u5b9e\u73b0\u91cf\u5316\u5206\u6790\u3002\u5b9e\u9a8c\u6bd4\u8f83\u4e86LLM\u4e0e\u5d4c\u5165\u5f0f\u6a21\u578b\u7684\u5339\u914d\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u793a\uff0c\u57fa\u4e8eLLM\u7684\u5339\u914d\u65b9\u6cd5\u5728\u72ec\u7acb\u6807\u6ce8\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u4e860.86\u7684macro-F1\u5206\u6570\uff1b\u4ec5\u7528\u5d4c\u5165\u6a21\u578b\u6548\u679c\u7565\u900a\uff0c\u4f46\u53ef\u7528\u4e8e\u9ad8\u6548\u7b5b\u67e5\u3002\u63d0\u51fa\u7684\u5ea6\u91cf\u53ef\u5bf9\u6bd4\u4e0d\u540c\u6765\u6e90\u7684\u7528\u6237\u6545\u4e8b\uff08\u5982\u4eba\u5de5\u4e0e\u751f\u6210\uff09\uff0c\u4e3a\u73b0\u6709\u8d28\u91cf\u5224\u636e\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u8d34\u8fd1\u539f\u59cb\u9700\u6c42\u7684\u65b0\u8865\u5145\u3002", "conclusion": "Text2Stories\u4efb\u52a1\u548c\u5ea6\u91cf\u4f53\u7cfb\u4e3a\u8f6f\u4ef6\u9700\u6c42\u81ea\u52a8\u5316\u5206\u6790\u5e26\u6765\u53ef\u6269\u5c55\u7684\u3001\u57fa\u4e8e\u539f\u59cb\u8bbf\u8c08\u6587\u672c\u7684\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u624b\u6bb5\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u7528\u6237\u6545\u4e8b\u4e0e\u5b9e\u9645\u9700\u6c42\u7684\u4e00\u81f4\u6027\u5224\u5b9a\u6548\u7387\u3002"}}
{"id": "2510.08623", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08623", "abs": "https://arxiv.org/abs/2510.08623", "authors": ["Anubhav Shrimal", "Aryan Jain", "Soumyajit Chowdhury", "Promod Yenigalla"], "title": "PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction", "comment": "EMNLP 2025 Industry Track", "summary": "Structured information extraction from unstructured text is critical for\nemerging Software 3.0 systems where LLM agents autonomously interact with APIs\nand tools. Recent approaches apply large language models directly to extraction\ntasks using existing JSON schemas, often with constraint decoding or\nreinforcement learning approaches to ensure syntactic validity, but treat JSON\nschemas as static contracts designed for human developers, leading to\nsuboptimal extraction performance, frequent hallucinations, and unreliable\nagent behavior when schemas contain ambiguous or incomplete specifications. We\nrecognize that JSON schemas themselves are a form of natural language\nunderstanding contract that encodes rules, relationships, and expectations\nabout data structure contracts that LLMs should be able to both interpret and\nsystematically improve. Consequently, we develop PARSE (Parameter Automated\nRefinement and Schema Extraction), a novel system with two synergistic\ncomponents: ARCHITECT, which autonomously optimizes JSON schemas for LLM\nconsumption while maintaining backward compatibility through RELAY (an\nintegrated code generation system), and SCOPE, which implements\nreflection-based extraction with combined static and LLM-based guardrails. We\nevaluate PARSE qualitatively and quantitatively on three datasets including\nSchema-Guided Dialogue (SGD), Structured Web Data Extraction (SWDE), and\ninternal retail conversation data, and find that it achieves up to 64.7%\nimprovement in extraction accuracy on SWDE with combined framework improvements\nreaching 10% across models, while reducing extraction errors by 92% within the\nfirst retry and and maintaining practical latency.", "AI": {"tldr": "\u63d0\u51faPARSE\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u4f18\u5316JSON schema\u548c\u6539\u8fdb\u62bd\u53d6\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5927\u5e45\u63d0\u5347LLM\u7ed3\u6784\u5316\u4fe1\u606f\u62bd\u53d6\u7684\u51c6\u786e\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u884c\u7ed3\u6784\u5316\u4fe1\u606f\u62bd\u53d6\u65f6\uff0c\u901a\u5e38\u76f4\u63a5\u5c06JSON schema\u4f5c\u4e3a\u9759\u6001\u5408\u7ea6\u5904\u7406\uff0c\u5bfc\u81f4\u62bd\u53d6\u6027\u80fd\u4e0d\u4f73\u3001\u5e7b\u89c9\u9891\u53d1\uff0c\u4ee5\u53ca\u5728schema\u542b\u6709\u6b67\u4e49\u6216\u4e0d\u5b8c\u6574\u8bf4\u660e\u65f6\u884c\u4e3a\u4e0d\u53ef\u9760\u3002\u4f5c\u8005\u8ba4\u4e3aJSON schema\u672c\u8eab\u662f\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u5408\u7ea6\uff0cLLM\u5e94\u80fd\u591f\u89e3\u91ca\u5e76\u52a0\u4ee5\u7cfb\u7edf\u6539\u8fdb\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86PARSE\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1aARCHITECT\u81ea\u52a8\u4f18\u5316JSON schema\u4ee5\u9002\u5408LLM\u5904\u7406\uff0c\u5e76\u901a\u8fc7RELAY\u4fdd\u6301\u5411\u540e\u517c\u5bb9\u6027\uff1bSCOPE\u5229\u7528\u53cd\u601d\u578b\u62bd\u53d6\u673a\u5236\u7ed3\u5408\u9759\u6001\u89c4\u5219\u548cLLM\u80fd\u529b\u8bbe\u7f6e\u591a\u91cd\u9632\u62a4\u3002", "result": "\u5728Schema-Guided Dialogue\u3001Structured Web Data Extraction\u548c\u5185\u90e8\u7535\u5546\u5bf9\u8bdd\u6570\u636e\u4e09\u5957\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cSWDE\u6570\u636e\u96c6\u4fe1\u606f\u62bd\u53d6\u51c6\u786e\u7387\u63d0\u5347\u6700\u591a\u8fbe64.7%\uff1b\u6574\u4f53\u6846\u67b6\u5728\u5404\u6a21\u578b\u5e73\u5747\u8868\u73b0\u63d0\u534710%\uff1b\u9996\u6b21\u91cd\u8bd5\u65f6\u62bd\u53d6\u9519\u8bef\u7387\u51cf\u5c1192%\uff0c\u4e14\u7ef4\u6301\u5b9e\u7528\u7684\u5ef6\u8fdf\u6c34\u5e73\u3002", "conclusion": "\u6539\u8fdb\u548c\u52a8\u6001\u4f18\u5316JSON schema\uff0c\u4ee5\u53ca\u7ed3\u5408\u9759\u6001\u4e0eLLM\u9632\u62a4\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u62bd\u53d6\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.09108", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09108", "abs": "https://arxiv.org/abs/2510.09108", "authors": ["Lukas Krodinger", "Altin Hajdari", "Stephan Lukasczyk", "Gordon Fraser"], "title": "Constraint-Guided Unit Test Generation for Machine Learning Libraries", "comment": "Accepted for SSBSE 2025", "summary": "Machine learning (ML) libraries such as PyTorch and TensorFlow are essential\nfor a wide range of modern applications. Ensuring the correctness of ML\nlibraries through testing is crucial. However, ML APIs often impose strict\ninput constraints involving complex data structures such as tensors. Automated\ntest generation tools such as Pynguin are not aware of these constraints and\noften create non-compliant inputs. This leads to early test failures and\nlimited code coverage. Prior work has investigated extracting constraints from\nofficial API documentation. In this paper, we present PynguinML, an approach\nthat improves the Pynguin test generator to leverage these constraints to\ngenerate compliant inputs for ML APIs, enabling more thorough testing and\nhigher code coverage. Our evaluation is based on 165 modules from PyTorch and\nTensorFlow, comparing PynguinML against Pynguin. The results show that\nPynguinML significantly improves test effectiveness, achieving up to 63.9 %\nhigher code coverage.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u6d4b\u8bd5\u5de5\u5177\u5bf9ML\u5e93API\u8f93\u5165\u7ea6\u675f\u8ba4\u8bc6\u4e0d\u8db3\u5bfc\u81f4\u4ee3\u7801\u8986\u76d6\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faPynguinML\uff0c\u5c06API\u7ea6\u675f\u77e5\u8bc6\u878d\u5165\u6d4b\u8bd5\u751f\u6210\u8fc7\u7a0b\uff0c\u7ecf\u5b9e\u8bc1\u9a8c\u8bc1\u53ef\u5927\u5e45\u63d0\u5347\u8986\u76d6\u7387\u548c\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u6d4b\u8bd5\u5de5\u5177\u65e0\u6cd5\u8bc6\u522bML\u5e93API\u590d\u6742\u7684\u8f93\u5165\u7ed3\u6784\u548c\u7ea6\u675f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u6d4b\u8bd5\u6837\u4f8b\u7ecf\u5e38\u4e0d\u5408\u89c4\uff0c\u6d4b\u8bd5\u5931\u8d25\u7387\u9ad8\uff0c\u4ee3\u7801\u8986\u76d6\u7387\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u6d4b\u8bd5\u751f\u6210\u5de5\u5177Pynguin\uff0c\u4f7f\u5176\u80fd\u591f\u7406\u89e3\u5e76\u5229\u7528API\u6587\u6863\u4e2d\u63d0\u53d6\u7684\u8f93\u5165\u7ea6\u675f\uff0c\u751f\u6210\u7b26\u5408\u7406\u60f3\u8f93\u5165\u7684\u6d4b\u8bd5\u6837\u4f8b\u3002", "result": "\u5728PyTorch\u548cTensorFlow\u7684165\u4e2a\u6a21\u5757\u4e0a\u8bc4\u4f30\uff0cPynguinML\u6bd4\u539f\u59cbPynguin\u5728\u6d4b\u8bd5\u6709\u6548\u6027\u548c\u4ee3\u7801\u8986\u76d6\u7387\u4e0a\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "PynguinML\u80fd\u591f\u663e\u8457\u63d0\u5347\u57fa\u4e8eML\u5e93\u7684\u6d4b\u8bd5\u751f\u6210\u5de5\u5177\u7684\u6709\u6548\u6027\u548c\u4ee3\u7801\u8986\u76d6\u7387\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe63.9%\u3002"}}
{"id": "2510.08624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08624", "abs": "https://arxiv.org/abs/2510.08624", "authors": ["Nisar Ahmed", "Muhammad Imran Zaman", "Gulshan Saleem", "Ali Hassan"], "title": "Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B", "comment": null, "summary": "Benchmarks for large language models (LLMs) often rely on rubric-scented\nprompts that request visible reasoning and strict formatting, whereas real\ndeployments demand terse, contract-bound answers. We investigate whether such\n\"evaluation scent\" inflates measured performance without commensurate\ncapability gains. Using a single open-weights model (GPT-OSS-20B), we run six\npaired A/B scenarios that hold task content and decoding fixed while varying\nframing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High):\ndeterministic math, strict code-fix, citation generation, incentive flips\n(caution vs. competence), CoT visibility, and multilingual (Urdu) headers.\nDeterministic validators compute accuracy, answer-only compliance,\nhedging/refusals, chain-of-thought (CoT) length, and schema compliance, with\npre-registered deltas and composite indices. Across scenarios, evaluation\nframing reliably inflates CoT (hundreds to >1000 characters) and reduces\nanswer-only compliance, with limited or inconsistent accuracy gains. In\nstructured outputs, it improves wrappers (e.g., fenced blocks, enumerated\nlists) but not regex-validated substance. Incentive wording reweights error\ncomposition: praising caution modestly improves accuracy at high reasoning and\nreduces wrong-but-confident errors, whereas praising competence yields terser\nbut riskier outputs. Urdu rubric headers reproduce these signatures and can\ndecrease accuracy at higher reasoning depth, indicating multilingual parity\nrisks. We provide a reproducible A/B framework (prompt banks, validators,\nper-run scores, scripts; versioned DOI) and practical guidance: neutral\nphrasing or dual-framing checks, contract-aware grading, style-delta reporting,\nconfidence governance, and multilingual dashboards to ensure that benchmark\ngains reflect deployable capability.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5e26\u6709\u201c\u8bc4\u6d4b\u5473\u9053\u201d\u7684\u63d0\u793a\u4f1a\u865a\u589e\u5927\u6a21\u578b\u7684\u63a8\u7406\u8868\u73b0\uff08\u5982\u63a8\u7406\u957f\u5ea6\uff09\uff0c\u4f46\u672a\u5fc5\u63d0\u5347\u771f\u5b9e\u51c6\u786e\u7387\u548c\u53ef\u5e94\u7528\u80fd\u529b\uff0c\u4e14\u53ef\u80fd\u635f\u5bb3\u7b54\u6848\u7b80\u6d01\u548c\u591a\u8bed\u79cd\u4e00\u81f4\u6027\u3002\u5efa\u8bae\u6539\u7528\u66f4\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u7684\u8bdd\u672f\u4e0e\u8bc4\u5206\u65b9\u5f0f\uff0c\u786e\u4fdd\u6a21\u578b\u8bc4\u6d4b\u53cd\u6620\u5b9e\u9645\u90e8\u7f72\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u591a\u4f9d\u8d56\u4e8e\u5e26\u6709\u8bc4\u5206\u63d0\u793a\u7684prompts\uff0c\u5f3a\u8c03\u663e\u6027\u63a8\u7406\u4e0e\u4e25\u683c\u683c\u5f0f\uff0c\u4f46\u771f\u5b9e\u5e94\u7528\u573a\u666f\u8981\u6c42\u7b80\u6d01\u4e14\u5951\u7ea6\u5316\u7684\u7b54\u6848\u3002\u4f5c\u8005\u5173\u6ce8\u8bc4\u6d4b\u73af\u5883\u662f\u5426\u865a\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u800c\u672a\u5e26\u6765\u5b9e\u9645\u80fd\u529b\u63d0\u5347\u3002", "method": "\u91c7\u7528\u516c\u5f00\u6743\u91cd\u6a21\u578bGPT-OSS-20B\u8bbe\u8ba1\u516d\u7ec4A/B\u5b9e\u9a8c\uff0c\u4efb\u52a1\u5185\u5bb9\u548c\u89e3\u7801\u8fc7\u7a0b\u56fa\u5b9a\uff0c\u4ec5\u6539\u53d8\u63d0\u793a\u8bdd\u672f\uff08\u8bc4\u6d4b\u5bfc\u5411vs\u771f\u5b9e\u573a\u666f\uff09\u53ca\u63a8\u7406\u6df1\u5ea6\uff08\u4e2d/\u9ad8\uff09\uff0c\u6db5\u76d6\u6570\u5b66\u3001\u4ee3\u7801\u4fee\u6b63\u3001\u5f15\u7528\u751f\u6210\u3001\u6fc0\u52b1\u8bdd\u672f\u3001\u663e\u6027\u63a8\u7406\u5c55\u793a\u3001\u591a\u8bed\u79cd\uff08\u4e4c\u5c14\u90fd\u8bed\uff09\u7b49\u3002\u7528\u81ea\u52a8\u68c0\u9a8c\u5de5\u5177\u8bc4\u4f30\u51c6\u786e\u7387\u3001\u4ec5\u7b54\u5408\u89c4\u3001\u62d2\u7b54\u3001\u63a8\u7406\u957f\u5ea6\u53ca\u7ed3\u6784\u5408\u89c4\uff0c\u5e76\u8bbe\u7f6e\u9884\u6ce8\u518c\u7684\u5dee\u503c\u4e0e\u7efc\u5408\u6307\u6807\u3002", "result": "\u8bc4\u6d4b\u5bfc\u5411\u63d0\u793a\u7a33\u5b9a\u63d0\u5347\u63a8\u7406\u957f\u5ea6\uff08\u6570\u767e\u5230\u5343\u5b57\uff09\uff0c\u4f46\u964d\u4f4e\u7eaf\u7b54\u5408\u89c4\u6027\uff0c\u51c6\u786e\u7387\u63d0\u5347\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\u3002\u7ed3\u6784\u5316\u8f93\u51fa\u65f6\uff0c\u63d0\u5347\u8f93\u51fa\u5c01\u88c5\uff08\u5982\u4ee3\u7801\u5757\u3001\u5217\u8868\uff09\u4f46\u4e0d\u6539\u5584\u5185\u5bb9\u51c6\u786e\u6027\u3002\u6fc0\u52b1\u8bdd\u672f\u6539\u53d8\u9519\u8bef\u7c7b\u578b\uff0c\u8868\u626c\u8c28\u614e\u63d0\u5347\u9ad8\u63a8\u7406\u4e0b\u51c6\u786e\u7387\uff0c\u51cf\u5c11\u81ea\u4fe1\u9519\u8bef\uff1b\u8868\u626c\u80fd\u529b\u5219\u8f93\u51fa\u66f4\u7b80\u4f46\u98ce\u9669\u589e\u52a0\u3002\u591a\u8bed\u79cd\u4e0b\uff0c\u7c7b\u4f3c\u63d0\u793a\u5f71\u54cd\u5728\u4e4c\u5c14\u90fd\u8bed\u91cd\u73b0\uff0c\u4e14\u9ad8\u63a8\u7406\u4e0b\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u8868\u73b0\u51fa\u591a\u8bed\u79cd\u4e00\u81f4\u6027\u98ce\u9669\u3002", "conclusion": "\u8bc4\u6d4b\u73af\u5883\u8bbe\u5b9a\u4f1a\u865a\u9ad8\u6a21\u578b\u8868\u73b0\uff0c\u672a\u5fc5\u53cd\u6620\u771f\u5b9e\u53ef\u90e8\u7f72\u80fd\u529b\uff0c\u5c24\u5176\u5728\u591a\u8bed\u79cd\u573a\u666f\u6709\u98ce\u9669\u3002\u4f5c\u8005\u63d0\u4f9b\u53ef\u590d\u73b0\u7684A/B\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e76\u5efa\u8bae\u91c7\u7528\u4e2d\u6027\u8bdd\u672f\u6216\u53cc\u91cd\u8bc4\u4ef7\u3001\u5951\u7ea6\u5316\u8bc4\u5206\u3001\u98ce\u683c\u5dee\u5f02\u62a5\u544a\u3001\u4fe1\u5fc3\u6cbb\u7406\u548c\u591a\u8bed\u79cd\u4eea\u8868\u76d8\uff0c\u4ee5\u8ba9\u8bc4\u6d4b\u7ed3\u679c\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.09134", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.09134", "abs": "https://arxiv.org/abs/2510.09134", "authors": ["Amal Elgammal", "Bernd J. Kr\u00e4mer", "Michael P. Papazoglou", "Mira Raheem"], "title": "A Semantic Framework for Patient Digital Twins in Chronic Care", "comment": "This manuscript is currently under review at Software and Systems\n  Modeling (SoSyM)", "summary": "Personalized chronic care requires the integration of multimodal health data\nto enable precise, adaptive, and preventive decision-making. Yet most current\ndigital twin (DT) applications remain organ-specific or tied to isolated data\ntypes, lacking a unified and privacy-preserving foundation. This paper\nintroduces the Patient Medical Digital Twin (PMDT), an ontology-driven in\nsilico patient framework that integrates physiological, psychosocial,\nbehavioral, and genomic information into a coherent, extensible model.\nImplemented in OWL 2.0, the PMDT ensures semantic interoperability, supports\nautomated reasoning, and enables reuse across diverse clinical contexts. Its\nontology is structured around modular Blueprints (patient, disease and\ndiagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse\nevents), formalized through dedicated conceptual views. These were iteratively\nrefined and validated through expert workshops, questionnaires, and a pilot\nstudy in the EU H2020 QUALITOP project with real-world immunotherapy patients.\nEvaluation confirmed ontology coverage, reasoning correctness, usability, and\nGDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous\ndata, operationalize competency questions, and support descriptive, predictive,\nand prescriptive analytics in a federated, privacy-preserving manner. By\nbridging gaps in data fragmentation and semantic standardization, the PMDT\nprovides a validated foundation for next-generation digital health ecosystems,\ntransforming chronic care toward proactive, continuously optimized, and\nequitable management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u8bba\u7684\u60a3\u8005\u6570\u5b57\u5b6a\u751f\uff08PMDT\uff09\u6a21\u578b\uff0c\u80fd\u591f\u6574\u5408\u591a\u6e90\u5065\u5eb7\u6570\u636e\uff0c\u652f\u6301\u667a\u80fd\u51b3\u7b56\u4e0e\u5206\u6790\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u63a8\u52a8\u6570\u5b57\u533b\u7597\u751f\u6001\u53d1\u5c55\u3002", "motivation": "\u76ee\u524d\u6162\u6027\u75c5\u7684\u4e2a\u4f53\u5316\u62a4\u7406\u9700\u8981\u6574\u5408\u591a\u6a21\u6001\u5065\u5eb7\u6570\u636e\uff0c\u5b9e\u73b0\u7cbe\u51c6\u3001\u9002\u5e94\u6027\u548c\u9884\u9632\u6027\u51b3\u7b56\u3002\u4f46\u73b0\u6709\u7684\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u5668\u5b98\u6216\u5b64\u7acb\u6570\u636e\u7c7b\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u57fa\u7840\u3002", "method": "\u63d0\u51faPatient Medical Digital Twin\uff08PMDT\uff09\uff0c\u57fa\u4e8e\u672c\u4f53\u8bba\u7684\u865a\u62df\u60a3\u8005\u6846\u67b6\uff0c\u7528OWL 2.0\u5b9e\u73b0\uff0c\u6db5\u76d6\u751f\u7406\u3001\u5fc3\u7406\u793e\u4f1a\u3001\u884c\u4e3a\u548c\u57fa\u56e0\u7ec4\u4fe1\u606f\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u84dd\u56fe\u7ed3\u6784\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u7814\u8ba8\u3001\u95ee\u5377\u548c\u771f\u5b9e\u4e16\u754c\u8bd5\u70b9\u7814\u7a76\uff08QUALITOP\u9879\u76ee\uff09\u53cd\u590d\u4f18\u5316\u548c\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u8bc1\u5b9ePMDT\u5728\u672c\u4f53\u8986\u76d6\u3001\u63a8\u7406\u6b63\u786e\u6027\u3001\u53ef\u7528\u6027\u548cGDPR\u5408\u89c4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u80fd\u591f\u7edf\u4e00\u5f02\u6784\u6570\u636e\uff0c\u652f\u6301\u591a\u79cd\u5206\u6790\u6a21\u5f0f\uff0c\u5e76\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u60c5\u51b5\u4e0b\u64cd\u4f5c\u3002", "conclusion": "PMDT\u4e3a\u7edf\u4e00\u548c\u6807\u51c6\u5316\u6162\u6027\u75c5\u533b\u7597\u6570\u636e\u3001\u652f\u6301\u591a\u79cd\u533b\u5b66\u5206\u6790\u548c\u8054\u5408\u9690\u79c1\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u6570\u5b57\u533b\u7597\u751f\u6001\u7cfb\u7edf\u548c\u6162\u6027\u75c5\u7684\u4e3b\u52a8\u3001\u6301\u7eed\u4f18\u5316\u7ba1\u7406\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.08626", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08626", "abs": "https://arxiv.org/abs/2510.08626", "authors": ["Prosenjit Biswas", "Pervez Shaik", "Abhinav Thorat", "Ravi Kolla", "Niranjan Pedanekar"], "title": "From What to Why: Thought-Space Recommendation with Small Language Models", "comment": "15 pages, 3 figures", "summary": "Large Language Models (LLMs) have advanced recommendation capabilities\nthrough enhanced reasoning, but pose significant challenges for real-world\ndeployment due to high inference costs. Conversely, while Small Language Models\n(SLMs) offer an efficient alternative, their reasoning capabilities for\nrecommendation remain underexplored. Existing systems often use natural\nlanguage rationales merely as unsupervised descriptive text, failing to harness\ntheir full potential as learning signals. In this work our main idea is to\ncreate a common understanding of user and items across multiple domains called\nThought Space with SLMs instead of using LLMs' distilled knowledge. To that end\nwe propose PULSE (Preference Understanding by Latent Semantic Embeddings), a\nframework that treats SLM-generated rationales as director learning signals,\nsupervising them with interaction histories to jointly model user actions\n(what) and their semantic drivers (why). Existing methods consider only\ninteractions such as sequences and embeddings, whereas PULSE treats rationales\nas first-class signals, this novel design yields embeddings that are more\nrobust and generalizable. Extensive experiments demonstrate that PULSE\noutperforms leading ID, Collaborative Filtering (CF), and LLM-based sequential\nrecommendation models across multiple benchmark datasets. Furthermore, PULSE\nexhibits superior transferability in cross-domain recommendation and\ndemonstrates strong performance on downstream tasks such as reasoning-oriented\nquestion answering. Our code is available\n\\href{https://anonymous.4open.science/r/Thinking_PULSE-0FC5/README.md}{here}.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8bf4\u660e\u4f5c\u4e3a\u76f4\u63a5\u5b66\u4e60\u4fe1\u53f7\uff0c\u6784\u5efa\u8de8\u9886\u57df\u63a8\u8350\u7684\u901a\u7528\u8868\u5f81\u7a7a\u95f4\u3002\u65b0\u65b9\u6cd5PULSE\u5728\u63a8\u8350\u3001\u8fc1\u79fb\u548c\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u7a81\u51fa\uff0c\u517c\u5177\u9ad8\u6548\u4e0e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63a8\u7406\u63a8\u8350\u80fd\u529b\u5f3a\uff0c\u4f46\u63a8\u7406\u6210\u672c\u9ad8\u4e0d\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff1b\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u4f46\u63a8\u7406\u80fd\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u8bf4\u660e\u5f53\u4f5c\u6709\u6548\u5b66\u4e60\u4fe1\u53f7\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u5229\u7528SLM\u6784\u5efa\u8de8\u9886\u57df\u901a\u7528\u7684\u7528\u6237\u3001\u7269\u54c1\u7406\u89e3\u7a7a\u95f4\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86PULSE\u6846\u67b6\uff0c\u5c06SLM\u751f\u6210\u7684rationales\uff08\u63a8\u7406\u8bf4\u660e\uff09\u4f5c\u4e3a\u76f4\u63a5\u5b66\u4e60\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u8fdb\u884c\u76d1\u7763\uff0c\u8054\u5408\u5efa\u6a21\u7528\u6237\u884c\u4e3a\u548c\u5176\u8bed\u4e49\u9a71\u52a8\u529b\u3002\u6b64\u5916\uff0cPULSE\u5c06rationales\u89c6\u4e3a\u6838\u5fc3\u4fe1\u53f7\uff0c\u63d0\u5347\u4e86\u5d4c\u5165\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "PULSE\u663e\u8457\u8d85\u8d8a\u5f53\u524d\u4e3b\u6d41\u7b97\u6cd5\uff0c\u5728\u63a8\u8350\u3001\u8de8\u9886\u57df\u8fc1\u79fb\u548c\u63a8\u7406\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002\u5d4c\u5165\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "PULSE\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684ID\u3001\u534f\u540c\u8fc7\u6ee4\u548c\u57fa\u4e8eLLM\u7684\u63a8\u8350\u6a21\u578b\uff0c\u5728\u8de8\u9886\u57df\u63a8\u8350\u548c\u9762\u5411\u63a8\u7406\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2510.09308", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09308", "abs": "https://arxiv.org/abs/2510.09308", "authors": ["Mira Raheem", "Amal Elgammal", "Michael Papazoglou", "Bernd Kr\u00e4mer", "Neamat El-Tazi"], "title": "A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms", "comment": "Disclaimer: This manuscript is currently under review at * MDPI\n  Informatics*", "summary": "Artificial intelligence (AI) has the potential to transform healthcare by\nsupporting more accurate diagnoses and personalized treatments. However, its\nadoption in practice remains constrained by fragmented data sources, strict\nprivacy rules, and the technical complexity of building reliable clinical\nsystems. To address these challenges, we introduce a model driven engineering\n(MDE) framework designed specifically for healthcare AI. The framework relies\non formal metamodels, domain-specific languages (DSLs), and automated\ntransformations to move from high level specifications to running software. At\nits core is the Medical Interoperability Language (MILA), a graphical DSL that\nenables clinicians and data scientists to define queries and machine learning\npipelines using shared ontologies. When combined with a federated learning\narchitecture, MILA allows institutions to collaborate without exchanging raw\npatient data, ensuring semantic consistency across sites while preserving\nprivacy. We evaluate this approach in a multi center cancer immunotherapy\nstudy. The generated pipelines delivered strong predictive performance, with\nsupport vector machines achieving up to 98.5 percent and 98.3 percent accuracy\nin key tasks, while substantially reducing manual coding effort. These findings\nsuggest that MDE principles metamodeling, semantic integration, and automated\ncode generation can provide a practical path toward interoperable,\nreproducible, and trustworthy digital health platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u7597AI\u7684\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u5f62\u5316\u9886\u57df\u4e13\u7528\u8bed\u8a00\uff08MILA\uff09\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u5b9e\u73b0\u8de8\u673a\u6784\u9ad8\u6548\u534f\u4f5c\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u5728\u764c\u75c7\u514d\u75ab\u6cbb\u7597\u5e94\u7528\u4e2d\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\u53ca\u964d\u4f4e\u5f00\u53d1\u590d\u6742\u5ea6\uff0c\u4e3a\u53ef\u9760\u7684\u533b\u7597\u6570\u5b57\u5e73\u53f0\u5efa\u8bbe\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1AI\u6709\u52a9\u4e8e\u533b\u7597\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u6570\u636e\u788e\u7247\u5316\u3001\u9690\u79c1\u6cd5\u89c4\u4ee5\u53ca\u4e34\u5e8a\u7cfb\u7edf\u6280\u672f\u58c1\u5792\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u65b9\u6cd5\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u4fc3\u8fdbAI\u5728\u533b\u7597\u9886\u57df\u7684\u91c7\u7528\u3002", "method": "\u63d0\u51fa\u5e76\u5e94\u7528\u4e86\u57fa\u4e8e\u5f62\u5f0f\u5143\u6a21\u578b\u3001\u9886\u57df\u4e13\u7528\u8bed\u8a00\uff08DSL\uff09\u53ca\u81ea\u52a8\u4ee3\u7801\u8f6c\u6362\u7684MDE\u6846\u67b6\uff0c\u6838\u5fc3\u4e3a\u652f\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u56fe\u5f62\u5316DSL\u2014\u2014MILA\uff1b\u7ed3\u5408\u4e86\u8054\u90a6\u5b66\u4e60\u67b6\u6784\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u8bc4\u4f30\u4e8e\u591a\u4e2d\u5fc3\u764c\u75c7\u514d\u75ab\u6cbb\u7597\u7814\u7a76\u3002", "result": "MILA\u4e0e\u8054\u90a6\u5b66\u4e60\u67b6\u6784\u7ed3\u5408\u540e\uff0c\u5b9e\u73b0\u4e86\u4e0d\u4ea4\u6362\u539f\u59cb\u6570\u636e\u7684\u591a\u673a\u6784\u534f\u4f5c\u5e76\u4fdd\u8bc1\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u764c\u75c7\u514d\u75ab\u6cbb\u7597\u7814\u7a76\u4e2d\u7531\u81ea\u52a8\u751f\u6210\u7684\u9884\u6d4b\u7ba1\u9053\u53d6\u5f97\u4e86\u6700\u9ad898.5%\u548c98.3%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u7f16\u7801\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\u7ed3\u5408\u9886\u57df\u4e13\u7528\u8bed\u8a00\u548c\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u80fd\u6709\u6548\u63d0\u5347\u533b\u7597AI\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u53ef\u590d\u73b0\u6027\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u6570\u5b57\u5065\u5eb7\u5e73\u53f0\u63d0\u4f9b\u5b9e\u9645\u8def\u5f84\u3002"}}
{"id": "2510.08630", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08630", "abs": "https://arxiv.org/abs/2510.08630", "authors": ["Jingbiao Mei", "Mingsheng Sun", "Jinghong Chen", "Pengda Qin", "Yuhong Li", "Da Chen", "Bill Byrne"], "title": "ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection", "comment": "Preprint", "summary": "Hateful memes have emerged as a particularly challenging form of online\nabuse, motivating the development of automated detection systems. Most prior\napproaches rely on direct detection, producing only binary predictions. Such\nmodels fail to provide the context and explanations that real-world moderation\nrequires. Recent Explain-then-Detect approaches, using Chain-of-Thought\nprompting or LMM agents, perform worse than simple SFT baselines, and even\nadvanced post-training methods such as GRPO fail to close the gap. Our analysis\nidentifies two key issues of such systems: important policy-relevant cues such\nas targets and attack types are not hypothesized by the model as a likely\nexplanation; and the binary reward signal is insufficient to guide reasoning.\nTo address these challenges, we propose ExPO-HM (Explain-then-Detect Policy\nOptimization for Hateful Memes), inspired by the training and evaluation\nprocess of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum\nlearning, and Conditional Decision Entropy (CDE) as both metric and reward for\nreasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves\nstate-of-the-art performance on binary detection, fine-grained classification,\nand reasoning quality, with up to 15\\% and 17\\% F1 improvement over the GRPO\nand DPO baselines, respectively. By moving hateful meme detection from simple\nbinary alarms to explanation-driven detection, ExPO-HM provides accurate,\ninterpretable, and actionable moderation support.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6a21\u4eff\u4eba\u5de5\u5ba1\u6838\u6d41\u7a0b\u63d0\u51faExPO-HM\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4ec7\u6068\u6897\u56fe\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u52a9\u529b\u5e73\u53f0\u7cbe\u7ec6\u3001\u9ad8\u6548\u5730\u5ba1\u6838\u590d\u6742\u5185\u5bb9\u3002", "motivation": "\u7f51\u7edc\u4e0a\u7684\u4ec7\u6068\u6897\u56fe\u662f\u4e00\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u5185\u5bb9\u6ee5\u7528\u5f62\u5f0f\uff0c\u76ee\u524d\u81ea\u52a8\u68c0\u6d4b\u7cfb\u7edf\u5927\u591a\u53ea\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff0c\u7f3a\u4e4f\u751f\u6210\u5bf9\u4e0a\u4e0b\u6587\u548c\u89e3\u91ca\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5185\u5bb9\u5ba1\u6838\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faExPO-HM\uff08Explain-then-Detect Policy Optimization for Hateful Memes\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408SFT\u9884\u8bad\u7ec3\u3001GRPO\u52a0\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u6761\u4ef6\u51b3\u7b56\u71b5\uff08CDE\uff09\u4f5c\u4e3a\u8861\u91cf\u548c\u5956\u52b1\u63a8\u7406\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u6a21\u4eff\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u4ec7\u6068\u6897\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cExPO-HM\u5728\u4e8c\u5143\u68c0\u6d4b\u3001\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u73b0\u6709\u6700\u4f18\u8868\u73b0\u3002\u4e0eGRPO\u548cDPO\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cF1\u503c\u5206\u522b\u63d0\u5347\u4e86\u6700\u9ad815%\u548c17%\u3002", "conclusion": "ExPO-HM\u65b9\u6cd5\u80fd\u591f\u5c06\u4ec7\u6068\u6897\u56fe\u68c0\u6d4b\u4ece\u7b80\u5355\u7684\u4e8c\u5143\u8b66\u62a5\u8f6c\u53d8\u4e3a\u4ee5\u89e3\u91ca\u4e3a\u9a71\u52a8\u7684\u68c0\u6d4b\uff0c\u5b9e\u73b0\u66f4\u52a0\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u64cd\u4f5c\u7684\u5185\u5bb9\u5ba1\u6838\u652f\u6301\u3002"}}
{"id": "2510.09400", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09400", "abs": "https://arxiv.org/abs/2510.09400", "authors": ["He Jiang", "Yufu Wang", "Hao Lin", "Peiyu Zou", "Zhide Zhou", "Ang Jia", "Xiaochen Li", "Zhilei Ren"], "title": "TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance in automated\nsource-to-target code translation through pretraining on extensive code\ncorpora. However, mainstream LLM-based code translation methods suffer from two\ncritical limitations. First, they are highly sensitive to language-specific\nfeatures, which often introduce source-language syntax or lexicon into the\noutput, leading to syntactic confusion. Second, they lack fine-grained semantic\nalignment due to an over-reliance on function-level parallel datasets,\nresulting in semantic misalignment between the translated code and the original\nsource. To overcome these limitations, we propose TIT, a Tree-structured\nInstruction Tuning paradigm for LLM-based code translation. Specifically, TIT\nconsists of three modules. First, to mitigate syntactic confusion, the\nsyntactic information representation module integrates language-agnostic\nsyntactic features via structured parsing. Then, to generate high-quality\nfine-grained parallel data, the fine-grained parallel dataset augmentation\nmodule aligns nodes with code segments through statement-level segmentation and\ncontrastive matching. Finally, we leverage the dual-stage tree instruction\ntuning module to alleviate the contextual processing burden on the LLM caused\nby the introduction of syntactic information. The first stage employs\nsyntax-aware fine-tuning to enable the LLM to autonomously comprehend\nstructured syntactic information, while the second stage utilizes code\ngeneration fine-tuning to guide the model in generating accurate target code\nbased on function-level syntactic dependencies. The experimental results\ndemonstrate that the proposed method significantly outperforms existing\napproaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in\ncode translation while markedly reducing syntactic confusion.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faTIT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u6cd5\u4fe1\u606f\u3001\u7ec6\u7c92\u5ea6\u6570\u636e\u589e\u5f3a\u548c\u53cc\u9636\u6bb5\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u7ffb\u8bd1\u6548\u679c\uff0c\u5e76\u51cf\u5c11\u8bed\u6cd5\u6df7\u6dc6\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1. \u5bf9\u8bed\u8a00\u7279\u6027\u8fc7\u4e8e\u654f\u611f\uff0c\u5bfc\u81f4\u6e90\u8bed\u8a00\u7684\u8bed\u6cd5\u6216\u8bcd\u6c47\u6df7\u5165\u76ee\u6807\u4ee3\u7801\uff0c\u5f15\u53d1\u8bed\u6cd5\u6df7\u6dc6\uff1b2. \u7531\u4e8e\u8fc7\u5ea6\u4f9d\u8d56\u51fd\u6570\u7ea7\u5e73\u884c\u6570\u636e\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u4ee3\u7801\u4e0e\u539f\u59cb\u6e90\u7801\u4e4b\u95f4\u8bed\u4e49\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIT\uff08Tree-structured Instruction Tuning\uff09\u7684\u6811\u7ed3\u6784\u6307\u4ee4\u5fae\u8c03\u8303\u5f0f\uff0c\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1a\u2460\u8bed\u6cd5\u4fe1\u606f\u8868\u793a\u6a21\u5757\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u6790\u6574\u5408\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u6cd5\u7279\u5f81\uff0c\u51cf\u5c11\u8bed\u6cd5\u6df7\u6dc6\uff1b\u2461\u7ec6\u7c92\u5ea6\u5e73\u884c\u6570\u636e\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u8bed\u53e5\u7ea7\u5206\u5272\u4e0e\u5bf9\u6bd4\u5339\u914d\uff0c\u5c06\u8282\u70b9\u4e0e\u4ee3\u7801\u7247\u6bb5\u5bf9\u9f50\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u5e73\u884c\u8bed\u6599\uff1b\u2462\u53cc\u9636\u6bb5\u6811\u6307\u4ee4\u5fae\u8c03\u6a21\u5757\uff0c\u9996\u9636\u6bb5\u5229\u7528\u8bed\u6cd5\u611f\u77e5\u5fae\u8c03\u8ba9LLM\u7406\u89e3\u7ed3\u6784\u5316\u8bed\u6cd5\u4fe1\u606f\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u5fae\u8c03\uff0c\u6307\u5bfc\u6a21\u578b\u751f\u6210\u51c6\u786e\u7684\u76ee\u6807\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTIT\u65b9\u6cd5\u5728\u591a\u4e2aLLM\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u7ffb\u8bd1\u6210\u529f\u7387\u63d0\u53471.22\u500d-1.75\u500d\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u8bed\u6cd5\u6df7\u6dc6\u3002", "conclusion": "TIT\u901a\u8fc7\u5f15\u5165\u6811\u7ed3\u6784\u7684\u8bed\u6cd5\u4fe1\u606f\u8868\u793a\u548c\u53cc\u9636\u6bb5\u5fae\u8c03\uff0c\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u7ffb\u8bd1\u8d28\u91cf\u548c\u8bed\u6cd5\u51c6\u786e\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e3b\u6d41\u65b9\u6cd5\u5b58\u5728\u7684\u8bed\u6cd5\u6df7\u6dc6\u548c\u8bed\u4e49\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2510.08632", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08632", "abs": "https://arxiv.org/abs/2510.08632", "authors": ["Cai Zhou", "Chenyu Wang", "Dinghuai Zhang", "Shangyuan Tong", "Yifei Wang", "Stephen Bates", "Tommi Jaakkola"], "title": "Next Semantic Scale Prediction via Hierarchical Diffusion Language Models", "comment": "Accepted to NeurIPS 2025", "summary": "In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a\nnovel family of discrete diffusion models for language modeling. HDLM builds on\na hierarchical vocabulary where low-level tokens with detailed semantics are\nsurjectively mapped to high-level tokens with coarse-grained meanings. In the\nforward process, each token is independently perturbed to its higher-level\nancestor with more abstract semantics according to the scheduler, while in the\nreverse process the model progressively predicts the next, more detailed\nsemantics. Taken together, HDLM provides a general time-varying next semantic\nscale prediction process for language modeling. We derive closed-form\nexpressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM\ncan be implemented in a flexible manner while including the existing MDLM as a\nspecial case. We also propose practical training techniques based on the\ninsights. Extensive text generation experiments validate the effectiveness of\nHDLM, which demonstrates consistently lower validation and generative\nperplexity than baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5206\u5c42\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08HDLM\uff09\uff0c\u901a\u8fc7\u5c42\u7ea7\u8bed\u4e49\u6620\u5c04\u548c\u6269\u6563\u6280\u672f\u63d0\u5347\u6587\u672c\u751f\u6210\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u56f0\u60d1\u5ea6\u66f4\u4f4e\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8bed\u4e49\u5c42\u7ea7\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u5c06\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u7c97\u7c92\u5ea6\u8bed\u4e49\u7ed3\u5408\u8d77\u6765\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08HDLM\uff09\uff0c\u5229\u7528\u5206\u5c42\u8bcd\u6c47\u8868\uff0c\u5c06\u8be6\u7ec6\u8bed\u4e49\u7684\u4f4e\u5c42\u7ea7token\u4e0e\u7c97\u7c92\u5ea6\u9ad8\u5c42\u7ea7token\u6620\u5c04\u3002\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5c06token\u63d0\u5347\u5230\u66f4\u62bd\u8c61\u8bed\u4e49\u5c42\u7ea7\uff0c\u518d\u901a\u8fc7\u9006\u8fc7\u7a0b\u9884\u6d4b\u66f4\u8be6\u7ec6\u8bed\u4e49\u3002\u5e76\u63a8\u5bfc\u4e86\u6269\u6563ELBO\u7684\u5c01\u95ed\u5f62\u5f0f\u8868\u8fbe\u65b9\u5f0f\uff0c\u63d0\u51fa\u4e86\u53ef\u7075\u6d3b\u5b9e\u73b0\u7684\u8bad\u7ec3\u6280\u5de7\u3002", "result": "HDLM\u5728\u591a\u9879\u6587\u672c\u751f\u6210\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u9a8c\u8bc1\u548c\u751f\u6210\u56f0\u60d1\u5ea6\u4e0a\u5747\u8868\u73b0\u51fa\u66f4\u4f4e\u503c\uff0c\u6548\u679c\u663e\u8457\u3002", "conclusion": "HDLM\u6269\u5c55\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u8bed\u8a00\u5efa\u6a21\u4e0a\u7684\u80fd\u529b\uff0c\u5c06\u5206\u5c42\u8bed\u4e49\u6709\u6548\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u5efa\u6a21\u7684\u8868\u73b0\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.08647", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08647", "abs": "https://arxiv.org/abs/2510.08647", "authors": ["Chengzhengxu Li", "Xiaoming Liu", "Zhaohan Zhang", "Shaochu Zhang", "Shengchao Liu", "Guoxin Ma", "Yu Lan", "Chao Shen"], "title": "Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression", "comment": "ACL2026 Under Review", "summary": "Recent developments have enabled advanced reasoning in Large Language Models\n(LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high\ncomputational costs and significant latency losses owing to the autoregressive\nnature of generative LLMs. CoT compression aims to improve efficiency in the\nreasoning process by reducing output length. Previous works trade reasoning\nefficiency by either laborious discrete prompt designing or the construction of\nexternal compressed CoT datasets that sacrifice key reasoning details. In this\nwork, we propose Upfront CoT (UCoT): an efficient reasoning framework with\nupfront thought embedding to automate CoT compression. UCoT is a cooperative\nworkflow involving a small model (compressor) and a large model (executor). The\nfirst stage of UCoT trains compressor to generate upfront thought embeddings\nrich in reasoning information for the executor, avoiding the drawbacks of\nmanually designed prompts. The second stage optimizes executor to utilize\nupfront thought embeddings to derive the correct answer with short reasoning,\nusing a reward mechanism. Extensive experiments show that UCoT maintains the\npowerful reasoning ability of executor while significantly reducing the length\nof CoT. It is worth mentioning that when applying UCoT to the\nQwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by\n50\\%, while the performance is 3.08\\% higher than that of the state-of-the-art\n(SOTA) method. The code and dataset are in supplementary material.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684UCoT\u65b9\u6cd5\u80fd\u9ad8\u6548\u4e14\u81ea\u52a8\u5730\u538b\u7f29\u601d\u7ef4\u94fe\u6761\uff0c\u663e\u8457\u51cf\u5c11\u7b97\u529b\u548c\u5ef6\u8fdf\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u63a8\u7406\u51c6\u786e\u7387\u3002", "motivation": "\u957f\u94fe\u5f0f\u601d\u7ef4\u80fd\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5e26\u6765\u4e86\u9ad8\u8ba1\u7b97\u5f00\u9500\u4e0e\u5ef6\u8fdf\u3002\u4ee5\u5f80CoT\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u8bcd\u6216\u5916\u90e8\u6570\u636e\u96c6\uff0c\u5bb9\u6613\u727a\u7272\u5173\u952e\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u4e14\u6709\u6548\u7684CoT\u538b\u7f29\u65b0\u65b9\u6cd5\u3002", "method": "UCoT\u91c7\u7528\u4e86\u5c0f\u6a21\u578b\uff08\u538b\u7f29\u5668\uff09\u4e0e\u5927\u6a21\u578b\uff08\u6267\u884c\u5668\uff09\u7684\u534f\u4f5c\uff0c\u5176\u4e2d\u538b\u7f29\u5668\u751f\u6210\u5bcc\u542b\u63a8\u7406\u4fe1\u606f\u7684\u601d\u7ef4\u5d4c\u5165\uff0c\u6267\u884c\u5668\u4f7f\u7528\u8fd9\u4e00\u5d4c\u5165\uff0c\u901a\u8fc7\u5956\u52b1\u673a\u5236\u4f18\u5316\uff0c\u5728\u77ed\u63a8\u7406\u8fc7\u7a0b\u4e0b\u83b7\u5f97\u6b63\u786e\u7b54\u6848\u3002", "result": "UCoT\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u663e\u8457\u538b\u7f29\u4e86\u63a8\u7406\u94fe\u957f\u5ea6\u3002\u5e94\u7528\u4e8eQwen2.5-7B-Instruct\uff0c\u5728GSM8K\u6570\u636e\u96c6\u4e2dtoken\u4f7f\u7528\u51cf\u5c1150%\uff0c\u6027\u80fd\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u53473.08%\u3002", "conclusion": "UCoT\u80fd\u591f\u5728\u5927\u5e45\u538b\u7f29CoT\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.08649", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08649", "abs": "https://arxiv.org/abs/2510.08649", "authors": ["Gustave Cortal", "Alain Finkel"], "title": "Formalizing Style in Personal Narratives", "comment": null, "summary": "Personal narratives are stories authors construct to make meaning of their\nexperiences. Style, the distinctive way authors use language to express\nthemselves, is fundamental to how these narratives convey subjective\nexperiences. Yet there is a lack of a formal framework for systematically\nanalyzing these stylistic choices. We present a novel approach that formalizes\nstyle in personal narratives as patterns in the linguistic choices authors make\nwhen communicating subjective experiences. Our framework integrates three\ndomains: functional linguistics establishes language as a system of meaningful\nchoices, computer science provides methods for automatically extracting and\nanalyzing sequential patterns, and these patterns are linked to psychological\nobservations. Using language models, we automatically extract linguistic\nfeatures such as processes, participants, and circumstances. We apply our\nframework to hundreds of dream narratives, including a case study on a war\nveteran with post-traumatic stress disorder. Analysis of his narratives\nuncovers distinctive patterns, particularly how verbal processes dominate over\nmental ones, illustrating the relationship between linguistic choices and\npsychological states.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5206\u6790\u4e2a\u4eba\u53d9\u4e8b\u8bed\u8a00\u98ce\u683c\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5fc3\u7406\u5b66\u7ed3\u5408\uff0c\u6210\u529f\u7528\u5927\u91cf\u68a6\u5883\u53d9\u8ff0\u548cPTSD\u8001\u5175\u6848\u4f8b\u5c55\u793a\u4e86\u8bed\u8a00\u9009\u62e9\u4e0e\u5fc3\u7406\u72b6\u6001\u7684\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1\u4e2a\u4eba\u53d9\u4e8b\u4e2d\u7684\u8bed\u8a00\u98ce\u683c\u5bf9\u8868\u8fbe\u4e3b\u89c2\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u98ce\u683c\u9009\u62e9\u7684\u6b63\u5f0f\u6846\u67b6\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7cfb\u7edf\u5206\u6790\u4e2a\u4eba\u53d9\u4e8b\u98ce\u683c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u529f\u80fd\u8bed\u8a00\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u5fc3\u7406\u5b66\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u81ea\u52a8\u62bd\u53d6\u53d9\u4e8b\u6587\u672c\u4e2d\u7684\u8bed\u8a00\u7279\u5f81\uff08\u5982\u8fc7\u7a0b\u3001\u53c2\u4e0e\u8005\u3001\u73af\u5883\uff09\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u987a\u5e8f\u6a21\u5f0f\u8fdb\u884c\u5206\u6790\uff0c\u5173\u8054\u5230\u5fc3\u7406\u5b66\u89c2\u5bdf\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u5206\u6790\u4e86\u6570\u767e\u4e2a\u68a6\u5883\u53d9\u8ff0\uff0c\u5305\u62ec\u4e00\u4e2a\u521b\u4f24\u540e\u5e94\u6fc0\u969c\u788d\uff08PTSD\uff09\u8001\u5175\u7684\u6848\u4f8b\u7814\u7a76\u3002\u8be5\u8001\u5175\u7684\u53d9\u4e8b\u8868\u73b0\u51fa\u9c9c\u660e\u7684\u8bed\u8a00\u6a21\u5f0f\uff0c\u5c24\u5176\u662f\u201c\u8a00\u8bed\u8fc7\u7a0b\u201d\u8fdc\u591a\u4e8e\u201c\u5fc3\u7406\u8fc7\u7a0b\u201d\uff0c\u4ece\u800c\u63ed\u793a\u51fa\u8bed\u8a00\u9009\u62e9\u4e0e\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u6587\u4e2d\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5206\u6790\u4e2a\u4eba\u53d9\u4e8b\u4e2d\u7684\u8bed\u8a00\u98ce\u683c\u9009\u62e9\uff0c\u5e76\u63ed\u793a\u53d9\u8ff0\u8bed\u8a00\u4f7f\u7528\u4e0e\u5fc3\u7406\u72b6\u6001\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u4e3a\u7406\u89e3\u4e3b\u89c2\u4f53\u9a8c\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.08663", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.08663", "abs": "https://arxiv.org/abs/2510.08663", "authors": ["Joe Watson", "Ivan O'Conner", "Chia-Wen Chen", "Luning Sun", "Fang Luo", "David Stillwell"], "title": "A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data", "comment": null, "summary": "Psychological assessments typically rely on structured rating scales, which\ncannot incorporate the rich nuance of a respondent's natural language. This\nstudy leverages recent LLM advances to harness qualitative data within a novel\nconceptual framework, combining LLM-scored text and traditional rating-scale\nitems to create an augmented test. We demonstrate this approach using\ndepression as a case study, developing and assessing the framework on a\nreal-world sample of upper secondary students (n=693) and corresponding\nsynthetic dataset (n=3,000). On held-out test sets, augmented tests achieved\nstatistically significant improvements in measurement precision and accuracy.\nThe information gain from the LLM items was equivalent to adding between 6.3\n(real data) and 16.0 (synthetic data) items to the original 19-item test. Our\napproach marks a conceptual shift in automated scoring that bypasses its\ntypical bottlenecks: instead of relying on pre-labelled data or complex\nexpert-created rubrics, we empirically select the most informative LLM scoring\ninstructions based on calculations of item information. This framework provides\na scalable approach for leveraging the growing stream of transcribed text to\nenhance traditional psychometric measures, and we discuss its potential utility\nin clinical health and beyond.", "AI": {"tldr": "\u672c\u7814\u7a76\u521b\u65b0\u6027\u5730\u7ed3\u5408LLM\u81ea\u52a8\u6253\u5206\u7684\u81ea\u7531\u6587\u672c\u4e0e\u4f20\u7edf\u91cf\u8868\uff0c\u5f00\u53d1\u4e86\u589e\u5f3a\u578b\u5fc3\u7406\u6d4b\u9a8c\u3002\u6709\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u6d4b\u91cf\u7cbe\u5ea6\u4e0e\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u65b9\u6cd5\u53ef\u5927\u89c4\u6a21\u62d3\u5c55\uff0c\u6709\u671b\u7528\u4e8e\u4e34\u5e8a\u5065\u5eb7\u7b49\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7406\u6d4b\u8bc4\u4e3b\u8981\u4f9d\u8d56\u7ed3\u6784\u5316\u7684\u91cf\u8868\u8bc4\u5206\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u53d7\u8bbf\u8005\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\u3002\u968f\u7740\u5927\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408LLM\u6253\u5206\u7684\u6587\u672c\u548c\u4f20\u7edf\u91cf\u8868\u9898\u9879\uff0c\u63d0\u5347\u6d4b\u8bc4\u7684\u7cbe\u5ea6\u548c\u4fe1\u606f\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06LLM\u5bf9\u53d7\u8bbf\u8005\u81ea\u7531\u6587\u672c\u7684\u81ea\u52a8\u6253\u5206\u7ed3\u679c\u4e0e\u91cf\u8868\u5206\u9879\u7ed3\u5408\uff0c\u6784\u6210\u589e\u5f3a\u578b\u6d4b\u9a8c\u3002\u4ee5\u6291\u90c1\u91cf\u8868\u4e3a\u6848\u4f8b\uff0c\u5728693\u540d\u4e2d\u5b66\u751f\u771f\u5b9e\u6570\u636e\u548c3000\u4e2a\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u5f00\u53d1\u548c\u8bc4\u4f30\u3002\u901a\u8fc7\u8ba1\u7b97\u201c\u9898\u76ee\u4fe1\u606f\u91cf\u201d\uff0c\u5b9e\u8bc1\u7b5b\u9009\u51fa\u6700\u6709\u6548\u7684LLM\u6253\u5206\u6307\u4ee4\uff0c\u65e0\u9700\u9884\u6807\u6ce8\u6570\u636e\u6216\u4e13\u5bb6\u5236\u5b9a\u590d\u6742\u8bc4\u5206\u89c4\u5219\u3002", "result": "\u5728\u7559\u51fa\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u589e\u5f3a\u578b\u6d4b\u9a8c\u5728\u6d4b\u91cf\u7684\u7cbe\u786e\u6027\u548c\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002LLM\u751f\u6210\u7684\u4fe1\u606f\u589e\u76ca\u76f8\u5f53\u4e8e\u539f\u91cf\u8868\u589e\u52a06.3\uff08\u771f\u5b9e\u6570\u636e\uff09\u523016.0\uff08\u5408\u6210\u6570\u636e\uff09\u4e2a\u9898\u9879\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u6d4b\u9a8c\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5fc3\u7406\u6d4b\u8bc4\u9886\u57df\u5e26\u6765\u4e86\u81ea\u52a8\u5316\u8bc4\u5206\u7684\u6982\u5ff5\u8f6c\u53d8\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u548c\u4e13\u5bb6\u89c4\u5219\u7684\u74f6\u9888\u3002\u5b83\u80fd\u591f\u89c4\u6a21\u5316\u5229\u7528\u8f6c\u5f55\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u5fc3\u7406\u91cf\u8868\u7684\u6d4b\u91cf\u6548\u5ea6\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u4e34\u5e8a\u5065\u5eb7\u53ca\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2510.08666", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08666", "abs": "https://arxiv.org/abs/2510.08666", "authors": ["Yuxin Ma", "Lun Du", "Lanning Wei", "Kun Chen", "Qian Xu", "Kangyu Wang", "Guofeng Feng", "Guoshan Lu", "Lin Liu", "Xiaojing Qi", "Xinyuan Zhang", "Zhen Tao", "Haibo Feng", "Ziyun Jiang", "Ying Xu", "Zenan Huang", "Yihong Zhuang", "Haokai Xu", "Jiaqi Hu", "Zhenzhong Lan", "Junbo Zhao", "Jianguo Li", "Da Zheng"], "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models", "comment": null, "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components-model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager-and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared with AR models (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with latest vLLM inference engine, dInfer still deliverers\n$2$-$3\\times$ speedup. The implementation of dInfer is open-sourced at\nhttps://github.com/inclusionAI/dInfer.", "AI": {"tldr": "dInfer\u662f\u4e00\u6b3e\u9ad8\u6548\u3001\u6613\u6269\u5c55\u7684\u6269\u6563\u578b\u5927\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6570\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u578b\u5927\u6a21\u578b\uff08dLLMs\uff09\u9010\u6e10\u6d8c\u73b0\u5e76\u5177\u5907\u56fa\u6709\u5e76\u884c\u7279\u6027\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u6548\u6807\u51c6\u7684\u63a8\u7406\u6846\u67b6\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u75db\u70b9\u3002", "method": "\u63d0\u51fadInfer\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u6d41\u7a0b\u62c6\u89e3\u4e3a\u56db\u4e2a\u6a21\u5757\uff1a\u6a21\u578b\u3001\u6269\u6563\u8fed\u4ee3\u7ba1\u7406\u3001\u89e3\u7801\u7b56\u7565\u548cKV\u7f13\u5b58\u7ba1\u7406\uff0c\u5e76\u5728\u5404\u6a21\u5757\u878d\u5165\u65b0\u7b97\u6cd5\u53ca\u7cfb\u7edf\u7ea7\u4f18\u5316\u3002\u901a\u8fc7\u7b97\u6cd5\u4e0e\u7cfb\u7edf\u534f\u540c\u63d0\u5347\u6574\u4f53\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728LLaDA-MoE\u7b49\u6a21\u578b\u4e0a\uff0cdInfer\u5728batch size\u4e3a1\u65f6\uff0c\u80fd\u5728HumanEval\u6d4b\u8bd5\u4e0a\u8d85\u8fc71100 tokens/s\uff0c\u57288\u00d7H800 GPU\u4e0a\u5404\u7c7b\u57fa\u51c6\u5e73\u5747\u8d85\u8fc7800 tokens/s\u3002\u4e0eFast-dLLM\u76f8\u6bd4\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea610\u500d\uff0c\u4e0e\u9ad8\u5ea6\u4f18\u5316\u7684AR\u6a21\u578bQWen2.5-3B\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u53472-3\u500d\uff0c\u6027\u80fd\u65e0\u663e\u8457\u635f\u5931\u3002", "conclusion": "dInfer\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u578b\u5927\u6a21\u578b\u63a8\u7406\u6548\u7387\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u57fa\u7840\u4e0a\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u63a8\u52a8\u8be5\u7c7b\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2510.08702", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08702", "abs": "https://arxiv.org/abs/2510.08702", "authors": ["Xianzhen Luo", "Wenzhen Zheng", "Qingfu Zhu", "Rongyi Zhang", "Houyi Li", "Siming Huang", "YuanTao Fan", "Wanxiang Che"], "title": "Scaling Laws for Code: A More Data-Hungry Regime", "comment": "Under Review", "summary": "Code Large Language Models (LLMs) are revolutionizing software engineering.\nHowever, scaling laws that guide the efficient training are predominantly\nanalyzed on Natural Language (NL). Given the fundamental differences like\nstrict syntax between code and NL, it is unclear whether these laws are\ndirectly applicable to code. To address this gap, we conduct the first\nlarge-scale empirical study of scaling laws for code, comprising 117\nexperimental runs with model sizes from 0.2B to 3.8B and training tokens from\n2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results\nshow that the more expressive Farseer law offers greater accuracy. Second, the\nanalysis reveals that Code LLMs scale effectively with model size. Crucially,\ncode represents a more data-hungry regime, requiring a substantially higher\ndata-to-parameter ratio than NL. Finally, two additional sets of experiments on\ncode-NL mixtures show that NL benefits resource-constrained scenarios, but\nbecomes a detriment at higher compute budgets.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u9996\u6b21\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u4ee3\u7801\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u6269\u5c55\u89c4\u5f8b\uff0c\u53d1\u73b0\u5176\u5bf9\u6570\u636e\u9700\u6c42\u66f4\u9ad8\u3001\u6269\u5c55\u89c4\u5f8b\u4e0d\u540c\u4e8e\u81ea\u7136\u8bed\u8a00\uff0cFarseer law\u66f4\u9002\u5408\u62df\u5408\u4ee3\u7801\u6a21\u578b\u8868\u73b0\uff0c\u4e14\u6570\u636e\u7ec4\u5408\u65b9\u5f0f\u9700\u89c6\u8ba1\u7b97\u8d44\u6e90\u8c03\u6574\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u8bed\u8a00\u5927\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u89c4\u5f8b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u5b58\u5728\u5982\u4e25\u683c\u8bed\u6cd5\u7b49\u6839\u672c\u6027\u5dee\u5f02\uff0c\u56e0\u6b64\u73b0\u6709\u89c4\u5f8b\u662f\u5426\u540c\u6837\u9002\u7528\u4e8e\u4ee3\u7801\u6a21\u578b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5bf9117\u6b21\u4e0d\u540c\u89c4\u6a21\uff08\u6a21\u578b\u53c2\u65700.2B\u52303.8B\uff0c\u8bad\u7ec3token\u65702B\u5230128B\uff09\u7684\u5b9e\u9a8c\uff0c\u5b9e\u8bc1\u6027\u5730\u5206\u6790\u4e86\u4ee3\u7801\u9886\u57df\u7684\u5927\u6a21\u578b\u6269\u5c55\u89c4\u5f8b\uff0c\u5e76\u5206\u522b\u62df\u5408\u4e86Chinchilla law\u4e0eFarseer law\u3002\u6b64\u5916\uff0c\u8fd8\u989d\u5916\u8fdb\u884c\u4e86\u4ee3\u7801-\u81ea\u7136\u8bed\u8a00\u6df7\u5408\u8bad\u7ec3\u7684\u5b9e\u9a8c\u3002", "result": "Farseer law\u5bf9\u4ee3\u7801\u5927\u6a21\u578b\u7684\u62df\u5408\u6548\u679c\u4f18\u4e8eChinchilla law\uff1b\u4ee3\u7801\u5927\u6a21\u578b\u968f\u6a21\u578b\u89c4\u6a21\u589e\u957f\u65f6\u8868\u73b0\u51fa\u826f\u597d\u6269\u5c55\u6027\u3002\u540c\u65f6\uff0c\u4ee3\u7801\u7c7b\u6a21\u578b\u5bf9\u6570\u636e\u7684\u9700\u6c42\u6bd4\u81ea\u7136\u8bed\u8a00\u66f4\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u7684\u6570\u636e-\u53c2\u6570\u6bd4\u3002\u5728\u6df7\u5408\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0c\u81ea\u7136\u8bed\u8a00\u6570\u636e\u5728\u4f4e\u7b97\u529b\u4e0b\u53ef\u63d0\u5347\u6548\u679c\uff0c\u4f46\u9ad8\u7b97\u529b\u4e0b\u5219\u4e0d\u5229\u4e8e\u4ee3\u7801\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u4ee3\u7801\u5927\u6a21\u578b\u7684\u6269\u5c55\u89c4\u5f8b\u4e0e\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u9488\u5bf9\u4ee3\u7801\u573a\u666f\u5355\u72ec\u7814\u7a76\u5176\u9ad8\u6548\u8bad\u7ec3\u89c4\u5f8b\u3002Farseer law\u4e3a\u66f4\u51c6\u786e\u7684\u62df\u5408\u5de5\u5177\uff0c\u5e76\u5e94\u6ce8\u610f\u91c7\u96c6\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u5408\u7406\u5b89\u6392\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u6570\u636e\u7684\u6bd4\u4f8b\u3002"}}
{"id": "2510.08710", "categories": ["cs.CL", "68T50", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.08710", "abs": "https://arxiv.org/abs/2510.08710", "authors": ["Li Zhang", "Matthias Grabmair", "Morgan Gray", "Kevin Ashley"], "title": "Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning", "comment": "21 pages, 7 figures", "summary": "Case-based reasoning is a cornerstone of U.S. legal practice, requiring\nprofessionals to argue about a current case by drawing analogies to and\ndistinguishing from past precedents. While Large Language Models (LLMs) have\nshown remarkable capabilities, their proficiency in this complex, nuanced form\nof reasoning needs further investigation. We propose a formal framework that\ndecomposes the process of identifying significant distinctions between cases\ninto three-stage reasoning tasks. Our framework models cases using factual\npredicates called factors, organizes them into a legal knowledge hierarchy, and\ndefines verifiable rules for identifying distinctions, analyzing their\nargumentative support, and evaluating their significance. Through comprehensive\nevaluation of modern reasoning LLMs, we reveal a paradox: while models achieve\nhigh accuracy on surface-level reasoning (Task 1), performance degrades on\nhierarchical reasoning (Task 2: 64.82%-92.09%) and collapses on integrated\nanalysis (Task 3: 11.46%-33.99%). Most strikingly, we find that models\nconsistently expend more computational resources on incorrect responses than\ncorrect ones, suggesting that \"thinking longer\" does not always mean \"thinking\nsmarter.\" Our work provides a methodology for fine-grained analysis of LLM\nreasoning capabilities in complex domains and reveals fundamental limitations\nthat must be addressed for robust and trustworthy legal AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e09\u9636\u6bb5\u63a8\u7406\u7684\u6848\u4f8b\u533a\u5206\u6846\u67b6\uff0c\u8bc4\u4f30LLM\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0cLLM\u8868\u5c42\u63a8\u7406\u51c6\u786e\u4f46\u5728\u66f4\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd8\u6d6a\u8d39\u8d44\u6e90\uff0c\u63d0\u793a\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u80dc\u4efb\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "motivation": "\u7f8e\u56fd\u6cd5\u5f8b\u5b9e\u8df5\u9ad8\u5ea6\u4f9d\u8d56\u6848\u4f8b\u63a8\u7406\uff0c\u4e13\u4e1a\u4eba\u58eb\u9700\u8981\u901a\u8fc7\u7c7b\u6bd4\u4e0e\u533a\u5206\u5386\u53f2\u6848\u4f8b\u6765\u8bba\u8bc1\u5f53\u524d\u6848\u4ef6\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bb8\u591a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u590d\u6742\u800c\u7ec6\u817b\u7684\u6848\u4f8b\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u9700\u6df1\u5165\u63a2\u7a76\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5c06\u8bc6\u522b\u6848\u4ef6\u95f4\u5173\u952e\u533a\u522b\u7684\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e09\u9636\u6bb5\u63a8\u7406\u4efb\u52a1\u3002\u6846\u67b6\u7528\u201c\u56e0\u7d20\u201d\u6765\u8868\u5f81\u6848\u4ef6\u4e8b\u5b9e\uff0c\u901a\u8fc7\u6cd5\u5f8b\u77e5\u8bc6\u5c42\u7ea7\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5e76\u8bbe\u5b9a\u53ef\u9a8c\u8bc1\u7684\u89c4\u5219\u6765\u8bc6\u522b\u533a\u522b\u3001\u5206\u6790\u8bba\u8bc1\u652f\u6301\u53ca\u8bc4\u4f30\u5176\u91cd\u8981\u6027\u3002\u901a\u8fc7\u5bf9\u73b0\u4ee3\u63a8\u7406\u578bLLM\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u6765\u68c0\u9a8c\u5176\u63a8\u7406\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u53d1\u73b0\uff0cLLM\u5728\u8868\u5c42\u63a8\u7406\u4efb\u52a1\uff08\u4efb\u52a11\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c42\u7ea7\u63a8\u7406\uff08\u4efb\u52a12\uff09\u51c6\u786e\u7387\u4e0b\u964d\u81f364.82%-92.09%\uff0c\u7efc\u5408\u5206\u6790\uff08\u4efb\u52a13\uff09\u66f4\u662f\u4ec5\u670911.46%-33.99%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u9519\u8bef\u56de\u7b54\u4e0a\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u53cd\u800c\u66f4\u9ad8\uff0c\u8bf4\u660e\u201c\u601d\u8003\u66f4\u4e45\u201d\u5e76\u4e0d\u7b49\u4e8e\u201c\u601d\u8003\u66f4\u806a\u660e\u201d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u9886\u57df\u7684\u80fd\u529b\u5206\u6790\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u4fe1\u7684\u6cd5\u5f8bAI\u65b9\u9762\u5b58\u5728\u7684\u6839\u672c\u6027\u5c40\u9650\u3002"}}
{"id": "2510.08720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08720", "abs": "https://arxiv.org/abs/2510.08720", "authors": ["Xianzhen Luo", "Jinyang Huang", "Wenzhen Zheng", "Qingfu Zhu", "Mingzheng Xu", "Yiheng Xu", "Yuantao Fan", "Libo Qin", "Wanxiang Che"], "title": "How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective", "comment": "Work in Progress", "summary": "Evaluating test cases automatically generated by Large Language Models (LLMs)\nis a critical yet challenging task. Existing benchmarks suffer from high\ncomputational costs, score inflation, and a bias towards trivial bugs over\nrare, critical faults. In this work, we ask two fundamental questions: (1) What\nis the minimal set of wrong codes sufficient to represent the entire error\nspace? and (2) What is the minimal set of test cases needed to distinguish\nthem? We introduce a framework that formalizes benchmark construction as\nfinding an optimal diagnostic basis in a binary code-test matrix. The rank of\nthis matrix specifies the minimal number of independent error patterns (wrong\ncodes) and provides a tight upper bound on the number of test cases required\nfor complete fault coverage. Our objective is to identify a basis of size equal\nto the matrix rank that maximizes internal diversity. To tackle this NP-hard\nproblem, we propose WrongSelect, an efficient approximation algorithm to select\nmaximally diverse wrong codes. Applying this framework to millions of\ncompetitive programming submissions, we construct TC-Bench, a compact, diverse,\nand inflation-resistant benchmark. Extensive experiments show that even the\nmost advanced test case generation methods achieve only ~60% exclusion rates on\nTC-Bench, exposing a significant gap in their diagnostic power. Our dataset is\navailable at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is\nat: https://github.com/Luowaterbi/TC-Bench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4ee5\u4e8c\u5143\u77e9\u9635\u79e9\u4e3a\u7406\u8bba\u57fa\u7840\u7684\u65b0\u578b\u6d4b\u8bd5\u57fa\u51c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86TC-Bench\u3002\u5b9e\u9a8c\u53d1\u73b0\u4e3b\u6d41\u65b9\u6cd5\u68c0\u6d4b\u80fd\u529b\u6709\u9650\uff0cTC-Bench\u66f4\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u6709\u6548\u8986\u76d6\u7f55\u89c1\u5173\u952e\u9519\u8bef\uff0c\u8bc4\u4ef7\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u5206\u6570\u865a\u9ad8\uff0c\u96be\u4ee5\u771f\u5b9e\u53cd\u6620\u5148\u8fdb\u751f\u6210\u6a21\u578b\u7684\u6d4b\u8bd5\u80fd\u529b\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u5408\u7406\u9ad8\u6548\u7684\u6d4b\u8bd5\u7528\u4f8b\u57fa\u51c6\u3002", "method": "\u5c06\u57fa\u51c6\u6784\u5efa\u5f62\u5f0f\u5316\u4e3a\u5728\u4e8c\u5143\u4ee3\u7801-\u6d4b\u8bd5\u77e9\u9635\u4e2d\u5bfb\u627e\u6700\u4f18\u8bca\u65ad\u57fa\u3002\u63d0\u51fa\u4e86WrongSelect\u7b97\u6cd5\uff0c\u5bf9\u201c\u9519\u8bef\u4ee3\u7801\u201d\u7684\u9009\u62e9\u8fdb\u884c\u9ad8\u6548\u8fd1\u4f3c\uff0c\u786e\u4fdd\u591a\u6837\u6027\u6700\u5927\u5316\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u7ade\u8d5b\u7f16\u7a0b\u6837\u4f8b\u8fdb\u884c\u5b9e\u8bc1\uff0c\u5e76\u5f00\u53d1\u4e86TC-Bench\u57fa\u51c6\u3002", "result": "\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6709\u6548\u786e\u5b9a\u6700\u5c11\u201c\u9519\u8bef\u4ee3\u7801\u201d\u53ca\u6700\u5c0f\u6d4b\u8bd5\u96c6\uff0c\u5e76\u6784\u5efa\u4e86TC-Bench\u57fa\u51c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5728TC-Bench\u4e0a\u7684\u6392\u9664\u7387\u4ec5\u7ea660%\uff0c\u663e\u793a\u51fa\u660e\u663e\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u5206\u6570\u81a8\u80c0\u548c\u5bf9\u7410\u788e\u9519\u8bef\u504f\u597d\u7b49\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4ee5\u77e9\u9635\u79e9\u4e3a\u57fa\u7840\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u7528\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u66f4\u52a0\u7d27\u51d1\u3001\u591a\u6837\u3001\u6297\u81a8\u80c0\u7684TC-Bench\u57fa\u51c6\uff0c\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524d\u4e3b\u6d41\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u7684\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2510.08730", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08730", "abs": "https://arxiv.org/abs/2510.08730", "authors": ["Gregory Yauney", "Shahzaib Saqib Warraich", "Swabha Swayamdipta"], "title": "How Reliable is Language Model Micro-Benchmarking?", "comment": null, "summary": "Micro-benchmarking offers a solution to the often prohibitive time and cost\nof language model development: evaluate on a very small subset of existing\nbenchmarks. Can these micro-benchmarks, however, rank models as consistently as\nthe full benchmarks they replace? And can they rank models more consistently\nthan selecting a random subset of data points? In many scenarios, we find that\nthe answer is no. We introduce a meta-evaluation measure for micro-benchmarking\nwhich investigates how well a micro-benchmark can rank two models as a function\nof their performance difference on the full benchmark. This approach can\ndetermine which model pairs can be ranked correctly by a micro-benchmark,\nallowing for a finer-grained analysis of the trade-off between micro-benchmark\nsize and reliability. Prior work has suggested selecting as few as 10 examples;\nwe find that no micro-benchmarking method can consistently rank model pairs 3.5\npoints of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In\norder to consistently rank model pairs with relatively similar performances, we\nshow that often as many as 250 examples must be selected, at which point random\nsampling is competitive with existing micro-benchmarking methods. When\ncomparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25\nexamples, we find that more than half of pairwise comparisons are not likely to\nbe preserved. Our work provides actionable guidance for both micro-benchmark\nusers and developers in navigating the trade-off between evaluation efficiency\nand reliability.", "AI": {"tldr": "\u5fae\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u4ee5\u5c0f\u6837\u672c\u91cf\u7a33\u5b9a\u5730\u6392\u5e8f\u6a21\u578b\uff0c\u4ec5\u80fd\u7528\u66f4\u591a\u6837\u672c\u65f6\u63a5\u8fd1\u5b8c\u6574\u57fa\u51c6\u7684\u53ef\u9760\u6027\uff0c\u4e14\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u7ade\u4e89\u529b\u4e0d\u4f4e\u3002\u5e94\u6743\u8861\u8bc4\u6d4b\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u5b8c\u6574\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff08micro-benchmarking\uff09\u901a\u8fc7\u53ea\u8bc4\u4f30\u6781\u5c0f\u7684\u6570\u636e\u5b50\u96c6\uff0c\u6210\u4e3a\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bc4\u6d4b\u65b9\u6848\u3002\u8be5\u6587\u5173\u6ce8\u5fae\u57fa\u51c6\u6d4b\u8bd5\u80fd\u5426\u50cf\u5b8c\u6574\u57fa\u51c6\u4e00\u6837\u51c6\u786e\u5730\u5bf9\u6a21\u578b\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u5e0c\u671b\u7814\u7a76\u5fae\u57fa\u51c6\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u4e0e\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u8bc4\u6d4b\uff08meta-evaluation\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5fae\u57fa\u51c6\u6570\u636e\u96c6\u5927\u5c0f\u4e0b\u6a21\u578b\u95f4\u6392\u5e8f\u6b63\u786e\u7387\uff0c\u5c24\u5176\u5173\u6ce8\u5fae\u57fa\u51c6\u80fd\u5426\u6839\u636e\u5b8c\u6574\u57fa\u51c6\u7684\u6027\u80fd\u5dee\u5f02\u51c6\u786e\u533a\u5206\u6a21\u578b\u3002\u8fdb\u4e00\u6b65\u6bd4\u8f83\u4e86\u6709\u9488\u5bf9\u6027\u4e0e\u968f\u673a\u62bd\u6837\u7684\u4e0d\u540c\u5fae\u57fa\u51c6\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u8bb8\u591a\u573a\u666f\u4e0b\uff0c\u5fae\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u50cf\u5b8c\u6574\u57fa\u51c6\u4e00\u6837\u7a33\u5b9a\u5730\u5bf9\u6a21\u578b\u8fdb\u884c\u6392\u5e8f\uff0c\u4e14\u65e0\u6cd5\u663e\u8457\u4f18\u4e8e\u968f\u673a\u62bd\u6837\u3002\u5373\u4f7f\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u8f83\u5927\uff0c\u6240\u9700\u6837\u672c\u6570\u91cf\u4e5f\u8fdc\u5927\u4e8e\u6b64\u524d\u7684\u5efa\u8bae\uff08\u5982250\u4f8b\u800c\u975e10\u4f8b\uff09\uff0c\u4e14\u968f\u673a\u62bd\u6837\u5df2\u548c\u5176\u4ed6\u5fae\u57fa\u51c6\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002\u4f8b\u5982\uff0c\u5728MMLU-Pro\u4e0a\u4ec5\u752825\u4f8b\u6bd4\u8f838B\u6a21\u578b\u65f6\uff0c\u8d85\u8fc7\u4e00\u534a\u7684\u6a21\u578b\u5bf9\u7adf\u65e0\u6cd5\u5f97\u5230\u7a33\u5b9a\u6392\u5e8f\u3002", "conclusion": "\u5fae\u57fa\u51c6\u6d4b\u8bd5\u867d\u7136\u80fd\u63d0\u5347\u8bc4\u6d4b\u6548\u7387\uff0c\u4f46\u5728\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u9650\u5236\u3002\u5bf9\u4e8e\u6027\u80fd\u8f83\u76f8\u8fd1\u7684\u6a21\u578b\uff0c\u5fc5\u987b\u4f7f\u7528\u66f4\u5927\u91cf\u7684\u6837\u672c\u624d\u6709\u53ef\u80fd\u5f97\u5230\u53ef\u9760\u6392\u5e8f\uff0c\u540c\u65f6\u968f\u673a\u91c7\u6837\u7684\u8868\u73b0\u4e5f\u5f88\u6709\u7ade\u4e89\u529b\u3002\u56e0\u6b64\uff0c\u7528\u6237\u548c\u5f00\u53d1\u8005\u5e94\u5728\u8bc4\u6d4b\u6548\u7387\u4e0e\u7ed3\u679c\u53ef\u9760\u6027\u4e4b\u95f4\u8c28\u614e\u6743\u8861\uff0c\u4e0d\u5e94\u76f2\u76ee\u4f9d\u8d56\u8fc7\u5c0f\u89c4\u6a21\u7684\u5fae\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4e3a\u5fae\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e94\u7528\u548c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.08741", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08741", "abs": "https://arxiv.org/abs/2510.08741", "authors": ["Tessa Masis", "Brendan O'Connor"], "title": "Coordinates from Context: Using LLMs to Ground Complex Location References", "comment": "Under review at ARR", "summary": "Geocoding is the task of linking a location reference to an actual geographic\nlocation and is essential for many downstream analyses of unstructured text. In\nthis paper, we explore the challenging setting of geocoding compositional\nlocation references. Building on recent work demonstrating LLMs' abilities to\nreason over geospatial data, we evaluate LLMs' geospatial knowledge versus\nreasoning skills relevant to our task. Based on these insights, we propose an\nLLM-based strategy for geocoding compositional location references. We show\nthat our approach improves performance for the task and that a relatively small\nfine-tuned LLM can achieve comparable performance with much larger\noff-the-shelf models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eLLM\u7684\u5730\u7406\u7f16\u7801\u7b56\u7565\uff0c\u53ef\u66f4\u6709\u6548\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u7684\u590d\u5408\u4f4d\u7f6e\u8868\u8fbe\uff0c\u5e76\u8bc1\u660e\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u5177\u5907\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\u7684\u80fd\u529b\u3002", "motivation": "\u9762\u5bf9\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u7684\u590d\u5408\u578b\u4f4d\u7f6e\u5f15\u7528\uff0c\u4f20\u7edf\u5730\u7406\u7f16\u7801\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660eLLM\u5728\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u56e0\u6b64\u5e0c\u671b\u5229\u7528LLM\u63d0\u5347\u590d\u5408\u5730\u7406\u7f16\u7801\u7684\u6548\u679c\u3002", "method": "\u5206\u6790LLM\u5728\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u8bba\uff0c\u8bbe\u8ba1\u5e76\u5e94\u7528\u4e86\u4e00\u79cdLLM\u9a71\u52a8\u7684\u5730\u7406\u7f16\u7801\u7b56\u7565\u3002\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\uff08\u5c0f\u578b\u5fae\u8c03\u548c\u5927\u578b\u9884\u8bad\u7ec3\uff09\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5730\u7406\u7f16\u7801\u590d\u5408\u4f4d\u7f6e\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5c0f\u578b\u5fae\u8c03LLM\u7684\u6548\u679c\u4e0e\u5927\u578b\u73b0\u6210\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5730\u7406\u7f16\u7801\u590d\u5408\u4f4d\u7f6e\u8bc6\u522b\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\uff1b\u4e14\u7ecf\u8fc7\u5fae\u8c03\u7684\u5c0f\u578bLLM\u53ef\u8fbe\u5230\u4e0e\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\u7684\u6548\u679c\u3002"}}
{"id": "2510.08776", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08776", "abs": "https://arxiv.org/abs/2510.08776", "authors": ["Kimaya Basu", "Savi Kolari", "Allison Yu"], "title": "Measuring Moral LLM Responses in Multilingual Capacities", "comment": "10 pages, 5 figures; referenced articles: arXiv:2303.08774,\n  arXiv:2303.12528, arXiv:2308.14132, arXiv:2505.12201, arXiv:2406.04428,\n  arXiv:2407.02273, arXiv:2404.01268, arXiv:2502.09747, arXiv:2507.13474,\n  arXiv:2505.21479, arXiv:2306.05685", "summary": "With LLM usage becoming widespread across countries, languages, and humanity\nmore broadly, the need to understand and guardrail their multilingual responses\nincreases. Large-scale datasets for testing and benchmarking have been created\nto evaluate and facilitate LLM responses across multiple dimensions. In this\nstudy, we evaluate the responses of frontier and leading open-source models in\nfive dimensions across low and high-resource languages to measure LLM accuracy\nand consistency across multilingual contexts. We evaluate the responses using a\nfive-point grading rubric and a judge LLM. Our study shows that GPT-5 performed\nthe best on average in each category, while other models displayed more\ninconsistency across language and category. Most notably, in the Consent &\nAutonomy and Harm Prevention & Safety categories, GPT scored the highest with\naverages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages\nof 1.39 and 1.98, respectively. These findings emphasize the need for further\ntesting on how linguistic shifts impact LLM responses across various categories\nand improvement in these areas.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u6d4b\u4e86\u4e3b\u6d41LLM\u5728\u591a\u8bed\u79cd\u548c\u591a\u7ef4\u5ea6\u4e0b\u7684\u54cd\u5e94\u8868\u73b0\uff0c\u53d1\u73b0GPT-5\u5728\u51c6\u786e\u6027\u3001\u5b89\u5168\u6027\u7b49\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u800c\u5176\u5b83\u6a21\u578b\u5728\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u7814\u7a76\u547c\u5401\u7ee7\u7eed\u5173\u6ce8\u8bed\u8a00\u5dee\u5f02\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u9886\u57df\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u4eba\u4eec\u5bf9\u5176\u591a\u8bed\u79cd\u54cd\u5e94\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u7684\u5173\u6ce8\u65e5\u76ca\u589e\u52a0\u3002\u5f53\u524d\u4e9f\u9700\u7406\u89e3\u548c\u89c4\u8303LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e0b\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u654f\u611f\u6216\u5b89\u5168\u76f8\u5173\u5185\u5bb9\u65f6\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e94\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u91c7\u7528\u5927\u89c4\u6a21\u6d4b\u8bd5\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5bf9\u524d\u6cbf\u548c\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u591a\u8bed\u79cd\u54cd\u5e94\u80fd\u529b\u6d4b\u8bd5\u3002\u8bc4\u4f30\u5de5\u5177\u5305\u62ec\u4e94\u5206\u5236\u8bc4\u5206\u6807\u51c6\u4ee5\u53ca\u4e00\u4e2aLLM\u88c1\u5224\u7cfb\u7edf\uff0c\u5206\u522b\u5728\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u6d4b\u91cfLLM\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "GPT-5\u5728\u5404\u8bc4\u4f30\u7c7b\u522b\u4e2d\u7684\u5e73\u5747\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u201c\u540c\u610f\u4e0e\u81ea\u4e3b\u6743\u201d\u3001\u201c\u4f24\u5bb3\u9884\u9632\u4e0e\u5b89\u5168\u201d\u7c7b\u522b\u5206\u522b\u83b7\u5f973.56\u548c4.73\u5206\u3002\u800cGemini 2.5 Pro\u5728\u4e0a\u8ff0\u4e24\u9879\u5f97\u5206\u6700\u4f4e\uff0c\u5206\u522b\u4e3a1.39\u548c1.98\u5206\u3002\u5176\u4ed6\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u7c7b\u522b\u4e2d\u7684\u4e00\u81f4\u6027\u8f83\u5dee\u3002", "conclusion": "\u4e0d\u540c\u8bed\u8a00\u548c\u7c7b\u522b\u95f4\uff0cLLM\u54cd\u5e94\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002GPT-5\u5728\u591a\u8bed\u79cd\u73af\u5883\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u6574\u4f53\u884c\u4e1a\u5728\u654f\u611f\u8bdd\u9898\u548c\u5b89\u5168\u9632\u62a4\u9886\u57df\u4ecd\u9700\u63d0\u5347\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u8bed\u8a00\u5dee\u5f02\u5f71\u54cdLLM\u8868\u73b0\u7684\u6301\u7eed\u6d4b\u8bd5\u4e0e\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.08798", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08798", "abs": "https://arxiv.org/abs/2510.08798", "authors": ["S M Rafiuddin", "Muntaha Nujat Khan"], "title": "Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models", "comment": "14 Pages, 2 Figures, 6 Table, Accepted at EMNLP 2025 Findings as a\n  Short Paper", "summary": "Transformer attention scales quadratically with sequence length O(n^2),\nlimiting long-context use. We propose Adaptive Retention, a probabilistic,\nlayer-wise token selection mechanism that learns which representations to keep\nunder a strict global budget M. Retention is modeled with Bernoulli gates\ntrained via a Hard-Concrete/variational relaxation and enforced with a simple\ntop-M rule at inference, making the method differentiable and drop-in for\nstandard encoders. Across classification, extractive QA, and long-document\nsummarization, keeping only 30-50% of tokens preserves >= 95% of full-model\nperformance while cutting peak memory by ~35-45% and improving throughput by up\nto ~1.8x. This architecture-agnostic approach delivers practical long-context\nefficiency without modifying base attention or task heads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u9ad8\u6548Transformer\u673a\u5236Adaptive Retention\uff0c\u5728\u4fdd\u8bc1\u7edd\u5927\u90e8\u5206\u6a21\u578b\u6027\u80fd\u7684\u6761\u4ef6\u4e0b\uff0c\u4ec5\u9700\u4fdd\u7559\u90e8\u5206token\u5c31\u53ef\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u901f\u5ea6\uff0c\u4e3a\u957f\u5e8f\u5217\u4efb\u52a1\u5e26\u6765\u5b9e\u7528\u7684\u6548\u7387\u63d0\u5347\uff0c\u65e0\u9700\u6539\u53d8\u539f\u6709\u6ce8\u610f\u529b\u7ed3\u6784\u3002", "motivation": "Transformer\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u5448\u5e73\u65b9\u589e\u957f\uff0c\u9650\u5236\u4e86\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eBernoulli\u95e8\u63a7\u673a\u5236\uff0c\u901a\u8fc7Hard-Concrete/\u53d8\u5206\u677e\u5f1b\u65b9\u5f0f\u8bad\u7ec3\uff0c\u5728\u6bcf\u5c42\u52a8\u6001\u5730\u9009\u62e9\u9700\u8981\u4fdd\u7559\u7684token\uff0c\u5e76\u901a\u8fc7top-M\u89c4\u5219\u5728\u63a8\u7406\u9636\u6bb5\u4e25\u683c\u63a7\u5236\u9009\u4e2dtoken\u6570\uff1b\u8fd9\u79cd\u65b9\u6cd5\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u6807\u51c6\u7f16\u7801\u5668\u4e2d\uff0c\u65e0\u9700\u66f4\u6539\u539f\u59cb\u6ce8\u610f\u529b\u6216\u4efb\u52a1\u5934\u90e8\u7ed3\u6784\u3002", "result": "\u5728\u6587\u672c\u5206\u7c7b\u3001\u62bd\u53d6\u5f0f\u95ee\u7b54\u548c\u957f\u6587\u6863\u6458\u8981\u7b49\u4efb\u52a1\u4e0a\uff0c\u4ec5\u4fdd\u755930-50% token\u540e\uff0c\u6a21\u578b\u6027\u80fd\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\uff08\u4fdd\u7559>= 95%\u7684\u5b8c\u6574\u6a21\u578b\u6027\u80fd\uff09\uff0c\u540c\u65f6\u5cf0\u503c\u5185\u5b58\u964d\u4f4e35-45%\uff0c\u541e\u5410\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe1.8\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684Adaptive Retention\u65b9\u6cd5\u80fd\u591f\u5728\u51e0\u4e4e\u4e0d\u635f\u5931\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u4ece\u800c\u63d0\u5347Transformer\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u7387\u3002"}}
{"id": "2510.08800", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08800", "abs": "https://arxiv.org/abs/2510.08800", "authors": ["Wangjie You", "Xusheng Wang", "Xing Wang", "Wenxiang Jiao", "Chao Feng", "Juntao Li", "Min Zhang"], "title": "Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated advanced reasoning\ncapabilities, their comprehensive evaluation in general Chinese-language\ncontexts remains understudied. To bridge this gap, we propose Chinese\nCommonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate\nLLMs' ability to integrate Chinese-specific factual knowledge with multi-step\nlogical reasoning. Specifically, we first construct a domain-balanced seed set\nfrom existing QA datasets, then develop an LLM-powered pipeline to generate\nmulti-hop questions anchored on factual unit chains. To ensure the quality of\nresulting dataset, we implement a human-in-the-loop verification system, where\ndomain experts systematically validate and refine the generated questions.\nUsing CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent\nlimitations in LLMs' ability to process long-tail knowledge and execute\nknowledge-intensive reasoning. Notably, retrieval-augmented generation\nsubstantially mitigates these knowledge gaps, yielding significant performance\ngains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u4e2d\u6587\u591a\u6b65\u63a8\u7406\u7684CCMOR\u57fa\u51c6\uff0c\u5e76\u7528\u5176\u8bc4\u6d4b\u591a\u79cdLLM\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u77e5\u8bc6\u878d\u5408\u4e0e\u63a8\u7406\u80fd\u529b\u4e0a\u6709\u9650\uff0c\u4f46\u68c0\u7d22\u589e\u5f3a\u6280\u672f\u80fd\u663e\u8457\u6539\u5584\u8fd9\u4e9b\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e2d\u6587\u9886\u57df\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u7efc\u5408\u6027\u57fa\u51c6\u6765\u5206\u6790\u5176\u5728\u591a\u6b65\u63a8\u7406\u4e0e\u4e2d\u6587\u77e5\u8bc6\u878d\u5408\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86CCMOR\uff08Chinese Commonsense Multi-hop Reasoning\uff09\u57fa\u51c6\uff0c\u901a\u8fc7\u4ece\u73b0\u6709\u95ee\u7b54\u6570\u636e\u96c6\u7b5b\u9009\u5e73\u8861\u7684\u79cd\u5b50\u96c6\uff0c\u501f\u52a9LLM\u751f\u6210\u57fa\u4e8e\u4e8b\u5b9e\u94fe\u6761\u7684\u591a\u6b65\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u7528\u4e13\u5bb6\u4eba\u5de5\u5ba1\u6838\u751f\u6210\u95ee\u9898\uff0c\u4fdd\u969c\u6570\u636e\u8d28\u91cf\u3002", "result": "\u901a\u8fc7CCMOR\u5bf9\u4e1a\u754c\u4e3b\u6d41LLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u957f\u5c3e\u77e5\u8bc6\u4e0e\u590d\u6742\u63a8\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u4e2d\u6587\u591a\u6b65\u63a8\u7406\u53ca\u957f\u5c3e\u77e5\u8bc6\u5904\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u77ed\u677f\uff0c\u53ef\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u83b7\u5f97\u6709\u6548\u6539\u8fdb\u3002"}}
{"id": "2510.08804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08804", "abs": "https://arxiv.org/abs/2510.08804", "authors": ["Siddeshwar Raghavan", "Tanwi Mallick"], "title": "MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding", "comment": null, "summary": "We present MOSAIC, a multi-agent Large Language Model (LLM) framework for\nsolving challenging scientific coding tasks. Unlike general-purpose coding,\nscientific workflows require algorithms that are rigorous, interconnected with\ndeep domain knowledge, and incorporate domain-specific reasoning, as well as\nalgorithm iteration without requiring I/O test cases. Many scientific problems\nalso require a sequence of subproblems to be solved, leading to the final\ndesired result. MOSAIC is designed as a training-free framework with specially\ndesigned agents to self-reflect, create the rationale, code, and debug within a\nstudent-teacher paradigm to address the challenges of scientific code\ngeneration. This design facilitates stepwise problem decomposition, targeted\nerror correction, and, when combined with our Consolidated Context Window\n(CCW), mitigates LLM hallucinations when solving complex scientific tasks\ninvolving chained subproblems. We evaluate MOSAIC on scientific coding\nbenchmarks and demonstrate that our specialized agentic framework outperforms\nexisting approaches in terms of accuracy, robustness, and interpretability.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u79cd\u65e0\u987b\u8bad\u7ec3\u3001\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u89e3\u51b3\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u662f\u79d1\u5b66\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002", "motivation": "\u5728\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u9886\u57df\u77e5\u8bc6\u6df1\u3001\u7b97\u6cd5\u590d\u6742\u3001\u9700\u6b65\u9aa4\u5206\u89e3\u7b49\u6311\u6218\uff0c\u4e14\u9519\u8bef\u7ea0\u6b63\u548c\u89e3\u91ca\u80fd\u529b\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u7684\u56f0\u96be\u3002", "method": "MOSAIC\u63d0\u51fa\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u4e0d\u9700\u8981\u989d\u5916\u8bad\u7ec3\uff0c\u6db5\u76d6\u81ea\u6211\u53cd\u601d\u3001\u903b\u8f91\u63a8\u6f14\u3001\u7f16\u5199\u4ee3\u7801\u548c\u8c03\u8bd5\u3002\u91c7\u7528\u2018\u5e08\u751f\u2019\u8303\u5f0f\uff0c\u7ed3\u5408Consolidated Context Window\uff08CCW\uff09\uff0c\u5b9e\u73b0\u9010\u6b65\u95ee\u9898\u5206\u89e3\u3001\u9488\u5bf9\u6027\u7ea0\u9519\u548c\u7f13\u89e3\u5927\u6a21\u578b\u5e7b\u89c9\u3002", "result": "MOSAIC\u5728\u79d1\u5b66\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u4e13\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u53ca\u53ef\u89e3\u91ca\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u3001\u4e13\u7528\u903b\u8f91\u7684LLM\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4e3a\u590d\u6742\u79d1\u5b66\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u66f4\u5f3a\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2510.08813", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08813", "abs": "https://arxiv.org/abs/2510.08813", "authors": ["Abhishek K. Mishra", "Antoine Boutet", "Lucas Magnana"], "title": "The Model's Language Matters: A Comparative Privacy Analysis of LLMs", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed across multilingual\napplications that handle sensitive data, yet their scale and linguistic\nvariability introduce major privacy risks. Mostly evaluated for English, this\npaper investigates how language structure affects privacy leakage in LLMs\ntrained on English, Spanish, French, and Italian medical corpora. We quantify\nsix linguistic indicators and evaluate three attack vectors: extraction,\ncounterfactual memorization, and membership inference. Results show that\nprivacy vulnerability scales with linguistic redundancy and tokenization\ngranularity: Italian exhibits the strongest leakage, while English shows higher\nmembership separability. In contrast, French and Spanish display greater\nresilience due to higher morphological complexity. Overall, our findings\nprovide the first quantitative evidence that language matters in privacy\nleakage, underscoring the need for language-aware privacy-preserving mechanisms\nin LLM deployments.", "AI": {"tldr": "\u4e0d\u540c\u8bed\u8a00\u7684\u7ed3\u6784\u5bf9LLM\u9690\u79c1\u6cc4\u9732\u5f71\u54cd\u663e\u8457\uff0c\u610f\u5927\u5229\u8bed\u6700\u6613\u6cc4\u9732\uff0c\u82f1\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u6709\u4e0d\u540c\u7279\u6027\uff0c\u5f3a\u8c03\u9700\u7ed3\u5408\u8bed\u8a00\u7279\u6027\u7684\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002", "motivation": "\u73b0\u6709LLM\u9690\u79c1\u8bc4\u4f30\u4e3b\u8981\u9488\u5bf9\u82f1\u6587\uff0c\u5ffd\u89c6\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8bed\u8a00\u7ed3\u6784\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\u3002\u5404\u8bed\u8a00\u7684\u5f62\u6001\u590d\u6742\u5ea6\u548c\u5197\u4f59\u5ea6\u4e0d\u540c\uff0c\u6545\u9700\u7cfb\u7edf\u6027\u5206\u6790\u8bed\u8a00\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6cc4\u9732\u3002", "method": "\u4f5c\u8005\u5bf9\u82f1\u6587\u3001\u6cd5\u6587\u3001\u897f\u73ed\u7259\u6587\u548c\u610f\u5927\u5229\u6587\u533b\u5b66\u8bed\u6599\u4e0a\u8bad\u7ec3\u7684LLM\u8fdb\u884c\u9690\u79c1\u653b\u51fb\u6d4b\u8bd5\uff0c\u5206\u6790\u516d\u79cd\u8bed\u8a00\u5b66\u6307\u6807\uff0c\u901a\u8fc7\u4e09\u79cd\u653b\u51fb\u65b9\u5f0f\uff08\u4fe1\u606f\u63d0\u53d6\u3001\u53cd\u4e8b\u5b9e\u8bb0\u5fc6\u3001\u6210\u5458\u5f52\u5c5e\u63a8\u65ad\uff09\u91cf\u5316\u9690\u79c1\u6cc4\u9732\u60c5\u51b5\u3002", "result": "\u610f\u5927\u5229\u8bed\u56e0\u5197\u4f59\u548c\u5206\u8bcd\u7c92\u5ea6\u6cc4\u9732\u6700\u5f3a\uff1b\u82f1\u8bed\u6210\u5458\u53ef\u5206\u79bb\u6027\u9ad8\uff0c\u4f46\u6cc4\u9732\u4e0d\u5982\u610f\u5927\u5229\u8bed\uff1b\u6cd5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u7531\u4e8e\u5f62\u6001\u590d\u6742\uff0c\u6cc4\u9732\u8f83\u5c0f\uff0c\u8868\u73b0\u66f4\u6709\u97e7\u6027\u3002", "conclusion": "\u8bed\u8a00\u7ed3\u6784\u663e\u8457\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002\u5e94\u5728LLM\u90e8\u7f72\u4e2d\u5f15\u5165\u8bed\u8a00\u654f\u611f\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002"}}
{"id": "2510.08825", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08825", "abs": "https://arxiv.org/abs/2510.08825", "authors": ["Jia Ao Sun", "Hao Yu", "Fabrizio Gotti", "Fengran Mo", "Yihong Wu", "Yuchen Hui", "Jian-Yun Nie"], "title": "Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive reasoning abilities\nyet remain unreliable on knowledge-intensive, multi-hop questions -- they miss\nlong-tail facts, hallucinate when uncertain, and their internal knowledge lags\nbehind real-world change. Knowledge graphs (KGs) offer a structured source of\nrelational evidence, but existing KGQA methods face fundamental trade-offs:\ncompiling complete SPARQL queries without knowing available relations proves\nbrittle, retrieving large subgraphs introduces noise, and complex agent\nframeworks with parallel exploration exponentially expand search spaces. To\naddress these limitations, we propose Search-on-Graph (SoG), a simple yet\neffective framework that enables LLMs to perform iterative informed graph\nnavigation using a single, carefully designed \\textsc{Search} function. Rather\nthan pre-planning paths or retrieving large subgraphs, SoG follows an\n``observe-then-navigate'' principle: at each step, the LLM examines actual\navailable relations from the current entity before deciding on the next hop.\nThis approach further adapts seamlessly to different KG schemas and handles\nhigh-degree nodes through adaptive filtering. Across six KGQA benchmarks\nspanning Freebase and Wikidata, SoG achieves state-of-the-art performance\nwithout fine-tuning. We demonstrate particularly strong gains on Wikidata\nbenchmarks (+16\\% improvement over previous best methods) alongside consistent\nimprovements on Freebase benchmarks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Search-on-Graph\uff08SoG\uff09\u65b9\u6cd5\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u9ad8\u6548\u9010\u6b65\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5bfc\u822a\u63a8\u7406\uff0c\u65e0\u9700\u9884\u5148\u68c0\u7d22\u5927\u5b50\u56fe\u6216\u590d\u6742\u89c4\u5212\uff0c\u53d6\u5f97\u4e86\u8de8\u591a\u4e2aKGQA\u57fa\u51c6\u7684\u6700\u4f73\u6216\u663e\u8457\u63d0\u5347\u6210\u679c\uff0c\u5e76\u5c55\u73b0\u4e86\u5bf9\u590d\u6742\u7ed3\u6784\u7684\u5f3a\u9002\u5e94\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u8868\u73b0\u51fa\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u3001\u591a\u8df3\u95ee\u9898\u4e0a\u4e0d\u53ef\u9760\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u65b9\u6cd5\u4e5f\u9762\u4e34\u8bf8\u591a\u6743\u8861\uff08\u5982SPARQL\u67e5\u8be2\u8106\u5f31\u3001\u5b50\u56fe\u68c0\u7d22\u566a\u97f3\u3001\u641c\u7d22\u7a7a\u95f4\u7206\u70b8\uff09\uff0c\u6545\u9700\u65b0\u7684\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86Search-on-Graph\uff08SoG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5355\u4e00Search\u51fd\u6570\uff0c\u8ba9LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8fdb\u884c\u9010\u6b65\u3001\u4fe1\u606f\u6307\u5bfc\u7684\u4ea4\u4e92\u5f0f\u5bfc\u822a\uff0c\u800c\u4e0d\u662f\u9884\u5148\u89c4\u5212\u8def\u5f84\u6216\u68c0\u7d22\u5927\u5b50\u56fe\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u201c\u89c2\u5bdf\u2014\u518d\u5bfc\u822a\u201d\u539f\u5219\uff0c\u6309\u9700\u9002\u5e94\u4e0d\u540c\u56fe\u8c31\u7ed3\u6784\u548c\u9ad8\u5ea6\u8fde\u63a5\u8282\u70b9\u3002", "result": "\u5728\u516d\u4e2aKGQA\u57fa\u51c6\u6570\u636e\u96c6\uff08\u6db5\u76d6Freebase\u548cWikidata\uff09\u4e0a\uff0cSoG\u65e0\u987b\u5fae\u8c03\u5373\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0cWikidata\u63d0\u5347\u5e45\u5ea6\u8fbe16%\uff0cFreebase\u4e5f\u6709\u7a33\u5b9a\u63d0\u5347\u3002", "conclusion": "SoG\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u548c\u68c0\u7d22\u80fd\u529b\uff0c\u5c24\u5176\u662f\u9002\u5e94\u591a\u6837\u7ed3\u6784\u548c\u5b9e\u9645\u9ad8\u6027\u80fd\u8868\u73b0\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.08859", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08859", "abs": "https://arxiv.org/abs/2510.08859", "authors": ["Ragib Amin Nihal", "Rui Wen", "Kazuhiro Nakadai", "Jun Sakuma"], "title": "Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models", "comment": null, "summary": "Large language models (LLMs) remain vulnerable to multi-turn jailbreaking\nattacks that exploit conversational context to bypass safety constraints\ngradually. These attacks target different harm categories (like malware\ngeneration, harassment, or fraud) through distinct conversational approaches\n(educational discussions, personal experiences, hypothetical scenarios).\nExisting multi-turn jailbreaking methods often rely on heuristic or ad hoc\nexploration strategies, providing limited insight into underlying model\nweaknesses. The relationship between conversation patterns and model\nvulnerabilities across harm categories remains poorly understood. We propose\nPattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation\npatterns to construct effective multi-turn jailbreaks through natural dialogue.\nEvaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve\nstate-of-the-art performance, uncovering pattern-specific vulnerabilities and\nLLM behavioral characteristics: models exhibit distinct weakness profiles where\nrobustness to one conversational pattern does not generalize to others, and\nmodel families share similar failure modes. These findings highlight\nlimitations of safety training and indicate the need for pattern-aware\ndefenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PE-CoA\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u5bf9\u8bdd\u6a21\u5f0f\u4e0b\u7684\u9ad8\u7ea7\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bad\u7ec3\u7684\u5c40\u9650\uff0c\u5e76\u547c\u5401\u53d1\u5c55\u9488\u5bf9\u4e0d\u540c\u5bf9\u8bdd\u6a21\u5f0f\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8f6e\u201c\u8d8a\u72f1\u201d\u653b\u51fb\u4e0b\u4f9d\u7136\u8106\u5f31\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u9010\u6b65\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u6765\u7ed5\u8fc7\u5b89\u5168\u7ea6\u675f\u3002\u8fd9\u4e9b\u653b\u51fb\u6d89\u53ca\u591a\u79cd\u6709\u5bb3\u7c7b\u522b\uff0c\u5e76\u91c7\u7528\u4e0d\u540c\u7684\u5bf9\u8bdd\u7b56\u7565\u3002\u5f53\u524d\u7684\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u63a2\u7d22\uff0c\u96be\u4ee5\u63ed\u793a\u6a21\u578b\u6df1\u5c42\u6b21\u7684\u5f31\u70b9\u3002\u5bf9\u8bdd\u6a21\u5f0f\u4e0e\u6a21\u578b\u8106\u5f31\u6027\u7684\u5173\u8054\u5c1a\u672a\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PE-CoA\uff08Pattern Enhanced Chain of Attack\uff0c\u6a21\u5f0f\u589e\u5f3a\u94fe\u5f0f\u653b\u51fb\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u7c7b\u5bf9\u8bdd\u6a21\u5f0f\u7cfb\u7edf\u6027\u6784\u5efa\u9ad8\u6548\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u5e76\u4ee5\u66f4\u81ea\u7136\u7684\u5bf9\u8bdd\u5f62\u5f0f\u5b9e\u73b0\u3002\u4f5c\u8005\u572812\u4e2aLLM\u6a21\u578b\u3001\u8986\u76d610\u79cd\u6709\u5bb3\u7c7b\u522b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "PE-CoA \u5927\u5e45\u63d0\u5347\u4e86\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u5bf9\u8bdd\u6a21\u5f0f\u4e0b\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u8106\u5f31\u6027\u7279\u5f81\uff1a\u5bf9\u67d0\u4e00\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u5e76\u4e0d\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u6a21\u5f0f\uff0c\u4e14\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u95f4\u5177\u6709\u76f8\u4f3c\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "LLM\u5728\u5bf9\u6297\u590d\u6742\u8d8a\u72f1\u7b56\u7565\u65f6\u4f9d\u7136\u5b58\u5728\u6a21\u5f0f\u76f8\u5173\u7684\u6f0f\u6d1e\uff0c\u73b0\u6709\u5b89\u5168\u8bad\u7ec3\u63aa\u65bd\u65e0\u6cd5\u5168\u9762\u8986\u76d6\u6240\u6709\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u5b89\u5168\u9632\u5fa1\u9700\u7ed3\u5408\u653b\u51fb\u6a21\u5f0f\u7279\u6027\u8bbe\u8ba1\uff0c\u63d0\u5347\u6a21\u578b\u6574\u4f53\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.08870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08870", "abs": "https://arxiv.org/abs/2510.08870", "authors": ["Krzysztof Mrozinski", "Minji Kang", "Ahmed Khota", "Vincent Michael Sutanto", "Giovanni Gatti De Giacomo"], "title": "Quality Estimation Reranking for Document-Level Translation", "comment": "9 pages, 4 figures", "summary": "Quality estimation (QE) reranking is a form of quality-aware decoding which\naims to improve machine translation (MT) by scoring and selecting the best\ncandidate from a pool of generated translations. While known to be effective at\nthe sentence level, its application to the increasingly prominent domain of\ndocument-level translation remains underexplored. In this work, we evaluate QE\nreranking performance on document-level (rather than the typical\nsentence-level) translation, using various learned and large language model\n(LLM)-based QE metrics. We find that with our best learned metric, SLIDE,\nBLEURT-20 scores improve by +2.00 with only two candidates, and by +5.09 with\n32, across both decoder-only LLM models and encoder-decoder neural machine\ntranslation (NMT) models. Using the best LLM-based metric, GEMBA-DA, gains of\n+1.63 and +4.30 are achieved under the same conditions. Although gains shrink\nwith longer inputs, reranking with 32 candidates yields improvements of +2.34\n(SLIDE) and +1.40 (GEMBA-DA) on our longest documents (512-1024 source tokens).\nThese findings demonstrate the practical value of document-level QE, with\nminimal runtime overhead given suitable translation models and hardware.", "AI": {"tldr": "\u5c06QE\u91cd\u6392\u5e8f\u65b9\u6cd5\u4ece\u53e5\u5b50\u7ea7\u62d3\u5c55\u5230\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\uff0c\u5229\u7528\u5b66\u4e60\u578b\u548cLLM\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u7ea7\u8bd1\u6587\u7684\u8d28\u91cf\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0c\u663e\u793a\u5176\u5728\u5b9e\u9645\u6587\u6863\u7ffb\u8bd1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4ee5\u5f80\u7684\u7ffb\u8bd1\u8d28\u91cf\u4f30\u8ba1\uff08QE\uff09\u91cd\u6392\u5e8f\u591a\u5e94\u7528\u4e8e\u53e5\u5b50\u7ea7\u522b\uff0c\u5bf9\u65e5\u76ca\u91cd\u8981\u7684\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u7684\u7814\u7a76\u548c\u5e94\u7528\u8f83\u4e3a\u7f3a\u4e4f\u3002\u672c\u6587\u60f3\u63a2\u7d22\u5728\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0cQE\u91cd\u6392\u5e8f\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e0d\u540c\u7684\u5b66\u4e60\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u57fa\u7840\u7684QE\u6307\u6807\uff0c\u5bf9\u6587\u6863\u7ea7\uff08\u800c\u975e\u4f20\u7edf\u53e5\u5b50\u7ea7\uff09\u7ffb\u8bd1\u8fdb\u884c\u91cd\u6392\u5e8f\u5b9e\u9a8c\u3002\u4f7f\u7528\u5305\u62ecSLIDE\u548cGEMBA-DA\u5728\u5185\u7684\u5404\u79cdQE\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u5019\u9009\u8bd1\u6587\u6570\u91cf\u548c\u8f93\u5165\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u901a\u8fc7\u6700\u4f73\u5b66\u4e60\u578b\u6307\u6807SLIDE\uff0c\u5728\u4e24\u5019\u9009\u4e0bBLEURT-20\u5206\u63d0\u9ad82.00\u5206\uff0c32\u5019\u9009\u4e0b\u63d0\u9ad85.09\u5206\uff1b\u6700\u4f73LLM\u6307\u6807GEMBA-DA\u4e0b\u5206\u522b\u63d0\u53471.63\u548c4.30\u5206\u3002\u5728\u957f\u6587\u6863\uff08512-1024\u8bcd\uff09\u4e0b\uff0c\u4f7f\u752832\u5019\u9009\u65f6\uff0cSLIDE\u548cGEMBA-DA\u5206\u522b\u63d0\u53472.34\u548c1.40\u5206\u3002", "conclusion": "\u6587\u6863\u7ea7\u7ffb\u8bd1\u7684\u8d28\u91cf\u4f30\u8ba1\u91cd\u6392\u5e8f\u5728\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4ec5\u9700\u5f88\u5c11\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u5408\u9002\u7684\u6a21\u578b\u548c\u786c\u4ef6\u4e0b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.08886", "categories": ["cs.CL", "cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08886", "abs": "https://arxiv.org/abs/2510.08886", "authors": ["Yan Wang", "Keyi Wang", "Shanshan Yang", "Jaisal Patel", "Jeff Zhao", "Fengran Mo", "Xueqing Peng", "Lingfei Qian", "Jimin Huang", "Guojun Xiong", "Xiao-Yang Liu", "Jian-Yun Nie"], "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs", "comment": null, "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u611f\u77e5\u3001\u591a\u6587\u6863\u7684\u8d22\u52a1\u5ba1\u8ba1\u57fa\u51c6FinAuditing\uff0c\u5e76\u7528\u5176\u8bc4\u6d4b\u4e8613\u6b3e\u4e3b\u6d41LLM\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u5c42\u7ea7\u7ed3\u6784\u4e0e\u672c\u4f53\u9a71\u52a8\u7684\u8d22\u52a1\u63a8\u7406\u4e0a\u7684\u660e\u663e\u4e0d\u8db3\uff0c\u76f8\u5173\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "motivation": "GAAP\u548cXBRL\u7684\u590d\u6742\u7ed3\u6784\u8ba9\u81ea\u52a8\u5316\u5ba1\u8ba1\u53d8\u5f97\u6781\u5177\u6311\u6218\uff0c\u800c\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u64c5\u957f\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u4f46\u5728\u6d89\u53ca\u7ed3\u6784\u5316\u3001\u5c42\u7ea7\u548c\u6709\u4e30\u5bcc\u672c\u4f53\u5206\u7c7b\u7684\u8d22\u52a1\u6587\u4ef6\u63a8\u7406\u65b9\u9762\u80fd\u529b\u672a\u77e5\u3002\u4f5c\u8005\u65e8\u5728\u7cfb\u7edf\u6027\u6d4b\u8bc4LLM\u5728\u8d22\u52a1\u5ba1\u8ba1\u4e2d\u7684\u63a8\u7406\u6548\u80fd\u3002", "method": "\u6784\u5efa\u4e86FinAuditing\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e0e\u672c\u4f53\u7ed3\u6784\u5bf9\u9f50\u3001\u9762\u5411\u7ed3\u6784\u5316\u591a\u6587\u6863\u7684\u57fa\u51c6\uff0c\u4f7f\u7528\u771f\u5b9eUS-GAAP\u5408\u89c4\u7684XBRL\u6587\u4ef6\uff0c\u8bbe\u8ba1\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\uff08FinSM\uff09\u3001\u5173\u7cfb\u4e00\u81f4\u6027\uff08FinRE\uff09\u3001\u6570\u503c\u4e00\u81f4\u6027\uff08FinMR\uff09\u4e09\u5927\u5b50\u4efb\u52a1\uff0c\u4ee5\u53ca\u96c6\u6210\u4e86\u68c0\u7d22\u3001\u5206\u7c7b\u548c\u63a8\u7406\u7b49\u591a\u7ef4\u8bc4\u6d4b\u6846\u67b6\u3002", "result": "\u5728\u5bf913\u4e2a\u5148\u8fdbLLM\u8fdb\u884c\u96f6\u6837\u672c\u5b9e\u9a8c\u540e\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u8bed\u4e49\u3001\u5173\u7cfb\u548c\u6570\u5b66\u63a8\u7406\u7ef4\u5ea6\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u5728\u5904\u7406\u591a\u6587\u6863\u5c42\u7ea7\u7ed3\u6784\u63a8\u7406\u65f6\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60-90%\u3002", "conclusion": "\u73b0\u6709LLM\u5728\u57fa\u4e8e\u672c\u4f53\u548c\u7ed3\u6784\u7684\u8d22\u52a1\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\uff0cFinAuditing\u4e3a\u53d1\u5c55\u53ef\u4fe1\u3001\u7ed3\u6784\u611f\u77e5\u548c\u5408\u89c4\u7684\u667a\u80fd\u8d22\u52a1\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.08892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08892", "abs": "https://arxiv.org/abs/2510.08892", "authors": ["Haomin Zhuang", "Yujun Zhou", "Taicheng Guo", "Yue Huang", "Fangxu Liu", "Kai Song", "Xiangliang Zhang"], "title": "Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR", "comment": null, "summary": "Reinforcement Learning has demonstrated substantial improvements in the\nreasoning abilities of Large Language Models (LLMs), exhibiting significant\napplicability across various domains. Recent research has identified that\ntokens within LLMs play distinct roles during reasoning tasks, categorizing\nthem into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior\napproaches have typically focused on restricting updates to indirectly\nencourage exploration, yet they do not explicitly facilitate exploratory\nbehavior during the token generation stage itself. In this work, we introduce a\ncomplementary approach that explicitly promotes exploration during sampling by\napplying distinct temperature settings for different token types. Specifically,\nour method employs higher temperatures for reasoning tokens to actively\nencourage exploration, while retaining lower temperatures for knowledge tokens\nto maintain factual correctness. Furthermore, we systematically investigate\nvarious multi-temperature scheduling strategies and their impacts within\nreinforcement learning contexts. Empirical evaluations on several reasoning\nbenchmarks demonstrate that our approach significantly enhances the reasoning\nperformance of LLMs. The code is available at\nhttps://github.com/zhmzm/Multi_Temperature_Verl.git.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4f9d\u636etoken\u7c7b\u578b\u5206\u522b\u8c03\u6574\u91c7\u6837\u6e29\u5ea6\u7684\u591a\u6e29\u5ea6\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e0b\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u63a2\u7d22\u6027\u4e0e\u51c6\u786e\u6027\u7684\u517c\u987e\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u9650\u5236\u6a21\u578b\u7684\u66f4\u65b0\u95f4\u63a5\u4fc3\u8fdb\u63a2\u7d22\uff0c\u4f46\u5728\u5b9e\u9645token\u751f\u6210\u9636\u6bb5\u5e76\u672a\u660e\u786e\u4fc3\u8fdb\u6a21\u578b\u7684\u63a2\u7d22\u884c\u4e3a\u3002\u800c\u4e0d\u540ctoken\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u626e\u6f14\u7740\u4e0d\u540c\u89d2\u8272\uff08\u9ad8\u71b5\u63a8\u7406token/\u4f4e\u71b5\u77e5\u8bc6token\uff09\uff0c\u8fd9\u5e26\u6765\u4e86\u6e29\u5ea6\u8c03\u8282\u7684\u65b0\u673a\u4f1a\u3002", "method": "\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684token\u65bd\u52a0\u4e0d\u540c\u7684\u91c7\u6837\u6e29\u5ea6\uff1a\u63a8\u7406token\u91c7\u7528\u8f83\u9ad8\u6e29\u5ea6\u4fc3\u8fdb\u63a2\u7d22\uff0c\u77e5\u8bc6token\u5219\u4fdd\u6301\u8f83\u4f4e\u6e29\u5ea6\u4fdd\u8bc1\u4e8b\u5b9e\u6b63\u786e\u6027\u3002\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u591a\u6e29\u5ea6\u8c03\u5ea6\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u91c7\u7528\u591a\u6e29\u5ea6\u8c03\u5ea6\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u63a8\u7406\u548c\u77e5\u8bc6token\u5206\u522b\u8c03\u6574\u91c7\u6837\u6e29\u5ea6\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0b\u7684LLM\u63a8\u7406\u80fd\u529b\u83b7\u5f97\u6709\u6548\u63d0\u5347\u3002\u591a\u6e29\u5ea6\u7b56\u7565\u4e3a\u6a21\u578b\u63a2\u7d22\u4e0e\u4e8b\u5b9e\u6b63\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u4f18\u5e73\u8861\u3002"}}
{"id": "2510.08902", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08902", "abs": "https://arxiv.org/abs/2510.08902", "authors": ["Tengxiao Lv", "Ling Luo", "Juntao Li", "Yanhua Wang", "Yuchen Pan", "Chao Liu", "Yanan Wang", "Yan Jiang", "Huiyi Lv", "Yuanyuan Sun", "Jian Wang", "Hongfei Lin"], "title": "A Unified Biomedical Named Entity Recognition Framework with Large Language Models", "comment": "Accepted as a short paper at BIBM2025", "summary": "Accurate recognition of biomedical named entities is critical for medical\ninformation extraction and knowledge discovery. However, existing methods often\nstruggle with nested entities, entity boundary ambiguity, and cross-lingual\ngeneralization. In this paper, we propose a unified Biomedical Named Entity\nRecognition (BioNER) framework based on Large Language Models (LLMs). We first\nreformulate BioNER as a text generation task and design a symbolic tagging\nstrategy to jointly handle both flat and nested entities with explicit boundary\nannotation. To enhance multilingual and multi-task generalization, we perform\nbilingual joint fine-tuning across multiple Chinese and English datasets.\nAdditionally, we introduce a contrastive learning-based entity selector that\nfilters incorrect or spurious predictions by leveraging boundary-sensitive\npositive and negative samples. Experimental results on four benchmark datasets\nand two unseen corpora show that our method achieves state-of-the-art\nperformance and robust zero-shot generalization across languages. The source\ncodes are freely available at https://github.com/dreamer-tx/LLMNER.", "AI": {"tldr": "\u672c\u6587\u5c06\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u8f6c\u5316\u4e3a\u751f\u6210\u5f0f\u95ee\u9898\uff0c\u7ed3\u5408\u7b26\u53f7\u6807\u6ce8\u548c\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u5904\u7406\u5d4c\u5957\u53ca\u591a\u8bed\u8a00\u5b9e\u4f53\uff0c\u65b9\u6cd5\u5728\u4e2d\u82f1\u6587\u591a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6548\u679c\uff0c\u5e76\u5c55\u73b0\u5f3a\u5927\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08BioNER\uff09\u5bf9\u4e8e\u533b\u5b66\u4fe1\u606f\u63d0\u53d6\u548c\u77e5\u8bc6\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5d4c\u5957\u5b9e\u4f53\u3001\u5b9e\u4f53\u8fb9\u754c\u6a21\u7cca\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u5c06BioNER\u4efb\u52a1\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7b26\u53f7\u6807\u6ce8\u7b56\u7565\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5e73\u9762\u548c\u5d4c\u5957\u5b9e\u4f53\u5e76\u660e\u786e\u8fb9\u754c\u3002\u6b64\u5916\uff0c\u8fd0\u7528\u591a\u4efb\u52a1\u53cc\u8bed\u8054\u5408\u5fae\u8c03\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u4f53\u9009\u62e9\u5668\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u5e76\u8fc7\u6ee4\u9519\u8bef\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u672a\u89c1\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5404\u9879\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00BioNER\u6846\u67b6\u521b\u65b0\u6027\u5730\u89e3\u51b3\u4e86\u5d4c\u5957\u5b9e\u4f53\u3001\u8fb9\u754c\u6a21\u7cca\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u96be\u9898\uff0c\u5177\u6709\u53ef\u89c2\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.08907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08907", "abs": "https://arxiv.org/abs/2510.08907", "authors": ["Xin Liu", "RunSong Zhao", "PengCheng Huang", "XinYu Liu", "JunYi Xiao", "ChunYang Xiao", "Tong Xiao", "Shengxiang Gao", "Zhengtao Yu", "JingBo Zhu"], "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors", "comment": "18 pages,9 figures", "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.", "AI": {"tldr": "SAC\u662f\u4e00\u79cd\u5168\u65b0\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff0c\u4e0d\u9700\u81ea\u7f16\u7801\u4efb\u52a1\u8bad\u7ec3\uff0c\u901a\u8fc7\u5173\u952etoken\u9009\u53d6\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6548\u7387\u548c\u5b9e\u9645\u4e0b\u6e38\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u957f\u6587\u672c\u63a8\u7406\u65f6\u9762\u4e34\u901f\u5ea6\u74f6\u9888\uff0c\u5c3d\u7ba1\u5df2\u6709\u65b9\u6cd5\u5c1d\u8bd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u538b\u7f29\u6765\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u81ea\u7f16\u7801\u4efb\u52a1\uff08\u5982\u91cd\u6784\uff09\u6765\u8bad\u7ec3\u4e0e\u5177\u4f53\u4efb\u52a1\u65e0\u5173\u7684\u538b\u7f29token\uff0c\u8fd9\u5bfc\u81f4\u538b\u7f29\u6548\u679c\u4e0e\u5b9e\u9645\u4e0b\u6e38\u5e94\u7528\u4efb\u52a1\u5b58\u5728\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u951a\u5b9a\u538b\u7f29\uff08Semantic-Anchor Compression, SAC\uff09\u65b9\u6cd5\uff0c\u4e0d\u518d\u4f9d\u8d56\u81ea\u7f16\u7801\u8bad\u7ec3\u4efb\u52a1\uff0c\u800c\u662f\u901a\u8fc7\u4ece\u539f\u59cb\u4e0a\u4e0b\u6587\u4e2d\u76f4\u63a5\u9009\u62e9\u79f0\u4e3a\u201canchor tokens\u201d\u7684\u5173\u952etoken\uff0c\u5e76\u6574\u5408\u5176key-value\u8868\u793a\uff0c\u8fbe\u5230\u538b\u7f29\u8bed\u5883\u7684\u76ee\u7684\u3002SAC\u5305\u62ec\u4e24\u9879\u5173\u952e\u8bbe\u8ba1\uff1a(1) \u5229\u7528\u201canchor embedding\u201d\u5e2e\u52a9\u8bc6\u522b\u5173\u952e\u4fe1\u606ftoken\uff1b(2) \u53cc\u5411\u6ce8\u610f\u529b\u4fee\u6b63\u673a\u5236\uff0c\u4f7fanchor tokens\u80fd\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "SAC\u5728\u5404\u79cd\u538b\u7f29\u6bd4\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u3002\u5728MRQA\u7684\u5206\u5e03\u5916\u6d4b\u8bd5\u4e2d\uff0cSAC\u57285\u500d\u538b\u7f29\u6761\u4ef6\u4e0b\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u4e861\u4e2aEM\u5206\u6570\uff0c\u4e14\u968f\u7740\u538b\u7f29\u6bd4\u7684\u63d0\u5347\uff0c\u4f18\u52bf\u8fdb\u4e00\u6b65\u6269\u5927\u3002", "conclusion": "SAC\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u521b\u65b0\u7ed5\u5f00\u4e86\u81ea\u7f16\u7801\u8bad\u7ec3\u5e26\u6765\u7684\u504f\u5dee\uff0c\u80fd\u66f4\u6709\u6548\u5b9e\u73b0\u4e0a\u4e0b\u6587\u538b\u7f29\uff0c\u6709\u671b\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u6548\u7387\u4e14\u66f4\u597d\u5730\u670d\u52a1\u5b9e\u9645\u4efb\u52a1\u9700\u6c42\u3002\u5b9e\u9a8c\u7ed3\u679c\u5145\u5206\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.08915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08915", "abs": "https://arxiv.org/abs/2510.08915", "authors": ["Nicholas Deas", "Kathleen McKeown"], "title": "Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions", "comment": "EMNLP 2025 Camera Ready", "summary": "We introduce and study artificial impressions--patterns in LLMs' internal\nrepresentations of prompts that resemble human impressions and stereotypes\nbased on language. We fit linear probes on generated prompts to predict\nimpressions according to the two-dimensional Stereotype Content Model (SCM).\nUsing these probes, we study the relationship between impressions and\ndownstream model behavior as well as prompt features that may inform such\nimpressions. We find that LLMs inconsistently report impressions when prompted,\nbut also that impressions are more consistently linearly decodable from their\nhidden representations. Additionally, we show that artificial impressions of\nprompts are predictive of the quality and use of hedging in model responses. We\nalso investigate how particular content, stylistic, and dialectal features in\nprompts impact LLM impressions.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LLM\u5185\u90e8\u4f1a\u5f62\u6210\u7c7b\u4f3c\u4eba\u7c7b\u523b\u677f\u5370\u8c61\u7684\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u5370\u8c61\u53ef\u7528\u7ebf\u6027\u6a21\u578b\u89e3\u7801\uff0c\u5e76\u4e14\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u4e0e\u6a21\u7cca\u6027\u3002prompt\u7684\u4e0d\u540c\u7279\u5f81\u4e5f\u4f1a\u5bf9\u5370\u8c61\u5f62\u6210\u9020\u6210\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u6587\u672c\u8f93\u5165\u65f6\uff0c\u53ef\u80fd\u5728\u5185\u90e8\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u5370\u8c61\u548c\u523b\u677f\u5370\u8c61\u7684\u6a21\u5f0f\u3002\u7406\u89e3\u8fd9\u79cd\u201c\u4eba\u5de5\u5370\u8c61\u201d\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u548c\u8f93\u51fa\uff0c\u6709\u52a9\u4e8e\u89e3\u91ca\u6a21\u578b\u7684\u504f\u89c1\u548c\u8868\u73b0\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86\u201c\u4eba\u5de5\u5370\u8c61\u201d\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u4e8c\u5143\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\uff08SCM\uff09\u6765\u5efa\u6a21\u3002\u901a\u8fc7\u5728\u7ebf\u6027\u63a2\u9488\u4e0a\u62df\u5408\u7528\u751f\u6210\u7684prompt\u9884\u6d4b\u5185\u90e8\u5370\u8c61\u5206\u6570\uff0c\u5e76\u5206\u6790\u8be5\u5206\u6570\u4e0e\u6a21\u578b\u884c\u4e3a\u3001prompt\u7279\u5f81\u7684\u5173\u7cfb\u3002", "result": "1. LLM\u5728\u88ab\u76f4\u63a5\u95ee\u53ca\u65f6\u62a5\u544a\u5370\u8c61\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4f46\u8fd9\u4e9b\u5370\u8c61\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u5728\u6a21\u578b\u9690\u5c42\u8868\u793a\u4e2d\u8f83\u4e00\u81f4\u5730\u89e3\u7801\u51fa\u6765\u30022. prompt\u7684\u4eba\u5de5\u5370\u8c61\u80fd\u591f\u9884\u6d4b\u6a21\u578b\u56de\u5e94\u4e2d\u7684\u8d28\u91cf\u53ca\u6a21\u7cca\u6027\uff08hedging\uff09\u4f7f\u7528\u30023. prompt\u4e2d\u7684\u5185\u5bb9\u3001\u98ce\u683c\u548c\u65b9\u8a00\u7279\u5f81\u4f1a\u5f71\u54cdLLM\u7684\u4eba\u5de5\u5370\u8c61\u3002", "conclusion": "LLM\u4f1a\u5728\u5185\u90e8\u751f\u6210\u53ef\u6d4b\u91cf\u7684\u4eba\u5de5\u5370\u8c61\uff0c\u8fd9\u4e9b\u5370\u8c61\u53ef\u9884\u6d4b\u6a21\u578b\u7684\u8f93\u51fa\u7279\u5f81\u3002\u5373\u4f7f\u6a21\u578b\u8f93\u51fa\u4e0d\u76f4\u63a5\u53cd\u6620\u5370\u8c61\uff0c\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e5f\u5305\u542b\u5370\u8c61\u4fe1\u606f\u3002\u7406\u89e3\u4eba\u5de5\u5370\u8c61\u6709\u52a9\u4e8e\u63ed\u793a\u6a21\u578b\u504f\u89c1\u4ee5\u53ca\u5f71\u54cd\u5176\u4e0b\u6e38\u884c\u4e3a\u7684\u539f\u56e0\u3002"}}
{"id": "2510.08942", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08942", "abs": "https://arxiv.org/abs/2510.08942", "authors": ["Jiaming Wang", "Zhe Tang", "Yilin Jin", "Peng Ding", "Xiaoyu Li", "Xuezhi Cao"], "title": "SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures", "comment": null, "summary": "As large language models (LLMs) are widely deployed as domain-specific\nagents, many benchmarks have been proposed to evaluate their ability to follow\ninstructions and make decisions in real-world scenarios. However, business\nscenarios often involve complex standard operating procedures (SOPs), and the\nevaluation of LLM capabilities in such contexts has not been fully explored. To\nbridge this gap, we propose SOP-Maze, a benchmark constructed from real-world\nbusiness data and adapted into a collection of 397 tasks from 23 complex SOP\nscenarios. We further categorize SOP tasks into two broad classes: Lateral Root\nSystem (LRS), representing wide-option tasks that demand precise selection; and\nHeart Root System (HRS), which emphasizes deep logical reasoning with complex\nbranches. Extensive experiments reveal that nearly all state-of-the-art models\nstruggle with SOP-Maze. We conduct a comprehensive analysis and identify three\nkey error categories: (i) route blindness: difficulty following procedures;\n(ii) conversational fragility: inability to handle real dialogue nuances; and\n(iii) calculation errors: mistakes in time or arithmetic reasoning under\ncomplex contexts. The systematic study explores LLM performance across SOP\ntasks that challenge both breadth and depth, offering new insights for\nimproving model capabilities. We have open-sourced our work on\nhttps://github.com/ADoublLEN/SOP-Maze.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ee5\u5b9e\u9645\u5546\u4e1a\u6d41\u7a0b\u4e3a\u57fa\u7840\u7684SOP-Maze\u57fa\u51c6\uff0c\u6db5\u76d6397\u4e2a\u590d\u6742SOP\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4ef7\u4e86\u5f53\u524dLLM\u5728\u6d41\u7a0b\u8ddf\u968f\u3001\u771f\u5b9e\u5bf9\u8bdd\u548c\u590d\u6742\u8ba1\u7b97\u7b49\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u4e0e\u4f01\u4e1a\u5e94\u7528\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662f\u4f5c\u4e3a\u7279\u5b9a\u9886\u57df\u4ee3\u7406\uff0c\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u5546\u4e1a\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u800c\u5546\u4e1a\u573a\u666f\u901a\u5e38\u6d89\u53ca\u590d\u6742\u7684\u6807\u51c6\u64cd\u4f5c\u6d41\u7a0b\uff08SOP\uff09\uff0c\u4f46\u5f53\u524d\u5728\u6b64\u7c7b\u73af\u5883\u4e0b\u5bf9LLM\u80fd\u529b\u7684\u8bc4\u4f30\u5c1a\u4e0d\u5145\u5206\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86SOP-Maze\uff0c\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5546\u4e1a\u6570\u636e\u6784\u5efa\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d623\u4e2a\u590d\u6742SOP\u573a\u666f\u7684397\u9879\u4efb\u52a1\u3002\u8fdb\u4e00\u6b65\u5c06\u4efb\u52a1\u5206\u4e3a\u4e24\u4e2a\u5e7f\u7c7b\uff1aLRS\uff08\u4fa7\u5411\u6839\u7cfb\u7edf\uff0c\u8003\u5bdf\u7cbe\u786e\u9009\u9879\u9009\u62e9\uff09\u4e0eHRS\uff08\u4e2d\u5fc3\u6839\u7cfb\u7edf\uff0c\u91cd\u903b\u8f91\u5206\u652f\u63a8\u7406\uff09\uff0c\u5e76\u5728\u5404\u7c7b\u4e3b\u6d41LLM\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u6027\u80fd\u5206\u6790\u3002", "result": "\u51e0\u4e4e\u6240\u6709\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728SOP-Maze\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4e3b\u8981\u9519\u8bef\u5305\u62ec\uff1a\u6d41\u7a0b\u8ddf\u968f\u56f0\u96be\uff08route blindness\uff09\u3001\u771f\u5b9e\u5bf9\u8bdd\u8106\u5f31\uff08conversational fragility\uff09\u3001\u4ee5\u53ca\u590d\u6742\u60c5\u5883\u4e0b\u7684\u8ba1\u7b97\u9519\u8bef\u3002\u6b64\u5916\uff0c\u5bf9\u6a21\u578b\u5728\u5e7f\u5ea6\uff08LRS\uff09\u548c\u6df1\u5ea6\uff08HRS\uff09\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4f01\u4e1aSOP\u6d41\u7a0b\u4efb\u52a1\u4e2d\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u6d41\u7a0b\u7406\u89e3\u3001\u5bf9\u8bdd\u628a\u63a7\u4ee5\u53ca\u8ba1\u7b97\u63a8\u7406\u7b49\u73af\u8282\u3002SOP-Maze\u4e3a\u6a21\u578b\u80fd\u529b\u63d0\u5347\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5e73\u53f0\u3002"}}
{"id": "2510.08956", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08956", "abs": "https://arxiv.org/abs/2510.08956", "authors": ["Mobina Noori", "Mahasweta Chakraborti", "Amy X Zhang", "Seth Frey"], "title": "A Human Behavioral Baseline for Collective Governance in Software Projects", "comment": "Algorithmic Collective Action Workshop @ NeurIPS 2025. arXiv admin\n  note: text overlap with arXiv:2509.16295", "summary": "We study how open source communities describe participation and control\nthrough version controlled governance documents. Using a corpus of 710 projects\nwith paired snapshots, we parse text into actors, rules, actions, and objects,\nthen group them and measure change with entropy for evenness, richness for\ndiversity, and Jensen Shannon divergence for drift. Projects define more roles\nand more actions over time, and these are distributed more evenly, while the\ncomposition of rules remains stable. These findings indicate that governance\ngrows by expanding and balancing categories of participation without major\nshifts in prescriptive force. The analysis provides a reproducible baseline for\nevaluating whether future AI mediated workflows concentrate or redistribute\nauthority.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5f00\u6e90\u793e\u533a\u5982\u4f55\u901a\u8fc7\u7248\u672c\u63a7\u5236\u7684\u6cbb\u7406\u6587\u4ef6\u63cf\u8ff0\u53c2\u4e0e\u548c\u63a7\u5236\u673a\u5236\u3002\u7814\u7a76\u5bf9\u8c61\u4e3a710\u4e2a\u9879\u76ee\u7684\u6cbb\u7406\u6587\u4ef6\u5feb\u7167\uff0c\u81ea\u52a8\u89e3\u6790\u4e3a\u201c\u884c\u4e3a\u8005\u3001\u89c4\u5219\u3001\u884c\u52a8\u3001\u5bf9\u8c61\u201d\uff0c\u4ee5\u4fe1\u606f\u71b5\u7b49\u5b9a\u91cf\u65b9\u6cd5\u6d4b\u91cf\u7ed3\u6784\u4e0e\u53d8\u5316\u3002", "motivation": "\u5f53\u524d\u5bf9\u5f00\u6e90\u793e\u533a\u6cbb\u7406\u673a\u5236\u7684\u52a8\u6001\u63cf\u8ff0\u548c\u91cf\u5316\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u53c2\u4e0e\u548c\u63a7\u5236\u673a\u5236\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u6f14\u5316\u89c4\u5f8b\u5c1a\u4e0d\u6e05\u6670\u3002\u5e0c\u671b\u57fa\u4e8e\u7ed3\u6784\u5316\u7684\u6587\u672c\u548c\u91cf\u5316\u6307\u6807\uff0c\u4e3a\u672a\u6765AI\u4ecb\u5bfc\u7684\u5de5\u4f5c\u6d41\u5bf9\u6743\u529b\u5206\u914d\u7684\u5f71\u54cd\u63d0\u4f9b\u57fa\u7ebf\u3002", "method": "\u6536\u96c6710\u4e2a\u5f00\u6e90\u9879\u76ee\u7684\u6cbb\u7406\u6587\u4ef6\u5feb\u7167\uff0c\u4f7f\u7528\u6587\u672c\u89e3\u6790\u63d0\u53d6\u56db\u5927\u5143\u7d20\uff08\u884c\u4e3a\u8005\u3001\u89c4\u5219\u3001\u884c\u52a8\u3001\u5bf9\u8c61\uff09\uff0c\u7528\u71b5\u3001\u4e30\u5bcc\u5ea6\u3001Jensen-Shannon\u6563\u5ea6\u7b49\u6307\u6807\u5b9a\u91cf\u5206\u6790\u5176\u53d8\u5316\u3002", "result": "\u6cbb\u7406\u6587\u4ef6\u4e2d\uff0c\u89d2\u8272\u548c\u884c\u52a8\u7c7b\u522b\u968f\u65f6\u95f4\u589e\u591a\u4e14\u5206\u5e03\u66f4\u5747\u8861\uff0c\u4f46\u89c4\u5219\u672c\u8eab\u5e76\u672a\u53d1\u751f\u5267\u70c8\u53d8\u5316\u3002\u6574\u4f53\u4e0a\uff0c\u6cbb\u7406\u7684\u53d1\u5c55\u8868\u73b0\u4e3a\u53c2\u4e0e\u7c7b\u522b\u7684\u6269\u5c55\u4e0e\u5e73\u8861\uff0c\u4f46\u6307\u5bfc\u6027\u89c4\u5219\u672a\u89c1\u663e\u8457\u8f6c\u53d8\u3002", "conclusion": "\u5f00\u6e90\u9879\u76ee\u968f\u7740\u65f6\u95f4\u63a8\u79fb\uff0c\u4f1a\u5b9a\u4e49\u66f4\u591a\u7684\u89d2\u8272\u548c\u884c\u52a8\uff0c\u8fd9\u4e9b\u5206\u5e03\u66f4\u52a0\u5747\u8861\uff0c\u4f46\u89c4\u5219\u7684\u6574\u4f53\u6784\u6210\u4fdd\u6301\u7a33\u5b9a\u3002\u8fd9\u8868\u660e\u6cbb\u7406\u662f\u901a\u8fc7\u6269\u5c55\u548c\u5e73\u8861\u53c2\u4e0e\u7c7b\u522b\u6765\u53d1\u5c55\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u5f3a\u5236\u6027\u89c4\u5219\u7684\u53d8\u5316\u3002"}}
{"id": "2510.08986", "categories": ["cs.CL", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.08986", "abs": "https://arxiv.org/abs/2510.08986", "authors": ["Bolun Sun", "Charles Chang", "Yuen Yuen Ang", "Pingxu Hao", "Ruotong Mu", "Yuchen Xu", "Zhengxin Zhang"], "title": "Creation of the Chinese Adaptive Policy Communication Corpus", "comment": null, "summary": "We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central\nGovernment) Corpus, the first open dataset of Chinese policy directives\nannotated with a five-color taxonomy of clear and ambiguous language\ncategories, building on Ang's theory of adaptive policy communication. Spanning\n1949-2023, this corpus includes national laws, administrative regulations, and\nministerial rules issued by China's top authorities. Each document is segmented\ninto paragraphs, producing a total of 3.3 million units. Alongside the corpus,\nwe release comprehensive metadata, a two-round labeling framework, and a\ngold-standard annotation set developed by expert and trained coders.\nInter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive\nlabels, indicating high reliability for supervised modeling. We provide\nbaseline classification results with several large language models (LLMs),\ntogether with our annotation codebook, and describe patterns from the dataset.\nThis release aims to support downstream tasks and multilingual NLP research in\npolicy communication.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u6db5\u76d61949-2023\u5e74\u4e2d\u56fd\u4e2d\u592e\u653f\u5e9c\u653f\u7b56\u6587\u672c\u7684\u5f00\u653e\u8bed\u6599\u5e93\uff0c\u7ec6\u81f4\u6807\u6ce8\u8bed\u8a00\u6e05\u6670\u5ea6\uff0c\u6570\u636e\u53ca\u6807\u6ce8\u8d28\u91cf\u53ef\u9760\uff0c\u5e76\u4e3a\u653f\u7b56\u6c9f\u901a\u4e0e\u591a\u8bed\u79cdNLP\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u57fa\u7840\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e2d\u6587\u653f\u7b56\u6307\u4ee4\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5c24\u5176\u7f3a\u5c11\u5bf9\u653f\u7b56\u8bed\u8a00\u6e05\u6670\u5ea6\u548c\u6a21\u7cca\u6027\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86NLP\u5728\u653f\u7b56\u6c9f\u901a\u9886\u57df\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "method": "\u6784\u5efaCAPC-CG\u8bed\u6599\u5e93\uff0c\u6536\u96c61949-2023\u5e74\u4e2d\u56fd\u4e2d\u592e\u653f\u5e9c\u53d1\u5e03\u7684\u6cd5\u5f8b\u3001\u6cd5\u89c4\u3001\u884c\u653f\u89c4\u7ae0\u3002\u91c7\u7528\u4e94\u8272\u5206\u7c7b\u4f53\u7cfb\u5bf9\u6587\u672c\u8fdb\u884c\u6e05\u6670\u548c\u6a21\u7cca\u8bed\u8a00\u6807\u6ce8\uff0c\u5206\u6bb5\u5904\u74063.3\u767e\u4e07\u6761\u6570\u636e\u3002\u901a\u8fc7\u4e24\u8f6e\u6807\u6ce8\u6d41\u7a0b\uff0c\u7531\u4e13\u5bb6\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u6807\u6ce8\u8005\u5efa\u7acb\u9ad8\u8d28\u91cf\u91d1\u6807\u51c6\u6807\u6ce8\u96c6\u3002\u5e94\u7528\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u5206\u7c7b\u8bd5\u9a8c\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u4e2d\u56fd\u653f\u7b56\u6c9f\u901a\u8bed\u6599\u5e93\uff0c\u542b\u7cfb\u7edf\u5143\u6570\u636e\u548c\u9ad8\u4e00\u81f4\u6027\u6807\u6ce8\u96c6\uff08Fleiss's kappa=0.86\uff09\u3002\u516c\u5f00\u4e86\u6807\u6ce8\u624b\u518c\u3001\u57fa\u7ebfLLM\u5206\u7c7b\u7ed3\u679c\u4e0e\u6570\u636e\u5206\u6790\u3002", "conclusion": "CAPC-CG\u8bed\u6599\u5e93\u4e3a\u653f\u7b56\u6c9f\u901a\u53ca\u591a\u8bed\u8a00NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u52a9\u529b\u76f8\u5173\u4e0b\u6e38\u4efb\u52a1\u4e0e\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2510.09001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09001", "abs": "https://arxiv.org/abs/2510.09001", "authors": ["Jingyu Zhou", "Lu Ma", "Hao Liang", "Chengyu Shen", "Bin Cui", "Wentao Zhang"], "title": "DARO: Difficulty-Aware Reweighting Policy Optimization", "comment": null, "summary": "Recent advances in large language models (LLMs) have shown that reasoning\nability can be significantly enhanced through Reinforcement Learning with\nVerifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has\nemerged as the de facto approach for RLVR, inspiring numerous variants.\nHowever, our mathematical analysis reveals that these methods are fundamentally\nweighted variations of GRPO. We provide a unified view, demonstrating that\ntheir reliance on static or overly simplistic weighting schemes tied to sample\ndifficulty prevents adaptation to a model's evolving capabilities. This creates\na significant loss scale issue, where training disproportionately focuses on\ncertain difficulty levels at the expense of others, hindering overall\nperformance. To address these limitations, we introduce\n\\textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that\ndynamically adjusts the loss contribution of each difficulty group based on the\nmodel's learning state. Extensive experiments on Qwen2.5-Math-1.5B,\nQwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading\nbaselines across six math benchmarks, achieving significantly faster\nconvergence and superior final performance.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u6709RLVR\u65b9\u6cd5\u7684\u7f3a\u9677\u5e76\u63d0\u51faDARO\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u635f\u5931\u6743\u91cd\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u5927\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u589e\u5f3a\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u5176\u4e2dGRPO\u7b97\u6cd5\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u7c7b\u65b9\u6cd5\u5728\u6743\u91cd\u5206\u914d\u4e0a\u8fc7\u4e8e\u9759\u6001\u4e14\u7b80\u5355\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u80fd\u529b\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u96be\u5ea6\u611f\u77e5\u7684\u91cd\u52a0\u6743\u7b56\u7565\u4f18\u5316\uff08DARO\uff09\u65b9\u6cd5\uff0c\u6839\u636e\u6a21\u578b\u5b66\u4e60\u72b6\u6001\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u96be\u5ea6\u7ec4\u522b\u7684\u635f\u5931\u6743\u91cd\u3002", "result": "\u5728Qwen2.5-Math-1.5B\u3001Qwen2.5-Math-7B\u548cLlama3.1-8B\u7b49\u6a21\u578b\u4e0a\uff0cDARO\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e\u56db\u4e2a\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u6536\u655b\u548c\u66f4\u9ad8\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u73b0\u6709\u505a\u6cd5\u5728\u6837\u672c\u96be\u5ea6\u5206\u7ec4\u548c\u635f\u5931\u6743\u91cd\u4e0a\u5b58\u5728\u56fa\u6709\u5f31\u70b9\uff0cDARO\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.09004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09004", "abs": "https://arxiv.org/abs/2510.09004", "authors": ["Yutao Mou", "Xiaoling Zhou", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models", "comment": "Work in Progress", "summary": "Safety alignment is essential for building trustworthy artificial\nintelligence, yet it remains challenging to enhance model safety without\ndegrading general performance. Current approaches require computationally\nexpensive searches for the optimal proportion of safety-critical and\ngeneral-purpose data to balance safety and general performance, incurring high\ncosts with limited gains. In this work, we show that LoRA-based\nRefusal-training enables performance-preserving safety alignment even when\ntrained solely on safety data, demonstrating that LoRA serves as\ncost-efficient, performance-preserving, and plug-and-play safety patches.\nBeyond empirical findings, we provide both theoretical and experimental\nevidence that LoRA effectively decouples safety into a low-rank subspace\nlargely orthogonal to the model's intrinsic transformation space, ensuring that\nsafety enhancements do not interfere with inherent capabilities.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86LoRA\u62d2\u7edd\u8bad\u7ec3\u53ea\u9700\u5b89\u5168\u6027\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u6027\u80fd\u65e0\u635f\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u6210\u672c\u4f4e\u7684AI\u5b89\u5168\u8865\u4e01\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u6784\u5efa\u503c\u5f97\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u65f6\uff0c\u5b89\u5168\u6027\u5bf9\u9f50\u975e\u5e38\u5173\u952e\uff0c\u4f46\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u5982\u4f55\u907f\u514d\u5f71\u54cd\u901a\u7528\u6027\u80fd\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u6765\u5bfb\u627e\u5b89\u5168\u5173\u952e\u6570\u636e\u548c\u901a\u7528\u6570\u636e\u7684\u6700\u4f73\u914d\u6bd4\uff0c\u6210\u672c\u9ad8\u4e14\u6536\u76ca\u6709\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u57fa\u4e8eLoRA\uff08Low-Rank Adaptation\uff09\u7684\u62d2\u7edd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u5728\u5b89\u5168\u6027\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u5b89\u5168\u6027\u8865\u4e01\u3002\u4f5c\u8005\u8fd8\u4ece\u7406\u8bba\u548c\u5b9e\u9a8c\u89d2\u5ea6\u5206\u6790LoRA\u5c06\u5b89\u5168\u6027\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5185\u4e0e\u6a21\u578b\u672c\u4f53\u80fd\u529b\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u7ed3\u679c\u5747\u8868\u660e\uff0cLoRA\u80fd\u591f\u9ad8\u6548\u4ee5\u8865\u4e01\u7684\u65b9\u5f0f\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u539f\u6709\u901a\u7528\u80fd\u529b\uff0c\u51e0\u4e4e\u4e0d\u4f1a\u4ea7\u751f\u6027\u80fd\u635f\u5931\u3002", "conclusion": "LoRA\u62d2\u7edd\u8bad\u7ec3\u53ef\u4f5c\u4e3a\u4f4e\u6210\u672c\u3001\u65e0\u6027\u80fd\u635f\u5931\u4e14\u5373\u63d2\u5373\u7528\u7684AI\u5b89\u5168\u6027\u589e\u5f3a\u63aa\u65bd\uff0c\u5728\u5b89\u5168\u5bf9\u9f50\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.09014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09014", "abs": "https://arxiv.org/abs/2510.09014", "authors": ["Shengmin Piao", "Jieun Lee", "Sanghyun Park"], "title": "LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction", "comment": null, "summary": "The Text-to-SQL task translates natural language questions into SQL queries,\nenabling intuitive database interaction for non-experts. While recent methods\nleveraging Large Language Models (LLMs) achieve strong performance, their\nreliance on proprietary models raise concerns about deployment feasibility and\ndata privacy. In this work, we introduce LitE-SQL, a Lightweight and Efficient\nframework with two components: (i) a Schema Retriever that performs efficient\nschema linking using a vector database of pre-computed schema embeddings, and\n(ii) a SQL Generator fine-tuned in two stages-supervised fine-tuning followed\nby execution-guided reinforcement-enabling self-correction without costly\nmulti-candidate generation. On BIRD, LitE-SQL achieves 72.10% execution\naccuracy, and on Spider 1.0 it reaches 88.45%, demonstrating comparable or\nsuperior performance to LLM-based methods despite using 2x to 30x fewer\nparameters. Our findings demonstrate that high-quality Text-to-SQL generation\nis feasible with lightweight models, offering a practical solution for\nprivacy-sensitive and resource-constrained settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u9ad8\u6548\u7684Text-to-SQL\u65b9\u6cd5LitE-SQL\uff0c\u901a\u8fc7\u4f18\u5316schema\u68c0\u7d22\u548cSQL\u751f\u6210\u6d41\u7a0b\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u6781\u5927\u964d\u4f4e\u6a21\u578b\u89c4\u6a21\uff0c\uff0c\u975e\u5e38\u9002\u5408\u9690\u79c1\u654f\u611f\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u7684Text-to-SQL\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u867d\u7136\u6548\u679c\u5f88\u597d\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u662f\u4e13\u6709\u7684\uff0c\u5b58\u5728\u90e8\u7f72\u96be\u548c\u6570\u636e\u9690\u79c1\u9690\u5fe7\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u3001\u9ad8\u6548\u53c8\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faLitE-SQL\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\uff081\uff09Schema Retriever\uff1a\u5229\u7528\u9884\u8ba1\u7b97\u7684\u6a21\u5f0f\u5d4c\u5165\u5411\u91cf\u6570\u636e\u5e93\uff0c\u8fdb\u884c\u9ad8\u6548\u7684schema linking\uff1b\uff082\uff09SQL Generator\uff1a\u4e24\u9636\u6bb5\u5fae\u8c03\uff08\u6709\u76d1\u7763\u5fae\u8c03+\u6267\u884c\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5b9e\u73b0\u4e0d\u9700\u591a\u5019\u9009\u751f\u6210\u7684\u81ea\u6211\u7ea0\u9519\u3002", "result": "\u5728BIRD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8672.10%\u7684\u6267\u884c\u51c6\u786e\u7387\uff1b\u5728Spider 1.0\u6570\u636e\u96c6\u4e0a\u8fbe\u523088.45%\u3002\u53c2\u6570\u91cf\u6bd4LLM\u65b9\u6cd5\u5c112\u523030\u500d\uff0c\u4f46\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08LitE-SQL\uff09\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684Text-to-SQL\u751f\u6210\uff0c\u975e\u5e38\u9002\u5408\u6ce8\u91cd\u9690\u79c1\u548c\u8d44\u6e90\u6709\u9650\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.09030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09030", "abs": "https://arxiv.org/abs/2510.09030", "authors": ["Keno Harada", "Lui Yoshida", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Automated Refinement of Essay Scoring Rubrics for Language Models via Reflect-and-Revise", "comment": null, "summary": "The performance of Large Language Models (LLMs) is highly sensitive to the\nprompts they are given. Drawing inspiration from the field of prompt\noptimization, this study investigates the potential for enhancing Automated\nEssay Scoring (AES) by refining the scoring rubrics used by LLMs. Specifically,\nour approach prompts models to iteratively refine rubrics by reflecting on\nmodels' own scoring rationales and observed discrepancies with human scores on\nsample essays. Experiments on the TOEFL11 and ASAP datasets using GPT-4.1,\nGemini-2.5-Pro, and Qwen-3-Next-80B-A3B-Instruct show Quadratic Weighted Kappa\n(QWK) improvements of up to 0.19 and 0.47, respectively. Notably, even with a\nsimple initial rubric, our approach achieves comparable or better QWK than\nusing detailed human-authored rubrics. Our findings highlight the importance of\niterative rubric refinement in LLM-based AES to enhance alignment with human\nevaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u53cd\u601d\u5e76\u8fed\u4ee3\u4f18\u5316\u8bc4\u5206\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u5728\u4e3b\u6d41\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u5206\u4e00\u81f4\u6027\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4eba\u5de5\u8be6\u7ec6\u8bc4\u5206\u89c4\u5219\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u53d7\u5230\u8bc4\u5206\u63d0\u793a\uff08rubric\uff09\u8d28\u91cf\u7684\u5f3a\u70c8\u5f71\u54cd\u3002\u73b0\u6709\u7684\u4eba\u5de5\u8bc4\u5206\u89c4\u5219\u5f80\u5f80\u96be\u4ee5\u4e0eLLM\u7684\u63a8\u7406\u65b9\u5f0f\u7d27\u5bc6\u7ed3\u5408\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8bc4\u5206\u4e0e\u4eba\u5de5\u8bc4\u5206\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u4e9f\u9700\u63a2\u7d22\u66f4\u6709\u6548\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347LLM\u81ea\u52a8\u8bc4\u5206\u7684\u51c6\u786e\u6027\u548c\u4e0e\u4eba\u4e3a\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u81ea\u6211\u53cd\u601d\u548c\u8bc4\u5206\u7ed3\u679c\u53cd\u9988\u7684\u8bc4\u5206\u63d0\u793a\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u3002\u5177\u4f53\u505a\u6cd5\u662f\u7ed3\u5408\u6a21\u578b\u5bf9\u5176\u8bc4\u5206\u7406\u7531\u7684\u53cd\u601d\u4e0e\u4e0e\u4eba\u5de5\u8bc4\u5206\u5206\u6b67\u7684\u6837\u672c\u53cd\u9988\uff0c\u5f15\u5bfc\u6a21\u578b\u81ea\u52a8\u8fed\u4ee3\u548c\u4f18\u5316\u8bc4\u5206\u63d0\u793a\u3002\u8be5\u65b9\u6cd5\u5728TOEFL11\u548cASAP\u4e24\u4e2a\u516c\u5f00\u4f5c\u6587\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u5229\u7528GPT-4.1\u3001Gemini-2.5-Pro\u548cQwen-3-Next-80B-A3B-Instruct\u4e09\u79cd\u4e3b\u6d41LLM\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728TOEFL11\u548cASAP\u6570\u636e\u96c6\u4e0a\uff0c\u4e0a\u8ff0\u65b9\u6cd5\u5206\u522b\u53d6\u5f97\u4e86\u6700\u9ad80.19\u548c0.47\u7684Quadratic Weighted Kappa\uff08QWK\uff09\u63d0\u5347\u3002\u5373\u4f7f\u521d\u59cb\u8bc4\u5206\u63d0\u793a\u8f83\u4e3a\u7b80\u5355\uff0c\u7ecf\u5386\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u540e\uff0c\u81ea\u52a8\u751f\u6210\u7684\u8bc4\u5206\u63d0\u793a\u80fd\u591f\u8fbe\u5230\u751a\u81f3\u4f18\u4e8e\u4eba\u5de5\u7f16\u5199\u8be6\u5c3d\u8bc4\u5206\u89c4\u5219\u7684\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8bc4\u5206\u63d0\u793a\uff08rubric\uff09\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u5728\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u4e0e\u4eba\u5de5\u8bc4\u5206\u7ed3\u679c\u66f4\u4e3a\u63a5\u8fd1\uff0c\u540c\u65f6\u7cfb\u7edf\u5177\u6709\u5feb\u901f\u81ea\u9002\u5e94\u548c\u63d0\u793a\u81ea\u6211\u4f18\u5316\u7684\u80fd\u529b\u3002\u5f3a\u8c03\u4e86\u63d0\u793a\u8fed\u4ee3\u5728LLM\u81ea\u52a8\u8bc4\u5206\u9886\u57df\u7684\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.09032", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09032", "abs": "https://arxiv.org/abs/2510.09032", "authors": ["Adity Khisa", "Nusrat Jahan Lia", "Tasnim Mahfuz Nafis", "Zarif Masud", "Tanzir Pial", "Shebuti Rayana", "Ahmedul Kabir"], "title": "Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language", "comment": null, "summary": "As an Indo-Aryan language with limited available data, Chakma remains largely\nunderrepresented in language models. In this work, we introduce a novel corpus\nof contextually coherent Bangla-transliterated Chakma, curated from Chakma\nliterature, and validated by native speakers. Using this dataset, we fine-tune\nsix encoder-based multilingual and regional transformer models (mBERT,\nXLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on masked\nlanguage modeling (MLM) tasks. Our experiments show that fine-tuned\nmultilingual models outperform their pre-trained counterparts when adapted to\nBangla-transliterated Chakma, achieving up to 73.54% token accuracy and a\nperplexity as low as 2.90. Our analysis further highlights the impact of data\nquality on model performance and shows the limitations of OCR pipelines for\nmorphologically rich Indic scripts. Our research demonstrates that\nBangla-transliterated Chakma can be very effective for transfer learning for\nChakma language, and we release our manually validated monolingual dataset to\nencourage further research on multilingual language modeling for low-resource\nlanguages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7ecf\u8fc7\u6bcd\u8bed\u8005\u9a8c\u8bc1\u7684\u5b5f\u52a0\u62c9\u6587\u8f6c\u5199Chakma\u8bed\u6599\u5e93\uff0c\u5e76\u5bf9\u4e3b\u6d41\u591a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5efa\u6a21\u6548\u679c\uff0c\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u3002", "motivation": "Chakma\u8bed\u662f\u4e00\u79cd\u6570\u636e\u7a00\u7f3a\u7684\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\uff0c\u5728\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u4e2d\u88ab\u4e25\u91cd\u5ffd\u89c6\u3002\u4e3a\u63d0\u5347\u8be5\u8bed\u8a00\u7684\u5efa\u6a21\u6548\u679c\uff0c\u9700\u8981\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8bed\u6599\uff0c\u5e76\u63a2\u7d22\u4e3b\u6d41\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u9002\u5e94\u6027\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u6bcd\u8bed\u8005\u9a8c\u8bc1\u7684\u3001\u5177\u8bed\u5883\u5173\u8054\u6027\u7684\u5b5f\u52a0\u62c9\u6587\u8f6c\u5199Chakma\u6587\u672c\u8bed\u6599\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bf9\u516d\u79cd\u4e3b\u6d41\u7f16\u7801\u5668\u578b\u591a\u8bed\u8a00\u53ca\u5730\u533a\u6027Transformer\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5177\u4f53\u4efb\u52a1\u4e3a\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\uff08MLM\uff09\u3002", "result": "\u5fae\u8c03\u540e\u7684\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u76f8\u8f83\u4e8e\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5b5f\u52a0\u62c9\u6587\u8f6c\u5199Chakma\u4e0a\u7684\u8868\u73b0\u66f4\u4f73\uff0ctoken\u51c6\u786e\u7387\u6700\u9ad8\u8fbe73.54%\uff0c\u56f0\u60d1\u5ea6\u6700\u4f4e\u4e3a2.90\u3002\u6b64\u5916\uff0c\u5206\u6790\u53d1\u73b0\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u8868\u73b0\u5f71\u54cd\u663e\u8457\uff0c\u540c\u65f6OCR\u6d41\u7a0b\u5728\u5177\u5f62\u6001\u4e30\u5bcc\u7684\u5370\u5730\u8bed\u7cfb\u6587\u672c\u5904\u7406\u4e2d\u4ecd\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u5b5f\u52a0\u62c9\u6587\u8f6c\u5199Chakma\u662fChakma\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u9014\u5f84\uff0c\u624b\u52a8\u6821\u9a8c\u7684\u5355\u8bed\u6570\u636e\u96c6\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\u3002\u7814\u7a76\u4fc3\u8fdb\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u5904\u7406\uff0c\u5e76\u9f13\u52b1\u76f8\u5173\u9886\u57df\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2510.09033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09033", "abs": "https://arxiv.org/abs/2510.09033", "authors": ["Chi Seng Cheang", "Hou Pong Chan", "Wenxuan Zhang", "Yang Deng"], "title": "Large Language Models Do NOT Really Know What They Don't Know", "comment": null, "summary": "Recent work suggests that large language models (LLMs) encode factuality\nsignals in their internal representations, such as hidden states, attention\nweights, or token probabilities, implying that LLMs may \"know what they don't\nknow\". However, LLMs can also produce factual errors by relying on shortcuts or\nspurious associations. These error are driven by the same training objective\nthat encourage correct predictions, raising the question of whether internal\ncomputations can reliably distinguish between factual and hallucinated outputs.\nIn this work, we conduct a mechanistic analysis of how LLMs internally process\nfactual queries by comparing two types of hallucinations based on their\nreliance on subject information. We find that when hallucinations are\nassociated with subject knowledge, LLMs employ the same internal recall process\nas for correct responses, leading to overlapping and indistinguishable\nhidden-state geometries. In contrast, hallucinations detached from subject\nknowledge produce distinct, clustered representations that make them\ndetectable. These findings reveal a fundamental limitation: LLMs do not encode\ntruthfulness in their internal states but only patterns of knowledge recall,\ndemonstrating that \"LLMs don't really know what they don't know\".", "AI": {"tldr": "LLMs\u65e0\u6cd5\u53ef\u9760\u5730\u533a\u5206\u4e8b\u5b9e\u4e0e\u5e7b\u89c9\u8f93\u51fa\uff0c\u5c24\u5176\u662f\u5728\u5e7b\u89c9\u4e0e\u77e5\u8bc6\u76f8\u5173\u65f6\uff0c\u5176\u5185\u90e8\u8868\u5f81\u4e0e\u6b63\u786e\u54cd\u5e94\u6781\u4e3a\u76f8\u4f3c\uff0c\u5bfc\u81f4\u96be\u4ee5\u68c0\u6d4b\u3002\u8fd9\u63ed\u793a\u4e86LLMs\u5728\u4e8b\u5b9e\u6027\u5224\u65ad\u4e0a\u7684\u6839\u672c\u5c40\u9650\u3002", "motivation": "\u6b64\u524d\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5185\u90e8\u8868\u5f81\uff08\u5982\u9690\u85cf\u72b6\u6001\u3001\u6ce8\u610f\u529b\u6743\u91cd\u6216token\u6982\u7387\uff09\u4f3c\u4e4e\u5305\u542b\u4e8b\u5b9e\u6027\u4fe1\u53f7\uff0c\u6697\u793a\u5b83\u4eec\u53ef\u80fd\u201c\u77e5\u9053\u81ea\u5df1\u4e0d\u77e5\u9053\u7684\u4e1c\u897f\u201d\u3002\u4f46LLMs\u5728\u9884\u6d4b\u65f6\u4e5f\u4f1a\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\uff0c\u5f15\u53d1\u4e86\u5185\u90e8\u8ba1\u7b97\u80fd\u5426\u53ef\u9760\u5730\u533a\u5206\u771f\u5b9e\u4e0e\u5e7b\u89c9\u8f93\u51fa\u7684\u7591\u95ee\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5bf9LLMs\u5904\u7406\u4e8b\u5b9e\u67e5\u8be2\u65f6\u7684\u673a\u5236\u5206\u6790\uff0c\u5bf9\u6bd4\u4e86\u4e24\u7c7b\u57fa\u4e8e\u4e3b\u4f53\u4fe1\u606f\u7684\u5e7b\u89c9\uff0c\u76d1\u6d4b\u5176\u5185\u90e8\u8868\u5f81\uff08\u9690\u85cf\u72b6\u6001\u7684\u51e0\u4f55\u8868\u73b0\uff09\u3002", "result": "\u5982\u679c\u5e7b\u89c9\u4e0e\u4e3b\u4f53\u77e5\u8bc6\u76f8\u5173\uff0cLLMs\u4f1a\u4f7f\u7528\u4e0e\u6b63\u786e\u56de\u7b54\u76f8\u540c\u7684\u5185\u90e8\u56de\u5fc6\u673a\u5236\uff0c\u5bfc\u81f4\u9690\u85cf\u72b6\u6001\u8868\u5f81\u91cd\u53e0\u3001\u96be\u4ee5\u533a\u5206\u771f\u5047\uff1b\u800c\u4e0e\u4e3b\u4f53\u77e5\u8bc6\u65e0\u5173\u7684\u5e7b\u89c9\u5219\u4ea7\u751f\u53ef\u5206\u8fa8\u7684\u805a\u7c7b\u8868\u5f81\u3002", "conclusion": "LLMs\u7684\u5185\u90e8\u72b6\u6001\u5e76\u4e0d\u7f16\u7801\u201c\u771f\u5b9e\u5ea6\u201d\uff0c\u800c\u4ec5\u53cd\u6620\u77e5\u8bc6\u56de\u5fc6\u7684\u6a21\u5f0f\uff0c\u56e0\u6b64\u5b83\u4eec\u5b9e\u9645\u5e76\u4e0d\u201c\u77e5\u9053\u81ea\u5df1\u4e0d\u77e5\u9053\u7684\u4e1c\u897f\u201d\u3002"}}
{"id": "2510.09051", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.09051", "abs": "https://arxiv.org/abs/2510.09051", "authors": ["Muhammad Ali Shafique", "Kanwal Mehreen", "Muhammad Arham", "Maaz Amjad", "Sabur Butt", "Hamza Farooq"], "title": "Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation", "comment": "Accepted to the EMNLP 2025 Workshop on Multilingual Representation\n  Learning (MRL)", "summary": "Developing a high-performing large language models (LLMs) for low-resource\nlanguages such as Urdu, present several challenges. These challenges include\nthe scarcity of high-quality datasets, multilingual inconsistencies, and safety\nconcerns. Existing multilingual LLMs often address these issues by translating\nlarge volumes of available data. However, such translations often lack quality\nand cultural nuance while also incurring significant costs for data curation\nand training. To address these issues, we propose Alif-1.0-8B-Instruct, a\nmultilingual Urdu-English model, that tackles these challenges with a unique\napproach. We train the model on a high-quality, multilingual synthetic dataset\n(Urdu-Instruct), developed using a modified self-instruct technique. By using\nunique prompts and seed values for each task along with a global task pool,\nthis dataset incorporates Urdu-native chain-of-thought based reasoning,\nbilingual translation, cultural relevance, and ethical safety alignments. This\ntechnique significantly enhances the comprehension of Alif-1.0-8B-Instruct\nmodel for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon\nthe pretrained Llama-3.1-8B, demonstrates superior performance compared to\nLlama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading\nmultilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct,\nand Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our\nresults demonstrate that high-performance and low-resource language LLMs can be\ndeveloped efficiently and culturally aligned using our modified self-instruct\napproach. All datasets, models, and code are publicly available at:\nhttps://github.com/traversaal-ai/alif-urdu-llm.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7684self-instruct\u65b9\u6cd5\u6784\u5efa\u548c\u8bad\u7ec3Alif-1.0-8B-Instruct\uff0c\u8be5\u6a21\u578b\u5728\u4e4c\u5c14\u90fd\u8bed\u4efb\u52a1\u4e0a\u8868\u73b0\u8d85\u8d8a\u4e3b\u6d41\u591a\u8bed\u8a00LLM\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u6587\u5316\u5bf9\u9f50\u521b\u65b0\u3002", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4c\u5c14\u90fd\u8bed\u5f00\u53d1\u9ad8\u6027\u80fd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u3001\u591a\u8bed\u8a00\u4e0d\u4e00\u81f4\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002\u73b0\u6709\u591a\u8bed\u8a00LLM\u901a\u8fc7\u7ffb\u8bd1\u5927\u91cf\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7ffb\u8bd1\u8d28\u91cf\u548c\u6587\u5316\u7ec6\u8282\u5f80\u5f80\u4e0d\u8db3\uff0c\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86Alif-1.0-8B-Instruct\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e4c\u5c14\u90fd\u8bed\u548c\u82f1\u8bed\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002\u5176\u8bad\u7ec3\u4f7f\u7528\u4e86\u7ecf\u8fc7\u6539\u8fdb\u7684self-instruct\u65b9\u6cd5\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u5408\u6210\u6570\u636e\u96c6\uff08Urdu-Instruct\uff09\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u7684\u72ec\u7279\u63d0\u793a\u4e0e\u79cd\u5b50\u3001\u5168\u7403\u4efb\u52a1\u6c60\uff0c\u5b9e\u73b0\u4e4c\u5c14\u90fd\u8bed\u539f\u751f\u63a8\u7406\u3001\u53cc\u8bed\u7ffb\u8bd1\u3001\u6587\u5316\u76f8\u5173\u6027\u548c\u4f26\u7406\u5b89\u5168\u5bf9\u9f50\u3002", "result": "Alif-1.0-8B-Instruct\u5728\u4e4c\u5c14\u90fd\u8bed\u76f8\u5173\u4efb\u52a1\u4e0a\u6027\u80fd\u4f18\u4e8eLlama-3.1-8B-Instruct\uff0c\u5e76\u8d85\u8fc7Mistral-7B-Instruct-v0.3\u3001Qwen-2.5-7B-Instruct\u548cCohere-Aya-Expanse-8B\u7b49\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u4e14\u8bad\u7ec3\u9884\u7b97\u4f4e\u4e8e100\u7f8e\u5143\u3002", "conclusion": "\u53ef\u4ee5\u9ad8\u6548\u4e14\u6587\u5316\u5bf9\u9f50\u5730\u5f00\u53d1\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9ad8\u6027\u80fdLLM\uff0c\u6240\u7528\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5747\u5df2\u516c\u5f00\u3002"}}
