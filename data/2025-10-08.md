<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 92]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: 文章提出了强化学习新方法解决软件测试资源动态分配问题，通过Q学习和混合奖励机制，实现了高效稳健的配置分配。实验证明本方法性能优于传统方法，展现出广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统环境高度异构且不断演变，故障概率随时间变化，对预生产测试提出极高需求。穷举所有测试配置不可行，因此如何高效分配有限测试资源成为关键难题。现有方法多为静态且不适应概率漂移的动态环境。

Method: 提出一种基于强化学习（RL）的配置分配框架，首次将Q学习与融合模拟结果和实时反馈的混合奖励机制结合，并设计了自适应线上-线下训练方案，使智能体可快速跟踪概率突变并保持长期稳定。

Result: 大量仿真实验表明，新方法在配置资源分配任务上持续优于静态和组合优化基线，性能接近最优（oracle）水平。

Conclusion: 强化学习为适应性配置分配问题提供了强有力的新范式，超越传统方法，适用于动态测试和资源调度等广泛领域。

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [2] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard通过离线验证+在线监控架构，为LLM代理在敏感领域的运行提供正式安全保证，显著增强了其可信性与安全性。


<details>
  <summary>Details</summary>
Motivation: 在如医疗等敏感领域部署自主AI代理面临安全、隐私和操控风险，现有系统缺乏对代理行为进行正式安全保证的有效机制。

Method: 提出VeriGuard框架，采用两阶段架构。第一阶段离线进行：明确定义用户意图，提取安全规范，对行为策略进行综合、测试及形式化验证，确保规范合规。迭代优化至完全正确。第二阶段为在线动作监控：实时验证代理操作是否一致于已验证策略、在执行前及时拦截风险行为。

Result: 通过离线全面验证与在线轻量化监控相结合，实现了对LLM代理的形式安全保证，大幅提升其鲁棒性和可信度。

Conclusion: VeriGuard有效弥补了现有AI系统在敏感应用中的安全缺口，能够为LLM代理的行为提供可验证、可执行的安全保障。

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [3] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 论文提出了基于布鲁姆认知层次对大语言模型（LLM）测试用例生成推理能力的系统评价方法。实验表明，模型在记忆和理解方面表现尚可，但在复杂变换下易失效。分析发现技术型输入更重要，少样本提示显著提升效果，提供了提升模型泛化能力和实际应用的新方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统化评价LLMs（大型语言模型）在自动化软件测试中，尤其是在处理自然语言bug报告和生成测试用例时的推理能力。现有研究多集中于模型的记忆能力，缺乏从认知与推理多层面考察其泛化能力的全面分析。

Method: 基于Bloom认知层次结构（记忆、理解、应用、分析、评价、创造），结合LIBRO框架，对StarCoder和GPT-4o在Defects4J、GHRB以及经过语言和语义变换的变种数据集上进行系统测试。

Result: 1. 两模型在“记忆”方面能较好复现以往结果，表现稳定。2. 在“理解”环节，对语言重述及翻译有一定鲁棒性，能发现独特的可复现bug。3. 在“应用”阶段，变量名等标识符变更会导致性能骤降60%以上。4. 在开放书环境下，类似案例的少样本提示能提升成功率达三倍。5. 组件级分析显示，结构化技术信息（如测试代码、方法名）对测试生成帮助远大于叙述性描述。

Conclusion: LLMs在测试生成时的认知与推理能力层次明显，且易受自然语言和语义变换影响。为提高泛化能力，可强化少样本学习和技术信息输入，并利用分层认知评估为模型改进提供实用依据。本研究建立了较为完善的实际评价范式。

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [4] [Who Do You Think You Are? Creating RSE Personas from GitHub Interactions](https://arxiv.org/abs/2510.05390)
*Felicity Anderson,Julien Sindt,Neil Chue Hong*

Main category: cs.SE

TL;DR: 本文提出并验证了一种基于软件仓库数据挖掘和画像分析方法，通过GitHub大数据样本，对研究软件领域的贡献者进行分类和行为模式识别，有助于团队优化协作与行为认知。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程（RSE）领域的项目团队和个人，如何通过数据挖掘方法理解他们在开源研究软件中的贡献模式与互动行为。

Method: 通过结合软件仓库挖掘和数据驱动的人物画像，分析GitHub上中型（10-300提交者）研究软件仓库的协作行为，并从42,284个候选记录中提取、分析出1,284个研究软件仓库及其115,174名贡献者的数据。

Result: 成功识别并总结了贡献者协作互动的七种不同画像（从低到高的互动度）：短暂贡献者、偶尔贡献者、项目组织者、中度贡献者、低过程闭合者、低编程闭合者和活跃贡献者。

Conclusion: 数据驱动的人物画像可以有效帮助研究软件团队和个人认识自身在软件项目中的行为和影响，且可以大规模地分析不同背景和领域的项目贡献者行为模式。

Abstract: We describe data-driven RSE personas: an approach combining software
repository mining and data-driven personas applied to research software (RS),
an attempt to describe and identify common and rare patterns of Research
Software Engineering (RSE) development. This allows individuals and RS project
teams to understand their contributions, impact and repository dynamics - an
important foundation for improving RSE. We evaluate the method on different
patterns of collaborative interaction behaviours by contributors to mid-sized
public RS repositories (those with 10-300 committers) on GitHub. We demonstrate
how the RSE personas method successfully characterises a sample of 115,174
repository contributors across 1,284 RS repositories on GitHub, sampled from
42,284 candidate software repository records queried from Zenodo. We identify,
name and summarise seven distinct personas from low to high interactivity:
Ephemeral Contributor; Occasional Contributor; Project Organiser; Moderate
Contributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.
This demonstrates that large datasets can be analysed despite difficulties of
comparing software projects with different project management factors, research
domains and contributor backgrounds.

</details>


### [5] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX是一款结合AI多智能体、形式化方法和LLM的开源自动化测试系统，显著提升了遗留代码的测试覆盖率、可读性和软件质量。


<details>
  <summary>Details</summary>
Motivation: 传统遗留代码的单元测试覆盖率低，测试难度大，现有大语言模型（LLM）在遗留代码的自动化测试和缺陷检测方面存在局限。为提升软件可靠性和可维护性，亟需新的自动化测试生成方法。

Method: 提出UnitTenX系统，结合多智能体系统、形式化方法与大语言模型，实现了针对遗留代码的自动化单元测试生成，并提升测试全面性和关键测试价值。

Result: 实验结果表明，UnitTenX能生成高质量的单元测试，提升测试覆盖率，发现潜在问题，并增强遗留代码的可读性和文档化。

Conclusion: UnitTenX作为开源多智能体AI系统，有效弥补LLM在遗留代码测试上的短板，为提升遗留代码的可靠性及可维护性提供了有力工具。

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [6] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 研究比较LLM与人类代码审查评论类型及解决率，发现LLM生成评论有实际价值，与人工审查互补，有助于提升自动化代码审查工具效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM自动生成的代码审核意见，并非全部都能推动代码变更。理解哪些生成意见最有可能促成开发者实际改动，能够提升自动化代码审核工具现实价值。

Method: 提出并开发了LLM-as-a-Judge工具，基于五类评论分类法，自动归类评审意见，并进行实证数据分析对比LLM与人类代码审查评论类型及解决率。

Result: 可读性、代码缺陷（bug）、可维护性相关的评论更高概率被开发者解决；设计相关评论解决率较低；LLM和人类审查员在项目背景下各表现出不同优势和不足。

Conclusion: LLM生成的代码审查评论中，有相当一部分是可执行的，可以被开发者采纳解决；LLM与人类审查员在评论表现上互补，提高了自动化代码审查的实际效用。

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [7] [An Empirical Study of Security-Policy Related Issues in Open Source Projects](https://arxiv.org/abs/2510.05604)
*Rintaro Kanaji,Brittany Reid,Yutaro Kashiwa,Raula Gaikovina Kula,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究分析了SECURITY.md在开源项目漏洞报告中的挑战，发现大多数相关需求是添加文件，且有链接的报告处理速度更快。建议改善安全政策以提升开源安全。


<details>
  <summary>Details</summary>
Motivation: GitHub建议项目使用SECURITY.md文件来明确漏洞报告流程，但该文件的有效性和操作中的实际挑战尚未完全了解。本文旨在澄清SECURITY.md文件在开源社区漏洞报告过程中面临的难题。

Method: 作者对711个SECURITY.md相关的GitHub issue进行了分类、内容分析，并对包括SECURITY.md在内的六种社区健康文件的issue关闭时间和回应数量进行了定量比较分析。

Result: 79.5%的SECURITY.md相关issue是要求添加该文件。包含链接的报告关闭时间中位数缩短了2天。

Conclusion: 研究结果为改进安全报告政策和社区管理提供了实践建议，有助于提升开源生态系统的安全性。

Abstract: GitHub recommends that projects adopt a SECURITY.md file that outlines
vulnerability reporting procedures. However, the effectiveness and operational
challenges of such files are not yet fully understood. This study aims to
clarify the challenges that SECURITY.md files face in the vulnerability
reporting process within open-source communities. Specifically, we classified
and analyzed the content of 711 randomly sampled issues related to SECURITY.md.
We also conducted a quantitative comparative analysis of the close time and
number of responses for issues concerning six community health files, including
SECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues
were requests to add the file, and reports that included links were closed,
with a median time that was 2 days shorter. These findings offer practical
insights for improving security reporting policies and community management,
ultimately contributing to a more secure open-source ecosystem.

</details>


### [8] [The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment](https://arxiv.org/abs/2510.05705)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: 本文介绍了用于整合与评估科研软件FAIR性的Software Observatory平台，通过可视化和量化评分促进了生命科学领域更优软件开发实践。


<details>
  <summary>Details</summary>
Motivation: 科学软件开发领域正迅速变化，了解现有趋势对于科学界来说至关重要，以便识别可能阻碍科学进步的空白。采用FAIR原则作为衡量软件现状和指导改进行动的依据。

Method: 提出并开发了Software Observatory这一个全新网络门户，通过整合多渠道软件元数据，对研究软件进行趋势分析和FAIR原则的综合评估，并通过FAIRsoft Evaluator实现分层次可视化和定量评分。

Result: 该平台能够整合和可视化生命科学研究软件的大量元数据，从整体到社区再到单个软件条目层级呈现，并借助FAIRsoft Evaluator工具对软件FAIR性进行量化评分和改进建议。

Conclusion: Software Observatory为研究人员、开发者及利益相关方提供了监测和改进研究软件FAIR性的有力工具，有助于推动更优的软件开发实践与FAIR原则的应用。

Abstract: In the ever-changing realm of research software development, it is crucial
for the scientific community to grasp current trends to identify gaps that can
potentially hinder scientific progress. The adherence to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles can serve as a proxy to
understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench
(https://openebench.bsc.es/observatory) is a novel web portal that consolidates
software metadata from various sources, offering comprehensive insights into
critical research software aspects. Our platform enables users to analyse
trends, identify patterns and advancements within the Life Sciences research
software ecosystem, and understand its evolution over time. It also evaluates
research software according to FAIR principles for research software, providing
scores for different indicators.
  Users have the ability to visualise this metadata at different levels of
granularity, ranging from the entire software landscape to specific communities
to individual software entries through the FAIRsoft Evaluator. Indeed, the
FAIRsoft Evaluator component streamlines the assessment process, helping
developers efficiently evaluate and obtain guidance to improve their software's
FAIRness.
  The Software Observatory represents a valuable resource for researchers and
software developers, as well as stakeholders, promoting better software
development practices and adherence to FAIR principles for research software.

</details>


### [9] [Digital Twins for Software Engineering Processes](https://arxiv.org/abs/2510.05768)
*Robin Kimmel,Judith Michael,Andreas Wortmann,Jingxi Zhang*

Main category: cs.SE

TL;DR: 数字孪生有望革新软件工程流程，通过更准确地表示和优化流程，提高效率和质量。作者提出了相关模型设想，并讨论了实现的技术挑战与需求。


<details>
  <summary>Details</summary>
Motivation: 应对软件工程师短缺的困境，寻求提升软件开发效率和质量的新方法。希望通过数字孪生更好地表示和优化软件工程流程。

Method: 本文主要为观点性论文，阐述利用数字孪生优化软件工程流程的模型设想，描述数字孪生可实现的功能与潜力，并讨论现存的技术缺口。

Result: 文章分析了数字孪生在软件工程领域的潜力，对其未来应用进行了展望，并指出了实现该愿景所缺失的必要条件。

Conclusion: 数字孪生能够改善软件工程过程，提升软件专家的效率，并协助领域专家产出高质量软件，但实现和部署仍有诸多挑战。

Abstract: Digital twins promise a better understanding and use of complex systems. To
this end, they represent these systems at their runtime and may interact with
them to control their processes. Software engineering is a wicked challenge in
which stakeholders from many domains collaborate to produce software artifacts
together. In the presence of skilled software engineer shortage, our vision is
to leverage DTs as means for better rep- resenting, understanding, and
optimizing software engineering processes to (i) enable software experts making
the best use of their time and (ii) support domain experts in producing
high-quality software. This paper outlines why this would be beneficial, what
such a digital twin could look like, and what is missing for realizing and
deploying software engineering digital twins.

</details>


### [10] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum是专为IDE交互补全设计的开源4B代码模型，通过分阶段训练与实际反馈优化，在实际环境下兼顾精度、响应和可用性，已大规模上线，且完全公开。


<details>
  <summary>Details</summary>
Motivation: 当前代码补全模型在交互效率、响应速度、上下文处理等方面存在限制，难以在实际IDE中大规模应用。作者希望通过专注于任务的紧凑模型，提升生产部署可行性，同时方便公开和复现。

Method: 提出了Mellum模型家族，是一种拥有4B参数的开放权重代码补全模型，采用Llama风格架构，并在4万亿多语言代码令牌上进行预训练。采用分阶段训练，包括fill-in-the-middle和通过监督微调获取项目上下文，以及使用真实用户反馈进行直接偏好优化。实现从数据治理到上线的工业流水线。

Result: （1）精细数据筛选与分阶段训练可显著提升模型性能，
（2）针对编辑器的上下文打包等能力是高质量补全的关键，
（3）紧凑且专注任务的模型能满足交互式补全的成本和延迟要求。模型已在JetBrains IDEs生产环境中大规模部署，效果良好。

Conclusion: 通过系统的管线设计、专注的模型训练和开放共享，Mellum为代码补全模型从科研到大规模生产部署提供了可行范本，显著提升了实际开发环境中的代码补全体验。

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [11] [A Wave of Resignations in the Aftermath of Remote Onboarding](https://arxiv.org/abs/2510.05878)
*Darja Smite,Franz Zieris,Lars-Ola Damm*

Main category: cs.SE

TL;DR: 本文基于Ericsson瑞典2016-2025年数据发现，疫情期间远程入职导致归属感不足，增加离职率。恢复现场办公、推进团队与资深员工共同在岗，有效提升新员工留任率，对未来HR政策制定具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 受COVID-19疫情影响，远程办公已成常态，但完全远程安排对软件团队带来挑战，需要探索不同工作模式对员工留任的具体影响。

Method: 采用2016-2025年Ericsson瑞典分公司的HR数据，分析疫情前、疫情期间及后疫情时期不同工作模式（现场、远程、混合）对员工留任率的影响，并结合离职调查。

Result: 2021夏至2023夏员工离职大幅增加，尤其是工作年限少于5年的员工。疫情期间远程入职员工离职概率更高，恢复办公室工作后留任率回升，证明混合及差异化政策有效。

Conclusion: 研究发现，疫情期间远程入职的员工在前3年内离职率较高，归因于缺乏组织归属感；而公司采用差异化政策并加强新员工与团队、资深员工的在岗互动，成功恢复了疫情前的留任率。

Abstract: The COVID-19 pandemic has permanently altered workplace structures,
normalizing remote work. However, critical evidence highlights challenges with
fully remote arrangements, particularly for software teams. This study
investigates employee resignation patterns at Ericsson, a global developer of
software-intensive systems, before, during, and after the pandemic. Using HR
data from 2016-2025 in Ericsson Sweden, we analyze how different work
modalities (onsite, remote, and hybrid) influence employee retention. Our
findings show a marked increase in resignations from summer 2021 to summer
2023, especially among employees with less than five years of tenure. Employees
onboarded remotely during the pandemic were significantly more likely to resign
within their first three years, even after returning to the office. Exit
surveys suggest that remote onboarding may fail to establish the necessary
organizational attachment, the feeling of belonging and long-term retention. By
contrast, the company's eventual successful return to pre-pandemic retention
rates illustrates the value of differentiated work policies and supports
reconsidering selective return-to-office (RTO) mandates. Our study demonstrates
the importance of employee integration practices in hybrid environments where
the requirement for in-office presence for recent hires shall be accompanied by
in-office presence from their team members and more senior staff whose
mentoring and social interactions contribute to integration into the corporate
work environment. We hope these actionable insights will inform HR leaders and
policymakers in shaping post-pandemic work practices, demonstrating that
carefully crafted hybrid models anchored in organizational attachment and
mentorship can sustain retention in knowledge-intensive companies.

</details>


### [12] [Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications](https://arxiv.org/abs/2510.05968)
*Scott Frees*

Main category: cs.SE

TL;DR: 本文提出了解决LLM驱动报表系统上下文窗口限制的架构设计模式，包括查询生成与数据检索解耦、双重响应和安全资源管理，为开发者提供了可落地的技术方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型能将自然语言转化为数据库查询，但其上下文窗口受限，无法在报表系统中直接应用，特别是完整数据集会耗尽可用令牌，而现有的 Model Context Protocol 只是定义了ResourceLink，缺乏实际可扩展架构的实现指导。

Method: 提出了一组架构模式，用于构建将查询生成与数据检索解耦的LLM驱动报表系统；引入了扩展ResourceLink的双重响应模式，支持迭代查询优化和带外数据访问，同时涵盖多租户安全性与资源生命周期管理模式。

Result: 所提出的设计模式能有效解决LLM应用于报表系统中的上下文受限、数据访问安全与资源管理等核心难题。

Conclusion: 新模式为开发者提供了实践参考，实现了LLM驱动报表系统的可扩展性和实用性。

Abstract: Large language models translate natural language into database queries, yet
context window limitations prevent direct deployment in reporting systems where
complete datasets exhaust available tokens. The Model Context Protocol
specification defines ResourceLink for referencing external resources, but
practical patterns for implementing scalable reporting architectures remain
undocumented. This paper presents patterns for building LLM-powered reporting
systems that decouple query generation from data retrieval. We introduce a
dual-response pattern extending ResourceLink to support both iterative query
refinement and out-of-band data access, accompanied by patterns for
multi-tenant security and resource lifecycle management. These patterns address
fundamental challenges in LLM-driven reporting applications and provide
practical guidance for developers building them.

</details>


### [13] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本研究通过调研91名软件工程师，总结了GenAI在开发流程中的实际应用：简单代码生成易用，复杂任务尚具挑战，迭代对话形式更受欢迎，提出了优化开发与工具设计的方向。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI工具深度融入软件开发领域，现有研究多聚焦具体技术方法，缺乏对开发者全流程工作习惯、实际需求及挑战的系统研究。本文希望填补该空白。

Method: 通过对91名软件工程师的大规模问卷调查，系统分析了开发者在软件开发流程中使用GenAI的具体方式，包括提问策略、对话模式及不同任务中的可靠性评估。

Result: （1）代码生成几乎已普及；（2）能熟练运用AI进行复杂任务（如调试、代码审查）的开发者更具优势；（3）开发者更偏好多轮迭代式对话而非一次性问询；（4）文档类任务最可靠，复杂代码生成和调试仍具备显著挑战。（5）总结并给出了可供后续改进的开发实践参考。

Conclusion: 本文显示生成式人工智能（GenAI）工具在软件开发中的应用范围广泛，从简单的代码生成到复杂的集成流程，未来应针对难点（如复杂代码生成和调试）持续优化设计。

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


### [14] [Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations](https://arxiv.org/abs/2510.06104)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文关注开源软件开发中的代码安全修改挑战，提出利用LLM将代码度量指标转换为易懂的风险解释和建议，预计这一方法有助于新手开发者做出更好决策。后续将通过用户实验量化分析实际效用。


<details>
  <summary>Details</summary>
Motivation: 开源软件维护高度依赖各类开发者的贡献，复杂系统中关联组件多，度量指标虽能揭示潜在风险，但初学者难以理解。因此，亟需将这些难以理解的指标转化为通俗易懂的解释和可操作建议，提升贡献者修改代码的安全性和效率。

Method: 该研究提出由LLM生成多种解释类型（描述性、上下文型、可操作性），并计划通过任务驱动的实证研究，与仅展示原始度量指标的基线进行比较，评估LLM生成解释对于OSS贡献者决策质量、完成时长和错误率的影响。

Result: 论文提出解释类型框架并制定了实验设计，但实际效果待后续通过任务实际评估，包括决策质量、完成时间和错误率等指标。

Conclusion: 作者认为，大型语言模型（LLM）有潜力将静态分析和缺陷预测工具的复杂度等指标转化为可理解的风险解释和可操作建议，从而帮助开源开发者安全高效地修改代码。

Abstract: Open Source Software (OSS) has become a very important and crucial
infrastructure worldwide because of the value it provides. OSS typically
depends on contributions from developers across diverse backgrounds and levels
of experience. Making safe changes, such as fixing a bug or implementing a new
feature, can be challenging, especially in object-oriented systems where
components are interdependent. Static analysis and defect-prediction tools
produce metrics (e.g., complexity,coupling) that flag potentially fault-prone
components, but these signals are often hard for contributors new or unfamiliar
with the codebase to interpret. Large Language Models (LLMs) have shown strong
performance on software engineering tasks such as code summarization and
documentation generation. Building on this progress, we investigate whether
LLMs can translate fault-prediction metrics into clear, human-readable risk
explanations and actionable guidance to help OSS contributors plan and review
code modifications. We outline explanation types that an LLM-generated
assistant could provide (descriptive, contextual, and actionable explanations).
We also outline our next steps to assess usefulness through a task-based study
with OSS contributors, comparing metric-only baselines to LLM-generated
explanations on decision quality, time-to-completion, and error rates

</details>


### [15] [Automated Program Repair of Uncompilable Student Code](https://arxiv.org/abs/2510.06187)
*Griffin Pitts,Aum Pandya,Darsh Rank,Tirth Bhatt,Muntasir Hoq,Bita Akram*

Main category: cs.SE

TL;DR: 本文发现，通过LLM自动修复不可编译学生代码，可极大丰富学生建模和知识追踪的数据基础。不过，不同模型在保持学生原意方面表现不同，对教学分析有实际影响。


<details>
  <summary>Details</summary>
Motivation: CS1课程中，许多学生提交的编程代码无法编译，导致这些数据无法被用于学生建模和知识追踪，传统方法往往直接丢弃此类代码。本文希望利用自动程序修复方法恢复不可编译代码，以保留对学生学习行为的观察。

Method: 采用大语言模型（LLM），包括GPT-5、Claude 3.5 Haiku和Gemini 2.5 Flash，作为修复代理。在高、低上下文提示下，对模型修复的代码在可编译率、编辑距离和结构/逻辑保留度方面进行评估。

Result: 三种LLM都能生成可编译的修复方案，但在保持学生原始代码控制流和结构方面有差异，这影响了各自的教学效用。

Conclusion: 自动程序修复特别是借助LLM，能够恢复被传统管道丢弃的大量不可编译学生代码，使得对学生编码过程和学习发展的分析更加完整和丰富。

Abstract: A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [On the Interplay of Cube Learning and Dependency Schemes in QCDCL Proof Systems](https://arxiv.org/abs/2510.05876)
*Abhimanyu Choudhury,Meena Mahajan*

Main category: cs.LO

TL;DR: 本文理论化并分析了结合Cube-learning与依赖方案的QCDCL证明系统，提升了QBF反驳效率，并比较了不同依赖方案的优势，为进一步优化QBF求解方法打下理论基础。


<details>
  <summary>Details</summary>
Motivation: QCDCL作为解决QBF公式的主要方法之一，存在因量词前缀变量排序带来的虚假依赖问题，影响反驳效率。本研究旨在进一步优化QCDCL，通过结合Cube-learning和依赖方案，提升求解性能并理论上分析其底层证明系统。

Method: 对使用Cube-learning和各阶段依赖方案的QCDCL推理底层证明系统进行形式化。引入标准及反射性分辨路径依赖方案（$D^{std}$与$D^{rrs}$），并给出其充足性条件，分析不同依赖方案结合Cube-learning时的证据系统强度，详细探讨决策顺序受限情况下的表现。

Result: 证明了采用$D^{std}$和$D^{rrs}$依赖方案放松决策顺序可以有效缩短反驳过程；当只在传播和学习阶段使用依赖方案且结合Cube-learning时，详细分析了相关证明系统的相对强弱，理论上提升了QBF求解效率。

Conclusion: 融合Cube-learning和全阶段依赖方案的QCDCL推理系统有充足的正确性与完全性且可提升反驳效率。标准与反射性依赖方案在决策顺序限制下结合Cube-learning时展现了不同的优势，为QBF求解与相关证明系统优化提供理论基础。

Abstract: Quantified Conflict Driven Clause Leaning (QCDCL) is one of the main
approaches to solving Quantified Boolean Formulas (QBF). Cube-learning is
employed in this approach to ensure that true formulas can be verified.
Dependency Schemes help to detect spurious dependencies that are implied by the
variable ordering in the quantifier prefix of QBFs but are not essential for
constructing (counter)models. This detection can provably shorten refutations
in specific proof systems, and is expected to speed up runs of QBF solvers.
  The simplest underlying proof system [BeyersdorffB\"ohm-LMCS2023], formalises
the reasoning in the QCDCL approach on false formulas, when neither cube
learning nor dependency schemes is used. The work of
[B\"ohmPeitlBeyersdorff-AI2024] further incorporates cube-learning. The work of
[ChoudhuryMahajan-JAR2024] incorporates a limited use of dependency schemes,
but without cube-learning.
  In this work, proof systems underlying the reasoning of QCDCL solvers which
use cube learning, and which use dependency schemes at all stages, are
formalised. Sufficient conditions for soundness and completeness are presented,
and it is shown that using the standard and reflexive resolution path
dependency schemes ($D^{std}$ and $D^{rrs}$) to relax the decision order
provably shortens refutations.
  When the decisions are restricted to follow quantification order, but
dependency schemes are used in propagation and learning, in conjunction with
cube-learning, the resulting proof systems using the dependency schemes
$D^{std}$ and $D^{rrs}$ are investigated in detail and their relative strengths
are analysed.

</details>


### [17] [On Equivalent Characterizations of NP in Abstract Models of Computation](https://arxiv.org/abs/2510.05894)
*Jeremy C. Kirn,Lucas Meijer,Tillmann Miltzow,Hans L. Bodlaender*

Main category: cs.LO

TL;DR: 本文探讨了带有一阶结构\mathcal{R}运算的计算模型，对NP(\mathcal{R})与更高层级复杂性类别进行三种等价刻画，扩展了经典描述性复杂性理论至包含无限词汇的结构，建立了统一理论体系。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在研究具备一阶结构\mathcal{R}运算的图灵机模型，探讨其复杂性类别NP(\mathcal{R})的多重等价刻画，并扩展描述性复杂性理论到拥有无限词汇表的结构。

Method: 作者采用理论方法，从算子机器、判定性问题（如SAT）及描述性复杂性三大角度，系统证明NP(\mathcal{R})的三种等价定义。同时，对更高层次多项式与布尔层级，使用oracle \mathcal{R}-machine的理论工具进行刻画推广。

Result: 在对\mathcal{R}具备一定约束的条件下，证明了NP(\mathcal{R})可由三种方式等价刻画，并首次将描述性复杂性理论成功拓展到无限词汇结构。还类比地证明了无常数布尔 NP 类（\exists\mathcal{R}）也有类似三重刻画，并进一步推广至多项式与布尔层级。

Conclusion: 该工作将原有单点结果融为一致的理论框架，并拓展了描述性复杂性至更一般的结构（如实向量空间），揭示了该理论处理无限词汇结构的强大能力。

Abstract: We investigate machine models similar to Turing machines that are augmented
by the operations of a first-order structure $\mathcal{R}$, and we show that
under weak conditions on $\mathcal{R}$, the complexity class
$\text{NP}(\mathcal{R})$ may be characterized in three equivalent ways: (1) by
polynomial-time verification algorithms implemented on $\mathcal{R}$-machines,
(2) by the $\text{NP}(\mathcal{R})$-complete problem $\text{SAT}(\mathcal{R})$,
and (3) by existential second-order metafinite logic over $\mathcal{R}$ via
descriptive complexity. By characterizing $\text{NP}(\mathcal{R})$ in these
three ways, we extend previous work and embed it in one coherent framework.
  Some conditions on $\mathcal{R}$ must be assumed in order to achieve the
above trinity because there are infinite-vocabulary structures for which
$\text{NP}(\mathcal{R})$ does not have a complete problem. Surprisingly, even
in these cases, we show that $\text{NP}(\mathcal{R})$ does have a
characterization in terms of existential second-order metafinite logic,
suggesting that descriptive complexity theory is well suited to working with
infinite-vocabulary structures, such as real vector spaces.
  In addition, we derive similar results for $\exists\mathcal{R}$, the
constant-free Boolean part of $\text{NP}(\mathcal{R})$, by showing that
$\exists\mathcal{R}$ may be characterized in three analogous ways. We then
extend our results to the entire polynomial hierarchy over $\mathcal{R}$ and to
its constant-free Boolean counterpart, the Boolean hierarchy over
$\mathcal{R}$. Finally, we give a characterization of the polynomial and
Boolean hierarchies over $\mathcal{R}$ in terms of oracle
$\mathcal{R}$-machines.

</details>


### [18] [A Timed Obstruction Logic for Dynamic Game Models](https://arxiv.org/abs/2510.06045)
*David Cortes,Jean Leneutre,Vadim Malvone,James Ortiz*

Main category: cs.LO

TL;DR: 本文提出了用于建模和验证实时网络安全博弈的定时阻断逻辑（TOL），兼具高表达性与可控复杂度，为关键基础设施中网络安全的形式化分析提供了理论和方法支持。


<details>
  <summary>Details</summary>
Motivation: 实时网络安全和隐私应用需要可靠的验证方法和系统设计工具，以确保其正确性。由于嵌入在关键基础设施中的这些系统容易受到网络攻击，亟需理论与工具支持来建模和防护。近年来，定时博弈论被认为是建模攻击者与防御者战略互动的有效基础。

Method: 提出了一种新的逻辑——定时阻断逻辑（TOL），这是对已有阻断逻辑（OL）的扩展，可以对具有实时目标的动态定时博弈进行建模与验证。文中设计算法实现了TOL的验证过程，并证明了其复杂度。

Result: TOL能够描述实时网络安全博弈中的重要属性，提供了对应的验证过程。验证复杂度为PSPACE-complete，与传统定时时序逻辑（如TCTL）相当。

Conclusion: 定时阻断逻辑（TOL）提升了属性表达能力，却未增加验证复杂度，为实时网络安全系统的建模和验证提供了更强且高效的工具。

Abstract: Real-time cybersecurity and privacy applications require reliable
verification methods and system design tools to ensure their correctness. Many
of these reactive real-time applications embedded in various infrastructures,
such as airports, hospitals, and oil pipelines, are potentially vulnerable to
malicious cyber-attacks. Recently, a growing literature has recognized Timed
Game Theory as a sound theoretical foundation for modeling strategic
interactions between attackers and defenders. This paper proposes Timed
Obstruction Logic (TOL), an extension of Obstruction Logic (OL), a formalism
for verifying specific timed games with real-time objectives unfolding in
dynamic models. These timed games involve players whose discrete and continuous
actions can impact the underlying timed game model. We show that TOL can be
used to describe important timed properties of real-time cybersecurity games.
Finally, in addition to introducing our new logic and adapting it to specify
properties in the context of cybersecurity, we provide a verification procedure
for TOL and show that its complexity is PSPACE-complete, meaning that it is not
higher than that of classical timed temporal logics like TCTL. Thus, we
increase the expressiveness of properties without incurring any cost in terms
of complexity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [Collaborative and Proactive Management of Task-Oriented Conversations](https://arxiv.org/abs/2510.05110)
*Arezoo Saedi,Afsaneh Fatemi,Mohammad Ali Nematbakhsh,Sophie Rosset,Anne Vilnat*

Main category: cs.CL

TL;DR: 本文针对现有任务型对话系统在主动规划方面的不足，提出了一种以信息状态为核心、结合LLM的对话管理模型，能够更好捕捉用户目标与偏好，在标准数据集上取得了较好表现。


<details>
  <summary>Details</summary>
Motivation: 当前的任务型对话系统大多依赖于大型语言模型（LLM），但在主动规划和目标感知方面存在不足，影响任务完成效果。因此，作者希望提出一种能更好利用目标感知规划的新模型。

Method: 本文基于信息状态对话管理方法，提出一种新的任务型对话模型。首先根据用户偏好定义插槽和文本成分，构建用户信息表示。分析对话中关键情境并生成相应信息组件，通过组合限定信息状态。随后定义对话动作，实现状态转移和必要操作。最后，利用LLM的in-context learning实现该模型，数据库查询以预定义插槽为中心，实体排序对应文本生成的顺序。

Result: 在MultiWOZ数据集上的测试结果显示，该模型在单领域对话任务下，在inform和success指标上达到最大值，并且相较于先前方法表现更优。

Conclusion: 本文提出的信息状态驱动任务型对话模型能够更有效利用中间信息实现目标感知规划，借助LLM的in-context learning机制提升了对话系统的任务完成率。

Abstract: Task oriented dialogue systems (TOD) complete particular tasks based on user
preferences across natural language interactions. Considering the impressive
performance of large language models (LLMs) in natural language processing
(NLP) tasks, most of the latest TODs are centered on LLMs. While proactive
planning is crucial for task completion, many existing TODs overlook effective
goal-aware planning. This paper creates a model for managing task-oriented
conversations, conceptualized centered on the information state approach to
dialogue management. The created model incorporated constructive intermediate
information in planning. Initially, predefined slots and text part
informational components are created to model user preferences. Investigating
intermediate information, critical circumstances are identified. Informational
components corresponding to these circumstances are created. Possible
configurations for these informational components lead to limited information
states. Then, dialogue moves, which indicate movement between these information
states and the procedures that must be performed in the movements, are created.
Eventually, the update strategy is constructed. The created model is
implemented leveraging in-context learning of LLMs. In this model, database
queries are created centered on indicated predefined slots and the order of
retrieved entities is indicated centered on text part. This mechanism enables
passing the whole corresponding entities to the preferences in the order of
congruency. Evaluations exploiting the complete test conversations of MultiWOZ,
with no more than a domain in a conversation, illustrate maximal inform and
success, and improvement compared with previous methods.

</details>


### [20] [Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System](https://arxiv.org/abs/2510.05113)
*Nisheeth Joshi,Pragya Katyayan,Palak Arora*

Main category: cs.CL

TL;DR: 针对古吉拉特语机器翻译评估，以监督学习方法建模，提出新指标，并通过与现有方法对比证明新指标具有更高的人类相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译评估方法主要针对英语及其他欧洲语言进行优化，对于印度语言尤其是古吉拉特语的评估效果不佳。为了提升对这些语言的机器翻译系统的评估准确性，提出新的评估方法势在必行。

Method: 提出了一种基于监督学习的古吉拉特语机器翻译评估指标。该指标采用了25种特征进行训练，分别训练了两种版本：一种模型使用6个隐藏层、训练500轮；另一种模型使用10个隐藏层、训练500轮。通过收集七个机器翻译系统的1000条输出与人工参考翻译进行对比，测试指标性能。

Result: 新提出的评估指标与现有评估方法相比，展现出更高的人类相关性。

Conclusion: 所提出的监督学习评估方法能够更好地提升古吉拉特语机器翻译系统的评估准确性，为针对特定语言构建评估方法提供了有效思路。

Abstract: Machine Translation (MT) Evaluation is an integral part of the MT development
life cycle. Without analyzing the outputs of MT engines, it is impossible to
evaluate the performance of an MT system. Through experiments, it has been
identified that what works for English and other European languages does not
work well with Indian languages. Thus, In this paper, we have introduced a
reference-based MT evaluation metric for Gujarati which is based on supervised
learning. We have trained two versions of the metric which uses 25 features for
training. Among the two models, one model is trained using 6 hidden layers with
500 epochs while the other model is trained using 10 hidden layers with 500
epochs. To test the performance of the metric, we collected 1000 MT outputs of
seven MT systems. These MT engine outputs were compared with 1 human reference
translation. While comparing the developed metrics with other available
metrics, it was found that the metrics produced better human correlations.

</details>


### [21] [Hallucination is Inevitable for LLMs with the Open World Assumption](https://arxiv.org/abs/2510.05116)
*Bowen Xu*

Main category: cs.CL

TL;DR: 本文认为大模型产生“幻觉”在开放世界环境下是无法彻底避免的，而这应被理解为泛化问题的一种体现，与人类的智能特性相关。建议未来对幻觉现象不止做工程解决，更要在结构和理念上容纳其存在。


<details>
  <summary>Details</summary>
Motivation: 语言大模型经常产生错误或虚构的信息（即“幻觉”），工程师多将之视为需减少的缺陷，但理论分析认为幻觉是不可避免的。对于追求人工通用智能（AGI）的问题，现有观点均不全面。

Method: 本文从泛化问题的角度重新审视“幻觉”现象，区分封闭世界（训练集与测试集一致）与开放世界（环境无界限）两种假设，并发展了幻觉的分类体系，分析哪类幻觉可纠正，哪类在开放世界条件下无法避免。

Result: 本文提出，幻觉不仅仅是工程缺陷，而是在开放世界条件下不可避免的结构性特征。幻觉现象甚至应被视为人工智能与人类智能兼容的组成部分。

Conclusion: “幻觉”应当作为结构性特征理解，不能只当做缺陷去消除，而需容纳和适应，使其与人类智能兼容，尤其是在开放世界的环境下。

Abstract: Large Language Models (LLMs) exhibit impressive linguistic competence but
also produce inaccurate or fabricated outputs, often called ``hallucinations''.
Engineering approaches usually regard hallucination as a defect to be
minimized, while formal analyses have argued for its theoretical inevitability.
Yet both perspectives remain incomplete when considering the conditions
required for artificial general intelligence (AGI). This paper reframes
``hallucination'' as a manifestation of the generalization problem. Under the
Closed World assumption, where training and test distributions are consistent,
hallucinations may be mitigated. Under the Open World assumption, however,
where the environment is unbounded, hallucinations become inevitable. This
paper further develops a classification of hallucination, distinguishing cases
that may be corrected from those that appear unavoidable under open-world
conditions. On this basis, it suggests that ``hallucination'' should be
approached not merely as an engineering defect but as a structural feature to
be tolerated and made compatible with human intelligence.

</details>


### [22] [Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models](https://arxiv.org/abs/2510.05121)
*Durgesh Nandini,Rebekka Koch,Mirco Schoenfeld*

Main category: cs.CL

TL;DR: 该论文证明了大语言模型（如Llama 3.1）能够有效从经济相关文本中提取主谓宾三元组，为自动化构建经济知识图谱提供了可行路径，并评估了不同提示方法的效果。


<details>
  <summary>Details</summary>
Motivation: 推动经济领域知识自动化提取，提升从自然语言文本中获取结构化信息（如法律贸易协定文本）的效率和准确性。

Method: 应用Llama 3.1语言模型于经济领域，通过零样本、单样本和少样本的提示方法，并结合正、负样本，提取主谓宾三元组；采用定量和定性指标对结果进行评估。

Result: 成功从区域贸易协议文本中提取与贸易相关的信息三元组，并对不同提示策略的效果进行了比较分析，提供了关键洞见与挑战。

Conclusion: 大语言模型在经济领域结构化知识提取中具有重要应用价值，未来可用于自动构建经济贸易知识图谱等多场景应用。

Abstract: This study investigates the effectiveness of Large Language Models (LLMs) for
the extraction of structured knowledge in the form of Subject-Predicate-Object
triples. We apply the setup for the domain of Economics application. The
findings can be applied to a wide range of scenarios, including the creation of
economic trade knowledge graphs from natural language legal trade agreement
texts. As a use case, we apply the model to regional trade agreement texts to
extract trade-related information triples. In particular, we explore the
zero-shot, one-shot and few-shot prompting techniques, incorporating positive
and negative examples, and evaluate their performance based on quantitative and
qualitative metrics. Specifically, we used Llama 3.1 model to process the
unstructured regional trade agreement texts and extract triples. We discuss key
insights, challenges, and potential future directions, emphasizing the
significance of language models in economic applications.

</details>


### [23] [CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation](https://arxiv.org/abs/2510.05122)
*Jie Zhu,Yuanchen Zhou,Shuo Jiang,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: CARE框架无需依赖大规模合成数据，通过强化推理和训练显著提升了情感支持对话系统的响应质量和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在数据增强和合成语料，对支撑有效情感支持的深层认知推理过程关注不足。该工作希望通过强化推理过程，提升对话系统为用户提供情感价值的能力。

Method: 提出了CARE框架，通过利用原有情感支持对话数据进行逻辑推理训练，并结合强化学习进一步优化推理过程。

Result: 实验结果表明，CARE在生成逻辑合理、具有支持性的响应方面表现优异，促进了更加人性化、富有认知和同理心的情感支持系统构建。

Conclusion: CARE框架能够显著提升对话系统在情感支持场景中的逻辑性和支持性质量，推进了具备同理心和认知能力的情感支持系统的发展。

Abstract: Emotional Support Conversation (ESC) plays a vital role in alleviating
psychological stress and providing emotional value through dialogue. While
recent studies have largely focused on data augmentation and synthetic corpus
construction, they often overlook the deeper cognitive reasoning processes that
underpin effective emotional support. To address this gap, we propose
\textbf{CARE}, a novel framework that strengthens reasoning in ESC without
relying on large-scale synthetic data. CARE leverages the original ESC training
set to guide models in generating logically coherent and supportive responses,
thereby explicitly enhancing cognitive reasoning. Building on this foundation,
we further employ reinforcement learning to refine and reinforce the reasoning
process. Experimental results demonstrate that CARE significantly improves both
the logical soundness and supportive quality of responses, advancing the
development of empathetic, cognitively robust, and human-like emotional support
systems.

</details>


### [24] [MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation](https://arxiv.org/abs/2510.05124)
*Mingjin Li,Yu Liu,Huayi Liu,Xiang Ye,Chao Jiang,Hongguang Zhang*

Main category: cs.CL

TL;DR: MADS框架通过多智能体自对话在无人工标注条件下生成高质量说服训练数据，有效提升小模型说服效果和业务价值。


<details>
  <summary>Details</summary>
Motivation: 当前行业面临用户数据匮乏、冷启动评估困难和提示效率低下问题，急需低成本生成高质量对话训练数据。

Method: 提出MADS多智能体自对话仿真框架，包含用户智能体、对话智能体和优化智能体，并结合CoA建模与LLM评估验证方法。

Result: MADS用于真实营销场景，小型LLM转化率提升22.4%，从1.83%增加到2.24%。

Conclusion: MADS框架在提升小型LLM说服能力和业务转化率方面表现突出，具有明显的商业价值。

Abstract: We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for
generating persuasive multi-turn dialogues via agent self-play. MADS employs
three coordinated agents: User Agents simulating diverse persona-driven
behaviors, a Dialog Agent executing task-oriented persuasion strategies and an
Optimization Agent evaluating and refining dialogue outcomes. We further
validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and
dedicated LLMs' persuasion assessment. This approach enables low-cost
generation of training data without human annotation, addressing key industry
challenges such as lack of user data, cold-start evaluation difficulties, and
prompt inefficiency. Applied to a real-world marketing scenario, MADS
significantly improved the persuasion capacity of small LLMs, increasing the
organic traffic conversion rate by 22.4\% (from 1.83\% to 2.24\%) ,
demonstrating clear business value.

</details>


### [25] [Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation](https://arxiv.org/abs/2510.05125)
*Reza Shirkavand,Xiaokai Wei,Chen Wang,Zheng Hui,Heng Huang,Michelle Gong*

Main category: cs.CL

TL;DR: 此论文提出了将物品交互历史当作语言方言嵌入LLM的新架构IDIOMoE，在多种数据集上实现了更强的推荐性能，并兼顾文本理解能力，解决了协同信号与文本语义融合难题。


<details>
  <summary>Details</summary>
Motivation: 目前推荐系统需要同时具备协同过滤的高效和大语言模型（LLMs）的强表达与推理能力。但两者在处理用户偏好时各有局限，协同信号虽高效但语义不透明，LLM则语义丰富但难以建模隐性偏好。如何融合两种信息以满足日益增长的用户期望成为挑战。

Method: 提出了Item-ID + Oral-language Mixture-of-Experts Language Model（IDIOMoE）。该模型将物品交互历史视为语言空间中的一种本地方言，将协同信号融入到与自然语言一致的理解体系中。具体做法是在预训练LLM的每个模块的前馈网络中，拆分为文本专家和物品专家，并通过token类型门控，实现文本与目录信息互不干扰。

Result: IDIOMoE在公开和私有数据集上都达到了优秀的推荐性能，并且保留了预训练模型的文本理解能力。

Conclusion: 通过专家混合结构，有效融合了文本和协同信号，实现了推荐准确率与模型表达能力的双提升。

Abstract: While collaborative filtering delivers predictive accuracy and efficiency,
and Large Language Models (LLMs) enable expressive and generalizable reasoning,
modern recommendation systems must bring these strengths together. Growing user
expectations, such as natural-language queries and transparent explanations,
further highlight the need for a unified approach. However, doing so is
nontrivial. Collaborative signals are often token-efficient but semantically
opaque, while LLMs are semantically rich but struggle to model implicit user
preferences when trained only on textual inputs. This paper introduces Item-ID
+ Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item
interaction histories as a native dialect within the language space, enabling
collaborative signals to be understood in the same way as natural language. By
splitting the Feed Forward Network of each block of a pretrained LLM into a
separate text expert and an item expert with token-type gating, our method
avoids destructive interference between text and catalog modalities. IDIOMoE
demonstrates strong recommendation performance across both public and
proprietary datasets, while preserving the text understanding of the pretrained
model.

</details>


### [26] [Improving Metacognition and Uncertainty Communication in Language Models](https://arxiv.org/abs/2510.05126)
*Mark Steyvers,Catarina Belem,Padhraic Smyth*

Main category: cs.CL

TL;DR: 微调能提升LLM表达不确定性的能力，但需多任务联合训练才能实现跨领域的广泛提升。


<details>
  <summary>Details</summary>
Motivation: LLMs在决策场景中常常无法有效表达不确定性，用户可能误信错误答案。现有研究显示LLM具有内部不确定性信号，但其外显信心表达失准，难以区分正确与错误结果，需要改进此能力。

Method: 对两类LLM进行监督微调，分别在单题信心估计和两题信心比较两种元认知任务上训练，涵盖通用知识、数学、开放式问答等数据集，并测试泛化至医疗、法律等未见领域。

Result: 微调显著提升了LLM的信心校准度和区分能力（正确答案信心更高），且提升能在跨领域泛化，但不同任务间提升不互通（单题与两题互不迁移），只有多任务微调才能在新领域实现全面改善。

Conclusion: LLMs的不确定性交流能力可以通过微调得到提升并且具有泛化性，但不同的元认知技能需要通过多任务训练共同培养，不会自然互补。

Abstract: Large language models (LLMs) are increasingly used in decision-making
contexts, but when they present answers without signaling low confidence, users
may unknowingly act on erroneous outputs. While prior work shows that LLMs
maintain internal uncertainty signals, their explicit verbalized confidence is
typically miscalibrated and poorly discriminates between correct and incorrect
answers. Across two types of LLMs, we investigate whether supervised finetuning
can improve models' ability to communicate uncertainty and whether such
improvements generalize across tasks and domains. We finetune the LLMs on
datasets spanning general knowledge, mathematics, and open-ended trivia, and
evaluate two metacognitive tasks: (1) single-question confidence estimation,
where the model assigns a numeric certainty to its answer, and (2) pairwise
confidence comparison, where the model selects which of two answers it is more
likely to have correct. We assess generalization to unseen domains, including
medical and legal reasoning. Results show that finetuning improves calibration
(alignment between stated confidence and accuracy) and discrimination (higher
confidence for correct vs. incorrect responses) within and across domains,
while leaving accuracy unchanged. However, improvements are task-specific:
training on single-question calibration does not transfer to pairwise
comparison, and vice versa. In contrast, multitask finetuning on both forms of
metacognition yields broader gains, producing lower calibration error and
stronger discrimination in out-of-domain evaluations. These results show that
while uncertainty communication in LLMs is trainable and generalizable,
different metacognitive skills do not naturally reinforce one another and must
be developed together through multitask training.

</details>


### [27] [Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models](https://arxiv.org/abs/2510.05128)
*Si-Ioi Ng,Pranav S. Ambadi,Kimberly D. Mueller,Julie Liss,Visar Berisha*

Main category: cs.CL

TL;DR: 提出BERT自动提取和排序图片描述中的信息单元，显著提升认知障碍评估自动化精度与效率，并已开源。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估认知语言障碍的方法，基于图片描述时常忽略了视觉叙事路径（即说话者描述图片元素的顺序与位置），而利用空间语义特征分析虽然能捕捉该路径，但人工标注或词典映射十分耗时。

Method: 该研究提出基于BERT的流水线，采用二元交叉熵和成对排序损失进行微调，实现对Cookie Theft图片描述中的CIU内容信息单元的自动提取与排序。

Result: 通过5折交叉验证，CIU检测精度中位数达93%，召回率96%，序列错误率24%。所提方法提取的特征与真实值具有较强皮尔逊相关性，且在外部验证中优于词典基线。在通过ANCOVA评估群体差异时，其表现亦可与人工标注特征媲美。

Conclusion: 该管线可有效表征认知障碍评估所需的视觉叙事路径，且相关代码与模型已开源。

Abstract: Current methods for automated assessment of cognitive-linguistic impairment
via picture description often neglect the visual narrative path - the sequence
and locations of elements a speaker described in the picture. Analyses of
spatio-semantic features capture this path using content information units
(CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This
study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and
pairwise ranking loss, for automated CIU extraction and ordering from the
Cookie Theft picture description. Evaluated by 5-fold cross-validation, it
achieves 93% median precision, 96% median recall in CIU detection, and 24%
sequence error rates. The proposed method extracts features that exhibit strong
Pearson correlations with ground truth, surpassing the dictionary-based
baseline in external validation. These features also perform comparably to
those derived from manual annotations in evaluating group differences via
ANCOVA. The pipeline is shown to effectively characterize visual narrative
paths for cognitive impairment assessment, with the implementation and models
open-sourced to public.

</details>


### [28] [Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models](https://arxiv.org/abs/2510.05129)
*Qingshu Xu,Hong Jiao,Tianyi Zhou,Ming Li,Nan Zhang,Sydney Peters,Yanbin Fu*

Main category: cs.CL

TL;DR: 作者系统比较了传统机器学习、语言模型微调和集成学习在测评题目对齐内容标准任务中的表现，发现基于语言模型的方法效果最好，集成学习并未带来额外提升。


<details>
  <summary>Details</summary>
Motivation: 在大规模测评中，将题目准确对应到内容标准对于得分解释的有效性至关重要。人工对齐费时且容易出错，因此有必要探索自动化对齐方法，提高效率和准确性。

Method: 研究评估了三种自动化对齐范式：1）提取向量嵌入后训练多种经典的监督学习模型，并考察降维对模型效果的影响；2）微调八个BERT及其变体模型用于领域与技能标签的对齐；3）尝试多种元模型的集成学习方法，包括多数投票和堆叠。

Result: DeBERTa-v3-base在领域对齐上取得了加权平均F1分数0.950，RoBERTa-large在技能对齐上取得了F1分数0.869。集成模型未能超越表现最好的单一语言模型。降维提升了基于嵌入的线性分类器，但整体效果不如语言模型。

Conclusion: 自动化方法（尤其是基于先进语言模型的方法）在对齐题目与内容标准方面表现出极强的能力，且整体优于传统机器学习和集成学习方法。

Abstract: Accurate alignment of items to content standards is critical for valid score
interpretation in large-scale assessments. This study evaluates three automated
paradigms for aligning items with four domain and nineteen skill labels. First,
we extracted embeddings and trained multiple classical supervised machine
learning models, and further investigated the impact of dimensionality
reduction on model performance. Second, we fine-tuned eight BERT model and its
variants for both domain and skill alignment. Third, we explored ensemble
learning with majority voting and stacking with multiple meta-models. The
DeBERTa-v3-base achieved the highest weighted-average F1 score of 0.950 for
domain alignment while the RoBERTa-large yielded the highest F1 score of 0.869
for skill alignment. Ensemble models did not surpass the best-performing
language models. Dimension reduction enhanced linear classifiers based on
embeddings but did not perform better than language models. This study
demonstrated different methods in automated item alignment to content
standards.}

</details>


### [29] [Submodular Context Partitioning and Compression for In-Context Learning-short paper](https://arxiv.org/abs/2510.05130)
*Shaoyi Zheng,Canyu Zhang,Tianyi Zhou,Shengjie Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的上下文选择方法Sub-CP，通过灵活分块和目标函数实现信息充分高效利用，显著提升了ICL在大模型上的少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有高效ICL方法通常采用分块处理，但容易出现信息冗余或表示不足问题，降低性能。因此需要一种更灵活且能有效控制块内容多样性的上下文选择方法。

Method: 提出了Sub-CP，这是一个利用次模目标控制块多样性的分块感知上下文选择框架，能够灵活调整各块的选择策略，从全局多样到局部连贯，还支持预计算。

Result: 大量实验表明，Sub-CP在不同任务和数据集、不同模型规模下都能一致提升表现。

Conclusion: Sub-CP能有效提升在多任务和多数据集上的表现，破解现有分块策略因冗余和表示不足导致的性能瓶颈。

Abstract: In-context learning (ICL) enables efficient few-shot learning in large
language models (LLMs) without training, but suffers from the quadratic input
complexity of transformers, limiting the maximum number of exemplars. While
various efficient ICL approaches partition the context into blocks to process
(e.g., ensembling, compression, cross-attention), they often ignore the
information redundancy or under-representation caused by different partition
strategies, leading to suboptimal performance. To tackle this problem, we
propose Sub-CP, a block-aware context selection framework that leverages
submodular objectives to control block diversity. Sub-CP supports a flexible
spectrum of selection strategies, allowing each block to range from globally
diverse to locally coherent. This allows fine-grained control over semantic
structure while enabling precomputation. Extensive experiments across diverse
tasks on multiple datasets show that Sub-CP consistently improves performance
across model scales.

</details>


### [30] [Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery](https://arxiv.org/abs/2510.05131)
*Bowen Wei*

Main category: cs.CL

TL;DR: 为解决特定术语和词法检索问题，提出了混合语义搜索系统，并从技术、评测和经济性等方面详细阐述了可实施方案，有效提升使用体验和检索成功率。


<details>
  <summary>Details</summary>
Motivation: Head Start项目的新成员或轮班员工在GoEngage平台主页上查找合适模块时遇到困难，主要问题有领域专用术语、系统特定术语和传统词法检索对拼写错误、词序多样性的不足。

Method: 提出了一种混合语义检索系统：结合容错词法检索、基于嵌入的向量相似性和受限的大语言模型重新排序，依托既有任务库和知识库，通过智能缓存、候选列表生成和优雅降级机制保证经济性和可信度。

Result: 系统具备低误报率、可根据术语变化演进，以及经济高效的架构。提出了资源需求、分阶段实施、具体评估指标（如Hit@K、MRR等）以及在线和离线评测方法。

Conclusion: 该混合语义检索方案提升了查找效率与成功率，为组织带来更适应变动和维护成本更低的平台模块检索能力。

Abstract: Head Start programs utilizing GoEngage face significant challenges when new
or rotating staff attempt to locate appropriate Tasks (modules) on the platform
homepage. These difficulties arise from domain-specific jargon (e.g., IFPA,
DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent
limitations of lexical search in handling typos and varied word ordering. We
propose a pragmatic hybrid semantic search system that synergistically combines
lightweight typo-tolerant lexical retrieval, embedding-based vector similarity,
and constrained large language model (LLM) re-ranking. Our approach leverages
the organization's existing Task Repository and Knowledge Base infrastructure
while ensuring trustworthiness through low false-positive rates, evolvability
to accommodate terminological changes, and economic efficiency via intelligent
caching, shortlist generation, and graceful degradation mechanisms. We provide
a comprehensive framework detailing required resources, a phased implementation
strategy with concrete milestones, an offline evaluation protocol utilizing
curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online
measurement methodology incorporating query success metrics, zero-result rates,
and dwell-time proxies.

</details>


### [31] [Training Large Language Models To Reason In Parallel With Global Forking Tokens](https://arxiv.org/abs/2510.05132)
*Sheng Jia,Xiao Wang,Shiva Prasad Kasiviswanathan*

Main category: cs.CL

TL;DR: 针对推理多样性与准确性的矛盾，提出SSFT方法，能显著提升大型语言模型在复杂推理任务中的表现，多样性与准确性兼顾，优于传统SFT。


<details>
  <summary>Details</summary>
Motivation: 现有通过温度等手段提升推理多样性的做法，在准确性与多样性间存在较大矛盾，尤其对于复杂问题，促进多样但正确推理的分叉token往往位于采样树深处，常规方法难以有效挖掘和保持多样推理路径。

Method: 将并行推理视为“下一个token集合”的预测问题。利用自监督二部匹配方法，将全局分叉token与独特推理路径对应，设计了全局集合loss并融入到SFT中，形成新的Set Supervised Fine-Tuning方法。

Result: 在多个推理基准上，SSFT方法在Pass@1和Cons@k等指标上表现优于SFT，能够有效避免推理模式坍缩，促进分叉token和推理路径的多样性。

Conclusion: 提出的SSFT方法能更好地保留和激发多样化且准确的推理模式，相较于传统SFT在多个推理基准测试中表现更优。

Abstract: Although LLMs have demonstrated improved performance by scaling parallel
test-time compute, doing so relies on generating reasoning paths that are both
diverse and accurate. For challenging problems, the forking tokens that trigger
diverse yet correct reasoning modes are typically deep in the sampling tree.
Consequently, common strategies to encourage diversity, such as temperature
scaling, encounter a worsened trade-off between diversity and accuracy.
Motivated by this challenge, we treat parallel reasoning as a
set-of-next-token-prediction problem, and incorporate a set-based global loss
into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching
between our global forking tokens and unique reasoning traces. We observe that,
while naive fine-tuning with multiple reasoning traces collapses these unique
reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT),
preserves these modes and produces emergent global forking tokens. Experiments
on multiple reasoning benchmarks show that our SSFT consistently outperforms
SFT under both Pass@1 and Cons@k metrics.

</details>


### [32] [Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios](https://arxiv.org/abs/2510.05133)
*Y. Du,G. Wu,G. Tang,W. Wang,Q. Fan*

Main category: cs.CL

TL;DR: 合成数据在NLP训练中应用广泛，但比例过高会带来性能等方面风险。20%以内为安全区，大模型更稳健，校准下降可作为预警。论文给出以任务和模型规模为依据的合成数据使用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型生成的合成数据已广泛用于自然语言处理训练，但关于不同比例的合成数据对模型性能和行为的影响尚缺乏系统性了解。

Method: 构建了一个对照实证研究，使用Pythia模型套件（410M-12B参数）在五个任务上，测试合成数据占比（0-50%）对训练后性能、校准和输出特性影响。进行了一至三轮训练迭代。

Result: 合成数据比例达到20%以内，模型性能保持稳定；超过30%后性能下降加剧。大模型对合成数据更具鲁棒性。模型校准下降早于准确率下降。推理类任务比检索类任务更易因合成数据而性能劣化。

Conclusion: 实际应用中，只要合成数据不超过20%，且外部数据维持80%以上，模型训练体系基本安全。论文为不同规模模型和任务需求提供了合成数据配置指导，并与相关工作做了对比分析。

Abstract: Synthetic data generated by large language models has become integral to
modern NLP training pipelines, from bootstrapping reasoning capabilities to
augmenting instruction-following datasets. While recent work demonstrates
successful applications maintaining high external data ratios, systematic
understanding of how synthetic data proportion affects model behavior across
different scales remains limited. This paper presents a controlled empirical
study examining model performance, calibration, and output characteristics when
trained on varying synthetic-to-external data ratios. Using the Pythia model
suite (410M-12B parameters) across five diverse tasks, we evaluate models after
one to three training iterations with synthetic data proportions ranging from
0-50\%. Our key findings include: models maintain stable performance with up to
20\% synthetic data, but degradation accelerates beyond 30\%; larger models
(6.9B-12B) show greater robustness to synthetic data than smaller models
(410M-1.4B); calibration degradation precedes accuracy loss, providing an early
warning signal; and task characteristics matter, with reasoning tasks degrading
faster than retrieval tasks under synthetic data training. Importantly, we find
that current best practices, such as those employed in STaR and Self-Instruct
systems that maintain greater than 80\% external data, operate well within safe
regimes identified by our experiments. We provide practical guidance for
practitioners on synthetic data budgets based on model scale and task
requirements, alongside detailed comparison with concurrent work including
Shumailov et al.'s model collapse findings.

</details>


### [33] [Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment](https://arxiv.org/abs/2510.05135)
*Vanya Bannihatti Kumar,Divyanshu Goyal,Akhil Eppa,Neel Bhandari*

Main category: cs.CL

TL;DR: 本文提出通过好奇心驱动且个性化的LLM评审方法，提升了模型在创造性写作等主观复杂任务中的主观判断一致性和表现，优于传统微调方法，特别适合多评审者标准不一的场景。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在客观任务（如数理推理、事实准确性）表现突出，但在评估创造力这类主观、细腻任务时效果有限。现有方法难以适应个体化的创造力判断标准，因此亟需一种能个性化学习主观评判的方法。

Method: 提出了一种基于好奇心驱动且可个性化定制的LLM评审框架，用于创造性写作评估，并采用了Torrance Test of Creative Thinking (TTCW)基准，其数据包含专家对故事原创性等主观维度的标注。通过实验比较，使用相关性（Pearson）、一致性（Cohen's）和F1分数等多种评价指标，系统性评估了方法效果。

Result: 该方法在多个主观维度上的相关性、一致性及F1指标，均优于基线的监督微调法，不同规模的模型均显示出能学习个体细腻判断的能力，尤其在多评审者标准不一的主观评价任务中表现突出。

Conclusion: 提出的方法能够让不同规模的模型学习到个体对创造性判断的细腻标准，并在多个主观评测维度上优于传统的监督微调方法，尤其在评审者意见分歧明显的主观评价场景下更具优势。

Abstract: Modern large language models (LLMs) excel at objective tasks such as
evaluating mathematical reasoning and factual accuracy, yet they falter when
faced with the nuanced, subjective nature of assessing creativity. In this
work, we propose a novel curiosity-driven LLM-as-a-judge for evaluating
creative writing which is personlized to each individual's creative judgments.
We use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in
Chakrabarty et al. (2024), which has stories annotated by expert humans across
various subjective dimensions like Originality, to test our hypothesis. We show
that our method enables models across various sizes, to learn the nuanced
creative judgments of different individuals, by showing improvements over
baseline supervised finetuning(SFT) method across various evaluation metrics
like Pearson correlation, Cohen's and F1 values. Our method is especially
useful in subjective evaluations where not all the annotators agree with each
other.

</details>


### [34] [Linguistic Characteristics of AI-Generated Text: A Survey](https://arxiv.org/abs/2510.05136)
*Luka Terčon,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文综述了AI生成文本的语言特征研究，发现其风格更正式且词汇多样性低，现有工作集中于英语和GPT模型，提示词敏感性和跨语言分析是未来研究重点。


<details>
  <summary>Details</summary>
Motivation: 由于AI生成文本在多个领域应用广泛，深入了解其语言特征对于语言学、计算语言学和自然语言处理等领域具有重要意义，但此前研究缺乏系统性综合。

Method: 该论文对现有研究进行了综述，并按语言描述层级、模型、体裁、语言和提示方式对相关工作与发现进行分类整理。

Result: 发现AI文本通常更正式、客观，名词、限定词和介词使用比例高，形容词和副词比例低，词汇多样性和词汇量较低，且重复性强。此外，研究目前偏重于英语和GPT模型，对其他语言和模型的关注较少，提示词敏感性尚未充分探讨。

Conclusion: 当前关于AI生成文本的语言特征的研究集中在英语和GPT模型，且未充分考虑提示词敏感性，未来需要更广泛的跨语言、跨模型和多提示研究。

Abstract: Large language models (LLMs) are solidifying their position in the modern
world as effective tools for the automatic generation of text. Their use is
quickly becoming commonplace in fields such as education, healthcare, and
scientific research. There is a growing need to study the linguistic features
present in AI-generated text, as the increasing presence of such texts has
profound implications in various disciplines such as corpus linguistics,
computational linguistics, and natural language processing. Many observations
have already been made, however a broader synthesis of the findings made so far
is required to provide a better understanding of the topic. The present survey
paper aims to provide such a synthesis of extant research. We categorize the
existing works along several dimensions, including the levels of linguistic
description, the models included, the genres analyzed, the languages analyzed,
and the approach to prompting. Additionally, the same scheme is used to present
the findings made so far and expose the current trends followed by researchers.
Among the most-often reported findings is the observation that AI-generated
text is more likely to contain a more formal and impersonal style, signaled by
the increased presence of nouns, determiners, and adpositions and the lower
reliance on adjectives and adverbs. AI-generated text is also more likely to
feature a lower lexical diversity, a smaller vocabulary size, and repetitive
text. Current research, however, remains heavily concentrated on English data
and mostly on text generated by the GPT model family, highlighting the need for
broader cross-linguistic and cross-model investigation. In most cases authors
also fail to address the issue of prompt sensitivity, leaving much room for
future studies that employ multiple prompt wordings in the text generation
phase.

</details>


### [35] [Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics](https://arxiv.org/abs/2510.05137)
*Maojia Song,Renhang Liu,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Soujanya Poria,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出了WebDetective基准与综合评估体系，发现现有RAG和Web Agent模型自主推理能力薄弱，并通过EvidenceLoop框架证明诊断可引导模型改进，促进真正自主的推理系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统与Web Agent在多跳深度搜索任务评估中存在两个主要问题：1）多数基准测试在问题中泄露了推理路径，导致模型依赖表面线索而不是自主发现推理链；2）评估通常只有单一的通过率得分，难以区分失败源自检索不足、知识利用不佳还是不恰当拒绝。

Method: 提出WebDetective基准，含无提示的多跳问题和受控Wikipedia沙盒，实现模型行为的全面可追踪，并使用分离检索充分性、知识利用和拒绝行为的多维评估框架；还提出EvidenceLoop agentic workflow，引入验证循环和系统性证据追踪提升搜索与综合。

Result: 对25个SOTA模型评估发现，所有架构在知识利用上存在系统性短板，即便证据充分；在缺乏证据时很少做出恰当拒绝。提出的EvidenceLoop显著提升了模型的检索与综合能力。

Conclusion: 当前RAG模型擅长执行给定推理路径但难以自主发现推理链。WebDetective不仅能细致诊断模型弱点，其方法论能指导架构改进，有助于推动从模式匹配向自主推理系统的发展。

Abstract: RAG (Retrieval-Augmented Generation) systems and web agents are increasingly
evaluated on multi-hop deep search tasks, yet current practice suffers from two
major limitations. First, most benchmarks leak the reasoning path in the
question text, allowing models to follow surface cues rather than discover
reasoning chains autonomously. Second, evaluation is typically reduced to a
single pass rate, which collapses diverse behaviours into one score and
obscures whether failures stem from inadequate search, poor knowledge use, or
inappropriate refusal. To address these issues, we present WebDetective, a
benchmark of hint-free multi-hop questions paired with a controlled Wikipedia
sandbox that ensures full traceability of model actions, and a holistic
evaluation framework that separates search sufficiency, knowledge utilisation,
and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals
systematic weaknesses across all architectures: models struggle with knowledge
utilisation despite having sufficient evidence and demonstrate near-absent
appropriate refusal when evidence is lacking. These patterns expose a
fundamental gap: today's systems excel at executing given reasoning paths but
fail when required to discover them. We develop an agentic workflow,
EvidenceLoop, that explicitly targets the challenges our benchmark identifies,
incorporating verification loops and systematic evidence tracking that improve
both search and synthesis capabilities. This baseline demonstrates that
WebDetective's diagnostic framework can guide concrete architectural
improvements, establishing our benchmark as a critical tool for developing
genuinely autonomous reasoning systems rather than pattern-following agents.

</details>


### [36] [LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation](https://arxiv.org/abs/2510.05138)
*Gregory Hok Tjoan Go,Khang Ly,Anders Søgaard,Amin Tabatabaei,Maarten de Rijke,Xinyi Chen*

Main category: cs.CL

TL;DR: 作者提出LiRA多智能体协作流程，系统模拟人类文献综述写作，能够生成高质量、可读且全面的综述文章，性能优于现有工具，显示出自动科学写作领域的广阔应用前景。


<details>
  <summary>Details</summary>
Motivation: 科学文献数量快速增长，导致系统性文献综述难以保持全面和最新。以往自动化研究多聚焦检索与筛选，写作阶段（尤其是可读性与事实准确性）尚未充分探索。

Method: 提出LiRA（文献综述代理），即多智能体协作流程，模拟人类文献综述过程。LiRA包含内容提纲、分段写作、编辑、审稿等专职代理，形成连贯、全面的综述文章。

Result: 在SciReviewGen和ScienceDirect数据集上，LiRA写作与引用质量优于现有AutoSurvey和MASS-Survey基线，并在与人类综述的相似度上保持较高水平。进一步测试了LiRA在真实场景中的检索表现及对审稿模型变化的鲁棒性。

Conclusion: 无需领域特定微调，代理式大模型（LLM）流程可显著提升自动化科学写作的可靠性和实用性。

Abstract: The rapid growth of scientific publications has made it increasingly
difficult to keep literature reviews comprehensive and up-to-date. Though prior
work has focused on automating retrieval and screening, the writing phase of
systematic reviews remains largely under-explored, especially with regard to
readability and factual accuracy. To address this, we present LiRA (Literature
Review Agents), a multi-agent collaborative workflow which emulates the human
literature review process. LiRA utilizes specialized agents for content
outlining, subsection writing, editing, and reviewing, producing cohesive and
comprehensive review articles. Evaluated on SciReviewGen and a proprietary
ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey
and MASS-Survey in writing and citation quality, while maintaining competitive
similarity to human-written reviews. We further evaluate LiRA in real-world
scenarios using document retrieval and assess its robustness to reviewer model
variation. Our findings highlight the potential of agentic LLM workflows, even
without domain-specific tuning, to improve the reliability and usability of
automated scientific writing.

</details>


### [37] [NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description](https://arxiv.org/abs/2510.05139)
*Hamed Jelodar,Mohammad Meymani,Parisa Hamedi,Tochukwu Emmanuel Nwankwo,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.CL

TL;DR: 本文提出了NLD-LLM框架，对主流语言模型进行自然语言描述任务评测，展示了提示工程对提升模型表现的关键作用，即便是小规模模型在良好提示下也能获得高质量输出。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理任务中，高质量的自然语言描述尤其是代码描述的自动生成较为困难，亟需系统方法评估和提升不同语言模型在该任务中的表现。

Method: 提出了NLD-LLM系统框架，结合标准化格式、明确任务指导和NLD提示设计，对多种主流变换器模型（如Qwen、DeepSeek、Phi、LLaMA、Mistral）进行统一评价，并通过迭代优化输出质量与适应性。

Result: 各类模型经过统一和优化的提示设计后，输出质量和准确性显著提升，证明提示工程对模型性能贡献巨大，尤其能够提升小模型的竞争力。

Conclusion: 提示工程对语言模型在自然语言描述任务中的表现影响显著，精心设计的提示可以使较小的模型也能取得与大型模型媲美的效果。

Abstract: Natural Language Description (NLD) is a Natural Language Processing (NLP)
task that requires models to generate structured and meaningful outputs from
natural language inputs. In this work, we propose NLD-LLM, a systematic NLP
framework to evaluate the performance of language models to generate accurate
and concise source code descriptions. This framework incorporates a diverse set
of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral,
spanning various sizes, architectures, and training approaches. Central to
NLD-LLM is a comprehensive prompt design strategy that includes standardized
formatting, clear task guidance, and NLD prompting, ensuring fair and
consistent evaluation. Additionally, we apply an iterative refinement process
to improve output's quality and assess the model's adaptability. Using semantic
and structural metrics, our analysis demonstrates that prompt engineering
significantly impacts the effectiveness of the model such that smaller models
often performing competitively when supported by well-crafted prompts.

</details>


### [38] [To model human linguistic prediction, make LLMs less superhuman](https://arxiv.org/abs/2510.05141)
*Byung-Doh Oh,Tal Linzen*

Main category: cs.CL

TL;DR: 主流LLM因超强记忆能力，词预测远超人类，反而无法拟合人类真实阅读过程，亟需具备类人记忆的模型及新实验数据。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLM）在预测下一个词方面表现优异，引发了利用LLM作为人类语言预测认知模型的研究兴趣。

Method: 通过回顾LLM对词预测能力与人类阅读行为的拟合表现，并分析其与人类记忆机能（长时记忆和短时记忆）差异，提出调整模型记忆属性的建议。

Result: 当前LLM在语言理解上已表现出“超人类”水平，即对下一个词的预测远超人类，导致模型对人类语言处理难度的拟合变差。主要原因是LLM拥有远超人类的长时和短时记忆。

Conclusion: 需要开发拥有类人记忆（长时/短时记忆）能力的模型，并补充设计用于测量该目标的新型人类实验数据，以提升模型对人类语言过程的拟合能力。

Abstract: When people listen to or read a sentence, they actively make predictions
about upcoming words: words that are less predictable are generally read more
slowly than predictable ones. The success of large language models (LLMs),
which, like humans, make predictions about upcoming words, has motivated
exploring the use of these models as cognitive models of human linguistic
prediction. Surprisingly, in the last few years, as language models have become
better at predicting the next word, their ability to predict human reading
behavior has declined. This is because LLMs are able to predict upcoming words
much better than people can, leading them to predict lower processing
difficulty in reading than observed in human experiments; in other words,
mainstream LLMs are 'superhuman' as models of language comprehension. In this
position paper, we argue that LLMs' superhumanness is primarily driven by two
factors: compared to humans, LLMs have much stronger long-term memory for facts
and training examples, and they have much better short-term memory for previous
words in the text. We advocate for creating models that have human-like
long-term and short-term memory, and outline some possible directions for
achieving this goal. Finally, we argue that currently available human data is
insufficient to measure progress towards this goal, and outline human
experiments that can address this gap.

</details>


### [39] [Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models](https://arxiv.org/abs/2510.05142)
*Xin Wang,Anshu Raj,Matthew Luebbe,Haiming Wen,Shuozhi Xu,Kun Lu*

Main category: cs.CL

TL;DR: 本文提出了一个基于大语言模型的多阶段信息抽取流程，能从文献中高效准确提取材料成分、工艺、微观结构和性能的详细数据，显著提升了数据库的完整性与准确性，为材料科学的数据驱动发现提供有力支持。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的材料发现离不开大规模的实验数据集，但目前大多数信息还停留在非结构化的文献中，现有数据挖掘方法只能提取有限的特征，未能涵盖关键的成分-工艺-微观结构-性能一体化信息，无法满足构建全面数据库的需求。

Method: 本文提出一种基于大语言模型的多阶段信息抽取流程，能够从实验性材料文献中系统提取涵盖成分、处理过程、微观结构和性能的47个特征。采用迭代抽取与溯源机制，加强准确性和可靠性。

Result: 在特征级和元组级评估中均获得了约0.96的F1分数。与传统单步抽取方法相比，微观结构类别的F1分数提高了10%（特征级）和13.7%（元组级），遗漏材料数量显著降低，错漏率由12.4%降至3.3%。最终构建的数据集具有高精度、极低遗漏率和零误报。

Conclusion: 该流程实现了高效、可扩展的文献挖掘，为机器学习和材料信息学提供了可靠的原始数据，模块化设计适用于多种材料类别，推动综合材料信息抽取的发展。

Abstract: Data-driven materials discovery requires large-scale experimental datasets,
yet most of the information remains trapped in unstructured literature.
Existing extraction efforts often focus on a limited set of features and have
not addressed the integrated composition-processing-microstructure-property
relationships essential for understanding materials behavior, thereby posing
challenges for building comprehensive databases. To address this gap, we
propose a multi-stage information extraction pipeline powered by large language
models, which captures 47 features spanning composition, processing,
microstructure, and properties exclusively from experimentally reported
materials. The pipeline integrates iterative extraction with source tracking to
enhance both accuracy and reliability. Evaluations at the feature level
(independent attributes) and tuple level (interdependent features) yielded F1
scores around 0.96. Compared with single-pass extraction without source
tracking, our approach improved F1 scores of microstructure category by 10.0%
(feature level) and 13.7% (tuple level), and reduced missed materials from 49
to 13 out of 396 materials in 100 articles on precipitate-containing
multi-principal element alloys (miss rate reduced from 12.4% to 3.3%). The
pipeline enables scalable and efficient literature mining, producing databases
with high precision, minimal omissions, and zero false positives. These
datasets provide trustworthy inputs for machine learning and materials
informatics, while the modular design generalizes to diverse material classes,
enabling comprehensive materials information extraction.

</details>


### [40] [SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation](https://arxiv.org/abs/2510.05144)
*Muskaan Chopra,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文提出SynCED-EnDe英德机器翻译错误检测数据集，规模更大、标签均衡、多细粒度维度标记，性能显著优于旧有标准，将加速MT安全性研究与应用落地。


<details>
  <summary>Details</summary>
Motivation: 现有的WMT21英德CED数据集在规模、标签平衡、领域覆盖等方面存在局限性，难以满足机器翻译安全性检测的需求。

Method: 作者构建了新的SynCED-EnDe资源，包括1000条金标注和8000条银标注句对，来源涉及StackExchange、GOV.UK等2024-2025新数据，数据在错误和非错误情况上实现了均衡，并引入了详细的错误类型、结构化触发标记和细粒度辅助评判。

Result: 该数据集支持比传统二元检测更系统地分析错误风险和复杂度，基于XLM-R等模型的实验显示，在标签与注释质量提升后，性能较WMT21大幅提高。

Conclusion: SynCED-EnDe为社区提供了高质量、安全性检测的数据和基线工具，有助于推动机器翻译在信息检索、人机对话等新兴场景安全应用，尤其适用于可穿戴AI设备等领域。

Abstract: Critical Error Detection (CED) in machine translation aims to determine
whether a translation is safe to use or contains unacceptable deviations in
meaning. While the WMT21 English-German CED dataset provided the first
benchmark, it is limited in scale, label balance, domain coverage, and temporal
freshness. We present SynCED-EnDe, a new resource consisting of 1,000
gold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between
error and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources
(StackExchange, GOV.UK) and introduces explicit error subclasses, structured
trigger flags, and fine-grained auxiliary judgments (obviousness, severity,
localization complexity, contextual dependency, adequacy deviation). These
enrichments enable systematic analyses of error risk and intricacy beyond
binary detection. The dataset is permanently hosted on GitHub and Hugging Face,
accompanied by documentation, annotation guidelines, and baseline scripts.
Benchmark experiments with XLM-R and related encoders show substantial
performance gains over WMT21 due to balanced labels and refined annotations. We
envision SynCED-EnDe as a community resource to advance safe deployment of MT
in information retrieval and conversational assistants, particularly in
emerging contexts such as wearable AI devices.

</details>


### [41] [Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs](https://arxiv.org/abs/2510.05148)
*Qi Li,Runpeng Yu,Haiquan Lu,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文针对离散扩散大语言模型(dLLM)中的模型归属识别问题，提出了结构化轨迹提取机制DDM和分布归属算法GTA。新方法在多场景下显著提升归属准确性，充分挖掘了解码轨迹结构信息，扩展了dLLM实际应用边界。


<details>
  <summary>Details</summary>
Motivation: dLLMs作为一种新兴的非自回归语言建模范式，在推理速度和任务性能上表现优异。但在实际应用中，如何进行模型归属（判别模型来源和区分不同版本/备份）仍是重要难题。

Method: 作者分析了dLLM的解码轨迹，发现直接使用每步模型置信度效果较差，原因在于双向解码导致置信度冗余。为此，提出“Directed Decoding Map”(DDM)以提取结构化信息，并进一步提出“Gaussian-Trajectory Attribution(GTA)”方法，通过高斯分布拟合不同模型的轨迹，根据轨迹的对数似然得出归属分数，实现模型归属判别。

Result: 实验证明，在不同设置下，该方法能有效提升模型归属的实用性和准确性，优于直接置信度等传统方法。

Conclusion: dLLM独特的解码机制不仅提升模型效能，也为高效准确的模型归属判别提供了新工具。所提DDM和GTA方法具备广泛适用性和有效性。

Abstract: Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive paradigm for non-autoregressive language modeling. Their
distinctive decoding mechanism enables faster inference speed and strong
performance in code generation and mathematical tasks. In this work, we show
that the decoding mechanism of dLLMs not only enhances model utility but also
can be used as a powerful tool for model attribution. A key challenge in this
problem lies in the diversity of attribution scenarios, including
distinguishing between different models as well as between different
checkpoints or backups of the same model. To ensure broad applicability, we
identify two fundamental problems: what information to extract from the
decoding trajectory, and how to utilize it effectively. We first observe that
relying directly on per-step model confidence yields poor performance. This is
mainly due to the bidirectional decoding nature of dLLMs: each newly decoded
token influences the confidence of other decoded tokens, making model
confidence highly redundant and washing out structural signal regarding
decoding order or dependencies. To overcome this, we propose a novel
information extraction scheme called the Directed Decoding Map (DDM), which
captures structural relationships between decoding steps and better reveals
model-specific behaviors. Furthermore, to make full use of the extracted
structural information during attribution, we propose Gaussian-Trajectory
Attribution (GTA), where we fit a cell-wise Gaussian distribution at each
decoding position for each target model, and define the likelihood of a
trajectory as the attribution score: if a trajectory exhibits higher
log-likelihood under the distribution of a specific model, it is more likely to
have been generated by that model. Extensive experiments under different
settings validate the utility of our methods.

</details>


### [42] [Chronological Thinking in Full-Duplex Spoken Dialogue Language Models](https://arxiv.org/abs/2510.05150)
*Donghang Wu,Haoyang Zhang,Chen Chen,Tianyu Zhang,Fei Tian,Xuerui Yang,Gang Yu,Hexin Liu,Nana Hou,Yuchen Hu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 该工作提出“Chronological Thinking”机制，使语音对话模型在听用户发言时可边思考边推理，提升了全双工实时对话系统的响应质量和自然性。


<details>
  <summary>Details</summary>
Motivation: 当前口语对话模型多聚焦在全双工系统，即系统能边听用户说话边生成响应，实现实时互动。然而现有系统在听的阶段往往只反复预测静音token，模型表现为“发呆”，并不符合人类在对话时持续思考的行为。

Method: 提出了一种名为Chronological Thinking的实时对话式推理机制。该机制要求系统只利用已获得的（过去的）音频信息进行逐步增量地推理，并在用户结束讲话时迅速做出响应，无需额外延迟。与传统的Chain-of-Thought等思维方式相比，适应了流式音频场景。

Result: 实验表明，Chronological Thinking机制在客观指标和主观人工评估上均提升了响应质量，并可稳健应对对话过程中的动态变化，在全双工互动指标上也有竞争力表现。

Conclusion: Chronological Thinking为全双工口语对话模型提供了一种符合人类思维、低延迟、高质量响应的新范式，推动了人机实时自然对话的发展。

Abstract: Recent advances in spoken dialogue language models (SDLMs) reflect growing
interest in shifting from turn-based to full-duplex systems, where the models
continuously perceive user speech streams while generating responses. This
simultaneous listening and speaking design enables real-time interaction and
the agent can handle dynamic conversational behaviors like user barge-in.
However, during the listening phase, existing systems keep the agent idle by
repeatedly predicting the silence token, which departs from human behavior: we
usually engage in lightweight thinking during conversation rather than
remaining absent-minded. Inspired by this, we propose Chronological Thinking, a
on-the-fly conversational thinking mechanism that aims to improve response
quality in full-duplex SDLMs. Specifically, chronological thinking presents a
paradigm shift from conventional LLM thinking approaches, such as
Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly
causal: the agent reasons incrementally while listening, updating internal
hypotheses only from past audio with no lookahead. (2) No additional latency:
reasoning is amortized during the listening window; once the user stops
speaking, the agent halts thinking and begins speaking without further delay.
Experiments demonstrate the effectiveness of chronological thinking through
both objective metrics and human evaluations show consistent improvements in
response quality. Furthermore, chronological thinking robustly handles
conversational dynamics and attains competitive performance on full-duplex
interaction metrics.

</details>


### [43] [Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA](https://arxiv.org/abs/2510.05151)
*Prudence Djagba,Abdelkader Y. Saley*

Main category: cs.CL

TL;DR: 该文分析了金融专属LLM——FinMA，在情感分析和分类上效果好，但数值推理、实体识别和摘要任务表现有限，为未来金融LLM优化提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着金融领域对自然语言处理技术需求不断提升，研究专门针对金融场景优化的大型语言模型（LLMs）对于提升金融任务的准确性和可靠性具有重要意义。

Method: 本文主要分析了基于PIXIU框架构建的FinMA模型，通过金融指令微调（FIT数据集）和FLARE基准进行评估，探讨其在金融专属任务上的表现。

Result: 实验结果显示，FinMA在情感分析和分类任务中表现优异，但在数值推理、实体识别和摘要等任务上仍存在显著挑战。

Conclusion: 针对金融领域的LLM具备一定专用性优势，但在某些复杂金融任务上仍需进一步优化。本研究为金融领域LLM的设计和评估方法提供了参考。

Abstract: This research explores the strengths and weaknesses of domain-adapted Large
Language Models (LLMs) in the context of financial natural language processing
(NLP). The analysis centers on FinMA, a model created within the PIXIU
framework, which is evaluated for its performance in specialized financial
tasks. Recognizing the critical demands of accuracy, reliability, and domain
adaptation in financial applications, this study examines FinMA's model
architecture, its instruction tuning process utilizing the Financial
Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark.
Findings indicate that FinMA performs well in sentiment analysis and
classification, but faces notable challenges in tasks involving numerical
reasoning, entity recognition, and summarization. This work aims to advance the
understanding of how financial LLMs can be effectively designed and evaluated
to assist in finance-related decision-making processes.

</details>


### [44] [A Single Character can Make or Break Your LLM Evals](https://arxiv.org/abs/2510.05152)
*Jingtong Su,Jianyu Zhang,Karen Ullrich,Léon Bottou,Mark Ibrahim*

Main category: cs.CL

TL;DR: 大模型评估与应用中，示例间分隔符的选择会显著影响模型输出表现，进而左右模型排名和任务结果。建议在提示工程中明确分隔符，并采用最佳方案以增强模型鲁棒性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）评估通常依赖于示例的呈现方式来引导模型输出预期风格，但对于如何格式化示例（如使用逗号、换行、分号或其它分隔符），相关研究较少，实际应用中用户常面临分隔方式选择的问题。

Method: 作者通过对常用的大模型家族（Llama，Qwen，Gemma）在标准基准任务（如MMLU）上的表现进行实验，对比不同分隔符对模型输出的影响，并分析了注意力头对输入关键令牌的关注情况。同时，作者提出在提示中明确指定分隔符以及推荐最佳分隔符以提升鲁棒性。

Result: 实验发现，仅仅更改示例间的分隔符就能导致模型性能在MMLU任务上波动高达±23%；甚至可以通过调整分隔符改变模型排名。该脆弱性在各模型家族及多种主题上普遍存在，且随着模型规模增加并未显著改善。指定分隔符能够有效提升模型对分隔符的鲁棒性。

Conclusion: 分隔符选择对LLM输出质量影响巨大，是评估与实际应用中不可忽视的因素。推荐在实际操作中明确指定分隔符，并采用作者提出的最优分隔符方案以提高模型表现和一致性。

Abstract: Common Large Language model (LLM) evaluations rely on demonstration examples
to steer models' responses to the desired style. While the number of examples
used has been studied and standardized, the choice of how to format examples is
less investigated. In evaluation protocols and real world usage, users face the
choice how to separate in-context examples: use a comma? new line? semi-colon?
hashtag? etc.? Surprisingly, we find this seemingly minor choice can
dramatically alter model response quality. Across leading model families
(Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$
depending on the choice of delimiter. In fact, one can manipulate model
rankings to put any model in the lead by only modifying the single character
separating examples. We find LLMs' brittleness pervades topics, model families,
and doesn't improve with scale. By probing attention head scores, we find that
good-performing delimiters steer attention towards key tokens in the input.
Finally, we explore methods to improve LLMs' robustness to the choice of
delimiter. We find specifying the selected delimiter in the prompt boosts
robustness and offer practical recommendations for the best-performing
delimiters to select.

</details>


### [45] [Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs](https://arxiv.org/abs/2510.05154)
*Shenzhe Zhu,Shu Yang,Michiel A. Bakker,Alex Pentland,Jiaxin Pei*

Main category: cs.CL

TL;DR: 本文提出了DeliberationBank以及DeliberationJudge，以人类标注为基础，提高协商摘要的评估公平性，并揭示主流LLM在少数观点表达上的不足，对AI在公共政策中的应用具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 随着大型公共协商大量自由文本出现，如何生成具有代表性和中立性的总结以用于政策制定变得十分重要。然而，大型语言模型（LLM）在生成总结时，存在忽视少数观点和输入顺序偏见的问题，给高风险环境下的公平性带来挑战。

Method: 作者构建了DeliberationBank数据集，包含来自3,000名参与者的10个协商主题意见数据，以及4,500名参与者对摘要的多维度评价（代表性、信息量、中立性、政策认可）。据此，训练了DeliberationJudge，一个基于DeBERTa微调的模型，可以从个人视角评价协商摘要，并与多种LLM判评效果对比。

Result: DeliberationJudge判评效率更高、与人类判断一致性更好。通过该系统对18种LLM协商摘要能力进行评估，发现主流LLM在少数观点表达上仍有明显缺陷。

Conclusion: DeliberationJudge和DeliberationBank为协商摘要的评估提供了可扩展、可靠的方法，有助于提高AI在政策协商中的代表性与公平性。

Abstract: Large-scale public deliberations generate thousands of free-form
contributions that must be synthesized into representative and neutral
summaries for policy use. While LLMs have been shown as a promising tool to
generate summaries for large-scale deliberations, they also risk
underrepresenting minority perspectives and exhibiting bias with respect to the
input order, raising fairness concerns in high-stakes contexts. Studying and
fixing these issues requires a comprehensive evaluation at a large scale, yet
current practice often relies on LLMs as judges, which show weak alignment with
human judgments. To address this, we present DeliberationBank, a large-scale
human-grounded dataset with (1) opinion data spanning ten deliberation
questions created by 3,000 participants and (2) summary judgment data annotated
by 4,500 participants across four dimensions (representativeness,
informativeness, neutrality, policy approval). Using these datasets, we train
DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation
summaries from individual perspectives. DeliberationJudge is more efficient and
more aligned with human judgements compared to a wide range of LLM judges. With
DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in
deliberation summarization, especially underrepresentation of minority
positions. Our framework provides a scalable and reliable way to evaluate
deliberation summarization, helping ensure AI systems are more representative
and equitable for policymaking.

</details>


### [46] [A novel hallucination classification framework](https://arxiv.org/abs/2510.05189)
*Maksym Zavhorodnii,Dmytro Dehtiarov,Anna Konovalenko*

Main category: cs.CL

TL;DR: 提出用嵌入+无监督学习自动检测LLM幻觉，各类幻觉与真实输出在空间上清晰可分，简单分类即可可靠分辨，为模型可靠性改进提供实证基础。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）生成的内容中存在幻觉问题，即模型输出与事实偏离，如何自动检测这些幻觉内容成为提升模型可靠性的重要课题。

Method: 提出一种通过系统性分类和提示工程，控制性地生成各种幻觉类型，并对专门的幻觉数据集进行嵌入编码，利用无监督学习技术对其进行降维分析。进一步量化各类输出（幻觉与真实）之间的距离。

Result: 分析表明不实信息的幻觉与正确输出在嵌入空间中表现出明显的空间分离，且失真程度越高，分离越大。

Conclusion: 简单的分类算法即可有效区分同一模型中的幻觉内容与真实内容，为提高模型可靠性提供了轻量高效的解决框架。

Abstract: This work introduces a novel methodology for the automatic detection of
hallucinations generated during large language model (LLM) inference. The
proposed approach is based on a systematic taxonomy and controlled reproduction
of diverse hallucination types through prompt engineering. A dedicated
hallucination dataset is subsequently mapped into a vector space using an
embedding model and analyzed with unsupervised learning techniques in a
reduced-dimensional representation of hallucinations with veridical responses.
Quantitative evaluation of inter-centroid distances reveals a consistent
correlation between the severity of informational distortion in hallucinations
and their spatial divergence from the cluster of correct outputs. These
findings provide theoretical and empirical evidence that even simple
classification algorithms can reliably distinguish hallucinations from accurate
responses within a single LLM, thereby offering a lightweight yet effective
framework for improving model reliability.

</details>


### [47] [Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.05251)
*Chenghao Yang,Lin Gui,Chenxiao Yang,Victor Veitch,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.CL

TL;DR: 本文提出了递变退火采样（EAD）方法，通过动态调整采样温度，在序列生成早期促进探索，后期保证生成质量。该方法能在各类RLVR算法和大语言模型规模下提升推理能力和训练稳定性，优于传统固定温度采样。


<details>
  <summary>Details</summary>
Motivation: 固定温度采样在探索和样本质量之间难以取得平衡：高温度提升探索但降低质量，低温度则限制多样性。需要一种更优的探索策略来提升RLVR在LLM推理上的能力。

Method: 提出了一种新的采样策略——递变退火采样（Exploratory Annealed Decoding, EAD），通过在序列生成时将采样温度从高到低动态调整，实现“开头探索、结尾利用”的生成机制。

Result: EAD方法简单轻量，可直接应用于现有RLVR算法和模型。实验证明，不同算法和模型规模下，EAD都优于固定温度采样，提升了样本效率和训练表现。

Conclusion: 将探索与序列生成的自然动态对齐，可以大幅提升大语言模型（LLM）的推理能力和训练稳定性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm
for enhancing the reasoning capabilities of large language models (LLMs), yet
its success hinges on effective exploration. An ideal exploration strategy must
navigate two fundamental challenges: it must preserve sample quality while also
ensuring training stability. While standard fixed-temperature sampling is
simple, it struggles to balance these competing demands, as high temperatures
degrade sample quality and low temperatures limit discovery. In this work, we
propose a simpler and more effective strategy, Exploratory Annealed Decoding
(EAD), grounded in the insight that exploration is most impactful on early
tokens which define a sequence's semantic direction. EAD implements an
intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by
annealing the sampling temperature from high to low during generation. This
dynamic schedule encourages meaningful, high-level diversity at the start, then
gradually lowers the temperature to preserve sample quality and keep the
sampling distribution close to the target policy, which is essential for stable
training. We demonstrate that EAD is a lightweight, plug-and-play method that
significantly improves sample efficiency, consistently outperforming
fixed-temperature sampling across various RLVR algorithms and model sizes. Our
work suggests that aligning exploration with the natural dynamics of sequential
generation offers a robust path to improving LLM reasoning.

</details>


### [48] [Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages](https://arxiv.org/abs/2510.05291)
*Tarek Naous,Anagha Savit,Carlos Rafael Catalan,Geyang Guo,Jaehyeok Lee,Kyungdon Lee,Lheane Marie Dizon,Mengyu Ye,Neel Kothari,Sahajpreet Singh,Sarah Masud,Tanish Patwa,Trung Thanh Tran,Zohaib Khan,Alan Ritter,JinYeong Bak,Keisuke Sakaguchi,Tanmoy Chakraborty,Yuki Arase,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出了专注亚洲文化的 Camellia 基准，系统评测了多语言 LLM 在文化适应与实体抽取等任务中的表现，发现其存在明显的文化偏见和语境适应短板，尤其在亚洲语言环境下，更加突出。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）多语种能力的增强，对其处理不同文化实体的能力提出了更高要求。此前研究发现，LLMs 在阿拉伯语环境下偏向西方相关实体，引发了对文化公平性的担忧。然而，由于缺乏多语种基准，目前尚不清楚此类偏见在其他非西方语言中是否也会出现。

Method: 本研究提出了 Camellia 基准，用于衡量九种亚洲语言、六种亚洲文化中的实体中心文化偏见。Camellia 包含 19,530 个实体，均手工注释了其所关联的亚洲或西方文化特性，还包括 2,173 个来自社交媒体的实体自然语境蒙版。采用 Camellia，对四类主流多语言 LLM 家族在文化适应、情感关联、实体抽取问答等任务上的表现进行了系统评估。

Result: 通过 Camellia 基准的分析发现：LLMs 在所有亚洲语言的文化适应任务中表现欠佳，且模型表现与研发区域所拥有的文化相关数据获取能力相关。另外，不同 LLM 家族拥有各自独特的文化偏见，尤其在文化情感关联上差异明显。此外，LLMs 在亚洲语言语境理解方面存在困难，导致不同文化间的实体抽取性能出现显著差距。

Conclusion: 目前多语言 LLM 仍存在较大的文化偏见和语境理解不足的问题，尤其在亚洲语言与文化实体处理上表现明显。这些问题和性能差距的根源与数据获取和模型设计相关，亟需针对性改进与优化。

Abstract: As Large Language Models (LLMs) gain stronger multilingual capabilities,
their ability to handle culturally diverse entities becomes crucial. Prior work
has shown that LLMs often favor Western-associated entities in Arabic, raising
concerns about cultural fairness. Due to the lack of multilingual benchmarks,
it remains unclear if such biases also manifest in different non-Western
languages. In this paper, we introduce Camellia, a benchmark for measuring
entity-centric cultural biases in nine Asian languages spanning six distinct
Asian cultures. Camellia includes 19,530 entities manually annotated for
association with the specific Asian or Western culture, as well as 2,173
naturally occurring masked contexts for entities derived from social media
posts. Using Camellia, we evaluate cultural biases in four recent multilingual
LLM families across various tasks such as cultural context adaptation,
sentiment association, and entity extractive QA. Our analyses show a struggle
by LLMs at cultural adaptation in all Asian languages, with performance
differing across models developed in regions with varying access to
culturally-relevant data. We further observe that different LLM families hold
their distinct biases, differing in how they associate cultures with particular
sentiments. Lastly, we find that LLMs struggle with context understanding in
Asian languages, creating performance gaps between cultures in entity
extraction.

</details>


### [49] [RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts](https://arxiv.org/abs/2510.05310)
*Yining She,Daniel W. Peterson,Marianne Menglin Liu,Vikas Upadhyay,Mohammad Hossein Chaghazardi,Eunsuk Kang,Dan Roth*

Main category: cs.CL

TL;DR: 外部LLM防护模型在面对检索式补充信息时，安全判定易被影响，具备明显鲁棒性问题，现有缓解措施效果有限，需进一步提升上下文适应能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被广泛应用，其安全性问题变得尤为重要。为了防止不安全输入输出，外部基于LLM的防护模型（guardrail）被广泛采用，但这些模型容易受到数据分布变动的影响。本文通过RAG案例，探究此类guardrail的鲁棒性问题。

Method: 系统性评估了3个Llama Guard和2个GPT-oss模型，向其上下文中嵌入额外的良性文档，并分别分析上下文各组成部分对防护判定的影响，还测试了两种缓解方法。

Result: 约11%输入判定和8%输出判定因插入良性文档而改变，说明现有guardrail模型在上下文鲁棒性方面存在缺陷。两种缓解方法仅带来轻微改进。

Conclusion: 现有LLM-based guardrail存在上下文鲁棒性短板，亟需开发更稳健的训练与评估方案，应对信息检索与查询组合的挑战。

Abstract: With the increasing adoption of large language models (LLMs), ensuring the
safety of LLM systems has become a pressing concern. External LLM-based
guardrail models have emerged as a popular solution to screen unsafe inputs and
outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are
vulnerable to data distribution shifts. In this paper, taking Retrieval
Augmentation Generation (RAG) as a case study, we investigated how robust
LLM-based guardrails are against additional information embedded in the
context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss
models, we confirmed that inserting benign documents into the guardrail context
alters the judgments of input and output guardrails in around 11% and 8% of
cases, making them unreliable. We separately analyzed the effect of each
component in the augmented context: retrieved documents, user query, and
LLM-generated response. The two mitigation methods we tested only bring minor
improvements. These results expose a context-robustness gap in current
guardrails and motivate training and evaluation protocols that are robust to
retrieval and query composition.

</details>


### [50] [WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives](https://arxiv.org/abs/2510.05336)
*Yongan Yu,Xianda Du,Qingchen Hu,Jiahao Liang,Jingwei Ni,Dan Qiang,Kaiyu Huang,Grant McKenzie,Renee Sieber,Fengran Mo*

Main category: cs.CL

TL;DR: 提出了首个评估历史天气档案检索与分析的基准WeatherArchive-Bench，并展示了当前主流检索与大模型在理解社会脆弱性方面的不足，推动了气候研究方法的进步。


<details>
  <summary>Details</summary>
Motivation: 气候科学家需要理解社会对极端天气事件的反应，而历史气象文档中蕴含了丰富却未被充分利用的相关信息。但这些文档的规模庞大、数字化质量较差且语言古老，难以转化为气候研究所需的结构化知识，因此迫切需要新的系统评估与处理方法。

Method: 提出了WeatherArchive-Bench，这是首个用于评估基于检索增强生成（RAG）系统在历史天气档案处理能力的基准。该基准包括两个任务：档案检索（测试系统从百万级新闻档案中检索相关段落的能力）和档案评估（评估大型语言模型对极端天气叙事中的社会脆弱性和韧性指标的分类能力）。对不同类型的检索器及多种LLM进行了广泛实验。

Result: 密集检索器在处理历史术语时表现不佳，LLM也常常误解社会脆弱性和韧性的相关概念。这表明现有方法在复杂社会指标推理方面存在明显局限性。

Conclusion: 该研究揭示了当前RAG系统在处理历史档案并理解社会气候响应方面的不足，为今后设计更强健的气候研究工具提供了方向，并公开了相关数据集和评估框架。

Abstract: Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

</details>


### [51] [Residualized Similarity for Faithfully Explainable Authorship Verification](https://arxiv.org/abs/2510.05362)
*Peter Zeng,Pegah Alipoormolabashi,Jihu Mun,Gourab Dey,Nikita Soni,Niranjan Balasubramanian,Owen Rambow,H. Schwartz*

Main category: cs.CL

TL;DR: 本文针对作者身份验证任务，提出RS方法，将神经网络与可解释特征系统结合，提升模型准确性并保持解释性。实验证明，该方法在四个数据集上与当前最佳模型持平，同时预测过程高度可解释。


<details>
  <summary>Details</summary>
Motivation: 现有的作者身份验证系统虽然准确率高，但缺乏可解释性，尤其是神经网络和大型语言模型（LLM），其预测过程不透明，难以用于现实中的决策支持。作者希望提升模型的可解释性和预测的可追溯性。

Method: 提出了Residualized Similarity（RS）方法，通过在可解释特征系统的基础上，结合神经网络来预测残差（即可解释系统预测的相似度误差），以提升性能同时保持可解释性。

Result: 在四个数据集上的评估显示，该方法不仅能达到当前最先进作者身份验证模型的性能，还能展现最终预测过程的忠实性和可解释性。

Conclusion: RS方法兼顾了高性能和强可解释性，为实际应用的作者身份验证系统提供了可靠且可追溯的解决方案。

Abstract: Responsible use of Authorship Verification (AV) systems not only requires
high accuracy but also interpretable solutions. More importantly, for systems
to be used to make decisions with real-world consequences requires the model's
prediction to be explainable using interpretable features that can be traced to
the original texts. Neural methods achieve high accuracies, but their
representations lack direct interpretability. Furthermore, LLM predictions
cannot be explained faithfully -- if there is an explanation given for a
prediction, it doesn't represent the reasoning process behind the model's
prediction. In this paper, we introduce Residualized Similarity (RS), a novel
method that supplements systems using interpretable features with a neural
network to improve their performance while maintaining interpretability.
Authorship verification is fundamentally a similarity task, where the goal is
to measure how alike two documents are. The key idea is to use the neural
network to predict a similarity residual, i.e. the error in the similarity
predicted by the interpretable system. Our evaluation across four datasets
shows that not only can we match the performance of state-of-the-art authorship
verification models, but we can show how and to what degree the final
prediction is faithful and interpretable.

</details>


### [52] [The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures](https://arxiv.org/abs/2510.05364)
*Alexander M. Fichtl,Jeremias Bohn,Josefin Kelber,Edoardo Mosca,Georg Groh*

Main category: cs.CL

TL;DR: 本文系统综述了近年来为解决变换器模型在长序列任务时计算瓶颈所提出的前沿替代方案，包括低复杂度注意力、RNN、状态空间模型等，并分析了这些方法的性能、局限及未来挑战变换器主导地位的可能性。


<details>
  <summary>Details</summary>
Motivation: 关注变换器注意力机制在处理长序列时计算和内存上的瓶颈，并探索解决方案，评估这些方法能否挑战现有架构的主导地位。

Method: 系统性地回顾了最近克服注意力机制计算瓶颈的研究进展，包括次二次复杂度注意力变体、循环神经网络、状态空间模型及混合架构。并从计算、内存复杂度、基准结果及其基本局限性等方面进行了分析和对比。

Result: 综述了多种解决长序列瓶颈的新模型及方法，比较了其性能与局限，提出现有技术正在逐步接近但尚未完全超越变换器的统治地位。

Conclusion: 本文认为虽然变换器在序列处理任务上表现出色，但其注意力机制的计算复杂度在处理长文本时依然是瓶颈。通过对现有替代方案进行分析，作者认为纯注意力机制的变换器未来有可能被其他结构挑战。

Abstract: Transformers have dominated sequence processing tasks for the past seven
years -- most notably language modeling. However, the inherent quadratic
complexity of their attention mechanism remains a significant bottleneck as
context length increases. This paper surveys recent efforts to overcome this
bottleneck, including advances in (sub-quadratic) attention variants, recurrent
neural networks, state space models, and hybrid architectures. We critically
analyze these approaches in terms of compute and memory complexity, benchmark
results, and fundamental limitations to assess whether the dominance of
pure-attention transformers may soon be challenged.

</details>


### [53] [Context Length Alone Hurts LLM Performance Despite Perfect Retrieval](https://arxiv.org/abs/2510.05381)
*Yufeng Du,Minyang Tian,Srikanth Ronanki,Subendhu Rongali,Sravan Bodapati,Aram Galstyan,Azton Wells,Roy Schwartz,Eliu A Huerta,Hao Peng*

Main category: cs.CL

TL;DR: 即使解决了长文本检索问题，LLM性能依然会随输入变长而下降。作者发现输入长度本身是不容忽视的瓶颈，并提出用“复述证据再作答”的方法有效缓解此问题，实测获得显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在长文本任务中的表现，并未随着其支持的文本长度线性提升。通常认为这主要是因为模型检索相关信息的能力有限。因此，近期研究主要聚焦于提升模型的检索性能，假定如果检索完美，则长文本任务效果应接近短文本任务。然而，作者质疑这一假设。

Method: 作者在5种开源和闭源LLM上，针对数学、问答和编程任务进行了系统性实验证明：即使模型能够完美检索全部相关信息，仍然随着输入长度的增加（仍在模型声明的最大长度范围内），表现显著下降。此外，无关信息被空白字符替换或全部被掩盖（只保留相关内容）时，性能下降依旧存在。将相关证据全部放在问题前，也发现了类似表现下降。

Result: 实验发现，随着输入长度增加，模型性能下降高达13.9%--85%，且这种下降与检索能力及干扰无关，仅与输入长度有关。作者提出了一种简单、模型无关的方法：将长文本任务转化为短文本任务，即先让模型复述检索到的相关证据再作答。在RULER数据集上，GPT-4o模型表现提升了最多4%。

Conclusion: 长文本本身会损害LLM性能，与检索和干扰无关。通过结构化地整理输入，可以缓解性能下降问题。本文提出的方法在实际测试中对模型已有表现有可观提升。

Abstract: Large language models (LLMs) often fail to scale their performance on
long-context tasks performance in line with the context lengths they support.
This gap is commonly attributed to retrieval failures -- the models' inability
to identify relevant information in the long inputs. Accordingly, recent
efforts often focus on evaluating and improving LLMs' retrieval performance: if
retrieval is perfect, a model should, in principle, perform just as well on a
long input as it does on a short one -- or should it? This paper presents
findings that the answer to this question may be negative. Our systematic
experiments across 5 open- and closed-source LLMs on math, question answering,
and coding tasks reveal that, even when models can perfectly retrieve all
relevant information, their performance still degrades substantially
(13.9%--85%) as input length increases but remains well within the models'
claimed lengths. This failure occurs even when the irrelevant tokens are
replaced with minimally distracting whitespace, and, more surprisingly, when
they are all masked and the models are forced to attend only to the relevant
tokens. A similar performance drop is observed when all relevant evidence is
placed immediately before the question. Our findings reveal a
previously-unrealized limitation: the sheer length of the input alone can hurt
LLM performance, independent of retrieval quality and without any distraction.
They motivate our simple, model-agnostic mitigation strategy that transforms a
long-context task into a short-context one by prompting the model to recite the
retrieved evidence before attempting to solve the problem. On RULER, we observe
a consistent improvement of GPT-4o up to 4% on an already strong baseline.

</details>


### [54] [Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation](https://arxiv.org/abs/2510.05387)
*Ananth Kandala,Ratna Kandala,Akshata Kishore Moharir,Niva Manchanda,Sunaina Singh*

Main category: cs.CL

TL;DR: 文章提出 CL-PDE 图谱框架，实现印度多语种心理压力表达的捕捉与本地化，为多语种、文化多样的心理健康 NLP 系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有健康本体和心理健康资源以英文及西方诊断为主，忽视印度等多语言、多文化环境下患者的真实表达，导致临床 NLP 工具在当地语境下表现欠佳。

Method: 提出了跨语言患者压力表达图谱（CL-PDE）框架，采用图结构方式捕捉文化嵌入的压力表达，并对不同语言的表达进行对齐，同时关联临床术语，实现跨语种知识融合。

Result: 有效弥补了多语境下心理健康交流的鸿沟，为 AI 驱动的心理健康 NLP 工具提供了更丰富的文化和语言基础，使工具更加包容和患者导向。

Conclusion: 通过跨语言图谱方法（CL-PDE），能够更好地表现印度多语言环境下患者压力表达，加强临床术语与本地文化表达的对接。该框架有助于构建更加包容和以患者为中心的多语种心理健康 NLP 系统。

Abstract: Mental health communication in India is linguistically fragmented, culturally
diverse, and often underrepresented in clinical NLP. Current health ontologies
and mental health resources are dominated by diagnostic frameworks centered on
English or Western culture, leaving a gap in representing patient distress
expressions in Indian languages. We propose cross-linguistic graphs of patient
stress expressions (CL-PDE), a framework for building cross-lingual mental
health ontologies through graph-based methods that capture culturally embedded
expressions of distress, align them across languages, and link them with
clinical terminology. Our approach addresses critical gaps in healthcare
communication by grounding AI systems in culturally valid representations,
allowing more inclusive and patient-centric NLP tools for mental health care in
multilingual contexts.

</details>


### [55] [Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care](https://arxiv.org/abs/2510.05410)
*Junyi Fan,Li Sun,Negin Ashrafi,Kamiar Alaei,Maryam Pishgar*

Main category: cs.CL

TL;DR: 本文通过DPO方法提升本地化AI模型对护理文档的生成质量，显著优化了ICU心衰护理记录的准确性与标准化，有助于推动临床AI文档工具落地。


<details>
  <summary>Details</summary>
Motivation: 重症监护室（ICU）护理文档经常存在术语不一致、表述随意及缺乏标准化问题，尤其在心力衰竭护理中影响较大。提升护理记录的规范化和准确性对于提升护理质量和患者安全至关重要。

Method: 研究采用Direct Preference Optimization（DPO）方法，针对本地可部署的大语言模型Mistral-7B进行微调。具体用8,838份MIMIC-III数据库中的心衰护理记录及21,210对由专家验证的GPT输出、模型生成结果与原始记录组成的偏好对训练模型。效果评估包括BLEU、ROUGE、BERTScore、困惑度以及专家定性评价。

Result: DPO显著提升了护理文档的质量。BLEU分数提升84%（从0.173到0.318），BERTScore提升7.6%（从0.828到0.891），在准确性、完整性、逻辑一致性、可读性与结构清晰度等专家评价中均有明显提升。

Conclusion: DPO方法可以使轻量临床语言模型强效对齐专家标准，实现隐私友好的AI辅助护理记录，降低管理负担并提升ICU患者安全。

Abstract: Nursing documentation in intensive care units (ICUs) provides essential
clinical intelligence but often suffers from inconsistent terminology, informal
styles, and lack of standardization, challenges that are particularly critical
in heart failure care. This study applies Direct Preference Optimization (DPO)
to adapt Mistral-7B, a locally deployable language model, using 8,838 heart
failure nursing notes from the MIMIC-III database and 21,210 preference pairs
derived from expert-verified GPT outputs, model generations, and original
notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert
qualitative assessments demonstrates that DPO markedly enhances documentation
quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore
improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy
(+14.4 points), completeness (+14.5 points), logical consistency (+14.1
points), readability (+11.1 points), and structural clarity (+6.0 points).
These results indicate that DPO can align lightweight clinical language models
with expert standards, supporting privacy-preserving, AI-assisted documentation
within electronic health record systems to reduce administrative burden and
improve ICU patient safety.

</details>


### [56] [A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis](https://arxiv.org/abs/2510.05414)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Haifeng Wang,Minghui Cheng*

Main category: cs.CL

TL;DR: 该文提出基于Llama-3.3 70B的多智能体系统，自动完成结构工程有限元建模流程，准确率显著优于现有主流大模型，推动结构工程领域的智能化自动化进程。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型已用于提升工程自动化，但在结构工程领域，尤其是有限元分析的几何建模及复杂推理上的应用仍有限。因此，作者希望填补LLM在结构工程中自动化有限元建模的应用空白。

Method: 提出一种基于大型语言模型的多智能体系统，利用Llama-3.3 70B Instruct模型，将结构分析流程分解为若干子任务，分别交由特定智能体完成，并最终自动生成和校验有效的有限元模型代码。

Result: 在20组基准问题和10次重复实验中，该系统准确率在大多数情况下超过80%，优于Gemini-2.5 Pro和ChatGPT-4o模型。

Conclusion: LLM驱动的多智能体系统能够高效、自动化地完成结构工程有限元建模任务，在准确性和性能上显著优于现有主流大型语言模型，展示了LLM在结构工程自动化中的广阔应用前景。

Abstract: Large language models (LLMs) have recently been used to empower autonomous
agents in engineering, significantly improving automation and efficiency in
labor-intensive workflows. However, their potential remains underexplored in
structural engineering, particularly for finite element modeling tasks
requiring geometric modeling, complex reasoning, and domain knowledge. To
bridge this gap, this paper develops a LLM-based multi-agent system to automate
finite element modeling of 2D frames. The system decomposes structural analysis
into subtasks, each managed by a specialized agent powered by the lightweight
Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis
Agent, which extracts geometry, boundary, and material parameters from the user
input. Next, a Geometry Agent incrementally derives node coordinates and
element connectivity by applying expert-defined rules. These structured outputs
are converted into executable OpenSeesPy code by a Translation Agent and
refined by a Model Validation Agent through consistency checks. Then, a Load
Agent applies load conditions into the assembled structural model. Experimental
evaluations on 20 benchmark problems demonstrate that the system achieves
accuracy over 80% in most cases across 10 repeated trials, outperforming
Gemini-2.5 Pro and ChatGPT-4o models.

</details>


### [57] [Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification](https://arxiv.org/abs/2510.05431)
*Yoo Yongmin,Zhang Xu,Cao Longbing*

Main category: cs.CL

TL;DR: 本文提出了通过自过滤蒸馏方法对LLM推理文本进行置信度加权和筛选，有效提升专利分类准确性和鲁棒性，优于传统监督和蒸馏方法，开创了基于推理信号的可靠监督新路径。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）生成的推理文本常存在逻辑错误、标签不匹配和专业领域误差，直接用这些推理作为监督信号可能带来噪声，影响训练稳定性。作者希望解决推理文本带来的训练风险问题，提升模型在专利分类任务上的可靠性。

Method: 提出了一种名为“自过滤蒸馏”（Self-Filtered Distillation）的新框架。该方法把LLM生成的推理看作置信信号而不是真实标签。用三种无监督置信度指标对推理文本进行选择性蒸馏：1）自一致性，2）类别蕴涵对齐，3）LLM一致性评分，三者整合成统一置信分数，在训练中主要对高置信样本加权，极低置信样本可过滤，从而实现基于推理文本的可靠监督。

Result: 在专利分类基准数据集USPTO-2M上实验表明，该方法在准确率、稳定性和可解释性等方面都优于仅用标签监督和传统蒸馏方法，显著提升专利分类模型的表现。

Conclusion: 自过滤蒸馏框架有效利用LLM推理中的置信信号，提高了专利分类任务中的准确性、训练稳定性和模型解释性，为领域推理信号可信应用奠定了新范式。

Abstract: Large language models (LLMs) increasingly generate natural language
rationales to enhance interpretability, but these often contain logical errors,
label mismatches, and domain-specific misalignments. Directly using such
rationales as supervision risks propagating noise and undermining training
stability. To address this challenge, we introduce Self-Filtered Distillation,
a framework specifically tailored for patent classification, which treats
LLM-generated rationales as trust signals rather than ground-truth supervision.
The framework employs selective distillation guided by three unsupervised trust
metrics: (1) Self-Consistency, which measures the stability of LLM-generated
rationales across multiple generations; (2) Class Entailment Alignment, which
assesses semantic coherence with patent-specific class definitions; and (3) LLM
Agreement Scoring, which validates rationale-label plausibility. These metrics
are integrated into a unified trust score that primarily weights training
samples while optionally filtering out extremely low-trust cases, enabling
reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used
benchmark for patent classification, show that our method outperforms
label-based learning and conventional distillation in accuracy, stability, and
interpretability, establishing a reliable paradigm for leveraging
reasoning-aware trust indicators in patent analytics.

</details>


### [58] [SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?](https://arxiv.org/abs/2510.05444)
*Yao Dou,Michel Galley,Baolin Peng,Chris Kedzie,Weixin Cai,Alan Ritter,Chris Quirk,Wei Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 提出SimulatorArena基准，证明精心设计的LLM模拟用户能高度贴近真实用户行为和评价，成为可靠且可扩展的助手评估新路径。


<details>
  <summary>Details</summary>
Motivation: 当前多轮对话中大型语言模型（LLMs）评估主要依赖人工，但人工评估昂贵、耗时且难以复现，因此需要自动化、可靠的用户模拟方法作为替代。

Method: 提出SimulatorArena基准，包括909个人类-LLM对话样本，涵盖数学辅导和文档创建任务；比对模拟用户与真实用户的行为和评价一致性，尝试多种模拟器，评测其效果。

Result: 发现基于用户画像（包括背景和消息风格）的模拟器，其行为和评价与真实用户高度一致，在两项任务上Spearman相关系数达到0.7。选用最佳模拟器，完成对18款助手（包括GPT-5、Claude 4.1 Opus、Gemini 2.5 Pro等）的基准评测。

Conclusion: 模拟器尤其是用户画像驱动的方案，可以有效替代人工评估，显著提升多轮对话助手的自动化评测效率和可扩展性。

Abstract: Large language models (LLMs) are increasingly used in interactive
applications, and human evaluation remains the gold standard for assessing
their performance in multi-turn conversations. Since human studies are costly,
time-consuming, and hard to reproduce, recent work explores using LLMs to
simulate users for automatic assistant evaluation. However, there is no
benchmark or systematic study to evaluate whether these simulated users are
reliable stand-ins for real users. To address this, we introduce
SimulatorArena, a benchmark of 909 annotated human-LLM conversations on two
interactive tasks -- math tutoring and document creation. SimulatorArena
evaluates simulators based on how closely their messages match human behavior
and how well their assistant ratings align with human judgments. Experiments on
various simulator methods show that simulators conditioned on user profiles,
capturing traits like background and message styles, align closely with human
judgments. They reach Spearman's $\rho$ of 0.7 on both tasks, providing a
practical, scalable alternative to human evaluation. Using the best simulator
for each task, we benchmark 18 assistants, including the latest LLMs such as
GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.

</details>


### [59] [AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering](https://arxiv.org/abs/2510.05445)
*Zheyuan Zhang,Kaiwen Shi,Zhengqing Yuan,Zehong Wang,Tianyi Ma,Keerthiram Murugesan,Vincent Galassi,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 作者提出了tAgentRouter，通过知识图与异构GNN实现多agent问答路由，实验显示其优于单模型和集成，方法具有效能与泛化优势。


<details>
  <summary>Details</summary>
Motivation: 当前大模型和基于agent的方法快速发展，不同模型和agent具有互补优势，但如何选择最佳组合用于下游问答任务存在不确定性。传统agent路由方法强调成本，忽略了问答任务中精细的上下文和关系结构。

Method: 提出tAgentRouter框架，将多agent问答建模为由知识图引导的路由问题，并通过经验证据进行监督。具体做法是将问答样例转化为包含查询、上下文实体及agent的知识图结构，并利用异构图神经网络(GNN)传播信息、生成针对任务的agent分配。采用软监督和加权聚合输出，实现agent协作。

Result: 实验表明tAgentRouter在多个基准数据与多种LLM主干上均优于单agent和集成基线，表现出更好的泛化能力与稳健性。

Conclusion: 基于知识图监督的多agent路由策略在问答任务中能够有效利用多agent互补性，显著提升性能和泛化能力，优于传统或简单集成方法。

Abstract: Large language models (LLMs) and agent-based frameworks have advanced
rapidly, enabling diverse applications. Yet, with the proliferation of models
and agentic strategies, practitioners face substantial uncertainty in selecting
the best configuration for a downstream task. Prior studies show that different
agents and backbones exhibit complementary strengths, and that larger models
are not always superior, underscoring the need for adaptive routing mechanisms.
Existing approaches to agent routing, however, often emphasize cost efficiency
while overlooking the fine-grained contextual and relational structure inherent
in QA tasks. In this paper, we propose tAgentRouter, a framework that
formulates multi-agent QA as a knowledge-graph-guided routing problem
supervised by empirical performance signals. Specifically, we convert QA
instance into a knowledge graph that jointly encodes queries, contextual
entities, and agents, and then train a heterogeneous graph neural network (GNN)
to propagate information across node types and produce task-aware routing
distributions over agents. By leveraging soft supervision and weighted
aggregation of agent outputs, AgentRouter learns principled collaboration
schemes that capture the complementary strengths of diverse agents. Extensive
experiments demonstrate that our framework consistently outperforms
single-agent and ensemble baselines, while generalizing across benchmarks and
LLM backbones. These results highlight the effectiveness and robustness of
graph-supervised multi-agent routing for question answering.

</details>


### [60] [SocialNLI: A Dialogue-Centric Social Inference Dataset](https://arxiv.org/abs/2510.05458)
*Akhil Deo,Kate Sanders,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文发布了一个针对讽刺、反语等复杂社会现象的对话推理数据集，并用其评估现有模型的理论心智能力，找出了模型目前的弱点，并为提升AI社交能力提出了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以理解对话中的复杂社会现象（如讽刺和反语），而这类推理能力对于具备社会交互能力的AI助手至关重要。

Method: 作者提出了SocialNLI (SoNLI)数据集，专注于包含复杂社会细微差别（如讽刺、反语）的对话，配对推理结论、可能性评分及人工解释。利用该数据集，通过多步反事实推理分析理论心智（ToM）能力，并评估当前大语言及推理模型在理论心智上的水平。

Result: 建立了第一个关注社会对话推理的数据集，系统分析了现有模型在理解复杂社会现象方面的不足，并提供了用于评估和未来改进模型的新基准。

Conclusion: SoNLI数据集有效揭示了当前模型在理论心智推理上的局限，尤其是在应对讽刺和反语等社会现象时，为推动AI社交智能发展打下了基础。

Abstract: Making theory-of-mind inferences from human dialogue is a strong indicator of
a model's underlying social abilities, which are fundamental for adept AI
assistants. However, large language and reasoning models struggle to understand
sophisticated social phenomena in transcript data, such as sarcasm and irony.
To assess the weaknesses of current models and to identify their solutions, we
introduce SocialNLI (SoNLI) -- the first social dialogue inference dataset.
SoNLI consists of a collection of dialogue transcripts hand-picked to center
complex social nuances like irony and sarcasm, paired with inferences,
corresponding likelihood scores, and human-written explanations. We explore
social inference analysis as a facet of theory-of-mind, and evaluate LLM and
reasoning model theory-of-mind ability through multi-step counterfactual
reasoning.

</details>


### [61] [TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation](https://arxiv.org/abs/2510.05485)
*Adam Filipek*

Main category: cs.CL

TL;DR: 提出TensorBLEU，实现了GPU高效、低内存BLEU指标逐句计算，比主流实现快数十倍，大幅减少训练瓶颈，已开源，对强化学习等模型微调研究有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现代NLP模型规模巨大，但现有评估工具（尤其是针对训练过程中的评价指标，如每句奖励信号）在GPU上计算效率低下，成为研究瓶颈。需要高效支持大批量token-ID计算的新方案。

Method: 提出TensorBLEU，专为GPU加速且支持PyTorch环境下逐句BLEU计算的方法。采用完全向量化方式，并设计了节省内存的n-gram计数机制，利用torch.unique生成批次特定的紧凑字典，避免传统hashing方法的大内存开销。

Result: TensorBLEU在NVIDIA T4等消费级GPU上达到13倍加速，在A100等数据中心级GPU上超过40倍加速，对比NLTK CPU实现。使BLEU计算瓶颈在训练环节变为可忽略的部分。

Conclusion: TensorBLEU显著加速训练中的BLEU计算，提升RL等领域的模型微调效率，对开发和研制大规模NLP模型具有推动作用。

Abstract: Modern natural language processing models have achieved unprecedented scale,
yet the tools for their evaluation often remain a computational bottleneck,
limiting the pace of research. This is particularly acute for in-training
evaluation metrics, such as per-sentence reward signals in Reinforcement
Learning, which must operate efficiently on batches of token IDs directly on
the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the
BLEU metric designed from the ground up for this specific use case. Our
approach is fully vectorized for GPU-accelerated, per-sentence computation
within PyTorch and introduces a memory-efficient counting mechanism. By
creating a compact, batch-specific dictionary of n-grams using
\texttt{torch.unique}, our method avoids the prohibitive memory costs of
traditional hashing-based vectorization, making it practical for
large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard
library for token-ID-based BLEU calculation on the CPU. Experiments show that
TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and
exceeding 40x on data-center-class hardware (NVIDIA A100). This performance
transforms a significant bottleneck into a negligible part of the training
loop. By clearly defining its role as a "Token-ID BLEU" for development
purposes and open-sourcing our implementation, we provide a powerful tool for
accelerating research in areas like RL-based model fine-tuning.

</details>


### [62] [Language Model as Planner and Formalizer under Constraints](https://arxiv.org/abs/2510.05486)
*Cassie Huang,Stuti Mohan,Ziyi Yang,Stefanie Tellex,Li Zhang*

Main category: cs.CL

TL;DR: 作者为现有规划基准引入丰富复杂的自然语言约束，实验显示LLMs在新设定下表现大幅下降，提示应关注其在复杂情境下的真实能力与安全性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在规划领域已广泛应用，但现有基准测试环境过于简单，这可能导致LLMs能力被高估，并在实际应用中产生安全隐患。作者希望通过更复杂的环境设定来真实评估LLMs的规划能力。

Method: 作者通过手动注释和丰富原有的规划基准测试集，加入细粒度、多样化的自然语言约束（分为四大类），并在4个主流LLM、3种形式化语言、5种方法和4个数据集上进行评测。

Result: 在引入约束后，LLMs在规划任务中的表现普遍下降一半，并且对于问题复杂度和词汇变动的鲁棒性明显减弱。

Conclusion: 现有简单基准测试可能高估了LLMs的规划能力，复杂约束的引入能有效考验其实际应用能力，对未来LLMs安全性和可靠性研究具有重要意义。

Abstract: LLMs have been widely used in planning, either as planners to generate action
sequences end-to-end, or as formalizers to represent the planning domain and
problem in a formal language that can derive plans deterministically. However,
both lines of work rely on standard benchmarks that only include generic and
simplistic environmental specifications, leading to potential overestimation of
the planning ability of LLMs and safety concerns in downstream tasks. We bridge
this gap by augmenting widely used planning benchmarks with manually annotated,
fine-grained, and rich natural language constraints spanning four formally
defined categories. Over 4 state-of-the-art reasoning LLMs, 3 formal languages,
5 methods, and 4 datasets, we show that the introduction of constraints not
only consistently halves performance, but also significantly challenges
robustness to problem complexity and lexical shift.

</details>


### [63] [LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation](https://arxiv.org/abs/2510.05490)
*Zhoutong Fu,Yihan Cao,Yi-Lin Chen,Aman Lunia,Liming Dong,Neha Saraf,Ruijie Jiang,Yun Dai,Qingquan Song,Tan Wang,Guoyao Li,Derek Koh,Haichao Wei,Zhipeng Wang,Aman Gupta,Chengming Jiang,Jianqiang Shen,Liangjie Hong,Wenjing Zhang*

Main category: cs.CL

TL;DR: 为求职平台“人岗匹配+解释”场景，本文提出LANTERN知识蒸馏框架，有效提升模型效率和输出质量，提升了用户申请率和高质量申请数量。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）取得诸多自然语言处理任务突破的背景下，将LLMs规模化应用于特定领域（如求职平台的人岗匹配及解释）仍面临挑战：需高质量结构化输出且推理延迟高，不适合在线服务。直接应用开源或微调LLMs效果有限。

Method: 提出了LANTERN知识蒸馏框架，专为人岗匹配任务设计：框架包括分类用编码器和解释用解码器，采用多级知识蒸馏策略（从强大的教师模型蒸馏到多个下游模型，结合数据级和logit级知识），并引入训练后技巧和提示工程以适应特定领域任务。

Result: LANTERN在岗匹配及解释任务的特定指标上表现显著提升；在线评测表明其有效提升了用户互动——申请率提升0.24%，高质量申请提升0.28%。

Conclusion: LANTERN可高效将大模型知识压缩至可在线部署的小模型，同时提升结构化输出质量和实际业务指标，是将LLMs落地特定领域任务的有效方案。

Abstract: Large language models (LLMs) have achieved strong performance across a wide
range of natural language processing tasks. However, deploying LLMs at scale
for domain specific applications, such as job-person fit and explanation in job
seeking platforms, introduces distinct challenges. At LinkedIn, the job person
fit task requires analyzing a candidate's public profile against job
requirements to produce both a fit assessment and a detailed explanation.
Directly applying open source or finetuned LLMs to this task often fails to
yield high quality, actionable feedback due to the complexity of the domain and
the need for structured outputs. Moreover, the large size of these models leads
to high inference latency and limits scalability, making them unsuitable for
online use. To address these challenges, we introduce LANTERN, a novel LLM
knowledge distillation framework tailored specifically for job person fit
tasks. LANTERN involves modeling over multiple objectives, an encoder model for
classification purpose, and a decoder model for explanation purpose. To better
distill the knowledge from a strong black box teacher model to multiple
downstream models, LANTERN incorporates multi level knowledge distillation that
integrates both data and logit level insights. In addition to introducing the
knowledge distillation framework, we share our insights on post training
techniques and prompt engineering, both of which are crucial for successfully
adapting LLMs to domain specific downstream tasks. Extensive experimental
results demonstrate that LANTERN significantly improves task specific metrics
for both job person fit and explanation. Online evaluations further confirm its
effectiveness, showing measurable gains in job seeker engagement, including a
0.24\% increase in apply rate and a 0.28\% increase in qualified applications.

</details>


### [64] [Prototype-Based Dynamic Steering for Large Language Models](https://arxiv.org/abs/2510.05498)
*Ceyhun Efe Kayan,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种原型驱动的动态推理调控方法（PDS），无需修改模型指令或进行微调，即可提升大型语言模型推理能力。实验证明，PDS在多项任务上显著提高了准确率，对高效增强推理具有极大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在推理能力上虽然表现广泛，但依赖显式的推理指令或静态的统一调控方法，难以实现自适应、无指令的推理增强。如何无需改变原始指令，就能提升推理能力，是一项挑战。

Method: 提出了基于原型的动态调控（PDS）方法：通过对链式思考(CoT)提示与中性提示下模型激活的差异进行聚类，构建“推理原型”。在测试时，将输入的隐藏状态投影到这些原型上，生成针对具体实例的调控向量，从而增强模型推理能力。

Result: 在GSM8K、AQuA-RAT和BIG-Bench等任务上，通过PDS方法，无需微调或提示工程，即显著提高了模型推理准确率。即使在显式抑制CoT以提高效率时，性能提升依然明显，表明PDS增强了模型的潜在推理能力，而非仅表层行为改变。

Conclusion: 原型引导的动态调控（PDS）是一种轻量级、无需训练或提示修改的新方案，能够有效提升LLM的推理能力，为强化大模型推理提供了新的方向。

Abstract: Despite impressive breadth, LLMs still rely on explicit reasoning
instructions or static, one-fits-all steering methods, leaving a gap for
adaptive, instruction-free reasoning amplification. We present Prototype-Based
Dynamic Steering (PDS), a test-time method that amplifies large language model
(LLM) reasoning without adding or altering instructions. We introduce
"reasoning prototypes" by clustering activation differences between
Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden
state is projected onto these prototypes to form an instance-specific steering
vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently
improves accuracy without fine-tuning or prompt engineering. Notably, the gains
persist even when CoT is explicitly suppressed to improve cost-efficiency,
indicating that the intervention strengthens latent reasoning processes rather
than inducing a superficial behavioral shift. These results position dynamic,
prototype-guided steering as a lightweight alternative to training-time
approaches for enhancing LLM reasoning.

</details>


### [65] [CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension](https://arxiv.org/abs/2510.05520)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Zihang Tian,Xu Chen,Quanyu Dai,Zhenhua Dong,Ruiming Tang*

Main category: cs.CL

TL;DR: 作者提出了一种受建构主义理论启发的智能记忆模块CAM，通过结构化聚类与动态激活机制，显著提高了大语言模型处理长文本任务的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在长文档处理时面临信息过载问题，亟需一种系统且有效的记忆机制以增强自主理解与推理能力，而现有方法多为启发式，缺乏系统化设计原理。

Method: 受到皮亚杰建构主义理论启发，设计了结构化、灵活吸收与动态调整特性的代理记忆系统，并以增量重叠聚类算法实现记忆结构，通过层级摘要和在线批量整合优化记忆管理。推理阶段，CAM能自适应激活与查询相关的记忆信息，实现类似人类的联想过程。

Result: CAM在长文本阅读理解、问答、基于查询的摘要与事实核查等任务上，相较于已有方法，在性能与效率上实现了双重领先。

Conclusion: 本文提出了一种基于建构主义理论的智能记忆模块CAM，能够显著提升大语言模型在长文本理解任务中的表现和效率。

Abstract: Current Large Language Models (LLMs) are confronted with overwhelming
information volume when comprehending long-form documents. This challenge
raises the imperative of a cohesive memory module, which can elevate vanilla
LLMs into autonomous reading agents. Despite the emergence of some heuristic
approaches, a systematic design principle remains absent. To fill this void, we
draw inspiration from Jean Piaget's Constructivist Theory, illuminating three
traits of the agentic memory -- structured schemata, flexible assimilation, and
dynamic accommodation. This blueprint forges a clear path toward a more robust
and efficient memory system for LLM-based reading comprehension. To this end,
we develop CAM, a prototype implementation of Constructivist Agentic Memory
that simultaneously embodies the structurality, flexibility, and dynamicity. At
its core, CAM is endowed with an incremental overlapping clustering algorithm
for structured memory development, supporting both coherent hierarchical
summarization and online batch integration. During inference, CAM adaptively
explores the memory structure to activate query-relevant information for
contextual response, akin to the human associative process. Compared to
existing approaches, our design demonstrates dual advantages in both
performance and efficiency across diverse long-text reading comprehension
tasks, including question answering, query-based summarization, and claim
verification.

</details>


### [66] [KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance](https://arxiv.org/abs/2510.05524)
*Kuangshi Ai,Jonathan A. Karr Jr,Meng Jiang,Nitesh V. Chawla,Chaoli Wang*

Main category: cs.CL

TL;DR: 该论文提出了结合知识图谱与大语言模型的新型知识抽取与推理框架，在安全关键领域实验中显著提升了全局性理解和系统层面洞察能力，为高风险领域的问答和推理任务带来新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在安全关键领域中的知识提取和推理效果有限，而传统的文本分块型RAG方法对于实现全局性推理和数据集级别的理解能力存在不足。因此，研究者希望通过知识图谱（KG）来增强LLM的推理能力，提升在运维等特定领域里的应用效果。

Method: 提出了KEO框架，在OMIn运维智能数据集基础上构建结构化知识图谱，将其集成到RAG（检索增强生成）流程，实现数据集范围内更连贯的推理。评测过程中，使用Gemma-3、Phi-4、Mistral-Nemo等本地部署型LLM，并由更强的模型（如GPT-4o、Llama-3.3）作为评判者进行比较分析。

Result: 实验显示，KEO框架显著提升了跨数据集的全局意识和系统洞察能力，可展现模式与规律，而传统文本分块RAG则更适合处理局部检索、细粒度操作任务。

Conclusion: KG增强的LLM在安全关键领域中具有显著提升推理和问答能力的潜力，可应用于高风险、领域特定的QA任务，有助于实现更安全可靠的知识抽取和推理。

Abstract: We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge
extraction and reasoning framework with large language models (LLMs) in
safety-critical contexts. Using the Operations and Maintenance Intelligence
(OMIn) dataset, we construct a QA benchmark spanning global sensemaking and
actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and
integrates it into a retrieval-augmented generation (RAG) pipeline, enabling
more coherent, dataset-wide reasoning than traditional text-chunk RAG. We
evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ
stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO
markedly improves global sensemaking by revealing patterns and system-level
insights, while text-chunk RAG remains effective for fine-grained procedural
tasks requiring localized retrieval. These findings underscore the promise of
KG-augmented LLMs for secure, domain-specific QA and their potential in
high-stakes reasoning.

</details>


### [67] [H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference](https://arxiv.org/abs/2510.05529)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: H1B-KV提出了一种对KV缓存极致压缩的新方法，能以极低显存让大模型实现长上下文推理，效果与全精度几乎一致，全面优于主流竞品，是内存受限场景下极具应用价值的新进展。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在自回归解码时，需要缓存越来越多的过去key-value（KV）对，导致长上下文推理受制于显存。在此背景下，现有技术的缓存压缩方案存在只压缩某一部分或丢弃信息等不完整性，无法兼顾内存节省和完整上下文。

Method: 提出了Hybrid One-Bit KV Cache（H1B-KV），一种综合的KV缓存压缩方法：用1-bit二进制sketch表示key向量，实现按位高效注意力计算；对value向量进行4-bit量化。该方法兼顾了key和value的极致压缩，并可通过轻量化微调保证性能不降。

Result: 在7B参数模型上，H1B-KV使8k上下文长度的缓存内存降到60MB以下，实现70倍压缩率。微调后，模型在困惑度、数学推理（GSM8K）、多任务理解（MMLU）、代码生成（HumanEval）等下游任务上与全精度几乎一致。

Conclusion: H1B-KV在不牺牲性能的前提下，大幅减少KV缓存内存消耗，在质量/字节方面显著优于现有主流压缩方法（KIVI、SparseLLM、Loki），为大模型在受限内存环境下部署提供了强有力的解决方案。

Abstract: Autoregressive decoding in large language models (LLMs) requires caching a
growing list of past key-value (KV) pairs, making long-context inference a
memory-bound problem. While recent methods have explored quantizing the cache,
evicting tokens, or using binary sketches for keys (e.g., Loki), these
approaches often provide an incomplete solution by leaving one component (like
values) uncompressed or by discarding context information. This paper
introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression
scheme that radically reduces memory usage without sacrificing context. H1B-KV
represents each key vector using a 1-bit binary sketch, enabling
hardware-friendly bitwise attention, and further compresses value vectors using
4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter
LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x
reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches
full-precision performance not only on perplexity benchmarks but also on
complex downstream tasks like mathematical reasoning (GSM8K), multi-task
understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV
significantly outperforms leading quantization (KIVI), token eviction
(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,
establishing it as a robust solution for deploying LLMs in memory-constrained
environments.

</details>


### [68] [On the Role of Difficult Prompts in Self-Play Preference Optimization](https://arxiv.org/abs/2510.05534)
*Yao Xiao,Jung-jae Kim,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 提示难度被证明会显著影响大语言模型自我博弈优化的最终表现。难提示不仅不能提升整体性能，反而略有损害；而合理筛除部分难提示可以提升训练效果。此外，模型越大，难易提示的影响差距越小。因此提升对齐效果时训练数据难度也需引起重视。


<details>
  <summary>Details</summary>
Motivation: 自我博弈偏好优化已成为调整大型语言模型（LLMs）的主流方法，但研究主要关注模型和奖励模型，相对忽略了输入提示（prompts）的作用。本论文动机在于探究不同难度的提示如何影响自我博弈偏好优化过程。

Method: 作者以提示对应生成响应的平均奖励作为难度度量，系统分析了难易提示对自我博弈偏好优化的影响。通过实验对比训练集包含易与难提示对整体性能的贡献，并进一步研究模型容量与提示难度的关系。同时探索删除部分难提示是否能提升最终性能。

Result: 实验发现，对大型语言模型来说，难提示的自我博弈优化效果远逊于易提示，加入难提示训练甚至略微降低整体性能。随着模型容量提升，难易提示间性能差距缩小。且适度剔除部分难提示可显著提升自我博弈优化效果。文中还报告了未能成功缓解难提示负面影响的尝试与经验教训。

Conclusion: 提示难度对自我博弈偏好优化有显著影响，应当在训练阶段合理选择和筛选难提示，以最大化大语言模型对齐效果，尤其是在小容量模型中尤为重要。

Abstract: Self-play preference optimization has emerged as a prominent paradigm for
aligning large language models (LLMs). It typically involves a language model
to generate on-policy responses for prompts and a reward model (RM) to guide
the selection of chosen and rejected responses, which can be further trained
with direct preference optimization (DPO). However, the role of prompts remains
underexplored, despite being a core component in this pipeline. In this work,
we investigate how prompts of varying difficulty influence self-play preference
optimization. We first use the mean reward of $N$ sampled responses of a prompt
as a proxy for its difficulty. We find that difficult prompts exhibit
substantially inferior self-play optimization performance in comparison to easy
prompts for language models. Moreover, incorporating difficult prompts into
training fails to enhance overall performance and, in fact, leads to slight
degradation compared to training on easy prompts alone. We also observe that
the performance gap between difficult and easy prompts closes as the model
capacity increases, suggesting that difficulty interacts with the model
capacity. Building on these findings, we explore strategies to mitigate the
negative effect of difficult prompts on final performance. We demonstrate that
selectively removing an appropriate portion of challenging prompts enhances
overall self-play performance, while also reporting failed attempts and lessons
learned.

</details>


### [69] [Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM](https://arxiv.org/abs/2510.05544)
*Ryan Solgi,Parsa Madinei,Jiayi Tian,Rupak Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了基于Pareto优化的低秩压缩PGSVD方法，有效缓解LLM和VLM部署时的资源消耗，并提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前LLM和VLM虽然性能强大，但部署时面临巨大的内存与计算压力，亟需有效的模型压缩方法。

Method: 提出低秩模型压缩的双目标优化，并通过Pareto-Guided SVD与交替最小二乘实现无需数据零样本的管道。

Result: PGSVD应用于LLM和VLM，在保持相同压缩率的条件下获得更高的准确率和推理加速。

Conclusion: 提出的PGSVD方法能够在同等压缩率下，提升LLM和VLM的精度与推理速度。

Abstract: Large language models (LLM) and vision-language models (VLM) have achieved
state-of-the-art performance, but they impose significant memory and computing
challenges in deployment. We present a novel low-rank compression framework to
address this challenge. First, we upper bound the change of network loss via
layer-wise activation-based compression errors, filling a theoretical gap in
the literature. We then formulate low-rank model compression as a bi-objective
optimization and prove that a single uniform tolerance yields surrogate
Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we
propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot
pipeline that improves activation-aware compression via Pareto-guided rank
selection and alternating least-squares implementation. We apply PGSVD to both
LLM and VLM, showing better accuracy at the same compression levels and
inference speedup.

</details>


### [70] [Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations](https://arxiv.org/abs/2510.05571)
*Chengzhi Liu,Yuzhe Yang,Kaiwen Zhou,Zhen Zhang,Yue Fan,Yannan Xie,Peng Qi,Xin Eric Wang*

Main category: cs.CL

TL;DR: 本文提出了EvoPresent自改进演示生成框架，通过多任务强化学习模型提升演示的叙事性和美感，并以系统评测集验证方法优势，有效解决现有学术演示自动生成的评价与改进难题。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文自动推广方法在叙事性、美感设计和自我调整方面存在局限，影响研究可见度提升，核心问题是缺乏有效评价机制。

Method: 提出EvoPresent框架，通过虚拟角色集成叙事、审美和现实感演示。核心模块PresAesth采用多任务强化学习审美模型，提供审美评分、缺陷调整和比较反馈，实现有限数据下的自我迭代改进。同时，构建EvoPresent Benchmark，体系化评估生成质量和审美意识。

Result: 建立了包含650份顶级AI会议论文的多模态生成质量数据集及2,000对美感差异幻灯片的审美评测集。研究发现，高质量反馈对代理自我改进至关重要；自动生成过程中内容与视觉设计存在权衡；多任务强化学习训练在审美泛化任务中表现更佳。

Conclusion: EvoPresent框架有助于提升学术演示自动生成的叙事性、美感和自我调整能力，多任务强化学习审美模型和基准集推动该领域系统性评估和迭代改进。

Abstract: The promotion of academic papers has become an important means of enhancing
research visibility. However, existing automated methods struggle limited
storytelling, insufficient aesthetic quality, and constrained self-adjustment,
making it difficult to achieve efficient and engaging dissemination. At the
heart of those challenges is a simple principle: \emph{there is no way to
improve it when you cannot evaluate it right}. To address this, we introduce
\textbf{EvoPresent}, a self-improvement agent framework that unifies coherent
narratives, aesthetic-aware designs, and realistic presentation delivery via
virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task
reinforcement learning (RL) aesthetic model that provides reliable aesthetic
scoring, defect adjustment, and comparative feedback, enabling iterative
self-improvement even under limited aesthetic training data. To systematically
evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a
comprehensive benchmark comprising: \textit{Presentation Generation Quality},
built on 650 top-tier AI conference papers with multimodal resources (slides,
videos and scripts) to assess both content and design; and \textit{Aesthetic
Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels,
supporting joint training and evaluation on scoring, defect adjustment, and
comparison. Our findings highlight that (i) High-quality feedback is essential
for agent self-improvement, while initial capability alone does not guarantee
effective self-correction. (ii) Automated generation pipelines exhibit a
trade-off between visual design and content construction. (iii) Multi-task RL
training shows stronger generalization in aesthetic awareness tasks.

</details>


### [71] [Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs](https://arxiv.org/abs/2510.05577)
*Dong Yan,Gaochen Wu,Bowen Zhou*

Main category: cs.CL

TL;DR: 提出FGDIP新框架，通过动态反馈和节点生成，提升LLM在开放领域多跳推理上的信息检索与推理能力，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理在多跳推理任务上有显著进展，但在开放领域问题中依赖固定行动序列，难以高效处理信息检索问题。

Method: 提出了反馈引导的动态交互式规划（FGDIP）框架，结合历史错误分析与实时反馈，动态生成并调整推理节点，采用深度优先搜索和创新节点生成策略优化多跳推理。

Result: 在HotpotQA和StrategyQA两个数据集上取得了最高54.47%和70.05%的F1分数，分别比最佳基线高出5.03%和7.25%。

Conclusion: FGDIP框架显著提升了开放领域多跳推理任务的性能，显示出增强语言代理能力的潜力和通用性。

Abstract: Recent advancements in language agents have led to significant improvements
in multi-hop reasoning tasks. However, existing approaches often struggle with
handling open-domain problems, which require massive information retrieval due
to their reliance on a fixed sequence of actions. To address this, we propose
Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework
tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive
strategies for information exploration in open-domain multi-hop reasoning
tasks. Our approach begins by identifying key entities relevant to the problem,
which serve as the initial nodes in the reasoning process. From these initial
nodes, we then generate reasoning child nodes with the process being refined
through a combination of historical error analysis and real-time feedback,
which allows the framework to dynamically adjust and optimize its reasoning
strategies. By integrating depth-first search with an innovative node
generation technique, our framework adapts based on both prior error paths and
concurrently generated nodes at the same hierarchical level. This dynamic
strategy effectively expands the search space while ensuring the reasoning
process systematically converges toward accurate solutions. Experimental
results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset
and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and
7.25% respectively, highlighting its versatility and potential to enhance
language agents in multi-hop reasoning tasks.

</details>


### [72] [A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks](https://arxiv.org/abs/2510.05608)
*Shuzheng Si,Haozhe Zhao,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种能显著提升LLM智能体长序列规划与执行能力的训练方法EAGLET，不仅性能优越，还能减少训练成本，无需人力与额外数据。


<details>
  <summary>Details</summary>
Motivation: 目前基于大语言模型（LLM）的智能体处理长序列任务时，缺乏全局规划能力，容易出现盲目试错和幻觉行为。

Method: 作者提出了一个“计划-执行”框架，并设计了EAGLET，一种高效的规划器训练方法。具体做法为：第一步，通过同源共识过滤策略，从先进LLM合成高质量计划并微调，实现冷启动；第二步，采用基于规则的强化学习阶段，并引入新的执行者能力提升奖励进一步优化规划器。该方法无需人工干预和额外训练数据。

Result: 在三个长序列任务上，配备该规划器的智能体表现优于已有方法，达到了新的SOTA性能。同时，EAGLET相比基于RL的基线方法，将训练成本降低了8倍。

Conclusion: EAGLET能够在无需人工和额外数据的情况下，大幅提高基于LLM的智能体的长期规划和任务执行能力，且训练高效。

Abstract: Agents based on large language models (LLMs) struggle with brainless
trial-and-error and generating hallucinatory actions due to a lack of global
planning in long-horizon tasks. In this paper, we introduce a plan-and-execute
framework and propose EAGLET, an efficient and effective planner training
method to enhance the executor agent's planning abilities without human effort.
Specifically, we train a plug-and-play global planner through a two-step
process: we first synthesize high-quality plans from an advanced LLM using our
proposed homologous consensus filtering strategy, and apply fine-tuning as a
cold start. Moreover, we further improve the planner with a rule-based
reinforcement learning stage using a novel executor capability gain reward,
ensuring it can handle task instructions of varying difficulty. Experiments on
three long-horizon agent tasks show that executor agents equipped with our
planner outperform existing methods, achieving new state-of-the-art
performance. Meanwhile, EAGLET reduces training costs by 8x compared to
RL-based baselines, and it does not require manual effort or extra training
data, offering an efficient and effective solution.

</details>


### [73] [MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction](https://arxiv.org/abs/2510.05611)
*Wei-Chieh Huang,Cornelia Caragea*

Main category: cs.CL

TL;DR: 本文针对电商多模态隐式属性值抽取任务，提出多MLLM智能体辩论框架，通过智能体间多轮迭代提升推理表现，显著增强了准确性和鲁棒性，为该类任务提供了新的有效方法。


<details>
  <summary>Details</summary>
Motivation: 隐式属性值抽取（AVE）对电商中产品表示至关重要，但由于多模态数据复杂性以及视觉-文本理解的困难，隐式AVE任务仍具挑战性。

Method: 提出了一种多智能体辩论框架（multi-agent debate framework），通过让多个多模态大语言模型（MLLM）智能体迭代相互验证和完善推理结果。通过多轮辩论，提升推理准确性和鲁棒性。

Result: 在ImplicitAVE数据集上的实验表明，仅需几轮辩论就能显著提升准确率，尤其是在初始表现低的属性上。系统评估了不同辩论配置（使用相同或不同MLLM智能体）及其对收敛动态的影响。

Conclusion: 多智能体辩论策略可以有效克服单智能体方法的局限性，为多模态电商中的隐式AVE任务提供了可扩展的解决方案。

Abstract: Implicit Attribute Value Extraction (AVE) is essential for accurately
representing products in e-commerce, as it infers lantent attributes from
multimodal data. Despite advances in multimodal large language models (MLLMs),
implicit AVE remains challenging due to the complexity of multidimensional data
and gaps in vision-text understanding. In this work, we introduce
\textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM
agents to iteratively refine inferences. Through a series of debate rounds,
agents verify and update each other's responses, thereby improving inference
performance and robustness. Experiments on the ImplicitAVE dataset demonstrate
that even a few rounds of debate significantly boost accuracy, especially for
attributes with initially low performance. We systematically evaluate various
debate configurations, including identical or different MLLM agents, and
analyze how debate rounds affect convergence dynamics. Our findings highlight
the potential of multi-agent debate strategies to address the limitations of
single-agent approaches and offer a scalable solution for implicit AVE in
multimodal e-commerce.

</details>


### [74] [The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP](https://arxiv.org/abs/2510.05644)
*Sheriff Issaka,Keyi Wang,Yinka Ajibola,Oluwatumininu Samuel-Ipaye,Zhaoyi Zhang,Nicte Aguillon Jimenez,Evans Kofi Agyei,Abraham Lin,Rohan Ramachandran,Sadick Abdul Mumin,Faith Nchifor,Mohammed Shuraim,Lieqi Liu,Erick Rosas Gonzalez,Sylvester Kpei,Jemimah Osei,Carlene Ajeneza,Persis Boateng,Prisca Adwoa Dufie Yeboah,Saadia Gabriel*

Main category: cs.CL

TL;DR: 论文提出了非洲语言NLP的大型数据与模型资源建设方案，构建了史上最大40语言非洲多模态数据集，微调实验全面优于基准，部分语言对比谷歌翻译有竞争力，同时建立了本地研究团队，显示巨大进步但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在全球语言中占比很高，但NLP资源极其稀缺，大量语言在计算语言学领域被忽视，亟需系统性技术和资源干预。

Method: 研发全新的、高质量的数据收集流程，涵盖40种非洲语言，构建包含语音及文本数据的大型多模态数据集；在模型训练与微调方面进行广泛实验并与主流翻译系统做对比分析；搭建研究和人才培养体系。

Result: 构建了最大规模的经质量控制的非洲多模态语音与文本数据集（40种语言，约190亿词，1.26万小时有标注语音数据）；微调模型实验在31种语言上，指标上大幅优于基线（ChrF++、COMET、BLEU分别提升23.69、0.33、15.34分）；与谷歌翻译比较，在若干语言上达到有竞争力的结果。

Conclusion: 非洲语言在现代NLP技术中极度匮乏，论文提出了Systematic数据建设与模型开发，应对这一问题，取得了一定成果，例如在多个评价标准和与谷歌翻译的对比中表现良好，但仍存在进一步提升空间。

Abstract: Despite representing nearly one-third of the world's languages, African
languages remain critically underserved by modern NLP technologies, with 88\%
classified as severely underrepresented or completely ignored in computational
linguistics. We present the African Languages Lab (All Lab), a comprehensive
research initiative that addresses this technological gap through systematic
data collection, model development, and capacity building. Our contributions
include: (1) a quality-controlled data collection pipeline, yielding the
largest validated African multi-modal speech and text dataset spanning 40
languages with 19 billion tokens of monolingual text and 12,628 hours of
aligned speech data; (2) extensive experimental validation demonstrating that
our dataset, combined with fine-tuning, achieves substantial improvements over
baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points
across 31 evaluated languages; and (3) a structured research program that has
successfully mentored fifteen early-career researchers, establishing
sustainable local capacity. Our comparative evaluation against Google Translate
reveals competitive performance in several languages while identifying areas
that require continued development.

</details>


### [75] [Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models](https://arxiv.org/abs/2510.05678)
*Haneul Yoo,Jiho Jin,Kyunghyun Cho,Alice Oh*

Main category: cs.CL

TL;DR: 本文提出通过代码切换演示，解决大模型多语言推理时依赖英语中介的问题，显著提升了多语种特别是低资源语言的表现，被证明是一种有效且稳健的方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然具备强大的多语种能力，但它们依赖英语作为潜在表示，这导致推理过程隐含地依赖于将其他语言内部翻译为英语。一旦这一内部翻译过程失败，非英语语言的性能就会急剧下降，影响了多语言应用的包容性。现有的跨语言in-context learning方法未能有效解决这一障碍。

Method: 提出了一种新的提示策略——代码切换代码演示学习（Code-Switching In-Context Learning, CSICL）。在task演示和指令中，逐渐将目标语言切换到英语，通过引导模型在演示过程中的受控语言转换，显式支持模型将推理过程过渡到英语。

Result: 在4个LLM、6个数据集、10种语言上的实验表明，CSICL较传统X-ICL基线有显著提升。目标语言和未见语言分别提升3.1和1.9个百分点，低资源场景下分别提升14.7和5.3个百分点。

Conclusion: 代码切换作为一种提示策略，有效消除了大型语言模型多语种推理时的翻译障碍，提升了模型的多语种公平性和应用效果。CSICL是一种稳健的方法，推动LLM迈向更加包容、有效的多语种系统。

Abstract: While large language models (LLMs) exhibit strong multilingual abilities,
their reliance on English as latent representations creates a translation
barrier, where reasoning implicitly depends on internal translation into
English. When this process fails, performance in non-English languages
deteriorates sharply, limiting the inclusiveness of LLM-based applications.
Existing cross-lingual in-context learning (X-ICL) methods primarily leverage
monolingual demonstrations, often failing to mitigate this barrier and instead
reinforcing it. In this work, we introduce code-switching in-context learning
(CSICL), a simple yet effective prompting strategy that progressively
transitions from a target language to English within demonstrations and
instruction to facilitate their latent reasoning in English. By explicitly
scaffolding the reasoning process through controlled code-switching, CSICL acts
as an implicit linguistic bridge that enhances cross-lingual alignment and
reduces reliance on the translation barrier. We conduct extensive experiments
across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive
and reasoning-oriented domains. Our results demonstrate that CSICL consistently
outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target
and unseen languages, respectively. The improvement is even more pronounced in
low-resource settings, with gains of 14.7% in target and 5.3% in unseen
languages. These findings establish code-switching as a principled and robust
approach for overcoming the translation barrier during inference, moving LLMs
toward more equitable and effective multilingual systems.

</details>


### [76] [DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision](https://arxiv.org/abs/2510.05691)
*Yongqi Leng,Yikun Lei,Xikai Liu,Meizhi Zhong,Bojian Xiong,Yurong Zhang,Yan Gao,Yi Wu,Yao Hu,Deyi Xiong*

Main category: cs.CL

TL;DR: DecEx-RAG通过决策执行建模和高效数据剪枝，显著提升了复杂任务处理性能和数据构建效率，在六个数据集上效果远超基线，为RAG训练提供了更优方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Agentic RAG利用动态检索和自适应流程增强复杂任务处理能力，但基于结果监督的强化学习（如Search-R1）仍面临探索效率低、奖励稀疏和全局反馈模糊等问题。

Method: 提出DecEx-RAG，将RAG建模为包含决策和执行的马尔可夫决策过程（MDP），并引入高效剪枝策略优化数据扩展。通过全面的流程级策略优化，提升自主任务分解、动态检索和高质量答案生成能力。

Result: DecEx-RAG在六个数据集上平均绝对性能提升6.2%，显著优于现有基线。剪枝策略提升数据构建效率近6倍。

Conclusion: DecEx-RAG通过MDP建模和高效剪枝策略，大幅提升了大型语言模型在复杂任务中的自主性和数据处理效率，为流程监督型RAG训练提供了一种高效解决方案。

Abstract: Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.

</details>


### [77] [Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities](https://arxiv.org/abs/2510.05744)
*Liza Fretel,Baptiste Cecconi,Laura Debisschop*

Main category: cs.CL

TL;DR: 本工作建立了利用NLP和LLM，从多个语义数据库映射并统一天文观测设施名称的方法，提高了资源检索和交流的标准化与互操作性。


<details>
  <summary>Details</summary>
Motivation: 天文观测设施的数据来源分散、命名不统一，影响数据检索和知识整合，因此需要跨多个语义资源进行标准化映射。

Method: 结合可自定义评分标准与多种自然语言处理技术（如词袋模型、序列方法、表层方法），对从八大语义资源（如Wikidata及专业天文数据库）提取的实体进行映射。利用各类属性（标签、定义、描述、外部标识符、波段、发射日期、资助机构等）综合比对，并引入大型语言模型（LLM）审核和解释映射建议，从而确保同义关系的合理性与FAIR性。

Result: 生成了多源标准化同义词集合，每个实体只对应唯一标准标签。该结果将用于Name Resolver API，同时融入国际虚拟天文台联盟（IVOA）词表及OntoPortal-Astro平台。

Conclusion: 提出并实现了一套多源天文实体标准化映射流程，显著提升了观测设施数据的一致性与可互操作性，为天文学数据检索、整合应用提供强大支持。

Abstract: This ongoing work focuses on the development of a methodology for generating
a multi-source mapping of astronomical observation facilities. To compare two
entities, we compute scores with adaptable criteria and Natural Language
Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches,
and surface approaches) to map entities extracted from eight semantic
artifacts, including Wikidata and astronomy-oriented resources. We utilize
every property available, such as labels, definitions, descriptions, external
identifiers, and more domain-specific properties, such as the observation
wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a
Large Language Model (LLM) to accept or reject a mapping suggestion and provide
a justification, ensuring the plausibility and FAIRness of the validated
synonym pairs. The resulting mapping is composed of multi-source synonym sets
providing only one standardized label per entity. Those mappings will be used
to feed our Name Resolver API and will be integrated into the International
Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro
platform.

</details>


### [78] [Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes](https://arxiv.org/abs/2510.05767)
*Peter Ochieng*

Main category: cs.CL

TL;DR: 该论文通过光谱分析和批次选择优化InfoNCE训练，Greedy算法加速训练，批内白化显著降低梯度方差，理论与实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 论文旨在分析和提升InfoNCE损失下的梯度表现，尤其关注非渐近情况下的梯度范数、谱特征和批次选择对优化效率的影响。

Method: 作者推导了非渐近光谱区间来限制InfoNCE梯度范数，并通过有效秩（effective rank）量化数据的各向异性，设计了基于谱信息的批次选择方法，包括一种快速贪心算法。此外，通过批内白化方法来促进各向同性并减少梯度方差。

Result: 论文发现，利用Greedy-64谱感知批次选择，在ImageNet-100任务中的训练速度快于随机选批（提高15%），且优于Pool--P3方案。在CIFAR-10任务中也有类似提升。同时，批内白化将50步梯度方差降低了1.37倍，且与理论上界一致。

Conclusion: 谱感知的批次选择与批内白化能有效加速模型训练和减小梯度方差，实验在多个数据集证实了理论推导的有效性。

Abstract: We derive non-asymptotic spectral bands that bound the squared InfoNCE
gradient norm via alignment, temperature, and batch spectrum, recovering the
\(1/\tau^{2}\) law and closely tracking batch-mean gradients on synthetic data
and ImageNet. Using effective rank \(R_{\mathrm{eff}}\) as an anisotropy proxy,
we design spectrum-aware batch selection, including a fast greedy builder. On
ImageNet-100, Greedy-64 cuts time-to-67.5\% top-1 by 15\% vs.\ random (24\%
vs.\ Pool--P3) at equal accuracy; CIFAR-10 shows similar gains. In-batch
whitening promotes isotropy and reduces 50-step gradient variance by
\(1.37\times\), matching our theoretical upper bound.

</details>


### [79] [InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience](https://arxiv.org/abs/2510.05769)
*Jianbin Shen,Christy Jie Liang,Junyu Xuan*

Main category: cs.CL

TL;DR: 本文针对摘要生成的信息丰富性提出了两种新方法，包括信息注意力和信息显著性提升，通过实验和人类评估证实了方法的有效性，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前摘要生成方法在信息丰富性方面有提升空间，因此作者尝试提出新方法，解决信息包含不足的问题。

Method: 采用了基于最优传输的注意力机制以增强参考摘要中的关键信息学习，以及在命名实体上进行联合熵递减的方法以突出信息显著性。

Result: 新方法在CNN/Daily Mail上取得了比现有方法更好的ROUGE分数，在XSum数据集上也有可竞争性能；在人类信息丰富性评估上明显优于强基线。

Conclusion: 提出的方法在摘要生成任务中提升了信息丰富性，在ROUGE分数和人类评估中表现优越。

Abstract: Abstractive text summarization is integral to the Big Data era, which demands
advanced methods to turn voluminous and often long text data into concise but
coherent and informative summaries for efficient human consumption. Despite
significant progress, there is still room for improvement in various aspects.
One such aspect is to improve informativeness. Hence, this paper proposes a
novel learning approach consisting of two methods: an optimal transport-based
informative attention method to improve learning focal information in reference
summaries and an accumulative joint entropy reduction method on named entities
to enhance informative salience. Experiment results show that our approach
achieves better ROUGE scores compared to prior work on CNN/Daily Mail while
having competitive results on XSum. Human evaluation of informativeness also
demonstrates the better performance of our approach over a strong baseline.
Further analysis gives insight into the plausible reasons underlying the
evaluation results.

</details>


### [80] [Mixture of Neuron Experts](https://arxiv.org/abs/2510.05781)
*Runxi Cheng,Yuchen Guan,Yucheng Ding,Qingguo Hu,Yongxian Wei,Chun Yuan,Yelong Shen,Weizhu Chen,Yeyun Gong*

Main category: cs.CL

TL;DR: 本文研究MoE模型参数稀疏性，提出神经元粒度的MoNE方法，只选择高激活神经元以提升参数利用率和推理效率。在广泛实验验证下，MoNE不减性能且显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在MoE层推理时，大多数激活参数存在高度稀疏性，许多神经元的激活值接近零，这说明部分参数利用率低，影响计算效率。

Method: 作者提出Mixture of Neuron Experts (MoNE)，在每个专家中只选择高激活神经元，通过top-k策略进行神经元粒度的专家选择，无需额外参数或专家间通信，且延迟极低。

Result: MoNE能在仅激活传统MoE一半参数的情况下达到相同性能，并在激活参数数量相同时显著优于传统MoE。

Conclusion: MoNE方法能有效提升MoE模型的参数利用率和推理效率，并在激活相同参数量时显著优于传统MoE。

Abstract: In this work, we first explore whether the parameters activated by the MoE
layer remain highly sparse at inference. We perform a sparsification study on
several representative MoE models. For each expert, we rank parameters by the
magnitude of their activations from the gate projection and progressively prune
the activated subset. Pruning up to 60% of parameters within that subset causes
only negligible task-performance degradation; substantial drops occur only
after more than 90% are removed. We further decompose experts into
neuron-granular MoE and visualize their activation values, finding that most
neuron activations are near zero. This observation motivates us to select only
high-activation neuron experts during pretraining. Based on this insight, we
propose Mixture of Neuron Experts (MoNE). MoNE achieves neuron-granular expert
selection by only applying a simple top-k selection within each expert, incurs
negligible latency, and requires no additional routing parameters or
inter-expert communication. Extensive experiments demonstrate that MoNE matches
traditional MoE performance while activating only 50% of the MoE-layer
parameters, and it consistently outperforms traditional MoE when compared at
equal numbers of activated parameters. These results suggest that MoNE is a
practical approach to improving parameter utilization and inference efficiency
in MoE-like models.

</details>


### [81] [Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech](https://arxiv.org/abs/2510.05799)
*Rikuto Kotoge,Yuichi Sasaki*

Main category: cs.CL

TL;DR: 针对TTS优化中样本不足和精细度不够问题，本文提出TKTO算法，无需配对或细粒度标注，显著提升日语TTS的准确率和经济性。


<details>
  <summary>Details</summary>
Motivation: 现有TTS优选方法需要成对的好坏语音样本，但这种数据稀缺，且现有方式只针对整句，不能细致调整每个音素或词语的发音准确性。

Method: 提出TKTO方法，不需成对样本，直接对token级单位进行训练，无需额外标注，自动产生细粒度对齐信号，实现更高效准确的数据利用。

Result: TKTO在日语TTS任务中准确率提升39%，字符错误率降低54%，对目标token奖励强度提升12.8倍。

Conclusion: TKTO能在无需成对样本和token级标注下实现端到端的细粒度TTS优化，大幅提升发音准确率和数据利用效率。

Abstract: Aligning text-to-speech (TTS) system outputs with human feedback through
preference optimization has been shown to effectively improve the robustness
and naturalness of language model-based TTS models. Current approaches
primarily require paired desirable and undesirable samples at the utterance
level. However, such pairs are often limited in TTS output data, and
utterance-level formulation prevents fine-grained token-level optimization
needed for accurate pronunciation alignment. In this study, we propose TKTO
that eliminates the need for paired data, enabling a more data-efficient
training paradigm, and directly targets token-level units, automatically
providing fine-grained alignment signals without token-level annotations. TKTO
improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,
automatically assigning 12.8 times stronger reward to targeted tokens.

</details>


### [82] [EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget](https://arxiv.org/abs/2510.05837)
*Liang Chen,Xueting Han,Qizhou Wang,Bo Han,Jing Bai,Hinrich Schutze,Kam-Fai Wong*

Main category: cs.CL

TL;DR: EEPO通过两阶段采样和遗忘机制，有效打破主导模式的自强化循环，大幅提升LLM在强化学习任务中的探索能力与表现。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的可验证奖励强化学习（RLVR）中，探索与利用的平衡始终是一个核心挑战。目前方法过度偏向利用，导致策略熵可能下降，探索能力受限，最终性能提升有限。即使增加策略随机性也常难以跳出主导模式，陷入自我强化的循环，进一步损害探索能力。

Method: 提出了一种名为EEPO（探索增强策略优化）的新框架。该框架采用两阶段回合（rollout）和自适应取消训练（unlearning）：第一阶段，模型生成一半轨迹；接着临时抑制已采样响应，第二阶段被迫探索不同输出区域。这种“采样—遗忘”机制打破自我强化循环，促进更广泛的探索。

Result: 在五个推理基准上，EEPO相较于现有GRPO方法表现更优。在Qwen2.5-3B上平均提升24.3%，Llama3.2-3B-Instruct提升33.0%，Qwen3-8B-Base提升10.4%。

Conclusion: EEPO有效解决了RLVR中过度利用、探索受限的问题，通过独特的采样—遗忘机制在实际基准测试中显著提升了大语言模型的性能。

Abstract: Balancing exploration and exploitation remains a central challenge in
reinforcement learning with verifiable rewards (RLVR) for large language models
(LLMs). Current RLVR methods often overemphasize exploitation, leading to
entropy collapse, diminished exploratory capacity, and ultimately limited
performance gains. Although techniques that increase policy stochasticity can
promote exploration, they frequently fail to escape dominant behavioral modes.
This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant
modes-that further erodes exploration. We introduce Exploration-Enhanced Policy
Optimization (EEPO), a framework that promotes exploration via two-stage
rollouts with adaptive unlearning. In the first stage, the model generates half
of the trajectories; it then undergoes a lightweight unlearning step to
temporarily suppress these sampled responses, forcing the second stage to
explore different regions of the output space. This sample-then-forget
mechanism disrupts the self-reinforcing loop and promotes wider exploration
during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,
achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on
Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.

</details>


### [83] [Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer](https://arxiv.org/abs/2510.05846)
*Maxence Lasbordes,Sinoué Gad*

Main category: cs.CL

TL;DR: 论文针对法语在多语言小型模型中的性能短板，提出了Luth系列，通过高质量法语数据后训练和模型融合，显著提升了法语表现，成为最新SOTA，同时保持英语能力，为小语种模型研究树立新基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）主要以英语为中心，导致包括法语在内的其他主要语言在表现上有明显差距，尤其是小语种模型（SLMs）。现有的多语言模型在法语上的表现远低于英语，且关于高效适应法语的方法研究较少。

Method: 通过针对性后训练，选用高质量的法语数据，对现有模型进行专门的法语适应。还采用了策略性模型融合，以进一步提升英法两种语言的表现。

Result: 所提出的Luth法语专用小型语言模型在多个法语基准测试上，均优于其他同等规模的开源模型，同时保留了原有的英语能力。模型融合后，英法性能均提升。

Conclusion: Luth成为法语SLM的新SOTA，并为未来法语人工智能研究提供了强有力的基线参考。

Abstract: The landscape of Large Language Models (LLMs) remains predominantly
English-centric, resulting in a significant performance gap for other major
languages, such as French, especially in the context of Small Language Models
(SLMs). Existing multilingual models demonstrate considerably lower performance
in French compared to English, and research on efficient adaptation methods for
French remains limited. To address this, we introduce \textbf{Luth}, a family
of French-specialized SLMs: through targeted post-training on curated,
high-quality French data, our models outperform all open-source counterparts of
comparable size on multiple French benchmarks while retaining their original
English capabilities. We further show that strategic model merging enhances
performance in both languages, establishing Luth as a new state of the art for
French SLMs and a robust baseline for future French-language research.

</details>


### [84] [DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization](https://arxiv.org/abs/2510.05858)
*Xue-Yong Fu,Elena Khasanova,Md Tahmid Rahman Laskar,Harsh Saini,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 针对对话摘要这一专业场景，持续预训练能无需大量人工标注数据，显著提升大语言模型的摘要能力，并具备良好泛化性，适合工业实际应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本摘要任务上表现优异，但在特定领域（如对话类数据）应用时效果有限，而高质量有监督微调数据稀缺且昂贵。

Method: 探索采用可扩展且自监督的持续预训练（continual pre-training）方法，利用大规模无标注商业对话数据，提升模型在真实世界嘈杂对话摘要任务中的能力。

Result: 持续预训练不仅显著提升了模型在领域内和领域外摘要基准任务的表现，还保持了较强的泛化能力和鲁棒性。还详细分析了数据选择策略的影响，并提出了在工业场景中应用的实际建议。

Conclusion: 持续预训练是一种高效且可行的方法，可利用无标注数据提升大语言模型在专业领域摘要任务中的表现，尤其适合缺乏高质量标注数据的工业应用场景。

Abstract: Large language models (LLMs) have achieved impressive performance in text
summarization, yet their performance often falls short when applied to
specialized domains %or conversational data that differ from their original
pre-training distribution. While fine-tuning can improve summarization quality,
it typically relies on costly and scarce high-quality labeled data. In this
work, we explore continual pre-training as a scalable, self-supervised approach
to adapt LLMs for downstream summarization tasks, particularly in the context
of noisy real-world conversation transcripts. We conduct extensive experiments
using large-scale, unlabeled business conversation data to investigate whether
continual pre-training enhances model capabilities in conversational
summarization. Our results demonstrate that continual pre-training yields
substantial gains in both in-domain and out-of-domain summarization benchmarks,
while maintaining strong generalization and robustness. We also analyze the
effects of data selection strategies, providing practical guidelines for
applying continual pre-training in summarization-focused industrial
applications.

</details>


### [85] [Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies](https://arxiv.org/abs/2510.05860)
*Luka Nenadic,David Rodriguez*

Main category: cs.CL

TL;DR: 该论文通过瑞士隐私法的案例，验证了自动合同生成器和大语言模型在提升企业法规合规性方面的效果，发现自动化工具能显著提高隐私政策合规度。


<details>
  <summary>Details</summary>
Motivation: 在面对不断增长的数字法规时，企业尤其是中小企业难以负担高昂的法律咨询费用，因此求助于自动化合同生成器，但相关工具的普及率和输出质量缺乏实证研究。

Method: 构建并注释了一个多语言基准数据集，涵盖瑞士及欧盟隐私法的合规义务，利用GPT-5方法对隐私政策的大规模合规性进行评估。

Result: 修法后，企业隐私政策的合规度显著提升。使用自动化生成器的网站占18%，其合规性比未使用者高出最多15个百分点。

Conclusion: 自动化合同生成器有助于提升企业的法律合规性，尤其在中小企业中。大语言模型（LLM）在跨语言法律分析方面具有潜力，自动化工具对法规合规水平和合同质量的提升具有重要作用。

Abstract: It has become increasingly challenging for firms to comply with a plethora of
novel digital regulations. This is especially true for smaller businesses that
often lack both the resources and know-how to draft complex legal documents.
Instead of seeking costly legal advice from attorneys, firms may turn to
cheaper alternative legal service providers such as automated contract
generators. While these services have a long-standing presence, there is little
empirical evidence on their prevalence and output quality.
  We address this gap in the context of a 2023 Swiss privacy law revision. To
enable a systematic evaluation, we create and annotate a multilingual benchmark
dataset that captures key compliance obligations under Swiss and EU privacy
law. Using this dataset, we validate a novel GPT-5-based method for large-scale
compliance assessment of privacy policies, allowing us to measure the impact of
the revision. We observe compliance increases indicating an effect of the
revision. Generators, explicitly referenced by 18% of local websites, are
associated with substantially higher levels of compliance, with increases of up
to 15 percentage points compared to privacy policies without generator use.
These findings contribute to three debates: the potential of LLMs for
cross-lingual legal analysis, the Brussels Effect of EU regulations, and,
crucially, the role of automated tools in improving compliance and contractual
quality.

</details>


### [86] [Revisiting Long-context Modeling from Context Denoising Perspective](https://arxiv.org/abs/2510.05862)
*Zecheng Tang,Baibei Ji,Juntao Li,Lijun Wu,Haijia Gui,Min Zhang*

Main category: cs.CL

TL;DR: 本文分析了长上下文模型易受噪声影响的问题，提出了IG分数度量噪声，并用CDT训练显著提升模型性能，实现开源模型逼近顶级大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 长上下文模型（LCMs）在处理长序列任务中展现了良好潜力，但它们容易受到上下文噪声（无关信息）的干扰，这会影响模型关注关键数据的能力。当前缺乏有效方法来识别并量化这类噪声。

Method: 提出了一种评估上下文噪声的有效度量方法——集成梯度（IG）分数；据此，进一步提出了上下文去噪训练策略（CDT），通过减少噪声提升模型对关键信息的关注和预测能力。

Result: 简单去除检测到的上下文噪声可显著增强模型对关键token的关注，提升下游预测效果。实验表明，采用CDT后，开源8B模型的性能接近GPT-4o。

Conclusion: 上下文噪声是长文本理解的主要障碍，本文提出的IG分数和CDT方法能够有效缓解该问题，大幅提升长文本模型的表现和鲁棒性。

Abstract: Long-context models (LCMs) have demonstrated great potential in processing
long sequences, facilitating many real-world applications. The success of LCMs
can be attributed to their ability to locate implicit critical information
within the context for further prediction. However, recent research reveals
that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,
that can mislead model attention. In this paper, we conduct a fine-grained
analysis of the context noise and propose an effective metric, the Integrated
Gradient (IG) score, to detect and quantify the noise information within the
context. Our findings reveal that even simple mitigation of detected context
noise can substantially boost the model's attention on critical tokens and
benefit subsequent predictions. Building on this insight, we propose Context
Denoising Training (CDT), a straightforward yet effective training strategy
that improves attention on critical tokens while reinforcing their influence on
model predictions. Extensive experiments across four tasks, under both context
window scaling and long-context alignment settings, demonstrate the superiority
of CDT. Notably, when trained with CDT, an open-source 8B model can achieve
performance (50.92) comparable to GPT-4o (51.00).

</details>


### [87] [Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input](https://arxiv.org/abs/2510.05864)
*Faeze Ghorbanpour,Alexander Fraser*

Main category: cs.CL

TL;DR: 本文系统评估了主流LLM在不同情境下检测长文本有害内容的能力，发现其在安全关键应用场景面临诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLMs）在处理长文本上下文时应用广泛，尤其在涉及推理和检索方面已有较多研究，但其在安全关键场景下对于有害内容的识别行为尚不清楚。本文旨在系统性研究LLMs在长文本情境下对有害内容的敏感性及相关特征。

Method: 通过实验，评估LLMs（如LLaMA-3、Qwen-2.5、Mistral）在不同有害内容类别（如毒性、冒犯、仇恨言论）下的表现。实验变量包括有害内容的类型（显性/隐性）、位置（开头、中间、结尾）、比例（0.01-0.5）、上下文长度（600-6000 tokens）。

Result: 实验结果发现：当有害内容占比中等（0.25）时，LLMs检测性能最佳；内容稀疏或过度主导时性能下降。随着上下文加长，召回率降低。开头的有害内容较其他位置更容易被检测。显性内容总体上比隐性内容更易识别。

Conclusion: 首次系统性展现了LLMs在长文本情境下对有害内容的优先性与校准方式，揭示其在安全关键场景下的能力及仍待解决的挑战。

Abstract: Large language models (LLMs) increasingly support applications that rely on
extended context, from document processing to retrieval-augmented generation.
While their long-context capabilities are well studied for reasoning and
retrieval, little is known about their behavior in safety-critical scenarios.
We evaluate LLMs' sensitivity to harmful content under extended context,
varying type (explicit vs. implicit), position (beginning, middle, end),
prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).
Across harmful content categories such as toxic, offensive, and hate speech,
with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance
peaks at moderate harmful prevalence (0.25) but declines when content is very
sparse or dominant; recall decreases with increasing context length; harmful
sentences at the beginning are generally detected more reliably; and explicit
content is more consistently recognized than implicit. These findings provide
the first systematic view of how LLMs prioritize and calibrate harmful content
in long contexts, highlighting both their emerging strengths and the challenges
that remain for safety-critical use.

</details>


### [88] [The fragility of "cultural tendencies" in LLMs](https://arxiv.org/abs/2510.05869)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文通过扩展实验对LSZ的结论进行了检验，发现LLM的“文化倾向”其实是实验设计造成的伪现象，提示语言对输出影响微弱，反驳了原研究的核心观点。


<details>
  <summary>Details</summary>
Motivation: LSZ认为大语言模型（LLM）在不同语言下展现出文化特定的倾向，如用中文提示时更具整体和依赖性，用英文时更分析和独立，并将这些差异归因于模型中的深层文化模式。鉴于此，本文作者希望重新审视这些结论以及背后的方法论。

Method: 作者对LSZ的研究进行了批判性再评估，在理论构架、实验方法与结论等方面提出质疑。他们设计了针对性复现实验，使用更广泛的LLM模型和更多的测试条目。

Result: 实验结果表明，提示语言对模型输出影响很小，挑战了LSZ关于模型具备扎根的文化信仰的主张。

Conclusion: LSZ报告的“文化倾向”并不是稳定特征，而是特定模型和任务设计下的脆弱产物。提示语言不足以引发实质性的文化转变。

Abstract: In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large
language models (LLMs), when prompted in different languages, display
culturally specific tendencies. They report that the two models (i.e., GPT and
ERNIE) respond in more interdependent and holistic ways when prompted in
Chinese, and more independent and analytic ways when prompted in English. LSZ
attribute these differences to deep-seated cultural patterns in the models,
claiming that prompt language alone can induce substantial cultural shifts.
While we acknowledge the empirical patterns they observed, we find their
experiments, methods, and interpretations problematic. In this paper, we
critically re-evaluate the methodology, theoretical framing, and conclusions of
LSZ. We argue that the reported "cultural tendencies" are not stable traits but
fragile artifacts of specific models and task design. To test this, we
conducted targeted replications using a broader set of LLMs and a larger number
of test items. Our results show that prompt language has minimal effect on
outputs, challenging LSZ's claim that these models encode grounded cultural
beliefs.

</details>


### [89] [Prompt reinforcing for long-term planning of large language models](https://arxiv.org/abs/2510.05921)
*Hsien-Chin Lin,Benjamin Matthias Ruppik,Carel van Niekerk,Chia-Hao Shen,Michael Heck,Nurul Lubis,Renato Vukovic,Shutong Feng,Milica Gašić*

Main category: cs.CL

TL;DR: 用强化学习思路对任务prompt优化，显著提升LLM多轮任务能力且通用性强。


<details>
  <summary>Details</summary>
Motivation: LLM在多轮交互中容易基于错误假设进行回复，且难以持续追踪用户目标，导致多轮任务效果欠佳。长期规划能力被认为对多轮交互任务至关重要。

Method: 通过生成逐轮反馈，并利用经验回放机制对任务指令prompt进行重写，优化LLM的多轮互动能力。

Result: 该方法在text-to-SQL和任务驱动的多轮对话等任务上显著提升了模型表现，并且能够通用于不同类型的LLM代理，还能利用多样化的LLM作为meta-prompting agents。

Conclusion: 提出的基于强化学习启发的prompt优化框架有效提升了LLM在多轮任务中的表现，并且具有良好的通用性和推广性。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of natural language processing tasks and can be adapted through prompting.
However, they remain suboptimal in multi-turn interactions, often relying on
incorrect early assumptions and failing to track user goals over time, which
makes such tasks particularly challenging. Prior works in dialogue systems have
shown that long-term planning is essential for handling interactive tasks. In
this work, we propose a prompt optimisation framework inspired by reinforcement
learning, which enables such planning to take place by only modifying the task
instruction prompt of the LLM-based agent. By generating turn-by-turn feedback
and leveraging experience replay for prompt rewriting, our proposed method
shows significant improvement in multi-turn tasks such as text-to-SQL and
task-oriented dialogue. Moreover, it generalises across different LLM-based
agents and can leverage diverse LLMs as meta-prompting agents. This warrants
future research in reinforcement learning-inspired parameter-free optimisation
methods.

</details>


### [90] [Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens](https://arxiv.org/abs/2510.05931)
*Mai AlKhamissi,Yunze Xiao,Badr AlKhamissi,Mona Diab*

Main category: cs.CL

TL;DR: 文章批评现有大模型文化评测方法过于简化，提出新评测框架并基于人类学理论给出改进建议，促进评测方法更贴近真实文化复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前文化基准将文化简单化，与人类学关于文化动态和多样性的理解相悖，所以需要重新审视评估方式，以更好地反映实际文化情境。

Method: 提出一个四部分评估框架，分析现有20个文化基准，识别常见方法问题，并借鉴人类学方法提出改进建议。

Result: 发现六个普遍方法错误，并提出具体改进措施，如用真实叙事、情境参与和社区协作等提升基准的表现力。

Conclusion: 目前的大语言模型文化评估标准存在方法论缺陷，需结合人类学方法来改进，使评测更加准确反映文化复杂性。

Abstract: Cultural evaluation of large language models has become increasingly
important, yet current benchmarks often reduce culture to static facts or
homogeneous values. This view conflicts with anthropological accounts that
emphasize culture as dynamic, historically situated, and enacted in practice.
To analyze this gap, we introduce a four-part framework that categorizes how
benchmarks frame culture, such as knowledge, preference, performance, or bias.
Using this lens, we qualitatively examine 20 cultural benchmarks and identify
six recurring methodological issues, including treating countries as cultures,
overlooking within-culture diversity, and relying on oversimplified survey
formats. Drawing on established anthropological methods, we propose concrete
improvements: incorporating real-world narratives and scenarios, involving
cultural communities in design and validation, and evaluating models in context
rather than isolation. Our aim is to guide the development of cultural
benchmarks that go beyond static recall tasks and more accurately capture the
responses of the models to complex cultural situations.

</details>


### [91] [EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models](https://arxiv.org/abs/2510.05942)
*Hadi Mohammadi,Anastasia Giachanou,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 本文提出EvalMORAAL框架对20种大型语言模型进行全球道德对齐评估，发现模型总体现状良好但地区存在显著偏差，模型间互评机制可作为质量检测工具，对跨文化AI提出新挑战。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在道德对齐方面的表现尚不明确，且可能存在地区性偏见。作者希望通过创新性评估框架，系统性且公平地比较不同模型在全球文化中的道德对齐情况，推动更文化敏感的人工智能发展。

Method: 该研究提出EvalMORAAL评估框架，包含透明的链式思维推理（CoT），两种评分方法（log概率与直接评分），以及模型作为评审（peer review）机制。使用World Values Survey和PEW Global Attitudes Survey的多国家多议题数据，对20个大型语言模型进行道德对齐测试，并量化模型与真实调查结果的相符程度。

Result: 顶尖模型的道德对齐度与全球调查结果密切相关（WVS的Pearson相关系数约0.90），但存在明显的地区性偏差：西方地区相关系数平均0.82，非西方地区平均0.61，相差0.21。模型评审机制可以有效发现348起冲突，且评审一致性与调查对齐度显著相关。

Conclusion: EvalMORAAL框架实现了全面且透明的道德对齐评估，推动了文化敏感性AI的研究，但也暴露了不同地区模型表现的差异与挑战，为后续改进提供依据。

Abstract: We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that
uses two scoring methods (log-probabilities and direct ratings) plus a
model-as-judge peer review to evaluate moral alignment in 20 large language
models. We assess models on the World Values Survey (55 countries, 19 topics)
and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,
top models align closely with survey responses (Pearson's r approximately 0.90
on WVS). Yet we find a clear regional difference: Western regions average
r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),
indicating consistent regional bias. Our framework adds three parts: (1) two
scoring methods for all models to enable fair comparison, (2) a structured
chain-of-thought protocol with self-consistency checks, and (3) a
model-as-judge peer review that flags 348 conflicts using a data-driven
threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,
both p<.001), supporting automated quality checks. These results show real
progress toward culture-aware AI while highlighting open challenges for use
across regions.

</details>


### [92] [Probing the Difficulty Perception Mechanism of Large Language Models](https://arxiv.org/abs/2510.05969)
*Sunbowen Lee,Qingyu Yin,Chak Tou Leong,Jialiang Zhang,Yicheng Gong,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文发现LLM具备内在问题难度感知并呈结构性分布，利用该能力可支持自动难度标注及相关应用，减少人工成本，并为模型理论研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否具备自动感知和评估问题难度的能力，以促进更高效的资源分配、自动标签标注，以及减少人工参与。

Method: 通过线性探针方法分析LLM对数学问题的难度编码，并定位Transformer层特定注意力头的激活模式，并进行消融实验验证。

Result: 发现LLM的最终Token表征中可以线性建模问题难度，特定注意力头展现了对简单/困难问题的不同激活模式，且消融实验验证了定位准确性。还揭示了Token级别的熵与难度感知存在显著差异。

Conclusion: LLMs不仅可以感知问题难度，而且其难度感知在模型中具有结构性组织。

Abstract: Large language models (LLMs) are increasingly deployed on complex reasoning
tasks, yet little is known about their ability to internally evaluate problem
difficulty, which is an essential capability for adaptive reasoning and
efficient resource allocation. In this work, we investigate whether LLMs
implicitly encode problem difficulty in their internal representations. Using a
linear probe on the final-token representations of LLMs, we demonstrate that
the difficulty level of math problems can be linearly modeled. We further
locate the specific attention heads of the final Transformer layer: these
attention heads have opposite activation patterns for simple and difficult
problems, thus achieving perception of difficulty. Our ablation experiments
prove the accuracy of the location. Crucially, our experiments provide
practical support for using LLMs as automatic difficulty annotators,
potentially substantially reducing reliance on costly human labeling in
benchmark construction and curriculum learning. We also uncover that there is a
significant difference in entropy and difficulty perception at the token level.
Our study reveals that difficulty perception in LLMs is not only present but
also structurally organized, offering new theoretical insights and practical
directions for future research.

</details>


### [93] [LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language](https://arxiv.org/abs/2510.05972)
*Periklis Mantenoglou,Rishi Hazra,Pedro Zuidberg Dos Martires,Luc De Raedt*

Main category: cs.CL

TL;DR: 本文提出了可扩展的自然语言约束规划基准LexiCon，并发现较强约束下，主流LLM的规划表现明显下降，表明其在现实复杂场景应用中仍有待提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）虽然具备一定的规划和推理能力，但大多只在无约束的规划场景下被评测。现实中，诸如安全等约束条件非常重要，因此需要检验LLM在存在约束（尤其是安全约束）场景下的规划能力。

Method: 提出了LexiCon，一个基于自然语言的约束规划基准套件。它通过在已有的规划环境中添加时序约束，并将这些约束问题转化为自然语言，交给LLM求解。同时，LexiCon具有可扩展性，支持新环境和自动生成约束，随着LLM能力提升可增加问题难度。

Result: 实验发现，当前最先进的LLM（包括GPT-5、o3和R1等）在约束加强的规划任务中表现明显下降。

Conclusion: LLM在存在复杂约束的规划任务中的能力有限，未来发展需关注提升模型对约束场景的适应性和推理能力。

Abstract: Owing to their reasoning capabilities, large language models (LLMs) have been
evaluated on planning tasks described in natural language. However, LLMs have
largely been tested on planning domains without constraints. In order to deploy
them in real-world settings where adherence to constraints, in particular
safety constraints, is critical, we need to evaluate their performance on
constrained planning tasks. We introduce LexiCon -- a natural language-based
(Lexi) constrained (Con) planning benchmark, consisting of a suite of
environments, that can be used to evaluate the planning capabilities of LLMs in
a principled fashion. The core idea behind LexiCon is to take existing planning
environments and impose temporal constraints on the states. These constrained
problems are then translated into natural language and given to an LLM to
solve. A key feature of LexiCon is its extensibility. That is, the set of
supported environments can be extended with new (unconstrained) environment
generators, for which temporal constraints are constructed automatically. This
renders LexiCon future-proof: the hardness of the generated planning problems
can be increased as the planning capabilities of LLMs improve. Our experiments
reveal that the performance of state-of-the-art LLMs, including reasoning
models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of
the planning tasks increases.

</details>


### [94] [Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments](https://arxiv.org/abs/2510.06001)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文比较了两种常用评估复杂句法习得的指标，发现直接最小对比法（wh-effect）比 DiD 法更能真实反映 GPT-2 模型在寄生空缺语法上的能力，强调研究中评估方法的选择对结论影响显著。


<details>
  <summary>Details</summary>
Motivation: 近期对于刺激贫乏论 (APS) 的研究，旨在探讨大型语言模型 (LLM) 是否能够通过不同评估指标学习复杂句法结构，但结果分歧引发了对这些指标有效性的质疑。本文希望澄清评估方法对研究结论的影响。

Method: 作者采用了 Wilcox 等人提出的直接最小对比分析法（wh-effect），生成了完整的八种排列的寄生空缺 (PG) 刺激材料，对 GPT-2 进行系统性测试，评估其对填补-空缺依赖关系的掌握。

Result: GPT-2 在所有四个条件下表现出色，表明其在复杂 PG 环境下对填补-空缺许可原则具备稳健的知识和泛化能力。

Conclusion: 与 DiD 式指标相比，直接最小对比法能更透明地诊断模型的句法能力，评估指标的选择对于判断 LLM 句法能力至关重要。

Abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS)
have applied Large Language Models (LLMs) to test the learnability of complex
syntax through surprisal-based metrics. However, divergent conclusions raise
questions concerning the insights these metrics offer. While Wilcox et al.
(2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate
that models successfully generalise knowledge of filler-gap dependencies, Lan
et al. (2024) used a Difference-in-Differences (DiD) metric and found that
models largely fail on parasitic gaps (PGs). This paper argues that the direct
minimal pair approach offers greater diagnostic transparency. We demonstrate
this by generating a full 8-permutation paradigm of refined PG stimuli and
evaluating the GPT-2 model used in previous studies with a systematic
Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across
all four tested conditions, indicating robust knowledge of filler-gap licensing
principles even in complex PG environments. This finding, which contrasts with
the more ambiguous results from DiD-style metrics, suggests that the choice of
evaluation metric is critical for assessing an LLM's syntactic competence.

</details>


### [95] [MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation](https://arxiv.org/abs/2510.06005)
*Qin Dong,Yuntian Tang,Heming Jia,Yunhang Shen,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Shaohui Lin*

Main category: cs.CL

TL;DR: LoRA存在特征抽取瓶颈，MASA通过多A专家不对称共享设计，提升了微调表现，在多个任务和基准上优于LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA是一种主流的大语言模型参数高效微调方法，但其仅使用单一的下投影矩阵A，容易因特征抽取能力有限而对复杂任务出现表现瓶颈。为了解决该代表性瓶颈，论文试图扩展特征适配能力，从而提升下游任务表现。

Method: 提出MASA（Multi-A Shared Adaptation）架构，采用多A单B结构，实现多A专家集在不同层之间的不对称共享，以保持参数效率。多个A专家负责捕捉多样特征，随后通过单个、层特定的B矩阵进行集成。

Result: 实验涵盖多领域泛化、单领域特化和多任务推理多个场景。以MMLU基准为例，MASA平均准确率达到59.62%，比标准LoRA高出1.08个百分点（相对提升1.84%），且可训练参数占比与LoRA相当（0.52%）。

Conclusion: MASA通过引入多A专家结构，有效提升特征抽取和下游任务适应能力，在参数高效前提下超越传统LoRA，在多个下游任务中表现出更强的通用性和实用性。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a dominant method in
Parameter-Efficient Fine-Tuning (PEFT) for large language models, which
augments the transformer layer with one down-projection $A$ and one
up-projection $B$. However, LoRA's reliance on a single down-projection matrix
($A$) creates a representational bottleneck, as this solitary feature extractor
is inherently insufficient for capturing the diverse signals required by
complex tasks. This motivates our architectural shift to focus on enriching the
feature adaptation to improve the downstream task adaptation ability. We
propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a
multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is
asymmetrically shared across layers to ensure parameter efficiency. In MASA,
these specialized experts capture diverse features, which are then integrated
by a single, layer-specific $B$-matrix. The effectiveness and versatility of
our method are validated through a comprehensive suite of experiments spanning
multi-domain generalization, single-domain specialization, and multi-task
reasoning. For example, on the MMLU benchmark, MASA achieves an average
accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative
improvement of 1.84%) with comparable learnable parameters of 0.52%.

</details>


### [96] [Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance](https://arxiv.org/abs/2510.06018)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: 论文针对LLM句法能力评测，提出刺激材料或数据集的结构和词汇质量会显著影响评估结果。通过改进数据集设计，GPT-2在语法预测任务上的表现受到显著提升，强调合理设计实验材料对模型能力判断的必要性。


<details>
  <summary>Details</summary>
Motivation: 近期关于大型语言模型（LLM）在“刺激贫乏论证”（APS）测试中的研究结果，在不同句法现象上存在差异。作者怀疑，近期研究所用刺激材料的词汇歧义和结构复杂性等因素，可能影响了模型表现。

Method: 方法分两步：1）在以前使用过的（筛选与未筛选）刺激材料上建立基线；2）用先进的生成式LLM（Gemini 2.5 Pro Preview）和语言学模板，生成一组经过改进的新数据集，以减少刺激材料中的混淆因素。主要围绕GPT-2模型进行评估。

Result: 初步结果显示，在这类精炼后的PG刺激材料上，GPT-2表现明显优于基线，表现出更高的句法预测能力。

Conclusion: 刺激材料质量对基于惊异度评估LLM句法能力的实验结果影响很大。改进刺激设计可提升模型的表现，因此应重视数据集质量对模型评估的重要性。

Abstract: Recent studies employing Large Language Models (LLMs) to test the Argument
from the Poverty of the Stimulus (APS) have yielded contrasting results across
syntactic phenomena. This paper investigates the hypothesis that
characteristics of the stimuli used in recent studies, including lexical
ambiguities and structural complexities, may confound model performance. A
methodology is proposed for re-evaluating LLM competence on syntactic
prediction, focusing on GPT-2. This involves: 1) establishing a baseline on
previously used (both filtered and unfiltered) stimuli, and 2) generating a
new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5
Pro Preview) guided by linguistically-informed templates designed to mitigate
identified confounds. Our preliminary findings indicate that GPT-2 demonstrates
notably improved performance on these refined PG stimuli compared to baselines,
suggesting that stimulus quality significantly influences outcomes in
surprisal-based evaluations of LLM syntactic competency.

</details>


### [97] [CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs](https://arxiv.org/abs/2510.06039)
*Chengwei Wu,Jiapu Wang,Mingyang Gao,Xingrui Zhuo,Jipeng Guo,Runlin Lei,Haoran Luo,Tianyu Chen,Haoyi Zhou,Shirui Pan,Zechao Li*

Main category: cs.CL

TL;DR: 本论文针对中文大模型评估难题，提出了覆盖多领域的结构化中文语料库和综合基准，有助于模型知识能力和泛化性能的科学测评，为研究社区提供了开源工具和数据。


<details>
  <summary>Details</summary>
Motivation: 中文大模型由于文本非结构化以及缺乏结构化表示，评估体系薄弱，现有主流基准多以英文为主，无法反映中文语言独有特性。缺乏高质量结构化数据集影响模型能力的全面评测。

Method: 构建了大规模中文结构化数据集CDTP，包含700万配对文本和结构三元组，以及1500万个三元组，支持知识图谱补全、三元组生成文本、问答等任务。提出CB-ECLLM基准，并通过广泛实验和消融研究验证基准和方法的有效性与鲁棒性。

Result: 建立了多领域、大规模中英文数据对齐的新基准，有效提升了中文大模型在知识驱动任务上的评估能力。开放源码并提出未来研究方向。

Conclusion: CDTP和CB-ECLLM显著丰富中文结构化语料资源，提升了中文大模型的细粒度评估和多任务适应性，为该领域后续研究打下坚实基础。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language processing tasks. However, Chinese LLMs face unique
challenges, primarily due to the dominance of unstructured free text and the
lack of structured representations in Chinese corpora. While existing
benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly
English-centric and fail to address the unique linguistic characteristics of
Chinese, lacking structured datasets essential for robust evaluation. To
address these challenges, we present a Comprehensive Benchmark for Evaluating
Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese
Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million
aligned text pairs, each consisting of unstructured text coupled with one or
more corresponding triples, alongside a total of 15 million triples spanning
four critical domains. The core contributions of CDTP are threefold: (i)
enriching Chinese corpora with high-quality structured information; (ii)
enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)
supporting multi-task fine-tuning to assess generalization and robustness
across scenarios, including Knowledge Graph Completion, Triple-to-Text
generation, and Question Answering. Furthermore, we conduct rigorous
evaluations through extensive experiments and ablation studies to assess the
effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.
To support reproducible research, we offer an open-source codebase and outline
potential directions for future investigations based on our insights.

</details>


### [98] [ASPO: Asymmetric Importance Sampling Policy Optimization](https://arxiv.org/abs/2510.06062)
*Jiakang Wang,Runze Liu,Lei Lin,Wenping Hu,Xiu Li,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: 现有LLM强化学习方法中，token权重分配不均影响训练。本文提出ASPO方法有效矫正重要性采样比例，并通过双剪裁机制提升稳定性和性能，实验验证效果优异。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型的后训练方法在强化学习中依赖于按token级别的剪裁机制，但作者发现现有的Outcome-Supervised RL (OSRL)范式存在一个根本缺陷：正向优势token的重要性采样（IS）比例失衡，导致正负token的权重分配不均。这会抑制低概率token的更新并过度强化高概率token，影响模型性能。

Method: 提出了Asymmetric Importance Sampling Policy Optimization (ASPO)方法，通过对正向优势token翻转IS比例，将其更新方向与负向token一致；此外还引入了软双剪裁机制，用于稳定极端更新并保持梯度流动。

Result: 在编码和数学推理基准任务上，ASPO显著缓解了模型过早收敛问题，提高了训练稳定性及最终性能，超过了强力的GRPO类基线方法。

Conclusion: 研究揭示了token级权重在OSRL中的新见解，强调了在LLM强化学习中修正IS比例的重要性。ASPO方法效果显著，相关代码和模型已公开。

Abstract: Recent Large Language Model (LLM) post-training methods rely on token-level
clipping mechanisms during Reinforcement Learning (RL). However, we identify a
fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance
Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to
unbalanced token weighting for positive and negative tokens. This mismatch
suppresses the update of low-probability tokens while over-amplifying already
high-probability ones. To address this, we propose Asymmetric Importance
Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy
that flips the IS ratios of positive-advantage tokens, aligning their update
direction with the learning dynamics of negative ones. AIS further incorporates
a soft dual-clipping mechanism to stabilize extreme updates while maintaining
gradient flow. Comprehensive experiments on coding and mathematical reasoning
benchmarks demonstrate that ASPO significantly mitigates premature convergence,
improves training stability, and enhances final performance over strong
GRPO-based baselines. Our analysis provides new insights into the role of
token-level weighting in OSRL and highlights the critical importance of
correcting IS in LLM RL. The code and models of ASPO are available at
https://github.com/wizard-III/Archer2.0.

</details>


### [99] [Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability](https://arxiv.org/abs/2510.06084)
*Taylor Sorensen,Benjamin Newman,Jared Moore,Chan Park,Jillian Fisher,Niloofar Mireshghallah,Liwei Jiang,Yejin Choi*

Main category: cs.CL

TL;DR: 传统后训练虽提升了模型指令遵循能力，但削弱了多结果空间下的灵活分布覆盖。本文提出Spectrum Suite和Spectrum Tuning，大幅增强语言模型在多样数据分布上的可引导性与对齐能力。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型在指令跟随和各种任务上的表现已经通过后训练得到了提升，但这种做法在多答案任务上带来了结果空间覆盖不足等问题。作者指出现有的后训练方案可能降低模型在上下文引导、结果有效性和分布对齐等三方面的能力，因此亟需更细致的评估标准和方法。

Method: 作者提出三项建模指标，并设计了Spectrum Suite——一个覆盖90余任务、整合40余数据源的大规模资源库，用于全面评测模型在多样数据分布上的表现。同时，作者提出了Spectrum Tuning方法，使用Spectrum Suite进行后训练，以提升模型的分布覆盖度和上下文可引导性。

Result: 实验发现，常规的后训练确实有助于提升模型固有知识提取，但削弱了模型可上下文引导的能力。采用Spectrum Tuning能有效改善模型在多样分布上的表现，在输出空间覆盖、上下文可引导性和分布对齐等指标上相较传统与指令微调模型均有提升。

Conclusion: Spectrum Tuning在多分布、多任务场景下提升了语言模型的灵活性和泛化能力，可更好覆盖有效输出空间，并更好地对齐真实分布，为后续大模型后训练提供了新方向和工具支持。

Abstract: Language model post-training has enhanced instruction-following and
performance on many downstream tasks, but also comes with an often-overlooked
cost on tasks with many possible valid answers. We characterize three
desiderata for conditional distributional modeling: in-context steerability,
valid output space coverage, and distributional alignment, and document across
three model families how current post-training can reduce these properties. In
particular, we disambiguate between two kinds of in-context learning: ICL for
eliciting existing underlying knowledge or capabilities, and in-context
steerability, where a model must use in-context information to override its
priors and steer to a novel data generating distribution. To better evaluate
and improve these desiderata, we introduce Spectrum Suite, a large-scale
resource compiled from >40 data sources and spanning >90 tasks requiring models
to steer to and match diverse distributions ranging from varied human
preferences to numerical distributions and more. We find that while current
post-training techniques help elicit underlying capabilities and knowledge,
they hurt models' ability to flexibly steer in-context. To mitigate these
issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite
to improve steerability and distributional coverage. We find that Spectrum
Tuning often improves over pretrained models and their instruction-tuned
counterparts, enhancing steerability, spanning more of the output space, and
improving distributional alignment on held-out datasets.

</details>


### [100] [The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2510.06101)
*Muyu He,Muhammad Ali Shafique,Anand Kumar,Tsach Mackey,Nazneen Rajani*

Main category: cs.CL

TL;DR: 本文研究了在小模型中蒸馏大模型的代码推理能力时，性能随训练数据量的变化规律，首次发现性能会先降后升，即存在“代码推理谷”，并指出数据的难度和正确性对性能提升规律的具体作用。


<details>
  <summary>Details</summary>
Motivation: 将具有推理能力的大型语言模型（LLM）的思维轨迹知识蒸馏至小模型已经被证明有效，但鲜有研究关注蒸馏数据量与模型性能之间的缩放关系。本文旨在揭示在不同数据量下，代码推理蒸馏对小模型性能的影响规律及其机制。

Method: 在两个小型、无推理能力的LLM上，蒸馏具备推理能力的大模型的“竞赛编程能力”，并通过调节训练数据量来观察下游任务性能变化趋势。此外，在相同数据集上于蒸馏过程的两个阶段分别微调模型，以分析不同学习阶段的小模型表现。进一步细分易难题目和训练数据输出正确性的作用。

Result: 发现当蒸馏数据量增加时，下游竞赛编程性能先下降后上升，呈现出比对数线性更快的提升趋势，存在“代码推理谷”效应；在数据量较低阶段，小模型从简单题目中受益更多；训练数据输出的正确性对蒸馏效果无影响。

Conclusion: 代码推理蒸馏过程中，数据量和题目难度对小模型性能提升有显著影响，揭示了训练动力学中混合难度和样本正确性的新见解，对未来相关模型微调方法有启发意义。

Abstract: Distilling the thinking traces of a Large Language Model (LLM) with reasoning
capabilities into a smaller model has been proven effective. Yet, there is a
scarcity of work done on how model performances scale with the quantity of
distillation data. In this work, we study the scaling trend of distilling
competitive coding skills on two small non-reasoning LLMs. We validate the
hypothesis that there is a $\textit{valley of code reasoning}$: downstream
performance on competitive coding first drops as data quantity increases, then
it steadily increases in a sharper-than-log-linear fashion. Having identified
the trend, we further fine-tune the models at two different distillation stages
on the same data to ground conclusions on their respective learning phases. We
learn that across stages in the low and medium-low data regimes, small models
benefit significantly from easier coding questions than from harder ones. We
also find that, surprisingly, the correctness of outputs in training data makes
no difference to distillation outcomes. Our work represents a step forward in
understanding the training dynamics of code reasoning distillation outside
intuition

</details>


### [101] [Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models](https://arxiv.org/abs/2510.06107)
*Gagan Bhatia,Somayajulu G Sripada,Kevin Allan,Jacobo Azcona*

Main category: cs.CL

TL;DR: 提出DST追踪与分析框架，发现LLM幻觉由模型内部层次与两种推理路径冲突导致，可据此预测和理解幻觉机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易产生幻觉，即生成看似合理但实际上不真实的陈述，作者希望探究其发生的深层次结构性原因。

Method: 提出了Distributional Semantics Tracing (DST) 框架，结合既有可解释性技术，能追踪模型内部语义失效路径，并基于分布式语义揭示原因。同时，分析模型各层的表示演化，定位“承诺层”，并利用双过程理论解释失效机制。

Result: 1. DST方法能清晰追踪模型推理中的误差点。2. 发现模型会在特定承诺层不可逆地偏离事实。3. 识别到内部两条路径（一条类比于直觉的System 1，一条对应于深层上下文的System 2），前者会主导导致失效。4. 量化上下文一致性后，发现与幻觉发生率显著负相关（$ho = -0.863$）。

Conclusion: 本文提出了统一的可解释框架，揭示了LLM幻觉的结构性根源，并系统解释了其产生的时机、表现层次与内在机制。提供了一种预测与干预幻觉发生的新视角。

Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of
plausible yet factually incorrect statements. This work investigates the
intrinsic, architectural origins of this failure mode through three primary
contributions.First, to enable the reliable tracing of internal semantic
failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified
framework that integrates established interpretability techniques to produce a
causal map of a model's reasoning, treating meaning as a function of context
(distributional semantics). Second, we pinpoint the model's layer at which a
hallucination becomes inevitable, identifying a specific \textbf{commitment
layer} where a model's internal representations irreversibly diverge from
factuality. Third, we identify the underlying mechanism for these failures. We
observe a conflict between distinct computational pathways, which we interpret
using the lens of dual-process theory: a fast, heuristic \textbf{associative
pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway}
(akin to System 2), leading to predictable failure modes such as
\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the
coherence of the contextual pathway reveals a strong negative correlation
($\rho = -0.863$) with hallucination rates, implying that these failures are
predictable consequences of internal semantic weakness. The result is a
mechanistic account of how, when, and why hallucinations occur within the
Transformer architecture.

</details>


### [102] [Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer](https://arxiv.org/abs/2510.06128)
*Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: 本文提出并行分词器框架，有效对齐语义等价词，提升多语言模型的跨语种迁移和下游任务表现，在低资源场景中表现尤为显著。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法未能有效促进语义等价词的共享表示，导致多语言模型跨语种迁移能力受限，特别是在低资源环境下。

Method: 先对各语言独立训练分词器，再结合双语词典或单词对单词翻译，将语义等价词的分词索引进行对齐，最终在13种低资源语言上对预训练编码器进行多项下游任务评测。

Result: 在情感分析、仇恨言论检测、情绪分类和句子嵌入相似度等任务上，采用并行分词器的模型全面优于传统多语言基线。

Conclusion: 采用并行分词器（parallel tokenizers）能够提升多语言模型在低资源语言上的跨语言迁移和下游任务表现。重新思考分词机制对于多语言表示学习具有重要意义。

Abstract: Tokenization defines the foundation of multilingual language models by
determining how words are represented and shared across languages. However,
existing methods often fail to support effective cross-lingual transfer because
semantically equivalent words are assigned distinct embeddings. For example, "I
eat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped to
different vocabulary indices, preventing shared representations and limiting
cross-lingual generalization. We introduce parallel tokenizers. This new
framework trains tokenizers monolingually and then aligns their vocabularies
exhaustively using bilingual dictionaries or word-to-word translation, ensuring
consistent indices for semantically equivalent words. This alignment enforces a
shared semantic space across languages while naturally improving fertility
balance. To assess their effectiveness, we pretrain a transformer encoder from
scratch on thirteen low-resource languages and evaluate it on sentiment
analysis, hate speech detection, emotion classification, and sentence embedding
similarity. Across all tasks, models trained with parallel tokenizers
outperform conventional multilingual baselines, confirming that rethinking
tokenization is essential for advancing multilingual representation
learning--especially in low-resource settings.

</details>


### [103] [CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits](https://arxiv.org/abs/2510.06133)
*Kangyu Wang,Zhiyun Jiang,Haibo Feng,Weijia Zhao,Lin Liu,Jianguo Li,Zhenzhong Lan,Weiyao Lin*

Main category: cs.CL

TL;DR: 本文提出Trace Credit与CreditDecoding算法，通过利用历史logit加速dLLM置信度收敛，减少冗余步骤，实现显著的解码加速和性能提升，且易于集成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型（dLLM）并行解码时，由于置信度初期较低，token常被重复掩码，导致冗余迭代并限制加速效果；急需减少无意义的解码步骤，提高推理效率。

Method: 分析dLLM在解码过程中的token预测轨迹，引入Trace Credit（通过累积历史logit衡量token收敛潜力），并提出无训练的CreditDecoding算法，将当前logit与Trace Credit融合，加速token置信度收敛，实现高效并行解码。

Result: CreditDecoding在八个基准测试中相较LLaDA-8B-Instruct速度提升5.48倍，性能提升0.48；较LLaDA-MoE-Instruct速度提升4.11倍，性能提升0.15。可扩展至长序列，与主流优化方法兼容。

Conclusion: CreditDecoding大幅减少了冗余迭代，提高了解码效率和鲁棒性，并在多个基准测试上取得显著性能提升，而且能够与主流推理优化方法协同工作。

Abstract: Diffusion large language models (dLLMs) generate text through iterative
denoising steps, achieving parallel decoding by denoising only high-confidence
positions at each step. However, existing approaches often repetitively remask
tokens due to initially low confidence scores, leading to redundant iterations
and limiting overall acceleration. Through the analysis of dLLM decoding
traces, we observe that the model often determines the final prediction for a
token several steps before the decoding step. To leverage this historical
information and avoid redundant steps, we introduce the concept of Trace
Credit, which quantifies each token's convergence potential by accumulating
historical logits. Furthermore, we propose CreditDecoding, a training-free
parallel decoding algorithm that accelerates the confidence convergence of
correct but underconfident tokens by fusing current logits with Trace Credit.
This process significantly reduces redundant iterations and enhances decoding
robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup
and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times
speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.
Importantly, CreditDecoding scales effectively to long sequences and is
orthogonal to mainstream inference optimizations, making it a readily
integrable and versatile solution.

</details>


### [104] [RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets](https://arxiv.org/abs/2510.06143)
*Jan Cegin,Branislav Pecher,Ivan Srba,Jakub Simko*

Main category: cs.CL

TL;DR: 本文提出了一种无需真人标注的LLM生成器评价新方法RoSE，通过训练小模型并互测合成数据表现，能有效选出最佳生成器，在多个语言和任务上超越现有评价指标，且与真实测试集结果高度相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）可以作为合成数据的强大生成器，帮助训练针对特定任务的小模型。这对于数据稀缺、人工标注困难的低资源语言尤为有价值。目前，如何有效选择最佳的LLM生成模型存在挑战，特别是因为依赖真人标注进行外部评估成本高，且内在评价指标与真实下游效果相关性较差。

Method: RoSE方法流程为：针对每个候选LLM生成器，用其合成数据训练一个小模型，然后让该小模型在其他LLM生成的数据上进行评估。最终RoSE分数取该模型在其它生成器样本上的平均表现，用以衡量每个LLM生成器的优劣。

Result: 提出的新方法RoSE在六个LLM、十一种语言和三项任务（情感、主题、意图）上均优于现有本体（intrinsic）启发式方法。RoSE能够更频繁地选出最佳生成器，并与最优基线的下游性能仅差0.76个百分点；同时，RoSE是唯一与人工测试数据性能呈正相关的方法。

Conclusion: RoSE不仅可以在无人工标注条件下有效选出最优LLM生成器，还在多个语言和任务上保持下游性能，与人工评估结果高度吻合，解决了低资源语言合成数据评估的难题。

Abstract: LLMs are powerful generators of synthetic data, which are used for training
smaller, specific models. This is especially valuable for low-resource
languages, where human-labelled data is scarce but LLMs can still produce
high-quality text. However, LLMs differ in how useful their outputs are for
training. Selecting the best LLM as a generator is challenging because
extrinsic evaluation requires costly human annotations (which are often
unavailable for low-resource languages), while intrinsic metrics correlate
poorly with downstream performance. We introduce Round robin Synthetic data
Evaluation (RoSE), a proxy metric for selecting the best LLM generator without
human test sets. RoSE trains a small model on the outputs of a candidate
generator (LLM) and then evaluates it on generated synthetic examples from all
other candidate LLMs. The final RoSE score is the mean performance of this
small model. Across six LLMs, eleven languages, and three tasks (sentiment,
topic, intent), RoSE identifies the optimal generator more often than any other
intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within
0.76 percentage points of the optimal generator baseline. This result is
measured in terms of downstream performance, obtained by training a small model
on the chosen generator's outputs (optimal vs. proxy metric selected) and
evaluating it on human-labelled test data. Additionally, RoSE is the only
metric to achieve a positive correlation with performance on human test data.

</details>


### [105] [VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization](https://arxiv.org/abs/2510.06175)
*Dingyu Yao,Chenxu Yang,Zhengyang Tong,Zheng Lin,Wei Liu,Jian Luan,Weiping Wang*

Main category: cs.CL

TL;DR: VecInfer是一种新型向量量化方法，通过变换抑制缓存异常值以实现极致KV缓存压缩，在极低比特下也能保持高性能，并通过CUDA优化显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: KV缓存在大型语言模型推理时引入大量内存开销，现有VQ方法在低比特下性能急剧下降，主要因Key缓存异常值影响码本利用率，亟需更高效且压缩率高的方案。

Method: 采用平滑和Hadamard变换抑制Key缓存中的异常值，使码本对原始数据分布有更全面覆盖，降低量化难度；同时设计融合计算和反量化的CUDA优化内核以减少内存访问开销。

Result: 2比特量化下，VecInfer可达全精度性能，同时长上下文理解和数学推理任务均优于主流量化基线；推理速度提升2.7倍，端到端延迟降低8.3倍（以Llama-3.1-8B的196k序列长度为例）。

Conclusion: VecInfer极大压缩KV缓存，显著减少内存开销，在极低比特情况下仍保持高性能，优于现有量化方法。

Abstract: The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.

</details>


### [106] [Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context](https://arxiv.org/abs/2510.06182)
*Yoav Gur-Arieh,Mor Geva,Atticus Geiger*

Main category: cs.CL

TL;DR: 针对语言模型在上下文推理中的实体绑定问题，作者发现传统的基于位置的检索机制在复杂场景下效果有限，模型会依赖词汇和反身机制补充检索能力。提出的三机制因果模型能高准确率预测文本分布，并能延伸到自然场景，升华了对大模型检索机制的理解。


<details>
  <summary>Details</summary>
Motivation: 此前研究发现，语言模型在少量实体绑定任务中主要通过位置机制进行实体检索，但尚不清楚随着绑定实体数量增加，这一机制是否依然可靠，特别是在更复杂场景下。作者希望系统分析和揭示语言模型绑定和检索实体的更多机制。

Method: 对九种模型和十个实体绑定任务进行大量实验，通过分析模型的行为模式，探索语言模型在绑定和检索实体时采用的不同机制，包括位置机制、词汇机制和反身机制，并提出综合三种机制的因果模型以预测下一个 token 的分布。

Result: 发现随着绑定实体数量增加，单一的位置信息检索变得不可靠，语言模型会补充词汇机制和反身机制用于检索实体；提出的因果模型结合三种机制后能以95%的准确率估计下一个 token 的分布，并证实在更长更开放文本场景下依旧有效。

Conclusion: 语言模型在上下文中结合位置、词汇与反身机制进行实体绑定和检索，三者混合驱动模型行为，这一发现完善了我们对语言模型上下文推理的理解。

Abstract: A key component of in-context reasoning is the ability of language models
(LMs) to bind entities for later retrieval. For example, an LM might represent
"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"
when asked "Who loves pie?" Prior research on short lists of bound entities
found strong evidence that LMs implement such retrieval via a positional
mechanism, where "Ann" is retrieved based on its position in context. In this
work, we find that this mechanism generalizes poorly to more complex settings;
as the number of bound entities in context increases, the positional mechanism
becomes noisy and unreliable in middle positions. To compensate for this, we
find that LMs supplement the positional mechanism with a lexical mechanism
(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism
(retrieving "Ann" through a direct pointer). Through extensive experiments on
nine models and ten binding tasks, we uncover a consistent pattern in how LMs
mix these mechanisms to drive model behavior. We leverage these insights to
develop a causal model combining all three mechanisms that estimates next token
distributions with 95% agreement. Finally, we show that our model generalizes
to substantially longer inputs of open-ended text interleaved with entity
groups, further demonstrating the robustness of our findings in more natural
settings. Overall, our study establishes a more complete picture of how LMs
bind and retrieve entities in-context.

</details>


### [107] [RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback](https://arxiv.org/abs/2510.06186)
*Chunyu Miao,Henry Peng Zou,Yangning Li,Yankai Chen,Yibo Wang,Fangxin Wang,Yifan Li,Wooseong Yang,Bowei He,Xinni Zhang,Dianzhi Yu,Hanchen Yang,Hoang H Nguyen,Yue Zhou,Jie Yang,Jizhou Guo,Wenzhe Fan,Chin-Yuan Yeh,Panpan Meng,Liancheng Fang,Jinhu Qi,Wei-Chieh Huang,Zhengyao Gu,Yuwei Han,Langzhou He,Yuyao Yang,Xue Liu,Irwin King,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出RECODE-H基准和ReCodeAgent框架，强调多轮交互和反馈提升LLM科研代码工作的表现，但生成复杂代码仍有困难。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在支持科学研究实施方面具有潜力，但其生成正确且可执行代码的能力仍有限。现有工作大多采用一次性代码生成方法，忽略了实际科学研究开发中的反馈驱动和迭代本质。作者希望弥补这一缺口。

Method: 提出了RECODE-H，一个包含102个来自科研论文和开源代码库的任务的基准，采用多轮交流，结合LLM模拟的人类反馈来评价LLM智能体。其设计包括结构化指令、单元测试和五级反馈层级，反映现实中的研究者-智能体协作。作者还提出了ReCodeAgent框架，将反馈融入迭代式代码生成流程。

Result: 最新版主流LLMs（如GPT-5、Claude-Sonnet-4、DeepSeek-V3.1、Gemini 2.5）在丰富反馈帮助下表现显著提升，但复杂科研代码生成仍面临挑战。

Conclusion: RECODE-H为发展能适应反馈、驱动科研实施的LLM智能体奠定了基础。

Abstract: Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation

</details>


### [108] [BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects](https://arxiv.org/abs/2510.06188)
*Jakir Hasan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本工作提出了BanglaTalk——首个支持孟加拉语区域方言的低延迟、低带宽实时语音助手系统，并通过微调IndicWav2Vec显著提升了方言识别准确率，推动了少数语言语音技术的包容性与可访问性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语是一种资源匮乏且方言多样的语言，现有语音助手系统未能实现实时优化，且只关注标准孟加拉语，导致区域语言用户难以受益。

Method: 提出BanglaTalk，一个实时孟加拉语区域方言语音助手系统，采用客户端-服务器架构及RTP协议降低延迟。并开发了BRDialect——一个在十种孟加拉语方言上微调IndicWav2Vec的方言感知ASR模型，并在RegSpeech12数据集上进行评测。

Result: BRDialect模型在十种孟加拉语方言数据上比基线模型高12.41-33.98%；BanglaTalk系统可在24kbps低带宽下运行，平均端到端延迟4.9秒。

Conclusion: BanglaTalk系统实现了低延迟、低带宽的孟加拉语多方言实时语音助手，明显提升了区域用户的信息可及性和包容性。

Abstract: Real-time speech assistants are becoming increasingly popular for ensuring
improved accessibility to information. Bengali, being a low-resource language
with a high regional dialectal diversity, has seen limited progress in
developing such systems. Existing systems are not optimized for real-time use
and focus only on standard Bengali. In this work, we present BanglaTalk, the
first real-time speech assistance system for Bengali regional dialects.
BanglaTalk follows the client-server architecture and uses the Real-time
Transport Protocol (RTP) to ensure low-latency communication. To address
dialectal variation, we introduce a dialect-aware ASR system, BRDialect,
developed by fine-tuning the IndicWav2Vec model in ten Bengali regional
dialects. It outperforms the baseline ASR models by 12.41-33.98% on the
RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of
24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low
bandwidth usage and minimal end-to-end delay make the system both
cost-effective and interactive for real-time use cases, enabling inclusive and
accessible speech technology for the diverse community of Bengali speakers.

</details>


### [109] [Latent Speech-Text Transformer](https://arxiv.org/abs/2510.06195)
*Yen-Ju Lu,Yashesh Gaur,Wei Zhou,Benjamin Muller,Jesus Villalba,Najim Dehak,Luke Zettlemoyer,Gargi Ghosh,Mike Lewis,Srinivasan Iyer,Duc Le*

Main category: cs.CL

TL;DR: LST通过高效聚合语音tokens，提升模型的数据和算力利用，显著优化语音-文本任务表现，将公开全部资源用于进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语音-文本模型因语音 token 长度远超文本 token，造成训练和推理过程中计算资源不均衡，影响语音与文本的表示对齐和模型扩展能力。

Method: 提出一种 Latent Speech-Text Transformer（LST）模型，该模型动态且高效地将语音 token 聚合为潜在语音 patch，用于更好地对齐与转移文本与语音表示能力，并提高模型训练与推理的效率。

Result: 在语音到语音以及文本到文本基准上，LST 在数据与计算受控设置下均超过了原始方法。例如，在 HellaSwag 故事补全任务中，LST 在计算受控训练下语音准确率提升6.5%、在数据受控下提升5.3%，同时文本性能也有提升，并承诺公开模型和代码。

Conclusion: Latent Speech-Text Transformer（LST）通过将语音 token 聚合为更高层次的语音 patch，提升了语音-文本模型在数据效率与计算效率方面的表现，显著优于原始方法。

Abstract: Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.

</details>


### [110] [Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction](https://arxiv.org/abs/2510.06198)
*Xinyu Guo,Zhengliang Shi,Minglai Yang,Mahdi Rahimi,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 论文提出CogRE框架，结合认知推理和RL优化，解决RE任务中的可解释性与准确性难题，在一次性关系抽取任务实验中获得F1和解释质量的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的关系抽取(RE)方法在可解释性和准确性方面存在不足，尤其是在缺乏语言解释监督的一次性学习(one-shot RE)任务中，易出现关注错位和学习能力有限的问题。

Method: 该论文提出了CogRE框架，主要包含两个部分：(1) 以认知科学启发的推理机制，将关系抽取建模为一系列文本处理步骤；(2) 通过强化学习(RL)和新颖奖励函数进行优化，奖励不仅提升任务准确性，也增强解释质量。框架自动构建高质量关系关键词词典，用于生成更好的语言解释，并在两个大型语言模型和两个RE数据集上一轮一实验。

Result: CogRE框架在One-shot NYT29测试集上利用Qwen2.5-15B-Instruct模型，实现了24.65%的F1值，优于之前的推理型设计。在强化学习奖励优化后F1值进一步提升23.46%(绝对提升)。人工评测表明，模型生成的关系关键词与真实标签高度吻合，解释质量评级提升54%。

Conclusion: CogRE显著提升了一次性关系抽取任务中的准确性和可解释性，为无监督解释的关系抽取方法提供了新的解决路径。此框架有望推广于其它需要高可解释性的NLP任务。

Abstract: This paper introduces a framework for relation extraction (RE) that enhances
both accuracy and explainability. The framework has two key components: (i) a
reasoning mechanism that formulates relation extraction as a series of
text-processing steps inspired by cognitive science, and (ii) an optimization
process driven by reinforcement learning (RL) with a novel reward function
designed to improve both task accuracy and explanation quality. We call our
approach CogRE. Our framework addresses the lack of supervision for
language-based explanations in traditional RE by promoting outputs that include
important relation keywords. These keywords are drawn from a high-quality
dictionary that is automatically constructed using an LLM. We evaluate our
approach for the task of one-shot RE using two LLMs and two RE datasets. Our
experiments show that CogRE improves explanation quality by addressing two
common failure patterns in one-shot RE: poor attention focus and limited
one-shot learning capability. For example, our cognitive-structured reasoning
with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing
prior reasoning-based designs. Optimizing this approach with RL using our
reward further improves performance by +23.46% (absolute). Finally, human
evaluation shows that our best model generates relational keywords closely
aligned with gold labels, increasing human explanation quality ratings by 54%
(relative).

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [111] [Iterating Non-Aggregative Structure Compositions](https://arxiv.org/abs/2510.06019)
*Marius Bozga,Radu Iosif,Florian Zuleger*

Main category: cs.FL

TL;DR: 本文研究了非聚合型的融合操作对图和关系结构集合树宽的影响，证明了用HR文法描述的集合在融合下树宽有界可判定，且可由HR文法有效表达，这对图结构的理论分析及自动逻辑检查有重要应用。


<details>
  <summary>Details</summary>
Motivation: 目前图代数和形式图语言理论主要依赖于聚合型组合操作（如不相交并集），但这些操作的接口受限于标记好的顶点或常量符号。研究非聚合型组合能否保持图的结构特征如树宽，有助于拓展对图结构的理解和应用。

Method: 研究一种非聚合型组合操作——融合（fusion），它通过非确定性地连接不同结构中的元素。利用有限的超边替换（HR）文法，对通过融合迭代生成结构树宽是否有界性进行判定，分析其可判定性并与HR文法的语言划分关联。

Result: 证明了对于用HR文法归纳描述的集合，通过融合闭包后是否保持树宽有界是可判定的；如果融合闭包结果确实有界，则此集合的语言可以由有效构造的HR文法来生成。

Conclusion: 融合操作有助于分析与扩展图结构的生成能力，相关树宽判定与表达能力具备可判定性和有效生成性，对结构理论及自动逻辑判定有直接意义。

Abstract: An aggregative composition is a binary operation obeying the
  principle that the whole is determined by the sum of its parts. The
  development of graph algebras, on which the theory of formal graph
  languages is built, relies on aggregative compositions that behave
  like disjoint union, except for a set of well-marked interface
  vertices from both sides, that are joined. The same style of
  composition has been considered in the context of relational
  structures, that generalize graphs and use constant symbols to label
  the interface.
  In this paper, we study a non-aggregative composition operation,
  called \emph{fusion}, that joins non-deterministically chosen
  elements from disjoint structures. The sets of structures obtained
  by iteratively applying fusion do not always have bounded
  tree-width, even when starting from a tree-width bounded set.
  First, we prove that the problem of the existence of a bound on the
  tree-width of the closure of a given set under fusion is decidable,
  when the input set is described inductively by a finite
  \emph{hyperedge-replacement} (HR) grammar, written using the
  operations of aggregative composition, forgetting and renaming of
  constants. Such sets are usually called \emph{context-free}.
  Second, assuming that the closure under fusion of a context-free set
  has bounded tree-width, we show that it is the language of an
  effectively constructible HR grammar. A possible application of the
  latter result is the possiblity of checking whether all structures
  from a non-aggregatively closed set having bounded tree-width
  satisfy a given monadic second order logic formula.

</details>
