<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 提出DecoRTL——一种侧重语法和对比学习的新解码策略，实现大语言模型在RTL代码生成上的准确性、多样性提升且几乎没有性能损耗，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在自动生成寄存器传输级（RTL）代码时表现出不足，主要原因是传统的解码策略仅为自然语言设计，无法满足RTL代码对结构和语义的严格要求，导致生成的代码出现逻辑错误、重复或无效。

Method: 作者首先通过实证分析生成过程中token级别的熵，发现模型在结构不明确或语义复杂区表现出较低信心。然后提出DecoRTL——一种新颖的、运行时的语法感知&对比式解码策略，具体包括：1）自一致性采样，根据token间一致性进行重排序，以提升正确性与多样性；2）语法感知温度自适应，针对语法/功能不同的token调整温度，对关键语法低温采样、探索性内容高温采样。整个方法仅在推理阶段实现，无需重新微调模型。

Result: 在多个开放源代码的大语言模型和VerilogEval基准测试集上评测，结果显示该方法在代码的语法有效性、功能正确性和生成多样性上都有显著提升，且算力开销几乎可以忽略。

Conclusion: 本论文揭示了标准LLM解码策略在RTL代码生成中的不足，并提出了无需模型重训练的高效解码改进方法DecoRTL，在保证推理性能的同时显著提高了生成代码的质量和多样性。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 提出用LLM将自然语言搜索自动转为结构化代码查询，有效提升代码搜索准确率和可用性，优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 开发者在理解API、学习代码模式和代码导航中经常需要查找代码。目前，大部分代码搜索依赖关键词和正则表达式，虽然易用，但功能有限。结构化代码搜索虽更强大，但通常依赖难以学习的领域特定语言（DSL）作为查询条件，阻碍了开发者的使用。

Method: 提出了一种将自然语言代码搜索请求翻译为结构化代码搜索工具可理解的DSL的方法，利用大语言模型（LLM）的推理能力，将自然语言查询转化为DSL查询，并通过现有的结构化代码搜索工具（如Semgrep和GQL）进行检索。并在Java项目上构建了包含400个查询的新基准用于评估。

Result: 该方法在结构化代码搜索基准上取得了55%至70%之间的高查准率与查全率，显著优于语义代码搜索及LLM直接检索的基线结果，F1分数提升幅度分别高达57%和14%。

Conclusion: 利用LLM将自然语言查询转化为结构化代码搜索DSL查询显著提升了搜索的准确性与易用性，极大降低了开发者使用结构化代码搜索工具的门槛。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 技术面试对求职者来说复杂且压力大，主流课程和准备方式实际帮助有限。作者通过调研分析准备现状，提出需改进课程与面试训练方式以更好支持求职者。


<details>
  <summary>Details</summary>
Motivation: 技术面试准备过程复杂且与实际课程内容脱节，现有教育体系难以让学生适应现实面试情境。探究不同准备方式及教学体系在技术面试准备中的作用。

Method: 作者通过向131位正在准备技术面试的候选人发放并回收问卷，分析调查数据，总结面试准备的方法及其效果。

Result: 调查显示，候选人很少在真实环境中训练，现有课程对面试准备支持有限，导致候选人压力大且准备不足。

Conclusion: 现有的计算机课程和常规准备方式并未有效支持技术面试的准备，候选人在面试准备时大量体验到压力和不足。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [4] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究以446个开源安卓应用为对象，探索通过源代码内部指标预测应用流行度。虽然回归表现较差，但用分类方法（多层感知机）可以取得较好F1分数（0.72），证实内部代码指标对预测流行度有一定帮助。


<details>
  <summary>Details</summary>
Motivation: 在移动应用市场竞争激烈的情况下，开发者希望在应用发布前预测其受欢迎程度，但这依然是个难题。作者希望探索用源代码内部可测量的软件度量指标来预测应用流行度的可能性。

Method: 作者收集了F-Droid上的446个开源Android应用，提取了系统级、类级和方法级的软件度量指标、代码异味和应用元数据，并结合来自Google Play的用户评论、下载量及权限数据。然后，设计回归和分类模型，对三种特征集（只包含应用规模、手动选定、自动特征选择）进行评估。

Result: 回归模型由于数据偏斜，表现不佳（低R^2）；但转为二分类（流行与不流行）后显著提升，多层感知机模型（Voting特征集）F1得分可达0.72，显示内部代码度量指标在预测应用流行度方面具备一定价值。

Conclusion: 内部代码度量指标虽解释力有限，但作为流行度预测因子仍具有一定作用，这与早期研究否定其作为软件质量预测因子的结论相矛盾。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [5] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 本研究比较了软件工程任务中自评压力与生物指标之间的差异，发现传统心理量表和部分生物指标难以捕捉由时间压力诱导的压力反应。提出今后研究需改进压力诱导和测量方法。


<details>
  <summary>Details</summary>
Motivation: 传统的人类因素（如幸福感、压力等）研究主要依赖于自我报告工具，但这些工具可能存在偏差，因此近年来人们越来越关注将自我报告与更客观的生理测量方法结合使用。

Method: 作者设计了一个实验，要求参与者在完成两个编程任务的过程中佩戴生物传感器，并在任务前后分别填写了调查问卷和简短的访谈。实验主要目的是比较心理测量与生物指标间的差异，并寻找生物数据中的压力相关模式。

Result: 实验结果显示各方面结论不一：心理量表没有检测到压力，访谈反馈为无压力或时间压力的混合感受，生物测量中只有EDA瞬时峰值表现出显著差异。

Conclusion: 通过给参与者设定更严苛时间限制诱发压力的方式，并没有有效诱发明显压力反应。作者给未来结合压力、生物测量和心理量表的研究提出了方法学启示。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [6] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 软件开发沟通平台的数据特性影响情感分析工具表现。本文提出基于数据特征推荐工具的方法，助力选择适合的分析工具，并验证了transformer模型的优越性，但强调需持续关注沟通情境变化对工具效果的影响。


<details>
  <summary>Details</summary>
Motivation: 在软件开发过程中，文本沟通十分重要，情感分析被用于理解团队动态和增强可信赖的AI分析，但现有工具在不同平台的数据集上的表现不一致，因此有必要研究其原因并提升工具选择的科学性和准确性。

Method: 分析了来自五个平台的10个开发者沟通数据集的语言和统计特征，并评估了14种情感分析工具的性能，据此提出了一种基于特征映射的方法和推荐问卷，用于为新数据集推荐合适的情感分析工具。

Result: 数据集的语言与统计特征可显著提升情感分析工具的选择，不同平台间存在较大差异。基于transformer的模型（如SetFit和RoBERTa）通常表现较好，但工具有效性依赖具体上下文。

Conclusion: 研究为软件工程中的情感分析工具选择提供了有效方案和支持，强调了随沟通场景变化需要持续评估工具。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [7] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 本文提出一种基于双LLM智能体协作的COBOL代码解释新方法，在函数、文件及项目等多个层次均显著优于基准，有效提升了COBOL老旧系统的易维护性和理解效率。


<details>
  <summary>Details</summary>
Motivation: COBOL语言由于年代久远、结构复杂且开发人员数量不断减少，维护变得越来越困难，尤其是缺乏文档使得新开发者难以理解和维护COBOL系统。现有LLM（大语言模型）在解释代码片段方面取得了一定成效，但COBOL的特殊结构和语法导致源代码经常超出LLM的token窗口，增加了解释难度。

Method: 提出了一种多智能体方法，使用两个基于LLM的agent协同工作。通过将代码库的上下文信息融合进代码解释提示，实现对函数、文件及整个项目级别的代码解释。方法在14个开源COBOL项目中进行了评测。

Result: 在函数级代码解释方面，本方法以METEOR提升12.67%、chrF提升18.59%、SentenceBERT提升0.62%显著优于基线。在文件级别上，对长度超过LLM token窗口的COBOL文件也能有效解释，分别在文件目的、功能和解释清晰度上超越基线4.21%、10.72%和14.68%。在项目级别，本方案能为82%的项目生成反映其功能和目的的解释。

Conclusion: 提出的多智能体LLM方法能有效缓解COBOL代码维护中的文档缺乏与复杂性难题，在代码功能解释的多个层次上均取得了优于现有方法的效果。方法特别适应超长COBOL文件和大型项目，有助于提升COBOL系统可维护性。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [8] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出RTED，一种结合类型约束分析和反射验证的自动化Python类型错误检测方法，相较现有技术精度和覆盖度大幅提升，并已在多项基准和实际项目中验证有效。


<details>
  <summary>Details</summary>
Motivation: Python类型错误常导致运行时故障，而现有静态分析和自动化测试生成工具误报率高，或生成的用例缺乏针对性，难以真实暴露类型错误。因此，有必要开发一种更高效且更准确的方法来自动检测Python类型错误。

Method: RTED结合了逐步类型约束分析和反射性验证，引导单元测试生成，以更有效地检测类型错误，并减少误报。研究通过在BugsInPy和TypeBugs这两个基准上的实验评估其效果。

Result: RTED比4种现有技术多检测出22-29个基准类型错误，并将误报率提升了173.9%~245.9%。此外，还在6个真实开源项目中发现了12个先前未知类型错误。

Conclusion: RTED显著提升了Python类型错误检测的准确性和实用性，不仅能检测到更多错误，还能有效减少误报，在实际开源项目中也发现了新型错误。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [9] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 本文提出了一个新框架VeFIA，能高效、无隐私泄露地审计垂直联邦学习场景下的数据方推理软件，具有极高的检测准确率和无在线延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的垂直联邦学习（VFL）方案无法审计数据方推理软件的执行正确性，存在安全和可信隐患。

Method: 提出了VeFIA（Vertical Federated Inference Auditing）框架，利用可信执行环境（TEE）和协调者，通过随机抽样和结果验证来进行软件执行的审计，无需获取原始数据，也不引入推理延迟。

Result: VeFIA可以在异常推理比例超过5.4%时，以99.99%的概率检测到异常，并且其随机抽样验证在检测异常推理的准确性（正预测值、负预测值、真正率）均达到100%。

Conclusion: VeFIA首次提出了VFL推理软件执行正确性的审计框架，兼顾安全性、隐私保护和效率，为跨域协作AI系统提供了更高的可信保障。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [10] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 本文提出一种基于变形测试和LLM自身生成与评估能力的公平性自动测试方法Meta-Fair，在偏见检测上取得了高效且高精度的结果，有助于提升LLM测试的自动化和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统中的公平性（即没有无根据的偏见）是一个核心原则，但当前大语言模型（LLM）的公平性测试方法多为人工评估、固定模板和人工构建数据集，难以扩展且耗费资源。作者希望提出一种自动化方法，减少依赖领域资源，提升测试的适用性与效率。

Method: 提出Meta-Fair方法，结合了变形测试（通过可控修改输入提示语来考察模型输出变化，依据预设的变形关系MRs）和利用LLM自身能力承担生成测试用例和输出分类。提供三种开源工具，支持用例生成、执行与评测。通过对12个预训练LLM，14种MRs、5个偏见维度和7900个自动化测试用例进行了实验。

Result: Meta-Fair对揭示LLM偏见有效，平均精度达92%，29%的测试中检测到偏见行为。LLM在评估中表现出较高一致性，最佳模型F1分数达0.79。尽管非确定性影响一致性，但通过精心设计MRs，可以缓解该问题。

Conclusion: Meta-Fair为自动化大语言模型公平性测试开辟了新路径，大幅简化和提升了偏见检测流程。尽管尚有挑战，但初步结果证明该方法具有良好前景，有望实现前所未有的自动化水平。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [11] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的需求获取访谈系统LLMREI，在模拟访谈中表现出与人类访谈者相当的错误率和上下文适应能力，有望提升大规模需求获取过程的自动化与效率。


<details>
  <summary>Details</summary>
Motivation: 需求获取访谈对于收集系统需求至关重要，但极度依赖经验丰富的分析师，导致成本高、人为偏见和沟通障碍。大型语言模型的发展为自动化该流程提供了新机会。

Method: 提出LLMREI聊天机器人，采用零样本提示（zero-shot prompting）和由易到难提示（least-to-most prompting）两种方法优化其需求获取表现，并在33场模拟访谈中评估了其性能。微调（fine-tuning）方法因早期效果不佳而放弃。

Result: LLMREI产生的错误数量与人类访谈者相近，能够提取大量相关需求，并展现出根据上下文生成问题的能力。碳针对多个受访者的大规模自动化访谈中优势明显。

Conclusion: LLMREI可以有效减小人工访谈中的常见错误，提高需求获取的自动化和可拓展性，尤其适用于涉及大量利益相关者的场景。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [12] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 本文针对自适应网络物理系统（CPS）中的人机团队协作难题，提出将人机交互与伦理价值系统性集成的方法与框架，提升CPS的协作效率和人性化水平。


<details>
  <summary>Details</summary>
Motivation: 自适应网络物理系统（CPS）需要实现更高级、无缝的人机协作，以充分发挥人类和机器的优势。但当前将人类高效整合到CPS反馈循环中面临操作节奏不同、用户隐私和人类价值尊重等重大挑战。

Method: 提出了新的人机协同集成方法和过程，专注于人机交互原理及其在CPS自适应反馈循环中的应用。同时开发了贯穿系统生命周期的伦理与人类价值嵌入、验证和确认框架，并将其从需求工程阶段开始实施。

Result: 研究开发了面向自适应CPS的人机协同方法、人机交互集成机制，以及涵盖伦理与人类价值要求的完整框架，有助于实现更高效且符合人本要求的CPS设计。

Conclusion: 为在自适应CPS中实现高效、道德且人性化的人机协作，本文提出了系统性的方法和流程，为未来CPS系统设计和实施提供理论和实践基础。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [13] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: RSE与SER群体因术语不同导致沟通难题，本文提出映射方法识别其异同，结果为双方合作和知识互补提供基础，也支持未来通过众包方式扩展改进。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程师（RSE）和软件工程研究者（SER）虽然在同一领域工作，但常因使用不同术语描述相似概念，导致沟通障碍。作者试图理解这些分歧并解决相关挑战。

Method: 作者采用系统性的方法，对SER社区的软件工程基础在RSE社区中的解释进行调查，寻找两者的一致观点、知识差距及可改进领域，并开展术语映射。

Result: 初步结果显示，RSE与SER社区间存在相互学习与合作的机会；所提出的术语映射方法为未来通过众包进行扩展和验证打下基础。

Conclusion: 研究发现，规范化术语和加强两类群体间的沟通可促进知识交流和协作，未来还需更多人共同参与检验和完善该术语映射。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [14] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: 本文提出RLHGNN框架，将事件日志异质图建模和强化学习结构选择结合，实现了对流程中顺序和非顺序关系的精准建模及高效预测，在多数据集上优于现有方法，并具备实时应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有用于业务流程中下一个活动预测的方法大多基于序列，但难以建模并行执行和条件依赖等非顺序关系。即使是图神经网络方法，也因为结构同质、不区分不同流程复杂度而受到限制，因此提升预测准确性和适应不同流程结构是一个有待解决的问题。

Method: 提出了一种全新的RLHGNN框架，将事件日志转化为包含三类边的异质流程图，并基于流程挖掘理论组合成四种灵活图结构，通过将结构选择建模为马尔可夫决策过程（MDP）的强化学习，自动为每个流程实例选择最优图结构，使用异质图卷积和关系特定的聚合策略进行下一个活动预测。

Result: RLHGNN在六个真实数据集上稳定优于当前最先进方法，并在每次预测时推理延迟大约为1毫秒，适用于实时业务流程监控。

Conclusion: RLHGNN 能自适应不同流程结构地融合顺序和非顺序关系，显著提升了下一个活动预测性能，且具有低延迟，非常适合应用于实际业务流程监控。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [15] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 该研究提出了通过“可持续性标记”识别云架构讨论中可持续性内容的新方法，经实验证明相比传统定义方法更准确、高效且易用。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和云架构的兴起，软件系统可持续性的重要性日益增加。然而，在业界实践者的讨论中识别可持续性概念非常困难，缺乏明确的指导标准。

Method: 通过分析多家云服务商的可持续性最佳实践，采用主题分析法提出了“可持续性标记”作为在相关讨论中指示可持续性概念的工具，并通过对比实验法在实际云架构论坛帖子中评估其有效性。

Result: 初步实验结果表明，使用可持续性标记虽然使得被归类为与可持续性相关的帖子数量变少，但在确定性和任务表现上都有明显提升，实践者也认为这些标记比单纯依靠定义更有用、更易理解。

Conclusion: 可持续性标记能够更有效、可靠、便捷地帮助软件架构讨论中识别可持续性相关内容，优于仅靠定义识别。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [16] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文提出一种基于文本蕴涵和上下文学习的新方法，从法律文本自动生成结构化Python代码表示，既减少人工标注，又提升了新法规适应能力。实验证明该方法在实际法规数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动从法律文本中提取元数据以辅助软件系统符合法规要求，这是一个劳动密集且对小型组织尤其困难的任务，现有自动化方法要么忽视了元数据间的关联，要么泛化性差，需要大量人工标注。

Method: 采用基于文本蕴涵和上下文学习的方法，利用手工设计的Python类结构作为领域专用元模型，从法律文本自动生成结构化、可执行的Python代码，并捕获元数据及其关联关系。

Result: 在13个美国州数据泄露通报法规上的实验显示，所生成的法律文本表示能够通过约89.4%的测试案例，精准率和召回率分别达到82.2和88.7。

Conclusion: 本文提出的方法能够提升从法律文本中自动生成结构化、可执行Python代码表示的能力，减少了对大量人工标注的需求，在新法规上的适用性更强。实验结果显示该方法具有较高的准确率和覆盖率。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [17] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 用GPT-4o模型自动生成访谈跟进问题，对比人工与AI生成。结果显示AI生成的提问质量不输甚至优于人类，特别是在错误类型引导下。说明大模型能实时提升需求访谈效率和质量。


<details>
  <summary>Details</summary>
Motivation: 面向软件系统需求获取阶段，访谈常用于收集利益相关者需求。然而，合格访谈问题实时提问面临诸多挑战，包括领域不熟悉、认知负担重和信息过载。为此，需要更高效地支撑访谈者开展工作。

Method: 该研究探索应用GPT-4o大语言模型，基于常见的访谈者错误类型框架和受访者言论，自动生成访谈的跟进问题。文中设计了两个对照实验：一是用最小引导下LLM-生成和人工编写问题对比，二是引导下（按错误类型）比较LLM生成与人工题目。

Result: 实验表明，无论是否引导，LLM生成的问题在清晰度、相关性和信息量方面不逊于人工问题。在按错误类型引导下，LLM生成的问题表现更优。

Conclusion: 大语言模型，特别是结合访谈常见错误类型引导下，能有效帮助提高访谈质量和便捷性。实践中LLM具备实时辅助访谈者的潜力。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: 该论文提出首个可支持字级操作的SMT-Sweep化简技术，极大提升了硬件验证中涉及复杂算术及数组的处理效率，实现性能大幅领先，并已开源。


<details>
  <summary>Details</summary>
Motivation: 随着硬件验证中逐渐采用更多字级（word-level）构造（如位向量操作、算数运算和数组），现有以SAT sweeping为代表的位级（bit-level）化简手段已难以应对，更缺乏字级处理的对应方案。

Method: 提出了一种名为SMT-Sweep的新技术，将SAT sweeping扩展到字级，基于SMT（可满足性模理论）框架。该方法结合了结构化HASH、模拟和等价检测，支持丰富的位向量与数组操作，并采用了随机和约束驱动的字级模拟。

Result: 与主流位级SAT sweeping及单体化字级SMT方法相比，SMT-Sweep平均分别加速44倍和69倍。

Conclusion: SMT-Sweep首次将sweeping技术应用到SMT驱动的硬件验证，支持复杂字级运算，性能显著提升，工具已开源。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [19] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: 本文在DHOL中简洁地实现了难以自动化支持的细化类型和商类型，完善了表达能力且维持了自动证明优势。


<details>
  <summary>Details</summary>
Motivation: 现有的高阶逻辑（HOL）自动定理证明器常无法优雅地支持细化类型和商类型，因为这些类型通常需要不可判定的类型系统。为了提升表达力和满足实践需求，作者希望在不会严重牺牲自动证明支持的前提下，实现这些类型扩展。

Method: 作者将细化类型和商类型作为子类型的一种特殊情况添加到DHOL体系中。借助DHOL对HOL的完备可翻译性，作者提出了细化与商类型的语法、语义和翻译方案，并证明了其健全性与完备性。通过巧妙设计，使相关的包含映射与投影映射成为恒等映射，避免引入表示层面的额外代价。

Result: 成功扩展了DHOL，支持细化类型和商类型，并且保留了到HOL的自动化定理证明优势。理论推导了扩展后系统的健全性与完备性。实现上，加入这两类类型变得既简洁又优雅，无须大幅更改表示方式。

Conclusion: 本文证明了依赖类型高阶逻辑（DHOL）可在几乎不增加复杂度的前提下优雅扩展细化类型与商类型，兼顾表达性和可自动定理证明能力，为类型系统与自动证明工具的发展提供了新路径。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


### [20] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 本文创新性地将含连续导数实函数的无量词可满足性判定归约为仅含实数判定，丰富了Tarski体系相关方法，实现了更复杂函数性质的检测。


<details>
  <summary>Details</summary>
Motivation: 目前，针对未加量词（unquantified）的语言的可满足性检测方法有限。作者希望扩展既有方法，将包含具有连续一阶导数的实值单变量函数的表达能力片段纳入其判定范围，支持更丰富的函数属性与关系判定。

Method: 设计一种预处理方法，将包含函数变量和导数的原公式，转化为等价于无函数声明、仅涉及实数的无量词公式。所有函数变量都会被实数变量集合替换。最终通过Tarski决策法判定结果。这一过程中，需引入足够灵活的$C^1$插值函数家族以确保保持可满足性的等价性。

Result: 得到了一个基于Tarski决策法的新可满足性测试框架，能够检验涉及函数变量、导数和多种区间属性的无量词公式。实函数相关判断成功转化为纯实数公式，无需直接处理函数。

Conclusion: 提出的新方法扩展了Tarski元素代数中针对涉及实函数及其属性的片段的可满足性检测能力，并已证明能等价地转换与检测相应无量词实例。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [21] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 本论文建立了直觉主义条件逻辑的新证明系统，包括引入“might”条件的新变体，并给出了相应的模型和公理化扩展，推动了条件推理的建设性分析研究。


<details>
  <summary>Details</summary>
Motivation: 直觉主义条件逻辑旨在对条件推理进行建设性的分析。在该框架下，would条件和might条件算子不再互可定义，而现有文献主要关注在既有CK条件逻辑（使用选择函数语义）的基础上，将其引入到直觉主义和建设主义框架下的可行性和扩展。作者试图弥补对带“might”条件运算符扩展与模型刻画的空白。

Method: 作者在传统CK条件逻辑和直觉主义模态逻辑的证明系统基础上，为无'might'条件算子的两个变体IntCK（直觉主义）和CCKbox（建设主义）分别引入了嵌套演算（nested calculus）和序列演算（sequent calculus）。进而基于序列演算提出同时具有would和might条件的新系统CCK，并给出了相应的模型类和公理化，并对若干扩展作了推广。

Result: 作者成功为IntCK和CCKbox引入了有效的证明系统。对于扩展的带“might”算子的CCK逻辑系统，给出了序列演算、模型语义和完整公理系统，并拓展到多个相关系统。整体上推动了直觉主义条件逻辑在理论体系和模型结构上的发展。

Conclusion: 本文系统完成了直觉主义条件逻辑的证明系统与语义模型的扩展，从无'might'到带'might'条件逻辑，并对模型类与相关公理系统进行了探讨，为后续条件推理研究提供了坚实的理论基础。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了一个源于中文语境的多任务偏见评测基准，并用以系统性地测试主流LLMs，揭示它们在中文环境下存在多样且显著的偏见，为模型偏见研究提供了新工具和洞见。


<details>
  <summary>Details</summary>
Motivation: 当前偏见评测数据集多集中于英语和北美文化，且类别单一，难以适用于其他文化环境。此外，面向中文及中国文化的系统评测数据集稀缺，现有资源通常仅支持单一评测任务，无法多维度评估LLMs中的偏见。研究意在填补上述空白。

Method: 本研究构建了一个涵盖4,077条实例的多任务中文偏见评测基准（McBE）。该基准覆盖12个单类偏见、82个子类，并引入了5种评测任务。随后，作者用不同系列与参数规模的主流LLM进行实证评测和分析。

Result: 该基准大大扩展了中文偏见评测任务和内容。多种主流LLMs在此基准上的测试显示，它们普遍存在不同程度的偏见。通过系统实验和多方面分析，研究为LLMs偏见提供了新的理解。

Conclusion: 研究最终指出，现有的大型语言模型（LLMs）在中文语境下表现出不同程度的偏见。通过多方面、系统性地评估，揭示了LLMs的偏见广泛性与复杂性，并提供了对LLMs偏见的新见解。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [23] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本研究首次系统检验了推理型与非推理型LLMs在多种对话摘要任务中的表现，发现显式推理无法一贯提升摘要质量，并可能引发冗长和事实错误，指出对话摘要需专门的建模与评价机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在对话摘要任务上有很大应用价值，但现有关于长链式思维（Chain-of-Thought, CoT）推理架构模型在对话摘要中的表现尚不明确。作者希望探索这些架构的实际效果和局限性。

Method: 系统性地评估了代表性推理型LLMs（如OpenAI-o1, DeepSeek-R1）和非推理型LLMs在三大对话摘要范式（通用、角色导向、查询导向）上的表现，涵盖多语种、多领域、多摘要长度，使用了SAMSum、DialogSum、CSDS、QMSum等基准，并采用LLM自动与人工启发式评价标准。

Result: 与其他重推理任务不同，明确分步推理并不能稳定提升对话摘要质量。推理型LLMs往往更啰嗦、事实不一致且摘要不够简明，反而可能不如非推理型模型。具体情境分析揭示了推理有时会对复杂对话场景的摘要起反作用。

Conclusion: 当前推理型LLMs在对话摘要方面存在明显局限，需针对实际应用设计更有针对性的建模和评价方法。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [24] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 尝试用递归Transformer内化链式推理，但对可解释的“隐式链式思维”支持有限，递归深度提升仅带来微弱增益，方法不及显式语言外化推理。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）推理提升了基于Transformer的语言模型在复杂数学和多步规划任务中的表现，但通常以语言形式外化推理，导致效率折中。一些研究尝试用递归结构将推理过程内化到隐空间，探索更高效的“隐式链式思维”。本文重点讨论这种结构在Huginn-3.5B模型中是否自然出现。

Method: 作者采用了Huginn-3.5B深度递归Transformer模型，该模型在推理时复用层而不增加参数量，并通过一系列探测（probing）技术（如Logit Lens和Coda Lens）分析模型在算术任务中的内部行为。他们跟踪最终及中间结果token排名变化，并比较不同递归块、层索引和解码方式下的可解释性。

Result: 结果显示模型的“隐式链式思维”能力有限：最终和中间结果token的排名轨迹只能显示出极为有限的可解释性。各递归块间探测结果表现出显著不一致，隐藏状态的可解释性依赖于具体层和解码方式。随着递归深度增加，模型性能提升有限，依然远逊于显式外化推理步骤的模型。

Conclusion: 作者认为，在当前的架构下，把推理内化到隐空间难以自然孕育出高效、可解释的链式思维。递归Transformer虽然参数高效，但在链式推理和中间步骤可解释性上存在明显短板。代码已开源。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [25] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 本论文提出了GDC Cohort Copilot工具，支持用户用自然语言在GDC癌症基因组数据库中构建复杂队列，本地开源模型效果优于GPT-4o，并已开源发布。


<details>
  <summary>Details</summary>
Motivation: GDC 提供高质量的癌症基因组数据和患者队列管理，但用户在众多字段中筛选和描述特定队列时，尤其是新手会感到困难。尽管界面支持可视化筛选，用户往往更习惯用自然语言表达需求。

Method: 提出并开发了GDC Cohort Copilot工具，用户可以用自然语言描述期望队列，由该工具自动生成对应的GDC队列筛选条件并导出回GDC进行后续分析。开发并评估了多个大语言模型（LLM），集成本地开源模型并与GPT-4o进行对比。

Result: GDC Cohort Copilot已开源，并提供docker镜像、代码及模型权重。实验证明其本地部署的LLM在生成GDC队列方面优于GPT-4o。工具带有交互界面支持结果进一步细化。

Conclusion: GDC Cohort Copilot能有效提升用户基于自然语言在GDC上筛选和定义队列的体验，本地模型表现优于商业大模型，是开源和可部署的解决方案。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [26] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 引入了一种新型分段记忆代理MemAgent，通过创新记忆更新与DAPO训练法，显著提升超长文本处理效能，外推性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有的扩展长度、提升注意力机制和引入记忆模块的方法难以在处理极长文本时同时保证线性复杂度和性能不下降。因此，高效处理超长文本序列是亟需解决的核心难题。

Method: 提出了一种新颖的agent工作流MemAgent，采用分段阅读并借助覆盖式（overwrite）记忆更新策略，结合DAPO算法的扩展来实现端到端的长文本任务优化。通过独立上下文的多轮生成支持训练过程。

Result: MemAgent在长文本处理上表现出卓越能力，能将训练于32K文本、8K上下文的模型外推至3.5M的QA任务，性能损失低于5%；在512K RULER测试中准确率达到95%以上。

Conclusion: MemAgent能够以线性复杂度高效处理超长文档，有效减少外推过程的性能下降，为长文本任务带来新的高效解决方案。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [27] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX创新性地用LoRA实现高效、灵活且低资源消耗的持续领域自适应预训练，为不同任务提供更适合的预训练模型，并能推广到更多微调场景。


<details>
  <summary>Details</summary>
Motivation: 现有的持续领域自适应预训练方法在算力消耗、对增量数据顺序敏感、以及优化目标不符等方面存在明显不足，阻碍了预训练模型的实用性与推广。

Method: 该方法使用了LoRA模块（一种高效参数微调技术）实现高效且并行的领域自适应预训练。通过此方法，实现了对领域顺序的鲁棒性与知识有效积累，可针对具体任务生成专用模型。

Result: DoMIX显著降低了计算资源消耗，提高了对领域顺序的适应能力，并能根据具体任务输出定制预训练模型。实验还表明该方法对普通LLM微调任务同样适用。

Conclusion: DoMIX解决了现有持续领域自适应预训练（continual DAP）中的高计算资源消耗、对数据顺序敏感、以及只生成单一通用模型的问题。新方法不仅能提供针对特定任务的定制预训练模型，还能扩展至普通大语言模型（LLM）微调场景。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [28] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 作者提出一种融合多模态大语言模型和few-shot检索方法的系统，用于科学视觉问答，在比赛中获得第三名，平均F1分数85.12。


<details>
  <summary>Details</summary>
Motivation: 科学视觉问答（SciVQA）任务需要系统能够理解和分析科学图像，并回答相关问题。现有方法在处理不同类型图像和题目时表现有限。本文旨在提升视觉问答的准确性和泛化能力。

Method: 本文提出了一套系统，融合了两种多模态大语言模型，并采用多种few-shot示例检索策略。在模型和few-shot设置上，根据图和问题类型进行选择，同时基于模型的置信度对答案进行筛选。

Result: 在盲测数据集上，该系统在七支队伍中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数达到85.12。代码已公开。

Conclusion: 基于多模态大语言模型和多类型few-shot检索策略的系统在科学视觉问答任务中具有较强的效果和竞争力，取得了第三名的成绩。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [29] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文通过将量子参数化电路（PQC）引入Transformer中FFN模块，提出QFFN-BERT，证明了在大幅减少参数情况下注重设计的量子层可实现与甚至优于传统神经网络的表现，尤其在小样本任务中更具优势，为高效Transformer模型提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer结构中，前馈网络（FFN）模块占据了大约2/3的参数量，影响模型的效率与参数冗余。近年来参数化量子电路（PQC）在提升神经网络表达能力方面表现出潜力，但此前多聚焦于自注意力模块。本工作希望探索PQCs在FFN中的应用，旨在实现参数高效且表现优异的模型。

Method: 提出QFFN-BERT，将紧凑版BERT中的FFN模块替换为基于PQC的层。系统性研究了PQC深度、可表达性与可训练性的权衡，最终设计中加入了残差连接、$R_Y$和$R_Z$旋转门、交替纠缠策略以保证训练稳定和高表达力。在SST-2和DBpedia基准上使用经典模拟器进行实验。

Result: 精心配置的QFFN-BERT在整体参数大幅减少（FFN参数量减少99%以上）情况下，准确率可达或略超基线（达到102.0%），并在小样本学习场景下展现出数据效率优势。消融实验表明，不优化的PQC无法有效学习，验证了合理设计PQC的重要性。

Conclusion: 在结合深度学习设计原则的前提下，PQCs可以作为经典FFN的强大且参数高效的替代方案，具备提升Transformer类模型性能和效率的潜力。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [30] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 通过参数模型筛选高质量数据，本文方法在数据量大幅减少的情况下实现更高代码生成与理解性能，兼顾效率与效果。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）在代码生成和程序理解方面取得了显著进展，但现有方法往往过度依赖大规模数据，忽视了数据质量问题，导致训练效率下降。

Method: 本文提出了一种基于参数模型的代码数据选择方法，通过优化参数模型，实现所选子集中数据分布的一致性和多样性，从而保证数据高质量。

Result: 实验结果表明，仅使用1万条样本，该方法在HumanEval基准上提升了2.4%，在MBPP基准上提升了2.3%，优于使用9.2万条全量数据的基线，以及其他采样方法，既提升了性能又大幅降低了计算成本。

Conclusion: 该方法能够有效提升大型语言模型在代码相关任务上的性能，同时显著降低训练资源消耗。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [31] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: Akan语自动语音识别模型在跨领域上表现欠佳，不同模型架构存在可读性和透明度的误差权衡，建议在低资源语种应用中结合领域适应和多语种训练等策略。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别（ASR）研究大多在单一领域数据集上评估模型，但很少关注其在不同语音环境中的泛化能力。针对Akan语等低资源语种，这一问题尤为突出。

Method: 作者在本文中基于Transformer架构（如Whisper和Wav2Vec2）训练了七个Akan语ASR模型，并利用涵盖多领域的四个Akan语语音语料库对模型进行基准测试。这些领域包括文化相关图像描述、非正式对话、圣经朗读和金融交流。作者通过对比词错误率（WER）和字符错误率（CER）来分析模型的领域依赖性和误差行为。

Result: 实验结果表明，ASR模型在训练领域内表现最佳，而在非匹配领域下准确率大幅下降。两种主流架构的错误表现各异：Whisper在不熟悉输入时生成更流畅但可能误导的转录，Wav2Vec2则输出不够流畅但更难以理解的结果。

Conclusion: ASR模型在低资源语种及多领域应用中存在明显的领域依赖和误差权衡，因此应重视面向领域的适应性方法、多语种训练和自适应路由策略的开发。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [32] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 该研究首次建立了Akan语言受损语音开源数据集，并开发社区友好的ASR数据收集与建模手册，推动低资源语种和受损人群ASR技术的发展。所用数据、算法和工具均已开放，有利于全球相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别（ASR）技术主要针对资源丰富的语言和普通话者，难以满足少数语言及讲话受损人群的需求。特别是低资源语言中，相关数据和技术的缺乏，进一步限制了包容性ASR技术的发展。

Method: 本研究开发了一套用于收集受损语音并建立ASR模型的流程和最佳实践“手册”，并对社区成员进行培训，推动数据的社区化收集与应用。作为验证，研究团队构建了首个加纳土著语言Akan的受损语音开源数据集，并开源了相应工具。最后，对现有开源ASR模型进行了针对Akan受损语音的微调。

Result: 研究收集并公开了Akan语言受损语音首个数据集，提供了配套训练和最佳实践手册，且社区成员参与数据收集。微调后的开源ASR模型在Akan受损语音上取得了改进效果。所有数据和工具均已开放共享。

Conclusion: 本研究为低资源语言和受损语音个体打造了数据支持和ASR技术方案，推动了包容性语音技术的发展，同时通过开源共享方式促进全球研究和实际应用。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [33] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 作者发布了首个以印度保释判决为核心的结构化数据集IndianBailJudgments-1200，并详细注释20余项法律相关属性，为相关法律NLP研究提供了重要支持。


<details>
  <summary>Details</summary>
Motivation: 由于结构化法律数据集稀缺，印度等地区的法律自然语言处理（NLP）尚不发达。为促进相关研究，需要高质量、结构丰富的数据集。

Method: 作者构建了IndianBailJudgments-1200数据集，包含1200份印度法院的保释判决文书，涵盖包括保释结果、IPC法条、犯罪类型和法律推理等20多个属性。数据注释通过精心设计的GPT-4o流程自动生成，并进行一致性校验。

Result: 首次公开提供专注于印度保释判例学的数据集。该数据集能够支持判决结果预测、文本摘要、公平性分析等多种法律NLP任务。

Conclusion: IndianBailJudgments-1200为印度及类似司法环境中的法律NLP研究提供了基础性资源，填补了领域内数据集的空白，可推动后续算法和应用的开发。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [34] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor通过创新的任务生成与强化学习算法，成功提升了开源智能体在高复杂度信息检索任务中的推理和决策能力，使其达到了专有系统的表现水平。


<details>
  <summary>Details</summary>
Motivation: 目前专有智能体系统（如DeepResearch）在复杂信息检索任务上实现了超越人类的表现，但开源模型未能达到这种能力。作者认为关键在于这些专有系统有更强的信息不确定性消解机制，而现有开源模型缺乏。为缩小两者差距，作者提出新方法。

Method: 作者提出了WebSailor方法，通过生成高不确定性任务（包括结构化采样和信息遮蔽）、RFT冷启动以及高效的智能体强化学习训练算法——Duplicating Sampling Policy Optimization (DUPO)，构建了一个完整的后训练流程。

Result: WebSailor在复杂信息检索任务中表现出色，显著优于所有开源智能体，并达到专有智能体的性能水平，有效缩小了两者之间的能力差距。

Conclusion: WebSailor是一套能赋予开源模型高阶不确定性消解能力的后训练方法，为开源智能体在复杂任务中的超人类表现提供了新路径。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [35] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文指出主动学习与标注流程普遍忽视了人类标签变异（HLV）这一重要信号，并提出了一套将HLV纳入主动学习各阶段的理论框架，为更好地应对真实世界中的标签复杂性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 在有监督学习中，高质量标注数据稀缺且获取困难。实际标注过程中，尤其是自然语言处理领域，常出现同一实例有不同标签（标签变异），但现有标注和主动学习框架普遍假设只有单一定论（唯一真实标签），忽视了“人类标签变异”（HLV）作为信息信号的价值。

Method: （1）分析当前主动学习和标签变异研究社区对标签真值和本质的基本假设；（2）强调将观测到的标签变异分解为有用信号（如HLV）与噪声（如标注错误）的必要性；（3）提出融合HLV概念的主动学习全流程框架，包括样本选择、标注者挑选和标签表示方法；（4）讨论将大语言模型（LLM）作为标注者时的相关问题。

Result: 提出了一套具备人类标签变异意识的主动学习理论框架，并梳理了当前学界在相关区分上的处理与不足。进一步扩展了主动学习流程，同时也探讨了结合大模型标注的可能性。

Conclusion: 文章为主动学习领域如何正确认识与利用人类标签变异（HLV）打下了理论基础，有助于提升现实场景下数据标注和模型训练的可靠性与表现。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [36] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 提出了一个新方法MPF，无需大量调参或微调，能用多视角分布机制有效缓解LLM偏见，使输出更符合人类基线且具备可解释性，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在输出中存在偏见问题，且缺乏易用且可解释的消除/缓解偏见的方法。因此，亟需一种无需复杂微调或再训练、能够有效缓解模型偏见的对齐方法。

Method: 提出了一种新的多视角融合（MPF）对齐框架，基于SAGED自动化基线生成流程。该方法通过将人类基线数据（如HR专业人士的情感分布）分解为多个可解释的视角成分，并据此加权采样、平衡模型生成的回应，使模型输出在多视角下与人类基线趋于一致。

Result: 实验证明，MPF方法能够将LLM的情感分布对齐至‘绝对平等’的反事实基线或偏向精英大学的HR真实基线，显著降低了KL散度和校准误差，并可泛化至未见问题。

Conclusion: MPF为部署中的LLMs提供了一种可扩展、可解释、无需复杂微调的偏见缓解、对齐方法，可直接应用于现有模型，具备良好应用前景。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [37] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 作者创造了GenderLexicon数据集及新框架，能量化并解释NLP中的性别及情境偏见，证实偏见不限于职业词汇，并在多数据集上验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 近年来，性别偏见在自然语言处理（NLP）领域日益受到关注。作者主要关注语言中的性别和情境偏见，尤其针对动词、名词及职业相关词汇。现有研究通常仅聚焦于职业偏见，缺乏解释性和更丰富场景的探究，因此有必要进一步细致分析此类偏见，以提升模型公平性和可解释性。

Method: 作者提出了一种全新数据集GenderLexicon，以及一个能够估计情境偏见及相关性别偏见的分析框架。此方法不仅可量化偏见并用分数解释，还能适用于多种场景与语言数据集，包括日语。

Result: 通过在五个多样化数据集上的验证，包括日语数据集，实验证明了所提方法能够有效解释和量化性别及情境偏见。此外，研究发现性别偏见不只存在于职业领域，也扩展到其他语言元素。

Conclusion: 本文提出的数据集与分析框架能更好地揭示和解释NLP模型中的性别和情境偏见，加深了对NLP领域性别偏见的理解，并为后续偏见消除研究提供了工具和方法。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [38] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了用于评估LLM发现论文局限性的LimitGen基准，通过引入文献检索提升了模型发掘与生成科学论文局限性的能力，为同行评审提供更有建设性的支持。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版数量持续增长，同行评审压力加大。现有LLM对辅助评审尤其是在发现论文局限性方面的潜力尚未被系统研究。作者旨在填补此空白，提升评审效率与质量。

Method: 构建了包含合成和真实子集的LimitGen基准，并将LLMs与文献检索功能整合，用以评估和提升其在发现论文局限性方面的表现。

Result: 提出了局限性分类体系、开发了LimitGen基准，两大数据集（合成与人工），并通过引入文献检索显著增强了LLMs识别与生成论文局限性的能力。

Conclusion: 通过提出和验证新的LimitGen基准，本研究证明结合文献检索可以提升LLMs在研究论文中识别和生成局限性（limitations）的能力，从而为同行评审提供更有效的早期反馈。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [39] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究首次测量并量化了英语母语者在元音发音时可区分的最小听觉空间距离（JPD），数值范围为14到51 mel。这为理解元音系统的音位分布提供了心理声学基础，对语音产生理论和元音系统结构有重要启示。


<details>
  <summary>Details</summary>
Motivation: 人类元音发音涉及复杂协调的动作，根据以往研究，这一过程受控于听觉空间中的目标机制，但对这种控制精确度尚不清楚。本文旨在测量人类能区分的最小元音听觉空间距离，并探讨其对语音理论和元音系统结构的影响。

Method: 本文采用元音模仿实验范式，两组英语母语者被要求模仿不同的前元音，通过测量参与者可区分的最小声学距离（Just Producible Difference, JPD），首次得出了JPD的具体数值。JPD在F1×F2声学空间上被定量估算。

Result: JPD被估算为F1×F2空间中14到51 mel。这一结果表明，人类可区别元音的声学下限有了具体量化依据，对元音系统的合理结构和分布提供了心理物理学基础。

Conclusion: 首次量化了英语元音在F1×F2空间内的最小可模仿差异（JPD），为语音产生中的表征理论和元音分布模式提供了实证参考，也限定了元音在声学空间中的最小间隔。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [40] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大型语言模型对自身输出的错误往往没有自我纠正能力，表现出显著盲点。这与模型训练数据有关，但可通过简单激活方式显著缓解。该研究为提升LLMs可靠性指明了方向。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型（LLMs）虽然有很强的能力，但在自身产生的错误上的纠正能力不足，表现出“自我纠正盲区”。本研究的动机是系统性分析这种现象的成因和影响。

Method: 提出了Self-Correction Bench框架，能够通过不同复杂度层次的受控错误注入，系统性测试模型的自我纠正能力，对14个模型进行对比评估。还根据训练数据成分和不同训练方式（如人类演示和RL训练）做了对比探讨。

Result: 结果显示平均64.5%的模型对自身错误具有纠正盲区。RL训练的模型因为有错误纠正反馈，表现优于人类演示训练。通过简单地给模型附加提示（如“Wait”），盲区率能下降89.3%。

Conclusion: 现有LLMs存在系统性的自我纠正盲点，这与训练数据结构有关，但激活这种能力并非不可能，简单干预有助改善该能力。针对盲区的增强方法可提升模型的可靠性和可信度。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [41] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本研究发现，具备推理机制的语言模型在面对社会偏见诱导时更易被攻击和利用，尤其是通过CoT提示的模型，表明推理能力不必然提升安全性。需开发更偏见敏感的推理设计方案。


<details>
  <summary>Details</summary>
Motivation: 近年来，具备推理能力的语言模型（Reasoning Language Models, RLMs）发展迅速，尤其是在多步推理任务和链式思维提示（CoT）等机制方面取得了显著进展。然而，这些推理能力对模型应对社会偏见的鲁棒性影响尚不清楚，因此作者希望系统性地评估RLMs在面对带有偏见的内容时的安全性和鲁棒性。

Method: 作者利用CLEAR-Bias基准测试，系统地评估了最先进的RLMs在不同社会文化维度下对偏见诱导的对抗鲁棒性。他们采用LLM-as-a-judge自动评分方法来衡量模型的安全性，并通过jailbreak技术检验模型安全机制的强度。主要从以下三方面展开：1）推理能力对模型公平性和鲁棒性的影响；2）微调推理模型与推理提示（CoT）模型在安全性上的差异；3）不同推理机制下，jailbreak攻击成功率的变化。

Result: 评测结果显示，具备显式推理能力的模型（包括CoT和微调推理模型）在偏见诱导方面普遍比无推理机制的基础模型更脆弱，推理反而可能为偏见加强开辟新途径。同时，推理微调模型的安全性略优于CoT提示模型，后者特别容易受到通过故事化提示、虚构人格和奖励型指令等上下文重构攻击。

Conclusion: 推理能力并不会天然提升模型的偏见鲁棒性，反而可能引入新的攻击通道。该结论挑战了业内普遍认为推理能力带来更高安全性的假设，凸显了未来在推理设计中需要更强的偏见防御意识。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [42] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 该论文提出MathV-DP多解集与推理方法，显著提升了多模态大模型的数学解答准确度和多样性，强调了多样性反思在推理模型中的价值。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在数学推理任务时，通常只用一对一的图文对和单一解答进行训练，忽略了多样化的有效推理路径和反思能力，限制了模型推理的多样性和深度。

Method: 提出了MathV-DP数据集，对每个图像-问题对提供多条不同的解答路径，支持更丰富的推理监督。基于Qwen-VL模型，提出Qwen-VL-DP，通过监督学习微调，并结合群组相对策略优化（GRPO）这一规则驱动的强化学习方法，引入了正确性判别及多样性奖励函数，引导模型学习区分不同的正确推理路径。

Result: 在MathVista's minitest和Math-V基准测试上，Qwen-VL-DP模型在准确率和生成多样性方面均大幅优于现有主流基础多模态大语言模型。

Conclusion: 本工作证明了融合多样化推理路径与反思过程，对于提升多模态数学推理模型的准确性与多样性具有重要作用。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [43] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 作者提出通过智能动态路由将问题分配到不同推理模式，可提升准确率同时大幅降低成本，较适合实际需求，并引入AIT综合评价指标。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在实际应用中既要兼顾性能又要考虑运营成本，尤其是在推理成本差异巨大时，亟需一种能够根据问题复杂度动态分配推理资源的高效机制。

Method: 提出并实现了基于机器学习的动态路由框架SynapseRoute，根据问题复杂度智能分配到'推理'或'非推理'模式。在医疗数据集上进行了实验，并与单一高推理模式进行了对比分析。

Result: 动态路由机制在实验中提升了整体准确率（0.8390 vs 0.8272），同时推理时间减少36.8%，Token消耗降低39.66%。提出AIT指标评价综合效益，并验证了简单问题过度推理的负面影响。

Conclusion: 研究表明，通过动态路由将问题分配给不同的推理模式，可以在保持甚至提升准确率的同时，显著降低运行成本和响应延迟。过度推理对简单问题反而有害。提出的AIT指标有助于综合评价准确性、延迟和花费之间的权衡。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [44] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出用于评测和提升语言模型精准指令遵循泛化能力的新基准和训练方法，显著提升模型在新颖约束下的表现，并公开相关数据与工具。


<details>
  <summary>Details</summary>
Motivation: 当前强大的语言模型在遵循具体输出约束的指令（例如“只回答是或否”、“至少提到3次特定词汇”等）时表现不佳，无法很好地泛化到新颖的约束类型。作者旨在研究和提升模型在精准履行这类指令上的能力。

Method: 作者提出了新的基准数据集IFBench，用于评测模型对58种多样且具有挑战性的全新验证型输出约束的泛化能力。此外，设计了约束验证模块，并通过可验证奖励的强化学习（RLVR）方法训练模型，以提升其精准指令遵循的泛化能力。同时释放了手工标注的新训练约束与验证函数、训练用提示词和代码。

Result: 通过RLVR技巧，模型在新的IFBench基准上展示出显著提升的约束遵循和泛化能力。进一步分析也揭示了影响泛化能力的训练数据和策略。

Conclusion: 当前主流语言模型在精准指令遵循方面易于过拟合，泛化能力较弱。通过新的评测基准、分析及RLVR训练，能够有效提升模型在未知指令约束下的泛化能力。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [45] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 即便权限有限，用户也能通过反馈实现模型投毒，包括改变知识、植入安全漏洞及虚假消息，说明LM基于偏好调优存在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 尽管通过用户反馈优化语言模型（LMs）的性能是提升模型实用性的重要方法，但现有研究很少关注恶意用户如何利用反馈系统对模型产生有害或持久的影响。本文的动机在于揭示并系统评估基于用户反馈训练的语言模型存在的新型安全威胁。

Method: 攻击者通过设计输入提示，让模型有概率输出“投毒”（poisoned）或正常响应，并通过上/下投票来偏向有害输出。在后续偏好微调阶段（preference tuning）利用这些反馈信号，使模型在未来即使在无恶意提示下也更倾向于输出有害内容。

Result: 攻击手法被证实可以：（1）让模型学到原本不具备的虚假知识；（2）改变代码生成模式，植入安全漏洞；（3）注入虚假的财经新闻。甚至受到高度限制的反馈数据也能精细操控模型行为。

Conclusion: 文章发现用户反馈微调LMs存在结构性安全风险，即单一用户可持续“投毒”模型知识与行为。此结果揭示了偏好微调机制的新弱点，丰富了现有有关预训练和部署阶段攻击的研究。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [46] [On Obtaining New MUBs by Finding Points on Complete Intersection Varieties over $\mathbb{R}$](https://arxiv.org/abs/2507.02492)
*Arindam Banerjee,Kanoy Kumar Das,Ajeet Kumar,Rakesh Kumar,Subhamoy Maitra*

Main category: cs.DM

TL;DR: 本文以代数几何方法分析MUBs的可扩展性，给出等价判据，并将MUBs结构与正交正规矩阵的极大可交换基联系起来，从而为完整MUBs集的存在与构造提供新途径。


<details>
  <summary>Details</summary>
Motivation: 互不相干基（MUBs）在量子物理学和数学结构中具有重要作用，但MUBs的结构、可扩展性及相关的代数描述尚未完全解决。本论文希望通过代数几何方法探究MUBs的扩展条件。

Method: 研究与MUBs有关的阿芬代数簇的实点，通过这些代数几何对象中的关系及其相关的完全交集结构，分析MUBs的可扩展性。同时，建立MUBs与一类正交正规矩阵的极大可交换基之间的对应关系。

Result: 论文提出了MUBs在$C^n$空间可扩展性的等价判据，指出部分相关代数簇是完全交集域。此外，证明了MUBs与$\\mathcal M_n({\\mathbb{C}})$中正交正规矩阵极大可交换基之间存在一一对应关系。

Conclusion: MUBs的代数可扩展性可通过代数几何视角刻画，并与矩阵理论中极大可交换基问题相关联。这种理论刻画有助于判定MUBs完整集的存在性，为量子信息及相关领域研究提供了工具。

Abstract: Mutually Unbiased Bases (MUBs) are closely connected with quantum physics,
and the structure has a rich mathematical background. We provide equivalent
criteria for extending a set of MUBs for $C^n$ by studying real points of a
certain affine algebraic variety. This variety comes from the relations that
determine the extendability of a system of MUBs. Finally, we show that some
part of this variety gives rise to complete intersection domains. Further, we
show that there is a one-to-one correspondence between MUBs and the maximal
commuting classes (bases) of orthogonal normal matrices in $\mathcal
M_n({\mathbb{C}})$. It means that for $m$ MUBs in $C^n$, there are $m$
commuting classes, each consisting of $n$ commuting orthogonal normal matrices
and the existence of maximal commuting basis for $\mathcal M_n({\mathbb{C}})$
ensures the complete set of MUBs in $\mathcal M_n({\mathbb{C}})$.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [47] [Engineering an LTLf Synthesis Tool](https://arxiv.org/abs/2507.02491)
*Alexandre Duret-Lutz,Shufang Zhu,Nir Piterman,Giuseppe de Giacomo,Moshe Y Vardi*

Main category: cs.FL

TL;DR: 作者提出了一种新的LTLf综合方法，通过MTBDD高效表示和在线求解，将LTLf直接转换为DFA，并开发出的工具在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升LTLf反应式综合的效率和性能，克服现有工具的局限性。

Method: 采用了从LTLf到DFA的全新直接转换方法，DFA用共享节点的MTBDD数组表示，并直接将该表示解释为可达性博弈，边构建边求解。

Result: 工具在作者的基准测试套件上取得了比已有工具更优的效果。

Conclusion: 实现了一种新的LTLf综合器，能够显著优于现有工具。

Abstract: The problem of LTLf reactive synthesis is to build a transducer, whose output
is based on a history of inputs, such that, for every infinite sequence of
inputs, the conjoint evolution of the inputs and outputs has a prefix that
satisfies a given LTLf specification. We describe the implementation of an LTLf
synthesizer that outperforms existing tools on our benchmark suite. This is
based on a new, direct translation from LTLf to a DFA represented as an array
of Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based
representation can be interpreted directly as a reachability game that is
solved on-the-fly during its construction.

</details>
