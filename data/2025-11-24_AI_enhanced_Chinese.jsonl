{"id": "2511.16707", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16707", "abs": "https://arxiv.org/abs/2511.16707", "authors": ["Yuki Kataoka", "Ryuhei So", "Masahiro Banno", "Yasushi Tsujimoto", "Tomohiro Takayama", "Yosuke Yamagishi", "Takahiro Tsuge", "Norio Yamamoto", "Chiaki Suda", "Toshi A. Furukawa"], "title": "Large language models for automated PRISMA 2020 adherence checking", "comment": null, "summary": "Evaluating adherence to PRISMA 2020 guideline remains a burden in the peer review process. To address the lack of shareable benchmarks, we constructed a copyright-aware benchmark of 108 Creative Commons-licensed systematic reviews and evaluated ten large language models (LLMs) across five input formats. In a development cohort, supplying structured PRISMA 2020 checklists (Markdown, JSON, XML, or plain text) yielded 78.7-79.7% accuracy versus 45.21% for manuscript-only input (p less than 0.0001), with no differences between structured formats (p>0.9). Across models, accuracy ranged from 70.6-82.8% with distinct sensitivity-specificity trade-offs, replicated in an independent validation cohort. We then selected Qwen3-Max (a high-sensitivity open-weight model) and extended evaluation to the full dataset (n=120), achieving 95.1% sensitivity and 49.3% specificity. Structured checklist provision substantially improves LLM-based PRISMA assessment, though human expert verification remains essential before editorial decisions.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ee5\u7248\u6743\u5408\u89c4\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u4e3a\u57fa\u7840\uff0c\u5bf9\u591a\u79cdLLM\u6a21\u578bPRISMA 2020\u6307\u5357\u9075\u5faa\u6027\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u6e05\u5355\u8f93\u5165\u80fd\u5927\u5e45\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u81ea\u52a8\u5316\u5ba1\u6838\u6709\u52a9\u4e8e\u52a0\u901f\u8bc4\u5ba1\uff0c\u4f46\u6700\u7ec8\u4ecd\u9700\u4eba\u5de5\u6838\u67e5\u3002", "motivation": "\u5728\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\u4e2d\uff0c\u8bc4\u4f30\u7cfb\u7edf\u7efc\u8ff0\u62a5\u544a\u662f\u5426\u9075\u5faaPRISMA 2020\u6307\u5357\u662f\u4e00\u9879\u7e41\u91cd\u7684\u4efb\u52a1\u3002\u6b64\u524d\u7f3a\u4e4f\u53ef\u5171\u4eab\u7684\u57fa\u51c6\u6570\u636e\u4ee5\u52a9\u529b\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7248\u6743\u5408\u89c4\u7684\u3001\u5305\u542b108\u7bc7\u201c\u521b\u4f5c\u5171\u7528\u201d\u8bb8\u53ef\u7684\u7cfb\u7edf\u7efc\u8ff0\u6587\u732e\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5728\u4e94\u79cd\u8f93\u5165\u683c\u5f0f\u4e0b\uff0c\u8bc4\u4f30\u4e86\u5341\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u5728\u5f00\u53d1\u7fa4\u4f53\u4e2d\uff0c\u5206\u522b\u6d4b\u8bd5\u4e86\u7ed3\u6784\u5316PRISMA 2020\u6e05\u5355\uff08\u5982Markdown\u3001JSON\u3001XML\u3001\u7eaf\u6587\u672c\uff09\u548c\u4ec5\u6709\u624b\u7a3f\u8f93\u5165\u7684\u8868\u73b0\u3002\u968f\u540e\uff0c\u6311\u9009\u51fa\u9ad8\u654f\u611f\u6027\u7684Qwen3-Max\u8fdb\u884c\u5168\u91cf\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002", "result": "\u7ed3\u6784\u5316\u6e05\u5355\uff08\u6bd4\u624b\u7a3f\u8f93\u5165\uff09\u6781\u5927\u63d0\u9ad8\u4e86LLM\u5bf9PRISMA\u51c6\u5219\u7684\u8bc4\u4f30\u51c6\u786e\u7387\uff08\u7ea679%\uff0cvs\u4ec5\u624b\u7a3f45%\uff09\uff0c\u4e0d\u540c\u7ed3\u6784\u5316\u683c\u5f0f\u95f4\u65e0\u663e\u8457\u5dee\u5f02\u3002\u5404\u6a21\u578b\u51c6\u786e\u7387\u572870.6%-82.8%\u4e4b\u95f4\uff0c\u5b58\u5728\u654f\u611f\u6027-\u7279\u5f02\u6027\u6743\u8861\u3002\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7ed3\u679c\u5f97\u4ee5\u590d\u73b0\u3002Qwen3-Max\u6a21\u578b\u5728\u5168\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8695.1%\u654f\u611f\u6027\u548c49.3%\u7279\u5f02\u6027\u3002", "conclusion": "\u5411LLM\u63d0\u4f9b\u7ed3\u6784\u5316PRISMA\u6e05\u5355\u80fd\u5927\u5e45\u63d0\u5347\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u7efc\u8ff0\u8d28\u91cf\u6307\u5357\u7684\u80fd\u529b\uff0c\u4f46\u5728\u7f16\u8f91\u51b3\u7b56\u524d\u4f9d\u7136\u9700\u4eba\u5de5\u4e13\u5bb6\u7684\u6838\u67e5\u3002"}}
{"id": "2511.16708", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.16708", "abs": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "title": "Multi-Agent Code Verification with Compound Vulnerability Detection", "comment": "18 pages, 3 figures, 9 tables", "summary": "LLMs generate buggy code: 29.6% of SWE-bench \"solved\" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.", "AI": {"tldr": "LLMs\u751f\u6210\u7684\u4ee3\u7801\u6f0f\u6d1e\u591a\uff0c\u73b0\u6709\u5de5\u5177\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u7387\u6709\u9650\u3002CodeX-Verify\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u5e45\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u901f\u5ea6\uff0c\u65e0\u9700\u6267\u884c\u6d4b\u8bd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u53ef\u884c\u6027\uff0c\u591a\u6f0f\u6d1e\u53e0\u52a0\u98ce\u9669\u8d85\u9884\u671f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u5927\u91cf\u6f0f\u6d1e\u548c\u7f3a\u9677\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u5b58\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u4e0d\u9ad8\u548c\u8bef\u62a5\u7387\u8f83\u9ad8\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u6709\u6548\u4e0e\u9ad8\u6548\u7684\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86CodeX-Verify\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7531\u56db\u4e2a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u7684\u4ee3\u7801\u6f0f\u6d1e\u3002\u4f5c\u8005\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u9a8c\uff0c\u8bba\u8bc1\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u6bd4\u5355\u4e00\u667a\u80fd\u4f53\u66f4\u80fd\u53d1\u73b0\u591a\u6837\u7684\u6f0f\u6d1e\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u7ec4\u5408\u4ee5\u4f18\u5316\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "result": "CodeX-Verify\u7cfb\u7edf\u80fd\u68c0\u6d4b76.1%\u7684\u6f0f\u6d1e\uff0c\u4e0e\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u6301\u5e73\uff0c\u4f46\u65e0\u9700\u6d4b\u8bd5\u6267\u884c\u4e14\u6548\u7387\u66f4\u9ad8\u3002\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u6700\u4f73\u53cc\u667a\u80fd\u4f53\u7ec4\u5408\u8fbe\u523079.3%\uff0c\u7cfb\u7edf\u5728\u5b9e\u9645\u73af\u5883\u4e0b\u80fd\u5728200ms\u5185\u5b8c\u6210\u4e00\u6b21\u68c0\u6d4b\uff0c\u9002\u5408\u751f\u4ea7\u5e94\u7528\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u53d1\u73b0\u4ee3\u7801\u540c\u65f6\u5b58\u5728\u591a\u79cd\u6f0f\u6d1e\u65f6\uff0c\u98ce\u9669\u5448\u6307\u6570\u7ea7\u4e0a\u5347\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7ed3\u5408\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u7684\u6f0f\u6d1e\u80fd\u5927\u5e45\u63d0\u5347\u4ee3\u7801\u5b89\u5168\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\uff0c\u4e14\u5728\u751f\u4ea7\u73af\u5883\u4e0b\u5177\u5907\u5b9e\u7528\u6027\u3002\u591a\u79cd\u6f0f\u6d1e\u540c\u65f6\u5b58\u5728\u4f1a\u6781\u5927\u63d0\u9ad8\u7cfb\u7edf\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2511.16858", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16858", "abs": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "comment": null, "summary": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u751f\u6210\u7684\u4ee3\u7801\u4ecd\u5b58\u5728\u660e\u663e\u6d4b\u8bd5\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5bf9\u63d0\u5347\u5b9e\u9645\u4fee\u590d\u8d28\u91cf\u63d0\u51fa\u4e86\u6311\u6218\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u5bb9\u6613\u51fa\u73b0\u4fee\u590d\u540e\u7684\u4ee3\u7801\u5728\u5df2\u77e5\u6d4b\u8bd5\u96c6\u901a\u8fc7\uff0c\u4f46\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u5931\u8d25\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u8fc7\u62df\u5408\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u6709\u5fc5\u8981\u91cd\u65b0\u8bc4\u4f30\u8fd9\u4e00\u95ee\u9898\u662f\u5426\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u4f5c\u8005\u91c7\u7528SWE-bench\u4ed3\u5e93\u7ea7\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6027\u6d4b\u8bd5\u5e76\u6bd4\u8f83\u81ea\u52a8\u5316\u4fee\u590d\u4ee3\u7801\u5728\u5df2\u77e5\u6d4b\u8bd5\u548c\u9690\u85cf\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5f53\u524d\u6280\u672f\u73af\u5883\u4e0b\uff0c\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u4f9d\u7136\u5b58\u5728\u6d4b\u8bd5\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4fee\u590d\u65b9\u6848\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u7684\u901a\u8fc7\u7387\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u6d4b\u8bd5\u8fc7\u62df\u5408\u4f9d\u65e7\u662f\u73b0\u4ee3\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u9886\u57df\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u7684\u95ee\u9898\u3002\u73b0\u6709\u5de5\u5177\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
