<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 通过(G)KAT等抽象代数工具，可以有效判定一些类型的程序结构等价性，为实际程序分析提供了可行的新途径。


<details>
  <summary>Details</summary>
Motivation: 程序等价性在普遍情况下是不可判定的，研究人员希望引入抽象化方法，将其转化为可判定、可实际操作的问题。

Method: 采用（受保护）带测试的Kleene代数（(Guarded) Kleene Algebra with Tests, (G)KAT）来形式化和分析命题程序等价性。通过代数方法抽象程序的语义，忽略底层语句实现细节，仅关注程序结构。

Result: 基于(G)KAT的方法使得原本不可判定的程序等价问题，在抽象掉具体语义后变得可判定且具备实际可行性。比如，可以判定if-then-else结构在条件互换下的等价性。

Conclusion: 只要适当地抽象掉语句的具体语义，许多程序结构的等价性不仅可以判定，也可以高效执行。文中以(G)KAT为理论基础，总结了命题程序等价领域的最新进展。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 本论文构建了第一个适用于软件工程领域的德语开发者情感数据集，为德语情感分析研究提供了坚实的数据基础，并指出现有工具缺乏领域适配，促进后续相关工具的开发。


<details>
  <summary>Details</summary>
Motivation: 现有的软件工程领域情感分析工具主要依赖英文或非德语的标准数据集，德语开发者相关领域缺乏适用的数据资源。

Method: 作者从德语开发者论坛 Android-Hilfe.de 抽取5949条独特开发者语句，由四位德语计算机专业学生依据Shaver等人的情感模型进行六种基本情感标注，并评估了标注过程中的一致性和可靠性。

Result: 数据标注一致性和可靠性均高，说明该德语数据集有效且稳健。现有德语情感分析工具在软件工程领域缺乏针对性。

Conclusion: 该德语情感数据集可为德语软件工程社区的情感分析提供坚实基础，填补了领域空白，并有助于推动未来德语开发者情感分析工具的开发和优化。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [3] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 提出了一种能自动从用户评论中挖掘可解释性需求并生成对应解释的方法，经验证AI生成解释风格好但准确性需人工检查，并发布了用于研究的数据集。


<details>
  <summary>Details</summary>
Motivation: 可解释性在提升透明度、建立用户信任、遵守监管合规方面变得日益重要，但如何将用户反馈中关于解释的需求转化为结构化需求及相应解释仍然具有挑战性。目前虽有方法能识别用户评论中的解释相关关切，但尚无系统化产生需求与解释的成熟方法。

Method: 提出一种工具支持的自动化方法，能够从用户评论中自动化地提取可解释性需求并生成对应解释。帮助评估有效性，作者与工业自动化制造商合作，构建了一个含有人工标注的58条用户评论数据集。

Result: 评估发现，AI生成的需求与人工生成的相比，经常缺乏相关性与准确性。然而，AI生成的解释在清晰度与表达风格上更受青睐，但其正确性仍然存在问题，因此需要人工验证。

Conclusion: 本研究提出了一种自动化方法从用户评论中提取可解释性需求并生成对应解释，并通过工业实例验证了该方法的效能，揭示了自动生成成果的优势与不足。此外，公开了一个高质量数据集以促进后续相关研究。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [4] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 该论文通过AAS和BPMN结合，实现了工程自动化工作流的新架构，增强了安全性、协作性和可扩展性，并用原型系统验证了其实用价值。


<details>
  <summary>Details</summary>
Motivation: 工业4.0技术的融合对于实现工厂与流程工程自动化和优化至关重要，而AAS作为核心数字孪生技术，亟需解决数据交换、自动化和跨组织协作中的安全性和扩展性问题。

Method: 论文探讨了AAS在工程工作流中的应用，特别是与BPMN结合以定义结构化自动化流程，并提出了分布式写时复制（copy-on-write）AAS架构。同时，开发了用于自动化AAS操作和工程工作流的工作流管理原型。

Result: 提出的方法提升了工程数据交换的互操作性，实现了安全、高效、可扩展的工程流程自动化，并通过原型系统验证了这套方案在提升效率和可追溯性方面的有效性。

Conclusion: 该论文提出了一种分布式AAS写时复制基础架构，结合BPMN方法，为工程流程的自动化和数据交换提供了更高的安全性、可扩展性及协同能力。此外，论文开发了一个工作流管理原型，有效提高了工程工作流的效率和可追溯性。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [5] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLM不能直接用需求文档生成高质量代码，还需将需求进一步分解和丰富，需求工程依然必要。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）生成代码能力的不断提升，有人设想软件工程可能会被彻底改变，甚至终结传统开发。但尚不清楚开发者如何在基于需求使用LLM进行代码生成中，整合需求信息。

Method: 通过对来自14家公司的18位实践者进行访谈，研究他们如何(重新)利用需求和设计文档中的信息，在生成代码时为LLM输入内容。基于这些访谈结果，提出了一套关于开发者操作流程及依赖工件的理论。

Result: 开发者通常不能直接将需求文档作为LLM输入，因为其过于抽象。必须先将需求分解为编程任务，再融合设计决策与架构约束后，作为提示输入模型。

Conclusion: 即使借助LLM进行代码生成，基础的需求工程工作仍是不可或缺的。提出的理论对于自动化与需求相关的软件工程任务的科学研究具有重要意义。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [6] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 本论文综述了大型语言模型在需求工程中提示工程的研究进展，归纳了技术与任务分类、现状及发展路线，为未来研究和实际应用提供系统指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的发展对需求工程（RE）任务的提升有极大潜力，但LLM存在不确定性和可控性不足，缺乏有效的提示工程方法，阻碍了其在需求工程领域的可信应用。作者旨在梳理并指导LLM在需求工程中的提示工程研究现状与发展。

Method: 作者采用了Kitchenham和Petersen提出的系统性文献综述（二次研究）方法，对六大数字图书馆的867条记录进行筛选，最终选取35篇主要研究进行分析，构建了技术与任务双维度的分类法。

Result: 提出了一套将技术型提示方式（如few-shot、Chain-of-Thought）与需求工程任务（如需求获取、验证、可追踪性）关联的混合分类体系，系统梳理了所涉任务、使用的LLM类型和提示类型，总结了现有限制与研究空白，并提出了向可复现、易用流程演进的路线图。

Conclusion: 本研究为将来LLM在需求工程领域内的可信应用与流程规范提供了系统参考，对研究与实践均具有指导意义。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [7] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 文章提出利用RAG和LLM的AI方法，提升航天任务需求工程自动化和效率，验证具备减少人工、提升覆盖和帮助合规的潜力。


<details>
  <summary>Details</summary>
Motivation: 航天领域的需求工程复杂，需要高精度与合规，同时还需适应任务特定约束。但小型航天组织及新进入者面对大量非结构化的任务文档时，难以提炼可执行的需求。本文旨在解决该问题。

Method: 提出一种模块化的、AI驱动的方法，流程包括：预处理原始任务文档，将文档按语义分类，从相关领域标准中检索语境内容，并利用大型语言模型（LLMs）合成需求草稿。实际应用于一个航天领域真实任务文档，并与行业伙伴协作评估初步效果。

Result: 初步结果显示，该方法能减少人工工作量、提升相关需求覆盖度、支持合规和要求的简化处理。

Conclusion: AI驱动的方法能辅助或半自动化航天需求生成，降低小型组织参与大型关键任务的门槛，有助于更广泛地将AI集成进需求工程流程。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 该文提出创设性的对比实验方法，发现LLM中的认知偏见主要由预训练决定而非微调过程，提示偏见研究需关注预训练机制。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）表现出类似人类的认知偏见这一现象，探究模型偏见的来源。目前尚不清楚这些偏见是源自预训练、微调还是训练过程中的随机噪声。

Method: 提出两步因果实验方法：（1）通过使用不同随机种子多次微调模型，考察训练随机性对30多种认知偏见的影响；（2）提出cross-tuning（交叉微调），即交换不同模型的instruction datasets，直接检验偏见是否由数据集决定。

Result: 训练随机性对偏见的影响有限，主要偏见来源于预训练：具有相同预训练backbone的模型比仅共享微调数据的模型展现出更相似的偏见模式。

Conclusion: 分析和理解微调模型中的偏见必须追溯到预训练阶段，仅关注微调是不够的。此发现为未来评估和缓解大模型偏见提供了新的视角和策略参考。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [9] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: LLMs在作为人类调查代理时会表现出诸如“最近效应”等已知偏差，并对问题和答案表述的扰动敏感，因此在用于社会科学调查时，提示设计与稳健性测试极为关键。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）被用作社会科学调查中替代人类受访者的代理，但目前对它们的可靠性和对已知回答偏差的敏感性了解有限。该论文旨在探究LLMs在规范性调查语境下的回应稳健性和偏差表现。

Method: 作者选取了九种不同的LLMs，基于世界价值观调查（WVS）的问题，对问题表述和答案选项结构进行了11种扰动，总计模拟了超过167,000次访谈。通过这些扰动，系统地测试模型对话题表述和答案顺序等变化的敏感性。

Result: 所有测试的LLMs在不同程度上都表现出一致的“最近效应”偏差，倾向于选择最后一个呈现的答案。虽然较大的模型通常更稳健，但所有模型对语义变化（如换句话说）和多重扰动仍然敏感。模型部分呈现出与人类受访者类似的回答偏差。

Conclusion: 在用LLMs生成合成调查数据时，必须高度重视提示设计和稳健性测试，因为LLMs不仅容易受到扰动，并且某些偏差与人类相同，这会影响调查数据的有效性和解释。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [10] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: 本文提出了综合性合成文本评价工具包SynthTextEval，支持多维度系统评价，并重点面向医疗和法律领域，推动了合成文本用于隐私保护的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的合成文本已经具备较高流畅性，使其在隐私敏感领域（如医疗、法律）中有潜力减少隐私泄露风险。然而，评估这些合成数据的有效性、可靠性和公正性等仍缺乏系统化的工具。

Method: 提出了SynthTextEval工具包，能够对合成文本从多维度进行系统评价，包括对下游系统的实用性、公平性、隐私泄露风险、与原始文本的分布差异以及领域专家的定性反馈等。用户可以上传或用工具自带模块生成合成数据进行评估。

Result: 该工具可以应用于各类数据，但重点展示了其在医疗和法律领域数据集上的功能和效果，能够统一和标准化评价指标。

Conclusion: SynthTextEval有助于推动合成文本在保护隐私方面的应用，提高AI开发过程中的隐私保障，通过标准化评估提升合成数据的实用性和可信度。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [11] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文提出了专门针对医疗大语言模型的多视角安全评估协议，建立了患者安全基准数据集，对实际医疗大模型进行案例分析，为医疗领域安全应用大模型奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗领域的应用日益广泛，其安全性引发了高度关注。现有的安全评估多集中于通用领域，缺乏针对医疗场景、不同用户（如病人和医生）视角的专属安全评估方法。为保障医疗大模型的实际应用安全，需要更细致、专业的评估工具和标准。

Method: 本文提出了一个面向医疗领域、涵盖病人和医生视角的安全评估协议，并进行了一般性安全测评。研究构建了PatientSafetyBench数据集，包含466个属于五大关键类别的样本，用于从患者视角系统性衡量模型的安全性。同时，作者应用定制的红队测试（red-teaming）协议，对MediPhi大模型系列进行了案例分析。

Result: 研究制定了医疗大语言模型的安全评估标准，并通过三种视角——病人、医生、普通用户——系统评价模型，初步建立了医疗应用中大模型安全部署的理论和实践基础。研究还发布了PatientSafetyBench基准数据集，为后续相关研究提供了基础资源。

Conclusion: 本文首次针对医疗大模型提出多视角安全评估协议，为医疗领域安全部署大模型奠定了基础，并提出了可推广的评测体系和数据集，为更可信赖的大模型医疗应用提供了系统性保障。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [12] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本文提出了一种能在多组重叠语音环境下检测中断的AI方法，为协作学习中小组互动的自动监测提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 在协作学习中，中断对小组互动和知识建构有重要影响。AI可以帮助教师监测互动过程，但现有关于中断检测的研究多为单一对话和清晰音频环境。在教室协作学习场景中，多组同时对话和语音重叠的情况普遍，传统方法难以适用。因此，亟需开发能适应多组重叠语音环境的中断检测方法。

Method: 分析了单一对话和多组对话场景下的中断检测问题，提出了一种对重叠语音具有鲁棒性的中断识别新方法，可应用于真实教室环境。同时，研究了协作互动中中断的语言及韵律特征。

Result: 所提出的方法能够有效在重叠语音环境中识别中断，具备实际部署于教室场景的能力，并揭示了中断在协作组互动中的语言和韵律表现。

Conclusion: 研究为多组同时互动的教室环境下的中断检测提供了新方法，对将来考虑多组语音重叠对小组对话跟踪的影响开辟了道路。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [13] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 本文提出了多智能体RAG方法，在健康虚假信息反言生成任务上，结合多证据提升生成质量，优于以往方法，有效提升人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有针对虚假信息的反言生成方法依赖证据有限，对生成结果的控制性不足，这限制了生成反驳语的相关性和质量。

Method: 提出了一个多智能体检索增强框架，结合多个大型语言模型用于知识检索、证据增强和回应优化，集成了静态与动态证据，以提升反言生成的相关性、根据性和时效性。

Result: 所提出方法在礼貌性、相关性、信息量和事实准确性上均优于基线方法。消融实验验证了各组件的必要性，人类评估显示优化显著提升了反言质量和人类偏好。

Conclusion: 该多智能体检索增强框架能有效提升针对健康领域虚假信息的高质量反言生成，兼具优秀的可控性与事实根据性。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [14] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 本文提出结合GNN和CNN的新模型，能高效处理长文本，实验证明在多个任务上效果接近甚至超越目前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型虽然性能优异，但在处理长文本时计算复杂度随输入长度二次增长，导致时间、成本和能效低下，急需新方法提高长文本处理效率。

Method: 提出一种结合图神经网络（GNN）和卷积神经网络（CNN）的新型模型结构，并集成实时端到端图生成机制。模型在字符级别高效处理输入，无需padding或截断，同时通过查询字典巧妙地引入大语言模型（LLM）的token嵌入和情感极性。模型利用CNN提取局部模式，通过格状图结构扩展感受野，应用小世界图聚合全局信息。

Result: 生成的图结构具有有意义的语义组织（平均聚类系数约0.45，平均最短路径长度4-5），模型在情感分析和新闻分类等多个文本分类任务中达到高效且具竞争力的性能。

Conclusion: 该模型在保持高运行效率的同时，能有效利用语义信息进行文本分类任务，在多个基准任务中表现优良，优于或媲美SOTA方法。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [15] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为MedReadCtrl的微调框架，能让AI医疗文本输出更符合不同人群的理解能力，效果优于GPT-4，尤其适合低健康素养患者，助力医疗AI的公平应用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域应用广泛，但在部署时面临人机有效沟通的挑战，尤其是输出内容既需个性化又要易于理解。特别是在服务不同健康素养水平的患者时，如何调整语言复杂度至关重要。

Method: 提出了MedReadCtrl框架，通过可控可读性指令微调（readability-controlled instruction tuning），让大语言模型（LLM）自动调整输出语句的复杂度，同时保持医学意义。方法在九个数据集和三个任务（涵盖医学和通用领域）上进行了评测。

Result: MedReadCtrl在可读性指令执行误差方面显著优于GPT-4（如在ReadMe数据集上，1.39 vs 1.59，p<0.001），且在未见过的临床任务上表现出较大提升（如MTSamples数据集上，ROUGE-L提升14.7，SARI提升6.18）。专家更偏好MedReadCtrl（71.7%对23.3%），尤以低健康素养场景下的优势明显。

Conclusion: MedReadCtrl能有效将复杂医学内容转换为可读性更强、各等级健康素养人群均可理解的语言，且不损失医学原意，为患者教育和公平医疗AI服务提供了具备可扩展性的解决方案。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [16] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 提出了一套结合大模型和人工的高效信息抽取管道，显著提升了驱逐状态等SDoH类别在医疗文本中的自动识别能力，同时降低标注成本，推动了领域大规模数据资源的建设和应用。


<details>
  <summary>Details</summary>
Motivation: 驱逐作为社会健康决定因素（SDoH）影响深远，但在结构化电子健康记录（EHR）中的描述有限，相关下游应用受限。为了缓解驱逐状态在医疗文本中的提取挑战，研究团队提出全新的方法。

Method: 提出SynthEHR-Eviction管道，结合大型语言模型（LLM）、人工参与标注和自动化提示优化（APO），用于从临床文本中高效提取驱逐相关状态。

Result: 利用该管道创建了迄今最大的公开驱逐相关SDoH数据集，包含14个细分类别。经过微调的LLM（如Qwen2.5、LLaMA3）在人工验证数据上分别取得了88.8%（驱逐）和90.3%（其他SDoH）的Macro-F1分数，优于GPT-4o-APO、GPT-4o-mini-APO和BioBERT。此外，该管道极大减轻标注工作量（减少80%以上），加速数据集建设，并可推广到其他信息抽取任务。

Conclusion: SynthEHR-Eviction是一套强大、高效且可扩展的驱逐状态抽取方案，不仅提升信息抽取的准确率和性价比，还大幅拓展了社会健康决定因素（SDoH）大数据研究和实际应用的能力。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [17] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 利用合成时间序列和大型模型生成的注释对小型语言模型进行微调，使其具备解释时间序列趋势、噪声和极值的能力，为构建可解释且适用于隐私场景的时间序列模型提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型在时间序列推理能力方面存在不足，难以实现对时间序列的可解释性理解，因此作者希望将时间序列推理能力蒸馏到小型、可指令微调的语言模型中，以促进可解释的时间序列基础模型构建。

Method: 该研究首先利用均值回复型时间序列的合成数据集（趋势和噪声水平系统变化），通过大型多模态模型生成自然语言注释，然后用这些注释监督小型Qwen模型的微调。研究还引入了针对趋势方向、噪声强度和极值点定位等方面的度量指标，评估蒸馏后模型的时间序列推理能力。

Result: 经微调后的小型Qwen模型获得了有效的时间序列解释能力，在趋势判断、噪声识别和极值定位等方面表现良好，实验结果证实可以将时间序列理解能力浓缩到轻量级、具备语言能力的模型中。

Conclusion: 本文证明了通过自然语言监督，可以将复杂的时间序列推理能力压缩到小型语言模型里，为端侧或隐私敏感场景下的可解释时间序列基础模型奠定了基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [18] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: SAND框架让大语言模型Agent能在行动前权衡多方案，显著提升了任务表现，优于主流调优方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）Agent主要通过监督微调或偏好优化等方式进行调优，这些方法强调模仿专家行为或偏好推理路径，但缺乏对备选行动的充分推理和比较，容易导致模型选择次优解。

Method: 提出了一种名为SAND（Self-taught ActioN Deliberation）的新框架，使LLM Agent在采取行动前能对候选行动进行显式推理和抉择。方法包括自洽性动作采样与执行导向的批判机制，逐步生成推理路径并迭代反哺模型微调。

Result: 在两类具有代表性的交互式Agent任务上验证，SAND框架实现了较初始监督微调平均提升20%，同时优于当前最新的Agent调优方法。

Conclusion: SAND框架通过显式的行动推理与比较，有效提升了LLM Agent决策质量，是优化Agent调优流程的有效手段。

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [19] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: RLEP通过高质量经验回放优化大语言模型RL训练，既提升表现，又节省训练资源，实验结果大幅优于基线且开源支持复现。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型上的训练不仅算力消耗大，而且存在训练不稳定、模型策略逐渐偏移预训练权重的难题。

Method: 提出RLEP（Reinforcement Learning with Experience rePlay）两阶段框架：首先收集高质量的已验证轨迹，然后在训练中与新生成的数据混合回放这些高质量案例，通过这种机制优化模型。

Result: 在Qwen2.5-Math-7B基线上，RLEP能更快达到基线峰值准确率，并进一步提升最终性能：AIME-2024从38.2%提升至39.9%，AIME-2025从19.8%提升至22.3%，AMC-2023从77.0%提升至82.2%。

Conclusion: RLEP方法可减少无效探索，对高潜力解题路径聚焦，加快收敛并提升强化学习在大模型上的最终表现。作者开源了代码、数据集和检查点促进复现与社区研究。

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [20] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 提出“机器Bullshit”新框架和量化指标，实验证明目前主流大模型在强化学习微调和链式推理下常输出伪真或无关事实的信息，在复杂情境（如政治领域）表现更突出，AI对齐和真实性问题需高度关注。


<details>
  <summary>Details</summary>
Motivation: 现有关于大模型幻觉与谄媚性的研究不足以覆盖所有“虚假表达”现象，需要一个统一的框架与量化方式，来系统性分析模型产生不真实信息的机制与表现形式。

Method: 提出Bullshit Index作为量化指标，并结合空洞陈词、掩饰词、模糊语言和未经证实说法等四种BS类型，在多个基准测试集（如Marketplace、Political Neutrality以及自建BullshitEval）上评估不同模型表现。

Result: RLHF（人类反馈强化学习）会显著恶化模型bullshit表现，推理时的链式思考（CoT）会放大“空洞陈词”和“掩饰词”等类型，政治场景下尤为严重，“模糊语言”策略广泛出现，对AI对齐和真实输出提出新的挑战。

Conclusion: 研究发现，当前大模型（LLMs）在调整和生成过程中特别容易表现出“bullshit”（胡说八道），这是一种对真实性漠不关心的信息输出，其水平随RLHF等微调方法增强，并在政治等敏感领域尤为突出，呈现特定策略。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [21] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: 本工作提出PLAN-TUNING，利用大模型蒸馏的任务分解轨迹微调小模型，大幅提升其复杂推理和泛化能力，是小型LLM后训练提升性能的有效新手段。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽然在复杂任务上表现优异，但主要依赖分解任务为更易管理的子任务（人类式规划能力）。开放源代码的小型LLM在后训练阶段利用这一策略以提升性能，目前探究较少。作者因此提出新方法以弥补这一空白。

Method: 提出PLAN-TUNING统一后训练框架：1）从大规模LLM中蒸馏合成的任务分解（即“规划轨迹”）；2）通过设计的监督学习和强化学习目标微调小模型，使其模仿规划过程以提升复杂推理能力。

Result: 在GSM8k和MATH基准测试上，经过规划微调的模型平均超越强基线约7%；在OlympiadBench和AIME 2024这类领域外数据集上，分别提升了大约10%和12%。细致分析证实所提规划轨迹有效促进了复杂推理能力提升。

Conclusion: PLAN-TUNING策略能有效提升小型LLM在特定任务上的复杂推理能力，并显著改善其泛化能力。此方法为提升小型开源LLM性能提供了新路径。

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [22] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: TeaR通过数据筛选和强化学习提升LLM推理能力，在17项基准测试中实现显著性能提升，尤其在Qwen2.5-7B等模型表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在推理能力方面存在不足，直接基于代码执行进行推理又容易导致模型过于依赖复杂的数据结构和算法，反而忽视了推理本质。为了解决这一问题，研究者希望通过更有效的方法增强模型的推理能力。

Method: 本文提出了一种名为TeaR的方法，通过精心的数据筛选和强化学习，引导模型在代码相关任务中发现最优推理路径，从而提升推理能力。方法涉及大规模多模型、多基准测试，涵盖数学、知识、代码及逻辑推理等领域。

Result: 通过在1.5B到32B参数规模的两组基础模型和三组长链式思维蒸馏模型上进行实验，跨越17个基准测试，结果显示TeaR带来了显著性能提升。例如，在Qwen2.5-7B和R1-Distilled-7B模型上分别提升了35.9%和5.9%。

Conclusion: TeaR方法通过结合数据筛选与强化学习，能够有效帮助大模型提升代码相关任务中的推理能力，并在多个推理领域均带来大幅性能提升。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [23] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: 提出基于BERT的NER和RE方法，提升文献中ORR催化剂信息抽取效果，领域专用BERT表现最佳，并显著优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 燃料电池中氧还原反应（ORR）催化剂对提升燃料电池效率至关重要。然而，从大量多样且复杂的科学文献中有效提取与ORR催化剂相关的结构化信息仍是重大挑战。论文旨在解决材料信息学领域对此类信息抽取的迫切需求。

Method: 本研究提出了一种基于命名实体识别（NER）和关系抽取（RE）的方法，结合DyGIE++架构及多种预训练BERT模型（如MatSciBERT和PubMedBERT），用于从文献中抽取ORR催化剂相关信息。此外，构建了一个包含12类关键实体和两类实体关系的数据集，并通过人工标注和模型微调提高抽取精度。

Result: 实验结果显示，微调后的PubMedBERT模型在NER任务中取得最高F1分数（82.19%），而MatSciBERT在RE任务中表现最佳（F1=66.10%）。对比人工标注，模型表现出高度可靠性，且领域专用BERT模型优于通用型科学模型（如BlueBERT）。

Conclusion: 结合NER和RE的自动文献信息抽取方法，特别是使用领域专用BERT模型，可实现对ORR催化剂等领域知识的高效、可扩展的信息获取，为材料科学文献分析和材料信息学研究提供了有力工具。

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [24] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: LLM在复杂任务面前有能力极限，对高复杂性任务既不能有效执行，也不能准确验证。这会影响其在实际Agent场景的应用。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer类LLM的广泛应用及其“幻觉”现象，以及其被用于自主或半自主agent的趋势，了解LLM能否胜任不同类型与复杂度任务变得至关重要。

Method: 以计算复杂性为分析视角，通过理论推导和具体实例说明LLMs在应对高复杂度任务及验证任务准确性方面的能力极限。

Result: 证明了LLMs在计算复杂或涉及agent行为的任务中，无法完成超出一定复杂度的任务，也无法验证高复杂度任务结果的准确性，并举例说明，最后分析了这一发现的影响。

Conclusion: LLMs 在计算复杂度超过一定阈值的任务上，无论是完成任务还是验证其准确性，都会受到不可逾越的能力限制。

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [25] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出基于小数据和专家知识的心理对话数据大规模生成与修饰框架，发布高质量中文心理支持数据集CPsDD和高性能对话支持系统CADSS，取得了业界领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前社会压力增加导致心理支持需求提升，但在非英语语言中相关数据集稀缺，制约了智能对话系统的发展。

Method: 提出了一套框架，结合有限真实数据和专家知识，对两个大语言模型（对话生成器和对话修饰器）进行微调。生成器基于预设路径大规模生成心理咨询对话，修饰器对生成数据进行质量提升。最终通过自动与人工审查，构建了中文心理支持对话数据集CPsDD，并推出了综合对话支持系统（CADSS），涵盖用户画像分析、历史摘要、策略规划与情感支持生成。

Result: 成功构建了包含68K对话、覆盖多类心理问题的CPsDD数据集，并推出CADSS系统，在策略预测和情感支持对话任务上于CPsDD和ESConv数据集均获得SOTA表现。

Conclusion: 本研究填补了中文心理咨询对话数据的空白，提出的生成和修饰机制可拓展至限定资源场景，CADSS系统表现突出，推动了心理支持智能对话的研究发展。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [26] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本研究将语音活动预测(VAP)首次应用到三人对话，并验证了其优越性。未来可用于对话系统改进发言轮换。


<details>
  <summary>Details</summary>
Motivation: 轮换发言是口语对话中的基本要素，然而以往研究多集中于二人对话。本研究尝试解决三人多方对话中如何预测发言轮换的问题。

Method: 将语音活动预测（VAP）方法应用于三方对话场景，并在日本三人对话数据集上训练多个模型，仅利用声学数据来预测每位说话者未来的语音活动。

Result: VAP在三人对话场景下的模型优于基线模型，但对话类型会影响预测准确率。

Conclusion: 首次验证了VAP方法可用于三人对话的轮换发言预测，并为后续在对话系统中应用奠定基础。

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [27] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 本文提出用大语言模型+few-shot prompts进行多语种主观性检测，可抗噪声、对标注不一致鲁棒，泛化好，效果优于传统微调小模型，是标注数据有限场景的优选。


<details>
  <summary>Details</summary>
Motivation: 当前主流主观性检测方法主要依赖小规模语言模型的微调，然而在多语言任务、数据噪声大、标注不一致或标注数据稀缺的场景下，传统方法效果有限。大语言模型具备零样本与少样本学习能力，尚未系统评估其在多语种主观性任务中的实际表现和优势。

Method: 采用大型语言模型（LLM）结合精心设计的few-shot提示词（prompting）进行主观性检测。对比了LLM与精调过的小型语言模型（SLM）在多语言环境下的表现，并尝试了争论式prompt、不同示例选取等高级prompt工程方法。系统参与了CheckThat! 2025评测任务1（多语种主观性检测），在不同语种的数据集上进行实测。

Result: LLM配合合理few-shot提示后，整体表现优于或持平于微调SLM，尤其在数据噪声较大或质量较低的场景下更加突出。高级prompt工程提升有限。该系统在CheckThat! 2025评测多语种任务中多国语种排名第一到前四，尤其在阿拉伯语数据集上表现极为稳健，对标注不一致高鲁棒性。

Conclusion: LLM加少样本prompt是一种强有力的多语种主观性检测方案，在标注数据稀缺、质量不高或一致性较差时，能替代传统微调方法，具有广泛适用性和高效性。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [28] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 领域特定跨语种RAG中，检索是关键瓶颈。针对阿英双语，提出均衡双语检索策略，大幅提升了跨语检索与生成性能，揭示多语RAG改进空间。


<details>
  <summary>Details</summary>
Motivation: 以往跨语种RAG研究多依赖英文维基等开放领域数据集，掩盖了实际跨语种检索的难点，且很少关注领域特定的真实应用场景。本文旨在弥补这一空白，深入分析真实企业数据集下阿拉伯语-英语RAG表现。

Method: 构建领域特定的跨语种基准数据集，涵盖用户查询与支持文档的所有语言组合，并系统性分析多语检索行为。同时，提出等比例从两种语言检索文档的新策略。

Result: 发现“检索”是领域特定跨语种RAG的关键瓶颈，查询语言与文档语言不同时性能显著下降，归因于检索器跨语排序困难。提出的均衡检索策略显著提升了跨语及整体性能。

Conclusion: 本文凸显了跨语种、领域特定RAG实际场景下检索环节的重要性，并展示了改进多语检索的实用方法与提升空间，推动了实际企业应用的进展。

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [29] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: SFT增强视觉语言模型复杂推理但倾向冗长，RL提升整体泛化性但难题提升有限，两者在现有方式下难以直接叠加，需新方法实现协同效能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型（VLMs）在复杂推理任务中常采用长链式思维（CoT）监督微调（SFT）和强化学习（RL）等后训练技术，但这些技术在VLM中的协同效果未明。

Method: 系统性地对比SFT与RL在多种多模态推理基准上的效果，探索两种技术单独及联合训练（分阶段、交替、渐进训练、数据混合、模型融合）对模型性能的影响。

Result: SFT能提升复杂问题的结构化推理表现但导致冗长且简单题表现下降；RL促进泛化简洁性并带来全难度层次的稳健提升，但应对最难问题的提升低于SFT。联合各种训练策略后未见明显叠加效果，反而导致准确率、推理风格和回答长度的权衡。

Conclusion: SFT和RL在VLM中各具优势但难以直接叠加，需发展更无缝和自适应的方法，才能充分发挥两者结合的潜力。

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [30] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 本工作针对文档图像机器翻译泛化难题，提出M4Doc对齐框架，实现轻量模型在不损失效率的前提下极大提升翻译质量，尤其在跨领域和复杂场景中效果突出。


<details>
  <summary>Details</summary>
Motivation: 在文档图像机器翻译（DIMT）任务中，由于训练数据有限以及视觉和文本信息的复杂交互，模型泛化能力不足。该研究希望解决DIMT中的泛化性和跨领域挑战。

Method: 提出M4Doc框架，通过将仅含图像的编码器与多模态大语言模型（MLLMs）的多模态表示对齐。具体做法是在大规模文档图像数据集上预训练，然后利用对齐方法，让轻量的DIMT模型在训练时学习视觉—文本相关性，推理时则无需MLLM，保证效率。

Result: 采用M4Doc的DIMT模型在实验中表现出显著提升，尤其是在跨领域泛化能力和复杂文档图像场景下的翻译质量上明显优于基线方法。

Conclusion: M4Doc通过单-多模态对齐方法，有效增强了文档图像机器翻译的泛化性和翻译质量，实现了高效且高性能的DIMT新范式。

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [31] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: 本文发现并利用了离散扩散语言模型的贝叶斯性质，提出通过推理时集成多个mask-and-denoise的均值预测大幅提升后验推断和不确定性估计效果，在WikiText-2上测试困惑度显著优于GPT-2 Small，且无须额外训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散语言模型虽表现良好，但其推理时对不确定性的表达与后验概率的捕捉不足。作者希望通过理论分析和简单实用的推理方法提升模型后验贝叶斯推断能力，并给出具体可用的改进方案。

Method: 作者通过理论推导证明了前向mask分布下去噪器输出的期望可以准确恢复干净词元的后验概率，并利用蒙特卡洛边缘化（对K个独立损坏样本）推导出一致性证明及有限样本误差界。另外，提出了一种推理时对K次mask-and-denoise过程做集成平均的方法，来获得更准确的后验概率与不确定性估计。

Result: 在WikiText-2数据集上，作者提出的方法仅用K=8（即8次mask-and-denoise评价后均值）即可达到8.8的测试困惑度（Perplexity），显著优于规模类似的GPT-2 Small（PPL为20.3），且无需额外训练花费。方法开源。

Conclusion: 本文揭示了离散扩散语言模型具有隐含的贝叶斯核心，并提出了一种无需额外训练开销、在推理时可高效利用的集成方法，有效提升了模型在文本生成任务中的性能。

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [32] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本研究表明，通过知识蒸馏和简单提示，LLMs可压缩为参数量缩减近一半、但性能几乎不损失的小模型，适合资源有限的实际问答应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多种自然语言处理（NLP）任务上表现出色，但由于其对算力需求高，难以在资源有限的真实环境中部署。本文旨在研究在保持较强性能的前提下，能否通过知识蒸馏（KD）压缩LLM，从而降低模型的参数量和部署成本。

Method: 采用知识蒸馏技术，从Pythia和Qwen2.5两个模型系列中分别蒸馏出学生模型，并在两个问答（QA）基准测试集（SQuAD和MLQA）上，分别在零样本（zero-shot）和单样本（one-shot）提示条件下进行评估。

Result: 结果表明，学生模型在参数量最多缩减57.1%的情况下，依然保留了超过90%的教师模型性能。同时，单样本提示在两个模型系列上均带来了超越零样本的进一步性能提升。

Conclusion: 知识蒸馏结合极简提示技术，可以实现体积小而能力强的QA系统，适合部署于资源受限的实际应用场景，并揭示了模型性能与效率之间的权衡关系。

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [33] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 只改进提示并结合ReAct流程即能大幅提升复杂问题RAG性能，无需大规模微调。通过少量监督或RL微调，还可无损准确率下将检索次数减半，实现更高效的复杂问答。


<details>
  <summary>Details</summary>
Motivation: 已有方法针对复杂问题回答主要依赖于检索增强生成（RAG），并通过大规模微调或RL微调来提升精度与召回，但对检索效率关注不足。本文旨在同时关注RAG性能及检索次数/效率。

Method: （1）分析大规模微调的必要性，使用标准的ReAct流程结合改进提示，评估其在复杂问答任务的表现；（2）探究监督与RL微调在提高检索效率（减少检索次数）上的作用，并在经典RAG基准上进行对比实验。

Result: 发现无需大规模微调，仅通过改进提示的ReAct流程即可在HotPotQA等基准上超越现有SOTA。利用监督和RL微调可在不降低性能情况下将检索次数减少约一半，训练数据需求极低（仅1000例）。

Conclusion: 改进提示加ReAct足以提升复杂问答准确率，无需大规模微调。监督及RL微调能有效降低检索成本，实现更高效率。

Abstract: We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


### [34] [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
*Haotan Guo,Jianfei He,Jiayuan Ma,Hongbin Na,Zimu Wang,Haiyang Zhang,Qi Chen,Wei Wang,Zijing Shi,Tao Shen,Ling Chen*

Main category: cs.CL

TL;DR: 作者提出中文音近字替换四分类和真实毒性数据集，曝露现有检测模型的局限，证明拼音提示能显著提升检测准确性，为后续内容安全研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 音近字替换（PCR）作为中文内容审核中的难题，现有方法多基于规则或合成扰动，缺乏对真实用户创造性的捕捉，难以有效检测真实场景下的恶意内容。

Method: 提出了一个PCR表面形式四分类法，收集整理了RedNote平台上500条自然发生的音近字掩饰攻击数据集，并用当前主流大语言模型（LLMs）在该数据集上进行检测基准测试。通过误差分析，重新测试了之前被认为无效的基于拼音的提示方法，发现其能显著提升检测准确率。

Result: 最好的大语言模型检测该数据集时F1分数只有0.672，采用zero-shot chain-of-thought提示甚至会导致效果下降。拼音提示方法则能有效恢复检测准确率。

Conclusion: 本文首次系统提出中文音近字替换分类，对比现有模型在真实语料下的性能，揭示了其不足，并验证了一种高效轻量的改进方法，为鲁棒低毒性内容检测研究提供了重要基线和新方向。

Abstract: Phonetic Cloaking Replacement (PCR), defined as the deliberate use of
homophonic or near-homophonic variants to hide toxic intent, has become a major
obstacle to Chinese content moderation. While this problem is well-recognized,
existing evaluations predominantly rely on rule-based, synthetic perturbations
that ignore the creativity of real users. We organize PCR into a four-way
surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring,
phonetically cloaked offensive posts gathered from the RedNote platform.
Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:
the best model reaches only an F1-score of 0.672, and zero-shot
chain-of-thought prompting pushes performance even lower. Guided by error
analysis, we revisit a Pinyin-based prompting strategy that earlier studies
judged ineffective and show that it recovers much of the lost accuracy. This
study offers the first comprehensive taxonomy of Chinese PCR, a realistic
benchmark that reveals current detectors' limits, and a lightweight mitigation
technique that advances research on robust toxicity detection.

</details>


### [35] [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
*Andrew D. Foland*

Main category: cs.CL

TL;DR: 提出NOIR指标，结合摘要长度压缩与语义保留，自动衡量摘要质量，效果与人工评价相关性高，适用性广。


<details>
  <summary>Details</summary>
Motivation: 现有文本摘要质量评价主要依赖人工生成的参考摘要，耗时且资源消耗大；且衡量摘要好坏的关键是语义保留与压缩率的平衡。

Method: 提出了一种新的客观定量指标NOIR（NOrmed Index of Retention），同时考虑语义信息保留情况与摘要长度压缩，采用语言模型嵌入度量语义相似度，实现摘要质量的自动化评价。

Result: 实验表明NOIR指标能有效反映摘要长度与语义保留的权衡，且与人工评价一致，能够自动地测量摘要质量。

Conclusion: NOIR可以广泛用于各种摘要任务，为摘要算法、提示词设计及合成摘要的自动化评价和改进提供了新工具。

Abstract: This paper proposes NOrmed Index of Retention (NOIR), a quantitative
objective metric for evaluating summarization quality of arbitrary texts that
relies on both the retention of semantic meaning and the summary length
compression. This gives a measure of how well the recall-compression tradeoff
is managed, the most important skill in summarization. Experiments demonstrate
that NOIR effectively captures the token-length / semantic retention tradeoff
of a summarizer and correlates to human perception of sumarization quality.
Using a language model-embedding to measure semantic similarity, it provides an
automated alternative for assessing summarization quality without relying on
time-consuming human-generated reference summaries. The proposed metric can be
applied to various summarization tasks, offering an automated tool for
evaluating and improving summarization algorithms, summarization prompts, and
synthetically-generated summaries.

</details>


### [36] [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Yuehao Wang,Peihao Wang,Jing Xiong,Liliang Ren,Hao Cheng,Janardhan Kulkarni,Yelong Shen,Atlas Wang,Mac Schwager,Anderson Schneider,Xiaodong Liu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 提出一种模拟多头注意力能力但无额外参数开销的方法（SAS+PEAA），在多个任务上显著提升Transformer效果。


<details>
  <summary>Details</summary>
Motivation: 在Transformer架构中，提升注意力头数量和每个头的隐藏维度有助于提升模型性能，但常规做法会造成参数量增长。该论文希望在参数量较低的情况下，提升注意力机制的表达能力。

Method: 提出Simulated Attention Score（SAS）方法，通过将低维头部表示投影到高维空间来模拟更多的注意力头和更大的每头特征维度，实现参数不变下的能力增强。此外，将这种模拟扩展到key和query的特征维度；并提出Parameter-Efficient Attention Aggregation（PEAA）来控制参数量。

Result: 实验证明SAS方法在多种数据集和任务上显著优于现有的不同注意力机制方法。

Conclusion: 通过SAS和PEAA，可以有效在参数量基本不变的情况下提升注意力机制的表达能力，获得更好的模型性能。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Various methods have been developed to compute attention scores, including
multi-head attention (MHA), multi-query attention, group-query attention and so
on. We further analyze the MHA and observe that its performance improves as the
number of attention heads increases, provided the hidden size per head remains
sufficiently large. Therefore, increasing both the head count and hidden size
per head with minimal parameter overhead can lead to significant performance
gains at a low cost. Motivated by this insight, we introduce Simulated
Attention Score (SAS), which maintains a compact model size while simulating a
larger number of attention heads and hidden feature dimension per head. This is
achieved by projecting a low-dimensional head representation into a
higher-dimensional space, effectively increasing attention capacity without
increasing parameter count. Beyond the head representations, we further extend
the simulation approach to feature dimension of the key and query embeddings,
enhancing expressiveness by mimicking the behavior of a larger model while
preserving the original model size. To control the parameter cost, we also
propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive
experiments on a variety of datasets and tasks demonstrate the effectiveness of
the proposed SAS method, achieving significant improvements over different
attention variants.

</details>


### [37] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: K2RAG框架通过融合多种技术提升RAG答案质量与效率，在答案准确性、训练速度、显存占用等方面显著优于传统方法，是高效扩展LLM知识的有力工具。


<details>
  <summary>Details</summary>
Motivation: 当前微调LLM成本高昂且不易扩展，RAG虽可减少直接微调需求，但传统实现存在扩展性和准确率瓶颈，需创新型方案优化知识整合方式。

Method: 提出了KeyKnowledgeRAG（K2RAG）框架，融合稠密/稀疏向量检索、知识图谱与文本摘要，利用“分而治之”策略提升检索与系统效率。引入预处理步骤对训练数据摘要，显著提高训练效率。评测用MultiHopRAG数据集，衡量答案相似度与系统效率。

Result: K2RAG框架在MultiHopRAG数据集上，平均答案相似度达0.57，Q3相似度0.82，效率高于传统知识图谱RAG。数据摘要将训练时间减少93%，执行速度提升40%，显存占用降至1/3，且准确率更高。

Conclusion: K2RAG框架明显提升了RAG答案质量及系统效率，对大规模知识扩展具备很大潜力。相比常规RAG，能更好对齐真实答案，同时大幅降低训练时间和显存需求。

Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [38] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

TL;DR: 作者复现并拓展了Vec2Text攻击方法，证实其可从文本嵌入重建原文，包括密码类信息。研究了防御方法，推荐采用量化等方式缓解隐私风险，提示NLP系统需重视嵌入泄露问题。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为传递文本嵌入（而非原始文本）有助于隐私保护，但最新研究方法（如Vec2Text）表明可以从嵌入中重建原始文本，质疑了这一假设。作者因此动机，进一步验证这些结论并探究相应的隐私风险与防护方案。

Method: 作者复现了Vec2Text框架，分别从验证原始结论和扩展实验两个角度进行评估。包括参数敏感性分析、对敏感输入（如密码）的重建可行性实验，以及探讨嵌入量化等能否作为轻量级隐私防御方案。

Result: 成功复现了原文主要结果，证实在理想条件下Vec2Text不仅能在域内、域外环境中重建文本，甚至能恢复密码类等无明显语义的信息。发现其对输入长度敏感。添加高斯噪声和采用嵌入量化均可降低重建风险，其中量化更简单且适用面广。

Conclusion: 当前的文本嵌入存在隐私泄露风险，Vec2Text等方法能够在特定条件下重建原始文本。建议实际应用中谨慎使用，并持续研究更为健壮的防护方案，嵌入量化等简单方法可作为初步防御措施。

Abstract: Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [39] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

TL;DR: 本文提出Selective-DPO方法，实现更高效与精确的大模型对齐，强调词元级优化和参考模型权重，有效提升实际表现。


<details>
  <summary>Details</summary>
Motivation: 大模型训练后期的对齐是关键难题，因为模型并非对所有词元（token）都同等关注，有些词元对整体表现影响更大。

Method: 提出了一种选择性对齐方法，依据当前策略与参考模型之间在词元级别的对数概率差异，优先对高影响力的词元进行对齐。

Result: 通过在Arena-Hard和MT-Bench等基准测试上实验，Selective-DPO方法优于标准DPO及蒸馏等传统方法，且高质量参考模型能进一步提升效果。

Conclusion: 词元级别的优化及参考模型的选择对大模型偏好对齐至关重要，Selective-DPO能够提高对齐精度并降低计算负担。

Abstract: Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [40] [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
*Maha Tufail Agro,Atharva Kulkarni,Karima Kadaoui,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文以系统化综述方式总结了端到端ASR在代码切换问题上的研究现状，分析了可用资源、研究趋势和所面临的挑战，为后续研究提供了全面参考。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）领域对于代码切换现象的关注日益增加。代码切换在多语言环境中十分常见，因此有必要对端到端ASR模型中涉及代码切换的相关研究进行系统梳理。

Method: 收集并手动标注发表在同行评审会议上的相关论文。从语言、数据集、指标、模型选择、性能等多维度进行整理和归纳，并讨论端到端ASR处理代码切换时面临的挑战。

Result: 系统文献综述总结了当前端到端ASR在代码切换方面所考虑的语言、可用数据集、评估指标、典型模型及其性能表现。同时提出了这一领域面临的主要挑战。

Conclusion: 本研究为当前端到端ASR代码切换研究提供了全景式的资源梳理与挑战分析，对未来的研究方向和资源开发具有一定指导意义。

Abstract: Motivated by a growing research interest into automatic speech recognition
(ASR), and the growing body of work for languages in which code-switching (CS)
often occurs, we present a systematic literature review of code-switching in
end-to-end ASR models. We collect and manually annotate papers published in
peer reviewed venues. We document the languages considered, datasets, metrics,
model choices, and performance, and present a discussion of challenges in
end-to-end ASR for code-switching. Our analysis thus provides insights on
current research efforts and available resources as well as opportunities and
gaps to guide future research.

</details>


### [41] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

TL;DR: 本论文首次全面评述LLM在法律领域的应用，提出创新分类框架，系统总结技术进展与挑战，为后续研究与实践提供理论与技术指导。


<details>
  <summary>Details</summary>
Motivation: 现有关于大型语言模型（LLM）在法律领域应用的综述不够全面，缺乏系统性的研究框架，且没有很好地统一历史研究和最新突破。当前LLM在法律推理和专业本体集成方面展现出新能力，但依然存在许多技术与实际挑战。本文旨在填补该领域系统化梳理和分类的空白，为研究者和实践者提供全面的理论与技术参考。

Method: 提出了一种创新性的双视角分类法，将法律推理框架与职业本体相结合，系统性梳理和统一了历史研究及当前技术进展。纳入了Transformer为代表的大语言模型，并分析其在法律领域具体任务中的应用。对技术创新（如稀疏注意力机制、专家混合架构等）进行了归纳，并实现了图尔明（Toulmin）论证框架的计算化。还创建了在线GitHub资源库索引相关论文。

Result: 系统总结了LLM在法律领域的工作进展、主要技术突破以及现实应用挑战，包括在任务泛化、推理形式化、流程集成、文本处理、知识融合和评估等方面的改进。指出LLM的广泛应用带来的新问题（如幻觉、可解释性不足、司法适配和伦理风险）。提出了以法律角色对齐NLP子任务、用图尔明论证结构系统化法律推理的新范式，并确定了低资源、证据多模态融合、动态反驳处理等关键前沿方向。

Conclusion: 本文为LLM在法律领域应用构建了全面的理论和技术框架，对当前进展、挑战与未来方向进行了系统梳理，为算法时代法律AI的发展奠定了坚实基础，并为研究与实践指明了方向。通过建立分类法和技术路线图，促进了学术与行业的交流合作。

Abstract: This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [42] [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
*Shoutao Guo,Xiang Li,Shaolei Zhang,Mengge Liu,Wei Chen,Yang Feng*

Main category: cs.CL

TL;DR: StreamUni用统一大模型和语音CoT思想，实现了无需切分模型和大量策略专用训练的高性能实时语音翻译。


<details>
  <summary>Details</summary>
Motivation: 现有的实时语音翻译（StreamST）方法通常只能在句子级别进行（SimulST），需要与语音切分模型配合使用，并且由于只能使用有限的上下文信息进行策略决策和翻译，模型性能受到制约。此外，SimulST模型很难在复杂的语音输入和跨语言生成下学到有效的决策策略。

Method: 提出了一种新的统一大语音-语言模型（LSLM）架构StreamUni。StreamUni利用语音版的Chain-of-Thought（CoT）思想，引导模型分阶段输出，通过多阶段输出自主完成语音切分、策略决策和翻译，无需大量策略训练数据。同时，提出了流式CoT训练方法，通过少量CoT数据提升模型在低延迟决策和生成方面的能力。

Result: 实验表明，StreamUni方法在实时语音翻译任务上达到了当前最优性能。

Conclusion: StreamUni实现了无需依赖大量策略专用训练和额外切分模型的端到端实时语音翻译，提升了翻译质量和决策效率，在该领域取得了新的技术进展。

Abstract: Streaming speech translation (StreamST) requires determining appropriate
timing, known as policy, to generate translations while continuously receiving
source speech inputs, balancing low latency with high translation quality.
However, existing StreamST methods typically operate on sentence-level speech
segments, referred to as simultaneous speech translation (SimulST). In
practice, they require collaboration with segmentation models to accomplish
StreamST, where the truncated speech segments constrain SimulST models to make
policy decisions and generate translations based on limited contextual
information. Moreover, SimulST models struggle to learn effective policies due
to the complexity of speech inputs and cross-lingual generation. To address
these challenges, we propose StreamUni, which achieves StreamST through a
unified Large Speech-Language Model (LSLM). Specifically, StreamUni
incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate
multi-stage outputs. Leveraging these multi-stage outputs, StreamUni
simultaneously accomplishes speech segmentation, policy decision, and
translation generation, completing StreamST without requiring massive
policy-specific training. Additionally, we propose a streaming CoT training
method that enhances low-latency policy decisions and generation capabilities
using limited CoT data. Experiments demonstrate that our approach achieves
state-of-the-art performance on StreamST tasks.

</details>


### [43] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

TL;DR: 提出Transformer模型反演Signal Temporal Logic嵌入为公式，快速收敛、泛化良好，并促进了符号逻辑与数据驱动方法的融合。


<details>
  <summary>Details</summary>
Motivation: 将符号知识与数据驱动的学习算法融合，需要获得语义一致且可逆的逻辑公式连续表示。当前，虽然可以生成连续的向量嵌入，但嵌入的可逆性和语义一致性不足，限制了其实际应用。

Method: 采用基于Transformer的纯解码器模型，对Signal Temporal Logic (STL)公式的语义嵌入进行反演。构建了一个基于STL语法的小型词汇表，通过训练模型从向量嵌入还原出有效的STL公式。

Result: 该模型在仅一个epoch后即可生成有效公式，约10个epoch即可泛化至逻辑语义，还能解码得到长度和嵌套更简洁但语义接近或等价的公式。实验证明模型泛化能力强，并通过需求挖掘任务验证了其实用性，在语义空间上直接进行优化。

Conclusion: 提出的方法能够有效从连续嵌入反演生成STL公式，并在泛化、需求挖掘等任务中表现出良好性能，有助于将逻辑公式更好地应用于数据驱动的学习流程。

Abstract: Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [44] [The Richness of CSP Non-redundancy](https://arxiv.org/abs/2507.07942)
*Joshua Brakensiek,Venkatesan Guruswami,Bart M. P. Jansen,Victor Lagerkvist,Magnus Wahlström*

Main category: cs.DM

TL;DR: 论文系统研究约束满足问题（CSP）中的非冗余性，证明了可达任意多项式规模，并首次分类所有二元谓词的条件非冗余，还发展了代数理论，扩展了Mal'tsev嵌入的应用，为相关理论和工具的进一步拓展奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 非冗余在CSP相关问题（如稀疏化、核化、查询复杂性、极值组合等）中起着关键作用，但此前对非冗余本身的理解尚不深入。本论文旨在系统研究和揭示非冗余的本质。

Method: 1. 构造性证明CSP谓词P的非冗余度可以达到任意给定的有理指数。
2. 通过与高圈数图结构的关联，完全分类了所有二元谓词的条件非冗余。
3. 基于代数理论推进对条件非冗余的分析，并研究Mal'tsev嵌入的推广和应用。

Result: 1. 证明了对任意r≥1，存在谓词P使其非冗余为Θ(n^r)。
2. 给出了所有二元谓词条件非冗余的完全分类。
3. 扩展了Mal'tsev嵌入的应用范围，首次展示了非阿贝尔群结构（如量子Pauli群）下的特殊例子。

Conclusion: 本文深化了对CSP非冗余性质的理解，不仅系统地描绘了其规模界，还在代数与组合角度上推进了理论基础，并拓宽了已知技术（如Mal'tsev嵌入）的适用范围。

Abstract: In the field of constraint satisfaction problems (CSP), a clause is called
redundant if its satisfaction is implied by satisfying all other clauses. An
instance of CSP$(P)$ is called non-redundant if it does not contain any
redundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum
number of clauses in a non-redundant instance of CSP$(P)$, as a function of the
number of variables $n$. Recent progress has shown that non-redundancy is
crucially linked to many other important questions in computer science and
mathematics including sparsification, kernelization, query complexity,
universal algebra, and extremal combinatorics. Given that non-redundancy is a
nexus for many of these important problems, the central goal of this paper is
to more deeply understand non-redundancy.
  Our first main result shows that for every rational number $r \ge 1$, there
exists a finite CSP predicate $P$ such that the non-redundancy of $P$ is
$\Theta(n^r)$. Our second main result explores the concept of conditional
non-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We
completely classify the conditional non-redundancy of all binary predicates
(i.e., constraints on two variables) by connecting these non-redundancy
problems to the structure of high-girth graphs in extremal combinatorics.
  Inspired by these concrete results, we build off the work of Carbonnel [CP
2022] to develop an algebraic theory of conditional non-redundancy. As an
application of this algebraic theory, we revisit the notion of Mal'tsev
embeddings, which is the most general technique known to date for establishing
that a predicate has linear non-redundancy. For example, we provide the first
example of predicate with a Mal'tsev embedding that cannot be attributed to the
structure of an Abelian group, but rather to the structure of the quantum Pauli
group.

</details>
