<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.CL](#cs.CL) [Total: 16]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dual-Numbers Reverse AD for Functional Array Languages](https://arxiv.org/abs/2507.12640)
*Tom Smeding,Mikołaj Konarski,Simon Peyton Jones,Andrew Fitzgibbon*

Main category: cs.PL

TL;DR: 该论文提出一种通过向量化代码转换和代码结构调整，实现对多维数组高效反向自动微分的方法。牺牲部分高阶扩展性，换取极小的性能开销和实际可用性。


<details>
  <summary>Details</summary>
Motivation: 当前，标准的双数（dual-numbers）结构在前向模式自动微分中应用良好，且具有简单性；虽然已被扩展到反向模式自动微分，但在处理数组程序时的实际性能表现仍不理想。本文旨在解决反向模式自动微分在多维数组上的性能瓶颈。

Method: 提出了一种针对多维数组的首级支持，采用三大松散耦合组件：一是保持语义的向量化代码转换(BOT)，二是将双数反向AD算法提升到一阶数组语言，三是符号解释以实现端到端的编译流程。

Result: 实验显示，本文方法可在性能几乎无额外开销的情况下支持多维数组的双数反向自动微分。但该方法牺牲了一些可推广性，特别是对高阶代码的支持受到限制，仅支持build、gather、scatter等部分高阶数组组合子。

Conclusion: 通过引入BOT并适当限制高阶特性，将高阶AD程序转化为一阶程序，从而实现了高效的多维数组反向自动微分。此方法虽然损失了部分通用性，但极大提升了AD在数组程序上的应用性能。

Abstract: The standard dual-numbers construction works well for forward-mode automatic
differentiation (AD) and is attractive due to its simplicity; recently, it also
has been adapted to reverse-mode AD, but practical performance, especially on
array programs, leaves a lot to be desired. In this paper we introduce
first-class support for multidimensional arrays in dual-numbers reverse-mode AD
with little to no performance overhead. The algorithm consists of three
loosely-coupled components: a semantics-preserving vectorisation code
transformation (the bulk-operation transform or BOT), a fairly straightforward
lifting of the basic dual-numbers reverse AD algorithm to a mostly first-order
array language, and symbolic interpretation to achieve an end-to-end
compilation pipeline. Unfortunately, we lose some of the nice generalisable
aspects of dual-numbers AD in the process, most importantly support for
higher-order code.
  We do support some higher-order array combinators, but only a
carefully-chosen set: 'build' (elementwise array construction), 'gather' and
'scatter'. In return, the BOT can eliminate the essential (for AD)
higher-orderness of the input program, meaning that AD gets essentially
presented with a first-order program. This allows the naive trick of lifting
dual numbers to "dual arrays" to work without much modification.

</details>


### [2] [Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications](https://arxiv.org/abs/2507.13091)
*Aurèle Barrière,Victor Deng,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 本文首次提出了能机械化验证且忠实于官方规范的现代正则表达式回溯语义模型，并在实际应用中取得突破性进展，包括新型上下文等价验证和PikeVM算法的正式化证明。


<details>
  <summary>Details</summary>
Motivation: 现有的现代正则表达式语言（如JavaScript正则表达式）缺乏完整且经过机械化证明的语义描述，尤其是能忠实反映回溯语义的描述，这给形式验证和应用带来困难。

Method: 设计了一套能够机械化、简明、实用、完整并经过证明忠实性的现代正则表达式回溯语义；通过与ECMAScript规范的逐行嵌入证明其忠实性；同时将所有定义和结果在Rocq证明助理中进行了机械化。

Result: 提出了第一个能机械化证明且忠实于官方语义的现代正则表达式回溯语义。基于此：1）定义了一种上下文等价的新概念，验证了一些已有的正则重写规则的正确与否；2）首次对被广泛应用的PikeVM算法进行了形式化证明。此外，该语义能完整记录所有的回溯结果和优先级。

Conclusion: 本文提出的语义模型不仅理论上可行且经过严格机械化验证，为正则表达式变换、验证及引擎实现提供了坚实基础，推动了正则表达式理论与实践的进步。

Abstract: We present the first mechanized, succinct, practical, complete, and
proven-faithful semantics for a modern regular expression language with
backtracking semantics. We ensure its faithfulness by proving it equivalent to
a preexisting line-by-line embedding of the official ECMAScript specification
of JavaScript regular expressions. We demonstrate its practicality by
presenting two real-world applications. First, a new notion of contextual
equivalence for modern regular expressions, which we use to prove or disprove
rewrites drawn from previous work. Second, the first formal proof of the PikeVM
algorithm used in many real-world engines. In contrast with the specification
and other formalization work, our semantics captures not only the top-priority
match, but a full backtracking tree recording all possible matches and their
respective priority. All our definitions and results have been mechanized in
the Rocq proof assistant.

</details>


### [3] [Towards Formal Verification of LLM-Generated Code from Natural Language Prompts](https://arxiv.org/abs/2507.13290)
*Aaron Councilman,David Fu,Aryan Gupta,Chengxiao Wang,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.PL

TL;DR: 本文提出用形式化查询语言表达用户意图，并用符号解释器自动验证LLM生成代码，有效提升了代码准确性和用户体验。提出并实现的Astrogator系统在实际测试中表现优良。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）在生成代码时经常会产生错误，用户难以发现和修复这些错误。需要一种方法保证LLM生成代码的正确性，特别是为编程知识有限的用户提供更可靠的体验。

Method: 引入一种形式化查询语言，用户可以用接近自然语言的方式表达并确认自己的意图；之后通过该查询对LLM生成的代码进行验证，确保其与用户意图一致。提出并实现了Astrogator系统，其中包括形式化查询语言、表示Ansible程序行为的演算以及用于验证的符号解释器。

Result: 在21个代码生成任务的基准测试中，该验证器能够在83%的情况下验证正确代码，并在92%的情况下识别出错误代码。

Conclusion: 通过结合形式化查询语言和符号验证手段，可以为LLM生成的代码提供更有力的正确性保证，有助于提升AI代码助手的可用性与安全性，尤其对非专业用户更为友好。

Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by
taking natural language descriptions and generating code based on it. However,
LLMs often generate incorrect code that users need to fix and the literature
suggests users often struggle to detect these errors. In this work we seek to
offer formal guarantees of correctness to LLM generated code; such guarantees
could improve the experience of using AI Code Assistants and potentially enable
natural language programming for users with little or no programming knowledge.
To address this challenge we propose to incorporate a formal query language
that can represent a user's intent in a formally defined but natural
language-like manner that a user can confirm matches their intent. Then, using
such a query we propose to verify LLM generated code to ensure it matches the
user's intent. We implement these ideas in our system, Astrogator, for the
Ansible programming language which includes such a formal query language, a
calculus for representing the behavior of Ansible programs, and a symbolic
interpreter which is used for the verification. On a benchmark suite of 21
code-generation tasks, our verifier is able to verify correct code in 83% of
cases and identify incorrect code in 92%.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 本文综述了LLMs在AIOps中的研究进展，系统回顾了相关文献，分析了数据源、任务类型、方法与评估，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，其在AIOps中的应用逐渐增多，但对其作用、潜力和局限性的系统性理解仍不完善，亟需全面综述厘清现状并指明未来方向。

Method: 调研法，系统地分析了2020年至2024年间183篇相关文献，围绕数据源、任务演进、方法应用与评估方式等四个关键研究问题进行梳理。

Result: 发现LLMs推动了AIOps任务、数据源与评估方式的创新，但仍存在研究空白与挑战，未来有待进一步深入探索。

Conclusion: 本文对LLMs在AIOps领域的应用进行了系统性综述，揭示了研究现状、进展与不足，并提出了未来研究的潜力方向。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [5] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本文提出利用大型语言模型（LLMs）自动在不同主流量子SDK之间转译量子程序，解决了传统转译器设计的复杂性和维护难题，提高了量子软件的可移植性和开发效率。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算平台有多种不同的量子软件开发工具包（QSDKs），如Qiskit、Cirq与PennyLane等，导致互通性和跨平台软件开发存在很大挑战。传统的基于规则的转译器设计复杂、耗时且需要深厚专业知识。

Method: 本研究探索利用大型语言模型（LLMs），依靠其预训练知识和上下文推理能力，将LLMs作为与编程语言无关的自动化转译器，实现QSDK间量子程序的自动转换。

Result: 所提出的方法无需手动定义转换规则，实现了量子软件在不同QSDK间的可扩展、自动转译，并保持程序的功能等价性。

Conclusion: 通过用LLMs为量子软件的转译提供智能、通用的方法，本研究推动了量子计算生态系统中软件可移植性的提升。

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [6] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: 本文提出Kodezi Chronos，实现对大规模代码库的全面理解与维护，显著提升调试效率和缺陷检测能力，为自主软件演化铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在代码生成和自动化方面取得进展，但受限于推理时的上下文窗口和缺乏显式代码结构推理能力。本文试图突破这一瓶颈，支持对超长代码上下文（如完整代码库、历史记录和文档）的理解和操作。

Method: 提出了一种新架构Kodezi Chronos，利用多层嵌入记忆引擎，将向量和图索引结合，持续进行基于代码的检索。引入了Multi Random Retrieval基准，专门针对软件工程领域，考查模型跨距离、跨文件溯源与理解复杂代码关系的能力。

Result: Chronos无需固定窗口限制下，实现对百万行代码的高效推理，支持仓库级理解、多文件重构及自愈。其在新基准下，代码缺陷检测比以前LLM提高23%，调试周期缩短40%。

Conclusion: Kodezi Chronos在超长代码理解、缺陷检测和自动运维方面取得突破，推动了自维护、持续优化的软件生态发展。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [7] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文是首篇系统梳理强化学习在软件工程领域应用的综述，通过分析115篇顶会论文，明确了研究现状、挑战及未来方向，对推动该领域发展具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性的增加和自动化需求的上升，研究者希望通过强化学习（RL）提升软件工程（SE）各任务中的智能性和自适应能力。然而，尽管RL在SE领域研究日益增长，仍缺乏对该领域全面而系统的综述。

Method: 作者回顾了自深度强化学习（DRL）以来，在22个顶级软件工程会议发表的115篇同行评审论文，对发表趋势、软件工程主题与RL算法进行分类，分析了数据集、模型设计与优化、评估实践等关键要素，并总结挑战与未来研究方向。

Result: 本文提供了RL在软件工程领域应用的首个系统性综述，并公开了相关研究资料库。

Conclusion: 本文系统梳理了RL在软件工程领域的应用现状、研究趋势、主要挑战与未来方向，为后续研究与实践提供了重要参考。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [8] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 该文提出RAGSum方法，将代码检索与注释生成整合于CodeT5模型，并在多项评测上超过主流基线，有效提升代码自动注释的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成方法在代码注释自动生成任务中，检索和生成过程通常是分开优化的，导致无关信息传播并影响最终生成质量。为提升推荐的有效性与效率，需要更加紧密结合检索与生成过程。

Method: 作者提出了一种新方法RAGSum，它基于CodeT5，将检索和生成过程融合在同一模型中。首先通过对比预训练使代码嵌入适合最近邻检索，然后用包含检索和生成共同损失的方式端到端训练。此外，还引入轻量级自我优化循环以提升最终输出质量。

Result: RAGSum在跨语言（Java、Python、C）的三个基准上进行了实验测试，并与三种主流方法做了对比，实验表明RAGSum无论在BLEU、METEOR还是ROUGE-L评测指标上均有明显提升。

Conclusion: 紧密耦合检索与生成过程可以提升代码自动注释的上限，新的RAGSum方法为注释自动化提供了更加有效的思路，并激发进一步重复和开发者定性研究的兴趣。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [9] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文提出用CodeT5等预训练模型为架构异味推荐重构方法，大规模实验显示显著优于现有方法，并推动自动化重构建议的发展。


<details>
  <summary>Details</summary>
Motivation: 现有检测架构异味（如God Class、循环依赖、枢纽依赖）的工具很少给出如何修复的建议，这阻碍了软件质量和可维护性的提升。

Method: 用预训练transformer模型（CodeBERT、CodeT5）对检测到的架构异味推荐恰当的重构，将该任务表述为三分类问题，并在11149个开源Java项目挖掘出的200多万实例上微调模型。

Result: CodeT5取得96.9%的准确率和95.2%的F1分数，优于CodeBERT和传统方法。

Conclusion: 基于transformer的模型能够有效实现从异味检测到可执行修复建议的桥接，为未来自动化重构推荐系统奠定基础；代码与数据已公开以支持复现与进一步研究。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [10] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 本研究通过强化学习优化大型语言模型，使其在量子编程领域的代码生成显著提升，尽管仍难以解决复杂任务，整体性能已大幅超过主流通用模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM（如Granite-20B-Code和StarCoder）输出的Qiskit量子电路代码存在较多错误，亟需提升其在量子编程中的准确性和可靠性。

Method: 作者将一个32B参数量模型分别使用两种强化学习方法（GRPO和ORPO）在带有丰富注释的合成数据集上进行微调。模型性能在Qiskit HumanEval和原版HumanEval基准测试上进行了评估。

Result: ORPO在Qiskit HumanEval测试中的Pass@1得分为56.29%（比Granite-8B-QK高约10个百分点），GRPO得分为49%，二者均优于通用基础模型。在原版HumanEval上，两者得分分别为65.90%和63.00%。在不同任务难度分布下，GRPO在基础任务更优，ORPO在中级任务更强，均未攻克高级任务。

Conclusion: 经两种RL方法微调的大模型在量子编程任务中显著超越现有通用LLM基准，并在不同任务层级展现优势，但在高级任务面前仍有明显提升空间。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [11] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 该论文提出了一种用于智能电网新信息和数据模型设计阶段的三阶段评估方法，结合显式和隐式评估，有效填补了当前方法的空白，并通过实际应用案例验证了其实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能电网的数字化进程加快，分布式能源系统中自动化信息交换日益增多，现有信息和数据模型难以满足需求，需要开发新的模型。在模型设计时，若不进行充分评估，可能会导致未来应用中出现缺陷甚至重大中断。因此，有必要在设计环节就评估新模型，以保障系统安全与可靠。

Method: 作者基于设计科学研究（design science research）方法，提出并设计了一套三阶段的评估方法。该方法结合了显式与隐式评估手段，结构化地应用于新信息和数据模型的开发过程中，并以工业灵活性描述相关模型的开发为例对该方法进行了精炼。

Result: 作者提出的三阶段评估方法既填补了原有方法只有高层次或只针对已用模型的局限，也适用于新模型开发过程。通过实际开发案例，进一步完善了该方法，并总结了实践经验和获得的启示。

Conclusion: 所提出的评估方法为智能电网领域新信息与数据模型在设计阶段的有效评估提供了明确的步骤和框架，可提升模型的健壮性和适用性，减少潜在的系统中断风险。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [12] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 作者用模糊逻辑方法替代传统Likert量表，更重视对终用户的持续正面影响，从而动态、全面地衡量项目成功，为社会科学领域项目评估提供新的方向。


<details>
  <summary>Details</summary>
Motivation: 现有Likert量表忽视了项目成功的多面性与情境依赖，难以准确捕捉不同项目目标和利益相关方复杂的评价。作者希望利用模糊逻辑的灵活性和动态性，提升评价准确度。

Method: 将模糊逻辑（层级型Type-1 Mamdani模糊系统）集成进现有项目成功评价构架，代替传统Likert量表评价，从多角度和情境相关性出发动态衡量项目成败。

Result: 提出了一种新型模糊系统评价框架，强调将最终用户的持续性正面影响作为最核心指标，并弱化了对外围成果（如利益相关者满意度和内部指标）的依赖。该方法具适应性和普适拓展潜力。

Conclusion: 文章提出用层级型Type-1 Mamdani模糊系统评估项目成功，强调持续正面影响优先于利益相关者满意及内部项目结果。这一方法能更全面、动态反映项目成功，未来将做实证测试和更多社会科学应用。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [13] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 本文提出了Single Conversation Methodology（SCM），通过贯穿全流程的持续对话，提升开发与大语言模型协作的软件开发的结构和主动性，强调开发者作用，提升开发过程和结果质量。


<details>
  <summary>Details</summary>
Motivation: 当前对生成式人工智能（如大型语言模型）的软件开发交互，大多采用零散、临时的方式，缺乏系统性和可追溯性。为纠正对LLMs的被动依赖，重新强调开发者的主导作用，提出更科学的开发流程。

Method: 提出了Single Conversation Methodology（SCM）：即在整个软件开发的全流程中，采用一场持续且有结构的对话，涵盖需求、架构、实现等阶段，确保开发过程连贯、可追溯，并注重模块化和文档化。详细定义了其阶段、最佳实践和理念。

Result: SCM理论能够提升开发过程的认知清晰度、可追溯性、结构化和文档完善程度，使开发者能够更主动有效地引导与LLMs的协作。

Conclusion: SCM为当前以LLMs为工具的软件开发提供了结构化的改进方向，减少被动依赖，强调开发者的主导架构与监督地位，有望提升开发效率和质量。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [14] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 本研究证实小型语言模型（如Phi-4等）可高效自动检测手动测试用例中的测试异味，准确率高且无需手动定义规则，具备提升测试质量和保护隐私的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 手动测试对于发现自动化难以捕捉的问题仍然非常重要，但手动测试用例常常存在诸如歧义、冗余或检查项缺失等测试异味问题，影响测试的可靠性和可维护性。现有的检测工具多依赖手动规则，难以扩展。

Method: 本研究评估了三种小型语言模型（Gemma3、Llama3.2和Phi-4）在自动检测测试异味方面的能力。研究对象为143个真实世界的Ubuntu测试用例，涵盖7种测试异味类型。通过比较各模型在检测测试异味句子上的表现。

Result: Phi-4模型在检测测试异味方面表现最佳，pass@2准确率达到97%，Gemma3和Llama3.2约为91%。此外，这些SLM模型不仅能进行检测，还能够自主解释问题并提出改进建议，即使没有明确的提示指令。这些模型不依赖大量规则或句法分析，成本低且能保护数据隐私。

Conclusion: 小型语言模型（SLMs）在自动检测手动测试用例的测试异味上显示出高效且具可扩展性的优势，可显著提升真实场景下的测试质量。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [15] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出iReDev框架，融合多智能体与人机协作机制，实现了高效、智能化的需求开发，在实验中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前软件开发中的需求开发需要大量人力和时间，且依赖于多方利益相关者的协作，易出现冲突。现有多智能体系统在需求开发阶段的支持有限，尤其忽视了人类知识注入和人机协作。

Method: 提出了iReDev——一个基于知识驱动的多智能体框架，用于智能化需求开发。iReDev拥有六个知识驱动代理，协作完成需求开发全部流程，并通过事件驱动的artifact pool进行高效通讯，且引入human-in-the-loop机制，强化人机协作和人类知识注入。

Result: iReDev生成的需求工件在多项指标上优于已有基线方法，展现出在人机协同和智能化需求开发方面的突出性能。

Conclusion: iReDev框架有效提升了需求开发的智能化和协作效率，强化了人类知识的注入及人机协同，具有广阔的应用前景。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [16] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 预训练大模型让软件系统需求工程面临全新挑战，本文提出了适应性框架和未来研究方向，为研究与实践指明了道路。


<details>
  <summary>Details</summary>
Motivation: 预训练大模型在现代软件系统中的广泛应用使得传统的软件需求工程方法面临挑战，现有方法难以应对模型的不确定性和演化特性。因此，亟需重新思考和调整需求工程方法以适应新环境。

Method: 本文提出了一个面向预训练模型驱动软件系统的需求工程概念框架，并据此框架梳理了若干有前景的研究方向。

Result: 概念框架有助于指引研究者和实际工作者解决预训练模型驱动系统需求工程所面临的新兴挑战。

Conclusion: 现有需求工程方法难以适用于预训练模型支持的系统，需基于新的理论框架和研究方向进行创新以应对现存及未来挑战。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [17] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 本论文提出方法，可从递归下降解析器中自动提取属性文法，既恢复语法也重建输入处理语义，实现更全面的软件输入规范恢复。实验结果表明生成的属性文法能准确重现原程序行为。


<details>
  <summary>Details</summary>
Motivation: 许多处理结构化输入的软件系统缺乏完整和最新的输入规范，仅有语法层面的恢复，语义处理尚未深入研究。

Method: 提出了一种新颖方法，通过动态分析递归下降解析器的实现，观察程序执行，将运行时行为映射到输入语法，从而重建输入处理中的语义，实现属性文法规格的自动推导。

Result: 该方法能够系统性地提取和嵌入语义操作到语法规则中。实验在一组程序上展示了可行性，并能通过生成的属性文法准确重现原程序行为。

Conclusion: 本文的方法不仅恢复了输入的语法结构，还系统性地捕获了输入处理的语义，有助于获得更完整的软件输入规范。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [18] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 该论文针对LLM生成代码检测中的现实挑战，提出了能抵抗代码微调的CodeGPTSensor+，通过对抗性训练和新型样本生成方法，大幅提高了检测鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在代码生成任务中的广泛应用，随之而来的是代码溯源、版权及代码质量等问题，对检测和规范生成代码的需求日益迫切。由于实际应用中LLMs生成的代码常常被人为微调，当前检测方法在处理经过修改的代码时鲁棒性不足，亟需更有效的解决方案。

Method: 提出了CodeGPTSensor+，在原有CodeGPTSensor基础上增强鲁棒性。其核心是采用对抗性训练方法，并引入了MIST（Multi-objective Identifier and Structure Transformation）对抗样本生成模块，可系统性地生成高质量、代表性的对抗样本，从而提升模型对于各种对抗性代码修改的抵抗力。

Result: 在HMCorp数据集上的实验表明，CodeGPTSensor+在对抗性测试集上检测准确率有显著提升，并且在原始测试集上仍保持高准确率，鲁棒性超过原始的CodeGPTSensor。

Conclusion: CodeGPTSensor+有效提升了检测LLMs生成代码的鲁棒性，尤其在代码被微调后依旧表现出更高的检测准确率，适合实际应用场景。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [Dependency Pairs for Expected Innermost Runtime Complexity and Strong Almost-Sure Termination of Probabilistic Term Rewriting](https://arxiv.org/abs/2507.12918)
*Jan-Christoph Kassing,Leon Spitzer,Jürgen Giesl*

Main category: cs.LO

TL;DR: 本文首次提出并实现了针对概率项重写系统的依赖对框架，能够实现其期望复杂性分析和强几乎必然终止性证明，并在工具AProVE上实验验证了其方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管依赖对（DP）框架已被用于概率项重写系统（PTRSs）的几乎必然终止性证明，但对PTRSs的自动复杂性分析仍未被深入研究。

Method: 作者提出了首个用于分析期望复杂性和证明概率项重写系统内层重写下的正或强几乎必然终止性的依赖对框架。该方法已经在AProVE工具中实现。

Result: 新DP框架可分析PTRSs的期望复杂性，并用于证明正或强几乎必然终止性。工具AProVE实验显示此方法较现有方法更具优势。

Conclusion: 本研究首次实现了对概率项重写系统自动复杂性分析以及强几乎必然终止性的有效证明。

Abstract: The dependency pair (DP) framework is one of the most powerful techniques for
automatic termination and complexity analysis of term rewrite systems. While
DPs were extended to prove almost-sure termination of probabilistic term
rewrite systems (PTRSs), automatic complexity analysis for PTRSs is largely
unexplored. We introduce the first DP framework for analyzing expected
complexity and for proving positive or strong almost-sure termination (SAST) of
innermost rewriting with PTRSs, i.e., finite expected runtime. We implemented
our framework in the tool AProVE and demonstrate its power compared to existing
techniques for proving SAST.

</details>


### [20] [Cyclic proof theory of positive inductive definitions](https://arxiv.org/abs/2507.13057)
*Gianluca Curzi,Lukas Melgaard*

Main category: cs.LO

TL;DR: 本文证明了在以正归纳定义扩展的皮亚诺算术（μPA）系统中，环状证明和归纳证明的证明论强度一致。作者利用带注释变体和第二阶算术片段形式化论证，并证明了即使加强了有效性条件，仍能证明相同的定理。该工作拓展了利用二阶算术片段进行算术理论非良基证明论分析的研究。


<details>
  <summary>Details</summary>
Motivation: 该论文关注于μPA系统，这是通过正归纳定义扩展的皮亚诺算术，等价于二阶算术中的一个不自守子系统（Π^1_2-CA_0）。这种系统的表达能力强，可以研究复杂的算数原理。主要动机在于分析环状证明显式与归纳证明确实和否相等，即证明系统之间的证明论强度是否一致。

Method: 作者将环状证明翻译为带注释的变体（基于Sprenger和Dam提出的一阶μ-演算系统），利用其更强的有效性条件，简化正确性证明。随后在Π^1_2-CA_0系统内部形式化上述论证，并利用Möllefeld的保守性结果。此外，还借助Curzi和Das关于Knaster-Tarski定理逆向数学的前期工作。

Result: 结果证明了μPA中的环状证明系统与归纳证明系统具有相同的证明论强度。采用的带注释变体虽然有效性条件更强，但它们与普通的环状证明能够证明相同的定理。

Conclusion: 该研究表明，针对μPA系统，无论采用环状还是归纳证明，其证明能力是等价的。本工作推进了针对算术理论采用不良基（non-wellfounded）证明和二阶算术片段的证明论分析。这一研究方向由Simpson的Cyclic Arithmetic和后续Das与Melgaard的工作开启和推进。

Abstract: We study cyclic proof systems for $\mu\mathsf{PA}$, an extension of Peano
arithmetic by positive inductive definitions that is arithmetically equivalent
to the (impredicative) subsystem of second-order arithmetic
$\Pi^1_2$-$\mathsf{CA}_0$ by M\"{o}llefeld. The main result of this paper is
that cyclic and inductive $\mu\mathsf{PA}$ have the same proof-theoretic
strength. First, we translate cyclic proofs into an annotated variant based on
Sprenger and Dam's systems for first-order $\mu$-calculus, whose stronger
validity condition allows for a simpler proof of soundness. We then formalise
this argument within $\Pi^1_2$-$\mathsf{CA}_0$, leveraging M\"{o}llerfeld's
conservativity properties. To this end, we build on prior work by Curzi and Das
on the reverse mathematics of the Knaster-Tarski theorem. As a byproduct of our
proof methods we show that, despite the stronger validity condition, annotated
and "plain" cyclic proofs for $\mu\mathsf{PA}$ prove the same theorems. This
work represents a further step in the non-wellfounded proof-theoretic analysis
of theories of arithmetic via impredicative fragments of second-order
arithmetic, an approach initiated by Simpson's Cyclic Arithmetic, and continued
by Das and Melgaard in the context of arithmetical inductive definitions.

</details>


### [21] [Monotone weak distributive laws over the lifted powerset monad in categories of algebras](https://arxiv.org/abs/2507.13058)
*Quentin Aristote*

Main category: cs.LO

TL;DR: 本论文探讨集合及紧致豪斯多夫空间上多层非确定性结构中，单调弱分配律能否通过弱提升自动获得，并刻画了其存在的具体条件与限制。


<details>
  <summary>Details</summary>
Motivation: 注意到在集合与紧致豪斯多夫空间上，结合两层非确定性的方法在结构上存在相似性，因此提出问题：后者的分配律能否由前者自动获得？进而推广到更一般的代数范畴中。

Method: 主要通过范畴论和单调弱分配律的理论分析，首先对集合与紧致豪斯多夫空间上的两层非确定性结构进行比较，然后对弱分配律的提升可能性进行系统判别和刻画，理论上构造相应反例和存在性证明。

Result: 部分情况下，可以实现弱提升；具体在紧致豪斯多夫空间中，作者展现了结合概率和非确定性的分配律的具体例子。而在其他很多代数范畴中，这种分配律则无法存在。

Conclusion: 研究得出了在某些具体情况下，可以通过弱提升的方法，将集合上的单调弱分配律扩展到紧致豪斯多夫空间上。然而，这种方法无法推广到其他代数范畴。作者进一步刻画了在何种代数范畴下，可以存在幂集幺半群上的单调弱分配律，并指出很多情况下并不存在这样的律。

Abstract: Noticing the similarity between the monotone weak distributive laws combining
two layers of nondeterminism in sets and in compact Hausdorff spaces, we study
whether the latter law can be obtained automatically as a weak lifting of the
former. This holds partially, but does not generalize to other categories of
algebras: we then characterize when exactly monotone weak distributive laws
over powerset monads in categories of algebras exist, exhibiting a law
combining probabilities and non-determinism in compact Hausdorff spaces and
showing on the other hand that such laws do not exist in a lot of other cases.

</details>


### [22] [Impact and Performance of Randomized Test-Generation using Prolog](https://arxiv.org/abs/2507.13178)
*Marcus Gelderie,Maximilian Luff,Maximilian Peltzer*

Main category: cs.LO

TL;DR: 本文针对用于系统测试用例生成的Prolog程序，探索了两种不同的随机化策略，并从理论和实验角度分析了它们的表现，有助于提升测试的覆盖率与生成效率。


<details>
  <summary>Details</summary>
Motivation: 测试系统时，经常需要生成复杂且具有逻辑相关性的测试序列。由于可能的测试用例集合极大甚至无限，如何有效随机生成这些测试用例成为一个实际难题。

Method: 本文提出了两种将随机化方法整合进Prolog测试生成程序的策略：一种基于标准Prolog语义，另一种通过改变SLD选择函数。此外，文章用马尔科夫链框架分析到达某测试用例的平均时间及平均生成测试用例数，并进行了实证比较。

Result: 通过理论分析与实验证明，这两种随机化策略在生成测试用例方面具有效果，并对比各自的性能表现。

Conclusion: 将随机化引入Prolog的测试生成过程中，能够提升覆盖大量或无限测试空间的能力。两种策略各有优劣，理论与实证结果可为今后同类问题提供参考。

Abstract: We study randomized generation of sequences of test-inputs to a system using
Prolog. Prolog is a natural fit to generate test-sequences that have complex
logical inter-dependent structure. To counter the problems posed by a large (or
infinite) set of possible tests, randomization is a natural choice. We study
the impact that randomization in conjunction with SLD resolution have on the
test performance. To this end, this paper proposes two strategies to add
randomization to a test-generating program. One strategy works on top of
standard Prolog semantics, whereas the other alters the SLD selection function.
We analyze the mean time to reach a test-case, and the mean number of generated
test-cases in the framework of Markov chains. Finally, we provide an additional
empirical evaluation and comparison between both approaches. Under
consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [23] [Just Verification of Mutual Exclusion Algorithms](https://arxiv.org/abs/2507.13198)
*Rob van Glabbeek,Bas Luttik,Myrthe Spronck*

Main category: cs.LO

TL;DR: 本研究通过模型检测探讨多种互斥算法在不同寄存器模型下的正确性，发现并展示了若干算法的不足并给出改进思路。


<details>
  <summary>Details</summary>
Motivation: 当前存在多种互斥算法，这些算法主要通过共享读/写寄存器实现通信，且寄存器可为原子或非原子。同时，验证这些算法的正确性（尤其是活性性质）具有挑战，容易受到伪反例的干扰。

Method: 采用模型检测技术来检验多种互斥算法的正确性，分析算法在不同共享寄存器（原子和非原子）上的表现。为了有效验证活性性质，引入了公正性假设（justness）作为完备性标准，并探讨了多种并发关系以模拟不同的寄存器工作假设。

Result: 识别出多种互斥算法在 correctness properties 上存在缺陷的执行路径，并对部分算法提出了改进建议。

Conclusion: 利用公正性作为完备性标准，可以更合理地在不同并发关系下通过模型检测验证互斥算法的正确性，并为算法完善提供指导。

Abstract: We verify the correctness of a variety of mutual exclusion algorithms through
model checking. We look at algorithms where communication is via shared
read/write registers, where those registers can be atomic or non-atomic. For
the verification of liveness properties, it is necessary to assume a
completeness criterion to eliminate spurious counterexamples. We use justness
as completeness criterion. Justness depends on a concurrency relation; we
consider several such relations, modelling different assumptions on the working
of the shared registers. We present executions demonstrating the violation of
correctness properties by several algorithms, and in some cases suggest
improvements.

</details>


### [24] [Solving SAT By Computing A Stable Set Of Points In Clusters](https://arxiv.org/abs/2507.13282)
*Eugene Goldberg*

Main category: cs.LO

TL;DR: 作者提出了一种分簇并行计算稳定点集(SSP)的新方法，提高了处理大规模CNF不可满足问題的效率，为并行SAT求解提供了新思路。


<details>
  <summary>Details</summary>
Motivation: SSP对于描述CNF公式不可满足性有理论和实际意义，但现实中SSP往往极大，逐点计算效率低。需要开发更高效的计算方法。

Method: 本文提出了将稳定点集(SSP)分成若干簇(cluster)来同时处理每个簇的方法，以此克服直接逐点计算SSP在规模大时不可行的问题。

Result: 提出的按簇计算方法，使得更高效地计算大规模CNF问题的SSP成为可能，同时也为并行SAT求解创造了条件。

Conclusion: 通过按簇(cluster)而不是逐点的方式计算SSP，可以更有效地处理SSP，而且有助于发掘更加高效的SAT算法，支持并行计算。

Abstract: Earlier we introduced the notion of a stable set of points (SSP). We proved
that a CNF formula is unsatisfiable iff there is a set of points (i.e. complete
assignments) that is stable with respect to this formula. Experiments showed
that SSPs for CNF formulas of practical interest are very large. So computing
an SSP for a CNF formula point by point is, in general, infeasible. In this
report, we show how an SSP can be computed in clusters, each cluster being a
large set of points that are processed simultaneously. The appeal of computing
SSPs is twofold. First, it allows one to better take into account formula
structure and hence, arguably, design more efficient SAT algorithms. Second,
SAT solving by SSPs facilitates parallel computing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [25] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 本文提出结合语言模型与概率程序的综合推理架构，显著提升了对新颖情境下人类类推理的模拟能力，可用于更好地理解与复现人类开放域推理。


<details>
  <summary>Details</summary>
Motivation: 人类在新颖情境下能从广泛的知识中提取相关信息并用以推理和预测，但其背后的机制仍不清楚。本文旨在探讨人类如何整合全球相关信息并进行连贯推理。

Method: 本文提出了“模型综合架构（MSA）”，结合了分布式和符号表示，运用语言模型实现相关信息检索与模型综合，并通过概率程序构建专属、连贯的世界模型。作者在“模型奥林匹克”运动场景推理数据集上评估该方法，比较它与仅用语言模型的基线方法在模拟人类判断上的表现。

Result: MSA在模拟人类推理判断方面效果优于仅使用语言模型的基线方法，无论在直接生成还是链式思维生成设定下均表现更佳。

Conclusion: MSA能够像人类一样，针对新颖变量和环境进行局部连贯且全球相关的推理，这为理解和复现人类的开放性推理能力提供了新路径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [26] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 该研究表明，语言模型内部存在有效区分模态类别的线性表征，且这种能力随模型规模增强，是可以利用机械可解释性方法研究并与人类模态分类行为相关联的，推翻了以往关于模型不能良好区分模态的观点，对理解模型与人类之间的认知机制有启示意义。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）被广泛应用于多种任务，如问答和写作。要想可靠地完成这些任务，模型需能够区分句子的模态类型（可能、不可能、荒谬等）。但最近的研究质疑LMs按模态对句子分类的能力。该论文旨在探究模型模态判别能力的本质。

Method: 作者利用机械可解释性方法，识别并分析了一类可以区分模态类别（模态差分向量）的线性表征，并研究这些向量在不同训练阶段、模型层数及参数规模下的变化和作用。还结合人类行为实验，相关分析投影结果与人的可解释特征评分。

Result: 研究发现，LM内部存在着可以有效辨别不同模态类别的向量，这些向量随模型能力提升会有规律性地产生。利用这些向量，机器能够模拟人类的细粒度模态分类判断，并与人类模态判断特征有显著相关性。

Conclusion: LM具备比先前认为更强的模态区分能力，且该能力随着模型能力提升有规律性地出现。这些发现可加深对模型内部表示与人类模态认知的理解。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [27] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 作者提出了第一个车臣语与俄语间的开源翻译模型，并发布了配套平行语料资源和多语种句子编码器，推动了对濒危车臣语的技术支持。


<details>
  <summary>Details</summary>
Motivation: 车臣语是一种濒危语言，缺乏高质量的机器翻译模型。当前开源多语种翻译系统大多未覆盖车臣语，因此需要为车臣语与俄语间的翻译提供有效工具。

Method: 作者收集并构建了车臣语和俄语之间的平行语料，通过微调多语种翻译大模型NLLB-200，使其支持车臣语。最终，他们训练并评估了第一个开源车俄机器翻译模型，并分发了相关语料及面向车臣语的多语种句子编码器。

Result: 模型在车俄翻译任务上取得了BLEU分数8.34和ChrF++分数34.69，俄车翻译方向的BLEU为20.89，ChrF++为44.55。此外，作者还发布了相关的语料资源和句子编码器。

Conclusion: 本论文首次提出了开源的车臣语与俄语互译模型，并公开配套语料和工具，为车臣语的研究与保护提供了重要基础。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [28] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 针对美国毒品过量死亡监测延误问题，本文基于死亡报告文本，评测并优化了多类NLP方法。微调后的BioClinicalBERT模型实现近乎完美的分类准确率，显著优于传统与通用方法，可极大提升药物相关死亡数据的提取与实时分析能力。


<details>
  <summary>Details</summary>
Motivation: 美国因芬太尼等毒品致死率快速上升，而当前依靠人工ICD-10编码处理自由文本调查报告，延误严重且信息流失。急需自动化、精准的文本分析工具辅助毒品过量死亡监测。

Method: 收集了来自多个美国司法辖区的35433条2020年死亡记录用于模型训练与内部测试，外部验证采用2023-2024年独立的3335条数据。分别评估了传统分类器、fine-tuned BERT/BioClinicalBERT（编码器-only），以及当代decoder-only大模型（Qwen 3、Llama 3）在未结构化文本分类上的性能，以macro F1为核心评价指标。

Result: 微调的BioClinicalBERT模型在内部测试集上macro F1高达0.998以上，外部验证macro F1为0.966，全面优于传统方法与其他大模型。证明了其稳定、准确和良好的领域泛化能力。

Conclusion: 论文证明了基于NLP的、特别是BioClinicalBERT等临床领域语言模型在药物过量死亡文本判别方面有极高准确率，可显著提升美国毒品致死相关的实时监测能力，优于传统机器学习及通用大模型。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [29] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: 本文提出了AdaptiSent跨模态自适应注意力框架，有效融合文本和图像信息，显著提升了多模态情感分析与方面抽取的准确性，在多个基准数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方面情感分析（MABSA）方法在利用文本和图像融合特征时，对情境的适应性不足，难以准确分析两种模态间的复杂关系。作者试图通过更高效的跨模态关注机制提升情感分类及方面术语抽取的准确性。

Method: 提出AdaptiSent框架，使用自适应跨模态注意力机制，结合动态模态加权和情境自适应注意，动态调整模型关注点，深度融合文本和图像信息，从而更好抽取情感及方面特征。

Result: 在标准Twitter多模态数据集上，AdaptiSent在精确率、召回率和F1分数等指标上显著优于传统文本模型及其他多模态方法，尤其擅长捕捉细腻的跨模态关系。

Conclusion: AdaptiSent框架大幅提升了多模态方面情感分析的表现，尤其是在理解和融合复杂多模态信息方面，显著超越了现有方法，为MABSA领域树立了新标准。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [30] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: 提出利用大型音频模型作为统一语音评测工具，通过多维集成提升与人类偏好的一致性，相关性最高达0.91，展示了较强鲁棒性，同时需解决啰嗦与位置偏差等缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有语音评测需为不同特征设计专门系统，自动化方法与人偏好相关性较差，故需研究统一、贴近人类偏好的自动音频评价方法。

Method: 采用AudioJudge系统，结合多种prompt工程策略（如音频拼接、上下文学习），并引入多维度集成评测，将语音评测任务细分为不同子评判员，用以检测发音、语速、说话人识别、语音质量等特征，并进行系统级人类偏好仿真。

Result: 提出的AudioJudge系统在音频特征检测和人类偏好仿真任务上表现出色，多维度集成方法在系统排序基准上与人类偏好的Spearman相关性高达0.91。模型对噪音有较强鲁棒性，但有啰嗦和位置偏差问题。

Conclusion: 大型音频模型（LAM）能够作为统一评测框架，有效提高语音评价与人类偏好相关性，但仍存在啰嗦和位置偏差等需改进之处。

Abstract: Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [31] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出FLEXITOKENS，一种具备可学习分词器且训练目标更灵活的字节级语言模型，有效解决现有模型分词器适应性差和分词碎片化严重的难题。实验表明方法提升了分词效率和下游任务最高10%的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在迁移到新数据分布时，简单微调效果有限，主要因为子词分词器的结构在适应过程中没有改变。这种不灵活性导致对于分布外数据、未知语言或文字的分词效率低下，容易出现过度分割问题。

Method: 本文提出了一种具备可学习分词器的字节级语言模型，使分词方式能够自适应。具体方法为模型内置一个学习预测字节序列边界的子模块，以可变长度片段进行编码。与已有分词器无关、需借助辅助损失强制固定压缩率的方式不同，作者提出了更为简化、灵活的训练目标FLEXITOKENS。

Result: 在多语言、多形态任务及领域的基准测试中，FLEXITOKENS能够持续减少分词过度碎片化现象，并且下游任务绩效最多提升到10%。

Conclusion: FLEXITOKENS以简单灵活的训练目标，实现了语言模型分词器在适应性和下游任务表现上的明显改进，超越了主流子词和梯度式分词器。

Abstract: Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [32] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出的TransEvalnia系统，基于大模型推理进行细致翻译评估和排名，在多个数据集上优于现有SOTA方案，实现了高一致性、可解释性和开源，为机器翻译评估领域提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 当前的机器翻译评估系统存在准确性、细粒度和主观评分一致性等问题，现有系统如MT-Ranker在多个维度上仍有提升空间。本文旨在开发一个利用推理能力提升翻译自动评估效果的新系统。

Method: 提出了TransEvalnia系统，基于Prompt的自动评估和排序，采用了多维质量评价框架(MQM)子集，通过大语言模型（例如Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct）进行细致的评测和评分，并对翻译排序中的位置偏置进行了分析和修正方法研究。

Result: TransEvalnia在作者自行构建的英日数据集及WMT多语言任务数据上，整体表现与或优于最先进的MT-Ranker，且自动评分与人工评分高度相关。提出并验证了缓解位置偏置的措施，所有数据和代码均已开源。

Conclusion: TransEvalnia系统不仅细粒度上优化了翻译质量评估和排名，还有良好的主观及客观一致性，并有效应对了排序中位置偏见的新挑战。该系统为自动翻译质量评价和排序提供了更为可靠的工具。

Abstract: We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [33] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种能根据游戏进程和玩家态度动态切换策略的狼人杀智能体，并通过对比实验证明该方法优于传统的固定或隐式策略方法。


<details>
  <summary>Details</summary>
Motivation: 之前的狼人杀智能体主要利用 prompt engineering，策略是隐式的，难以适应游戏中的不断变化。这引发了对能根据玩家态度和对话上下文灵活切换策略方法的需求。

Method: 提出一种新方法：根据游戏上下文和对其他玩家角色的估算，显式选择并切换预设策略，用以增强狼人杀智能体的表现。

Result: 通过与基线方法（使用隐式或固定策略的智能体）对比，实验验证了该方法的有效性。

Conclusion: 显式基于游戏情境和角色估算调整狼人杀智能体决策策略，能较好地适应环境变化，提高博弈表现。

Abstract: This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [34] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: 本研究提出无需额外训练即可显著提升大模型长链推理能力的解码方法ThinkLogit及其优化版本，实验上在多个数学推理任务中取得了显著性能提升，具备高效实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（LRMs）具备复杂推理能力，但通常需要额外训练才能激发其长链推理能力。这项工作关注于是否可以在无需任何额外训练的情况下激发模型的这种能力，从而降低推理成本并提升模型实用性。

Method: 提出了一种在解码时激发长链推理能力的新方法ThinkLogit。该方法利用logits算术，在解码阶段用一个显著更小的模型作为引导者，调整目标大模型以实现长链推理。进一步地，提出了ThinkLogit-DPO，通过偏好优化训练引导模型，优化来自目标大模型和引导模型的正确与错误推理对。

Result: 实验表明，在使用R1-Distill-Qwen-1.5B（仅约目标大模型1/21规模）作为引导时，ThinkLogit方法在四个数学数据集上使Qwen2.5-32B模型的pass@1指标提升了26%，ThinkLogit-DPO提升了29%。此外，证明ThinkLogit可以迁移通过强化学习获得的长链推理能力，使得pass@1相对提升13%。

Conclusion: 本文提出的ThinkLogit及其增强方法可在无需或极少额外训练的情况下高效激发大模型的长链推理能力，大幅提升推理效果并降低计算资源消耗。

Abstract: Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [35] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

TL;DR: Synergy是一种端到端字节级语言模型，摆脱了分词器依赖，在性能不减的前提下表现出更高灵活性和更少token数，验证了无分词器方案的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理不同抽象层级之间的结合时存在不足，尤其是低层级如字节级建模通常依赖外部分词器，灵活性和泛化能力受限。

Method: 提出Synergy模型，通过学习路由机制，实现端到端跨抽象层建模。模型以字节为输入，无需传统的分词器，训练为字节级语言模型，并通过对比实验与Llama3进行性能评估。

Result: Synergy模型能够自动学习将字节映射为概念token，生成的token数量少于BBPE分词器，性能可比。与Llama3同等规模下，Synergy表现出优势。中间层移除位置编码后性能提升，显示其能自发学到无位置依赖的概念。

Conclusion: Synergy模型验证了无分词器架构的可行性，为更健壮、灵活的语言模型管线奠定了基础。

Abstract: In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [36] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出了一种基于对比学习和否定句数据蒸馏的方法，高效提升了BERT类文本编码器及LLM的否定理解能力，兼顾常规文本嵌入表现。


<details>
  <summary>Details</summary>
Motivation: 本文指出，尽管自回归大型语言模型（LLM）被广泛采用，但体积较小的文本编码器在需要丰富上下文表征的文本理解任务中依然重要。然而，现有方法在捕捉否定信息方面存在不足，影响了许多下游依赖文本嵌入的任务。

Method: 作者提出了一种提高文本编码器对否定的鲁棒性的策略：利用大型语言模型，通过多样的否定和模糊表达模式进行数据蒸馏。随后，采用标准的对比学习方法，对强大的BERT模型进行微调。

Result: 通过该方法，BERT类模型在否定理解方面有了显著提升，同时在常规基准上仍保持有竞争力的表现。此外，这一策略同样可以被适配到更大的LLM中，也能有效提升否定相关基准测试的表现。

Conclusion: 提出的利用多样否定与模糊表达数据蒸馏，并通过对比学习微调的方案，有效提升了文本编码器和LLM对否定理解的能力，且兼顾了整体性能。

Abstract: Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [37] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

TL;DR: 这篇论文通过让大语言模型根据文本生成MIDI音乐，并用生成数据训练AI进行分类、补全等测试，发现LLM能够学习到一些基础音乐结构，但因缺乏音乐背景，仍有明显局限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在自然语言及其他符号领域（如编程、数学）表现突出，但其对符号音乐的建模能力尚未充分探索。本文旨在研究LLM如何隐式地表征音乐概念。

Method: 通过从描述不同风格和类型组合的文本提示生成符号音乐（MIDI文件），无需专门音乐训练，并用这些LLM生成的数据集训练神经网络，完成音乐体裁和风格分类、旋律补全等任务，与已有方法进行对比。

Result: 实验表明，LLM能够从文本中推断出基础的音乐结构和时间关系，部分地隐式编码音乐模式，但其生成能力受限于缺乏明确音乐背景知识。

Conclusion: LLM具备一定生成符号音乐的潜力，能部分捕捉音乐模式，但也有显著不足，需进一步完善其音乐领域应用。

Abstract: Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [38] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文系统分析多语言模型事实知识的一致性问题，发现当前模型在跨语言一致性上存在瓶颈，并验证code-switching训练和词对齐目标有助于提升一致性和多语性能。


<details>
  <summary>Details</summary>
Motivation: 跨语言一致性对评估模型迁移能力、保持多语言事实知识和保障模型表现一致性至关重要，然而现有研究少有分析模型在不同语言间知识一致性的工作。

Method: 通过分析混合使用不同语言的指代句（code-mixed coreferential statements），并借助多种可解释性方法，研究并解释多语言模型在事实知识跨语言上下文中的表现和一致性。同时评估提升多语言性能的常用策略是否也能改善知识一致性。

Result: 发现多语言模型的一致性水平随语言家族、语言学因素以及特定层的瓶颈而变化。主流提升多语性能的方法未必提升知识一致性，而code-switching训练和跨语言词对齐目标对提升知识一致性和多语性能均表现突出。

Conclusion: cross-lingual一致性不仅对多语模型性能提升有重要作用，对知识保持具有实际意义。建议在多语言模型训练中重视跨语言对齐监督和code-switching训练，以进一步提升一致性和性能。

Abstract: Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [39] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 本文提出将解码器结构由单层扩展为多层分层解码，通过微调实现，显著提升文本生成与分类相关任务性能。


<details>
  <summary>Details</summary>
Motivation: 受到人类分层思维能力的启发，希望推动解码器结构由单一层转向多层，使不同层次能同时参与文本生成，以增强模型多样化表达与推理能力。

Method: 采用预训练的语言模型，将最后一层的解码头复制到特定中间层，并针对不同任务输入进行微调，从而实现分层解码结构。

Result: 通过实验验证，中间层被适配后能够产生有意义且合理的内容，在分层文本分类、分类引导生成与分层文本生成等多任务上取得SOTA水平表现。

Conclusion: 本研究表明，将解码器仅限于最后一层的做法可以被打破，通过构建分层解码结构，可有效提升多任务上的表现，并为从零预训练的通用分层推理模型提供了可能性。

Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [40] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 本论文提出以大语言模型为核心，实现多步骤、代码驱动的西班牙语表格问答解法，取得85%准确率，流程高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在解决西班牙语表格问答（PRESTA任务）中的问题，推动表格理解和自动化问答方法在西班牙语环境下的发展。

Method: 该方法采用大语言模型（LLM）驱动的Python代码生成，通过分析表格内容、选择关键列、生成自然语言指令、将其转化为代码并执行，以及错误处理，全面解决问答任务。每一步骤均使用开源LLM并针对性优化提示词。

Result: 该方案在PRESTA任务中取得了85%的准确率，显示出高效的表格理解和问答能力。

Conclusion: 使用LLM进行分步代码生成和表格处理为西班牙语表格问答提供了有效方案，表现出了较高的准确率，并且流程具有可拓展性和高自动化潜力。

Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [41] [Spectral Moment of Order Four and the Uniqueness of the CCZ class of Dublin APN Permutation](https://arxiv.org/abs/2507.12853)
*Valérie Gillot,Philippe Langevin,Abdoulaye Lo*

Main category: cs.DM

TL;DR: 提出了基于6位布尔函数分类的6比特APN函数搜索新方法，并取得了新的成果。


<details>
  <summary>Details</summary>
Motivation: 研究APN函数在密码学中的重要性，因此通过6位布尔函数的分类，探索构造6比特APN函数的新方法。

Method: 基于6位布尔函数的分类，采用新的构造和分析方法来搜索6比特的APN函数。

Result: 获得了搜索6比特APN函数的新结果，提出了一些新的构造方法。

Conclusion: 本文为6比特APN函数的发现提供了新的思路和结果，丰富了该领域的知识体系。

Abstract: The note provides new apparoaches and results for the search of 6-bit
APN-functions based on the classification of 6-bits Boolean functions.

</details>
