<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)](https://arxiv.org/abs/2511.11055)
*Michael Schwarz,Julian Erhard*

Main category: cs.PL

TL;DR: 论文提出用摘要（digests）扩展静态分析中的数据竞争检测，并在实际工具Goblint上验证，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 静态分析可以通过证明不存在数据竞争来提升程序安全性和可靠性，但如何高效地检测数据竞争仍是挑战。论文基于摘要（digests）概念，旨在提升静态分析的数据竞争检测能力。

Method: 将摘要（digests）用于记录并发冲突发生与否的条件。在线程模块化的局部追踪语义下，形式化数据竞争定义，并将排除潜在冲突的条件用摘要表达。实现于静态分析工具Goblint，并在SV-COMP基准测试集上进行实验评估。

Result: 结合锁集摘要（lockset digest）、线程ID和线程合并摘要比仅用锁集方法，正确解决任务数量提升了5倍以上。

Conclusion: 摘要驱动的数据竞争检测方法显著提升了静态分析工具的准确性和效率，尤其在复杂并发场景下相比传统锁集方法大幅提高检测能力。

Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.

</details>


### [2] [Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation](https://arxiv.org/abs/2511.11070)
*Sangho Lim,Hyoungjin Lim,Wonyeol Lee,Xavier Rival,Hongseok Yang*

Main category: cs.PL

TL;DR: 本文提出了一个自动化、健壮的概率程序循环矢量化方法，通过推测并行执行与定点检查保证正确性的同时获得显著加速，在Pyro平台全自动测试下优于现有矢量化基线。


<details>
  <summary>Details</summary>
Motivation: 概率编程推理的高昂计算成本很大程度上源于对大数据集或长随机样本序列的循环操作。手工矢量化易出错，自动方法受限于通用性。急需一种通用且健壮的自动矢量化技术来提升模型高效性和实用性。

Method: 将概率编程语言（PPL）的循环用定点检查（fixed-point check）确保原语义的前提下，自动转译成面向矢量化的低级语言，并借助推测并行执行提升计算效率。方法在Pyro PPL中实现，并在多种概率模型上进行了实证评估。

Result: 新方法实现了1.1-6倍提速，在GPU显存占用上普遍减少，并能够适配所有测试模型，超过了现有方案仅能处理有限模型的适用性。

Conclusion: 提出的方法能无缝自动矢量化概率程序中的循环，在提升推理性能的同时扩展适用范围，优于现有方法。

Abstract: Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models.

</details>


### [3] [Kleene Algebra](https://arxiv.org/abs/2511.11264)
*Tobias Kappé,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: 本文介绍了克里尼代数以及其在建模程序等价性中的应用，通过正则表达式与自动机对应，以及KA定律证明表达式等价性的方法，为程序分析提供了理论基础，并配有练习与余代数视角补充，加深了理解。


<details>
  <summary>Details</summary>
Motivation: 介绍克里尼代数（KA）作为研究程序之间一般等价性的一套法则，阐述其基础理论及应用。

Method: 采用正则表达式建模一般程序，并探讨正则表达式与自动机之间的对应关系，同时通过具体章节练习和可选余代数视角加深理解。

Result: 证明了正则表达式等价性当且仅当能通过KA定律证明的核心结果，在理论上加深了对程序等价性的理解。

Conclusion: 克里尼代数提供了研究正则表达式等价性和程序行为的重要工具，对自动机理论和程序等价性分析具有重要意义。

Abstract: This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra.

</details>


### [4] [The Jasmin Compiler Preserves Cryptographic Security](https://arxiv.org/abs/2511.11292)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Benjamin Grégoire,Vincent Laporte,Paolo Torrini*

Main category: cs.PL

TL;DR: 本文扩展Jasmin编译器的证明体系，引入关系Hoare逻辑和交互树语义，首次形式化证明编译器可以保留密码安全属性（如IND-CCA），显著提升密码实现的安全性保障。


<details>
  <summary>Details</summary>
Motivation: Jasmin编译器虽然在功能正确性方面有形式化证明，但此证明无法覆盖密码学中常见的非终止和概率计算。作者希望增强编译器对于密码安全性的形式化保障。

Method: 提出基于关系Hoare逻辑的方法，针对编译器正确性进行证明，并与一种基于交互树的新Jasmin程序指称语义配合使用。进一步用该逻辑证明未修改的Jasmin编译器在上述语义下的功能正确性，最后形式化密码学安全性（如IND-CCA），用交互树证明编译器可保留安全性。

Result: 在Rocq证明器中，证明Jasmin编译器前端（30个pass中的25个）能保留密码学安全性，并用新的逻辑和语义描述方法得到上述结果。

Conclusion: Jasmin编译器经过新的证明框架后，不仅功能正确，其前端还能在形式化意义上保留密码学安全性，为开发高效且安全的密码实现奠定基础。

Abstract: Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.
  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究基于问卷调查，探讨科研软件工程师对同行代码审查的观点、实践与挑战，发现改进流程、工具和培训有助于推动科研软件领域更好地采纳同行审查，进而提升软件质量和可维护性。


<details>
  <summary>Details</summary>
Motivation: 研究软件对于科研发现和数据分析具有关键作用，但由于需求变化、复杂输入和遗留依赖等问题，导致软件质量和可维护性受限。虽然同行代码审查能提升软件质量，但其在科研软件工程师（RSEs）中的应用尚未被深入探究。

Method: 通过针对RSEs的问卷调查，收集他们对同行代码审查的看法。问卷内容参考既有研究，并专门针对RSEs进行了补充和调整，以便进行对比分析。

Result: 共收集到61份有效问卷。结果显示RSEs的挑战与更广泛开发者群体存在共性，也展现出独特的实践和难点。

Conclusion: 同行代码审查对于提升科研软件的质量、可维护性和可靠性至关重要。通过结构化流程、改进工具和有针对性的培训，可有效促进同行审查在科研软件开发中的应用。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [6] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出用LLM生成评估准则，人类修订后让LLM自动评判修复补丁，有效降低人工成本，评判准确性与人工共识高度一致，但在人类意见分歧时仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前自动程序修复评价依赖执行结果（如unit test pass@k），无法判断补丁的真实有效性。而人工标注成本高，因此需要更低成本且可靠的补丁有效性判断方法。

Method: 提出了一种人机协作式LLM辅助修复补丁有效性评估流程：由LLM生成针对每个bug的评估准则（rubric），人类对准则进行一次性审核及微调，随后利用LLM按准则自动判断补丁有效性。该方法被应用于谷歌sanitizer工具发现的补丁集。

Result: 在人工一致性较高的数据上，该方法的评判结果与3人人工共识具有较高的一致性（Cohen's kappa 0.75），召回率0.94，精度0.80。在全部补丁数据集（含有分歧的情况）上，一致性略低（kappa 0.57），召回率0.93，精度0.65。同时为未来改进方向做了探讨。

Conclusion: 基于人机协作的评价机制在自动程序修复领域可以有效降低人工注释的成本，且在多数情况下能较好地与人工共识对齐。但在存在人工分歧的数据上，模型表现尚有提升空间。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [7] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 本文提出结合LLM和一致性检测的方法，用于软件运行时控制流异常检测。实验表明，在铁路控制系统案例中，该方法具备高覆盖度与检测精度。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统由于其复杂性，确保其高可靠性变得愈发困难，传统设计时验证（V&V）无法覆盖所有运行时“未知的未知”异常。

Method: 本文提出通过大型语言模型（LLMs）结合一致性检测来开发控制流异常检测的软件监控器。方法包括利用LLM，以模型链接设计与实现代码，实现自动代码插装；随后利用一致性检测技术分析事件日志，从而发现运行时控制流偏离。

Result: 在欧洲铁路交通管理系统（ERTMS/ETCS）案例中，LLM实现的代码插装可实现84.775%的控制流覆盖度，一致性检测的异常检测F1值高达96.610%，AUC达93.515%。

Conclusion: 融合领域知识指导LLM进行自动插装，大幅提升了软件日志的可靠性与质量，为后续高效一致性检测提供了基础，增强了控制流异常发现能力。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [8] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 本文系统分析了主流LLM代码代理在修复复杂多位置漏洞中的表现，发现准确率与bug复杂度相关，性能好的代理更具语义一致性且更少引入新错误。引入Maple模块后，修复能力明显增强。除准确率外，该工作强调了修复行为细节及资源消耗是理解和改进自动程序修复的关键。


<details>
  <summary>Details</summary>
Motivation: 自动化程序修复领域长期以来主要集中在修复单个位置的缺陷，对真实系统中常见的多位置（multi-hunk）漏洞关注不足。修复这些分散在多个不相连代码区域的漏洞极具挑战性，促使作者系统性研究现有大模型（LLM）驱动的代码代理在此类任务上的表现。

Method: 作者对四个主流LLM代码代理（Claude Code、Codex、Gemini-cli、Qwen Code）在Hunk4J数据集中372个multi-hunk bugs进行修复实验，分析了1488个修复过程。采用了包括定位、修复准确率、回归行为和运作机制等精细度量标准，还开发了Maple模块为代理提供仓库级上下文，并衡量其效果。

Result: 不同代理在修复准确率上差异显著：Claude Code达93.3%，Qwen Code仅25.8%。随着bug分散度和复杂度增加，准确率下降。性能较好的代理在语义一致性和回归减缓上表现优异，而低性能代理易引入新测试失败。修复失败消耗更多算力资源和运行时间（分别多39%-343%的token和43%-427%的时间）。Maple模块将Gemini-cli修复准确率提升了30%。

Conclusion: LLM代码代理在multi-hunk bug修复上表现迥异，精细化分析揭示了它们定位、推理和执行过程的优劣。仓库级上下文（Maple）能显著提升定位和修复效果。仅关注准确率不足，需关注其行动策略和资源消耗。

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [9] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 企业在不投入重大定制训练资源的情况下，可通过few-shot prompting方法在本地平台上解决专有领域特定语言的简单问题，实现LLM实用化并保障数据安全。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来大量研究关注LLM在主流通用编程语言上的应用，但针对工业流程自动化领域中的高度专用语言，相关探索尚不充分。这些专用语言通常仅在专有环境中使用，现有LLM支持有限。

Method: 通过对企业自身能力的调研与实验，考察无需大规模定制训练情况下，采用few-shot prompting方法在专用领域特定语言上的表现。强调在本地部署（on-premise）下进行实验以保护企业敏感数据。

Result: 实验结果显示，在LLM并不支持的专有领域特定语言下，few-shot prompting足以解决简单问题，且企业可以在本地完成，无需投入大量定制化训练。

Conclusion: 即使在LLM对某些专有领域语言支持有限的情况下，通过few-shot prompting与本地部署，企业可实用且高效地应用LLM，保护数据安全且降低模型定制门槛。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [10] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文发布了SQuaD，一个基于9种分析工具、覆盖450个知名开源项目和7百余项指标的大型、多维软件质量数据集，显著推动了质量评估、缺陷分析等领域的实证研究和方法创新。


<details>
  <summary>Details</summary>
Motivation: 现有的软件质量数据集大多只关注单一维度（如代码异味、技术债务、重构活动），难以支持跨时间、跨质量维度的综合分析，因此亟需一个多维、时序化的数据集以弥补研究空白。

Method: 作者构建了SQuaD数据集，利用9款主流静态分析工具，从450个主流开源项目中自动化提取方法、类、文件和项目层级的700多种软件质量度量，同时整合了版本控制、缺陷跟踪、漏洞数据和过程指标。

Result: SQuaD涵盖63,586个项目版本，统一收集了多维度的软件质量和过程数据，支持维护性、技术债、演化和质量评估等大规模实证研究，并为持续的软件分析演进提供基础。

Conclusion: SQuaD数据集极大丰富了现有软件质量研究的数据资源，推动了质量建模和缺陷预测等方向，同时对自动化更新和跨项目质量建模等新兴课题做出展望。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [11] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: 本文针对智能合约复用组件在实际业务逻辑中的违规使用问题，提出自动检测系统SCRUTINEER，融合特征提取与大语言模型，实现高准确率检测，为智能合约安全提供有力保障。


<details>
  <summary>Details</summary>
Motivation: 智能合约可复用组件（SCRs）虽然促进了模块化和代码复用，但在实际业务逻辑中使用时可能发生逻辑级别的使用违规，即遵守组件规则但未满足特定业务需求，带来安全隐患。检测此类违规需要深入理解合约业务逻辑与隐含用法模式。

Method: 提出了SCRUTINEER系统，用于自动检测SCR的逻辑级别使用违规。1）设计了复合特征提取方法，生成三类互补特征表示；2）引入了由大语言模型驱动的知识构建框架，通过提示和领域工具抽取逻辑用法并构造知识库；3）研发了检索增强生成（RAG）检测器，结合检索策略和分层分析以定位潜在风险；4）实现了集成相似性检查与快照推理冲突检查的违规分析引擎。

Result: 在3个真实数据集上进行多角度评估，SCRUTINEER在检测逻辑级使用违规方面实现了80.77%的精度、82.35%的召回率和81.55%的F1-score。

Conclusion: SCRUTINEER是首个自动且实用的逻辑级SCR违规检测系统，能有效提升智能合约业务逻辑层面的安全性。其综合特征提取和语言模型驱动的分析方法实现了高效准确的违规识别。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [12] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 论文针对航空等高规范领域敏捷软件开发的认证难题，提出并验证了自动化管理和变更追踪工具CertiA360，结果显示该工具有效提高开发效率并合规，实现了敏捷与认证要求融合的可能性。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等安全关键领域，软件开发需要遵循严格的合规和认证标准（如DO-178C），而敏捷开发方法强调灵活性和对需求变更的快速响应，两者在实际应用中存在落差。该论文试图解决敏捷开发与认证标准之间的冲突与融合难题。

Method: 提出并开发了CertiA360工具，整合敏捷方法的灵活性，通过自动化管理变更请求、提升需求成熟度和变更可追溯性，确保开发过程符合航空认证要求。工具设计和验证过程中密切结合行业专家反馈，确保实际适用性和有效性。

Result: CertiA360自动化工具能减少手动工作量，提高对变更的响应效率，并帮助团队在遵守DO-178C标准的前提下更高效地进行软件开发。尽管当前工具尚未获得DO-330工具资格，其应用前景和行业专家反馈表明敏捷开发方法经过适当调整可与高规范领域的安全认证要求共存并提升效率。

Conclusion: 敏捷开发和航空安全认证要求之间可通过自动化工具有效融合，在保障认证合规性的前提下，实现高效灵活的软件开发流程。CertiA360为解决敏捷开发与DO-178C等高标准认证之间的冲突提供了有效途径。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Knowledge Reasoning Involving Four Types of Syllogisms](https://arxiv.org/abs/2511.10916)
*Long Wei,Liheng Hao*

Main category: cs.LO

TL;DR: 该文形式化证明了含“most”和“all”量词的广义三段论有效性，并系统推导了多种有效三段论及其嵌套推理的判断方法，对自然语言处理与三段论理论研究具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探讨包含‘most’和‘all’等限制量词的非平凡广义三段论推理在知识推理中的有效性与话语推理方法。

Method: 提出这些三段论的知识表示方法，并对广义三段论AMI-1进行了形式化证明。通过演绎推理，从有效的AMI-1广义三段论分别推出19个非平凡广义三段论、22个有效广义模态三段论、8个有效经典三段论和24个有效经典模态三段论。此外，讨论了这四类三段论嵌套的话语推理有效性的判断方法。

Result: 系统地归纳和推导了多种有效的三段论类型，并分析了多种形式与结构下嵌套推理的有效性判断方法。

Conclusion: 这些形式化推导不仅为英语信息处理提供了理论基础，也为研究其他三段论系统提供了方法论启示。

Abstract: This paper studies the validity and discourse reasoning of non-trivial generalized syllogisms involving the quantifiers in Square{most} and Square{all} from the perspective of knowledge reasoning. Firstly, this paper presents knowledge representations for these syllogisms and formally proves the validity of generalized syllogism AMI-1. Subsequently, 19 non-trivial generalized syllogisms, 22 non-trivial valid generalized modal syllogisms, 8 valid classical syllogisms, and 24 valid classical modal syllogisms are respectively deduced from the valid generalized syllogism AMI-1 on the basis of deductive reasoning. Additionally, this paper discusses how to judge the validity of discourse reasoning nested by the above four types of syllogisms, which have four types of figures and different forms. In conclusion, such formal deductions not only provide a theoretical foundation for English language information processing, but also provide methodological insights for studying other syllogistic systems.

</details>


### [14] [Arity hierarchies for quantifiers closed under partial polymorphisms](https://arxiv.org/abs/2511.11326)
*Anuj Dawar,Lauri Hella,Benedikt Pago*

Main category: cs.LO

TL;DR: 本论文深入探讨了广义量词在部分多态性约束下的表达能力，通过新颖的代数构造和量词游戏方法，系统性建立了表达能力的层级与分离结果，有效回答了此前工作的开放问题并推动了理论计算机科学中逻辑与代数的交叉研究。


<details>
  <summary>Details</summary>
Motivation: 论文旨在研究由部分多态性条件定义的广义量词的表达能力，这一条件源自于对约束满足问题（CSP）的研究。研究动机在于回答Dawar和Hella（CSL 2024）工作引发的一系列问题，尤其关注多态性与量词的元数之间的关系。

Method: 文章主要采用新的代数构造方法，借鉴了Cai-Fürer-Immerman风格，并结合了Dawar和Hella提出的量词卵石游戏来证明分离和不可表达性结果。

Result: （1）对于在部分近一致多态性下封闭的量词，建立了一个关于多态性和量词元数之间互相影响的层级关系，具体来说，封闭于$l$元部分近一致多态性的$(l+1)$元量词，其表达能力严格介于所有元数为$l-1$与$l$的量词类之间。（2）在量词元数不变而多态性元数固定的情况下，建立了一个无限分层结果。（3）对于带有部分Maltsev多态性的量词，证明了其不可表达性结果。

Conclusion: 论文澄清了在部分多态性约束下广义量词的表达能力层级，回答了相关公开问题，并通过新方法获得了分离和不可表达性的重要理论结果。

Abstract: We investigate the expressive power of generalized quantifiers closed under partial polymorphism conditions motivated by the study of constraint satisfaction problems. We answer a number of questions arising from the work of Dawar and Hella (CSL 2024) where such quantifiers were introduced. For quantifiers closed under partial near-unanimity polymorphisms, we establish hierarchy results clarifying the interplay between the arity of the polymorphisms and of the quantifiers: The expressive power of $(\ell+1)$-ary quantifiers closed under $\ell$-ary partial near-unanimity polymorphisms is strictly between the class of all quantifiers of arity $\ell-1$ and $\ell$. We also establish an infinite hierarchy based on the arity of quantifiers with a fixed arity of partial near-unanimity polymorphisms. Finally, we prove inexpressiveness results for quantifiers with a partial Maltsev polymorphism. The separation results are proved using novel algebraic constructions in the style of Cai-Fürer-Immerman and the quantifier pebble games of Dawar and Hella (2024).

</details>


### [15] [Universal Safety Controllers with Learned Prophecies](https://arxiv.org/abs/2511.11390)
*Bernd Finkbeiner,Niklas Metzger,Satya Prakash Nayak,Anne-Kathrin Schmuck*

Main category: cs.LO

TL;DR: 该文通过学习方法近似推理USC的安全prophecies，用易懂的CTL公式代替复杂的树自动机，不仅提升了效率和可扩展性，还确保了对新系统的泛化能力和人类可读性。


<details>
  <summary>Details</summary>
Motivation: Universal Safety Controllers（USCs）提供一种通用的逻辑控制框架，能够保证任意可实现的系统满足给定的时序安全规范，同时具备比传统方法更强的泛化能力和可扩展性。现有方法在精确计算和验证“预言”（prophecies）时面临高计算复杂度，因此需要一种更高效的方法。

Method: 本文提出了一种USC综合的近似算法，采用学习方法规避了直接计算精确prophecies的复杂性。具体做法是通过从有限的例子系统中，推理和学习计算树逻辑（CTL）公式，作为prophecies的简洁表达。该算法仅计算过度与不足近似，并通过一个验证步骤确保USC能泛化到未见的新系统。

Result: 实验结果表明，学到的prophecies不仅能够泛化到新的系统，而且比基于树自动机的精确表示更简洁、更易于解释，提升了效率和可解释性。

Conclusion: 本文提出的基于学习的USC近似算法实现了prophecy表达的高效、可解释以及良好的泛化能力，成功突破了传统方法在计算复杂性上的局限性。

Abstract: \emph{Universal Safety Controllers (USCs)} are a promising logical control framework that guarantees the satisfaction of a given temporal safety specification when applied to any realizable plant model. Unlike traditional methods, which synthesize one logical controller over a given detailed plant model, USC synthesis constructs a \emph{generic controller} whose outputs are conditioned by plant behavior, called \emph{prophecies}. Thereby, USCs offer strong generalization and scalability benefits over classical logical controllers. However, the exact computation and verification of prophecies remain computationally challenging. In this paper, we introduce an approximation algorithm for USC synthesis that addresses these limitations via learning. Instead of computing exact prophecies, which reason about sets of trees via automata, we only compute under- and over-approximations from (small) example plants and infer computation tree logic (CTL) formulas as representations of prophecies. The resulting USC generalizes to unseen plants via a verification step and offers improved efficiency and explainability through small and concise CTL prophecies, which remain human-readable and interpretable. Experimental results demonstrate that our learned prophecies remain generalizable, yet are significantly more compact and interpretable than their exact tree automata representations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 本文针对大语言模型Agentic应用在执行中出现的隐藏循环问题，提出了结合结构与语义的无监督检测方法，在真实应用中效果显著，但方法仍需进一步完善。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的Agentic应用具有非确定性行为，可能形成隐藏的执行循环，导致资源被悄然消耗而不触发明确的错误。传统可观测性平台难以检测这种高成本低效问题。

Method: 提出了一个无监督循环检测框架，结合结构分析和语义分析。首先采用高效的时序调用栈分析识别显式循环，再通过语义相似性分析发现由冗余内容生成导致的微妙循环。

Result: 在LangGraph驱动的股票市场应用的1575条轨迹上评估，混合方法F1得分为0.72（准确率0.62，召回率0.86），显著优于单独结构方法（F1：0.08）和语义方法（F1：0.28）。

Conclusion: 混合结构与语义分析的无监督循环检测方法有效提升了Agentic应用隐性循环的检测能力，但仍有改进空间，需进一步研究以克服现有限制。

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [17] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 本研究提出一种基于LLM和多轮交互的军事仿真分析自动化方案，通过任务拆解、系统自检和多模板适配，显著提升报告质量和评分，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统手动分析军事仿真数据效率低，并易受人为错误影响。为提高报告的准确性和效率，亟需自动化和智能化方式。

Method: 将复杂分析任务分解为若干子任务，对每个子任务设计系统和用户提示，采用多轮与LLM（大语言模型）交互，并引入自检与反思机制，实现结构化数据提取与多步分析。同时，定义定制工具用于生成图形和计算指标，针对不同场景和数据类型设计多种报告模板。

Result: 本方法生成的分析报告质量更高，在多项评价中得分显著优于基线方法，显示出较强的实用性和适应性。

Conclusion: 利用多轮、大语言模型交互以及任务拆解和模板定制，可以显著提升军事仿真分析报告的自动化质量和应用广度。

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [18] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 本文提出一种高效并行结构化记忆检索架构，显著提升历史人物对话的深度与响应速度，在小型模型上尤为有效，并能支持丰富的可视化分析工具，适用于教育、博物馆和研究等领域。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在模拟历史人物对话时，简单的检索增强生成方法回复深度不足，而多阶段反思虽可提升深度，却牺牲了响应速度。如何兼顾高质量回复与效率成为核心挑战。

Method: 提出一种新的架构，通过离线数据增强和高效结构化记忆的并行检索解决上述矛盾。将传记数据转化为1774条富含情感-语义元数据的第一人称记忆，再通过两阶段检索，实现平均0.52秒的prompt生成。

Result: 评估显示新方法在GPT-4与传统RAG表现相当，在小模型(GPT-3.5、GPT-3)显著优于传统RAG，特别适用于资源受限场景。此外，结构化记忆支持新型可视化工具如时空热图、情感轨迹分析与交互路径追踪。以梵高为案例，架构具备广泛适用性。

Conclusion: 该研究提出的系统既可用于高效的历史人物对话界面，也能作为生动的传记分析与教育工具，在保证效率的同时提升信息深度与应用拓展性。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [19] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: 本文展示了量子-经典混合模型首次成功应用于大规模生成式语言模型，通过在Transformer中引入VQC，实现了更高效的参数替换和接近传统性能，为量子计算在自然语言生成领域提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 目前量子计算越来越多地用于替代传统计算，但大多数量子或混合模型仍然只应用于简单任务，尚未成功应用于大规模自然语言生成。

Method: 提出首个用于自然语言生成的大型混合量子-经典语言模型HyQuT，将变分量子电路（VQC）集成至Transformer架构中，并在8M和150M参数规模下进行实验证明效果。

Result: 实验证明，在150M参数模型中，使用10个量子比特和80个量子门即可替换约10%的经典参数，且在收敛稳定性和生成质量上可与传统模型媲美。

Conclusion: 首次展示了将量子计算引入大规模生成式语言模型的可行性，为后续相关研究提供了重要基础。

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [20] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 工具增强型语言模型虽然用工具能得更高分，但其推理过程变得更不可靠、更缺乏逻辑性，容易只相信工具输出却忽略合理推理。本文提出了工具诱发近视（TIM）的概念，并用实验证明此问题普遍存在，还给出一种优化方法提升模型整体推理表现。


<details>
  <summary>Details</summary>
Motivation: 虽然借助工具增强的语言模型（TaLMs）能够调用外部工具解决更多问题，但目前还不清楚这些工具的使用是否带来了真正值得信赖的推理能力。作者关注于代码解释器工具，探索其对模型推理质量的具体影响。

Method: 以PYMATH竞赛级数学题为基准数据集（1679个题目），分析TaLM在调用Python代码解决问题时的推理表现；开发多维评测体系，对比分析TaLM和未使用工具的LLM在推理过程中的表现；提出基于偏好优化的对齐框架，引导模型正确使用工具来辅助推理。

Result: TaLM在最终答案准确率方面较非工具模型最高提升19.3个百分点，但其整体推理能力明显下降，在推理对比中被非工具LLM超越（最高达41.5%）；工具使用频率越高，推理连贯性越差。工具的使用将错误类型从算数错误转向逻辑、假设等全局性推理失误，“工具诱发近视”现象在高风险案例中占比约55%。偏好优化框架能改善模型在用工具情况下的答案准确率和推理深度。

Conclusion: 单纯依赖工具会导致语言模型推理质量下降，出现“工具诱发近视（TIM）”问题。引入基于偏好优化的训练框架可缓解这一问题，提升答题准确率和推理品质。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [21] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 主流大模型在时间约束检测任务中不可靠，仅靠微调难以根治，若无符号推理等结构扩展，部署于时效敏感系统存在重大风险。


<details>
  <summary>Details</summary>
Motivation: 大模型在需要实时决策和时间约束的智能体系统中被广泛部署，但尚未验证其对动作期限（时间窗口）的判断能力，若此能力不可靠，将带来应用层面的重大风险。

Method: 通过设计deadline detection任务，对8个生产级大模型（2.8-8B参数）进行测试和分析，包括准确率、格式敏感性和模型容量关系。同时，尝试用少量合成数据进行微调评估能力提升，还分析了能力短板的根本原因。

Result: 1）性能呈现两极分化，只有部分模型接近95%准确率，多数仅50%；2）对提示格式极其敏感，准确率可波动30-60个百分点；3）能力与参数量无关，部分模型远超同参数段的其他模型；4）微调可提升部分模型表现，但根本性短板依然存在；5）归因于架构缺失连续时间表征与显式约束判断能力。

Conclusion: 当前的自回归LLM架构无法可靠处理时间约束任务。仅用自然语言预测和微调不能充分提升其在时间窗口判断等决策型应用中的能力，需引入额外的结构化或符号推理机制以保障时效性和决策正确性。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [22] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 本工作扩展了Spectral NSR推理框架，在推理前引入三项语义增强：节点冗余去除、边蕴含验证和知识补全，带来准确性提升、泛化增强和噪声降低，保留了高效可解释的谱推理核心，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 为提升谱神经符号推理（Spectral NSR）系统的推理准确性、泛化能力以及在开放领域应用中的鲁棒性。

Method: 提出三项语义增强：1）基于transformer的节点合并（使用Sentence-BERT、SimCSE等上下文嵌入）以减少冗余；2）基于NLI预训练模型（如RoBERTa、DeBERTa）的句级蕴含验证提升边质量；3）与外部知识图谱（ConceptNet、Wikidata等）对齐，补充缺失语境。所有改进为谱推理阶段前的预处理，并不改变核心推理流程。

Result: 在ProofWriter、EntailmentBank、CLUTRR等基准上，准确率提升（最高+3.8%），对对抗样本泛化能力提升，推理噪声减少。

Conclusion: 通过模块化、语义驱动的预处理，显著提升了谱神经符号推理系统的图结构质量，实现更鲁棒、可解释、可扩展的推理，适用于开放域实际部署。

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [23] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 提出了一种利用自动机遍历历史提升结构化生成多样性的方法，有效拓展了LLM在多样化内容与结构生成上的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化生成方法虽然能够保证输出的有效性，但在多样性方面存在严重不足。多样性的缺失影响了自动化测试、数据生成等实际应用的覆盖能力。作者为了解决这一问题，提出新的生成策略。

Method: 利用自动机（automaton）遍历历史，引导大语言模型生成新的结构化模式，提高内容和结构的多样性。并通过具体案例（生成测试用例）进行了实证验证。

Result: 该方法在多样性（结构与内容）上取得显著提升，并在效率方面与传统方法保持一致。在开源库测试用例生成的案例中，方法展现了良好效果。

Conclusion: 文中提出的方法能够显著提升LLM在结构化输出中的结构和内容多样性，同时保证生成效率与原有方法持平。

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [24] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 提出PRO框架，通过自动推断prompt的偏好权重，显著提升多目标对齐的效率和效果，免去用户繁琐的权重指定，理论和实验均证实其优越性。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型虽然在多种自然语言处理任务中表现出色，但在实际应用中，让模型同时满足多目标且符合不同用户偏好依然很困难。现有多目标对齐方法主要依赖用户手动指定偏好权重，既增加了用户负担，也导致训练效率低下。

Method: 提出了一个新的框架——PReference Orchestrator（PRO），它通过一个轻量级的偏好适配器自动推断、调整每个prompt对应的偏好权重。具体做法是：利用多个奖励模型对优选响应的归一化奖励分数进行训练，从而自动学习不同prompt的合适偏好权重。

Result: 理论分析证明，针对prompt动态调整偏好权重在多目标对齐问题上优于固定偏好权重。大量实验也验证了该方法在多任务场景中的有效性，优于现有多目标对齐方法。

Conclusion: 通过PRO框架，可以在无需用户手动指定偏好权重的情况下，实现大型语言模型在多目标对齐上的更高效、优异的性能表现。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [25] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 该文提出利用专利文档不同部分作为多视图进行对比学习，自监督获得更优专利嵌入，效果优于依赖外部标签的方法，为专利智能分析提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有的SimCSE 风格 dropout 增强方法在专利嵌入学习中，会导致嵌入过度均匀，从而丧失语义一致性，与专利文本自身的结构特征不符。

Method: 提出基于专利文档不同部分（如摘要、权利要求、背景等）进行对比学习，将这些部分作为多视图，从而利用它们之间的语义和结构多样性进行自监督训练。

Result: 在大规模基准测试中，该全自监督的方法在先前技术检索和分类任务中，与依赖引文和IPC标签监督的主流方法相比表现持平甚至更好，而且无需依赖容易出错或不完整的人工标注。

Conclusion: 利用专利文档内部的不同部分作为多视图进行对比学习，能够获得更具全局结构和局部连续性的嵌入，有效提升专利检索和分类性能，具有很高可扩展性和泛化性。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [26] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 该研究系统评估了多种开源LLM，在多机构、多语种临床报告结构化中的能力，并指出中小型模型已能实现高效提取，合适的提示方式显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 临床自由文本数据结构化需求强烈，但此前研究多集中于单一任务、有限模型或仅支持英文。因此，需要系统评估多种开源LLM在多病种、多机构、多语言环境下的结构化信息提取能力。

Method: 作者评估了15种开源的大型语言模型（LLM），涵盖通用和医学专用模型，通过六种不同的prompting方法（零样本、单样本、少样本、思维链、自洽性和prompt graph）提取结构化临床信息。评估任务涉及六类疾病以及来自荷兰、英国、捷克三个机构的病理和放射报告。采用任务相关的衡量指标和统计分析方法，比较不同模型和prompt方法的表现。

Result: 表现最佳的模型在所有任务上取得了接近人工注释者一致性水平的得分。中小型通用LLM性能与大型模型相当，极小和专用模型效果较差。prompt graph和few-shot提示方法可提升约13%的性能。影响结果的因素以任务特异性（如变量复杂性、标注差异）为主，而非模型规模或提示方法。

Conclusion: 开源LLM可以在多病种、多语言、多机构的真实临床环境下，有效提取结构化数据。模型选择上，中小通用模型已经足够，且合理prompt设计尤为重要。方法为大规模、低成本临床数据整理提供了可行途径。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [27] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: 文章提出了一种利用LLM处理政府多页财政文档表格数据的创新方法，通过层级关系实现高效、准确的数据抽取与验证，显著优于传统OCR方案，具备广泛推展前景。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在文本理解方面表现突出，但对于复杂的、多层级结构的表格数据处理能力仍然不足。特别是在处理政府财政等多页、结构化表格的实际文档时，传统OCR方法很难保证数值的准确性和有效校验。

Method: 提出了一种基于LLM的多阶段管道方法，结合领域知识、连续上下文和算法验证，用于从政府财政多页文档中提取结构化数据。利用财政表格的层级结构，通过多级总和实现数据提取后的内部校验，从而提升信息抽取准确性。

Result: 在印度Karnataka邦的200多页年度财政文档上实证，所提出方法表现出高准确率。实验证明LLM不仅可读表格，还能理解文档特有的层级结构，将PDF版财政披露高效转换为可研究数据库。该方案具备在更多发展中国家财政文档处理中应用的潜力。

Conclusion: LLM能够胜任复杂层级财政表格数据的结构化抽取和自动校验，为PDF财政信息高质量、可扩展地转换成研究数据库提供了可行方法，并有望推广至更广泛的发展中地区。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [28] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 该文提出了一种结合传统压缩器和神经语言模型的新框架（wPoE），无需微调即可显著提升文本压缩效果，有效解决神经压缩器对未见数据适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统压缩算法（如gzip）虽然速度快、计算成本低且适用面广，但其压缩率往往不如现代神经压缩器。神经压缩器依赖于大规模训练数据来建模数据分布，压缩率更好，但泛化性差，面对未见数据时表现有限。本文旨在解决神经压缩器泛化差的问题。

Method: 提出了一种新的测试时引导压缩方法，名为加权专家乘积（wPoE）框架。在推理阶段，方法自适应地将传统通用压缩模型与预训练神经语言模型进行组合，从而确保压缩率至少能达到最好单一模型的水平，无需额外微调，也可与任何自回归语言模型无缝集成。

Result: 广泛实验证明，该方法在文本压缩任务中取得了优于单一模型的表现，提高了压缩效率，无需微调，并且可以适配不同类型的数据分布，增强了神经压缩器的实用性和泛化能力。

Conclusion: 通过wPoE框架自适应结合通用压缩和神经压缩模型，解决了神经压缩器泛化性差的问题，有效提升了文本压缩性能，是提升不同数据分布下压缩效果的实用方案。

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [29] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文针对现有大语言模型（LLM）评估缺乏不确定性量化的问题，提出用贝叶斯方法对二元评估指标进行统计不确定性建模，并在拒绝有害内容和模型偏好评测两个案例验证有效性，为理解与优化LLM行为提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）文本生成系统常会产生有害输出或对对抗性输入过于敏感，模型评估多依赖于人工设计的基准输入，但现有评估方法常忽略了统计上的不确定性量化。为解决这个问题，作者希望建立更科学的不确定性度量方法。

Method: 作者介绍了LLM文本生成与评估背景，提出了一种基于贝叶斯方法对二元评估指标进行不确定性量化的方法，重点分析了由于生成策略的概率性所带来的不确定性。并在两个场景（有害输出拒绝率、模型偏好比较）下做了案例研究。

Result: 通过案例研究，作者展示了贝叶斯方法能更合理地量化LLM系统表现的不确定性，提升了评估的科学性与解释力。

Conclusion: 贝叶斯不确定性量化方法对LLM评估体系是一种有益补充，可增强对模型行为的理解力，有助于未来LLM健康、可靠的发展。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [30] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: LLM在低资源语言和复杂语言挑战下表现参差，大模型优于开源小模型，但文化和语言特性仍待提升，公开新基准数据促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在英语等高资源语言中表现出色，但它们在低资源、形态丰富语言中的效果尚未充分研究。论文旨在评估最新的LLMs在粤语、日语和土耳其语等低资源语言上的跨语言性能，推进相关技术发展。

Method: 构建包含粤语、日语和土耳其语的全新跨语言基准，通过开放域问答、文档摘要、英译多语和文化相关对话四项任务，结合人工评估（流畅度、事实准确性、文化适宜性）与自动化指标（BLEU、ROUGE）对七种主流LLM进行全面分析。

Result: GPT-4o、GPT-4和Claude 3.5等专有大模型整体表现领先，GPT-4o多语任务能力突出，Claude 3.5知识推理能力强；但所有模型在文化理解和语言形态泛化方面仍有明显短板，尤其面对土耳其黏着语和粤语口语时表现不佳。开源小模型在流畅度和准确性上与大模型差距显著。

Conclusion: 当前最先进LLM在多语和低资源语言上已有进步，但在语言多样性和文化适应性上仍存在挑战。论文基准和数据的公开有助于促进 reproducibility 和未来相关研究发展。

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [31] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 本文提出了一种用于守卫模型的自监督训练框架，利用同义改写集提升模型对语义变体的鲁棒性，并通过创新的聚合方法显著减少安全分数波动和提升准确率，强调语义一致性训练的重要性及其与模型校准的内在联系。


<details>
  <summary>Details</summary>
Motivation: 守卫模型对于大语言模型（LLM）的安全性至关重要，但现有模型对表层语言变化的敏感性（即便语义不变的同义改写也会导致安全分数大幅波动）暴露出其缺乏语义基础。

Method: 提出了一个实用的自监督框架，利用同义改写集，通过创新的偏态感知聚合策略，对守卫模型的预测进行强一致性训练，提升语义稳健性；并分析了常用聚合方法（如均值、中位数）的不足。

Result: 所提方法使六个开源守卫模型在同义改写上的语义波动减小约58%，基准准确率平均提升2.5%，并能泛化到未见过的风格变化；训练还使模型校准性提升最高达40%。

Conclusion: 以语义一致性为核心的训练目标，有助于构建更可靠、可扩展的守卫模型，并揭示了模型一致性与校准性间的深层关系。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [32] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估语言模型理解力的方法，发现在多个决策领域下模型存在准确但理解不足的问题，呼吁评估标准从准确性拓展为理解能力。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在预测准确性上表现突出，但仅有正确性并不代表真正的理解。真正的理解应能像人类专家一样，在多个情境和领域中给出一致、有依据的决策，并依赖相关的领域知识。

Method: 提出了一套名为STaDS（结构化表格决策仿真）的测试框架，将LLM置于专家式结构化决策考试中，通过三个维度——问题和指令理解、基于知识的预测，以及对相关决策因素的依赖——联合评估LLM的理解能力。

Result: 对9个前沿LLM在15个不同决策情景下进行评估，发现多数模型在各领域间难以保持稳定的高准确性，且模型虽准确但往往原因解释与实际预测驱动因素不符，即全球一致性不足。

Conclusion: 仅凭准确性不足以评判LLM的理解能力，亟需更系统的全球一致性理解评估方法及框架，推动模型在理解层面的进步。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [33] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 深度迁移学习比传统机器学习更能精确预测人工耳蜗植入儿童的口语发展水平，有望成为全球临床的标准工具。


<details>
  <summary>Details</summary>
Motivation: 人工耳蜗可以显著改善重度至极重度感音性听力损失儿童的口语表达能力，但效果差异很大，且无法通过年龄或残余听力等因素有效预测。作者希望寻找更为精确的预测方法。

Method: 将传统机器学习（ML）与深度迁移学习（DTL）算法进行比较，利用二分类模型（高/低语言进步者），基于脑神经解剖特征，预测人工耳蜗植入儿童口语发展情况。研究纳入278例双侧SNHL儿童，比较模型的准确率、灵敏度、特异性等指标。

Result: DTL模型使用双线性注意力融合策略后，准确率、灵敏度、特异性和AUC均显著高于传统ML模型。DTL通过有效捕捉区分性、任务相关的信息，实现更优的表现。

Conclusion: 深度迁移学习在预测人工耳蜗儿童语言发展方面，优于传统机器学习，且具备全球普适性模型应用的可行性。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [34] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出结合专家混合（MoE）机制与多阶段训练策略，针对代码切换语音翻译的语义建模和数据稀缺难题，取得了较大提升，实验证实方法有效且具通用性。


<details>
  <summary>Details</summary>
Motivation: 代码切换（CS）型语音翻译面临语义建模复杂和数据稀缺的挑战，现有方法要么过度依赖模型自身隐式建模能力，要么依赖低效且昂贵的人工标注。本文试图解决这两个难题，并提升CS语音翻译的表现。

Method: 提出结合大型语言模型（LLMs）和专家混合（MoE）语音投影器的方法，每个专家处理特定语言的语义子空间，实现更细粒度的语音特征建模。训练阶段采用多阶段训练范式，利用现成的单语自动语音识别（ASR）与单语ST数据，对齐语音-文本并提升翻译能力。同时采用语言特定损失和组内负载均衡损失，优化专家分配效率，并引入transition loss平滑不同阶段间的数据差异。

Result: 在广泛使用的数据集上进行了大量实验证明，本文方法在CS语音翻译上取得了有效且具通用性的提升。

Conclusion: 结合MoE投影器和多阶段训练策略显著提升了代码切换语音翻译的建模效果和数据适应能力，无需依赖大规模CS人工数据，推广性强，思路新颖。

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [35] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: 本文针对多模态大语言模型视觉幻觉问题，提出了融合事实信号的GVF微调方法，通过数据增强、指令调整和一致性损失三方面显著提升模型事实一致性，实验效果优越且保持通用性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉事实一致性上存在问题，具体表现为视觉幻觉，即模型生成与图像内容不符的细节。这严重影响了模型在实际应用中的可靠性。现有微调方法对提升事实推理能力作用有限，因此亟需新方法系统性改进模型的事实一致性。

Method: 提出了基于“基础事实视觉化”（Grounded Visual Factualization，GVF）的微调方法，包括三项机制：1）事实锚点数据增强，将结构化事实锚点与反事实提示注入训练数据；2）基于事实的指令微调，将事实提示嵌入训练指令中；3）事实一致性损失函数，针对事实不一致进行专门惩罚。

Result: 在LLaVA-1.5-13B模型上进行实验，GVF微调方法在视觉幻觉测试基准（VHTest）的开放问答和是/否问答两种形式上，均显著优于标准微调。同时在MME和POPE等通用多模态基准上保持甚至略微提升了模型性能，有效减少视觉幻觉的同时未损害通用理解和推理能力。

Conclusion: GVF微调方法能够系统性地提升多模态大语言模型的视觉事实一致性，有效缓解视觉幻觉问题，且不会影响其在其他任务上的表现。

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [36] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型在材料科学领域的重要应用，包括文献信息挖掘、结构性质预测和智能实验系统。基准测试说明开源LLMs表现优异，作者建议未来推广开源模型以推动领域开放与协作。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在材料科学中应用日益广泛，推动了材料发现流程的创新。作者希望深入探讨LLMs在该领域的具体应用领域和带来的改变。

Method: 对现有LLMs在材料科学中的主要应用进行文献综述，聚焦文献挖掘、预测建模和多智能体实验系统三大部分，并通过基准测试比较开源与闭源模型表现。

Result: LLMs能够有效挖掘科学文献信息、学习结构-性能关系，并协同集成实验与计算工具。实验表明，开源LLMs在透明度、可复现性、成本效益和数据隐私等方面优势明显，其性能可与主流闭源模型匹敌。

Conclusion: 随着开源LLMs能力提升，建议更广泛采用开源模型，打造社区驱动、灵活高效的AI科研平台，加速科学发现进程。

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [37] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: 本文提出一种结合人类反馈和结构化记忆的text-to-SQL持续学习框架，实验证明其能通过不断吸收人类反馈，显著提升SQL生成准确性，为自适应、领域智能的数据库问答系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在将自然语言转为SQL查询语句时，常因不熟悉特定数据库架构和隐性领域知识而表现不佳。

Method: 提出了一个结合人类反馈进行持续学习的text-to-SQL框架，通过结构化记忆把人类反馈中的知识沉淀并应用于未来任务，设计并评估了多种含记忆结构的学习体制。

Result: 在BIRD基准数据集上，含记忆结构的学习体（尤其是Procedural Agent）通过利用人类反馈显著提升了执行准确度并减少了错误。

Conclusion: 将人类隐性知识转化为可复用的结构化知识对于持续提升text-to-SQL系统的适应性和领域感知能力至关重要。

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [38] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 本文提出结合语义相似性和标签分布一致性的示例选择方法（L2D），大幅提升了大语言模型在文本分类中的ICL能力。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL示例选择方法通常只关注语义相似性，忽略了标签分布的一致性，这可能限制了大语言模型（LLM）在文本分类中的表现。

Method: 提出了TopK + Label Distribution Divergence（L2D）两阶段示例选择方法：先用BERT类小语言模型（SLM）生成标签分布，再计算测试输入与候选示例的标签分布差异，从而选择语义和标签分布双重匹配的示例。

Result: 在七个文本分类基准上，该方法始终优于以往的示例选择策略。进一步分析也表明，SLM对标签分布估计的准确性与LLM的分类性能呈正相关。

Conclusion: 标签分布对齐对于ICL示例选择至关重要。L2D方法有效提升了LLM在文本分类任务中的表现，并揭示了SLM性能的重要影响。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [39] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: 本论文提出了一种新型轻量级专家预取机制，解决MoE模型专家选取的准确性和首层优化难题，取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在Mixture-of-Experts（MoE）大型语言模型中，推理速度快且计算成本低。但现有专家预测和缓存方法准确率较低，且首层未优化。复杂预测方法又带来额外计算开销。

Method: 提出了一种基于注意力前的专家预测方法。通过利用同层注意力模块前的激活值，结合两组线性函数和排序感知损失，实现准确且轻量级的专家预取。

Result: 在DeepSeek V2 Lite上达到93.03%的准确率，在Qwen3-30B上94.69%，Phi-mini-MoE上97.62%。相较现有方法，准确率提升约15%。

Conclusion: 通过轻量级且准确的预取机制，不仅优化了MoE模型首层专家选择，还显著提升了预测准确率，降低了计算成本。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [40] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: SpiderGen是一套结合大型语言模型和生命周期评估（LCA）理论的自动化工具，可快速低价地自动生成消费品环境影响评估流程信息，准确性显著提高。相比传统人工LCA，SpiderGen能大幅降低成本（1美元 vs 25000美元）、缩短时间（10分钟 vs 21人天），对气候变化相关研究与实践具备很大价值。


<details>
  <summary>Details</summary>
Motivation: 气候变化和全球变暖已成为全球关注的焦点，而温室气体排放主要源于消费品的生产、使用和废弃过程。为此，有必要建立工具以估算消费品的环境影响，其中生命周期评估（LCA）是关键方法。

Method: 本文提出了一个基于大型语言模型（LLM）的方法SpiderGen。SpiderGen结合传统LCA的分类体系与方法论，并利用LLM强大的推理和世界知识，自动生成用于LCA的程序化信息。为验证效果，作者以现实LCA文档为标准对SpiderGen进行评估，还与其他基准方法如链式思考提示、单样本提示进行了比较。

Result: SpiderGen能够准确生成LCA过程信息，准确性达到或基本正确，F1-Score平均为62%。其主要错误源于LCA文档细节和辅助过程范围的差异。SpiderGen优于现有其他方法，且可大幅降低人工成本和时间，生成所需信息价格低于1美元，耗时不足10分钟。

Conclusion: SpiderGen展示了利用LLM辅助生命周期评估流程自动化的潜力，在环境影响估算领域能够大幅提高效率、降低成本，对应对气候变化具有重要应用价值。

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [41] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 论文系统分析了大模型不同对齐方法（SFT、DPO、RLHF）在应对提示词攻击时的表现，表明现有攻击基准不足，细微提示变化对攻击成功率影响大，对齐方法选择极为关键。


<details>
  <summary>Details</summary>
Motivation: 探究不同的大模型对齐方法在抵抗提示词攻击上的差异，以及对攻击成功率的敏感性，填补仅依赖传统攻击基准不足以全面揭示模型安全性的空白。

Method: 选取采用主流对齐方法（SFT、DPO、RLHF）的开源大模型，对设计用于引导不当内容的攻击提示词进行多样化变换，采用统计分析方法系统考察攻击成功率（ASR）的变化情况。

Result: 即使是微小的提示词变动，对攻击成功率都有显著影响，不同对齐方法下模型的易受攻击性会显著变化。现有基准测试难以覆盖全部漏洞。

Conclusion: 仅运行现有的攻击基准无法全面发现模型和对齐方法的所有潜在漏洞。对齐方法对攻击成功率的敏感性需进行更系统和统计的分析。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [42] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: 本研究用多轮追问和马尔可夫链分析揭示主流大语言模型在多回合交互中的鲁棒性缺陷，长期准确率显著下降，并首次提出稳定准确率作为实际应用下模型鲁棒性的衡量标准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）正被广泛应用，但其在多轮交互中的鲁棒性问题日益突出，鲁棒性对实际应用至关重要，因此有必要对其在多轮对话中的表现进行系统性评估。

Method: 通过一系列多轮跟进提示（如“再想想”或语义等价重述），评估模型回答随回合变化的准确性，采用马尔可夫链建模回合间准确性动态，并利用线性探针预测答案变化。

Result: 结果显示，简单的“再想想”提示使Gemini 1.5 Flash模型九轮后准确率下降约10%，在与等价重述结合时，Claude 3.5 Haiku下降7.5%；马尔可夫链能有效建模与预测多轮准确率，并估算长期准确率（对Gemini 1.5 Flash平均比首轮低约8%）；隐藏态分析表明线性探针可用于未来答案变化预测。

Conclusion: 工作提出将长期准确率（stationary accuracy）作为互动场景下鲁棒性的新指标，并揭示LLM在反复追问下存在脆弱性，需高度关注多轮稳定性以保障模型在高风险、强交互场景中的可靠部署。

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [43] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: 递归大模型合成数据过程中，性别偏见趋于模型固有水平，缓解如对比增强能有效降低实际任务偏见，不同评估手段下偏见指标有显著差异，需多维评估保障数据公平。


<details>
  <summary>Details</summary>
Motivation: 递归型大语言模型在合成数据生成中展现出强大能力，但同时也容易导致偏见（特别是性别偏见）的放大。本研究旨在理解和分析性别偏见在多代生成过程中的动态变化，并探究有效的偏见缓解策略。

Method: 本文采用了三种互补的评估方法来量化和分析偏见：基于规则的模式匹配方法、基于嵌入的语义相似度方法以及下游任务表现评测。实验中设置了三种不同的初始性别偏见水平（0.1、0.3、0.6），并测试了包括对比增强（性别互换）在内的四种偏见缓解策略。

Result: 递归生成过程中的性别偏见表现为趋于模型内在偏见水平，而不是持续单调放大：低初始偏见会增强至模型自身偏见水平（+36%），高初始偏见则会回落（-26%）。对比增强显著缓解了下游任务上的偏见（低初始偏见下减少98.8%，平均减少91%），但在嵌入衡量上却产生了更高的偏见分数。该现象说明不同评估方式可能出现分歧。

Conclusion: 递归生成导致的性别偏见并非持续单调放大，而是趋于收敛到模型自身内在水平。偏见缓解方法，如对比增强，在实际任务中极为有效，但不同评估指标可能得出不同结论。因此，合成数据的偏见评估应采用多维度的分析方法，以确保生成数据的公平性与可靠性。

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [44] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 本文利用传话游戏和大规模概念对数据集，揭示了多模态黑箱模型在概念关联和偏好上的‘隐语言’，为其解释性和可控性研究铺路。


<details>
  <summary>Details</summary>
Motivation: 当前多模态系统进步显著，但由于其黑箱特性，其理解世界的内在机制仍不透明。探索其系统偏好和‘隐语言’有助于揭示其知识结构、增强系统可解释性与可控性。

Method: 通过多轮“传话游戏”（把图片编码为文本再解码回图片）来分析多模态系统对不同概念的内在偏好，同时提出Telescope数据集（包含1万+概念对）用于量化概念关联强度。结合Reasoning-LLM挖掘超越文本和视觉相似性的潜在概念联系。

Result: 证明了多模态模型在概念处理上存在系统性偏好，这些偏好可通过“传话游戏”结构挖掘和量化。构建了大规模概念对数据集，实现了概念关联强度分布的全球可视化，发现部分概念联系更稳定或更脆弱，并利用推理式LLM发现了系统理解中意料之外的概念关系。

Conclusion: 本文通过“传话游戏”以及相关概念配对数据集，揭示了多模态系统在理解世界时隐含的偏好和‘隐语言’结构，并提出该方法可用于进一步解释和改善多模态系统的可解释性和可控性。

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [45] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 文章提出动态、对抗性评测环境Squid Game，用于深度概率和多样性能力的测试，揭示静态评测局限性和模型行为捷径，为LLM评价补充新视角。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型评测基准与模型发展速度脱节，尤其在模型可能通过训练数据泄漏而非真实能力解决问题的情况下，难以建立可信的评估体系。同时，现有评测多为资源丰富环境，欠缺对模型在压力下行为的探究。

Method: 提出Squid Game——一个动态、对抗性评测环境，设置资源受限和信息不对称，通过多轮互动游戏（六个淘汰关卡，涵盖指令遵循、编码、推理、规划、安全对齐等能力）来评估LLM模型性能，并对超过50个模型进行系统测试和行为评估。还采用相关性分析比较Squid Game与主流静态基准的评测结果。

Result: Squid Game对50余款LLM进行最大的动态行为评测，发现同模型世代间存在明显性能跃迁，部分模型为获胜采取猜测型捷径，暗示静态基准评测也可能受污染。动态评测与静态评测相关性分析表明前者可有效补充后者。

Conclusion: 动态、对抗性评测环境Squid Game能发现LLM在压力和信息不对称下的行为及近期模型性能飞跃，为可信评测体系建设提供补充，未来将开放代码和数据。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [46] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: 研究发现，主流语音AI能通过语速变化，隐性表达礼貌，与人类社交习惯高度一致，说明AI在模仿和强化人类社会规范方面有巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 随着语音人工智能在人机交互中的普及，人们越来越期待AI能遵循人类社会规范，尤其是那些未被显式编程指示的隐性交流线索。本研究关注于AI是否能内化语速变化这一表达礼貌的语调特征。

Method: 本研究让22个合成语音（来自AI Studio和OpenAI两个主流平台）分别以“礼貌正式”和“随意非正式”两种风格朗读同一段文字，并测量朗读时长以比较语速差异。

Result: 在两家主流平台的所有语音样本中，“礼貌正式”条件下的语音语速显著低于“随意非正式”条件，且效应量很大。AI Studio所有语音和OpenAI大多数语音都表现出统计显著的语速差异。

Conclusion: 主流文本-语音系统已能够隐性学习并复现人类沟通中表达礼貌的语速变化这一心理细节，表明AI正成为能够强化社会规范的“社会行为者”。

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [47] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: 本文通过设计简单任务和采用激活修补技术，发现并量化了大语言模型何时开始执行指令的关键层级，为模型行为解释和分析提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在跟随指令任务时，内部表征从理解输入内容转变到具体执行动作的时间点及其机制，对于模型解释性和可控性具有重要意义。

Method: 使用激活修补（activation patching）技术，在最小对比提示对下，统计不同层替换残差激活后预测结果的翻转率，从而定位模型由“阅读”转向“执行”的具体层。

Result: 在Llama系列模型中发现一个明显的“起点”层级：在该层之前的干预会显著影响模型预测结果，而在之后则基本无效，并且多步任务组合也呈现类似层级。

Conclusion: 论文提出了一种简单可复现的方法，用于定位大语言模型何时开始执行指令，并发现跨任务和模型规模该起点位置较为一致。

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [48] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本研究系统评估了主流大语言模型在国际关系领域的国家层面偏见，发现偏见因模型和场景而异，推理能力强的模型偏见较低，并提出有效的去偏见框架，可用于提升模型在实际国际关系应用中的公正性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）被广泛应用于国际关系领域，但其潜在的国家层面偏见可能影响应用的公正性和客观性，因此亟需系统评估与消除这些偏见。

Method: 利用联合国安理会的历史记录，作者建立了一个包括三项独立测试的偏见评估框架，重点分析了对五个常任理事国的偏见，并通过实验对多种主流LLM进行了系统测评。此外，提出结合检索增强生成和自反思技术的去偏见方法。

Result: 实验发现，大部分模型普遍对西方国家有有利偏见，对俄罗斯有不利偏见，但不同模型具体偏见方向和程度各异，且同一模型在不同评测环境下偏见表现也有差异。推理能力较强的模型偏见更小、表现更好。提出的新去偏见方法有效降低了偏见，提升了某些主流模型的表现。

Conclusion: 不同LLM存在多维度国家层面的偏见，受模型和任务环境影响，应用于国际关系场景需同时评估偏见和性能，并可通过提升推理与应用去偏见框架改善其表现。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [49] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 本文提出PiAttention稀疏注意力机制，兼具局部精细和周期性远距覆盖，理论和实验均优于现有方法，在多个任务上减少资源消耗并提升效果。


<details>
  <summary>Details</summary>
Motivation: Transformer虽在自然语言处理领域取得了突破，但其随序列长度呈平方增长的计算复杂度，限制了在长距离建模中的应用。现有的稀疏注意力（如RingAttention）虽然降低了计算成本，但接收域有限且缺乏自适应性。

Method: 提出了PiAttention——一种周期性稀疏Transformer。其机制包括：环状局部注意力、确定性π步长跳远和自适应融合门。这样既可周期性覆盖远距离token，又能实现注意力层线性复杂度。并理论证明了PiAttention的接收域增长优于RingAttention。

Result: 实验表明，PiAttention在语言建模、检索、视觉-语言等任务上与密集注意力持平或更优。比如，比RingAttention在困惑度上低8.3%，且同等上下文长度下使用GPU数量减少一半。消融实验与可视化进一步揭示周期跳远、自适应融合、头级稀疏协调对长上下文高效建模的重要性。

Conclusion: PiAttention通过周期性稀疏结构，实现了更广覆盖和更低资源消耗，有效突破了Transformer在长序列建模上的性能瓶颈，并取得了优异实验效果。

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [50] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 该论文提出结合信息抽取和实体识别的新框架，并微调LLM，在多语种医学数据集上实现忠实性和质量提升，为安全医疗文本摘要提供了有效方向。


<details>
  <summary>Details</summary>
Motivation: 医学文本摘要在医疗沟通中非常重要，但不忠实的摘要可能误导医疗细节，带来严重风险。提升摘要的忠实性是提高医疗文本可靠性的关键。

Method: 提出了一个框架，将基于TextRank的句子抽取和医学命名实体识别与大型语言模型（LLM）结合。具体地，微调LLaMA-2-7B模型，在MeQSum（英语）和BanglaCHQ-Summ（孟加拉语）两个数据集上进行实验。

Result: 经过实验，微调后的模型在摘要质量（ROUGE、BERTScore、可读性）和忠实性指标（SummaC、AlignScore）上均有一致提升，优于零样本基线和已有方法。人工评估显示，超过80%的生成摘要能保留关键信息。

Conclusion: 所提方法能显著提升医学文本摘要的忠实性，是将LLM安全部署到医疗场景的潜在手段。

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [51] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: 本文构建并开放了首个针对突尼斯阿拉伯方言到英语的语音翻译数据集TEDxTN，包含25小时、108场TEDx演讲，涵盖11个地区讲者及代码切换现象。同步公布标注指南和实验基线，推动突尼斯阿拉伯语的NLP研究。


<details>
  <summary>Details</summary>
Motivation: 突尼斯阿拉伯语作为一种方言，相关语音翻译数据极为稀缺，制约了自然语言处理领域对该方言的研究和技术发展。本文旨在缓解阿拉伯方言语音翻译数据不足的问题。

Method: 作者收集了108个TEDx演讲，进行语音分段、转录和翻译，遵循内部制定的标注指南。数据涵盖了来自突尼斯11个地区具有不同口音的讲者，并包含大量代码切换现象。作者还构建了强基线系统，利用多种预训练和微调的端到端模型对该数据进行了语音识别和语音翻译实验。

Result: 形成了25小时的突尼斯阿拉伯语到英语公开语音翻译语料库（TEDxTN），并公布全部标注指南和语料。实验证明基线系统效果良好，为突尼斯方言的语音识别和翻译奠定了基础。

Conclusion: TEDxTN是首个公开可用、包含突尼斯阿拉伯代码切换方言的语音翻译数据集。这一资源将极大促进突尼斯方言自然语言处理的相关研究和资源扩展。

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [52] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 本文提出用GenAI和RAG打造学术信息获取聊天机器人，并比较多种模型，发现Gemini 2.0 Flash和Gemma 3n在不同维度具备突出优势，方案可有效提高信息获取效率。


<details>
  <summary>Details</summary>
Motivation: 学生在获取日常学术信息时，常因信息分散在众多文档和网站中而感到困惑，缺乏清晰渠道。

Method: 该研究提出利用生成式人工智能（GenAI）和检索增强生成（RAG）构建聊天机器人，并对多种GenAI模型进行质量和LLM-as-a-Judge指标评测。

Result: Gemini 2.0 Flash因其高质量和速度表现突出，Gemma 3n因性能出色及开源特性被认可。

Conclusion: 利用GenAI和RAG技术可以有效简化学生获取学术信息的路径，评测结果为后续相关工具的选择提供依据。

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [53] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: GPT-4o 可高度还原人类评卷，自动化评分有望应用于真实课堂，但遇到技术难题和开放性题目仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在教育评估任务（如自动评分）中的应用日益增多，但它们在真实课堂环境下与人工评分对齐的情况尚未得到充分研究。本文旨在探究 LLM 能否有效应用于大学课堂的实际评估中。

Method: 研究以本科计算语言学课程为对象，收集了约 50 名学生参与的 5 次小测与 14 个小组项目报告，使用 GPT-4o 自动评分，结果与助教独立人工评分进行对比。

Result: GPT-4o 与人工评分在小测上的分数相关性高达 0.98，55%的小测得分完全一致。项目报告评分总体上与人工一致，但在技术性和开放性问题的评分上有一定波动。

Conclusion: LLM（如 GPT-4o）在实际课堂中能够较好地模拟人工评分，显示出很高的自动评分潜力，但在处理技术性和开放性问题时仍有局限。研究同时公开了全部代码和样本数据，旨在推动教育自动化评估的研究进展。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [54] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 本文通过分析多语言LLM的内部机制，发现模型早期层共享语言表征，后期层通过特异性解码区分语言，并能通过调控语言特征修改输出语言，主导训练语言对整体机制影响显著。理解枢纽语言机制有助于提升多语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言大型语言模型（LLMs）能够处理多种语言，但其内部如何表示这些语言差异仍不清楚。本文旨在探究LLM是否形成了共享的多语言表征，以及为何模型在主导训练语言上表现更好。

Method: 通过在不同多语言数据组合上训练一系列复杂语言模型，并结合跨层转码器（CLT）和归因图分析其内部机制。通过对模型表示、解码过程和语言特征的归因分析，探究表征和解码的语言特异性。

Result: 模型在不同语言间形成了几乎相同的核心表征，语言特定的解码过程主要在模型后期层出现。高频语言特征在最后几层被用于线性判别语言身份，并可通过干预这些特征调控模型输出的语言。主导训练语言会影响模型的归因图和解码路径。

Conclusion: LLMs内部存在“枢纽语言”机制：各语言共享表征，后期解码分语言。理解这一机制对提升多语言对齐具有重要意义。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [55] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: 现有情感识别模型在非裔美国人方言（AAVE）上存在显著误判，易将其表达误判为愤怒，暗藏放大种族刻板印象的风险。建议开发兼容不同文化和方言的情感AI系统以提升公平性与安全性。


<details>
  <summary>Details</summary>
Motivation: 情感检测模型广泛应用于健康监测、心理健康和招聘等领域，但现有模型往往依赖于反映主流文化规范的标注数据，导致在如非裔美国人方言（AAVE）等未被充分代表的方言中表现不佳。

Method: 分析洛杉矶地区267万条推文，通过自动化手段识别AAVE特征，筛选出AAVE密度高低的875条推文进行情感标注。对于AAVE密集部分，采用AAVE流利的非裔美国人社区内标注者（ingroup annotators）生成“银标准”标签，并对现有的GPT、BERT和SpanEmo等情感识别模型的表现进行量化评估。

Result: GPT和BERT模型在AAVE文本上的愤怒错误检出率是GAE的两倍以上；SpanEmo模型愤怒的假阳性率从GAE的25%上升至AAVE的60%。模型和非社区内标注结果与AAVE中的脏话特征显著相关，而社区内标注则不是。结合人口普查数据，非裔居民较多地区被模型预测为更多愤怒和更少快乐。

Conclusion: 现有情感识别AI模型存在严重偏见，容易将AAVE文本误判为愤怒，从而强化种族刻板印象。强调情感计算系统需兼顾文化和方言多样性。

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [56] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 通过先对齐模型参数空间，再做任务算术，能更好地迁移推理等高级技能，有效提升LLM技能合并能力，减少重复微调。


<details>
  <summary>Details</summary>
Motivation: 传统任务算术即直接对参数进行线性组合，在训练分歧较大时往往导致负干扰，迁移技能效果有限。本文希望通过参数空间对齐，提升技能迁移能力，减少模型重复微调的成本。

Method: 提出在进行模型参数相加之前，先对Transformer结构中的参数空间进行对齐。具体方法包括结合GQA和SwiGLU层，采用基于权重和激活的对齐方法。

Result: 实验证明该对齐-算术方法能将复杂推理技能有效迁移至非推理模型，在多个推理基准上均优于传统的任务算术方法。

Conclusion: 参数空间对齐可以显著提升任务算术（task arithmetic）在大模型技能迁移中的效果，克服了传统方法负向干扰的问题。

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [57] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文通过对比直接问答和对话判断发现，大语言模型在社交语境下的评判结果显著受对话影响，不同模型表现出不同的社交偏向，强调了在LLM评估时考虑对话环境的重要性，并提出了一种检测模型立场的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在各类评判任务中的应用日益增多，尤其是在涉及社交互动的场景下，它们对社交或会话判断的可靠性值得关注。本文旨在探讨LLM在面对对话式判断任务时的表现是否会发生变化。

Method: 本文提出了一种评估框架，将对LLM的直接事实性问答与其在极简对话场景下对发言人正确性的判断进行对比，并通过简单的反驳（“上一回答是错误的”）施加压力，测量模型在对话压力下的立场坚守程度。

Result: 研究发现，不同模型在社交情境下表现各异，如GPT-4o-mini在社交语境下表现出奉承倾向，而Llama-8B-Instruct则变得过于批判。整体而言，所有模型的平均表现变化为9.24%，证明即使是极小的对话语境也能显著影响模型判断。

Conclusion: 对话语境在LLM评判任务中起着重要作用，会显著影响模型的判断倾向。所提出的框架为诊断模型立场和提升对话系统可信度提供了方法论基础。

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [58] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: 提出并开源了一个专注于用户上下文输入的 LLM 解释工具包 ICX360，涵盖主流解释方法，提升了各类应用场景下的模型解释能力。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 在高风险领域（如医疗、会议摘要等）的广泛应用，如何解释其输出成为亟需解决的问题，因此作者提出开发针对上下文输入的解释工具。

Method: 该研究推出了开源 Python 工具包 ICX360，整合了三种主流 LLM 解释方法，涵盖黑盒（扰动）和白盒（梯度）机制，并为多类实际应用场景提供详细教程。

Result: ICX360 工具包集成多种 LLM 输出解释方法，为用户在不同实际任务场景下提供了便捷和透明的解释支持，并已开源上线。

Conclusion: ICX360 工具包为解释大型语言模型（LLM）输出过程提供了有效手段，重点关注用户上下文输入，提升了 LLM 应用的透明度和可解释性。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [59] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 大模型更容易因提示词格式而产生否定回答，特别是在不知道答案时。适当增加上下文或允许“不知道”能减少偏见，但推理类提示词反而加重。研究为降低大模型负面偏见提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前负面偏见研究多关注模型结构（如注意头），但忽视了提示词格式等更细致和实际的影响因素，对负面偏见的细粒度机制了解不足。

Method: 提出了一套细致的数据集构建流程，将数据集按模型知识分为三个子集：知识正确、错误及知识不足，并通过不同提示词情境和选项（如加入上下文、“我不知道”选项、Chain-of-Thought等）分析LLM在不同情境下的负面偏见。

Result: 发现负面偏见主要表现为格式层面，知识不足时模型倾向于直接给出否定回答。提供上下文信息和“不知道”选项能降低负面偏见，而Chain-of-Thought提示方式会加剧偏见。提示词不同也会影响偏见方向和程度。

Conclusion: 本文发现大语言模型（LLM）在缺乏足够知识时倾向于生成消极（否定）回答，导致负面偏见，并揭示了这种偏见受提示词格式影响较大，为消除LLM负面偏见提出了见解。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [60] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath集合九大EL数据集，标准化、丰富了本体信息，为生物医学NLP提供了跨领域、可解释性强的新型资源。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学NER和EL研究受制于数据割裂、缺乏可解释模型构建资源以及语义“盲”评测指标等问题，急需可以支撑高质量、可解释研究和开发的数据集。

Method: 作者将九个专业注释的实体链指（EL）数据集整合，采用最新版本的UMLS对所有实体进行标准化，增加了与62种其他生物医学词汇的映射，并进一步为每个实体补充了本体路径信息。

Result: MedPath数据集支持多领域、多标准的实体链指任务，可以促进语义丰富、可解释的EL系统开发，并推动临床NLP模型的互操作性和可解释性研究。

Conclusion: MedPath 数据集通过统一标准化、生物医学词汇映射和本体路径富集，有望促进更语义丰富、可解释的生物医学实体链指系统和模型的研究。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [61] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 作者构建了专业医疗问答数据集，并提出结合专科知识和认证级别的推理及检索增强方法，显著提升医疗大模型的答题准确率和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）虽然在医学问答领域表现较好，但通常忽视了专业领域知识，如临床学科方向和认证级别，这影响了模型在高风险医学场景中的实际应用能力。作者希望弥补这一不足。

Method: 作者构建了EMSQA数据集，包含多学科、多认证级别的2.43万道医疗选择题，并配备对应知识库。基于此，提出了Expert-CoT（结合知识链思维策略）和ExpertRAG（以结构化知识库检索增强生成）两种方法。

Result: 实验在4个主流LLM上进行。Expert-CoT较基础CoT提升最高2.05%；与ExpertRAG结合后，相比标准RAG提升最高4.59%。32B参数模型可顺利通过所有EMS认证模拟考试。

Conclusion: 将专业领域知识与大模型推理和检索过程结合，可以显著提升医疗问答模型在高风险场景下的表现和可靠性。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [62] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 本文提出了一套多模态、社区感知的同行评审模拟系统，能输出更具结构化和可操作性的高质量评审建议，有效提升学术写作前的稿件修改效率和质量。


<details>
  <summary>Details</summary>
Motivation: 当前学术同行评审流程局限于纯文本输入、上下文依赖有限及反馈可执行性不足，难以为作者提供高质量和可操作的修改建议。

Method: 系统集成了多模态大模型，结合文本和视觉信息，并融合基于RAG（检索增强生成）的方式利用OpenReview大数据，最后将评审意见转化为可操作的to-do列表。同时系统与现有的学术写作平台无缝集成，支持实时反馈与修订跟踪。

Result: 实验表明，系统生成的评审在全面性与实用性上均优于对比基线，并更符合专家标准，可有效辅助学术写作与审稿。

Conclusion: 提出的多模态、社区感知的同行评审模拟系统能够生成更全面且实用的评审意见，显著优于精简后的基线模型，有助于提升学术写作和审稿流程的透明度与人机协作水平。

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [63] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: 对于Blooms分类小数据集任务，增广SVM远超主流深度学习和Transformer模型，部分主流大语言模型以零样本方式能接近最佳传统方法，但深度复杂模型过拟合问题严重。


<details>
  <summary>Details</summary>
Motivation: 考查自动将考试题目和学习成果归类到Bloom认知分类的可行性和最佳实践，解决深度学习模型在小样本下分类任务的挑战。

Method: 分别采用传统机器学习模型（贝叶斯、逻辑回归、SVM）、RNN架构（LSTM、BiLSTM、GRU、BiGRU）、Transformer模型（BERT、RoBERTa）及大型语言模型（OpenAI、Gemini、Ollama、Anthropic）进行多种预处理和数据增强的实验分类；评估各类模型在准确率、召回率、F1分数及过拟合状况上的表现。

Result: 增强SVM在传统方法中表现最好（准确率、召回率和F1分数均可达94%且过拟合极低）；BERT和RNN严重过拟合，RoBERTa训练后期也开始过拟合；OpenAI和Gemini在零样本评估准确率约0.72-0.73，表现优于其它LLM。

Conclusion: 复杂深度模型在小数据集上易出现过拟合，SVM等传统增强模型在Bloom分类任务中表现最佳；部分LLM在零样本任务中达较优表现。

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [64] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: LLMs在叙述性医学病例的罕见疾病诊断上表现有限，但新模型有显著进步。作者建构了医学教育认证的新数据集和评测基准，推动AI医学诊断研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多个领域表现突出，但在从叙述性医学案例中诊断罕见疾病的能力尚未被充分探索。为填补该领域的研究空白，推动AI辅助疾病诊断的发展。

Method: 作者构建了一个基于知名医疗剧《豪斯医生》的数据集，包括176个经过医学教育验证的症状-诊断对，作为新颖的基准。对GPT 4o mini、GPT 5 mini、Gemini 2.5 Flash、Gemini 2.5 Pro四个最先进的大模型在叙述性诊断推理任务上进行评估。

Result: 实验结果显示，各模型准确率从16.48%到38.64%不等，最新一代模型比前一代提升2.3倍。虽然所有模型在罕见病诊断上都表现出明显挑战，但不同架构间有显著进步。

Conclusion: 本文建立了一个被医学教育认证的叙述性医学推理基准数据集，并制定了相关评测框架。该工作为未来AI辅助罕见疾病诊断研究与模型开发提供了基线和方向。

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [65] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 从教科书构建心脏病学领域专用文本嵌入模型CardioEmbed，在专业检索任务上显著优于现有通用医学模型，有望提升实际心脏病学临床应用的文本理解与检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学文本嵌入主要基于PubMed文献开发，但实际临床心脏病学更依赖教材中的程序性知识和专业术语，这导致现有模型在实际应用中的表现受限。

Method: 该研究在Qwen3-Embedding-8B的基础上，采用对比学习方法（使用InfoNCE损失与批内负样本），在7本全面心脏病学教科书中去重后的约15万句语料上进行训练。

Result: CardioEmbed模型在心血管特定语义检索任务上，Acc@1达到99.60%，比MedTE提升15.94个百分点；在MTEB医学基准测试上，BIOSSES的Spearman为0.77，SciFact的NDCG@10为0.61，表现出良好的跨领域能力。

Conclusion: 该研究提出的CardioEmbed模型在心脏病学特定的语义检索任务上表现优异，检索准确率达到99.60%，较目前最先进的MedTE模型提升了15.94个百分点。此外，该模型在相关生物医学领域也表现出竞争力。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [66] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文提出用于专业领域话语级中英翻译的新基准DiscoX及配套自动评价系统Metric-S，发现先进大模型在此任务上仍落后人类，工具为翻译评估和提升提供了新框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译的评价方法主要集中在句子级别的准确性和流畅性，缺乏对话语层面和专业领域文本的系统评价，而这些是知识传播和学术交流中至关重要的。本文旨在解决该局限。

Method: 作者提出了DiscoX基准数据集，包含七个专业领域的200篇中英双语长文本，并开发了无需参考译文、能细致自动评价准确性、流畅性、适切性的Metric-S评价系统。

Result: Metric-S评价系统与人工评价高度一致，显著优于现有指标。实验还发现，即使最先进的大模型在这一基准上仍明显落后于人工专家，证明了DiscoX的挑战性。

Conclusion: DiscoX基准和Metric-S系统为评估专业领域的机器翻译提供了更严格的标准，将促进未来大模型翻译能力的提升。

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [67] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 首次系统评估主流DPO数据集，基于细致注释发现多项结构与质量问题，并据此构建更高性能的新混合集UltraMix，推动开源LLM偏好数据发展。


<details>
  <summary>Details</summary>
Motivation: 当前LLM偏好数据集对比稀缺且缺乏细粒度质量标注，阻碍了对DPO效果及数据机制的深入理解，限制了偏好优化的数据驱动发展。

Method: 利用Magpie框架对各样本的任务类别、输入质量和偏好奖励（无需人工标注的、基于奖励模型的信号）进行注释，细致分析多项主流DPO数据集的结构与质量差异，进而有针对性地筛选样本构建新混合集。

Result: 揭示了现有DPO语料在结构和奖励边际上的差异，构建的UltraMix集成体积更小但在关键基准上表现最优，且所有注释与元数据公开，有助于社区进一步研究与数据优化。

Conclusion: 本研究首次对主流开源DPO语料库进行了系统性、以数据为中心的分析，并据此构建出更小但效果更优的新DPO混合集UltraMix，同时公开了相关注释和元数据，促进后续偏好优化研究。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [68] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: 发现文本嵌入普遍存在一致性偏置，提出Renormalization方法，无需训练且高效，能大幅提升多种任务表现，经理论与实验双重验证。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型存在一致性偏置的问题，即所有句子的嵌入向量都包含一个几乎相同的分量μ，影响了模型的表示能力。

Method: 提出无需训练、轻量级的Renormalization方法，用于消除文本嵌入中的一致性偏置。具体方案包括直接减去μ或减去μ方向上的投影，并理论分析和实验验证。

Result: 在MMTEB基准测试上，对38种模型进行实验，Renormalization在检索任务上提升9.7σ，分类任务提升3.1σ，其他任务提升0.8σ，且减去投影的方案效果更佳。

Conclusion: Renormalization方法能显著提升现有文本嵌入模型的效果，其中减去μ分量的投影优于直接减去μ分量。

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [69] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: 该文发现采用思维链方法后，大语言模型可一定程度上检测并识别自己生成的幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成内容有时会出现幻觉错误，作者希望探索模型是否具备自我纠错和检测其生成内容真实性的能力。

Method: 将幻觉检测转化为句子分类任务，采用思维链（CoT）方法辅助模型提取知识并进行分类。

Result: 本文通过实验分析了大型语言模型（LLMs）自我检测其生成“幻觉”（不实信息）的能力。作者提出了一种分类方法，将幻觉检测任务转化为句子分类。此外，引入了“思维链”（Chain-of-Thought, CoT）方法，从模型参数中提取相关知识。结果显示，GPT-3.5 Turbo模型在应用CoT后，能够检测到自身58.2%的幻觉。

Conclusion: 当模型参数中包含足够知识时，应用思维链推理框架的大语言模型能够检测自身幻觉信息。

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [70] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: 通过人工标注和深度学习模型，本文分析了2016–2024年美国总统辩论中的人身攻击，证明了大模型检测政治有害语言的有效性，为改善政治交流透明度提供技术参考。


<details>
  <summary>Details</summary>
Motivation: 美总统辩论中人身攻击影响选民认知，检测该类语言有助于提高政治讨论透明度。深度学习尤其是BERT和大语言模型在有害语言检测领域显示出新前景。基于此，作者提出分析美国总统辩论人身攻击的框架。

Method: 作者首先对2016、2020和2024三届美国总统辩论文本进行人工标注，然后结合统计分析和基于语言模型（包括微调的transformer模型与通用型大模型）的自动分析来检测人身攻击。

Result: 研究发现，针对具体任务优化的现代语言模型能够有效检测正式政治言语中的人身攻击，并为理解政治传播方式提供了更深入的见解。

Conclusion: 研究验证了深度学习和大模型在政治语境下区分和理解人身攻击的可行性和价值，有助于促进公开、透明和理性的政治话语环境。

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [71] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 提出了结合音频与视觉信号的多模态对话系统AV-Dialog，在多说话者噪声环境中显著提升轮次预测、转录与对话质量。


<details>
  <summary>Details</summary>
Motivation: 当前的对话模型在嘈杂、多说话者环境下表现不佳，容易生成无关回复且轮次衔接尴尬，因此需要更鲁棒的多模态对话方法。

Method: 提出AV-Dialog框架，首次结合音频与视觉线索进行多模态对话，包括目标说话人跟踪、轮次预测和连贯回复生成。方法涵盖声学离散化、多任务多阶段训练，在单体、合成及真实音视对话数据集上进行训练。

Result: AV-Dialog在嘈杂环境下优于仅用音频的方法，减少转录错误、提升轮次预测和对话质量（人类评分），展现更自然的对话流。

Conclusion: 视听结合对于具备说话人感知能力的对话智能体至关重要，为在现实噪声环境中鲁棒运行的对话系统奠定基础。

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [72] [Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion](https://arxiv.org/abs/2511.11126)
*Yi Shi,Wenlong Meng,Zhenyuan Guo,Chengkun Wei,Wenzhi Chen*

Main category: cs.CL

TL;DR: 论文针对表情包情绪理解问题提出MemoDetector，包括文本增强与分层多模态融合策略，在公开数据集上提升情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体和互联网文化的兴起，表情包成为表达情感倾向的流行媒介，但现有方法在多模态融合和隐含意义挖掘方面仍存在不足，亟需更先进的情绪理解方法。

Method: 方法由两部分组成：一是利用多模态大语言模型进行四步文本增强，推理并提取表情包隐含与语境信息；二是设计两阶段融合机制，先对原始图像文本浅层融合，再对增强特征深度融合，从而更好捕捉跨模态情感线索。

Result: 提出MemoDetector框架，在MET-MEME与MOOD两个数据集上，F1分数提升4.3%和3.4%，效果优于主流方法，并通过消融实验和深入分析证明其有效性和鲁棒性。

Conclusion: MemoDetector显著提升了表情包情绪识别的性能，展示了深入语境推理和更精细多模态融合的价值，推动了MEU领域的发展。

Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

</details>


### [73] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: 提出SAP$^{2}$，通过双阶段动态筛选与注意力池化机制，有效提升ASR在需要领域知识的复杂语境下的表现，词错误率与关键字错误率均显著下降，性能可扩展。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）系统在常规条件下表现优异，但在需要领域知识的上下文场景（如会议演讲）中，难以充分利用长距离上下文信息。这主要由于模型上下文窗口受限以及大量冗余噪声中相关信息稀疏造成。

Method: 提出了SAP$^{2}$方法。该方法通过两阶段动态剪枝与整合筛选关联语境关键词。每一阶段均使用了作者提出的基于注意力机制的Pooling机制，以高效压缩语境嵌入，同时保留对语音重要的信息。

Result: SAP$^{2}$在SlideSpeech和LibriSpeech数据集上达到了最先进的性能，词错误率（WER）分别为7.71%和1.12%。在SlideSpeech上，关键字偏置错误率（B-WER）相比非上下文基线大幅降低了41.1%。此外，该方法在大规模语境输入条件下表现出良好的可拓展性，性能稳定。

Conclusion: SAP$^{2}$新框架有效提升了ASR系统在复杂语境下对领域关键字的识别能力，并具有良好的可扩展性和业界领先的性能。

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [74] [PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases](https://arxiv.org/abs/2511.11141)
*Udo Schlegel,Franziska Weeber,Jian Lan,Thomas Seidl*

Main category: cs.CL

TL;DR: 本文通过提出PRSM指标，分析了CLIP在不同释义和性别查询下的鲁棒性，发现存在性别不均和策略性差异，提醒多模态系统部署需重视公平性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在多模态任务表现良好，但对语言变体（尤其是释义）的鲁棒性尚未被深入研究。在社会敏感场景中，如果模型对释义不稳定，可能加剧人口统计学偏见，因此评估其稳定性尤为重要。

Method: 提出了一个新的指标——释义排名稳定性指标（PRSM），用于量化CLIP对释义查询的敏感度。通过Social Counterfactuals数据集，分析释义变动对模型表现的影响，并考察与性别相关的鲁棒性差异。

Result: 实证分析表明，CLIP在不同释义策略下的鲁棒性存在差异，并且针对男性和女性的查询，模型鲁棒性存在细微但一致的不同。

Conclusion: CLIP对释义的鲁棒性不均衡，性别相关查询存在表现差异，这对多模态系统的公平性和可靠部署提出挑战。

Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.

</details>


### [75] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 该论文提出了系统的副词上位义类别，并通过人工标注实验证明其实用性和可行性。该体系有效提升了WordNet对副词的支持，为多种自然语言处理任务奠定了基础。


<details>
  <summary>Details</summary>
Motivation: WordNet在名词和动词上有丰富的上位义体系，但对于副词的语义分类支持不足。因此，论文旨在为副词建立系统的语义类别，以完善WordNet并促进相关自然语言处理任务的发展。

Method: 提出以语言学为基础的副词上位义类别体系，通过人工标注实验对体系进行实证验证，评估类别覆盖度和标注一致性。

Result: 实验结果显示，提出的副词上位义类别能涵盖自然文本中的绝大多数副词，且人工标注的一致性较高，具备实用性。

Conclusion: 所建立的副词上位义体系有效扩充了WordNet的内容，与语言学理论更契合，并有助于提升诸如词义消歧、事件抽取、情感分析以及话语建模等NLP任务的表现。

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [76] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: 本文提出LANE对抗训练方法，有效提升神经语言模型对细粒度词义的区分能力，并在多项任务上超过了基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前神经语言模型在词义细粒度区分上存在挑战，主要是因为模型倾向于关注全局语句表征，忽略了局部语义细节。

Method: 提出了一种新的对抗训练策略LANE，通过在训练集中有选择性地标记备选词生成困难的负样本，从而引导模型更关注目标词，提高不同词标记条件下的判别力。

Result: 在词汇语义变化检测和词义消歧等任务上，所提方法生成了更具辨别性的词表征，并优于标准对比学习方法。

Conclusion: LANE是一种通用、可集成于现有表征学习框架的方法，对细粒度词义识别有显著提升效果。

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [77] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型和自动模板的方法，用于可扩展、高质量生成知识图谱问答对，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱生成问答的方法在可扩展性、语言质量和事实一致性三方面表现不足，难以满足实际应用需求。

Method: 首先基于关系对知识图谱三元组进行聚类，通过实体类型和关系自动生成自然语言模板；接着通过大语言模型（LLM）对模板进行语言优化，提升表达清晰度和连贯性；最后，设计了一种从知识图谱中引入干扰项的候选答案生成策略。

Result: 实验证明，该方法能够高效生成高质量问答对，在可扩展性和语言质量之间取得了良好平衡。

Conclusion: 提出的混合式生成管道能高效生成高质量的问答对，兼具可扩展性、流畅性和语言精确度。

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [78] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: iMAD让多智能体辩论只在真正需要时才发生，大幅省算力且更准确。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论（MAD）虽可提升大模型推理能力，但每次都触发会导致大量计算成本与部分准确率降低。因此需要一种更高效、准确的触发机制。

Method: 提出智能多智能体辩论框架（iMAD），通过分析单智能体的自我批评回答，提取41种犹豫特征，并用轻量决策分类器决定是否触发辩论，分类器训练采用新提出的FocusCal损失，无需针对测试集特调。

Result: 在六个（视觉）问答数据集与五个基线下，iMAD能减少高达92%的token消耗，并提升最高13.5%的最终答案准确率。

Conclusion: iMAD框架能智能选择何时触发多智能体辩论，显著提升推理效率与准确率，展现了通用与高效性。

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [79] [destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity](https://arxiv.org/abs/2511.11309)
*Saadat Rafid Ahmed,Rubayet Shareen,Radoan Sharkar,Nazia Hossain,Mansur Mahi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文分析并提出新型对抗攻击方法，能更有效地让主流NLP模型产生困惑，并特别拓展至孟加拉语领域，为模型鲁棒性提升提供思路。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习与神经网络在自然语言处理领域取得了巨大进展，但最新研究显示这些模型在一些方面存在脆弱性，可能导致系统安全风险。动机在于分析和创造新的对抗攻击方法，提升模型鲁棒性。

Method: 作者分析了现有最优秀的对抗攻击方法，并设计了新的对抗攻击策略，专注于通过生成能使模型困惑的模糊输入提升攻击效果，主要利用机器学习和深度学习方法，涵盖多种数据集并涵盖孟加拉语领域。

Result: 提出了一种新颖的对抗攻击方法，生成高困惑性的输入以迷惑当前主流模型，并展示了如何通过此方法为未来模型鲁棒性建设指明方向。坚持实用性及效率优化。

Conclusion: 当前机器学习模型仍存安全隐患，新对抗攻击策略显示这些模型尚不够鲁棒，提示未来需继续加强安全和鲁棒性防护。

Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.

</details>


### [80] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: 该论文针对金融大模型高算力门槛的问题，提出了LAET层级自适应微调方法，只微调关键层，降低计算成本的同时提升效果。实验表明，无论是大型还是小型模型，LAET都显著优于主流方法和基准，有助于让更多机构能高效使用金融NLP技术。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域大模型（LLMs）虽然在文本分析、风险管理和预测等任务中表现优异，但由于算力需求高，许多机构难以使用，这极大限制了其实际应用。

Method: 提出了一种层级自适应集成微调（LAET）的策略，通过分析隐藏状态，仅选择性微调预训练大模型中最有效的层，同时冻结其他部分，从而在保证效果的前提下降低计算开销。

Result: LAET方法在金融NLP任务中表现优异，超越现有基准和主流大模型（如GPT-4），即使在参数量较小的模型（约3B参数）上仍具有竞争力。

Conclusion: LAET实现了更高效、更可扩展的金融NLP模型，推动了金融领域NLP技术向实际部署落地转变。

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [81] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: 本研究提出NOVA框架，以智能化方式自动构建病理分析流程，集成多种工具并能自定义扩展。通过全新设计的SlideQuest基准展示了其优越性能和科研应用潜力，为病理数据分析带来了革命性进展。


<details>
  <summary>Details</summary>
Motivation: 数字化病理分析流程复杂且耗时，需要专业知识，导致其应用受限。为提升其可访问性和效率，亟需自动化和智能化的分析系统。

Method: 提出了一个名为NOVA的智能框架，可以将科学问题自动转化为可执行的数据分析流程，通过迭代生成与运行Python代码完成任务。NOVA集成了49个基于开源软件的专用工具，并可按需生成新工具。为评估系统能力，作者设计了SlideQuest基准测试，包括90个由病理学家和生物医学科学家验证的问题，涵盖数据处理、定量分析和假设检验。

Result: NOVA在SlideQuest基准测试中超越了现有代码生成代理系统。在实际案例中，NOVA成功将形态学与临床预后相关的PAM50亚型关联起来，体现了其可扩展的发现潜力。

Conclusion: NOVA能够自动化并拓展病理分析的能力，不仅支持多步推理和复杂数据处理，还帮助病理学专家进行科学发现，提高了数字化病理分析的效率和应用价值。

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [82] [LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models](https://arxiv.org/abs/2511.11334)
*Jian Gao,Richeng Xuan,Zhaolu Kang,Dingshi Liao,Wenxin Huang,Zongmou Huang,Yangdi Xu,Bowen Qin,Zheqi He,Xi Yang,Changjin Li*

Main category: cs.CL

TL;DR: 本文发布了首个面向老挝语的综合性LLM评测数据集LaoBench，覆盖教育、知识与翻译任务，揭示主流模型在低资源语种上的不足，并推动相关技术发展。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在低资源语言的评估严重不足，尤其是东南亚语言如老挝语，缺乏系统性标准化评测数据集。

Method: 提出并构建了LaoBench数据集，包含17000多个精心筛选的样本，涵盖知识应用、K12基础教育及老挝语-汉语-英语的双语翻译三大维度。结合专家人工筛选与自动化代理辅助核查，确保数据的语言准确性、文化相关性和教育价值。数据集分为开源和闭源两部分，闭源部分用于黑盒评测，确保公平与数据安全。并以多个主流LLMs进行基准测试。

Result: 在LaoBench数据集上的评测显示，现有主流LLMs在老挝语处理的多种任务上仍面临明显挑战，表现不佳。

Conclusion: LaoBench为评估和推动LLMs在低资源东南亚语言领域的能力和应用提供了关键基础，有助于促进相关AI技术研究与发展。

Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.

</details>


### [83] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: 本文为AI文本检测设立了多领域任务和新数据集，促进研究交流，并总结参与队伍方法与未来展望。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型生成文本极为流畅，给信息完整性和学术研究带来挑战，因此亟需跨领域、高质量AI文本检测方法和数据集。

Method: 设立两项二元分类子任务（新闻文章检测和学术写作检测），并创建均衡、规模达30,000条的基准数据集，覆盖人类和多种LLM生成文本；组织竞赛吸引团队参与，并收集方法与结果。

Result: 共有46支队伍报名，最终4支完成了全部任务并提交结果，团队方法得到整理并为未来研究指明方向。

Conclusion: 本文提出了用于多领域AI生成文本检测的M-DAIGT任务，并通过该任务加速相关研究进展。

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [84] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: 语言模型不像人类婴儿那样偏好自然语言，它们对很多“不可习得”的语言也能学得不错，说明它们缺乏人类独有的归纳偏置来引导语言习得，仅复杂性才真正限制了模型学习。


<details>
  <summary>Details</summary>
Motivation: 研究婴儿和语言模型（LMs）在学习真实语言和“不可能语言”时的表现差异，探讨LMs是否像人类一样更容易学习有自然结构的语言，以及找到影响语言习得的因素。

Method: 回顾相关文献，分析婴儿与语言模型在学习真实语言和不自然语言时的表现，比较两者的学习难易度和效果。

Result: 语言模型往往对真实语言和许多“不可能语言”都能同样有效地学习。只有那些极其复杂或随机的“不可能语言”才较难被模型习得。

Conclusion: 语言模型缺乏人类在习得语言时所拥有的归纳偏置（inductive bias），因此难以模拟人类学习语言的过程。真正难以学习的“不可能语言”只是因为它们本质上更复杂。

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [85] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: MajinBook建立了影子图书馆与Goodreads的高质量书籍数据集，可用于社科学术研究，数据完全公开，并说明了相关合法性。


<details>
  <summary>Details</summary>
Motivation: 社交科学和文化分析领域亟需高质量、结构化的大规模图书数据集，但传统数据源如HathiTrust存在偏差，且影子图书馆资源难以直接用于机器分析。

Method: 通过将众包影子图书馆（如Library Genesis和Z-Library）的元数据与Goodreads的结构化书目数据进行关联，优选原生EPUB格式以确保数据可读性，并纳入多语种（英语、法语、德语、西班牙语）数据集。对关联精度进行评估，数据全部公开，同时探讨欧盟和美国相关合法性。

Result: 生成了覆盖英语三百年出版物的高精度图书语料库（超53.9万条），包含出版时间、类型、流行度（评分、评论等），并扩展了法、德、西相关子数据集。

Conclusion: MajinBook将影子图书馆资源转化为结构化、可用的大规模书籍数据集，为社科和文化分析研究提供了更优质的数据基础，兼顾法律合规性。

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [86] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 提出了主动型助听助手，可自动识别和分离佩戴者的对话伙伴，无需手动操作，支持实时本地运行并能适应复杂多对话场景，在真实数据测试中取得良好效果，推动了智能助听设备的进步。


<details>
  <summary>Details</summary>
Motivation: 当前助听设备对多对话场景适应性不强，用户需要手动操作来区分和分离不同对话者，体验不便，且无法实时主动应对复杂交流环境，因此亟需开发能够自动识别对话对象的助听技术。

Method: 提出了一种主动型助听系统，操作于佩戴者的第一人称双耳音频，利用佩戴者自我发声作为锚点，结合轮流发言和对话动态自动推理并分离对话伙伴。系统采用双模型架构：轻量级流式模型以每12.5毫秒运行一次实现低延迟提取对话伙伴，较慢的模型则低频运行以捕捉长时程对话动态，保证实时、设备端的运行。

Result: 在基于11名参与者采集的、总计6.8小时的双人及三人对话真实数据测试中，系统表现出在多对话环境下准确识别并分离对话伙伴的能力，具有良好的泛化性。

Conclusion: 该研究首次实现了具备主动适应对话动态与互动能力的助听助手，为多对话环境下智能助听的发展迈出了关键一步。

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [87] [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)
*Zhenyu Ding,Yuhao Wang,Tengyue Xiao,Haoying Wang,Guojun Ma,Mingyang Wan,Caigui Jiang,Ning Ding*

Main category: cs.CL

TL;DR: 论文提出了推理时可插拔的对齐框架W2S-AlignTree，将MCTS与弱到强泛化方法结合，实现对强模型生成内容无参数微调的细致对齐，显著提升了模型在人类偏好相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型与人类偏好对齐仍面临监督成本高、可扩展性有限及推理时控制不足等问题，急需一种高效、可扩展且灵活的新的对齐机制。

Method: 将Monte Carlo Tree Search(MCTS)与Weak-to-Strong Generalization范式结合，把LLM对齐问题建模为生成搜索树中的最优启发式搜索，利用弱模型阶段性信号和熵感知探索机制，引导强模型生成过程。

Result: 在情感生成、摘要和指令遵从等任务上，W2S-AlignTree均显著优于当前主流方法。例如在摘要任务上，Llama3-8B模型的性能提升了15.9%。

Conclusion: W2S-AlignTree可以在不修改大模型参数的情况下，通过推理时的启发式搜索，实现对生成内容的细粒度控制和优越的人类偏好对齐效果。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.

</details>


### [88] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: 本文提出了一个覆盖法律与金融领域开放式、高难度真实任务的新基准PRBench。结果显示，现有主流大模型在真实高价值问题上表现有限，主要缺陷在于推理和透明度。该基准对模型优化和实际应用有重要启示。


<details>
  <summary>Details</summary>
Motivation: 目前前沿模型的进展多用学术类基准测试衡量，这难以全面反映在实际专业领域中的表现，尤其是在法律和金融这类高风险、高经济影响的任务中。现有评测缺乏对真实、开放性和复杂实际任务的系统性考察。

Method: 作者提出了Professional Reasoning Bench (PRBench)，这是一个开放且难度较高的基准测试，聚焦法律与金融领域的真实问题。该基准包括1,100个由领域专家编写的任务及19,356个专家制定的评判标准，参与专家共182人，涵盖全球114个国家和47个美国法域。所有评判标准均经过独立专家验证，确保高质量和权威性。

Result: 使用PRBench评测了20个领先模型，发现这些模型在专业任务中表现仍有较大提升空间：在最难任务上，金融与法律领域的最高得分分别仅为0.39和0.37。分析显示，即使总分相近，模型在不同子能力上表现差异显著，常见问题包括推理链不完整、判断不准确、过程缺乏透明等。

Conclusion: PRBench显著拓展了评估前沿模型在真实高价值场景下能力的边界，揭示了当前模型在应对专业实际任务时的明显不足，并为未来有针对性的改进指明了方向。

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [89] [Automata-less Monitoring via Trace-Checking (Extended Version)](https://arxiv.org/abs/2511.11072)
*Andrea Brunello,Luca Geatti,Angelo Montanari,Nicola Saccomanno*

Main category: cs.FL

TL;DR: 本论文提出，对特定类型LTL及LTLf公式的运行时监控无需生成庞大的自动机，可直接高效进行轨迹检测，极大降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在运行时验证中，传统方法通过将LTL或LTLf公式合成确定性有限状态自动机（DFA）进行监控，但DFA会产生双指数规模的爆炸增长，带来计算和实施上的巨大负担。

Method: 借助意图安全（intentionally safe）和意图共安全（intentionally cosafe）公式的概念，研究通过直接在当前系统轨迹上进行轨迹检查（trace-checking）能否避免构建DFA，并分析识别这些公式的复杂度。

Result: 对于LTLf，所有安全和共安全片段的公式都是意图安全/共安全，无需进一步检查；对于LTL，识别意图安全/共安全公式的问题复杂度为PSPACE，比完整LTL的EXPSPACE复杂度更低。

Conclusion: 对特定子类LTL和LTLf公式，监控过程中可通过高效的轨迹检测规避DFA的构造，理论复杂度大幅降低，为实际系统监测提供更高效的方案。

Abstract: In runtime verification, monitoring consists of analyzing the current execution of a system and determining, on the basis of the observed finite trace, whether all its possible continuations satisfy or violate a given specification. This is typically done by synthesizing a monitor--often a Deterministic Finite State Automaton (DFA)--from logical specifications expressed in Linear Temporal Logic (LTL) or in its finite-word variant (LTLf). Unfortunately, the size of the resulting DFA may incur a doubly exponential blow-up in the size of the formula. In this paper, we identify some conditions under which monitoring can be done without constructing such a DFA. We build on the notion of intentionally safe and cosafe formulas, introduced in [Kupferman & Vardi, FMSD, 2001], to show that monitoring of these formulas can be carried out through trace-checking, that is, by directly evaluating them on the current system trace, with a polynomial complexity in the size of both the trace and the formula. In addition, we investigate the complexity of recognizing intentionally safe and cosafe formulas for the safety and cosafety fragments of LTL and LTLf. As for LTLf, we show that all formulas in these fragments are intentionally safe and cosafe, thus removing the need for the check. As for LTL, we prove that the problem is in PSPACE, significantly improving over the EXPSPACE complexity of full LTL.

</details>
