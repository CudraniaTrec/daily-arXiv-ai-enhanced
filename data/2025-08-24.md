<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.CL](#cs.CL) [Total: 19]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出一种能自动判定及利用UDAF同态性的演算方法，并验证其可大幅提升聚合函数在Spark等平台上的执行效率。


<details>
  <summary>Details</summary>
Motivation: 当前数据处理框架如Spark和Flink支持用户自定义聚合函数（UDAFs），允许集成领域特定的逻辑，但高效执行的前提是UDAF需满足同态性，这直接影响了增量和并行计算的能力。如何验证和利用UDAF的同态性成为实际需求。

Method: 提出了一种新的同态演算（homomorphism calculus），能够验证或否决UDAF是否为dataframe同态。如果成立，还能够据此构建用于增量计算和并行执行的合并操作符，并实现了基于该演算的算法。

Result: 通过对实际UDAF的实验，所提出的方法在效率上显著优于现有的两个主流合成器。

Conclusion: 本文方法能够系统性地判定UDAF的同态性，并自动合成对应的合并操作符，显著提升了UDAF在数据处理框架中的效率。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [2] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新颖的软件模型检测算法，通过摘要驱动搜索和组合静态分析，大幅提高检测准确性与效率，在公开基准和竞赛中超越了当前主流工具。


<details>
  <summary>Details</summary>
Motivation: 传统软件模型检测在处理长且输入依赖的错误路径时效率低下，且有时无法保证能找出所有存在的错误。该工作旨在提升检测效率和完整性。

Method: GPS将模型检测任务视为程序状态的定向搜索，并利用基于摘要的组合静态分析来剪枝不可行路径并引导测试生成。同时采用两层搜索策略与新颖的仪器化技术，保证能发现所有存在的错误。

Result: GPS在SV-COMP和相关文献程序测试集上，解决的基准数目和运行时间均优于现有最先进模型检测器，包括SV-COMP中的顶尖工具。

Conclusion: GPS算法通过组合静态分析和定向搜索，有效提升了软件模型检测的性能，能够更高效地发现程序中的错误路径及安全性问题。实验结果显示其在多个基准测试中优于现有主流模型检测工具。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [3] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 提出了big-stop语义，作为大步语义的扩展，简洁地解决了发散行为的表达难题，无需引入复杂机制，理论与应用意义显著。


<details>
  <summary>Details</summary>
Motivation: 由于大步语义在描述动态程序行为时更简洁，被众多程序语言实践者所偏好。然而，大步语义难以表达某些小步语义能处理的重要程序行为，特别是发散（无限循环）等现象。

Method: 本文提出了一种鲜为人知的大步语义扩展方法，即通过归纳定义来捕捉发散计算，而无需引入错误状态。这一扩展即称为big-stop语义。作者将其应用于有类型、无类型及带副作用的PCF语言变体以及以while循环为基础的命令式语言。

Result: 所提出的big-stop语义仅通过少量额外推理规则扩展了标准大步推理规则，能够定义与小步语义的反身-传递闭包等价的求值判断。该方法简单明了，相比于其他文献中的解决方案，避免了大量新的推理规则、全局状态，以及复杂的推理原则（如余递归）。

Conclusion: 该文通过对大步语义的简单扩展，实现了对发散行为的有效建模，兼具简洁性和表达力，克服了大步语义传统上的局限。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [4] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: 该论文提出Praline，拓展了Datalog以处理部分已知相关性下的概率推理，方法具备高精度和可扩展性，经实验证实有效。


<details>
  <summary>Details</summary>
Motivation: 以往的概率逻辑编程方法（如ProbLog）未能充分考虑输入事实之间的统计相关性，导致在现实复杂场景中推理不够精确。该论文旨在解决这一问题。

Method: 论文提出了Praline，这是Datalog的全新扩展，能针对部分已知输入相关性下进行精确概率推理。推理被形式化为受约束的优化问题解决。此外，作者还设计了一种基于约束求解、静态分析和迭代精化的高效δ-精确算法，以提升大规模程序下的推理效率。

Result: 在多项具有挑战性的真实基准测试（如侧信道分析）中，所提方法不仅具备良好可扩展性，同时输出概率界限紧致。

Conclusion: Praline支持对具有部分相关性的输入进行精确、高效的概率推理，相比以往方法更适合实际复杂应用场景。

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [5] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 本文通过提出并分析Core ECS形式模型，揭示了ECS软件设计模式实现确定性并发的潜力，指出现有框架未完全利用这一优势，为后续ECS技术创新提供理论支撑。


<details>
  <summary>Details</summary>
Motivation: ECS设计模式被广泛应用于游戏开发，但在其他领域鲜为人知且理解有限，现有解释多依赖具体框架或不恰当的类比，缺乏统一严谨的理论框架。作者旨在全面厘清ECS模式的本质。

Method: 作者设计了一个抽象的形式模型——Core ECS，将ECS模式从具体实现细节中抽离出来，并对其进行了理论分析。同时，作者利用该模型对多个真实世界的ECS框架进行了调研和比较。

Result: 通过Core ECS模型，作者发现存在一类ECS程序可实现无论调度如何都保持确定性的并发行为，而现有ECS框架并未充分利用这一机会。调研显示，这些框架在确定性并发方面尚有提升空间。

Conclusion: 该研究明确指出了ECS实现技术在提高确定性并发能力上的改进方向，为ECS框架拓展新设计和性能优化提供理论依据。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [6] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 论文提出了资源感知的活跃对象模型与类型系统，解决了并发分布式系统中程序公平终止的问题。


<details>
  <summary>Details</summary>
Motivation: 活跃对象系统在分布式计算和业务流程建模中具有重要作用，由于并发和资源管理的本质需求，急需资源感知的形式化模型以保证系统的正确性和可终止性。

Method: 将分级语义和类型系统技术与同步会话的公平终止技术结合，构建出资源感知活跃对象系统的形式化方法。

Result: 论文开发了适用于资源感知活跃对象的核心演算及类型系统，有效确保了程序的公平终止性。

Conclusion: 该论文提出了一种资源感知的活跃对象核心演算和类型系统，可以保证良类型化的程序会公平终止。

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [7] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 本论文提出了一种适用于不同内存模型的组合符号执行平台新理论基础，既支持分离逻辑也支持错误分离逻辑分析，并已用定理证明工具机械化及多模型验证，填补了领域内理论空缺，提升平台灵活性及互操作性。


<details>
  <summary>Details</summary>
Motivation: 此前的多种工具利用分离逻辑（SL）或错误分离逻辑（ISL）进行组合符号执行，但在支持不同内存模型的平台领域，缺乏坚实的形式化理论基础。Gillian平台是唯一可参数化内存模型的工具，展现了可定制性与兼容性的优势，但相关理论支撑不足。本文旨在填补这一空白。

Method: 本文提出并机械化了一个针对内存模型参数化的组合符号执行平台的全新形式化基础，通过交互式定理证明器 Rocq 实现。通过将基础实例化到多种内存模型（如C和CHERI），并兼容两类分析（SL与ISL），采用标准定义确保与现有工具平台的互操作性。

Result: 论文提供的理论基础不仅被验证能适配多种主流内存模型，兼容功能正确性验证和错误发现分析，并与社区认可的定义标准一致，实现了理论与实际工具间的无缝衔接。

Conclusion: 本文建立了一个内存模型参数化的组合符号执行平台的坚实形式化理论基础，兼容主流分析类型和模型，可支持更广泛语言和自动化需求，并推动此领域理论与工具的标准化发展。

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [8] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出了适用于神经符号程序综合的新型主动学习方法，能够有效管理神经网络误判并大幅提升正确率和效率，实验显示SmartLabel工具能在大多数情况下比传统方法更快更准确地找到目标程序。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习在程序综合领域大多集中于符号设置，但随着神经符号程序综合方法的流行，传统方法在面对神经组件的误判时效果不佳，容易导致最终综合出的程序并非用户意图。为此，本文旨在解决这一神经网络误判带来的独特挑战。

Method: 本文提出了一种新型主动学习方法，核心为受约束一致评估（Constrained Conformal Evaluation，CCE），该策略在主动学习循环中迭代提升评估精度，通过结合用户反馈，有效管理神经网络的误判风险，直至候选程序观测等价。该方法被实现为工具SmartLabel，并在多个神经符号领域中进行实验验证。

Result: 实验结果显示，SmartLabel在98%的基准测试中成功找到了真实目标程序，平均仅需不到5轮用户交互。相比之下，以往主动学习技术最多只能在65%的基准测试中收敛到正确程序。

Conclusion: 本文针对神经符号程序综合中的主动学习难题提出了新的解决思路与方法，显著提升了正确程序发现率与用户交互效率，推动了主动学习在实际神经符号程序综合中的应用效果。

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: APR工具修复代码时可能引入新问题和降低功能性，本研究提出更全面的评估框架，以促进安全有效的工具应用。


<details>
  <summary>Details</summary>
Motivation: 当前自动化程序修复（APR）工具评价方法过于单一，只关注清除违规行为，而忽略了工具可能引入新的违规、功能变化及代码结构恶化，因此需要全面评估框架。

Method: 本研究建立了全面的APR工具评估框架，并以业界领先的Sorald工具为例，通过修复Stack Overflow上提取的2,393段Java代码中的3,529个SonarQube违规情况，检验该方法的有效性。

Result: Sorald虽然能修复特定规则的违规，但也引入了2,120个新缺陷（包括32个Bug和2,088个代码异味），导致单元测试失败率达24%，代码结构也受到负面影响。

Conclusion: 该研究验证了全面评估框架的有效性，强调在实际应用APR工具时，需系统性评估其副作用以确保安全与有效采纳。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [10] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 论文建议未来GenAI系统应融合AI能力和传统工程原则，提出理论框架和设计指南，为开发自适应、健壮的AI系统提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前GenAI在实际部署时存在不可预测性和低效等问题，亟需基于GenAI特性和传统软件工程结合的新范式来提升系统的适用性和鲁棒性。

Method: 提出五大GenAI原生设计原则（可靠性、卓越性、可进化性、自主性和保障性），并提出了GenAI原生单元、有机基质、可编程路由等新型架构模式。同时，系统性地分析了GenAI原生软件栈的关键组成部分和多方面影响。

Result: 提出了GenAI原生设计策略、架构模式与实现要素，为未来GenAI系统开发提供了理论参考，并指出相关领域有必要进一步实验和验证这一框架。

Conclusion: 论文提出了一套面向未来GenAI原生系统的设计原则和架构指南，强调要结合GenAI的认知能力和传统软件工程原则，以实现更可靠、自适应和高效的系统。

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [11] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统评估了知识蒸馏在代码理解任务中的效果。结果表明，特别是基于特征的蒸馏方法大幅提升学生模型表现，低参数学生模型能极高程度保留教师模型能力，并对更高效的代码理解模型部署提供了实证支撑。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型（PLM）在代码理解方面表现出色，但其高计算资源消耗和推理延迟限制了大规模应用。知识蒸馏作为压缩和加速模型的有力手段，在代码理解领域尚未得到系统性探索。本文意在填补这一研究空白。

Method: 系统评估两类知识蒸馏方法（基于logit和基于特征），在八个学生模型和两个不同领域的教师预训练语言模型（PLM）间进行实验，覆盖三个代码理解相关下游任务。比较了KD和标准微调的效果。

Result: 知识蒸馏方法在不同规模的学生模型中均有较大性能提升，代码专用PLM作为教师模型效果更好。最新的基于特征的知识蒸馏方法可让学生模型用仅5%参数达到教师模型98%的性能。学生模型和教师模型架构相似度对性能并无明确积极影响。

Conclusion: 知识蒸馏（KD）可显著提升代码理解任务中学生模型的性能，尤其是基于特征的KD方法，实现了高效且保留大部分教师模型能力的学生模型。学生模型与教师模型架构的相似性并非性能提升的保障。

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [12] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: 本文提出SynthCoder，通过多样化数据集构建、跨文件上下文增强与双阶段训练，有效提升代码补全性能，并在多项主流基准上超过现有方法，解决了模型重复代码的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在代码补全领域应用广泛，但由于需要近实时响应，通常采用参数较小或中等的基本模型，并配合多种优化和后训练技术。这些优化常常出现折中效应，即在某些数据集或指标上性能提升，其他方面却下降，甚至低于基线模型。本研究动机在于解决代码补全任务中优化方法的平衡问题，并提升整体性能。

Method: 本方法提出了SynthCoder。首先，结合抽象语法树（AST）节点提取与模拟开发者行为的启发式方法，构建多样化的数据集。然后通过BM25算法和调用图增强训练语料的跨文件上下文信息，提高模型在文件级和仓库级场景下的补全能力。最后，以Seed-Coder-8B-Base为基础，采用双阶段训练流程：先用课程学习技术微调模型，后利用直接偏好优化（DPO）结合拒绝采样生成偏好对进行对齐训练。

Result: 实验结果显示，最终模型在主流仓库级代码补全基准测试上表现优异，涵盖aiXcoder、ExecRepoBench、CrossCodeEval和CoLT。此外，精心策划的数据集有效减缓了模型重复已有代码的倾向，解决了广泛存在的代码补全模型常见问题。

Conclusion: SynthCoder结合多项业界领先实践，实现了对代码补全任务的性能突破，尤其在仓库级场景表现突出，有效缓解了模型代码复用倾向，为相关应用提供了新路径。

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [13] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 为解决任务型聊天机器人可靠性评估的数据难题，本文基于Rasa在GitHub上收集并整理了两个高质量数据集TOFU-R和BRASATO，并提供了相应工具，为后续相关研究提供了关键支持。


<details>
  <summary>Details</summary>
Motivation: 随着任务型聊天机器人在实际服务中的广泛应用，对其可靠性、安全性和鲁棒性的评估却因缺乏大规模、高质量数据集而未得到充分探索。现有自动化质量评估方法常依赖于有限、陈旧或不流行的聊天机器人示例，这影响了评估的有效性。

Method: 本文提出并构建了两个新的数据集（TOFU-R和BRASATO），并提供了相应的工具支持用于数据集的创建与维护。TOFU-R为GitHub上基于Rasa的开源聊天机器人的快照，BRASATO则是经过精心挑选的、在对话复杂性、功能复杂性和实用性方面最具代表性的聊天机器人子集。

Result: TOFU-R数据集展现了当前开源Rasa聊天机器人的实际应用水平，BRASATO则为相关性、复杂性和可复现性研究提供高质量样本，大大便利了聊天机器人可靠性等相关领域的研究。

Conclusion: 本文提供了高质量、可扩展的Rasa聊天机器人数据集，及其构建与维护工具，为任务型聊天机器人在可靠性等方面的研究奠定了坚实的数据基础，方便研究者开展更有效的自动化质量评估。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [14] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文针对LLM在软件工程中的应用，提出了研究类型分类和八项用于提升经验性研究透明性与可复现性的准则，并在线持续维护相关资源。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在软件工程领域的广泛应用，其非确定性、训练数据不透明和架构不断变化，使得经验性研究的可复现性和可重复性变得复杂。因此需要对LLM相关研究展开规范和指导。

Method: 组织社区力量，分析现有LLM在软件工程研究中的实践，总结归纳LLM基础上经验性研究的类型，并制定八条设计和报告准则。这些准则分为必要和期望两类，覆盖从模型选择到数据公开的全过程。

Result: 提出了LLM基础研究类型的分类体系，并给出了八项透明性和报告准则：申明LLM用途和角色、报告模型版本及配置、记录工具架构、公开提示词及交互日志、采用人工验证、使用开放LLM为基线、报告基准及指标、公开局限及缓解措施。这些建议正在持续更新并在社区资源平台llm-guidelines.org上公布。

Conclusion: 本研究通过社区协作，总结并规范了LLM相关经验性研究的设计与报告流程，为提升开源科学环境下此类研究的可复现性与可重复性提供了标准化建议。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [15] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: 软件可维护性经常被忽视，本文通过调研和设计科学方法提出了QUPER-MAn模型，帮助企业更好管理和提升软件可维护性。


<details>
  <summary>Details</summary>
Motivation: 当前软件行业对源代码可维护性重视不足，需求工程阶段可维护性目标设定和管理不到位，需寻找新的方法加以改善。

Method: 采用设计科学方法，开发了QUPER-MAn模型，并通过对业界可维护性相关需求工程实践的探索性研究，分析现有问题。

Result: 行业实践中可维护性常被视为次等质量需求，相关要求多为泛泛而谈。提出的QUPER-MAn模型融入了可维护性基准，有望指导企业有效设定和实现可维护性目标。

Conclusion: QUPER-MAn工具可以帮助企业系统性地设定和管理可维护性目标，将可维护性从被忽视的问题转变为主动管理的质量目标。

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [16] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: 本文提出的VERMEI测试方法通过分析僵尸逻辑并生成高复杂性的等价变体，有效提高了FPGA综合工具Bug挖掘能力，实验效果优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 当前FPGA逻辑综合工具存在缺陷，可能导致意外行为甚至安全风险，因此亟需提升其测试强度。然而，现有自动测试方法生成的测试程序语义和逻辑复杂度不足，难以发现深层次问题。

Method: 作者提出了VERMEI方法，通过预处理、等价变异和Bug识别三个模块协作实现。预处理模块先仿真和覆盖分析，找出不会影响电路输出的“僵尸逻辑”。等价变异模块利用贝叶斯抽样，从历史Verilog设计中抽取并在僵尸区域修剪或嵌入逻辑片段，生成结构更复杂、控制流更丰富的等价变体。最后，Bug识别模块采用差分测试，比较原始和变体综合后的输出，以此发现工具漏洞。

Result: VERMEI方法在Yosys、Vivado、Quartus三款主流综合工具上的实验显示，其Bug发现能力超过现有方法。五个月内，VERMEI向厂商报告15个Bug，其中9个被确认是新问题。

Conclusion: VERMEI能够有效提升FPGA逻辑综合工具的测试质量，显著提升Bug挖掘效率，相较于已有技术具备突出优势。

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [17] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 本文通过对IT团队的行动研究，探索并验证了以工作坊为基础的技术债务管理流程，不仅切实可行，还能提升和维持团队的技术债务意识，并引入了多种实用创新措施。


<details>
  <summary>Details</summary>
Motivation: 现有技术债务管理理论经常被研究但很少在实践中应用，作者希望推动理论向实际的转化，建立企业可实施的TDM流程，并评估其对团队技术债务意识的影响。

Method: 采用行动研究法，在16个月内与一个信号处理IT团队进行了五个行动循环。包括分析每次工作坊中的问卷、观察团队会议、引入心理学觉察测量方法（TD-SAGAT），以及评估待办事项数据。

Result: 实践者更倾向于基于系统演化和成本计算来设定技术债务的偿还与优先级，倾向先处理“低垂果实”。待办事项中的提醒机制（如复选框、文本模板），有助于持续提高团队对技术债务的觉察。

Conclusion: 工作坊为基础的技术债务管理（TDM）方法在实际IT团队中是可行的，并能带来可持续的流程改进。过程中还涌现出多项可能适用于其他IT团队的创新TDM做法。

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [18] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 本文结合PREVENT故障预测和REACT故障排查模块，提升了造船工业系统的问题预警和应对能力，并为类似工业产品的应用提供了参考。


<details>
  <summary>Details</summary>
Motivation: 工业系统复杂且容易因磨损、误用或故障出现异常，因此需要及时检测并解决问题。

Method: 采用最新的故障预测方法PREVENT，并结合故障排查模块REACT，对造船设备进行应用。

Result: 实现了异常检测与故障排查程序的集成，对问题的发现和处理具有实际效果。

Conclusion: 为未来将类似分析方法部署到其他工业产品提供了经验和启示。

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training](https://arxiv.org/abs/2508.14904)
*Jianfeng Si,Lin Sun,Zhewen Tan,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 作者提出一种统一共训练方法，用特殊token即可让模型灵活切换多种安全行为，实现高效、可控的内容安全，性能超过现有更大模型且节约成本。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型（LLMs）内容安全的方法，如监督微调（SFT）和从人类反馈中强化学习（RLHF），通常依赖多阶段训练，且缺乏部署后的细粒度控制。该工作旨在克服这些局限。

Method: 提出统一共训练框架，将多种安全行为（正向、负向、拒绝）高效集成到单一SFT阶段。通过系统指令或特殊token实现推理时动态激活和行为切换。

Result: 实现了不同安全模式下的响应分布清晰分离，引入了“安全对齐边界”，模型在安全鲁棒性和可控性上有显著提升。8B模型在安全性能上超过DeepSeek-R1（671B），同时训练和部署成本显著降低。

Conclusion: 该方法提供了可扩展、高效且高度可控的LLM内容安全解决方案，兼顾安全对齐质量和资源消耗。

Abstract: Current methods for content safety in Large Language Models (LLMs), such as
Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback
(RLHF), often rely on multi-stage training pipelines and lack fine-grained,
post-deployment controllability. To address these limitations, we propose a
unified co-training framework that efficiently integrates multiple safety
behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and
rejective (refusal-oriented/conservative) within a single SFT stage. Notably,
each behavior is dynamically activated via a simple system-level instruction,
or magic token, enabling stealthy and efficient behavioral switching at
inference time. This flexibility supports diverse deployment scenarios, such as
positive for safe user interaction, negative for internal red-teaming, and
rejective for context-aware refusals triggered by upstream moderation signals.
This co-training strategy induces a distinct Safety Alignment Margin in the
output space, characterized by well-separated response distributions
corresponding to each safety mode. The existence of this margin provides
empirical evidence for the model's safety robustness and enables unprecedented
fine-grained control. Experiments show that our method matches the safety
alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1
(671B) in safety performance, while significantly reducing both training
complexity and deployment costs. This work presents a scalable, efficient, and
highly controllable solution for LLM content safety.

</details>


### [20] [Preliminary Ranking of WMT25 General Machine Translation Systems](https://arxiv.org/abs/2508.14909)
*Tom Kocmi,Eleftherios Avramidis,Rachel Bawden,Ondřej Bojar,Konstantin Dranch,Anton Dvorkovich,Sergey Dukanov,Natalia Fedorova,Mark Fishel,Markus Freitag,Thamme Gowda,Roman Grundkiewicz,Barry Haddow,Marzena Karpinska,Philipp Koehn,Howard Lakougna,Jessica Lundin,Kenton Murray,Masaaki Nagata,Stefano Perrella,Lorenzo Proietti,Martin Popel,Maja Popović,Parker Riley,Mariya Shmatova,Steinþór Steingrímsson,Lisa Yankovskaya,Vilém Zouhar*

Main category: cs.CL

TL;DR: 本报告公布了WMT25机器翻译任务的自动评价初步排名，强调最终排名将基于人工评价，当前结果仅供参考。


<details>
  <summary>Details</summary>
Motivation: 本报告旨在为WMT25通用机器翻译任务的参与者提供基于自动评价的初步系统排名结果，以便他们参考和准备系统提交论文。

Method: 采用自动评价指标对参赛机器翻译系统进行排名，但指出这种方法可能偏向使用重排序（如质量估计重排序或最小贝叶斯风险译码）技术的系统。

Result: 报告给出初步的自动评价排名结果，并提示这种排名不是最终官方结果。最终官方排名将基于更为可靠的人类评价。

Conclusion: 当前的排名仅供参考，最终WMT25官方排名需要依赖人工评价结果，自动排名由于方法偏差存在一定局限性。

Abstract: We present the preliminary ranking of the WMT25 General Machine Translation
Shared Task, in which MT systems have been evaluated using automatic metrics.
As this ranking is based on automatic evaluations, it may be biased in favor of
systems that employ re-ranking techniques, such as Quality Estimation
re-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be
based on human evaluation, which is more reliable and will supersede the
automatic ranking.
  The purpose of this report is not to present the final findings of the
General MT task, but rather to share preliminary results with task
participants, which may be useful when preparing their system submission
papers.

</details>


### [21] [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)
*Israel Abebe Azime,Tadesse Destaw Belay,Dietrich Klakow,Philipp Slusallek,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 该论文提出一个用于数学语言问题文化本地化的新框架，有效缓解了现有数据集中过度依赖英语实体的问题，并提升了低资源语言下的模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有多语言数学推理数据集缺乏本地化，低资源语言受制于无本地人物、组织和货币等实体，影响模型表现和公平性。

Method: 提出一种由大型语言模型驱动的文化本地化框架，自动从现有资源中构建包含本地实体的数据集。

Result: 通过大量实验，证明该框架增强了多语言数学推理能力，并能在多种语言场景下提升包含本地实体的任务鲁棒性。

Conclusion: 框架能有效缓解数学问题中的英语中心实体偏见，提高低资源语言的数学推理鲁棒性。

Abstract: Large language models (LLMs) have demonstrated significant capabilities in
solving mathematical problems expressed in natural language. However,
multilingual and culturally-grounded mathematical reasoning in low-resource
languages lags behind English due to the scarcity of socio-cultural task
datasets that reflect accurate native entities such as person names,
organization names, and currencies. Existing multilingual benchmarks are
predominantly produced via translation and typically retain English-centric
entities, owing to the high cost associated with human annotater-based
localization. Moreover, automated localization tools are limited, and hence,
truly localized datasets remain scarce. To bridge this gap, we introduce a
framework for LLM-driven cultural localization of math word problems that
automatically constructs datasets with native names, organizations, and
currencies from existing sources. We find that translated benchmarks can
obscure true multilingual math ability under appropriate socio-cultural
contexts. Through extensive experiments, we also show that our framework can
help mitigate English-centric entity bias and improves robustness when native
entities are introduced across various languages.

</details>


### [22] [Improving LLMs for Machine Translation Using Synthetic Preference Data](https://arxiv.org/abs/2508.14951)
*Dario Vajda,Domen Vreš,Marko Robnik-Šikonja*

Main category: cs.CL

TL;DR: 作者利用DPO训练方法并自动生成优质训练数据，显著提升了大语言模型在斯洛文尼亚语翻译上的性能，并减少语言、格式错误，方法具备广泛借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在机器翻译方面表现优异，但如何利用少量易获得的数据资源进一步提升其性能仍需探索。本文以斯洛文尼亚语为案例，意在证明通过新颖训练方法和数据处理可以显著提升翻译效果。

Method: 使用Direct Preference Optimization（DPO）方法对GaMS-9B-Instruct模型进行微调。通过两个LLM（GaMS-9B-Instruct和EuroLLM-9B-Instruct）自动翻译英文维基百科文章，再结合启发式方法和自动评测指标（如COMET）对翻译质量进行排序，构建DPO所需的训练对。

Result: 微调后的模型在维基百科翻译任务上COMET分数分别比生成训练数据的两基线模型高0.04和0.02，同时显著减少了语言和格式错误。

Conclusion: 少量且易获得的数据资源结合DPO训练，可以有效提升大语言模型在机器翻译中的表现，且改善翻译质量和稳定性。

Abstract: Large language models have emerged as effective machine translation systems.
In this paper, we explore how a general instruction-tuned large language model
can be improved for machine translation using relatively few easily produced
data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct
model using Direct Preference Optimization (DPO) training on a programmatically
curated and enhanced subset of a public dataset. As DPO requires pairs of
quality-ranked instances, we generated its training dataset by translating
English Wikipedia articles using two LLMs, GaMS-9B-Instruct and
EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics
coupled with automatic evaluation metrics such as COMET. The evaluation shows
that our fine-tuned model outperforms both models involved in the dataset
generation. In comparison to the baseline models, the fine-tuned model achieved
a COMET score gain of around 0.04 and 0.02, respectively, on translating
Wikipedia articles. It also more consistently avoids language and formatting
errors.

</details>


### [23] [Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems](https://arxiv.org/abs/2508.14982)
*Qianli Wang,Tatiana Anikina,Nils Feldhus,Simon Ostermann,Fedor Splitt,Jiaao Li,Yoana Tsoneva,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文提出了两个多语言数据集（MultiCoXQL和Compass）和一种新的解析方法，系统性提升了ConvXAI系统在多语言、用户自定义输入场景下的能力，并通过多个模型进行了细致评测。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的对话式可解释人工智能（ConvXAI）系统依赖意图识别技术，但存在多语言泛化能力差和对用户自定义输入支持有限的问题。特别是在训练数据稀缺、低资源语言中的泛化能力面临挑战。

Method: 1. 构建并发布MultiCoXQL，这是一个涵盖五种语言（含一种低资源语言）的多语言CoXQL扩展数据集；2. 提出新的解析方法提升多语言解析性能；3. 针对不同解析策略，评估三种LLM在MultiCoXQL上的表现；4. 发布Compass数据集，用于支持多语言自定义输入，每种语言涵盖11种意图，并在单语、跨语和多语言场景下采用不同模型（LLM和BERT类模型）进行实验。

Result: 新解析方法显著提升了多语言解析性能。三种LLM和BERT模型在MultiCoXQL和Compass数据集上的分析实验展示了它们在单语、跨语和多语场景下对多语言、定制输入的支持能力和局限性。

Conclusion: 本研究通过提出MultiCoXQL和Compass两个多语言数据集和新解析方法，测试并提升了ConvXAI系统在多语言及自定义输入场景下的性能，为多语言对话式可解释AI提供了新的评价基准和方法。

Abstract: Conversational explainable artificial intelligence (ConvXAI) systems based on
large language models (LLMs) have garnered considerable attention for their
ability to enhance user comprehension through dialogue-based explanations.
Current ConvXAI systems often are based on intent recognition to accurately
identify the user's desired intention and map it to an explainability method.
While such methods offer great precision and reliability in discerning users'
underlying intentions for English, a significant challenge in the scarcity of
training data persists, which impedes multilingual generalization. Besides, the
support for free-form custom inputs, which are user-defined data distinct from
pre-configured dataset instances, remains largely limited. To bridge these
gaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL
dataset spanning five typologically diverse languages, including one
low-resource language. Subsequently, we propose a new parsing approach aimed at
enhancing multilingual parsing performance, and evaluate three LLMs on
MultiCoXQL using various parsing strategies. Furthermore, we present Compass, a
new multilingual dataset designed for custom input extraction in ConvXAI
systems, encompassing 11 intents across the same five languages as MultiCoXQL.
We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,
employing three LLMs of varying sizes alongside BERT-type models.

</details>


### [24] [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
*Bolian Li,Yanran Wu,Xinyu Luo,Ruqi Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种高效的测试时对齐算法（SSS），能在几乎不增加推理开销的情况下，使大模型更好地符合人类偏好，兼顾效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时对齐技术需要大量计算资源，导致实际应用受限。因此需要更高效的测试时对齐方法。

Method: 提出了一种名为reward-Shifted Speculative Sampling (SSS) 的算法。该算法将草稿模型与人类偏好对齐，而目标模型保持不变，并通过修改接受准则和奖励token分布来利用两个模型间的分布变化。

Result: 在测试时弱到强对齐实验中，本算法以显著降低的推理成本，获得更高的gold reward分数，验证了其有效性和高效性。

Conclusion: SSS算法能够在保持效率的同时，实现LLM推理阶段与人类偏好一致的优越性能。

Abstract: Aligning large language models (LLMs) with human preferences has become a
critical step in their development. Recent research has increasingly focused on
test-time alignment, where additional compute is allocated during inference to
enhance LLM safety and reasoning capabilities. However, these test-time
alignment techniques often incur substantial inference costs, limiting their
practical application. We are inspired by the speculative sampling
acceleration, which leverages a small draft model to efficiently predict future
tokens, to address the efficiency bottleneck of test-time alignment. We
introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the
draft model is aligned with human preferences, while the target model remains
unchanged. We theoretically demonstrate that the distributional shift between
the aligned draft model and the unaligned target model can be exploited to
recover the RLHF optimal solution without actually obtaining it, by modifying
the acceptance criterion and bonus token distribution. Our algorithm achieves
superior gold reward scores at a significantly reduced inference cost in
test-time weak-to-strong alignment experiments, thereby validating both its
effectiveness and efficiency.

</details>


### [25] [LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text](https://arxiv.org/abs/2508.15085)
*MohamamdJavad Ardestani,Ehsan Kamalloo,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文提出LongRecall框架，通过分解事实、词汇与语义过滤、结构化检验，提升了机器文本召回评估的系统性与准确率，在多项长篇问答任务中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本在医疗、法律等专业领域以及列表式问答任务中，信息遗漏可能带来严重后果。然而，现有召回率评估方法过度依赖词汇重叠，容易因实体不符或答案表达不同而出现评估偏差，同时依赖大语言模型作为评判者也存在误判和幻觉问题。

Method: 提出LongRecall，一个通用的三阶段召回评估框架。该方法首先将答案分解为独立的事实，随后通过词汇与语义过滤逐步缩小可能匹配范围，最后通过结构化蕴含检验确保契合度。该设计减少了误报与漏报，并能兼容多样表达与语境变化。

Result: 在三个具有挑战性的长篇问答数据集上，通过人工标注和大语言模型评审进行评估，LongRecall在召回准确率上显著优于强力的词汇法和LLM-as-a-Judge基线方法。

Conclusion: LongRecall框架系统性地提升了机器生成文本召回率的评估准确度，有效兼容多样化的答案表达和语境，并为后续自动化召回评估提供了坚实基础。

Abstract: LongRecall. The completeness of machine-generated text, ensuring that it
captures all relevant information, is crucial in domains such as medicine and
law and in tasks like list-based question answering (QA), where omissions can
have serious consequences. However, existing recall metrics often depend on
lexical overlap, leading to errors with unsubstantiated entities and
paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts
capture broader semantics but remain prone to misalignment and hallucinations
without structured verification. We introduce LongRecall, a general three-stage
recall evaluation framework that decomposes answers into self-contained facts,
successively narrows plausible candidate matches through lexical and semantic
filtering, and verifies their alignment through structured entailment checks.
This design reduces false positives and false negatives while accommodating
diverse phrasings and contextual variations, serving as a foundational building
block for systematic recall assessment. We evaluate LongRecall on three
challenging long-form QA benchmarks using both human annotations and LLM-based
judges, demonstrating substantial improvements in recall accuracy over strong
lexical and LLM-as-a-Judge baselines.

</details>


### [26] [Mapping the Course for Prompt-based Structured Prediction](https://arxiv.org/abs/2508.15090)
*Matt Pauk,Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: 该论文结合LLM与符号推理进行结构化预测实验，发现组合推理和结构化目标微调能提升预测的一致性与准确性，表明传统结构化学习在大模型时代仍具实用价值。


<details>
  <summary>Details</summary>
Motivation: LLMs在许多语言任务上表现优异，但由于其自回归性质，易出现幻觉和复杂推理方面的问题。在结构化预测任务中，如何保证预测一致性和准确性仍是挑战。

Method: 提出将LLMs与组合推理相结合，并通过各种提示策略估算LLM的置信度，结合符号推理进行结构化预测。还研究了通过校准和结构化目标微调提升性能的方法。

Result: 不论使用何种提示策略，结合符号推理后都能获得更一致且更准确的预测。在结构化目标的微调和校准下，在复杂任务上性能进一步提升。

Conclusion: 结构化学习和组合推理在LLMs时代依然具有重要价值，可以有效解决LLMs在结构化预测中的部分问题。

Abstract: LLMs have been shown to be useful for a variety of language tasks, without
requiring task-specific fine-tuning. However, these models often struggle with
hallucinations and complex reasoning problems due to their autoregressive
nature. We propose to address some of these issues, specifically in the area of
structured prediction, by combining LLMs with combinatorial inference in an
attempt to marry the predictive power of LLMs with the structural consistency
provided by inference methods. We perform exhaustive experiments in an effort
to understand which prompting strategies can effectively estimate LLM
confidence values for use with symbolic inference, and show that, regardless of
the prompting strategy, the addition of symbolic inference on top of prompting
alone leads to more consistent and accurate predictions. Additionally, we show
that calibration and fine-tuning using structured prediction objectives leads
to increased performance for challenging tasks, showing that structured
learning is still valuable in the era of LLMs.

</details>


### [27] [Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset](https://arxiv.org/abs/2508.15096)
*Rabeeh Karimi Mahabadi,Sanjeev Satheesh,Shrimai Prabhumoye,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.CL

TL;DR: 该论文提出了一套创新的科学文本抽取流程，打造了全球最大最优质的开源数学数据集Nemotron-CC-Math，极大提升了大模型在数学、代码、推理等任务上的能力，并开源了全部代码和数据。


<details>
  <summary>Details</summary>
Motivation: 用高质量的结构化数学和代码数据预训练大语言模型，可以显著提升推理能力。然而，现有数学数据集（主要来源于Common Crawl）存在质量下降问题，原因包括低效的抽取方法、HTML转文本过程丢失信息以及数学结构的损坏。

Method: 提出了一种全新的、领域无关的科学文本抽取流程，包括利用lynx进行版面感知渲染，以及基于LLM的清理步骤，能够保留和标准化各种格式的数学表达（MathJax、KaTeX、MathML等），输出一致的LaTeX格式，并去除多余内容、修正不一致之处。

Result: 采集得到Nemotron-CC-Math-3+（1330亿tokens）和Nemotron-CC-Math-4+（520亿tokens）高质量大规模数学语料库。Nemotron-CC-Math-4+比此前所有开源数学数据集都更大，数据质量超越MegaMath、FineMath、OpenWebMath等主流数据集。基于此数据预训练Nemotron-T 8B时，MATH和MBPP+任务上分数分别提升4.8~12.6及4.6~14.3分，并且在MMLU/MMLU-Stem等通用领域任务上也有提升。

Conclusion: 该工作首次提出了一套可以稳定、可靠地从大规模混杂网页数据中抽取科学、数学内容的流程，显著提升了下游模型在数学、代码及通用推理任务中的表现，刷新了开源数学预训练语料的新标准，并公开代码和数据支持社区发展。

Abstract: Pretraining large language models (LLMs) on high-quality, structured data
such as mathematics and code substantially enhances reasoning capabilities.
However, existing math-focused datasets built from Common Crawl suffer from
degraded quality due to brittle extraction heuristics, lossy HTML-to-text
conversion, and the failure to reliably preserve mathematical structure. In
this work, we introduce Nemotron-CC-Math, a large-scale, high-quality
mathematical corpus constructed from Common Crawl using a novel,
domain-agnostic pipeline specifically designed for robust scientific text
extraction.
  Unlike previous efforts, our pipeline recovers math across various formats
(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx
and a targeted LLM-based cleaning stage. This approach preserves the structural
integrity of equations and code blocks while removing boilerplate,
standardizing notation into LaTeX representation, and correcting
inconsistencies.
  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+
(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,
Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including
MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens
than FineMath-4+, which was previously the highest-quality math pretraining
dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to
+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,
while also improving general-domain performance on MMLU and MMLU-Stem.
  We present the first pipeline to reliably extract scientific
content--including math--from noisy web-scale data, yielding measurable gains
in math, code, and general reasoning, and setting a new state of the art among
open math pretraining corpora. To support open-source efforts, we release our
code and datasets.

</details>


### [28] [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)
*Zijie Wang,Eduardo Blanco*

Main category: cs.CL

TL;DR: 通过将识别问题错误假设转化为事实验证任务，并结合外部证据和原子假设验证，显著提升LLMs在应对带有错误假设问题时的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 人们在提问时常常带有错误假设，这类问题没有常规答案，大型语言模型（LLMs）因幻想容易产生误导性回答。因此，需要首先识别这些错误假设。

Method: 将此问题转化为事实验证任务，并提出利用外部证据缓解模型幻想的方法。具体包括在回答问题时检索和引入外部证据，以及针对问题生成、验证原子性假设，由此识别并说明哪些是假设错误。

Result: 实验涉及五种LLMs，结果显示：引入检索到的外部证据有助于提升答案质量；生成并验证原子假设不仅改善回答，还能明确指出错误假设，提升答案可解释性。

Conclusion: 结合外部证据和原子假设的生成验证方法能够显著提升LLMs在含错误假设问题上的表现，减少幻想产生，提升结果的准确率和解释性。

Abstract: People often ask questions with false assumptions, a type of question that
does not have regular answers. Answering such questions require first
identifying the false assumptions. Large Language Models (LLMs) often generate
misleading answers because of hallucinations. In this paper, we focus on
identifying and answering questions with false assumptions in several domains.
We first investigate to reduce the problem to fact verification. Then, we
present an approach leveraging external evidence to mitigate hallucinations.
Experiments with five LLMs demonstrate that (1) incorporating retrieved
evidence is beneficial and (2) generating and validating atomic assumptions
yields more improvements and provides an interpretable answer by specifying the
false assumptions.

</details>


### [29] [ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following](https://arxiv.org/abs/2508.15164)
*Seungmin Han,Haeun Kwon,Ji-jun Park,Taeyang Yoon*

Main category: cs.CL

TL;DR: 本文提出了更真实权威的多模态对话推理基准MMDR-Bench，并开发了提升现有LVLM任务表现的新方法CoLVLM Agent，在人类评测和多项细粒度能力上超过SOTA商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言及视觉-语言模型在处理复杂、多轮、需要深入推理与长期上下文理解的多模态任务上仍有明显不足，现实中的多模态对话交互动态且复杂，现有基准难以真实衡量模型能力，导致上下文丢失和视觉幻觉等问题。

Method: 提出了MMDR-Bench基准，包括300个精心设计的复杂多轮对话场景，平均每个场景5-7轮，对六大维度（如视觉实体跟踪和推理深度）进行评估；此外设计了CoLVLM Agent框架，通过“记忆-感知-规划-执行”的迭代周期增强现有LVLM的推理和指令遵循能力，无需模型大规模重训。

Result: 在MMDR-Bench上，CoLVLM Agent获得了4.03的平均人工评价分数，显著优于GPT-4o（3.92）和Gemini 1.5 Pro（3.85），在推理深度、指令遵循、误差抑制以及多轮对话持续性能上均表现突出。

Conclusion: 提出的新基准和方法能够有效缓解现有LVLM在复杂多模态任务中的不足，模块化和迭代设计显著提升了对话系统的推理与跟踪能力。

Abstract: Despite significant advancements in Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs), current models still face substantial
challenges in handling complex, multi-turn, and visually-grounded tasks that
demand deep reasoning, sustained contextual understanding, entity tracking, and
multi-step instruction following. Existing benchmarks often fall short in
capturing the dynamism and intricacies of real-world multi-modal interactions,
leading to issues such as context loss and visual hallucinations. To address
these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning
Benchmark), a novel dataset comprising 300 meticulously designed complex
multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across
six core dimensions including visual entity tracking and reasoning depth.
Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic
framework that enhances existing LVLMs with advanced reasoning and instruction
following capabilities through an iterative
"memory-perception-planning-execution" cycle, requiring no extensive
re-training of the underlying models. Our extensive experiments on MMDR-Bench
demonstrate that CoLVLM Agent consistently achieves superior performance,
attaining an average human evaluation score of 4.03, notably surpassing
state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro
(3.85). The framework exhibits significant advantages in reasoning depth,
instruction adherence, and error suppression, and maintains robust performance
over extended dialogue turns, validating the effectiveness of its modular
design and iterative approach for complex multi-modal interactions.

</details>


### [30] [SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling](https://arxiv.org/abs/2508.15190)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: SemToken利用语义信息优化分词方式，在不损失性能的情况下，大幅减少token数量和加速计算，为大模型带来更高效的推理与训练。


<details>
  <summary>Details</summary>
Motivation: 现有的分词方法如BPE和WordPiece只基于频率统计，忽略文本的语义结构，导致了语义冗余片段被过度分词，以及在长文本场景下上下文连贯性利用不足。

Method: 提出了一种名为SemToken的语义感知分词框架，先通过轻量编码器提取上下文语义嵌入，再进行局部语义聚类以合并语义等效的token。根据语义密度动态分配分词粒度，在信息丰富区域细分，在重复或低熵片段采用更粗粒度压缩。

Result: 在WikiText-103和LongBench等长文本建模基准上，SemToken实现了高达2.4倍的token数量减少和1.9倍的计算加速，且在困惑度和下游准确度上几乎无损失。

Conclusion: 引入语义结构为大型语言模型的分词与计算带来了新的优化方向，能够显著提升效率且不影响性能。

Abstract: Tokenization plays a critical role in language modeling, yet existing
approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on
frequency statistics, ignoring the underlying semantic structure of text. This
leads to over-tokenization of semantically redundant spans and underutilization
of contextual coherence, particularly in long-context scenarios. In this work,
we propose \textbf{SemToken}, a semantic-aware tokenization framework that
jointly reduces token redundancy and improves computation efficiency. SemToken
first extracts contextual semantic embeddings via lightweight encoders and
performs local semantic clustering to merge semantically equivalent tokens.
Then, it allocates heterogeneous token granularity based on semantic density,
allowing finer-grained tokenization in content-rich regions and coarser
compression in repetitive or low-entropy spans. SemToken can be seamlessly
integrated with modern language models and attention acceleration methods.
Experiments on long-context language modeling benchmarks such as WikiText-103
and LongBench show that SemToken achieves up to $2.4\times$ reduction in token
count and $1.9\times$ speedup, with negligible or no degradation in perplexity
and downstream accuracy. Our findings suggest that semantic structure offers a
promising new axis for optimizing tokenization and computation in large
language models.

</details>


### [31] [Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models](https://arxiv.org/abs/2508.15202)
*Yuanchen Zhou,Shuo Jiang,Jie Zhu,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CL

TL;DR: 论文提出金融领域专用流程奖励模型Fin-PRM，能更细致地评价和引导大型语言模型的中间推理步骤。实验证明其在金融任务中的轨迹选择和下游表现均显著优于通用方法。


<details>
  <summary>Details</summary>
Motivation: 现有的流程奖励模型（PRM）主要用于监督大型语言模型的中间推理过程，但大多数仅在通用或STEM领域训练，在金融等领域存在不足，无法有效处理结构化、符号化且高度敏感于事实和合规要求的金融推理。

Method: 本文提出了Fin-PRM，一种专为金融任务设计的、具有轨迹感知能力的流程奖励模型。Fin-PRM结合了步骤级和轨迹级的奖励监督，用于细致评估符合金融逻辑的推理过程，并分别在离线和在线奖励学习中应用：包括推理轨迹选择、强化学习中的密集奖励反馈，以及测试时的奖励引导推断。

Result: 在金融推理基准测试（如CFLUE和FinQA）上，Fin-PRM在轨迹选择质量上显著优于通用PRM和金融领域基线模型。以Fin-PRM进行下游模型训练，监督学习提升了12.9%，强化学习提升了5.2%，测试时性能提升了5.1%。

Conclusion: Fin-PRM作为金融领域专用流程奖励模型，有效提升了大型语言模型在金融推理任务中的表现，验证了领域专用奖励建模对LLM金融逻辑对齐的价值。

Abstract: Process Reward Models (PRMs) have emerged as a promising framework for
supervising intermediate reasoning in large language models (LLMs), yet
existing PRMs are primarily trained on general or Science, Technology,
Engineering, and Mathematics (STEM) domains and fall short in domain-specific
contexts such as finance, where reasoning is more structured, symbolic, and
sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM},
a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate
reasoning steps in financial tasks. Fin-PRM integrates step-level and
trajectory-level reward supervision, enabling fine-grained evaluation of
reasoning traces aligned with financial logic. We apply Fin-PRM in both offline
and online reward learning settings, supporting three key applications: (i)
selecting high-quality reasoning trajectories for distillation-based supervised
fine-tuning, (ii) providing dense process-level rewards for reinforcement
learning, and (iii) guiding reward-informed Best-of-N inference at test time.
Experimental results on financial reasoning benchmarks, including CFLUE and
FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs
and strong domain baselines in trajectory selection quality. Downstream models
trained with Fin-PRM yield substantial improvements with baselines, with gains
of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in
test-time performance. These findings highlight the value of domain-specialized
reward modeling for aligning LLMs with expert-level financial reasoning. Our
project resources will be available at https://github.com/aliyun/qwen-dianjin.

</details>


### [32] [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)
*Huanxuan Liao,Yixing Xu,Shizhu He,Guanchen Li,Xuanwu Yin,Dong Li,Emad Barsoum,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文提出SPARK，一种针对大语言模型长文本推理时KV缓存通道进行稀疏剪枝的方法，无需训练且易于集成，显著缩减内存消耗并维持或提升模型精度，比传统方法更高效和鲁棒。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长文本时，KV缓存（键值缓存）成为瓶颈，导致内存随序列长度线性增长，注意力计算则是二次增长。现有解决方法多只关注时间轴上的压缩，而忽略了不同特征维度（通道）上的重要性差异，导致效率与准确性难以兼顾。

Method: 提出SPARK，一种无需训练、即可直接应用的方法，通过在KV缓存的通道维度进行非结构化稀疏化剪枝，并在注意力得分计算时动态恢复被剪掉的条目。该方法可与现有KV压缩和量化技术结合使用。

Result: SPARK减少了通道冗余，使同等内存预算下可处理更长序列。对于同长度序列，SPARK比基于逐条驱逐的方法，KV缓存存储减少30%以上，同时精度不降反升。即使采用高达80%的剪枝比例，性能下降仍低于驱逐法的5%，展现了较强的鲁棒性和有效性。

Conclusion: SPARK方法能显著降低大模型长文本推理时KV缓存的内存消耗，同时在不牺牲模型性能的前提下提升处理效率，并兼容其他压缩方法。

Abstract: Long-context inference in large language models (LLMs) is increasingly
constrained by the KV cache bottleneck: memory usage grows linearly with
sequence length, while attention computation scales quadratically. Existing
approaches address this issue by compressing the KV cache along the temporal
axis through strategies such as token eviction or merging to reduce memory and
computational overhead. However, these methods often neglect fine-grained
importance variations across feature dimensions (i.e., the channel axis),
thereby limiting their ability to effectively balance efficiency and model
accuracy. In reality, we observe that channel saliency varies dramatically
across both queries and positions: certain feature channels carry near-zero
information for a given query, while others spike in relevance. To address this
oversight, we propose SPARK, a training-free plug-and-play method that applies
unstructured sparsity by pruning KV at the channel level, while dynamically
restoring the pruned entries during attention score computation. Notably, our
approach is orthogonal to existing KV compression and quantization techniques,
making it compatible for integration with them to achieve further acceleration.
By reducing channel-level redundancy, SPARK enables processing of longer
sequences within the same memory budget. For sequences of equal length, SPARK
not only preserves or improves model accuracy but also reduces KV cache storage
by over 30% compared to eviction-based methods. Furthermore, even with an
aggressive pruning ratio of 80%, SPARK maintains performance with less
degradation than 5% compared to the baseline eviction method, demonstrating its
robustness and effectiveness. Our code will be available at
https://github.com/Xnhyacinth/SparK.

</details>


### [33] [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)
*Bolei He,Xinran He,Run Shao,Shanfu Shu,Xianwei Xue,Mingquan Cheng,Haifeng Li,Zhenhua Ling*

Main category: cs.CL

TL;DR: S2K 框架高效融合内外部领域知识，通过自选知识和结构化推理，显著提升多领域问答表现且成本远低于传统领域预训练。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在领域问答上仍有知识分布长尾、噪音检索导致幻觉和延迟、持续预训练成本高及跨领域灵活性不足等问题。创新性类比人类学习过程，逐渐递进式获得并应用领域概念。

Method: 提出 Selct2Know (S2K) 框架，通过自选内外部知识策略和选择性监督微调，以及结构化推理数据生成和 GRPO 集成提升推理能力。

Result: S2K 在医学、法律、金融问答基准测试上均超越现有方法，低成本下效果可与领域预训练模型媲美。

Conclusion: S2K 框架能够以较低成本实现对领域知识的内化，并在多个领域问答任务上优于现有方法，表现媲美领域预训练的 LLM。

Abstract: Large Language Models (LLMs) perform well in general QA but often struggle in
domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces
external knowledge but suffers from hallucinations and latency due to noisy
retrievals. Continued pretraining internalizes domain knowledge but is costly
and lacks cross-domain flexibility. We attribute this challenge to the
long-tail distribution of domain knowledge, which leaves partial yet useful
internal knowledge underutilized. We further argue that knowledge acquisition
should be progressive, mirroring human learning: first understanding concepts,
then applying them to complex reasoning. To address this, we propose Selct2Know
(S2K), a cost-effective framework that internalizes domain knowledge through an
internal-external knowledge self-selection strategy and selective supervised
fine-tuning. We also introduce a structured reasoning data generation pipeline
and integrate GRPO to enhance reasoning ability. Experiments on medical, legal,
and financial QA benchmarks show that S2K consistently outperforms existing
methods and matches domain-pretrained LLMs with significantly lower cost.

</details>


### [34] [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)
*Sijia Cui,Aiyao He,Shuai Xu,Hongming Zhang,Yanna Wang,Qingyang Zhang,Yajing Wang,Bo Xu*

Main category: cs.CL

TL;DR: 为了让大语言模型更好地自动选择和使用多步工具，作者提出了一个会动态记忆过去成功案例并用以优化后续决策的方法（SEER），无需繁复人工构建样例，有效提升了多步函数调用任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）已经能够通过函数调用与外部系统交互，但在多步工具使用时，LLMs在工具选择、参数生成和工具链规划等方面表现不佳。现有方法严重依赖人工构造的示例或库，随着工具种类和任务难度的提升，这种方式变得低效且复杂。

Method: 作者提出了一种自引导的方法Stepwise Experience Recall（SEER），从不断更新的经验池中进行细粒度、逐步检索。该方法会将成功的历史案例逐步纳入经验池，实现动态扩充和持续性能提升，无需依赖静态或人工构造的库。

Result: 在ToolQA基准测试中，SEER方法对于简单和困难问题分别带来了6.1%和4.7%的平均性能提升。在τ-bench（涵盖两个真实领域）中的实验表明，SEER让Qwen2.5-7B和Qwen2.5-72B模型准确率分别提升了7.44%和23.38%。

Conclusion: SEER提供了一种无需人工大量参与、可持续自我改进的函数调用链生成方式，有效提升了多步工具操作中LLMs的决策与执行效率，在多个基准测试与实际场景下验证了其优势。

Abstract: Function calling enables large language models (LLMs) to interact with
external systems by leveraging tools and APIs. When faced with multi-step tool
usage, LLMs still struggle with tool selection, parameter generation, and
tool-chain planning. Existing methods typically rely on manually designing
task-specific demonstrations, or retrieving from a curated library. These
approaches demand substantial expert effort and prompt engineering becomes
increasingly complex and inefficient as tool diversity and task difficulty
scale. To address these challenges, we propose a self-guided method, Stepwise
Experience Recall (SEER), which performs fine-grained, stepwise retrieval from
a continually updated experience pool. Instead of relying on static or manually
curated library, SEER incrementally augments the experience pool with past
successful trajectories, enabling continuous expansion of the pool and improved
model performance over time. Evaluated on the ToolQA benchmark, SEER achieves
an average improvement of 6.1\% on easy and 4.7\% on hard questions. We further
test SEER on $\tau$-bench, which includes two real-world domains. Powered by
Qwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains
of 7.44\% and 23.38\%, respectively.

</details>


### [35] [Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?](https://arxiv.org/abs/2508.15218)
*Momoka Furuhashi,Kouta Nakayama,Takashi Kodama,Saku Sugawara*

Main category: cs.CL

TL;DR: 文章研究自动生成评判清单在大模型评估中的效果，发现选择性使用清单能提升配对比较表现，但直接评分效果不稳定。部分清单与人工评分标准存在不一致，建议未来工作明确定义评价标准以提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 自动评估生成任务时，大型语言模型由于评判标准模糊，难以获得稳定且可信赖的结果。自动生成评判清单被认为是一个有潜力的方法，但其实用性未被充分探索。

Method: 对是否应该为所有问题或有选择性地使用清单进行研究，采用六种方法生成清单，在八种模型规模上评估其有效性，并分析哪些清单条目与人工评估相关。通过配对比较和直接评分两项实验考察清单使用效果。

Result: 有选择地使用清单在配对比较任务中提升评估性能，而在直接评分任务中的效果则不够一致。此外，部分与人工评分相关性低的清单条目，实际上符合人工编写的评判标准，反映出人工评估本身存在标准不一致的问题。

Conclusion: 自动评估生成任务需界定更清晰的客观评判标准，以更好地指导人工及自动化评估方法。清单法在特定场景下有效，但需进一步优化标准定义和应用策略。

Abstract: Automatic evaluation of generative tasks using large language models faces
challenges due to ambiguous criteria. Although automatic checklist generation
is a potentially promising approach, its usefulness remains underexplored. We
investigate whether checklists should be used for all questions or selectively,
generate them using six methods, evaluate their effectiveness across eight
model sizes, and identify checklist items that correlate with human
evaluations. Through experiments on pairwise comparison and direct scoring
tasks, we find that selective checklist use tends to improve evaluation
performance in pairwise settings, while its benefits are less consistent in
direct scoring. Our analysis also shows that even checklist items with low
correlation to human scores often reflect human-written criteria, indicating
potential inconsistencies in human evaluation. These findings highlight the
need to more clearly define objective evaluation criteria to guide both human
and automatic evaluations. \footnote{Our code is available
at~https://github.com/momo0817/checklist-effectiveness-study

</details>


### [36] [VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models](https://arxiv.org/abs/2508.15229)
*Hanling Zhang,Yayu Zhou,Tongcheng Fang,Zhihang Yuan,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: 本文针对SLM在边缘设备部署时的内存瓶颈，提出了VocabTailor框架，利用词汇动态选择与嵌入分离，极大减少内存使用且保持性能，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的小语言模型（SLM）在边缘设备上内存限制严重，其大部分内存由词表相关组件占用，现有的静态词汇裁剪存在信息丢失和灵活性不足等问题，需要更高效灵活的减小内存占用的方案。

Method: 提出了VocabTailor，基于词汇局部性和计算特性不对称原则，将词嵌入和LM Head分离，实现了一种混合静态-动态词汇选择策略，实现词汇相关组件的按需加载和嵌入外部存储。

Result: VocabTailor在多种下游任务上将词汇相关组件的内存占用减少了高达99%，但任务性能几乎没有损失，效果远优于传统静态词表裁剪方法。

Conclusion: VocabTailor方法显著减少了SLM中与词汇相关组件的内存占用量，同时在任务性能上几乎无损失，表现优于现有的静态词汇裁剪方法。

Abstract: Small Language Models (SLMs) provide computational advantages in
resource-constrained environments, yet memory limitations remain a critical
bottleneck for edge device deployment. A substantial portion of SLMs' memory
footprint stems from vocabulary-related components, particularly embeddings and
language modeling (LM) heads, due to large vocabulary sizes. Existing static
vocabulary pruning, while reducing memory usage, suffers from rigid,
one-size-fits-all designs that cause information loss from the prefill stage
and a lack of flexibility. In this work, we identify two key principles
underlying the vocabulary reduction challenge: the lexical locality principle,
the observation that only a small subset of tokens is required during any
single inference, and the asymmetry in computational characteristics between
vocabulary-related components of SLM. Based on these insights, we introduce
VocabTailor, a novel decoupled dynamic vocabulary selection framework that
addresses memory constraints through offloading embedding and implements a
hybrid static-dynamic vocabulary selection strategy for LM Head, enabling
on-demand loading of vocabulary components. Comprehensive experiments across
diverse downstream tasks demonstrate that VocabTailor achieves a reduction of
up to 99% in the memory usage of vocabulary-related components with minimal or
no degradation in task performance, substantially outperforming existing static
vocabulary pruning.

</details>


### [37] [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)
*Peerat Limkonchotiwat,Pume Tuchinda,Lalita Lowphansirikul,Surapon Nonesung,Panuthep Tasawong,Alham Fikri Aji,Can Udomcharoenchaikit,Sarana Nutanong*

Main category: cs.CL

TL;DR: 论文提出并构建了高质量泰语指令数据集WangchanThaiInstruct，并通过实验证明：模型用原生数据集微调后性能显著优于仅用翻译数据的方案。强调低资源语言模型的提升需本地化、专业化的数据支持。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在英语上的指令遵循能力很强，但在低资源语言（如泰语）上表现未被充分研究。现有的评测基准多依赖翻译，忽略了真实使用场景中的文化和领域特有细节。论文希望解决低资源语言模型评测和微调时缺乏本地化、高质量数据的问题。

Method: 提出了WangchanThaiInstruct，一个由人工编写的泰语数据集，覆盖四个专业领域和七种任务类型。该数据集经过多阶段质量控制，包括标注员、领域专家和AI研究人员的参与。利用此数据集，进行了零样本评测和指令微调消融实验，分别分析模型在特定任务上的性能差距以及原生监督数据的作用。

Result: 用WangchanThaiInstruct微调后的模型在领域内和领域外的基准测试中均优于使用翻译数据训练的模型。

Conclusion: 要提升低资源、语言多样性环境下LLM的对齐度和实际表现，需构建具有当地文化和专业背景的原生指令数据集，而不是仅依赖翻译数据。

Abstract: Large language models excel at instruction-following in English, but their
performance in low-resource languages like Thai remains underexplored. Existing
benchmarks often rely on translations, missing cultural and domain-specific
nuances needed for real-world use. We present WangchanThaiInstruct, a
human-authored Thai dataset for evaluation and instruction tuning, covering
four professional domains and seven task types. Created through a multi-stage
quality control process with annotators, domain experts, and AI researchers,
WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing
performance gaps on culturally and professionally specific tasks, and (2) an
instruction tuning study with ablations isolating the effect of native
supervision. Models fine-tuned on WangchanThaiInstruct outperform those using
translated data in both in-domain and out-of-domain benchmarks. These findings
underscore the need for culturally and professionally grounded instruction data
to improve LLM alignment in low-resource, linguistically diverse settings.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [38] [Transition-based vs stated-based acceptance for automata over infinite words](https://arxiv.org/abs/2508.15402)
*Antonio Casares*

Main category: cs.FL

TL;DR: 本文综述了无限对象自动机接受条件从状态到转换的转变动因，比较了两种方式在实际应用中的影响并推荐基于转换的接受方式。


<details>
  <summary>Details</summary>
Motivation: 由于自动机在逻辑和形式化验证中的广泛应用，最近对基于转换的接受条件兴趣增长，作者希望明确这种转变的原因和实际影响。

Method: 本文通过分析现有文献与实例比较，系统阐述状态与转换为基础的两种接受条件在不同问题中的影响，结合具体问题展示差异原因。

Result: 展示了在部分问题下，选择不同的自动机接受条件（状态/转换）会导致形式化表达与处理方法显著不同，揭示并解释了这些差异。

Conclusion: 该综述认为在无限对象自动机中，采用基于转换的接受条件比传统的基于状态的方法有显著优势，并应予以推广。

Abstract: Automata over infinite objects are a well-established model with applications
in logic and formal verification. Traditionally, acceptance in such automata is
defined based on the set of states visited infinitely often during a run.
However, there is a growing trend towards defining acceptance based on
transitions rather than states.
  In this survey, we analyse the reasons for this shift and advocate using
transition-based acceptance in the context of automata over infinite words. We
present a collection of problems where the choice of formalism has a major
impact and discuss the causes of these differences.

</details>


### [39] [List of Results on the Černý Conjecture and Reset Thresholds for Synchronizing Automata](https://arxiv.org/abs/2508.15655)
*Mikhail V. Volkov*

Main category: cs.FL

TL;DR: 本文系统回顾和整理了Cerný猜想在有限自动机不同类别上的最新证明及已知长度上界，展示了截至2025年的研究现状及未来潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: Cerný猜想是自动机理论中的核心问题，关乎有限自动机Reset词的最短长度界。论文旨在系统总结该领域最新（截至2025年8月）的研究进展，推动问题进一步研究。

Method: 采用文献回顾方法，梳理已有学术成果，列举已解决和未解决的自动机类别及相关长度上界。

Result: 归纳出哪些自动机类别的Cerný猜想已被证实，以及哪些类别虽然猜想尚未解决，但已确认最短Reset词长度的二次上界。

Conclusion: 该文总结了文献中关于Cerný猜想在不同有限自动机类别下成立的结果，并指出了一些类别目前仍未解决，但已知Reset词最短长度存在二次上界。

Abstract: We survey results in the literature that establish the \v{C}ern\'y conjecture
for various classes of finite automata. We also list classes for which the
conjecture remains open, but a quadratic (in the number of states) upper bound
on the minimum length of reset words is known. The results presented reflect
the state of the art as of August 21, 2025.

</details>
