<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL提出通过统一中间表示(IR)解决多语言互译复杂度爆炸问题，支持主流GPU和系统语言间的双向翻译，通过模块化架构实现易扩展，“写一次、部署任意处”，实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 传统语言互译方案复杂度极高，随着语言数量增长需额外开发数量级增多的语言对之间的转换器。本研究旨在通过统一中间表示法，显著降低开发和维护成本，提高语言间代码迁移与复用能力，助力通用编程。

Method: 设计了一个统一的中间表示CrossGL，所有支持的语言通过专属的lexer/parser模块生成AST，然后转换为CrossGL，再通过对应后端模块生成目标语言代码。采用模块化结构，内建和扩展多种编程语言支持。通过全面的测试评估各类语言编译与执行效果。

Result: 实现了跨CUDA、HIP、Metal、DirectX HLSL、OpenGL GLSL、Vulkan SPIR-V、Rust、Mojo等多种主流语言之间的高效双向翻译，在各类编程领域验证了转译结果的正确编译与执行。架构证明仅需添加语言相关前后端模块即可快速扩展新语言支持。

Conclusion: CrossTL大幅简化多编程语言间的翻译流程，通过统一中间表示（IR）实现多语言通用和双向转译，推动语言无关的编程范式，为“写一次、部署任意处”提供技术基础。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [2] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: 本文阐述了 Mathlib 通过弃用体系、linters 等自动化工具和自定义流程，有效应对大规模成长中的变更管理、代码质量、编译性能和贡献筛选问题，减轻了维护者压力。


<details>
  <summary>Details</summary>
Motivation: Mathlib 正快速成长，在推动形式化数学发展的同时，也带来了库体量、贡献增长与变更管理上的新挑战。需要系统性策略保证高质量、可维护性与开发效率，并避免维护者超负荷。

Method: 本文介绍了通过弃用(deprecation)机制、代码质量检查工具（linters）、对库进行优化以加快编译速度、技术债务管理，以及自定义工具以提升代码审查和新贡献的筛选效率等方法。

Result: 实现了对库演化中破坏性变更的可控管理，提供用户友好型的质量反馈、明显缩短编译时间，以及提升整体维护和贡献审查效率。

Conclusion: 通过多种策略与工具，Mathlib 能有效地应对库的快速增长与变化，实现可持续发展并减少维护人员负担。

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本文探索了利用LLM和RAG流式生成技术，将UML模型自动转化为Qiskit量子代码，实验获得显著质量提升，并为未来量子软件开发自动化的多种方向奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 面对当前量子及混合量子-经典软件系统开发中平台异构和开发者技能短缺的问题，寻找自动化、智能化的模型转代码/文本方法以降低开发成本和风险。

Method: 利用大型语言模型（LLMs）及其增强检索生成（RAG）技术，从UML模型实例自动生成量子计算机可用的 Python（Qiskit）代码。RAG 流水线结合了从GitHub等公共资源获取的Qiskit代码示例，并通过精心设计的 prompt 进行实验。

Result: 实验表明，优化prompt设计后，生成的代码在CodeBLEU评测中得分提升最大可达四倍，生成的量子代码准确性和一致性显著增强。

Conclusion: 提出了利用LLM+RAG从模型向代码/文本自动转换的新方向，实验验证了部分想法的有效性，并且表明该方法在未来具有更大应用潜力。还可进一步扩展至模型实例作为RAG信息源、代码到代码转换（如转译）等其他场景。

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [4] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: 本文提出了UTRL对抗性强化学习框架，让两个大语言模型互相提升：单元测试生成器尽可能暴露代码缺陷，代码生成器则努力通过更多测试。实验证明该方法能让小模型生成更高质量单元测试，甚至超过GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成方法受限，人类撰写与LLM自动化生成质量不高，且如何训练模型产出高质量测试仍不足。旨在解决单元测试自动化生成的质量瓶颈。

Method: 提出了一种新颖的强化学习框架，采用两个大模型（单元测试生成器与代码生成器）对抗训练，通过奖励机制提升各自生成能力。

Result: 实验证明，通过UTRL训练后的Qwen3-4B模型，单元测试质量优于同模型的监督微调版本，且超越了如GPT-4.1等前沿大模型。代码评测结果更贴近真实地面测试集。

Conclusion: UTRL 框架能够有效提升大语言模型生成高质量单元测试的能力，而且优于传统监督微调方法和当前前沿模型。

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [5] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: 本文提出一种基于LoRA微调的大语言模型，用于自动化bug分派，实验显示其候选开发者命中率高于传统方法，并在最新数据上有望大幅提升准确性。


<details>
  <summary>Details</summary>
Motivation: 在大型项目中，bug三方分配过程常因手动决策而变得慢且不一致，因此需要高效且智能的方法自动分派新问题给合适的开发者，降低人为成本。

Method: 提出了一种轻量级框架，利用LoRA适配器微调后的大语言模型，通过候选受限解码方式，保证bug分配的候选开发者输出合理。并在EclipseJDT和Mozilla两大数据集上进行了实验评估。

Result: 在EclipseJDT和Mozilla数据集上，该模型在Top-10结果中命中率（Hit@10）高达0.753，近期快照数据上Top-1准确率显著提升，体现了框架在人机协同真是应用场景下的潜力。

Conclusion: instruction-tuned的大语言模型（LLM）结合LoRA adapter和限制候选开发者的解码，能够有效提升bug分派的候选名单质量，是现有传统特征工程和图方法的实用替代方案。

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [6] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 在软件工程任务自动化中，简单的删旧信息方法和LLM摘要管理长上下文相比，速度更快，成本更低，效果还略好。复杂方案未必带来收益。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的软件工程代理在解决复杂任务时需处理长且昂贵的历史上下文。这些代理通常借助LLM摘要来优化，但并不清楚复杂化带来的实际收益是否优于简单删除旧观测的方法。该论文希望系统性评估不同上下文管理策略的有效性和效率。

Method: 以SWE-agent在SWE-bench Verified基准上，采用五种不同模型配置系统性比较LLM摘要和简单观察屏蔽（删去旧信息）两种上下文管理策略，评估各自的成本和任务解决率。

Result: 实验表明，简单的观察屏蔽策略相比原始代理能将成本减半，且解决率与LLM摘要策略相当甚至略高。例如，Qwen3-Coder 480B模型下，解决率从53.8%提升至54.8%，并以更低成本与LLM摘要相竞争。

Conclusion: 在SWE-agent解决SWE-bench Verified任务中，最简单的观察屏蔽方法不仅效果出色，还更高效，其性能无明显劣于复杂的LLM上下文摘要。代码与数据已公开，便于复现。

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [7] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 本文提出了用大语言模型增强指针分析的设想，可更好处理用户自定义函数，并通过自然语言策略提升分析的精度和效率，是指针分析领域的创新尝试。


<details>
  <summary>Details</summary>
Motivation: 现有的指针分析框架由于对代码语义理解不足，特别在用户自定义函数方面处理过于保守，导致错误信息传播，影响分析精度。该问题长期困扰指针分析领域。

Method: 提出了LMPA框架，将大语言模型（LLMs）集成进指针分析流程，提升指针分析的精度和可扩展性。具体方法包括：利用LLMs识别类似于系统API的用户自定义函数，并针对性建模，减少错误传播；在基于摘要分析方面，推断初始指向集合，并通过自然语言增强摘要策略。

Result: LMPA能够有效缓解错误跨调用上下文传播问题，在指针分析中通过更细致的语义处理提升整体分析质量，并提出了自然语言辅助的创新性策略。

Conclusion: 结合LLM的语义理解能力，LMPA为指针分析带来了新的提升方向，显示出更高的精度与扩展性，但实际部署还面临一些挑战需要进一步探讨。

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [8] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: 提出了MPTCS自动化测试用例选择方法，实现RL环境跨政策复用的高效测试，对政策相关难点、覆盖面、成本及测试多样性等进行了探索与验证。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理政策测试生成的测试集仅适用于特定政策，难以复用和对新政策测试其可靠性及性能。本工作旨在构建能跨多策略复用且具检测普适缺陷能力的自动化测试用例集。

Method: 提出MPTCS方法，根据候选池中用难度评分选取多策略可复用的测试用例，并设计测试用例多样性促进机制（如离散化的通用测试用例描述面）。评估难度评分、政策数量对效果和成本影响，以及多样性机制对状态空间覆盖及错误行为触发的作用。

Result: MPTCS方法能挑选出覆盖多种典型错误、可跨政策复用且具备难度和多样性的测试用例。测试用例选取机制、参与政策数量以及多样性推动手段共同影响测试覆盖率与效果。

Conclusion: MPTCS能够生成多策略、可复用且具备多样性和普适难度的测试用例，从而揭示RL代理常见缺陷；其测试有效性与成本受到参与政策数量影响。

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [9] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: AI生成代码更简洁重复，但更易出现安全漏洞和未使用代码；人工代码结构复杂但维护难度大。AI和人工代码各有缺陷，AI辅助开发需专门的质量保障措施。


<details>
  <summary>Details</summary>
Motivation: AI代码助手在软件开发中的应用日益广泛，然而AI生成代码与人工代码在质量上的差异尚未得到系统研究。理解这种差异对于确保软件的可靠性、可维护性和安全性至关重要。

Method: 作者对比分析了人类开发者与三种先进大语言模型（ChatGPT、DeepSeek-Coder、Qwen-Coder）生成的代码，在代码缺陷、安全漏洞和结构复杂性等多个软件质量维度上进行大规模评测。研究涵盖50万条Python和Java代码样本，分别采用Orthogonal Defect Classification和Common Weakness Enumeration进行缺陷和安全分类。

Result: 研究发现，AI生成的代码通常更简单且较为重复，但更容易出现未使用的结构和硬编码调试内容；人类代码则结构更复杂且可维护性问题显著。值得注意的是，AI代码存在更多高风险安全漏洞。

Conclusion: AI与人工编写的代码在缺陷分布和安全隐患上有明显差异，AI辅助编程亟需针对性的质量保证措施以降低风险。

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [10] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本文通过访谈法，评估了敏捷方法在DevOps实践中的应用，发现敏捷在DevOps各阶段均具有融合潜力，提出了二者关系的新理论，为IT行业软件开发提供新参考。


<details>
  <summary>Details</summary>
Motivation: IT行业对快速交付及功能丰富的软件产品的需求日益增长，敏捷和DevOps逐步取代传统瀑布方法。本文旨在评估敏捷与DevOps实践，探讨敏捷方法在DevOps中的可行性与应用性。

Method: 采用半结构化访谈法，对IT行业中不同领域的敏捷与DevOps从业者进行访谈，并通过主题分析，提炼出51个独特编码并归纳为19个主题，分析了敏捷方法在DevOps生命周期各阶段的融合与应用。

Result: 确定了敏捷方法在DevOps实践中的可行性，深入分析了其在DevOps不同阶段的整合方式，并提出了新见解以指导未来软件开发实践。

Conclusion: 基于研究发现，本文提出了敏捷方法与DevOps实践相互关系的全新理解，满足了研究目标。

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [Interpolation for Converse PDL](https://arxiv.org/abs/2508.21485)
*Johannes Kloibhofer,Valentina Trucco Dalmas,Yde Venema*

Main category: cs.LO

TL;DR: 本文研究了带有反向操作的命题动态逻辑（Converse PDL），通过建立新的循环序列推理系统并改编已有证明方法，证明了该逻辑的插值和可定义性重要性质。


<details>
  <summary>Details</summary>
Motivation: 扩展命题动态逻辑，研究其在程序对偶操作下的插值性质与可定义性问题。

Method: 将 Maehara 的证明方法改编到 Converse PDL，提出了一个健全且完全的循环序列系统，并引入分析性割规则以及聚焦机制用于识别成功循环。

Result: 证明了 Converse PDL 的局部 Craig 插值性质和 Beth 可定义性性质，并为其构建了相应的序列推理系统。

Conclusion: Converse PDL 具有局部 Craig 插值性质，且满足 Beth 可定义性性质。

Abstract: Converse PDL is the extension of propositional dynamic logic with a converse
operation on programs. Our main result states that Converse PDL enjoys the
(local) Craig Interpolation Property, with respect to both atomic programs and
propositional variables. As a corollary we establish the Beth Definability
Property for the logic.
  Our interpolation proof is based on an adaptation of Maehara's
proof-theoretic method. For this purpose we introduce a sound and complete
cyclic sequent system for this logic. This calculus features an analytic cut
rule and uses a focus mechanism for recognising successful cycles.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: CoBA通过语义三元组转换实现反偏差数据增强，有效减少深度学习模型中的多种伪相关性并提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在训练过程中容易学习到伪相关性，并依赖于这些非目标特征做出决策，导致模型在未见数据上的泛化能力较差。

Method: 提出了一种更通用的反事实数据增强方法，称为counterbias数据增强（CoBA），通过将文本分解为主语-谓语-宾语的三元组，并对这些三元组进行选择性修改，以打破伪相关性，然后重构文本以生成反偏差数据。

Result: 实验表明，CoBA方法不仅提升了下游任务的性能，还有效减少了模型偏差，提高了模型在分布外样本上的鲁棒性。

Conclusion: CoBA为深度学习模型应对伪相关性带来的泛化挑战，提供了统一且有效的数据增强解决方案。

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [13] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 本研究推出首个德语大型带年龄标签的有害言论数据集，揭示了不同年龄群体在网络交流中的有害言论差异，为改进内容审核及语言变异研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有有害言论数据集缺乏人口统计背景，限制了对不同年龄层线上交流方式的理解。该研究旨在填补这一空白，帮助建立更公平且关注年龄差异的内容审核机制。

Method: 研究构建了德国首个大型带年龄估算的有害言论数据集，结合人工与语言模型注释，从Instagram、TikTok和YouTube平台收集评论，并按预定义有害关键词筛选。

Result: 最终数据集包含3,024条人工注释与30,024条LLM注释评论，并归类为侮辱、虚假信息和广播费用批评等关键类别。该数据集为跨年龄群体的语言变异和内容审核研究提供了新资源。

Conclusion: 该研究表明，不同年龄群体在网络交流中的有害言论存在明显差异。年轻群体更倾向于使用表达性语言，年长群体则更常出现虚假信息和贬低言论。

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [14] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite R2提出了一组企业级高性能英文嵌入模型，提升了上下文长度、检索速度和准确性，在多种任务与评测中表现优秀，适合开源企业实际部署和研究。


<details>
  <summary>Details</summary>
Motivation: 当前企业级密集检索需求日益增长，需要在不同场景（如文本、代码、长文档、多轮对话、表格数据等）中兼具高性能、速度和数据治理合规的开源嵌入模型。现有模型在上下文处理长度、速度和准确性等方面存在一定局限。

Method: 提出Granite Embedding R2家族模型，涵盖bi-encoder和cross-encoder结构，包含高效的22层和12层retriever模型以及高质量reranker模型，全部基于企业合规数据集训练；上下文长度扩展到8192 tokens，并在多个检索领域下进行了性能和速度评测。

Result: Granite R2模型在同类检索任务上，达到了新一代开源嵌入模型的性能标杆：与主流竞品相较，速度提升19-44%，保持更优准确性，并在标准和IBM自有评测集、实际企业场景下表现优异。

Conclusion: Granite Embedding R2模型兼具卓越的检索性能、企业就绪的合规性和透明数据溯源，适合关键企业级应用，并以Apache 2.0协议开源，支持学术和商用。

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [15] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 本文提出了Transformer架构的手写笔迹生成模型TrInk，通过创新的跨注意力机制与合理的评估流程，大幅提升了手写文本生成的准确率与风格表现。


<details>
  <summary>Details</summary>
Motivation: 现有的手写生成模型在准确表达输入文本与生成笔迹的对齐性和风格一致性上存在不足，并且难以捕捉全局依赖。

Method: 提出了一种基于Transformer的笔迹生成模型TrInk。该模型在跨注意力模块中引入了缩放位置嵌入与高斯记忆掩码，以提升文本与生成笔迹点的对齐效果。同时设计了主观和客观的评估流程综合评价生成手写的可读性与风格一致性。

Result: 在IAM-OnDB数据集上，TrInk相较于先前方法在字符错误率（CER）上降低了35.56%，在单词错误率（WER）上降低了29.66%。

Conclusion: TrInk显著提升了生成手写笔迹的准确性与风格一致性，在手写识别任务中有效优于以往方法。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [16] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: LLM在价格谈判任务中会受到锚定效应影响，但增强推理性可降低影响，人格特质与之无关，这对安全应用LLM有借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，大型语言模型（LLMs）可能像人类一样受到认知偏差影响，这影响了它们的可靠性。文章特别关注于“锚定效应”在价格谈判中的作用。

Method: 设计了以卖方身份的LLM代理，明确要求其应用锚定效应，并通过客观和主观指标评估其谈判表现。同时探讨了推理能力和人格特质与锚定效应的关系。

Result: 实验显示，LLMs会像人类一样受到锚定效应影响。具备较强推理能力的模型较少受锚定效应影响，而人格特质与易受锚定效应影响之间并无显著关联。

Conclusion: 推理能力能够缓解LLM中的锚定效应，这有助于理解和应对认知偏差，推动LLM安全、负责任地应用于社会。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [17] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: MLLMs在基础感知任务上的表现远逊于其在复杂推理任务的能力，任务复杂度越高表现越差，亟需提升其基本视觉认知能力。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态大语言模型（MLLMs）在代码、数学与科学等领域取得显著进展，但针对其在基础感知任务（如识别基本几何图形与结构）上的能力研究较少。本文为弥补这一不足，评估MLLMs在简单、无噪声生成图像上的感知能力。

Method: 作者新构建了名为Percept-V的数据集，包括7200张程序生成的基础视觉图像，涵盖30类不同的感知任务。利用该数据集，对GPT-4o、Gemini、Claude等最新MLLM以及OpenAI o4-mini、DeepSeek R1等大型推理模型在这些感知任务上的表现进行系统评测。

Result: 实验结果显示，尽管MLLMs在复杂任务上表现优异，但在基础感知任务中，随着任务复杂度增加，其表现显著下降。不同模型在同类感知技能测试中准确率具有相同比例的趋势，某些感知技能任务相对较难。

Conclusion: 当前多模态大语言模型在基础视觉感知任务中的能力有限，复杂度提升时性能大幅下降。该结果提示，MLLMs的视觉感知能力亟需进一步提升，Percept-V为相关研究提供了公正的新基准。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [18] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 综述了以数据为核心的科学大模型发展历程，分析数据挑战，提出分类法与知识结构，盘点主流模型与数据集，指出持续进化、可信赖AI系统新方向。


<details>
  <summary>Details</summary>
Motivation: 当前科学数据复杂、异质、跨尺度、多模态，传统NLP模型难以满足科学研究需求，因此亟需以数据为核心，重塑Sci-LLMs发展路径。

Method: 本文通过文献综述、数据分析、提出科学数据分类法和知识层次模型，系统梳理了Sci-LLMs与科学数据的关系，评估了大量数据集和评测协议。

Result: 统一科学数据分类与知识结构，收集分析270+数据集与190+评测基准，揭示Sci-LLMs需求与挑战，介绍半自动注释、专家验证等新方法，以及向自主实验、知识库持续进化的闭环系统转变的趋势。

Conclusion: Sci-LLMs的发展需要以科学数据为中心，强调模型与数据的协同演进，将推动构建可信赖、可持续进化的AI系统助力科学发现。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [19] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 简单讲，模型的“品牌”标签会明显影响评分结果，不论内容好坏。为减少偏见，未来的LLM评测应采用盲评或多模型协作的方式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）越来越多地被用于输出结果评价，但其判断可能受到偏见影响。本研究旨在探究不同模型（ChatGPT、Gemini和Claude）自我与交叉评价中的偏见问题。

Method: 选取由三种大语言模型（ChatGPT、Gemini、Claude）生成的博客文章，并由上述三种模型在四种条件下（无标签、真实标签、两种错误标签）互评。评价方式包括总体偏好投票及针对连贯性、信息量和简洁性的质量评分，所有分数均标准化为百分比以便直接比较。

Result: 发现明显的非对称性偏见：仅凭“Claude”标签评分普遍提升，而“Gemini”标签评分普遍降低，与实际内容无关。错误标签能导致排名逆转，最高偏好票差达50个百分点，质量评分变化可达12个百分点。在真实标签下，Gemini自评分骤降，Claude自评分激增。

Conclusion: 模型身份认知会严重扭曲高级判断，并微妙影响详细质量评分。为保证LLM评测公平性，必须采用盲评或多模型评价方案。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [20] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 创新性地将贝叶斯实验设计原理引入大模型问答流程，提出能主动高效搜集信息的BED-LLM方法，在多项任务中领先现有策略，提升了LLM的交互与推理能力。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型（LLM）智能自适应获取用户或外部信息的能力，使其成为更有效的多轮对话代理与环境交互接口。

Method: 提出BED-LLM方法：采用贝叶斯实验设计（BED）框架，基于最大化期望信息增益（EIG），通过迭代选择能最大增益关键信息的问题或查询，结合LLM的概率分布和创新性的EIG估算器、条件机制优化与候选问题提出策略。

Result: BED-LLM在20问游戏、用户偏好主动推断等测试场景下，相较直接prompt与其他自适应方法均有显著性能提升。

Conclusion: 基于贝叶斯实验设计的BED-LLM极大提升了LLM在多轮问答和主动信息搜集任务中的表现。

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [21] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 通过强化学习（GRPO）优化语言模型，对航空人为因素自动分类取得显著准确率提升，表现超过现有顶尖大模型，具备高效边缘部署价值。


<details>
  <summary>Details</summary>
Motivation: 分析航空事故背后的人为因素对于预防未来事故至关重要，但传统HFACS方法在可扩展性与一致性方面存在局限。为改善航空安全分析的自动化与精度，亟需新的方法。

Method: 提出一种基于强化学习与GRPO优化的HFACS自动分类框架，通过对Llama-3.1 8B语言模型微调，结合多组件奖励系统和合成数据生成以平衡数据集。

Result: GRPO优化模型在精确匹配准确率上提升了350%（从0.0400提升至0.1800），部分匹配准确率达到0.8800，模型在关键指标上优于GPT-5-mini与Gemini-2.5-fiash等SOTA大模型。

Conclusion: 经过特别训练的小型、领域优化模型在航空安全分析中表现优越，具备高效、低延迟部署于边缘设备的潜力。同时提出新的评估基准，有助于评估语言模型的高级推理能力。

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [22] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 本论文提出用像素级图片表示词语来替代传统文本嵌入，大幅提升了语言模型对拼写攻击和多语种输入的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自回归语言模型在面对拼写变体（如跨语言字母掺杂）时性能大幅下降，主要原因在于分词器和嵌入层的词汇表覆盖有限。

Method: 将词汇渲染为单独的图像，利用像素级表示代替原本的文本嵌入，并在多个多语言和鲁棒性数据集上进行评估。

Result: 在LAMBADA、WMT24及SST-2等数据集上，方法表现出对输入噪声的更高鲁棒性以及良好的多语言适应能力。

Conclusion: 基于像素的生成式语言模型在应对拼写攻击和多语言文本方面具有更强的鲁棒性，实验验证了其有效性和通用性。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [23] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本研究发现自监督语音模型未展现语言习得关键期效应，延迟第二语言学习反提升表现，延迟第一语言学习却加速遗忘。


<details>
  <summary>Details</summary>
Motivation: 尽管关键期效应已在文本语言模型中得到研究，但其在语音模型中的表现依然缺乏探讨，而口语在语言习得中具有核心作用。

Method: 通过改变模型的第二语言（L2）训练起始时间和第一语言（L1）训练终止时间，并在儿童语音语料上训练模型，随后评估其音素区分能力。

Result: 模型未出现关键期效应：延迟L2暴露反而提升L2表现，延迟L1暴露则导致L1遗忘。

Conclusion: 自监督语音模型在语音领域未呈现出明显的关键期效应。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [24] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 论文发现自洽方法中生成答案存在冗余，提出DMP方法，大幅提升效率，性能无损。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型在句子级生成时仍然容易出现幻觉，且目前的检测方法在该场景下表现不佳或依赖大量领域知识。自洽方法虽能一定程度解决问题，但计算成本高。

Method: 首次分析了自洽生成答案中冗余信息（即不同答案间的共享前缀token），发现非准确答案的token对语义贡献低。基于此，提出了Decoding Memory Pipeline（DMP）——通过选择性推理和退火解码来加速生成过程。

Result: DMP方法在多响应生成任务中，在不降低AUROC性能的前提下，实现了最高3倍的加速。

Conclusion: DMP是一种与模型、数据集、解码策略及自洽方法均正交的加速方案，可扩展用于对齐与推理任务，在提升效率的同时保证检测性能。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [25] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: jina-code-embeddings是一套高效新颖的代码嵌入模型，利用预训练自回归骨干与last-token pooling，在代码检索与相似性任务上实现了高性能，模型相对较小但效果领先。


<details>
  <summary>Details</summary>
Motivation: 目前代码嵌入模型在从自然语言检索代码、技术问答、跨语言语义相似性识别等任务中仍有提升空间。作者希望通过更高效的模型结构和训练方法提升性能。

Method: 提出了一套新颖的代码嵌入模型——jina-code-embeddings。采用预训练的自回归骨干网络，训练时结合文本与代码，通过最后一个token进行池化生成嵌入向量，凝练地捕捉代码语义。

Result: 尽管模型体积较小，该方法在多项代码相关任务中取得了领先性能，证明了模型结构和训练方案的有效性。

Conclusion: 小规模、高效的自回归模型结合独特的池化方式，能在代码嵌入和检索任务上取得卓越表现，为代码表示学习带来新的解决思路。

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [26] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: 该论文提出了更新后的BLUEX数据集，引入了最新考试题与由先进模型自动生成的图片说明，将可用问题数量翻倍，并评估了LLMs利用caption进行视觉推理的能力，有助于提升LLMs的评估方法和数据多样性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的提升，尤其是在多语言和非英语环境下，对其进行可靠评估的方法需求不断增加。

Method: 更新了BLUEX数据集，纳入了2024-2025年的考试题目，并使用最新的图像生成模型自动生成图片说明（captions），还采用了不同的caption策略。随后评估了商业和开源LLMs利用caption进行视觉语境推理的能力。

Result: caption策略将可访问的文本问题数量提升了40%以上，得到1422个可用问题，数量是原始BLUEX的两倍多。

Conclusion: 对于LLMs在多语言环境下的评估，改进的数据集和caption策略极大丰富了问题多样性和评测的广度，提高了数据可用性，也更有利于研究数据污染对LLMs预训练的影响。

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [27] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 本文综述了LLM发展的16项关键挑战，通过GPT-4o与DeepSeek-V3-0324的对比，揭示了封闭源与开源模型在安全与适应性上的权衡，并分析了不同应用场景下模型选择的策略，为AI从业者理解和应用LLM提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）广泛应用于各行各业，但开发和部署过程仍然复杂。本文旨在梳理其中的关键挑战，为相关领域研究人员和决策者提供指导。

Method: 本综述论文总结了构建与应用LLM的16项主要挑战，以两个具代表性的模型（OpenAI的封闭源GPT-4o与DeepSeek-V3-0324开源混合专家模型）为例，对比了它们在应对挑战上的不同策略。

Result: 论文梳理了封闭源模型（如GPT-4o）在安全性和可靠性调优上的优势，以及开源模型（如DeepSeek-V3-0324）在高效性与适应性上的特色，并阐述这些模型在不同应用场景（如聊天机器人、编程工具、医疗、教育）中的优劣。

Conclusion: 文章总结了当前LLM的能力、局限性及最佳实践，为AI研究人员和应用开发者提供了模型选择与应用部署的参考。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [28] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 文章通过统计意义的“正常”概念重释图灵测试，提出图灵测试关心的是普通智能而非超常智能，因此大模型难以真正通过。文章质疑了仅用常态智能解释人类心智的合理性，并指出大模型展现的是“人工聪明”而非严格意义上的人工智能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过“常态性”来重新审视图灵测试，质疑当前对于智能的评判标准，并回应人工智能发展（如ChatGPT）与图灵测试的关系。

Method: 作者采用概念分析和理论论证方法，通过统计学上“正常”或“平均”含义的讨论，分析图灵测试及其评判标准，结合对大语言模型（如ChatGPT）能力的分析，提出理论区分。

Result: 论文指出，图灵测试实质上是考察“普通/平均”人类智能，由多名评判者形成的集体判断决定。一方面合格机器需表现出像真实人中常见的错误与不完美，而非追求完美的过人表现；另一方面，当前的大语言模型偏向模拟超常智能而非常态智能，因此难以真正通过图灵测试。

Conclusion: 论文认为，图灵测试实际上考察的是常态智能而非超常智能，因此当前大语言模型（如ChatGPT）属于“人工聪明”而非真正意义上的“人工智能”。同时，文章进一步发问，若人类思维本身无法还原为常态化的平均智能，则图灵测试对理解人类认知的贡献也需重新评估，此议题超越图灵测试本身，涉及更广泛的认知与常态范式基础。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [29] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 该论文发现当前自动文本摘要评估面临重大可复现性难题，特别是新兴LLM方法。通过统一框架和实验，揭示了评估指标在效能、人类一致性与计算稳定性之间的结构性权衡，呼吁行业采取更可靠和标准化的评测协议。


<details>
  <summary>Details</summary>
Motivation: 近年来大量文本摘要评估方法涌现，尤其是依赖于大模型的自动评测，但文献报告的性能往往缺乏可复现性；推动行业对于评测可靠性和可比性的关注。

Method: 通过在SummEval数据集上，对六种代表性指标（从传统如ROUGE到最新LLM指标如G-Eval、SEval-Ex）进行实验，分析它们在人类一致性和计算稳定性上的表现，同时提出统一的开源框架支持公平透明的比对。

Result: 发现与人类评判高度一致的指标通常计算开销大且稳定性差；LLM评测受到随机性、技术依赖和难以复现等问题困扰。提出需要加强评测流程的文档记录和方法标准化。

Conclusion: 本文强调了当前自动文本摘要评估在可复现性上的主要挑战，尤其是依赖LLM的方法随机性强且复现困难，建议实行更严格的评测协议和标准化方法以增强可靠性。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [30] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文发现主流大语言模型自动化学术评审工具对论文逻辑错误不敏感，难以识别相关缺陷，作者据此提出改进建议并开放了评测数据集与框架。


<details>
  <summary>Details</summary>
Motivation: 近年来，大语言模型（LLMs）被越来越多地用于自动化生成学术论文的评审（ARGs），但这些工具可能存在系统性偏差和错误，威胁科学诚信。因此，了解当前SOTA自动评审工具的优缺点十分重要。本文聚焦于高质量同行评审的核心技能之一：检测研究逻辑的错误（即评估论文结果、解释与主张间的内在一致性）。

Method: 作者提出了一个全自动逆事实评估框架，能在受控条件下隔离并测试自动评审生成器在检测研究逻辑错误方面的能力。

Result: 通过对多种自动化评审方法进行测试，研究发现这些工具在论文逻辑存在缺陷时，其生成的评审结果并未受到显著影响。

Conclusion: 自动评审工具目前尚不能有效识别论文中的研究逻辑错误，这对学术评审实践提出了警示。论文根据实验结果，提出了三条有实际指导意义的后续研究建议，并公开了逆事实数据集和评估框架。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [31] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 该论文提出了医学领域首个奖励模型和裁判机制评价基准Med-RewardBench，覆盖多系统多科室，严控数据质量。评估32种主流模型发现专业对齐仍存挑战，微调可带来显著性能提升。该工作填补了医学模型评判方面的空白。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医学领域表现出巨大潜力，但医学任务对模型的准确性、专业性和语境敏感度要求极高，然而目前医学奖励模型和评判机制研究不足，缺乏专门用于临床需求评价的基准。现有基准聚焦于通用能力或问题解决，忽视诊断准确性和临床相关性等重要维度。

Method: 提出了Med-RewardBench，这是一套专门用于医学场景下评价奖励模型和评判机制的基准，包括跨13个系统和8个科室的多模态数据集，共有1026个专家标注案例，通过三步流程在六个临床维度保证数据质量。对32个先进的多模态语言模型进行评估，并开发了通过微调显著提升性能的基线模型。

Result: 揭示了当前领域模型与专家判断对齐存在显著挑战。微调后的基线模型在评价任务上取得了明显的性能提升。

Conclusion: Med-RewardBench为医学奖励模型和裁判机制的研究提供了首个专用基准，促进了模型输出与临床专家判断的对齐，对推动医学AI的可靠应用具有重要意义。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [32] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出了一种新模型（DCSRM），能将词嵌入分解为有解释力的语义子维度，并证实这些子维度具有认知与脑神经合理性，为语义表征研究带来更精细的工具。


<details>
  <summary>Details</summary>
Motivation: 理解语言和大脑中语义组织的核心维度是认知研究的重要问题，但现有方法多依赖预设粗略语义维度，难以捕捉细致的概念区分。为解决这一问题，需要一种能揭示粗粒度维度下更细致语义结构的创新方法。

Method: 提出了“可分离连续语义表示模型”（DCSRM），可以将大语言模型中词嵌入分解为多个子嵌入，每个子嵌入编码特定语义信息。通过这些子嵌入，作者识别并解释了一系列语义子维度，并用体素编码模型将子维度与脑激活进行关联分析，以评估其神经合理性。

Result: 获得了更细致且可解释的语义子维度，发现这些维度根据不同原则结构化，极性是驱动其分解为子维度的关键因素。已识别子维度的神经相关性支持了其认知和神经科学上的合理性。

Conclusion: 该方法能够实现对词语的语义表示进行细粒度分解，并找到具有解释性和神经相关性的语义子维度，为理解语义在语言和大脑中的结构提供了新视角。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [33] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 本文分析大语言模型的意识形态深度及可操控性。通过提示和激活操控发现，部分模型政治立场易变，部分则更坚守。内部机制分析显示，深层模型政治特征丰富且抽象，干预后表现出逻辑一致性；浅层模型则容易拒绝作答。意识形态深度具有可衡量性，操控性是理解模型政治倾向的重要线索。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）表现出明显的意识形态倾向，但其稳定性和深度尚不清楚。例如，通过简单的提示设计可以操控模型的表层回答，这使得其是否有一致的底层意识形态受到质疑。论文试图解决LLM内部“意识形态深度”定义及衡量问题。

Method: 本文采用双重方法：1）通过指令提示和激活操控测量两个开源LLM的“可操控性”；2）利用稀疏自编码器（SAE）探查模型内部机制，分析其政治特征的抽象与分布。

Result: 部分模型能够轻松在自由派和保守派观点间切换，其他模型则表现出更高的抵抗或拒绝率，暗示更深层的意识形态结构。模型可操控性越低，内部具有更抽象和分明的政治特征。同尺寸下，一个模型政治特征数量为另一个模型的7.3倍。针对“深层”模型核心政治特征的消融使其推理逻辑发生一致改变，“浅层”模型则增加拒绝输出。

Conclusion: 意识形态深度是LLM可量化的属性，可操控性为理解其潜在政治结构提供了窗口。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [34] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 基于RLAIF框架使用AI奖励信号有效提升了小语言模型在中文祝福语创意写作中的表现，尤其是创新的LLM-as-a-Judge方法在生成质量和训练效率方面优于传统手段。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）虽然具备极强的创意写作能力，但算力开销很大，难以广泛应用。提升小语言模型（SLM）的能力是一个有前景的方向，但现有方法如SFT在生成新颖内容方面能力欠缺，而RLHF又训练成本高。

Method: 提出了在AI反馈强化学习（RLAIF）框架下，两种用于激发7B参数小语言模型创造性写作（生成中文祝福语）的AI奖励方案：1. 基于多智能体拒绝采样机制获得高质量偏好数据训练的奖励模型（RM）；2. 创新的原理驱动型LLM-as-a-Judge，通过对抗训练和反思机制优化其奖励函数，直接产生奖励信号。

Result: 两种方法都极大提升了SLM在创意写作上的表现，其中原理驱动型LLM-as-a-Judge在生成质量、训练效率和对人工标注依赖上效果更佳。此外，自动化评测方法与人工评判高度一致。

Conclusion: 原理驱动LLM-as-a-Judge方案实现了更高效、低成本且可扩展的创造性SLM训练，并为自动化评价体系提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [35] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 针对假新闻传播，作者提出一种重视分类器多样性和性能的新自动选取方法，并在多数据集实验中取得较优结果，展示了集成模型构建的创新思路。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的假新闻由于心理偏差（如确认偏误）而易于传播，带来了健康和政治等领域的重大后果。已有研究尝试用机器学习进行事实核查，其中集成方法效果突出，但集成效果高度依赖于分类器的多样性，如何自动选出真正多样的分类器仍是一大挑战。

Method: 本文提出一种新的自动分类器选择方法，以多样性为优先，同时兼顾性能。方法首先计算各分类器间的成对多样性，利用层 次聚类分组，再通过HierarchySelect遍历不同层次组织、选取具有不同组内多样性的分类器池，最终挑选最具多样性的池进行集成。整个过程还引入性能评价指标，确保集成模型的泛化能力。

Result: 在六个不同领域的数据集（共40个异质分类器）上与Elbow启发式和其它主流方法对比，所提方法在两个数据集上获得了最高准确率。

Conclusion: 该方法能更有效地选出多样且高性能的分类器集成以提升假新闻检测等任务的准确率。

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [36] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 文章提出了马拉地语句子相似性数据集（MahaSTS）和基于微调的MahaSBERT-STS-v2模型，显著提升了马拉地语句子相似性任务效果，为低资源马拉地语NLP领域提供了关键支撑。


<details>
  <summary>Details</summary>
Motivation: 马拉地语是一种低资源语言，缺乏句子相似性任务的高质量数据集和模型。现有数据和模型无法有效支持马拉地语的文本匹配与相似度评分需求。作者试图解决这一空白，以促进马拉地语相关NLP应用发展。

Method: 作者人工标注了16860个马拉地语句子对，分别赋予0-5之间的连续相似度分数，并均匀分布在6个分数桶中。基于该数据集，作者对Sentence-BERT模型进行了回归任务微调，生成了“MahSBERT-STS-v2”模型，并与MahaBERT、MuRIL、IndicBERT和IndicSBERT等模型进行了对比评测。

Result: 实验结果表明，基于上述数据集微调得到的MahSBERT-STS-v2模型在马拉地语文本相似性任务上效果优异。均衡分布与高质量人工标注提升了模型稳定性和泛化能力。

Conclusion: 该论文推出了马拉地语句子相似性标注数据集和高性能微调模型，为低资源语言的文本相似性任务提供了基础资源和方法，推动了马拉地语NLP的发展。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [37] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文面向文本数据匿名化技术进行全面综述，涵盖基础与前沿方法，重点分析LLM带来的新机遇和隐患，并指出隐私保护与数据效用之间的核心挑战，为后续研究和实际部署提供参考。


<details>
  <summary>Details</summary>
Motivation: 大量含敏感信息的文本数据在多个领域广泛存在，亟需兼顾隐私保护与数据可用性的高效匿名化技术。学界和工业界面临法规要求和实际应用需求，推动对匿名化方法深入系统的梳理。

Method: 综述方法，包括文献调研、技术梳理与领域专题分析，覆盖了基础的命名实体识别方法、LLM应用、领域特定解决方案，以及评估框架和工具包。

Result: 系统回顾了现有文本匿名化技术及其在医疗、法律、金融和教育等关键领域的实际应用，探讨了LLM对匿名化的促进作用及其带来的去匿名化风险。归纳了当前评估体系、指标和工具，总结了面临的挑战和未来趋势，提出了改进方向。

Conclusion: 本文总结了文本匿名化领域的最新研究进展，明确了大语言模型（LLM）在提高匿名化和挑战隐私安全方面的双重角色，并指出了当前在隐私-效用权衡、准标识符处理及LLM影响等方面仍存在重大挑战，为今后的学术和实际工作提供了研究方向。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [38] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出了一种结合诊断和优化的动态数据优化框架Middo，能够持续提升大语言模型训练效果，在多个基准上模型准确率平均提升7.15%。


<details>
  <summary>Details</summary>
Motivation: 受监督微调（SFT）需要高质量数据，但当前方法在静态数据集创建方面难以适应模型不断变化的能力，因此提出新方法以动态优化训练数据。

Method: 提出了Middo框架，包括：1）通过三轴模型信号（损失、嵌入聚类、模型对齐分数）诊断低质样本，2）自适应优化引擎将这些低质样本优化为有教学价值的数据，3）这一流程随着模型能力动态演化，实现闭环持续优化。

Result: 在多个基准测试中，Middo提升了种子数据质量，使大型语言模型性能平均提高7.15%，且不增加原始数据集规模。

Conclusion: 通过Middo动态闭环优化方法，可以可持续提升大语言模型训练的数据质量和模型性能。

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [39] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 不同性格的用户对于GPT-4与Claude 3.5的模型表现存在明显偏好。理性型偏好GPT-4，理想型偏好Claude 3.5，且偏好随任务类型而变化，情感分析证实这一趋势。传统整体评价会掩盖这些与用户个性相关的模型差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）已逐渐融入日常工作流程，但用户因个性差异，对不同模型的偏好是否存在系统性差异尚不清楚。本研究旨在探索个性特质如何影响用户对LLM的选择与偏好。

Method: 招募了32名参与者，根据四种Keirsey性格类型平均分组。每人分别与GPT-4和Claude 3.5模型在四类协作任务（数据分析、创意写作、信息检索、写作辅助）中互动，收集用户反馈及任务表现，并进行情感分析。

Result: 理性型用户明显更偏好GPT-4，尤其在目标导向任务上；理想主义者则偏好Claude 3.5，特别是在创意和分析性任务上。其他类型用户的偏好则依赖具体任务。情感分析进一步证实了这些模式。

Conclusion: 用户个性对LLM模型偏好有显著影响，而这种差异在聚合性评估下常常会被掩盖。对LLM差异的理解应结合个性维度，以发现更加细致的差异性。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [40] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: QZhou-Embedding模型通过创新的数据合成和两阶段训练策略，利用大模型数据生成能力，在主流基准任务上取得最优成绩，推动文本嵌入领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型在处理多任务和复杂语义时存在性能瓶颈。本文旨在通过改进数据构建和训练方法，充分利用大语言模型，通过提升数据多样性和难度，推动文本嵌入模型能力升级。

Method: 以Qwen2.5-7B-Instruct为基础，采用统一多任务框架，结合专用数据转换和训练策略。数据转换方案支持多样化数据集；训练策略提升模型学习效率；通过LLM API实施数据合成，包含同义改写、增强、困难负样本生成等技术。训练分为两阶段：先进行检索为主的预训练，后进行全任务微调。

Result: QZhou-Embedding模型在MTEB和CMTEB基准上均排名第一（2025年8月27日），同时在重排序、聚类等任务上也获得了最优结果。模型及代码均已开源，可复现。

Conclusion: 高质量、多样化的数据对于检索模型性能提升至关重要，利用大模型生成能力进一步优化训练数据能带来嵌入模型的新突破。所提出的QZhou-Embedding模型在多项主流文本嵌入任务基准上取得了领先成绩。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [41] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文发布了一个大规模、开放的误导性可视化基准数据集（含真实和合成样本）、评测了主流AI方法，发现该任务仍很困难，并已公开全部资源以推动研究进展。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和网络上的误导性可视化促进了错误信息的传播，但当前自动检测这类可视化的AI模型因缺乏大规模、开放、多样的数据集而受限。

Method: 提出并公开了Misviz基准数据集，包含2,604个真实世界的带注释误导性可视化图表（涵盖12种误导方式），以及基于真实数据表用Matplotlib生成的合成数据集Misviz-synth（81,814个样本）。随后用最先进的多模态大模型、基于规则的系统和微调分类器对这些数据集进行全面评估。

Result: 评估结果显示，即使最先进的模型在检测和识别误导性可视化违规设计方面仍有很大挑战。所有数据集及代码都已开源，供后续研究使用。

Conclusion: 该工作为误导性可视化检测提供了重要数据资源和基准，但相关模型在该任务上仍面临显著困难，期待推动后续更有效的方法研究。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [42] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 提出参数区隔离和融合新方法，有效解决SFT多任务微调的干扰与遗忘难题，实验优于主流基线。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM监督微调方法（SFT）在多任务适配时经常出现“跷跷板现象”：对某任务性能提升时，往往会损害其他任务。作者希望解决这一任务干扰问题。

Method: 提出了Core Parameter Isolation Fine-Tuning（CPI-FT）框架，具体方法包括：a) 首先对每个任务独立微调LLM，通过参数更新幅度提取其核心参数区；b) 根据多任务间核心参数区的重叠度分组，实现任务聚类联合建模；c) 引入参数融合技术，将每个任务独立微调得到的核心参数直接移植到统一骨干网络，非核心参数通过球面线性插值（SLERP）平滑融合；d) 混合任务数据继续轻量训练，并冻结已得到的核心区，防止遗忘。

Result: 实验显示，该方法在多个公开多任务基准数据集上显著减少了任务干扰和遗忘问题，并且在性能上优于传统的多任务和多阶段微调方法。

Conclusion: CPI-FT有效隔离并融合多任务核心参数区，缓解了“跷跷板现象”和灾难性遗忘，提升了LLM多任务微调表现。

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [43] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: 本文针对推理密集型回归任务（RiR），提出了MENTAT方法，显著提升了性能，验证了目前主流大模型和微调方法在此类任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理密集型回归（RiR）任务上的表现存在不足，因为这类任务通常涉及对文本进行深入分析，并且可用的数据和计算资源有限。而这类任务在实际中（如细致评分、特定领域检索）并不少见，需要有更合适的解决方法。

Method: 该文提出了一个新的方法MENTAT：它结合了批量反思性提示优化（batch-reflective prompt optimization）与神经集成学习（neural ensemble learning），用于提升LLM在RiR任务的表现。文中还将三个现实问题转化为RiR任务，对常用的冻结LLM提示和Transformer微调方法进行基准测试。

Result: 实验表明，MENTAT方法在对应基准任务上比冻结LLM提示和Transformer微调方法有显著提升，最多可提升65%，但仍有进一步提升的空间。

Conclusion: 现有的LLM和微调方法在RiR任务上表现有限。提出的MENTAT方法显著改善了性能，但RiR仍是一个挑战性很大的研究方向。

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [44] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: 提出了PiCSAR方法，能通过联合对数似然打分高效自动选出正确推理链，不仅精度明显提升，还比现有方法更省样本。


<details>
  <summary>Details</summary>
Motivation: 当前最佳采样方法常依赖于手工或额外训练的打分函数，难以在没有真实标签的情况下自动选出正确推理链。因此，亟需一种简单且无需训练的候选选择机制。

Method: 提出了一种无需训练的新方法——Probabilistic Confidence Selection And Ranking (PiCSAR)。该方法通过联合推理链与最终答案的对数似然分数对每个候选采样进行打分，从而在多候选生成中选择最优。

Result: PiCSAR在多个基准测试中取得了显著精度提升，如在MATH500提升10.18分，在AIME2025提升9.81分，并且在16/20组实验中仅用一半样本量获得更优结果。

Conclusion: PiCSAR能够有效提升最佳采样方法在大型语言模型和推理模型上的准确率。在绝大多数基准测试中，使用更少的候选样本即超过了现有方法的表现，证明了其高效性和有效性。

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [45] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 提出了基于ElasticSearch的分析框架，可实时检索与分析大规模训练数据，有助于提升数据安全与质量，为AI系统的高质量发展提供保障。


<details>
  <summary>Details</summary>
Motivation: 现今大型语言模型（LLM）主要依赖于像Common Crawl这样大规模的网页数据，但这类数据获取方式存在数据质量、安全与伦理等问题。已有关于有害内容的研究受限于计算资源，只能分析小样本。本文旨在解决对海量训练数据进行有效和实时分析的难题。

Method: 提出了一个基于ElasticSearch的数据集索引与分析框架，并将其应用于SwissAI的FineWeb-2语料库（1.5TB，涵盖四种语言），实现了高效的查询管道。

Result: 绝大多数搜索可以在毫秒级响应，所有查询都在2秒内完成。系统在高效检索和分析大规模语料上的性能表现优异。

Conclusion: 本文框架实现了实时、高效的数据集分析，为提升AI系统的安全性和问责性提供了实用工具。

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [46] [Constructive l2-Discrepancy Minimization with Additive Deviations](https://arxiv.org/abs/2508.21423)
*Kunal Dutta*

Main category: cs.DM

TL;DR: 文章针对高维向量符号序列的问题，提出了一个多项式时间的随机算法，首次实现了最优的$O(\sqrt{d}+\log n)$构造界，同时也优化了解Steinitz问题的相关界，解决了该领域两个关键猜想。创新点在于引入线性/谱正交约束和高级浓度不等式分析。


<details>
  <summary>Details</summary>
Motivation: 之前关于签名序列问题及其Steinitz问题的构造界未达最优。已知的非构造结果界为$O(\sqrt{d+\log n})$，而最优构造算法只能达到$O(\sqrt{d\log n})$，两者在高维或大$n$情况下相差明显。希望找到理论最优的构造算法以封闭该领域的重要猜想。

Method: 算法基于Bansal和Garg的框架，并引入了线性和谱正交约束以控制随机游走中协方差矩阵的变化，同时使用了依赖过滤的Hanson-Wright浓度不等式的“Freedman-like”版本来分析二阶涨量。该算法为多项式时间的随机算法。

Result: 提出了一个多项式时间的随机算法，能够找到取$\pm1$的符号，使最大部分和的$\u000bl_2$范数为$O(\sqrt{d+\log^2 n})$，即$O(\sqrt{d}+\log n)$。此结果也通过已有的归约方法，优化了Steinitz问题的界。对于$d\geq\log^2 n$时，完全解决了两个长期开放的猜想。

Conclusion: 本文提出的算法为签名序列问题（signed series problem）和Steinitz问题在$l_2$范数下给出了最优的构造界，为$O(\sqrt{d}+\log n)$，并且在$d\geq\log^2 n$时解决了两个长期猜想。

Abstract: The \emph{signed series} problem in the $\ell_2$ norm asks, given set of
vectors $v_1,\ldots,v_n\in \mathbf{R}^d$ having at most unit $\ell_2$ norm,
does there always exist a series $(\varepsilon_i)_{i\in [n]}$ of $\pm 1$ signs
such that for all $i\in [n]$, $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i
v_i\|_2 = O(\sqrt{d})$. A result of Banaszczyk [2012, \emph{Rand. Struct.
Alg.}] states that there exist signs $\varepsilon_i\in \{-1,1\},\; i\in [n]$
such that $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i v_i\|_2 =
O(\sqrt{d+\log n})$. The best constructive bound known so far is of
$O(\sqrt{d\log n})$, by Bansal and Garg [2017, \emph{STOC.}, 2019, \emph{SIAM
J. Comput.}]. We give a polynomial-time randomized algorithm to find signs
$x(i) \in \{-1,1\},\; i\in [n]$ such that \[ \max_{i\in [n]} \|\sum_{j=1}^i
x(i)v_i\|_2 = O(\sqrt{d + \log^2 n}) = O(\sqrt{d}+\log n).\] By the
constructive reduction of Harvey and Samadi [\emph{COLT}, 2014], this also
yields a constructive bound of $O(\sqrt{d}+\log n)$ for the Steinitz problem in
the $\ell_2$-norm. Thus, our result settles both conjectures when $d \geq
\log^2n$. Our algorithm is based on the framework on Bansal and Garg, together
with a new analysis involving $(i)$ additional linear and spectral
orthogonality constraints during the construction of the covariance matrix of
the random walk steps, which allow us to control the quadratic variation in the
linear as well as the quadratic components of the discrepancy increment vector,
alongwith $(ii)$ a ``Freedman-like" version of the Hanson-Wright concentration
inequality, for filtration-dependent sums of subgaussian chaoses.

</details>
