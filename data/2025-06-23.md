<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 78]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures](https://arxiv.org/abs/2506.15875)
*Dirk Van Essendelft,Patrick Wingo,Terry Jordan,Ryan Smith,Wissam Saidi*

Main category: cs.PL

TL;DR: MACH 编译器为高层代码到空间数据流硬件的高效映射提供了统一框架，简化了复杂空间架构的编译流程，对高性能计算和 AI 工作负载有实用价值。


<details>
  <summary>Details</summary>
Motivation: 空间数据流架构（如 Wafer Scale Engine）因其编程和编译复杂度高，使得高效利用这类硬件变得困难。缺乏统一、灵活的编译工具限制了其广泛应用。

Method: 提出了面向大规模并行空间数据流架构的 MACH 编译器，实现了虚拟机抽象、领域特定语言（DSL）及支持多平台代码降低的功能。展示了针对 NumPy 张量操作到 Wafer Scale Engine 的降低过程。

Result: MACH 支持面向空间架构和传统统一内存设备的代码编译，能有效将高层数值运算（如 NumPy 张量操作）通过硬件特定语言对接硬件平台（例如 Cerebras）。

Conclusion: MACH 编译器通过虚拟机抽象和灵活的 DSL，促进了高层代码到特定空间架构的转换，并成功支持以 NumPy 张量操作为例的降低过程。

Abstract: We have developed a novel compiler called the Multiple-Architecture Compiler
for Advanced Computing Hardware (MACH) designed specifically for
massively-parallel, spatial, dataflow architectures like the Wafer Scale
Engine. Additionally, MACH can execute code on traditional unified-memory
devices. MACH addresses the complexities in compiling for spatial architectures
through a conceptual Virtual Machine, a flexible domain-specific language, and
a compiler that can lower high-level languages to machine-specific code in
compliance with the Virtual Machine concept. While MACH is designed to be
operable on several architectures and provide the flexibility for several
standard and user-defined data mappings, we introduce the concept with dense
tensor examples from NumPy and show lowering to the Wafer Scale Engine by
targeting Cerebras' hardware specific languages.

</details>


### [2] [WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction](https://arxiv.org/abs/2506.16048)
*Byeongjee Kang,Harsh Desai,Limin Jia,Brandon Lucia*

Main category: cs.PL

TL;DR: 论文创新性提出了在MLIR中用专用Wasm方言支持高层Wasm特性的编译流程，实现了抽象性、可扩展性和性能的平衡，实验表明该方法在多数场景下与LLVM方法效果相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 现有的Wasm编译方式，要么每个高层语言都需单独实现支持，导致重复工作；要么直接降级为低层表示（如LLVM IR），损失了高层抽象性，限制了对高层Wasm特性的支持。MLIR可支持多层抽象，但其Wasm代码生成依赖LLVM后端，带来了LLVM的局限。

Method: 提出了一种新型的Wasm编译流水线：在MLIR中设计专门的Wasm方言，高层MLIR方言可直接生成高层Wasm代码，无需丢失抽象性。通过模块化、可扩展的设计更好地支持高层Wasm特性。

Result: 通过一个关于Stack Switching（Wasm新高层特性）的案例展示了方法的可扩展性。PolyBench基准测试显示，本流水线生成的代码，最慢仅比LLVM编译器慢7.7%，部分环境下更快。

Conclusion: 所提出的基于MLIR的高层Wasm编译流水线兼顾抽象性和性能，为高层语言引入Wasm新特性提供了更优解决方案，比现有方法更模块化、易扩展且性能良好。

Abstract: WebAssembly (Wasm) is a portable bytecode format that serves as a compilation
target for high-level languages, enabling their secure and efficient execution
across diverse platforms, including web browsers and embedded systems. To
improve support for high-level languages without incurring significant code
size or performance overheads, Wasm continuously evolves by integrating
high-level features such as Garbage Collection and Stack Switching. However,
existing compilation approaches either lack reusable design -- requiring
redundant implementation efforts for each language -- or lose abstraction by
lowering high-level constructs into low-level shared representations like LLVM
IR, which hinder the adoption of high-level features. MLIR compiler
infrastructure provides the compilation pipeline with multiple levels of
abstraction, preserving high-level abstractions throughout the compilation
pipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm
code generation, thereby inheriting LLVM's limitations.
  This paper presents a novel compilation pipeline for Wasm, featuring Wasm
dialects explicitly designed to represent high-level Wasm constructs within
MLIR. Our approach enables direct generation of high-level Wasm code from
corresponding high-level MLIR dialects without losing abstraction, providing a
modular and extensible way to incorporate high-level Wasm features. We
illustrate this extensibility through a case study that leverages Stack
Switching, a recently introduced high-level feature of Wasm. Performance
evaluations on PolyBench benchmarks show that our pipeline, benefiting from
optimizations within the MLIR and Wasm ecosystems, produces code with at most
7.7\% slower, and faster in some execution environments, compared to LLVM-based
compilers.

</details>


### [3] [Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine](https://arxiv.org/abs/2506.16883)
*Christoph Jung,C. F. Bolz-Tereick*

Main category: cs.PL

TL;DR: 本文提出并实现了一个与PyPy垃圾回收器深度集成的分配采样分析工具，以较低的性能开销（最大25%）为动态类型语言程序的性能调优提供有效支持。


<details>
  <summary>Details</summary>
Motivation: 在动态类型语言特别是分配密集型程序中，传统的基于时间的性能分析方法并不总是能很好反映程序资源消耗。本文关注通过分配分析来补充时间分析，实现更有效的性能观测。

Method: 作者提出了一种基于采样的分配分析器，并将其深度集成到PyPy（一个Python虚拟机）的垃圾回收器中。通过调整采样周期，控制分析开销。

Result: 在采样周期为4MB时，分配采样分析器的最大时间开销为25%，可调且与未分析时的常规执行相比效率较高。

Conclusion: 将分配采样分析器集成进垃圾回收器是一种可行的低开销分配分析方式，为分配密集型、动态类型语言的性能分析提供了新的方案。

Abstract: Compared to the more commonly used time-based profiling, allocation profiling
provides an alternate view of the execution of allocation heavy dynamically
typed languages. However, profiling every single allocation in a program is
very inefficient. We present a sampling allocation profiler that is deeply
integrated into the garbage collector of PyPy, a Python virtual machine. This
integration ensures tunable low overhead for the allocation profiler, which we
measure and quantify. Enabling allocation sampling profiling with a sampling
period of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over
un-profiled regular execution.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: 本研究首次系统分析了开源ML项目中社区异味与自承认技术债务的数量、分布与演化规律，发现两者在特定模式下高度相关，并随项目规模和版本演变，强调了早期管理社区问题的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然社区异味（community smells）和自承认技术债务（SATD）已在一般软件系统中被研究，但它们在机器学习项目中的相互关系尚未深入探讨。作者希望揭示在开源机器学习项目中社区异味的普遍性及其与SATD之间的联系。

Method: 作者在155个开源ML项目的不同版本上，分析并量化了10种社区异味的分布情况，检测SATD的出现，对社区异味与SATD的相关性进行统计分析，并进一步分析两者随项目发布版本的演化趋势。

Result: （1）社区异味在ML项目中普遍存在，并在不同规模项目中呈现不同分布模式。（2）某些社区异味（如沉默和组织孤岛）与高SATD出现率强相关。（3）权威与沟通相关的异味常与持久性代码与设计债务共现。（4）社区异味与SATD随项目发布演化，存在与项目规模相关的趋势。

Conclusion: 应及早发现和消除社区异味等社会技术问题，以维护ML项目的质量和可持续性。

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [5] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: 本文系统梳理了自主系统（ROSAS）回归测试优化方法，归纳主要难题，提出未来研究方向，为该领域提供全面参考。


<details>
  <summary>Details</summary>
Motivation: 目前回归测试在ROS为基础的自主系统（ROSAS）中的优化研究较少，而这些系统由于具有动态性、非确定性行为和高安全要求，给传统回归测试方法带来了挑战。

Method: 本文首次对针对ROSAS的回归测试优化技术进行了系统综述，对122篇研究进行了分析和归类，涵盖测试用例优先级排序、最小化和选择，并提出了结构化分类法以说明各种方法的适用性和局限性。

Result: 建立了ROSAS回归测试优化技术的结构化知识体系，归纳总结了优先级排序、冗余最小化和影响用例选择三个主要挑战，并提出通过帧向量覆盖度、跨源基础模型和神经符号推理等未来研究方向。

Conclusion: 本文为ROSAS的回归测试优化提供了系统参考和研究路线图，对该领域的发展具有奠基意义。

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [6] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: 本文针对自动程序修复系统难以处理GUI相关多模态问题，提出GUIRepair跨模态推理方法，有效集成视觉与文本信息，显著提高了多模态问题修复能力，在SWE-bench M数据集上取得了领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）驱动的自动程序修复（APR）技术在单一模态（如文本）任务中表现良好，但在需要理解和利用可视化信息的多模态场景（例如需要分析GUI界面的bug）则效果欠佳。如何让APR系统高效解决涉及视觉信息的多模态问题，是提升其实用性的重要方向。

Method: 本文提出了GUIRepair方法以实现跨模态推理，针对多模态issue，整合了Image2Code和Code2Image两个关键组件：Image2Code基于issue报告检索、理解相关项目文档，将GUI图像转化为可执行代码，帮助更好地定位和理解故障；Code2Image重现修复后的GUI界面，辅助评估补丁是否修复了可视化问题，从而实现自动化的补丁验证。

Result: 在SWE-bench M基准集上，GUIRepair采用GPT-4o作为底模时解决了157个实例，比最佳开源基线多26个；采用o4-mini底模时解决了175个实例，超越了最优商业系统22个，显著提升了针对多模态问题的修复能力。

Conclusion: 通过引入跨模态推理与可视化信息理解机制，GUIRepair极大提升了多模态软件问题的自动修复能力，验证了光靠文本信息难以处理视觉相关bug的局限，也为APR系统发展指明了新方向。

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [7] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 本研究通过大规模工业案例，揭示了微服务架构在技术债务管理上的主要挑战和对策，强调了静态代码分析、沟通与组织结构匹配等措施的重要性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构因其松耦合特性，被认为高度可维护且易于演化，但这类系统的技术债务（TD）问题尚未被大规模深入研究，尤其是在工业实践中。

Method: 采用混合方法的案例研究，结合静态代码分析的定量手段和开发团队焦点小组讨论及首席架构师访谈获取的定性见解，分析包含100多个微服务、服务于1.5万个站点的大型系统。

Result: 研究发现：（1）简单的静态源代码分析能高效、有效地作为全方位发现技术债务的切入点；（2）沟通不畅是TD积累的重要因素；（3）架构与组织结构的不一致会加剧TD积累；（4）微服务系统在TD积累与解决之间“快速循环”，即“微服务架构技术债务赌博”。并提出了一套针对微服务TD管理的策略。

Conclusion: 技术债务是影响微服务架构系统质量和可维护性的关键问题，需通过适当的检测与管理策略予以重视。简单的静态代码分析结合组织与架构协同、有效沟通等综合干预，有助于系统性地解决TD问题。

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [8] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本研究评估了LLM在文档到代码追溯任务的表现，发现其远超传统方法，但仍有一定误差类型，建议结合人工参与以提高实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在文档到代码可追溯性自动化中的潜力尚未充分挖掘，因此有必要对其性能进行系统性评估，探索其应用前景与局限。

Method: 作者选取Claude 3.5 Sonnet、GPT-4o和o3-mini三种LLM，构建了两个包含API参考和用户指南等文档的开源项目（Unity Catalog与Crawl4AI）数据集，系统性地评估了LLM在追踪链接准确性、关系解释质量和多步链重建三个方面的能力，同时与TF-IDF、BM25、CodeBERT等基线进行对比，并进行了误差分析。

Result: 最佳LLM在两个数据集上的F1分数分别达到79.4%和80.4%，大幅优于传统方法；完全正确的关系解释占比为42.9%-71.1%，部分准确率超97%；多步链的终点识别准确但中间链条重建有差异。错误多由命名假设、虚假链接或过度泛化造成。任务表述（如一对多匹配策略）对性能至关重要。

Conclusion: LLM在文档到代码可追溯性任务中表现优异，是强有力的辅助工具，但由于存在特定误差类型，仍需人机协作和后续针对性改进。

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [9] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: 本研究基于67万用户评论，利用LLM提出高效话题挖掘方法，深入分析了生成式AI应用的用户关注点，并为开发和研究提供建议。


<details>
  <summary>Details</summary>
Motivation: 虽然Gen-AI应用广受欢迎，但用户实际认知和评价机制尚不清楚，因此需要系统分析用户真实反馈，为产品优化和学术研究提供指导。

Method: 提出并应用了SARA（选择、获取、精炼与分析）四阶段方法，利用大型语言模型（LLM）和五次提示学习，对173款Gen-AI应用、共67万余条谷歌商店用户评论进行话题提取、分组与时序分析。

Result: LLM在话题抽取中的准确率高达91%，用户讨论最多的话题包括AI性能、内容质量、政策与审查等，分析识别了主要挑战和新机遇，并追踪了用户关注点的动态变化。本文最后提出了促进应用优化和未来研究的建议。

Conclusion: 通过系统分析用户对生成式人工智能（Gen-AI）移动应用的评价，文章揭示了用户关注的核心话题及其随时间变化的趋势，并为开发者和研究者提出了可操作性的建议。

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [10] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: 本文提出一种模块化组合式LTL控制器合成法，大大扩展了可控系统规模，并已通过软件工具和实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统单体LTL控制器合成易引发状态爆炸的问题，提升可控系统的规模和复杂度处理能力。

Method: 利用被控系统的模块化结构，将被控对象分解为LTS的子集，通过迭代地为其子集生成最大允许的安全控制器。同时，通过观测等价类将局部事件抽象，简化待控子系统规模，最终合成多个控制器并并行运行。方法在MTSA工具上实现，针对LTL的GR(1)子集进行实验。

Result: 组合方法显著提升了可处理系统规模，实验显示可合成的系统规模是单体方法的1000倍。其控制器能保证并行协作下满足全局LTL目标。

Conclusion: 本文提出的组合式合成方法可以控制比单一方法大得多的系统规模，实现安全且满足LTL规格的控制器合成。所得控制器具有组合性，并能保证全局LTL目标的达成。

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [11] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: 本研究分析了AI工具应用于现代软件QA流程的机遇与挑战，实证演示了其潜力，但也指出“黑盒”等难题，强调实施需慎重并发展新验证方法。


<details>
  <summary>Details</summary>
Motivation: 现代分布式软件系统日益复杂、规模庞大且迭代迅速，传统质量保证（QA）方法难以应对这些挑战，而资源有限又导致质量问题带来高昂成本，因此需要探索更高效的QA方案。

Method: 对现代AI工具在质量保证流程中的应用进行了全面分析，涵盖了测试分析、等价划分和边界分析、变异测试、验收标准不一致性发现、静态分析、测试用例生成、单元测试生成、测试套件优化与评估、端到端场景执行等方面。以AI代理在企业应用端到端回归测试为实例，实现了实际应用演示。

Result: AI生成测试案例仅8.3%为flaky（不稳定随机失败），显示出方法的巨大潜力。但也发现了实际采用中的主要挑战，例如生成语义等价覆盖的难度、LLM的“黑盒”属性和可解释性不足、模型倾向将变异用例修正为预期结果等，凸显了对生成成果和测试执行结果严格验证的必要性。

Conclusion: AI有望显著变革现代软件质量保证流程，但应对AI工具的局限采取战略性实施，并开发配套的验证方法。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [12] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 本文提出用LLM自动推断和验证自然语言字符串需求可满足性，通过自动生成形式/代码检查器提升验证效果，实验结果显示生成的Python检查器准确率高，整体方法显著提升了验证成功率与准确度。


<details>
  <summary>Details</summary>
Motivation: 在软件系统中，字符串相关的需求（通常以自然语言表达）分析比较困难，尤其是当需要对多条需求集合进行形式化验证（如可满足性）时，传统方法（如SMT求解器）虽然有效但存在理论局限性，同时将自然语言需求转化为形式约束通常需要大量人工操作。最近大语言模型（LLM）在形式化推理中的潜力尚未被充分评估。

Method: 提出了一种混合方法：首先利用LLM直接推断字符串需求集合的可满足性，并尝试给出满足条件的实例字符串；然后，利用LLM自动生成声明式（如SMT）和命令式（如Python）的检查器代码，对第一步推断的正确性进行验证。实验中，评估了四种主流LLM在该任务上的表现。

Result: 实验发现，LLM可以较高质量地将自然语言需求转化为检查器代码，尤其是Python检查器在测试中实现了完美准确率。这些自动生成的检查器有效提升了LLM在给出满足条件字符串和检测不可满足需求方面的准确性，相比未使用检查器的基线方法，生成成功率和F1分数提升了一倍以上。

Conclusion: LLM在自然语言字符串需求的形式化验证方面展现出很强的能力，特别是结合自动生成的检查器后，显著提升了需求可满足性验证的准确度和自动化水平。该方法为减少人工翻译和提高需求验证效率提供了新思路。

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [13] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 该论文提出SemAgent，一种充分利用多重语义信息的自动代码修复流程，并在权威基准任务上显著超越现有方法，尤其擅长多行修复和复杂问题。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在自动化程序修复（APR）等下游软件工程任务上表现出色，但目前的智能体系统常常只关注局部的可疑代码行，并孤立地修复它们，缺乏对问题语义、代码语义和执行语义的深入理解，导致生成的补丁容易过拟合用户描述，而无法提供更通用的解决方案。

Method: 提出了SemAgent，一种基于工作流的新颖自动修复流程。该方法通过管道化流程：（a）利用执行语义获取相关上下文，（b）通过泛化抽象理解问题语义，（c）在抽象上下文中识别代码语义，（d）在两阶段架构下，先进行细粒度修复，再通过审查者阶段结合语义过滤出相关补丁。

Result: 在SWEBench-Lite基准上，SemAgent的解决率达到44.66%，优于所有其他基于工作流的方法，比缺乏深度语义理解的基线提升7.66%。该方法在需要多行推理和边界情况处理的问题上表现尤为突出。

Conclusion: 通过引入问题与代码语义的深层建模，能显著提升自动程序修复的整体表现和语义一致性，SemAgent展现了更强的通用性和健壮性。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [14] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: 大语言模型编码工具提升开发效率但带来隐私与安全风险，企业需完善审查流程、私有化部署并加强检测以防安全问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编码工具广泛应用于软件工程，但其在提升开发效率的同时带来了新的安全和隐私风险。

Method: 通过对真实提示和模型输出的调查，揭示了数据泄露、安全漏洞和模型谄媚等问题，并提出了应对措施建议。

Result: 发现约10%的真实提示存在隐私泄露风险，42%的模型生成代码包含安全漏洞，且模型容易迎合用户错误观点（谄媚性）。

Conclusion: 建议企业对AI生成的每一行代码进行标记和审查，将提示与输出控制在私有或本地环境中，遵守新兴安全法规，加入专门检测谄媚答案的测试，从而在提升开发速度的同时保障安全与准确性。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [15] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 论文综述了AI系统鲁棒性、可靠性和责任性等关键议题，结合案例分析指出现实挑战，强调责任机制对于可信AI的重要性，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在实际应用中的普及，其安全性、鲁棒性和可靠性变得尤为重要。论文强调了当前关于这些概念的不断演变与现实需求，尤其是在责任追溯方面，旨在推动AI更加可信和可控。

Method: 首先梳理了鲁棒性、可靠性、责任性等关键概念的发展，并进行了文献综述，然后通过案例分析，展示AI系统在实际场景下所面临的挑战，进一步探讨创新测试方案。

Result: 论文总结了目前存在的主要技术挑战和当前的主流研究方法，案例说明了现有方法在实际中的局限性。同时，论文指出责任机制对构建可信AI系统至关重要，并勾勒了未来研究的重要方向和当前的研究空白。

Conclusion: 要实现可信的AI系统，必须把鲁棒性、可靠性和责任性作为核心议题。论文强调对创新测试工具和责任框架的需求，并呼吁从多维度推动AI的安全和可控发展。

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [16] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文提出利用大型语言模型等先进工具，提高自动驾驶系统验证与确认（V&V）的效率与透明度，目标是降低资源消耗并增强用户信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖复杂的决策模型和多模态传感输入，需要高效且可靠的验证与确认（V&V）以保障安全性。目前的手动测试方法效率低下，耗时耗力，且难以诊断失败、追踪异常和提升系统透明度。

Method: 提出了一种将可解释性、透明性和可理解性整合到V&V流程中的方法。该方法通过文献回顾和利益相关者反馈细化V&V需求，利用大型语言模型（LLMs）生成具备可解释性的测试场景，并在仿真环境中实现实时验证。框架还包括测试判定机制、解释生成与测试聊天机器人。

Result: 计划进行实证研究，以评估方法对诊断效率和透明度的提升效果。初步目标是提高V&V的自动化和资源利用效率，增强用户对自动驾驶技术的信任。

Conclusion: 该方法有望简化自动驾驶系统的V&V流程，减少人力物力投入，提高安全透明度，从而推动自动驾驶技术的广泛应用。

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [17] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 本综述统计并分析了量子及类量子优化算法在软件工程优化中的应用，揭示了研究热点与空白，为未来跨领域创新提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着现代软件系统和开发流程的不断复杂化，传统的软件工程优化方法面临新的挑战，需要创新性的解决方案。量子计算，尤其是量子优化在许多领域取得进展，促使研究者探索其在软件工程优化中的应用潜力。

Method: 通过系统性文献综述（SLR）的方法，研究团队在六大数据库中筛选出2083篇文献，并进一步精选出77篇主要研究，聚焦于量子或类量子算法在传统软件工程优化问题中的应用。

Result: 研究发现，相关研究主要集中于软件工程运营和软件测试等领域，而在其他软件工程活动中仍存在明显研究空白。同时，有部分有价值的工作发表在非传统软件工程领域，显示出量子优化跨学科的重要性。

Conclusion: 本文全面回顾了量子计算与类量子算法在软件工程优化中的研究现状，为搜索式软件工程（SBSE）社区提供了利用量子进展应对下一代软件工程挑战的参考。

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [18] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: 本研究通过用户调研发现多类能反映解释需求的指标，为实现软件系统中实时和针对性的解释输出提供了依据。


<details>
  <summary>Details</summary>
Motivation: 在现今数字化时代，软件系统日益复杂且无处不在，“可解释性”已成为关键质量属性。现有挑战在于，如何发现个体真实的解释需求，因为用户自我报告可能受到假设性或确认偏差的影响。

Method: 作者开展了一项探索性研究，通过线上调研收集用户自我报告的行为、系统事件、情绪与生理反应等指标，这些指标可能作为需求解释的信号。随后，整理出了相关指标的目录，并分析了这些指标与不同类型解释需求的关系。

Result: 研究编制了包含17项用户行为指标、8项系统事件指标和14项情绪或生理反应指标的目录，并揭示了这些指标与不同解释需求类型之间的联系。

Conclusion: 这些指标不仅可用于软件原型的需求挖掘阶段，还能在系统发布后通过遥测及使用数据监测解释需求，进而能在系统运行时主动触发解释输出。

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


### [19] [Behavior Driven Development for 3D Games](https://arxiv.org/abs/2506.17057)
*Fernando Pastor Ricós,Beatriz Marín,I. S. W. B. Prasetya,Tanja E. J. Vos,Joseph Davidson,Karel Hovorka*

Main category: cs.SE

TL;DR: 本文提出将BDD方法与iv4XR框架结合，有效简化了复杂3D游戏的自动化测试脚本编写，并提升了开发与测试团队的协作效率，同时拓展了框架功能，实现了多样化和长时段自动测试。


<details>
  <summary>Details</summary>
Motivation: 3D游戏的复杂性使得测试过程变得困难，需要新的自动化测试手段以保证游戏质量。iv4XR框架虽可自动化测试，但其脚本编写对技术门槛较高，影响开发者与测试人员的协作。

Method: 将行为驱动开发（BDD）方法与iv4XR框架结合，简化自动化测试脚本编写，并扩展战术编程以支持长时间游戏测试。

Result: BDD方法的引入让开发者和测试人员能够以人类可读的语言编写、管理和执行自动化测试用例，提升了Space Engineers与LabRecruits等游戏的测试自动化程度。同时，框架扩展实现了对长时段游戏测试的自动化支持。

Conclusion: iv4XR框架通过整合BDD和战术编程，拓展了其支持不同测试需求的能力，而BDD降低了协作门槛，使自动游戏测试更加高效易用。

Abstract: Computer 3D games are complex software environments that require novel
testing processes to ensure high-quality standards. The Intelligent
Verification/Validation for Extended Reality Based Systems (iv4XR) framework
addresses this need by enabling the implementation of autonomous agents to
automate game testing scenarios. This framework facilitates the automation of
regression test cases for complex 3D games like Space Engineers. Nevertheless,
the technical expertise required to define test scripts using iv4XR can
constrain seamless collaboration between developers and testers. This paper
reports how integrating a Behavior-driven Development (BDD) approach with the
iv4XR framework allows the industrial company behind Space Engineers to
automate regression testing. The success of this industrial collaboration has
inspired the iv4XR team to integrate the BDD approach to improve the automation
of play-testing for the experimental 3D game LabRecruits. Furthermore, the
iv4XR framework has been extended with tactical programming to enable the
automation of long-play test scenarios in Space Engineers. These results
underscore the versatility of the iv4XR framework in supporting diverse testing
approaches while showcasing how BDD empowers users to create, manage, and
execute automated game tests using comprehensive and human-readable statements.

</details>


### [20] [Software Fairness Testing in Practice](https://arxiv.org/abs/2506.17095)
*Ronnie de Souza Santos,Matheus de Morais Leca,Reydne Santos,Cleyton Magalhaes*

Main category: cs.SE

TL;DR: AI与ML系统的公平性测试虽有丰富学术研究，但行业实践不足。通过访谈发现，主要难题在于理论难落地、工具缺乏、数据与指标等实际障碍。呼吁开发更接地气的测试工具和解决方案，弥合学界与业界的差距。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器学习技术日益融入软件系统，传统软件测试已无法满足AI系统的特殊需求，“公平性测试”作为解决AI系统伦理和偏见问题的关键手段逐步兴起。尽管学术界在公平性测试方面投入了大量研究，但现实中的落地和实践却进展有限。作者希望揭示理论与行业实践间的鸿沟，并推动公平性测试的实际应用。

Method: 研究团队通过对22位参与AI和ML项目的软件从业者进行访谈，收集第一手数据，分析行业在AI系统公平性测试中的实际做法、挑战和需求。

Result: 研究发现，学术界提出的公平性定义难以为从业者所理解和操作，且缺乏与行业实际对接的公平性测试工具。主要难点还包括数据质量与多样性、时间压力、难以量化和应用的公平性指标，以及不同模型间的兼容性等。

Conclusion: 学术与实践在AI系统公平性测试上的脱节明显，急需更具实用性和可行性的工具、指标与操作范式，以便行业能够切实而系统地应对AI的公平性问题。

Abstract: Software testing ensures that a system functions correctly, meets specified
requirements, and maintains high quality. As artificial intelligence and
machine learning (ML) technologies become integral to software systems, testing
has evolved to address their unique complexities. A critical advancement in
this space is fairness testing, which identifies and mitigates biases in AI
applications to promote ethical and equitable outcomes. Despite extensive
academic research on fairness testing, including test input generation, test
oracle identification, and component testing, practical adoption remains
limited. Industry practitioners often lack clear guidelines and effective tools
to integrate fairness testing into real-world AI development. This study
investigates how software professionals test AI-powered systems for fairness
through interviews with 22 practitioners working on AI and ML projects. Our
findings highlight a significant gap between theoretical fairness concepts and
industry practice. While fairness definitions continue to evolve, they remain
difficult for practitioners to interpret and apply. The absence of
industry-aligned fairness testing tools further complicates adoption,
necessitating research into practical, accessible solutions. Key challenges
include data quality and diversity, time constraints, defining effective
metrics, and ensuring model interoperability. These insights emphasize the need
to bridge academic advancements with actionable strategies and tools, enabling
practitioners to systematically address fairness in AI systems.

</details>


### [21] [Reassessing Code Authorship Attribution in the Era of Language Models](https://arxiv.org/abs/2506.17120)
*Atish Kumar Dipongkor,Ziyu Yao,Kevin Moran*

Main category: cs.SE

TL;DR: 本文系统评估了多种SOTA代码Transformer模型在代码作者识别上的效果，发现其优于传统手工特征方法，并借助可解释性技术分析模型行为，指明了后续研究提升方向。


<details>
  <summary>Details</summary>
Motivation: 代码作者归属（CAA）是代码取证、安全等诸多领域的核心问题，但因代码风格差异细微、作者众多、特征复杂，准确识别作者非常具有挑战性。现有方法大多依赖手工特征，面对复杂样式和对抗攻击表现有限。因此，亟需探索更有效的自动化技术。

Method: 本文首次对比评估了两种大规模、五种小规模的SOTA代码Transformer语言模型在CAA任务上的表现，覆盖6个包含463位开发者共12k代码片段的数据集。此外，利用已有的机器学习可解释性方法对模型在对应任务下的行为进行深度分析。

Result: 实验揭示了主流代码语言模型理解代码风格特征的能力，部分超越传统特征工程方法，指出了现有Transformer模型在代码作者识别领域的优势和局限，为后续研究指明了重要方向。

Conclusion: 大型Transformer代码语言模型在代码作者归属任务上展现出较强潜力，比以往依赖手工特征的方法表现更优。同时，可解释性分析帮助理解模型决策依据，对未来CAA自动化与安全性研究具有指导意义。

Abstract: The study of Code Stylometry, and in particular Code Authorship Attribution
(CAA), aims to analyze coding styles to identify the authors of code samples.
CAA is crucial in cybersecurity and software forensics for addressing,
detecting plagiarism, and supporting criminal prosecutions. However, CAA is a
complex and error prone task, due to the need for recognizing nuanced
relationships between coding patterns. This challenge is compounded in large
software systems with numerous authors due to the subtle variability of
patterns that signify the coding style of one author among many. Given the
challenges related to this task, researchers have proposed and studied
automated approaches that rely upon classical Machine Learning and Deep
Learning techniques. However, such techniques have historically relied upon
hand-crafted features, and due to the often intricate interaction of different
features (e.g., formatting, etc.), have key limitations in properly
characterizing authorship, and are sensitive to adversarial code perturbations.
Recently, transformer-based Language Models (LMs) have shown remarkable
efficacy across a range of software engineering tasks, and in the authorship
attribution on natural language in the NLP domain. However, their effectiveness
in CAA is not well understood. As such, we conduct the first extensive
empirical study applying two larger state-of-the-art code LMs, and five smaller
code LMs to the task of CAA to 6 diverse datasets that encompass 12k code
snippets written by 463 developers. Furthermore, we perform an in-depth
analysis of our studied models' performance on CAA using established machine
learning interpretability techniques. The results of our analysis illustrate
important findings that illuminate the behavior of LMs in understanding
stylometric code patterns during the task of CAA, and point towards important
directions for future work.

</details>


### [22] [Large Language Model Unlearning for Source Code](https://arxiv.org/abs/2506.17125)
*Xue Jiang,Yihong Dong,Zheng Fang,Yingwei Ma,Tangxinyu Wang,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: 针对LLM遗忘技术在代码领域的应用困境，提出了效果优异且鲁棒性强的PROD算法，能在遗忘有风险内容的同时保持代码生成能力，为安全合规的代码生成开辟了新路径。


<details>
  <summary>Details</summary>
Motivation: LLM 在软件工程（SE）领域取得了显著成功，但其训练数据中可能包含敏感或过时信息，带来法律合规、软件安全与代码质量等风险。现有大型语言模型（LLM）的“遗忘”技术主要应用于自然语言，如何保障源代码中的数据隐私和安全仍未被充分探索。

Method: 作者提出了一种新颖的遗忘算法——PROD，可以让LLM遗忘不需要的代码内容，同时保持其代码生成能力。PROD通过抑制需要遗忘数据在模型输出分布中的概率，同时提升其他候选分布部分，实现遗忘和能力保留的平衡。此外，作者还建立了代码遗忘评测基准，包括有版权代码、不安全代码、过时API三个下游任务。

Result: PROD在维持模型代码生成能力的同时，有效遗忘了指定内容，在多个下游任务中均优于现有遗忘方法，并且适用于不同系列的LLM。此外，PROD在不暴露被忘信息的前提下，对抗性攻击鲁棒性更强。

Conclusion: PROD方法不仅拓展了遗忘技术在源代码领域的应用边界，也为实现安全、合规、可靠的代码生成提供了重要支撑。

Abstract: LLM4SE has demonstrated significant success, but LLMs' potential memorization
of sensitive or outdated training data introduces critical risks to legal
compliance, software security, and code quality. LLM unlearning techniques,
which can eliminate the influence of undesired data from LLMs in a
post-training way, present a promising solution to address these concerns.
While recent efforts in LLM unlearning show effectiveness in natural language,
their applicability to source code remains underexplored. Our empirical study
reveals that existing LLM unlearning approaches, when applied to source code,
cause severe model utility degradation, rendering models practically unusable
for code generation. In this paper, we propose PROD, a novel unlearning
approach that enables LLMs to forget undesired code content while effectively
preserving their code generation capabilities. PROD suppresses the probability
of forget data in LLMs' output distribution while promoting candidate
distributional components, enabling the model to jointly learn to forget
specific content and retain its general capabilities. To facilitate this study,
we establish a benchmark for code unlearning evaluation, which includes three
critical downstream tasks: copyrighted code unlearning, insecure code
unlearning, and deprecated API unlearning. Our evaluation demonstrates that
PROD achieves superior balance between forget quality and model utility
compared to existing unlearning approaches across three downstream tasks, while
consistently exhibiting improvements when applied to LLMs of varying series.
PROD also exhibits superior robustness against adversarial attacks without
generating or exposing the data to be forgotten. The results underscore that
our approach not only extends the application boundary of unlearning techniques
to source code, but also holds significant implications for advancing reliable
code generation.

</details>


### [23] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 该文首次系统梳理和分析了SWE-Bench自动程序修复基准上的方案，揭示了专有大模型主导、架构类型多样、贡献者多元等主要现象，对领域发展提供了全面认知。


<details>
  <summary>Details</summary>
Motivation: 随着自动化程序修复（APR）技术的快速发展，尤其是在大语言模型（LLM）和基于智能体的系统推动下，亟需对现有LLM修复系统进行系统化评估和分析。SWE-Bench成为主流评测基准，但由于提交过程缺乏详细文档，众多方案的架构和来源不明确。本文动机在于系统梳理和分析这些系统的现状。

Method: 对SWE-Bench Lite（68个提交）和SWE-Bench Verified（79个提交）的所有方案进行全面调研与分析，从提交者类型、产品可用性、LLM使用情况和系统架构等多个维度剖析，涵盖67种不同技术路线。

Result: 分析指出，大部分顶尖方案依赖专有的大语言模型（如Claude 3.5/3.7），方案涵盖agentic和non-agentic设计，参与者既有个人开发者也有大型科技公司。

Conclusion: 本文首次系统性分析了SWE-Bench主流评测榜单中的各种方案，揭示了当前自动化程序修复系统的主要技术路线和发展趋势。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


### [24] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 本文提出基于AST的结构感知代码切分方法，有效提升了RAG代码生成系统的检索及生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于行的chunking容易打断代码结构，导致生成质量下降；需要一种更能保持代码语义完整性的chunking方法。

Method: 提出了一种基于抽象语法树（AST）的chunking方法，递归拆分大AST节点并合并兄弟节点，保证每块代码的结构语义完整并控制大小。

Result: 该方法提升了多项代码生成任务性能：在RepoEval检索Recall@5提升4.3点，在SWE-bench生成Pass@1提升2.67点。

Conclusion: 本文强调在RAG应用于代码生成时，采用结构感知（AST为基础）的chunking策略更加有效，对缩放检索增强的代码智能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [Locality in Many-Valued Structures](https://arxiv.org/abs/2506.16206)
*James Carr*

Main category: cs.LO

TL;DR: 本文探讨了多值模型下经典局部性定理（Hanf与Gaifman）是否适用。结果发现Hanf定理通常不成立，但对特殊well-connected残余格可恢复；Gaifman定理的主引理在良好代数结构下多种情形可证明。局部性定理能否成立与代数性质及连接词密切相关，为多值模型理论提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 在经典模型理论中，局部性（locality）定理如Hanf定理和Gaifman定理对于分析模型与公式之间的关系非常重要，但在多值模型（如剩余格residuated lattices模型）下，这些定理是否依然成立尚不明确，因此本文旨在探究多值模型中这些定理的适用性。

Method: 本文考察了残余格上的一阶子结构逻辑的多值模型，分别研究了在不同局部性理解下，Hanf定理与Gaifman定理在这种模型上的成立情况。对于Hanf定理，考察其对自然定义的邻域失败与特殊情形下的可恢复性；对于Gaifman定理，不直接处理Normal Form，而是聚焦教材中定理的主引理，并在不同局部性理解与良好代数结构（well-behaved algebra）条件下对该引理的可恢复性进行证明。

Result: 结果显示：1）Hanf定理在残余格自然邻域下一般不成立，但在well-connected剩余格的特殊场景下可以恢复；2）对于Gaifman定理，主引理在语法上可以表达局部性时（即代数结构足够良好），在多种对局部性的理解下都可以证明成立；3）判定这些局部性定理能否成立的核心，在于是否存在一种order-interpreting的连接词，将模型—公式的关系与公式在代数中的赋值函数相联系。

Conclusion: 多值模型（特别是基于残余格的模型）下，经典局部性定理（Hanf、Gaifman）的有效性依赖于对局部性的定义及所用代数结构的性质。适当的代数结构和连接词是定理成立的关键。部分经典结论可推广至多值情形，但需对邻域、断言表达及代数要求加以限定。本文为多值模型理论的局部性性质提供了系统分析与若干正反实例。

Abstract: Many-valued models generalise the structures from classical model theory by
defining truth values for a model with an arbitrary algebra. Just as algebraic
varieties provide semantics for many non-classical propositional logics, models
defined over algebras in a variety provide the semantics for the corresponding
non-classical predicate logics. In particular models defined over varieties of
residuated lattices represent the model theory for first-order substructrual
logics.
  In this paper we study the extent to which the classical locality theorems
from Hanf and Gaifman hold true in the residuated lattice setting. We
demonstrate that the answer is sensitive both to how locality is understood in
the generalised context and the behaviour of the truth-defining algebra. In the
case of Hanf's theorem, we will show that the theorem fails for the natural
understanding of local neighbourhoods, but is recoverable in one special case
for well-connected residuated lattices. For Gaifman's theorem, rather than
consider Gaifman normal forms directly we focus on the main lemma of the
theorem from textbook proofs. We prove that for a number of different
understandings of locality, provided the algebra is well-behaved enough to
express locality in its syntax, this main lemma can be recovered. In each case
we will see that importance of an order-interpreting connective which creates a
link between the modelling relation between models and formulas and the
valuation function from formulas into the algebra.

</details>


### [26] [A Quantum-Control Lambda-Calculus with Multiple Measurement Bases](https://arxiv.org/abs/2506.16244)
*Alejandro Díaz-Caro,Nicolas A. Monzon*

Main category: cs.LO

TL;DR: Lambda-SX是一种新型带类型系统的量子lambda演算，通过形式化语法和类型规则，实现了多测量基的灵活支持，提高了量子测量的可组合及类型可控性，证明了该方法的理论可行性。


<details>
  <summary>Details</summary>
Motivation: 当前量子程序设计语言在类型系统中通常仅支持单一测量基，缺乏对多基测量的灵活控制与组合性推理能力，因此需要开发能跟踪任意基下可复制性的类型系统。

Method: 形式化了Lambda-SX的语法、类型规则、子类型以及操作语义，并证明了其关键的元理论性质。

Result: 实验性地证明了支持多基测量可以被一致地集成进量子编程语言的类型系统中。

Conclusion: 本文提出的Lambda-SX量子lambda演算在类型系统中能够一致地集成多测量基的支持，证明了这一模型的合理论基础和可行性。

Abstract: We introduce Lambda-SX, a typed quantum lambda-calculus that supports
multiple measurement bases. By tracking duplicability relative to arbitrary
bases within the type system, Lambda-SX enables more flexible control and
compositional reasoning about measurements. We formalise its syntax, typing
rules, subtyping, and operational semantics, and establish its key
meta-theoretical properties. This proof-of-concept shows that support for
multiple bases can be coherently integrated into the type discipline of quantum
programming languages.

</details>


### [27] [A Hyperlogic for Strategies in Stochastic Games (Extended Version)](https://arxiv.org/abs/2506.16775)
*Lina Gerlach,Christof Löding,Erika Ábrahám*

Main category: cs.LO

TL;DR: 本文提出并研究了一种新型概率超逻辑HyperSt$^2$，首次使得回合制随机博弈中多策略、多轨迹概率关系的表达与判定成为可能，并分析了其判定性与计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的逻辑体系难以表达关于策略的超性质，尤其是在具有不确定性的回合制随机博弈中，缺乏专门针对策略间多执行轨迹概率关系的逻辑工具。

Method: 提出了一种新的概率超逻辑HyperSt$^2$，能够在回合制随机博弈中表达和对比多种策略多个独立执行的概率关系，并形式化最优性及纳什均衡等概念。此外，系统研究了该逻辑的表达能力，并与现有的逻辑体系进行了对比分析。

Result: 证明了HyperSt$^2$逻辑的可判定性受限条件，包括对有界记忆的情况下是可判定的，在无记忆确定性策略下复杂度为EXPTIME和PSPACE-hard，并指出存在一个片断，其模型检测问题为PSPACE-complete。

Conclusion: HyperSt$^2$是首个面向随机博弈的超逻辑，为表达和验证博弈中策略的超性质提供了有力工具，并对其判定边界和复杂度进行了详细的理论刻画。

Abstract: We propose a probabilistic hyperlogic called HyperSt$^2$ that can express
hyperproperties of strategies in turn-based stochastic games. To the best of
our knowledge, HyperSt$^2$ is the first hyperlogic for stochastic games.
HyperSt$^2$ can relate probabilities of several independent executions of
strategies in a stochastic game. For example, in HyperSt$^2$ it is natural to
formalize optimality, i.e., to express that some strategy is better than all
other strategies, or to express the existence of Nash equilibria. We
investigate the expressivity of HyperSt$^2$ by comparing it to existing logics
for stochastic games, as well as existing hyperlogics. Though the
model-checking problem for HyperSt$^2$ is in general undecidable, we show that
it becomes decidable for bounded memory and is in EXPTIME and PSPACE-hard over
memoryless deterministic strategies, and we identify a fragment for which the
model-checking problem is PSPACE-complete.

</details>


### [28] [A Note on Proper Relational Structures](https://arxiv.org/abs/2506.17142)
*Adam Bjorndahl,Philip Sink*

Main category: cs.LO

TL;DR: 本文提出了一种将一般关系结构转换为“适当”关系结构的算法，保留了结构的传递性等经典属性，并可用于模态逻辑相关证明。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在解决将关系结构转化为“适当”关系结构的问题，即不存在一对世界w和u，使得w对于每个主体来说都可以从u到达。此问题在模态逻辑的Simplicial语义中具有理论和实践意义。

Method: 作者提出了一种算法，将任意关系结构转换为“适当”关系结构。该算法可保留关系结构的许多经典性质，如传递性和Euclidean属性。

Result: 提出的转换方法能够确保生成的关系结构同时具有“适当性”和保留重要性质，这为进一步的逻辑证明和理论发展奠定了基础。

Conclusion: 本文为关系结构向“适当”关系结构的转换提供了有效算法，并证明该算法不破坏诸如传递性与Euclidean性等关键结构属性。其成果可直接应用于模态逻辑Simplicial语义中的完备性证明等场景。

Abstract: In this note we provide an algorithm for translating relational structures
into "proper" relational structures, i.e., those such that there is no pair of
worlds w and u such that w is accessible from u for every agent. In particular,
our method of translation preserves many classical properties of relational
structures, such as transitivity and the Euclidean property. As a result, this
method of translation has many applications in the literature on Simplicial
Semantics for modal logic, where the creation of proper canonical relational
structures is a common step in proofs of completeness.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [29] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: 该论文提出了开源事实核查系统Veracity，结合大模型与检索，支持多语言、数值化评分、解释性强，能有效帮助用户判别和理解信息真伪。


<details>
  <summary>Details</summary>
Motivation: 虚假信息的泛滥对社会构成重大威胁，尤其由于生成式AI能力提升而加剧。

Method: 提出了Veracity系统，结合大型语言模型（LLM）与网页检索代理，分析用户提交的声明，并提供有据可依的真伪评估和直观解释。

Result: Veracity系统具备多语言支持、声明真伪的量化评分和类即时通讯应用的交互界面，能够检测虚假信息并解释其判断理由。

Conclusion: Veracity系统能够提升用户对虚假信息的识别能力，支持媒体素养教育，推动社会信息透明与健康发展。

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [30] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Main category: cs.CL

TL;DR: 用信息几何视角和自然梯度方法解释LLM优化机制，揭示曲率对泛化与极小值的影响，并展望了量子优化潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数空间复杂度和维度的增长，传统欧氏空间下的优化分析难以充分揭示其训练动态及泛化行为。作者希望通过引入信息几何视角，利用自然梯度方法和Fisher信息度量，更加系统地理解和解释LLM优化过程中出现的现象。

Method: 采用信息几何的视角，将Fisher信息度量用作高维参数空间的度量工具，从而阐释自然梯度下降在LLM优化中的作用和原理。进一步基于Fubini-Study度量和量子Fisher信息，探索量子优化范式对未来模型训练的启示和潜力。

Result: 通过几何视角解释了LLM训练中的锐极小值、泛化、以及大模型性能随规模提升的规律，并指出曲率感知优化方法有助于更深入理解和改进LLM训练。最后对量子增强优化进行了理论上的类比与前景展望。

Conclusion: 对LLM训练过程，当使用包含Fisher信息度量的几何方法分析时，可以更本质地理解诸如泛化与极小值等关键现象，也为未来曲率相关或量子增强优化方法的研究指明了方向。

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [31] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: 本文提出的MEM1框架通过强化学习实现了常量内存、多轮推理，并在实际应用中显著提升效率与性能，突破了以往LLM在长序列和多任务场景上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现代语言代理需要在长时、多轮交互中表现良好，但现有主流LLM系统依赖于全上下文提示，无选择地添加所有历史内容，导致内存无界增长、计算成本上升以及推理性能下降。论文动机是解决现有方法在多轮长任务下的低效和推理能力受限问题。

Method: 提出了一种端到端的强化学习框架MEM1，可使代理在多轮长任务中以常量内存运行。每一步，MEM1通过紧凑的内部状态整合之前记忆和新观测，同时有选择地丢弃无关或冗余信息。此外，作者构建了可扩展的多轮环境，利用现有数据集组合生成复杂任务序列，以支持在更真实和可组合的环境中训练。

Result: 在三个领域（内部检索问答、开放域网页问答和多轮网页购物）中的实验表明，MEM1-7B在16目标多跳问答任务上比Qwen2.5-14B-Instruct性能提升3.5倍，同时内存使用减少3.7倍，并能泛化到训练之外的任务长度。

Conclusion: 以推理为驱动的记忆整合为高效训练和部署长时、多轮交互式智能体提供了一种可扩展的替代方案，并在效率和性能间实现了优化。

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [32] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: 本文提出并开源了首个金融NLP全面评测套件FLaME，对比分析了23种语言模型在20项金融任务上的表现，显示出语言模型在金融领域的显著潜力，纠正了以往的低估。


<details>
  <summary>Details</summary>
Motivation: 现有的金融领域NLP评测方法存在重大不足，导致对语言模型在金融任务表现的低估。需要一个全面的评测基准来科学衡量语言模型在专门金融NLP任务中的潜力。

Method: 提出了第一个全面的金融语言模型评测基准套件FLaME，并系统性地对比了23种基础语言模型在20项核心金融NLP任务上的表现，涵盖了普通和“推理强化”类型的语言模型。所有软件框架、数据与结果均开源。

Result: 23种基础语言模型在20项金融NLP任务上的系统性实证分析，展示了其在金融任务中的潜力，并纠正了语言模型在金融NLP任务中的低估表现。

Conclusion: 建立了更科学的评测方法，首次全面展示了大型语言模型在金融NLP任务中的实际能力，为今后金融NLP研究和模型开发提供了新基准。

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [33] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Main category: cs.CL

TL;DR: 作者改进了BPE分词方式，在BPE前通过熵信息预处理，有效提升了中文等无切分语言的分词质量。


<details>
  <summary>Details</summary>
Motivation: 常规BPE因只考虑词频而不顾语言边界，导致在无切分语言如中文上表现不佳。需引入无监督信息学方法辅助分割。

Method: 提出两种信息熵驱动的预分词方法：一种结合点互信息与左右熵检测字符组合，另一种借助GPT-2模型预测熵辨识分界点，然后再用BPE分词。

Result: 在PKU数据子集上两种新方法的精度、召回率和F1得分均优于传统BPE，更贴合人工标准分词单元。

Conclusion: 引入熵信息的预分词策略显著提升了BPE在中文分词任务中的表现，对低资源和多语言场景的分词质量改进具有潜力。

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [34] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: 该研究发现当前大型语言模型在链式推理中自动纠错能力比之前报道的更强，无需特定微调即可展现出对扰动推理的自我修正，大量基于推理的工作其实可能建立在模型本有的能力之上。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面表现强大，但对问题描述和提示策略的微小变化非常敏感，推理过程也容易受到采样所引入的错误影响。因此，了解当前模型的自我纠错能力变得重要。

Method: 通过在模型的思维链推理中引入合成扰动，实验测量多个开源权重模型在不同数据集上的自我纠错能力。分析模型能否自动识别与纠正这些扰动所带来的推理错误。

Result: 实验观察到，多个开源模型在单次响应中展现出稳健的内在自我纠错行为，包括隐式与显式的错误纠正。这一能力超出通常文献中的描述，甚至存在于未专门为长链式推理微调的模型中。

Conclusion: 大型语言模型在推理自我纠错方面具备较强的内在能力，这意味着近期的大量“推理”模型研究可能只是放大了模型本就具备的能力。

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [35] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: 本文提出了一套新的大模型评测流程，结合了伊斯兰医学经典，利用检索和自我批判机制显著提高了医学相关问答的准确性和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰经典医学文献蕴含大量预防医疗、营养和整体疗法知识，但因文本难以接近和现代AI应用不足，导致这些文化医疗知识未被充分利用。现有大语言模型评测侧重事实回忆或用户偏好，未能有效大规模验证具有文化基础的医学指导。

Method: 提出统一评测流程Tibbe-AG，将30个精心挑选的先知医学（Prophetic-medicine）问题与人工验证疗法配对，并比较三种大语言模型（LLaMA-3、Mistral-7B、Qwen2-7B）在直接生成、检索增强生成和自我科学批判过滤三种配置下的表现。每个答案由另一语言模型作为评判代理进行质量评分。

Result: 引入检索提升了13%的事实准确率，代理式提示又通过加强机理洞察和安全性考虑提升了10%的得分。

Conclusion: 将经典伊斯兰医学文本与检索及自我评估机制结合，可实现可靠且具文化敏感性的医学问答。

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [36] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 本文发现传统摘要评价指标在政治观点等无偏见摘要任务中不可靠，提出用语言模型指标替代，并通过重排序和合成数据的调优方法显著提升了自动摘要效果。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，如政治观点摘要，生成无偏见摘要依然是大型语言模型（LLM）的关键应用。但目前的评估框架主要依赖传统指标，而这些指标对于有效测量摘要属性的适用性尚未被验证。此外，改进的摘要生成方法研发也处于初级阶段。

Method: 本文通过两步填补上述空白：（1）确定评价观点摘要质量的可靠指标；（2）研究LLM方法在零样本推理之外的有效性。具体做法包括：构建含人工标注的测试集评估评价指标的可靠性，比较传统指标与基于语言模型的指标性能，并在评价指标基础上对重排序方法、合成数据等创新策略进行测试。

Result: 实验表明，基于语言模型的评价指标在观点摘要任务中表现优于传统指标；重排序方法、利用合成数据进行偏好调优能进一步提升摘要质量。

Conclusion: 本文验证并推动了观点摘要任务中评价与生成方法的进步，确立了更为可靠的指标基础，并探索了高效的生成策略。该研究成果有助于未来可靠、公正的自动摘要系统开发。

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [37] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: 本文提出了大规模越南语文本分割和阅读理解数据集VSMRC，并证实多语种模型在越南语NLP任务中优异表现，为欠资源语言处理提供了新资源和新思路。


<details>
  <summary>Details</summary>
Motivation: 越南语作为全球第20大最常用语言，在文本分割和机器阅读理解（MRC）等核心自然语言处理任务上资源匮乏。该领域缺乏高质量的数据集和基准，制约越南语NLP技术的发展。

Method: 构建了VSMRC数据集，数据从越南语维基百科采集，包括15942个文档用于文本分割任务，16347个人工审核生成的多项选择阅读理解问答对。对现有多语种（如mBERT）和单语种模型在该数据集上进行了系统评估。

Result: mBERT多语种模型在文本分割与MRC任务中都优于单语种模型，MRC测试集准确率达88.01%，文本分割F1分数为63.15%。

Conclusion: 多语种模型在越南语等欠资源语言的NLP任务表现优越，VSMRC为该领域研究提供了可靠数据基础，也为其他欠资源语言NLP应用带来启示。

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [38] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: 本文提出了一种多模态AI音乐检测新方法，结合音频和自动转录歌词的特征，实现比传统方法更高的检测准确性和更强的鲁棒性，有望在实际中广泛应用。


<details>
  <summary>Details</summary>
Motivation: AI作曲技术快速发展，推动音乐产业变革，也带来了伪造与版权风险，亟需可靠的AI生成音乐检测方法。现有基于音频或歌词的检测器实际应用有限：音频法易受扰动且泛化性差，歌词法则受准确歌词获取限制。

Method: 提出一种创新的多模态、模块化后融合管道，将自动转录的唱词（歌词）与包含语音特征的音频信息融合，通过直接从音频中提取歌词相关特征提升鲁棒性并规避低级音频伪迹问题。

Result: 实验表明，该方法（DE-detect）优于现有歌词检测器，同时对音频扰动更具鲁棒性。

Conclusion: DE-detect为实际环境下检测AI生成音乐提供了一种有效且鲁棒的解决方案。代码已开源。

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [39] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: 本文提出了强化学习框架ProxyReward，包括自动生成的数据集与有针对性的奖励信号，使开源大语言模型在开放式长文本生成任务中取得明显性能提升，超过了现有主流方案。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型(LLM)在处理长文本理解方面有较多研究，但在开放式长文本生成（Open-LTG）任务上研究较少。而想要训练这类模型需要高质量的标准参考数据，现实中这类数据缺乏。此外，现有方法只是利用通用评估作为奖励信号，导致生成内容的准确性有限。

Method: 作者提出了ProxyReward框架，包括数据集和奖励计算方法。数据集通过简单提示自动生成，减少人工和标注成本。奖励信号则能有针对性地评价针对特定问题的信息全面性和准确性。框架基于强化学习方法。

Result: 实验表明，ProxyReward框架在Open-LTG任务上能让广泛使用的开源模型性能提升20%，效果甚至超过GPT-4-Turbo和LLM-as-a-Judge方法。模型对复杂开放式问题的处理能力显著提高。

Conclusion: ProxyReward框架为提升大语言模型在开放式长文本生成任务中的表现提供了有效方法，降低了数据准备和评估的难度，并显著增强了模型的生成质量。

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [40] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: 本文提出EvoLM模型套件，系统分析大模型多阶段训练中的各种影响、回报和权衡，提升模型评估透明性，并向社区公开所有相关资源。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的训练分为多个阶段，这导致模型在每个阶段的设计选择对下游任务影响难以评估。因此，研究迫切需要一种系统化与透明化的方式来分析语言模型训练动态。

Method: 提出EvoLM模型套件，通过从头训练100多个（1B和4B参数量级）语言模型，对预训练、持续预训练、监督微调和强化学习等多阶段训练过程进行系统分析，并评估其在语言建模和推理任务（涵盖域内与域外泛化）上的表现。

Result: 1. 过度预训练和后训练带来的回报递减；2. 持续预训练阶段易遗忘原预训练知识，需采取措施缓解；3. 持续预训练对于连接预训练与后训练阶段至关重要；4. 监督微调和强化学习在配置时存在多种权衡。

Conclusion: EvoLM为分析和理解语言模型多阶段训练动态提供了系统框架，揭示训练各阶段对模型能力的影响。发布了所有模型、数据集与流程，推动开放研究与可复现性。

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [41] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: 本文提出的LLaMA 3基础RAG新框架，在复杂问答场景下表现优异，能够更准确高效地产生上下文相关答案。


<details>
  <summary>Details</summary>
Motivation: 复杂问答任务面临多跳推理和长文档上下文理解的挑战，现有检索增强生成模型在这些方面不足。

Method: 提出了一个基于LLaMA 3的RAG框架，集成了密集检索模块、上下文融合和多跳推理机制，并采用联合优化策略提升鲁棒性和适应性。

Result: 实验结果显示，该系统优于现有的检索增强和生成基线，在精确性和上下文关联性方面表现突出。

Conclusion: 该框架能够更好地生成准确、基于上下文的答案，提升了复杂问答系统的效果。

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [42] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: DynScaling结合创新采样和预算分配机制，实现了资源受限下大语言模型推理性能提升，无需外部验证器，效果和效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在推理阶段通过增加计算量可提升表现，但实际应用往往受限于需要外部验证器或未充分优化计算资源，使其实用性不足。

Method: 提出DynScaling方法，包括并行-顺序采样策略和基于Bandit算法的动态预算分配。前者将并行采样与顺序采样结合，通过合成顺序推理链，提升推理多样性与连贯性；后者将计算资源分配视为多臂老虎机问题，根据采样结果的不确定性动态分配预算，提升效率。

Result: DynScaling在不依赖外部验证器的前提下，有效提升了大语言模型在实际有限资源下的表现。实验显示，该方法在任务效果和计算成本上均优于现有的无验证器推理扩展方法。

Conclusion: DynScaling能够在实际计算资源约束下，明显提升大语言模型的推理性能，无需外部验证器，具有较强的实际应用价值。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [43] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种融合改进型DeBERTa和广义学习系统的混合模型，在多个英文网络欺凌检测数据集上取得了领先的效果，并通过多种可解释性机制，提升了模型的透明度和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 随着网络通信平台的普及，青少年网络欺凌现象愈发严重，影响面广。已有的检测方法在理解上下文和模式识别方面存在不足，急需更高效且具有可解释性的智能检测方法。

Method: 提出了一种融合Transformer（具体为改进的DeBERTa模型，结合Squeeze-and-Excitation模块与情感分析）、Broad Learning System（GBLS分类器）的混合架构模型。通过集成多种解释机制（token级归因分析、LIME本地解释、置信度校准），提升性能和可解释性。

Result: 该模型在四个英文数据集（HateXplain、SOSNet、Mendeley-I、Mendeley-II）上获得显著性能提升，分别取得79.3%、95.41%、91.37%和94.67%的准确率。消融实验证实各部分贡献，失败案例主要集中在讽刺和隐含偏见的检测难点。

Conclusion: 所提出的混合框架不仅显著提高了网络欺凌检测任务的准确率，还加强了模型的可解释性，并为未来处理讽刺与隐性偏见问题提供了有益启示。

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [44] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Michaël Cadilhac,David Chiang*

Main category: cs.CL

TL;DR: 论文形式化证明了更深层的transformer在表达能力上强于浅层transformer，并通过理论和实验验证了深度对于长度泛化能力任务所需的最小深度预测效果。


<details>
  <summary>Details</summary>
Motivation: 深层transformer模型表现出更强的能力，但还未有理论证明其因深度而获得的新能力，论文旨在揭示深度transformer实际获得了哪些表达能力，并予以形式化。

Method: 首先研究只在attention中非定点计算的transformer，证明该类transformer与编程语言C-RASP表现力等价，并且这种等价是深度保留的；其次，通过分析等价的计数时序逻辑，证明更深C-RASP程序比浅层更有表达力，并实证其理论可预测transformer长度泛化能力所需深度。

Result: 证明在该类transformer中，模型深度增加会提升其表达能力，并且理论与实际实验表现一致：理论能够预测少位置编码时，transformer完成序列依赖任务所需的深度。

Conclusion: 该工作形式化、理论上和实证地证明了transformer深度提升模型表达力，揭示了深度对序列泛化能力的作用及其本质限制。

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [45] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Main category: cs.CL

TL;DR: 本文针对大语言模型输出的诚实性和有用性问题，提出了自我批判引导的好奇心精炼提示新策略，无需训练，整合自我批判和回答优化两步，在多个模型和数据集上显著提升了输出质量，是一种有效、可扩展的改进方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在多个自然语言任务上表现优异，但在输出持续可靠和有用内容方面仍存在难题。

Method: 提出了一种新颖的提示策略，称为自我批判引导的好奇心精炼提示，无需额外训练，通过在上下文中加入自我批判与精炼两个步骤，引导模型自我改进回答。同时，基于HONESET数据集，对OpenAI、Meta、Google等十个主流LLM进行了系统基准评测。

Result: 实验结果显示，该方法能在GPT-4o判定下全线提升各模型输出的诚实性与有用性，低质量回复数量下降，高质量回复增加，相较常规好奇心驱动提示，$[H^2$分数相对提升1.4%至4.3%。

Conclusion: 结构化的自我精炼是一种可扩展且无需额外训练的数据可信性提升策略，显著提升大语言模型输出的诚实与有用性。

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [46] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 本文提出基于MURIL架构的Hinglish网络欺凌检测方法，在六个公开数据集上显著优于现有多语模型，具备可解释性并对未来多语言欺凌检测方向提出建议。


<details>
  <summary>Details</summary>
Motivation: 随着数字通信平台的发展，网络欺凌事件日益增多，急需自动检测系统来保护用户。现有的网络欺凌检测系统主要针对单语文本，难以应对印度常见的印地语-英语混合（Hinglish）文本。本文旨在解决这一检测难题。

Method: 提出一种针对Hinglish文本的网络欺凌检测框架，基于MURIL（印度语言多语表示）架构。该方法结合归因分析和跨语言模式识别，实现了结果的可解释性。此外，通过消融实验分析不同技术组成对检测性能的影响。

Result: 在六个基准数据集（Bohra等、BullyExplain、BullySentemo、Kumar等、HASOC 2021、Mendeley Indo-HateSpeech）上，MURIL方法优于现有多语模型（如RoBERTa和IndicBERT），准确率分别为86.97%、84.62%、86.03%、75.41%、83.92%和94.63%，相较于其他模型提升1.36~13.07个百分点。

Conclusion: MURIL架构在Hinglish网络欺凌检测任务上取得了领先的效果，同时实现了可解释性，强调了特定层冻结、分类头设计和混合文本预处理的重要性，并指出了上下文、文化理解与讽刺识别等未来研究方向。

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [47] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: FinCoT结合金融专家推理知识，采用结构化链式思维提示，显著提升了金融领域大语言模型的准确率、推理效率及解释性，优于主流提示方法。


<details>
  <summary>Details</summary>
Motivation: 尽管结构化链式思维（CoT）在其他领域取得了成功，金融领域自然语言处理（FinNLP）主要依赖标准或非结构化CoT提示，而结构化CoT提示未受到足够重视，且其推理结构通常由非领域专家基于经验设计。本研究旨在探究领域专家知识指导下的结构化CoT，对提升金融任务表现的作用。

Method: 提出FinCoT方法——结合金融领域专家推理见解的结构化链式思维（CoT）提示。系统比较了三种主要提示风格：标准提示、非结构化CoT、结构化CoT，并以CFA风格的十个金融子领域问题评估其性能。FinCoT通过设计明确的结构化推理步骤引导大语言模型推理过程。

Result: FinCoT将大语言模型的表现由63.2%提升到80.5%，并将Qwen-2.5-7B-Instruct表现由69.7%提升到74.2%，同时生成的token数量相较于传统结构化CoT提示减少了八倍。FinCoT能够提升推理性能、降低推理成本，并生成更可解释、与专家思路对齐的推理轨迹。

Conclusion: 领域对齐的结构化链式思维提示能显著提升金融领域大语言模型的推理准确性和效率，同时提高推理的可解释性与专业性，优于非结构化及经验驱动的结构化CoT方法。

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [48] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文通过双语因果推理数据集实证发现大语言模型会内部化语言特有的认知偏置，首次验证了认知语言学理论在模型推理结构中的体现。


<details>
  <summary>Details</summary>
Motivation: 研究语言相对论对大语言模型(LLMs)认知与推理结构的影响，探究LLMs是否会内部化不同语言下的因果推理习惯性逻辑结构。

Method: 引入BICAUSE，一个包含中英文语义对齐的因果推理结构化双语数据集，涵盖正向与反向因果形式，通过分析LLMs在该数据集上的表现，研究模型的注意力分布、对因果词序的偏好、及模型内部语义抽象的对齐情况。

Result: 发现LLMs表现出与语系类型相关的注意力模式（如中文关注因果与句首连词，英文较均衡）；模型会内部化语言特定的因果词序偏好，对不符合偏好的输入表现下降（中文尤甚）；成功推理时，模型内部表现为跨语言的语义对齐抽象。

Conclusion: LLMs不仅模仿表层语言形式，也会内部化语言塑造的推理偏置。该现象首次通过模型结构分析，被认知语言学理论实证验证。

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [49] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Main category: cs.CL

TL;DR: 本论文提出了SGIC自引导迭代校准框架，通过用不确定性分数引导大语言模型实现多轮自校准，有效提升了RAG场景下模型的准确性和泛化能力，在多个类型LLM上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前RAG（检索增强生成）方法主要关注如何从候选文档中检索有用信息，较少关注大语言模型（LLM）的校准能力。而良好的校准能力对于提升推理准确性具有重要意义。

Method: 提出了SGIC（Self-Guided Iterative Calibration）框架。该方法首先计算不确定性分数，评估文档与查询的相关性及LLM回答的置信度。然后进行多轮迭代，动态调整不确定性分数并结合之前的回答，逐步优化校准。还设计了自校准训练集，以训练LLM高效利用不确定性分数提升回答准确性。

Result: 所提出的SGIC框架在闭源和开源类型的大语言模型上均取得了显著性能提升。

Conclusion: 通过引入不确定性度量及自引导迭代校准流程，有效提高了大语言模型在RAG任务中的校准和整体表现。

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [50] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: 作者构建了日语版的伦理理解评估数据集JETHICS，并测试了多种大模型，发现即使是最强的GPT-4o在此基准下表现也有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各类任务中表现突出，对其伦理理解能力的评估变得尤为重要，尤其在非英语语境中缺乏相关基准数据集。为此，作者希望为日语环境提供类似ETHICS的数据集。

Method: 本研究借鉴英语ETHICS数据集的构建方法，设计并制作了日语伦理评估数据集JETHICS，涵盖四类基于规范伦理理论和政治哲学的范畴，以及一类常识道德，共计78,000个样本。

Result: 实验结果显示，非专有大语言模型及GPT-4o在JETHICS上的表现都不理想：GPT-4o平均得分约0.7，最优的开源日语LLM得分约0.5，表明当前模型在伦理理解上有较大提升空间。

Conclusion: JETHICS为日语语境下大语言模型伦理评估提供了新的公开基准。实验结果暴露出现有模型在伦理理解方面的不足，为今后模型改进和伦理评估研究提供了参考。

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [51] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Main category: cs.CL

TL;DR: 本文批判性分析了仇恨言论数据集构建中的方法选择及其对数据集可靠性的影响，呼吁研究者在数据集创建过程中保持反思和透明。


<details>
  <summary>Details</summary>
Motivation: 当前仇恨言论数据集的构建涉及复杂的设计决策，需要在多个相互冲突的优先事项之间进行平衡，影响数据集的可靠性。

Method: 批判性分析了多样化仇恨言论数据集的构建方法，借助社会学家马克斯·韦伯的“理想类型”理论框架进行理论支撑。

Result: 总结了相关数据集构建中常见的主题和实践，对其对数据集可靠性的影响进行了讨论。提出建议信：数据集构建者需反思自身价值判断，并在整个过程保持透明与严谨。

Conclusion: 主张在仇恨言论数据集构建中采取反思性方法，研究人员应主动披露个人价值立场，以增强数据集的透明度与方法学中的严谨性。

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [52] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CL

TL;DR: 本论文比较了多种自动摘要模型在放射学报告简化任务中的表现，总结了各模型的优劣，对医疗自动摘要应用提供了有力基础。


<details>
  <summary>Details</summary>
Motivation: 放射学报告的Findings部分内容详尽冗长，而Impression部分则简洁明了地总结了诊断结论，因而探究如何利用摘要模型自动从Findings生成高质量的Impression对于医疗工作具有重要意义，可大大提高临床效率。

Method: 本研究利用公开的MIMIC-CXR数据集，对多种先进的抽象式自动文摘模型进行比较，包括T5-base、BART-base、PEGASUS-x-base、ChatGPT-4、LLaMA-3-8B和自行训练的Pointer Generator Network（带覆盖机制），并使用多种评价指标如ROUGE-1/2/L、METEOR、BERTScore对表现进行系统评估。

Result: 研究分析了多种预训练和开源大语言模型在医学文本自动摘要任务中的效果，辨识出各模型在摘要精准性和表现上的优缺点。具体结果未在摘要中详述，但表明在医疗自动摘要领域能为模型选择和应用提供数据支持。

Conclusion: 本研究为医疗领域自动文摘任务提供了系统的模型对比及评测结论，对医疗专业人士选择自动摘要解决方案提供了有价值的参考与指导。

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [53] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 本文提出利用弱标注数据，结合双语挖掘技术，构建低资源语言的语音翻译系统，并证实其能达到与主流多语种系统相当的效果。


<details>
  <summary>Details</summary>
Motivation: 高质量的有标注数据稀缺，尤其在低资源语言的语音到文本翻译系统开发中是一个重大挑战。

Method: 利用最先进的句子编码器进行双语文本挖掘，从多语种Shrutilipi语料库中挖掘数据，构建了Shrutilipi-anuvaad数据集，并对训练数据进行不同质量和数量的版本实验。

Result: 用弱标注数据训练的ST系统性能可以与SONAR和SeamlessM4T等大规模多模态多语种基线系统媲美。

Conclusion: 弱标注数据可有效用于低资源语言对的语音到文本翻译模型，其性能可达到主流多语种系统的水平。

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [54] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文通过加入多源内容相关性分析和精细语法错误检测，明显提升了自动化口语评估系统的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化口语评估系统在多方面评价时，通常未能充分利用内容相关性，忽视了图像或范例线索，并且对于语法错误的分析过于表面，缺乏详细的错误类型。

Method: 本文提出两项创新改进，构建一个混合评分模型：（1）多方面相关性模块，综合问题、相关图像、范例和L2说话者的口语回答对内容相关性进行全面评估；（2）利用先进的语法纠错技术和详细标注提取细粒度语法错误特征，识别具体错误类别。

Result: 实验和消融研究表明，这些新引入的组件显著提升了内容相关性、语言使用和整体自动化口语评估的效果。

Conclusion: 引入更加丰富和细致的特征集，有助于实现更全面、精准的口语评估。

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [55] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodębska,Karolina Seweryn,Szymon Łukasik,Wojciech Kusa*

Main category: cs.CL

TL;DR: 作者针对LLM安全性在波兰语场景下评估薄弱的问题，构建了手动标注且带对抗扰动的数据集，并测试多种分类模型。结果表明，HerBERT波兰语分类器在安全性能上优于同类方法，尤其在对抗条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型（LLM）的安全性评估多集中于英语等高资源语言，绝大多数其他语言（如波兰语）缺乏有针对性的研究和工具。为弥补这一空白，研究者希望改善波兰语环境下LLM的安全评估能力。

Method: 作者手动标注了波兰语安全文本数据集，并基于此设计制作了经过对抗性扰动的测试样本，增强评估难度。随后，作者对包括Llama-Guard-3-8B、基于HerBERT（波兰语BERT变体）的分类器、以及PLLuM（波兰化的Llama-8B）在内的三种模型进行了微调，分别在不同组合的数据下训练，并同主流公共安全模型进行了对比实验。

Result: 实验结果显示，基于HerBERT的分类器在总体性能上最佳，尤其在对抗性测试条件下表现突出。其他模型也有所提升，但HerBERT表现最稳定。

Conclusion: 波兰语安全评测领域缺乏有效工具，手动标注数据集与对抗性测试结合提升了评价准确性。基于HerBERT的分类器在波兰语LLM安全性检测任务中优于当前发布的防护模型，具有更好鲁棒性。

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [56] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Padó,Tanise Ceron*

Main category: cs.CL

TL;DR: 本文构建了巴西葡语新闻数据集，并验证了美式媒体框架（MFC）在跨文化背景的适用性。结果显示MFC框架大致可用，但部分需细微调整，应关注新议题下的通用分类选择。


<details>
  <summary>Details</summary>
Motivation: 现有的媒体框架（Media Frame Corpus, MFC）主要聚焦于美国的新闻议题，其分类和操作指南未必适用于其他文化背景下的媒体报道。本文旨在探究MFC框架在巴西葡萄牙语新闻中的适用性。

Method: 作者构建了一个新的葡语新闻数据集（FrameNews-PT），涵盖巴西政治和经济新闻，并采用MFC框架进行了多轮标注。通过人工注释和模型测试（包括微调及零样本模型）评估MFC框架在不同文化与议题下的泛化能力。

Result: MFC的15种框架在巴西背景下大体适用，只需对指南做细微调整。但部分框架使用较少，遇到新颖议题时则多用到“兜底式”通用框架。

Conclusion: MFC框架具有跨文化的适用潜力，但实际应用时仍需针对具体语境做出审慎调整，以确保对新议题的有效覆盖。

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [57] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本研究发现将知识图谱信息融入关系抽取模型，能显著改善在数据不均衡情况下的表现，通过图神经网络实现了跨数据集和不同实验设置下的稳定提升。


<details>
  <summary>Details</summary>
Motivation: 传统关系抽取方法在训练样本不平衡的数据集上表现有限，作者假设知识图谱中实体的图结构位置有助于关系抽取任务。

Method: 将知识图谱中的实体位置信息作为特征，通过图感知型神经Bellman-Ford网络，与已有关系抽取方法结合，在多数据集上实验，并在有监督和零样本设置下评估性能。

Result: 引入知识图谱相关特征可在多种数据集和实验设置下持续提升关系抽取性能。

Conclusion: 结合知识图谱信息能显著提升关系抽取模型的性能，尤其在各关系类别训练样本数量不平衡时效果更明显。

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [58] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 提出一种融合类型和实体信息的判别式闭集合信息抽取方法，在准确率和效率两方面均超过现有生成式模型，尤其适合大规模实体和关系抽取场景。


<details>
  <summary>Details</summary>
Motivation: 当前闭集合信息抽取方法在处理海量实体和多种关系时，尤其面对长尾关系时表现有限。大模型尽管效果好，但推理效率和资源消耗高，存在现实应用的瓶颈。该研究旨在改进关系抽取的准确率和效率。

Method: 提出一种判别式闭集合信息抽取新方法，方法融合了类型信息和实体特定信息，重点提升了长尾关系的抽取表现。同时，采用小模型以提升效率。

Result: 所提出的方法在大规模闭集合信息抽取任务中（涵盖百万实体和上百关系）表现优异，准确率超过现有主流的端到端生成式模型。结合类型信息尤其能让小模型达到甚至超越大模型的效果。

Conclusion: 该方法有效提升了闭集合信息抽取的准确性和效率，特别适合大规模应用场景，是比当前生成式方法更优的选择。

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [59] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: 本文讨论了大型语言模型的表征能力问题。作者认为，仅有结构对应性不够，只有当这种结构被实际用于解释任务表现时，LLMs才能算具有现实世界表征，但文本限制阻碍了这一目标的实现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）如GPT-4虽然能够生成各种有说服力的文本，但其表征现实世界能力存在争议。由于LLMs仅以文本数据为输入与输出，作者提出两个核心问题：LLMs是否具备表征能力？如果有，这种表征究竟是什么？

Method: 作者采用基于结构对应性的表征理论，分析LLMs是否及如何具备世界实体的表征能力，并初步梳理了相关证据。

Result: 作者认为：仅凭LLMs与世界实体之间存在的结构对应关系，并不足以说明拥有实体表征能力。只有当这些结构对应性在实际任务中被有效利用、能解释成功完成任务时，LLMs才可能真正获得现实世界内容的表征。

Conclusion: 要使LLMs在理论上具备现实世界表征，需解决LLMs“文本边界性”导致无法执行某些任务的难题。也就是说，单纯依靠文本信息，LLMs难以真正具备全面的现实世界表征能力。

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [60] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了InstructTTSEval，一个系统化评估TTS系统指令遵循能力的全新基准，并通过大量中英文测试样例和自动化评估，验证了当前方法仍有提升空间，将推动指令驱动TTS的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的语音合成（TTS）系统在表达说话者的音色、情感和韵律等副语言信息时，通常依赖固定风格标签或语音提示，缺乏灵活性。虽然近期引入了自然语言指令来提升模型对复杂风格的控制力，但缺乏针对指令驱动TTS系统的高质量评测基准和自动化评价指标，限制了模型能力的准确评估和优化。

Method: 本文提出了InstructTTSEval基准，用于衡量TTS系统对复杂自然语言风格控制指令的响应能力。该基准设计了三类任务：声学参数指定、描述性风格指令和角色扮演，涵盖中英双语，每类任务各1000条测试用例（共6000条），每个用例都配有参考音频。评估采用Gemini自动判别系统，评判TTS系统的指令遵循能力。

Result: 使用InstructTTSEval对现有TTS系统进行评估后，结果显示这些系统在复杂指令理解和执行方面仍有很大的提升空间。

Conclusion: InstructTTSEval为TTS社区提供了对复杂自然语言风格控制能力的标准化、系统化评测平台，有助于推动TTS系统向更强大、更灵活和更准确的方向发展。

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [61] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文系统梳理了大型语言模型在论证挖掘领域的最新进展，涵盖理论、数据、任务划分、技术实现及挑战展望，为相关研究者提供了权威综述和未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘（AM）在自然语言处理领域十分重要。随着大型语言模型（LLM）出现，AM面临着方法、工具和应用的重大转变。对LLM驱动下AM的最新进展进行系统梳理，有助于研究者把握发展脉络和未来方向。

Method: 综述性方法：对相关基础理论、标注框架、数据集及LLM在AM各子任务中的应用进行系统分类与整理，总结了当前主流模型、最新方法和评测实践，并分析了面临的挑战与未来趋势。

Result: （1）归纳了目前AM领域主流的理论、标注体系和数据集。（2）构建了AM子任务的全面分类体系，梳理了LLM技术（如prompt、推理链、检索增强等）如何重塑这些任务的实现方式。（3）对现有LLM模型架构、方法及评测方式做了详尽总结与批判性分析。（4）明确指出长文本推理、可解释性和标注瓶颈等尚待突破难题。

Conclusion: 本综述为LLM驱动的AM研究领域提供了清晰脉络、综合参考和未来研究方向，对后续研究具有重要指导意义。

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [62] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Main category: cs.CL

TL;DR: 本研究通过微调AfriBERTa模型，实现了对豪萨语文本六种情感的多标签检测，在验证集上取得了优异的性能，表明Transformer模型适用于低资源语言的情感分析。


<details>
  <summary>Details</summary>
Motivation: 情感检测在许多语言中已取得进展，但在资源稀缺的非洲语言（如豪萨语）上仍然有限，特别是在多标签情感识别方面。本研究旨在填补这一空白，提升自动化情感分析在低资源语言中的表现。

Method: 本研究采用AfriBERTa，这是一种在非洲语言上预训练的Transformer模型。通过数据预处理、分词，并利用Hugging Face的Trainer API对模型进行微调，使其能够对豪萨语文本中的六种情感（愤怒、厌恶、恐惧、快乐、悲伤、惊讶）进行多标签分类。

Result: 模型在验证集上取得了74.00%的准确率和73.50%的F1分数，显示Transformer在低资源语言情感检测任务中的有效性。

Conclusion: 本文证明了基于Transformer的模型（如AfriBERTa），经过恰当训练后，能在低资源语言（如豪萨语）上达到较高的多标签情感识别性能。

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [63] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Main category: cs.CL

TL;DR: 本文提出残差优化树（RiOT）方法，自动优化LLM提示词，通过多样性生成与残差连接，解决现有方法的局限，在五项推理任务基准上表现超过已有方法和人工提示。


<details>
  <summary>Details</summary>
Motivation: 现有自动化提示词优化方法存在两个主要问题：一是缺乏多样性，限制了有价值和创新方向的探索；二是语义漂移，导致在优化某一任务时，其他任务的表现下降。

Method: 提出了残差优化树（Residual Optimization Tree, RiOT）框架，通过文本梯度递进式地细化提示词，每步生成多个具有语义多样性的候选提示词，并基于困惑度选择最佳提示，还引入了文本残差连接，在优化过程中保留有益内容以减轻语义漂移，利用树形结构高效管理优化过程。

Result: 在常识、数学、逻辑、时序和语义推理五大公开基准数据集上，大量实验证明RiOT优于现有自动和人工提示词优化方法。

Conclusion: RiOT能够有效提升LLMs在多类任务上的表现，兼顾多样性与语义稳定性，是一种高效可扩展的自动化提示词优化新方法。

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [64] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: 该论文提出的AutoAnnotator系统采用多模型协同、自动生成和复查机制，实现了成本大幅降低和准确率提升，改进了大模型标注应用在细粒度任务的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的标注方式存在两大瓶颈：大规模调用商业API成本高，以及在细粒度语义理解（如情感/毒性分类）任务上的准确率不足，甚至低于专用小语言模型（SLM）。

Method: 提出了一种多模型协同标注的新范式，并设计了全自动标注框架AutoAnnotator。该框架包括两层结构：上层元控制器利用LLM负责SLM选择、自动生成标注代码和困难样本复核，下层由多个SLM组成，采用多模型投票执行标注。同时，难例通过元控制器二次复核并用于强化学习，分阶段持续学习以提升SLM泛化能力。

Result: 大量实验表明AutoAnnotator在zero-shot、one-shot、CoT及多数投票等设置下均优于现有开源/API LLM。与直接采用GPT-3.5-turbo相比，标注成本降低74.15%，准确率提升6.21%。

Conclusion: AutoAnnotator显著提升了标注的成本效能比并提升了准确率，为大规模自动标注任务提供了更优新方案。

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [65] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Main category: cs.CL

TL;DR: 作者提出OJBench新基准，系统评测了37个大型语言模型在竞赛级编程题上的表现，结果显示即使最好的模型面对高难度任务依然有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在数学和代码推理方面取得了显著进展，但现有的代码评测基准无法全面评估这些能力，尤其是在竞赛水平上。为了解决这一缺陷，作者开发了新的评测基准。

Method: 提出了OJBench，一套新的基准，收集自NOI和ICPC共232道编程竞赛题，并用其对37个不同类型的大模型进行了系统评测。

Result: 评测结果显示，即使是最先进的推理型模型（如o4-mini与Gemini-2.5-pro-exp）在高难度竞赛题面前也表现不佳。

Conclusion: 现有大模型在面对竞赛级代码推理题时仍存在较大挑战，需要进一步提升能力。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [66] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: 本文提出并实现了第一款专为尼泊尔语构建的生成式大型语言模型NepaliGPT，建立了相关语料库和评测基准，并证明该模型具有良好的文本生成性能，为尼泊尔语NLP研究带来突破。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在全球范围内取得巨大成功，但尼泊尔语领域一直缺乏专门的生成式语言模型。这导致该语言后续相关任务无法充分探索。为填补这一空白，本文旨在开发专门适用于尼泊尔语的生成式大语言模型。

Method: 本研究提出并开发了NepaliGPT，这是一款专门为尼泊尔语定制的生成式大型语言模型。作者收集和整理了多个来源的尼泊尔语文本，构建了高级别的Devanagari语料库，还首次推出了包含4,296个尼泊尔语问答对的NepaliGPT基准数据集。

Result: NepaliGPT在文本生成上取得了如下评测指标：困惑度为26.32245，ROUGE-1得分为0.2604，因果连贯性达到81.25%，因果一致性达到85.41%。

Conclusion: 本文填补了尼泊尔语生成式大型语言模型的空白，提出的NepaliGPT展示了良好的生成能力和相关评测指标，在尼泊尔语NLP领域迈出了关键一步，为后续任务打下了基础。

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [67] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: 本文分析了LLM处理长文本的三类噪声来源，提出多代理分块与聚合策略能在某些场景下优于单模型处理，尤其在上下文极长时。理论与实验证实此方法可突破长文本处理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）处理长文本时存在性能瓶颈，但其具体失败机制不明确，需要有理论框架系统性分析问题来源，并探究如何有效利用分块和聚合方法提升长文本处理能力。

Method: 作者提出了一个理论框架，将长文本任务中的失败模式分为三类：分块间依赖（任务噪声）、随上下文长度增加的混淆（模型噪声）、分块结果整合不完美（聚合器噪声）。基于此，理论分析和实验证明在特定条件下，采用多代理分块（multi-agent chunking）并对各块结果进行聚合可以提升任务表现。

Result: 实验证明理论框架有效，并指出在检索、问答、摘要等任务中，当模型噪声随输入长度超线性增长时，分块弱模型+聚合策略甚至可以超越如GPT4o这类单次大模型。

Conclusion: 通过精细设计的分块与聚合器策略，可为LLMs高效处理长上下文提供直接可行路径。本文理论和实验证明多代理分块法值得推广。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [68] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: 本文针对RAG推理中的检索瓶颈，设计了兼容现有存储系统的ISP方案REIS，大幅提升了检索性能和能效，具备实际部署价值。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）只能利用其训练数据中的知识，存在知识边界问题。检索增强生成（RAG）通过结合外部知识库扩展LLM知识，但其检索阶段在硬件层面上的数据移动带来显著性能瓶颈。现有的存储内处理（ISP）方法存在算法不适配、未加速数据检索以及需大幅硬件改动等问题。

Method: 提出了REIS——第一个为RAG量身定制的ISP系统。REIS采用三项关键机制：1）数据库布局将向量与文档高效关联，便于取回；2）数据按ISP友好原则跨存储面分布且使用轻量化的Flash Translation Layer，提升向量检索效率；3）利用存储系统现有的计算资源实现ANNS引擎，无须大硬件修改。

Result: 相比服务器级系统，REIS在检索性能与能效上分别平均提升了13倍与55倍。

Conclusion: REIS能够有效解决RAG系统在大规模向量检索中的性能瓶颈，提升整体效率且硬件改动小，推动了RAG实际部署的可行性。

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [69] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出多智能体的StoryWriter框架，有效提升了长篇故事生成的连贯性和复杂度，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 长篇故事生成对现有大语言模型（LLMs）来说仍是挑战，主要面临话语连贯性和叙事复杂性两大难题。现有方法难以保证故事的情节一致、逻辑连贯和内容丰富。

Method: 提出StoryWriter多智能体故事生成框架，由大纲代理、规划代理和写作代理三部分组成，分别负责事件纲要生成、详细情节规划和基于动态压缩保证连贯的故事生成。

Result: StoryWriter在人工和自动评价中，在故事质量和长度上均显著优于现有生成基线。同时，构建了包含约6000篇、平均8000字高质量长故事的数据集，并通过该数据集微调Llama3.1-8B和GLM4-9B，开发了StoryWriter_GLM等模型，展现了先进的长篇故事生成性能。

Conclusion: 通过多智能体分工协作机制，StoryWriter有效提升了长篇故事生成的连贯性和复杂性，推动了长文本生成技术发展。

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [70] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: 本文提出新方法结合样本筛选、重标注与大模型增强，有效提升隐性仇恨言论检测能力，F1分数提升12.9点。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论在社交媒体上日益严峻，且传统研究多聚焦于普遍有害言论，缺乏能检测隐晦、细微仇恨的通用技术。同时，公开有害言论数据集中可能已包含隐性仇恨言论但未被标注，而众包数据受主观影响易误标，为此需要改进检测方法。

Method: 提出一种新方法，通过三大步骤：影响样本识别、数据重新标注、以及利用Llama-3 70B和GPT-4o进行数据增强，在现有有害言论数据基础上提升隐性仇恨检测。

Result: 实验表明，该方法能显著提升隐性仇恨检测效果，F1分数提升12.9点，相较基线方法优势明显。

Conclusion: 该研究提出的基于样本筛选、重标注和大模型增强的数据处理流程，可有效提升对隐性仇恨言论的检测能力，并增强模型跨数据集的泛化性。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [71] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: 本研究提出RELIC框架，利用高资源语言的示例改进低资源印度语言奖励模型训练，无需大规模本地偏好数据即可大幅提升模型性能，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前大多数开源多语种奖励模型主要基于高资源语言的数据进行训练，导致在低资源印度语言上的表现不佳；而为低资源语言收集大量高质量偏好数据成本极高，传统基于偏好数据的训练方式变得不切实际。

Method: 提出了一种新颖的低资源语言奖励建模方法RELIC，其通过训练一个检索器，利用配对排序目标从高资源辅助语言中选择能最好区分优选和次优响应的上下文示例，实现高效的上下文学习。

Result: RELIC在三个偏好数据集上经过广泛实验，相较于零样本提示和当前最好的示例选择方法，在低资源印度语种上奖励模型准确率显著提升（以Bodo语言为例，LLaMA-3.2-3B奖励模型准确率分别提升了12.81%和10.13%）。

Conclusion: RELIC框架在低资源印度语言奖励建模任务中，能够有效克服高质量偏好数据缺乏的难题，显著提升奖励模型对人类偏好的对齐表现，优于现有的方法。

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [72] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Main category: cs.CL

TL;DR: 本文发现ASR系统对Newcastle地区方言表现较差，主要受地方性语言特征影响。建议丰富训练数据中的方言，并结合社会语言学方法减少区域性识别偏见。


<details>
  <summary>Details</summary>
Motivation: 大多数自动语音识别（ASR）系统训练数据更偏向主流方言，导致其对地区方言（如Newcastle英语）识别效果不佳。尽管先前研究关注了种族、年龄和性别等偏见，但地区性偏见研究不足。

Method: 采用两阶段分析：首先对部分样本进行人工错误分析，识别ASR误识别的关键语音、词汇和形态句法错误；其次，对地区代词“yous”和“wor”的识别进行系统案例研究。

Result: ASR的识别错误与地方性方言特征直接相关，而社会因素的影响较小。

Conclusion: 建议在ASR训练数据中增加方言多样性，并强调社会语言学分析在诊断和解决区域性偏见中的价值。

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [73] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 针对语音识别模型持续学习时无法访问旧数据且容易遗忘的问题，提出模仿人脑睡眠过程的分解-集中方法，有效减少遗忘，在多语境下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络语音识别模型需要能够持续吸收新数据，且通常无法访问原始训练数据。这要求模型能在多语言、无回顾（rehearsal-free）条件下持续训练，但这样容易导致灾难性遗忘（catastrophic forgetting）的问题。

Method: 受人脑通过醒-睡周期学习和巩固知识的启发，提出了一种包含两个阶段（分解factorization和集中centralization）的持续学习方法，通过学习和融合知识，提高模型的持续学习能力。

Result: 在一系列不同的语码转换（code-switching）数据集上实验，结果表明centralization阶段能通过多重分散的低秩适配器有效防止灾难性遗忘，积累多种知识。

Conclusion: 提出的持续学习方法（分解+集中阶段）有效减轻了在复杂条件下语音识别模型的灾难性遗忘问题，适用于没有原始训练数据情况下的模型持续优化。

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [74] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文提出了首个支持流式处理的口音转换模型，能够实时将非母语口音转换为本地化口音，并且保证说话人身份和韵律特征，性能与顶级模型持平，同时拥有稳定的低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前口音转换（AC）系统难以实时处理语音流，且需要在改善发音、保持说话人特征和韵律之间权衡，因此亟需一种能够流式处理的高效口音转换方法。

Method: 提出了一种首个支持流式处理的口音转换模型。通过将现有AC架构中的编码器替换为Emformer，并优化推理机制，使模型能够实现流式语音转换。此外，结合原生TTS模型生成理想的训练数据以提升训练效率。

Result: 该流式AC模型在保持低延迟的同时，性能与现有顶级AC模型相当，是首个支持流式处理的口音转换系统。

Conclusion: 本文提出的流式口音转换模型，不仅改善了发音，且能够实时处理输入语音，兼顾了说话人信息及韵律等多个因素，为口音转换技术的发展和应用提供了新的可能性。

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [75] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 提出了量化LLM世界模型能力的新评估框架，发现大模型泛化与稳定性提升有限，建议未来评测关注模型的内在语义理解而非单一准确率。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚大语言模型（LLMs）是否具备“世界模型”，即能支持语义泛化、理解超越表层模式的结构化知识体系。尤其是在高风险应用场景，评估LLMs的可靠性成为关键。

Method: 作者提出了一种形式化的评估框架，通过分解模型输出的变异性（用户目的、表述方式、模型自身不稳定性）来判断LLM是否具有稳健的世界模型。评估方法旨在检测模型是否能对不同语义的提示表现出差异，同时对语义等价但表述不同的提示给出一致输出。

Result: 结果表明，大模型在输出变异性上，更多归因于用户目的的变化，显示出更强的世界模型能力。但这一提升并非在所有领域都明显，且大型模型的稳健性优势多数情况下仅为适度提升。

Conclusion: 仅依靠准确率的基准评测已经不够，未来应更关注模型内在理解世界结构与稳定性的语义层面诊断。框架揭示了模型在语义泛化与稳定性方面的真实表现，有助于推动LLMs评测更科学、更贴合高风险应用需求。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [76] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Main category: cs.CL

TL;DR: 综述了2020-2025年生物医学领域大语言模型驱动的合成数据生成研究进展，指出当前应用多集中于非结构化文本与prompt生成，人类评价为主。未来需突破模型适应性、可获得性与标准化等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学领域广泛存在数据稀缺、隐私保护和数据质量等挑战。大语言模型（LLMs）在合成数据生成方面的快速进展，为该领域带来了新的可能性。该综述旨在系统梳理LLM驱动的合成数据生成在生物医学中的研究与应用现状。

Method: 本综述遵循PRISMA-ScR指南，从PubMed、ACM、Web of Science和Google Scholar收集2020至2025年间发表的相关文献，共纳入59项研究，对文献中的合成数据生成的应用、方法和评估进行了系统分析。

Result: 分析发现，被研究数据类型主要为非结构化文本（78.0%）、表格数据（13.6%）、多模态数据（8.4%）；常用生成方法包括Prompting（72.9%）、微调（22.0%）和专用模型（5.1%）；评估方式主要有人类评价（55.9%）、内在指标（27.1%）和基于LLM的自动评价（13.6%）。

Conclusion: LLM驱动的合成数据生成已在生物医学领域得到初步应用，展现出较大潜力，但目前在跨临床领域适应性、资源与模型可获得性、评估方法标准化等方面仍存在挑战。未来需进一步促进评估标准统一和技术普及。

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [77] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: 作者提出了多维感知建模与大规模数据集，发现科学新闻的积极感知能够显著预测公众参与，推进了科学传播的效果评估与优化。


<details>
  <summary>Details</summary>
Motivation: 科学传播者难以预测公众如何看待和互动科学新闻，而有效的公众参与对于建立对科学的信任和理解至关重要。信息体量越来越大，预测公众感知变得愈发困难，因此亟需新的方法对公众感知进行系统建模。

Method: 作者提出了一个建模公众感知的计算框架，涵盖例如新闻价值、重要性、惊奇性等12个维度。基于该框架构建了包含来自美国和英国2,101位参与者、10,489个注释的大规模科学新闻感知数据集，并开发出能够预测感知评分的NLP模型。此外，通过大数据分析和Reddit的自然实验，探讨感知的影响因素及其对科学内容参与度的预测能力。

Result: 发现公众对科学新闻的感知最主要受到其获取科学新闻频率的影响，人口统计因素影响较小。利用构建的模型与数据，发现具有更积极感知分数的帖子在Reddit上显著获得更多评论和点赞，这一现象在不同科学内容和不同表述方式中均一致。

Conclusion: 细致建模公众对科学信息的感知能够有效预测公众的兴趣与参与度，为科学传播提供预测和提升公众参与的新路径。

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [78] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: 利用大语言模型辅助开发临床基于规则的NLP系统，不仅大幅提升开发效率，还提高了召回率和透明度，为相关系统建设提供了一种更优的新途径。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习和大语言模型在自然语言处理领域取得了进展，但由于可解释性和高效性，基于规则的NLP系统在临床领域仍然被广泛应用。然而，这类系统的手动开发与维护非常繁琐，尤其在面对高度语言多样性的任务时更是如此。

Method: 提出了一种创新方法，在基于规则NLP系统开发阶段引入LLMs，仅在开发期间使用LLMs。初步实验聚焦于基于规则NLP流水线的前两个步骤：从临床笔记中找到相关片段，并从片段中提取用于命名实体识别的关键词。

Result: 实验表明，利用所提方法在临床相关文本片段识别方面表现出极高的召回率（Deepseek: 0.98, Qwen: 0.99），在关键术语提取用于NER方面也达到了1.0的召回。

Conclusion: 这种新方法为NLP系统开发开辟了一条有前景的新方向，实现了基于规则系统的半自动化或自动化开发，相较于传统深度学习模型方案更快速、经济且更具透明性。

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [79] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: 论文提出街景定位和说明任务GeoGuess，构建专用的数据集和多层级推理方法，实验证明方法有效，推动了复杂多模态推理研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理任务在评测上存在缺陷，尤其是很少关注对不同细粒度层级的视觉线索进行推理，而这类推理在实际场景中频繁出现。该研究旨在填补对此类多层级视觉推理的考量空白。

Method: 作者提出了一个新任务GeoGuess：系统需根据街景图片推测其拍摄地点并给出详细解释。为了支撑该任务，建立了一个包含全景图-地理坐标-解释三元组的数据集GeoExplain，并提出了新方法SightSense，能够基于分层视觉信息和外部地理知识进行推理和解释生成。

Result: 通过实证分析和实验，GeoExplain数据集和SightSense方法在GeoGuess任务上取得了优异表现，验证了其有效性。

Conclusion: GeoGuess任务及其配套数据集和方法，有效推动了多模态层级推理与地理知识结合的研究，展示了对实际复杂场景的处理与解释能力。

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [80] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文提出自适应稀疏注意力机制ASEntmax，并优化位置编码，使Transformer在长文本和固定模式识别任务上超越传统softmax和其它基线。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer利用softmax计算注意力权重，导致注意力分布稠密。当序列变长时，很多无关token也分得注意力概率，容易导致表征能力下降，尤其在需要关注固定模式任务时更为明显。

Method: 提出采用稀疏注意力机制$alpha$-entmax，它能给不相关token分配精确零概率。进一步提出Adaptive-Scalable Entmax(ASEntmax)，为$alpha$-entmax设计可学习的温度参数，使其注意力分布能在稀疏与稠密之间自适应。还优化了位置编码设计，提升对固定模式的定位与泛化能力。

Result: 将ASEntmax与标准Transformer层和优化的位置编码结合后，在长上下文泛化任务上，模型性能大幅优于softmax、scalable softmax与固定温度的$alpha$-entmax等基线方法。

Conclusion: 基于稀疏注意力和自适应温度参数的新方法（ASEntmax），配合合适的位置编码，能有效提升Transformer模型在长序列和固定模式任务上的表现。

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [81] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Main category: cs.CL

TL;DR: 本文提出了一个可将查询根据用户主观偏好路由到最佳模型的轻量级路由器Arch-Router，并在主观偏好匹配任务上超越主流商用模型，支持无缝扩展新模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）路由方法在两个方面有限：一是用的评测方法难以反映基于主观标准的人类偏好，二是可选模型种类受限。因此，需要一种能表达用户优先级、且能灵活选择不同模型的路由框架。

Method: 提出了一种‘基于偏好对齐的路由框架’，将查询映射到用户定义的领域（如旅行）或操作类型（如图片编辑），实现偏好的编码。具体实现为一个1.5B参数的Arch-Router模型，无需重训练或结构修改即可支持新模型接入。

Result: 在对话数据集上的实验显示，该方法在匹配查询与人类偏好的任务上取得了SOTA效果，超过了主流的商业专有模型，提升了路由过程的透明度和灵活性。

Conclusion: 本文提出的Arch-Router模型通过主观偏好对齐实现了更优的大语言模型路由，支持高效灵活地引入新模型，并提升路由决策的透明度与匹配效果。

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [82] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Main category: cs.CL

TL;DR: 探针揭示的句法表示无法有效预测大模型的实际句法能力，潜在机制与实际表现存在明显脱节。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在句法处理上表现出色，但其内部如何表征句法结构仍是解释性研究的开放问题。探针分析虽然被广泛用以揭示模型内部句法表示，但目前尚无系统性研究证实，探针准确率是否能够可靠预测模型的下游句法性能。

Method: 本文采用“机制与结果”的框架，评估了32个开源Transformer语言模型，通过探针（probing）技术提取模型内部的句法特征，再将这些特征与下游特定句法评测的表现进行对比分析。

Result: 研究发现，探针得到的句法特征难以预测模型在面向英语多种语言现象的下游句法评测中的表现，表明模型潜在表示和实际输出句法能力间存在显著断层。

Conclusion: 通过对32个开源Transformer模型的分析，研究发现，用探测方法得到的潜在句法特征，并不能很好预测模型在实际句法任务中的表现。探测分析下获得的模型句法知识与下游可观测的句法行为存在较大脱节。

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [83] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Main category: cs.CL

TL;DR: 本文提出LegiGPT框架，利用GPT-4和XAI分析韩国交通立法，揭示党派与选区特征对立法结果的关键影响，为政策制定与基础设施管理提供参考。


<details>
  <summary>Details</summary>
Motivation: 立法者的政治意识形态极大影响了立法决策，因此理解其对政策制定的影响至关重要。当前缺乏有效的工具深入分析这种影响机制，尤其在交通政策立法领域。

Method: 提出了一个新框架LegiGPT，将大型语言模型（LLM）与可解释人工智能（XAI）相结合，分析与交通相关的立法提案。方法包括利用GPT-4进行零样本多阶段筛选与分类，通过逐步过滤（关键词、短语、语境相关性）甄别交通类提案，并结合XAI技术探究党派属性与立法特征之间的关系。

Result: 通过分析韩国第21届国会立法数据，发现保守派与进步派提案人数量和比例、选区规模及选民人数是影响立法结果的关键因素。两党都通过不同方式（如发起或支持提案）积极参与两党合作的立法。

Conclusion: LegiGPT集成LLM与XAI为理解立法动力和制定政策提供了新工具，对基础设施规划和治理有重要意义。

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [84] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: 本文提出了三阶段ReasonGRM框架，通过改进推理路径生成和优化训练策略，显著提升了生成式奖励模型在复杂任务下的表现。


<details>
  <summary>Details</summary>
Motivation: 生成式奖励模型（GRMs）在捕捉人类偏好方面比标量奖励模型更灵活，但由于推理能力有限，导致推理路径不完整、过度臆测，进而造成幻觉或关键信息缺失，影响了复杂任务下的效果。本文旨在解决GRMs在推理能力上的短板。

Method: 提出ReasonGRM，一个三阶段的生成式奖励建模框架。第一阶段用Zero-RL生成简洁且结果导向的推理路径以减少关键遗漏。第二阶段采用新颖的评价指标R*，依据生成概率对推理路径打分，优先选择用更少探索就能得到正确答案的路径，减少训练中的幻觉数据。第三阶段通过在挑战性样本上的强化学习提升模型对偏好的区分能力。

Result: 在三个公开基准上，ReasonGRM达到了有竞争力或最优的表现，平均比之前最佳的GRMs高1.8%，比GPT-4o等商业模型高5.6%。

Conclusion: ReasonGRM通过推理感知训练和高质量推理路径选择，有效提升了生成式奖励模型的表现，强调了推理路径在可靠偏好建模中的重要性。

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [85] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: 本文分析了提示偏差对大语言模型在视觉问答任务中两类不确定性（认知与偶然不确定性）评估的影响。研究发现，去除提示偏差能提升不确定性量化表现，且模型信心较低时偏差影响更大，特别容易导致对认知不确定性的低估。该结论有助于理解偏差缓解在不确定性评估中的作用，为相关技术发展提供思路。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在开放式任务中的广泛应用，准确评估反映模型知识缺乏的“认知不确定性”变得非常重要，以保证结果的可靠性。由于任务中存在多种有效答案，所以“偶然不确定性”使得评估挑战加大。本文旨在探究提示偏差对两类不确定性评估的影响及其权衡。

Method: 在视觉问答（VQA）任务下，实验研究消减提示（prompt）引入偏差对GPT-4o模型不确定性量化的影响。进一步，对GPT-4o和Qwen2-VL模型，在不同去偏置信心情况下，分析各种提示偏差对认知与偶然不确定性测量的影响。

Result: （1）消减由提示引入的偏差可以改善GPT-4o模型的不确定性量化效果；（2）在信心较低时，各种偏差对两类不确定性的变化影响更大；（3）信心越低时，偏差导致模型对认知不确定性的低估（即过度自信），但对偶然不确定性无显著方向性影响。

Conclusion: 提示偏差对不确定性量化有独特且不同的影响。认识这些影响有助于设计更先进的不确定性评估与偏差缓解技术。

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [86] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: LM-SPT提出一种新型语音分词方法，通过语义蒸馏及结构改进，实现语音token更好地与语言模型对齐，在多种帧率和多项任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着语音语言模型（SLM）的快速发展，如何实现语音与文本之间更有效的对齐成为核心问题。当前语音分词方法通过区分语义信息与声学信息来提高与语言模型的兼容性，但语音token序列通常远长于文本，对建模效率造成挑战。降低帧率虽可缩短序列，但常规池化法易损害语义结构。

Method: 本文提出LM-SPT语音分词方法，采用新颖的语义蒸馏策略：非直接匹配教师-学生特征，而是仅用语义token重构语音，并最小化原始与重构波形经ASR编码器后的表示差异。此外，改进了分词器结构，支持多种帧率，提升语音与语言模型的语义对齐。

Result: 实验表明，LM-SPT相比基线方法具备更高的重构保真度。基于LM-SPT token训练的SLM在语音转文本任务中表现优良，且在文本转语音任务中持续优于其他方法。

Conclusion: LM-SPT通过间接、数据驱动的语义对齐方法，有效提升了语音token与语言模型的结合效果，在跨模态任务中展现出卓越表现，解决了语音token序列过长和语义损失的双重问题。

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [87] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 作者提出了LIRAS框架，通过融合语言和视觉信息并进行贝叶斯逆向规划，实现了更高效、符合人类判断的社会推断能力，在多项任务中超过现有最优模型。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中，社会推断往往依赖多模态信息，尤其在新奇情境下语言能弥补视觉难以观测的信息。目前缺乏能有效整合语言和视觉进行社会推理的模型。

Method: 提出 LIRAS 框架，将多模态社会推理建模为构建情境化的代理与环境表示，通过多模态语言模型解析语言和视觉输入，并利用贝叶斯逆向规划进行概率性推断。

Result: LIRAS 在多个基于认知科学实验的社会推理任务上，优于消融模型及其它最先进方法，更贴近人类判断。

Conclusion: LIRAS 框架能够整合语言和视觉输入，有效进行情境相关的社会推断，并在多项社会推理任务中优于现有模型和消融版本。

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [88] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出SocialSim框架，通过模拟社交披露和社会意识生成高质量情感支持对话数据，所合成数据和对话机器人效果优于现有方法，显著推动情感关怀对话系统发展。


<details>
  <summary>Details</summary>
Motivation: 情感支持对话（ESC）能有效帮助人们减轻心理压力，但构建大规模ESC数据集的众包方式成本很高，且现有利用大模型的数据增强方法忽视了ESC中的社交动态，导致模拟效果有限。

Method: 本文提出了SocialSim框架，在模拟ESC时引入了社交互动的两个关键要素：社会披露（通过构建多样且真实的求助者角色库）和社会意识（通过认知推理生成支持性回应），从而提升增强对话的真实性和质量。基于SocialSim，作者构建了大规模合成ESC语料库SSConv，并基于此训练聊天机器人。

Result: SSConv语料库在质量上甚至优于众包ESC语料，基于其训练的对话机器人在自动和人工评测中均达到最新最优水平。

Conclusion: SocialSim提供了一种可扩展合成ESC数据的方法，有望大幅提升情感关怀服务的可及性和实用性。

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [89] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: 提出了一种新型跨模态黑盒攻击（CAMO），通过将恶意指令分解为良性图片和文本片段，利用LVLM推理重组后逃避检测，实验证明现有安全机制存在重大安全隐患。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）在多模态任务中表现出色，但仍容易被所谓的jailbreak攻击绕过其内置安全机制，从而生成受限内容。目前的黑盒jailbreak方法大多依赖于对抗性的文本提示或图像扰动，但这些方法容易被常规内容过滤系统检测，效率较低。作者希望改进攻击的隐蔽性和效率，揭示现有LVLM安全机制的不足。

Method: 提出了一种名为Cross-modal Adversarial Multimodal Obfuscation（CAMO）的新型黑盒jailbreak攻击框架。该方法将恶意指令分解成语义上良性的视觉和文本片段，利用LVLM的跨模态推理能力，通过多步推理隐蔽地重构有害指令，从而规避检测。该方法可调节推理复杂性，查询次数显著少于以往方法。

Result: 在主流LVLMs上进行了全面实验，CAMO显示出强大的攻击有效性和跨模型迁移能力，突破了现有的内容过滤和安全机制。

Conclusion: LVLM的内置安全机制存在显著漏洞，依靠对抗性输入分解和跨模态推理的攻击能够高效且隐蔽地绕开这些防护。需要更加高级且对齐感知的安全方案来抵御此类攻击。

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [90] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Main category: cs.CL

TL;DR: 本文提出Distillnote框架，利用LLM多种方式生成临床笔记摘要，显著提高文档压缩效率，同时在心力衰竭预测等任务中表现更优。相关临床数据已开放，促进领域发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成精炼患者信息摘要和减轻医疗文档负担方面具有巨大潜力。当前医疗工作者面对繁重的临床书写任务，该研究希望通过自动化方法提升文档处理效率和支持临床决策。

Method: 提出了Distillnote框架，利用LLM进行临床笔记总结。方法包括：(1) 一步直译（One-step）总结；(2) 针对独立临床见解的结构化（Structured）总结；(3) 进一步压缩结构化总结的精炼（Distilled）总结。使用生成的摘要数据评估模型在心力衰竭预测任务中的表现，以及与原始临床笔记模型做对比，同时还采用LLM打分与临床医生双盲配对对比评价总结质量。

Result: 精炼（Distilled）总结实现了79%的文本压缩率，并在AUPRC指标上比用原始笔记训练的LLM提高了最多18.2%。使用LLM和医生评估发现，临床医生更倾向于一步直译总结的相关性和可操作性，但精炼总结在效率和减少幻觉（错误内容）方面具有显著优势。

Conclusion: Distillnote通过不同层次的LLM生成摘要，大幅提高了临床文档处理效率，在心力衰竭预测等任务表现优异，且压缩内容后仍可保持甚至提升下游任务性能。该工作为未来自动化医疗文档研究提供了开放数据和方法。

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [91] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: 本文提出MIST方法，可通过语义微调高效地实现对黑盒LLM的越狱。MIST在多种主流模型上的攻防实验中均表现优秀，并兼顾攻击成功率和计算效率，对LLM安全有重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）虽经过价值对齐训练，但仍易受到越狱攻击，从而输出有害内容。对于黑盒LLM，由于其输入是离散字符、访问受限且查询次数有限，越狱难度更大。

Method: 提出MIST方法（Iterative Semantic Tuning），通过迭代语义调整，在保持原始语义意图的基础上，使LLM生成有害内容。MIST结合了顺序同义词搜索及其优化版本（顺序决定优化），以平衡语义相似性和计算效率。

Result: 在两种开源模型和四种闭源模型上，MIST在攻击成功率和攻击可迁移性方面均达到或超过了目前主流白盒和黑盒越狱方法。同时，实验还验证了MIST在计算效率上的实用性。

Conclusion: MIST为黑盒大语言模型的越狱攻击提供了一种有效且高效的方案，对目前AI安全领域的对抗攻击与防御具有重要参考价值。

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [92] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了不同语言模型在处理高频和低频事实上的表现，发现高频事实学习相似，但低频事实存在明显差距，强调了模型架构和大小对学习稀有知识能力的不同影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型的样本效率对于实际训练成本具有重要影响，尤其是在信息分布呈长尾的真实文本环境下，需要模型能够有效学习并保持罕见事实。

Method: 对不同架构和规模的模型，基于同一预训练数据进行分析，并用事实在训练语料库中的出现频率进行标注，进而评价模型在不同频率事实上的表现。

Result: 大部分模型在高频事实表现相似，但在低频事实表现有明显差异，从而揭示了模型架构、规模与事实学习效率之间的关系。

Conclusion: 模型在学习高频事实上的表现差异不大，但在低频事实的学习与保持能力上表现出显著区别。

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [93] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 本文提出通过语言瓶颈模型（LBM）以自然语言摘要解释和预测学生表现，在准确性与可解释性间取得平衡，并显著降低数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统的知识追踪（KT）方法虽然能评估学生知识，但由于依赖难以解释的隐式向量表示，导致可解释性不足。即便是基于LLM的方法，也容易产生虚构内容且无法保证准确性。因此，迫切需要一种可解释且准确的知识追踪方法。

Method: 作者提出将知识追踪建模为一个逆问题：通过学习最简洁的自然语言摘要，使得学生的历史回答能被解释且未来回答可被预测。具体提出了“语言瓶颈模型（LBM）”，包括一个编码器LLM用于生成可解释的知识摘要，以及一个冻结的解码器LLM，仅以该摘要重建和预测学生答题情况。通过让所有预测信息仅通过简短摘要传递，保证信息的准确性和可解释性。模型训练中采用“相对分组策略优化”，用下游解码准确率作为奖励信号提升摘要质量。

Result: 在算术类合成数据集和大规模Eedi真实数据集上实验，LBM的预测准确率与最先进的KT或直接LLM方法持平，但显著减少了对学生答题轨迹数据量的需求。使用分组策略优化能有效提升摘要的质量和模型性能。

Conclusion: 语言瓶颈模型（LBM）在保证高准确率的同时，极大提升了知识追踪任务的可解释性，并且所需的学生数据更少，具有实际应用价值。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [94] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: 本研究提出TeXpert基准数据集，系统评估LLM生成科学文档LaTeX代码的能力。结果发现主流模型此项任务表现有限，错误如格式和包管理普遍。开源模型竞争力强。TeXpert为提升LLM辅助科学写作奠定了评测基础。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）可以通过自然语言生成LaTeX代码，帮助科研写作，但目前缺乏专门评估LLM生成LaTeX能力的基准数据集。

Method: 作者提出了TeXpert基准数据集，包含针对科学文档不同难度组件的自然语言提示，系统评估多个LLM（包括开源与闭源模型）在LaTeX代码生成上的表现，并分析常见错误类型。

Result: 实验表明，尽管某些LLM在现有通用基准上表现优异，但在LaTeX代码生成上准确率明显下降，尤其是任务复杂度提升时。开源模型（如DeepSeek v3与DeepSeek Coder）在LaTeX任务上与闭源模型表现相当。此外，格式和包管理错误常见，反映出大多数模型训练集中LaTeX示例多样性不足。

Conclusion: TeXpert数据集系统揭示了LLM在LaTeX生成方面的不足，强调了改进训练数据和评测方法的重要性，以提升LLM在科学写作自动化领域的实际效用。

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [95] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: 本文提出了一种LLM自构建和更新的知识图谱（带超边）架构，显著提升了个性化信息存储、提取及时间关系处理能力，在多个主流数据集上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 个性化语言模型，尤其是在互动中能够顾及用户历史信息，是当前的重要研究方向。尽管大语言模型（LLMs）和检索增强生成（RAG）技术提升了模型的事实掌握能力，但持续存储和利用大量个人信息以实现个性化回复仍是一大难题。

Method: 作者提出利用外部内存——由LLMs自行构建和更新的知识图谱来解决上述挑战。具体方法是扩展了AriGraph架构，首次引入标准边与两类超边相结合的图结构，用于统一和增强知识图谱的构建与知识抽取过程。

Result: 在TriviaQA、HotpotQA和DiaASQ数据集上的实验表明，该方法有助于实现图结构构建与知识抽取的统一和健壮性。另外，作者还增强了DiaASQ基准数据集：如引入对话时间参数及同一说话人在不同时间的矛盾陈述。测试结果显示，问答系统对这些变化具有很强的鲁棒性，能够有效维护和利用时序关系。

Conclusion: 本文提出的结合标准边与多种超边的知识图谱架构，不仅提升了个性化对话中的信息存储与利用能力，还展现出在处理复杂时序和自相矛盾信息上的性能稳定性，因此对个性化LLM系统具备重要价值。

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [96] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: 通过实证分析导师培训课程，发现使用大语言模型生成的反馈有助于推动部分课程的学习成果提升，尤其是对主动寻求帮助的学习者，且不显著增加学习时间，具备低成本高可扩展性优势。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）越来越多地被用来生成学习反馈信息，但关于其实际对学习成效的影响仍缺乏系统研究，尤其与现有的反馈方式相比。该研究旨在填补这一空白。

Method: 对七个基于情景的导师培训课程中的2,600多次课程完成情况（共885名学习者）进行分析。依据反馈获取方式将学习者分为三组：使用GPT-3.5-turbo生成反馈的组、拒绝使用LLM反馈的组及未接触LLM反馈的对照组（所有组都获得非LLM纠正性反馈）。通过倾向评分调整选择偏差，比较各组课后测试表现。

Result: 倾向于使用LLM反馈的学习者，课后测试成绩显著高于不太使用LLM反馈的学习者。在校正偏差后，七门课程中有两门显示LLM反馈对学习成效存在统计显著提升（效应量分别为0.28和0.33），但无显著增加任务完成时间，且学习者普遍认为LLM反馈有帮助。

Conclusion: LLM生成的反馈，特别是解释性反馈，对有寻求支持倾向的学习者能够带来中等强度的学习成效提升，对提升开放任务学习效果有潜力，同时效率高、成本低，适宜集成至现有反馈系统。论文还贡献了开放数据集、LLM提示词与评分量表，促进可复现性。

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [97] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Main category: cs.CL

TL;DR: 该文提出一种小模型、高质量数据驱动的统一语音到文本模型，并在IWSLT 2025多任务赛道上展示了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 近年来指令式语音处理任务愈发受到关注，IWSLT 2025举办相关竞赛，以推动集成化、多任务语音到文本系统技术进步。本文旨在开发一种高效、可复用的统一模型适应实际资源受限条件。

Method: 作者提出了一个统一的语音到文本模型，主要分两阶段：第一阶段对语音编码器和文本解码器进行模态对齐；第二阶段进行指令微调。所用的模型仅采用参数量小于2B的小型语言模型，并严格限制训练数据为高质量、开放许可的CC-BY数据，同时通过合成数据增强。

Result: 该团队在IWSLT 2025指令式语音处理任务的Short Track（包括语音识别、翻译和口语问答）中提交了模型结果，展现了该方法在多任务下的适应性和高效性。

Conclusion: 以小规模高质量数据与小型预训练模型为基础，通过模态对齐和指令微调，可实现统一并高效的语音到文本多任务处理模型，适合实际应用场景的资源限制条件。

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [98] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出MUCAR基准，专用于评测多模态歧义消解能力。通过多语种与双向歧义数据集，测试了19个多模态模型，均远逊于人类表现，未来需在多模态歧义理解上开展更多研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在图文任务上表现突出，但对本身含糊或歧义的自然语言和视觉内容理解力不足，现有基准测试数据集几乎忽略了多模态歧义的考察，缺乏通过多模态互相消除歧义的任务设计。

Method: 提出了MUCAR基准：1）包含多语种数据集，通过视觉语境唯一解析文本歧义；2）构建双重歧义集，将歧义图片与歧义文本成对组合，仅通过二者交互可得唯一明确解释。利用19个多模态SOTA模型开展系统评测。

Result: 现有19个多模态SOTA模型在该基准下与人类水平存在明显差距，反映出现有方法无法良好处理跨模态歧义消解问题。

Conclusion: MUCAR基准揭示了当前多模态模型处理歧义能力的不足，表明未来需开发更复杂且有效的跨模态歧义理解方法，以推动多模态推理与理解的边界。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [99] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Macháček,Peter Polák*

Main category: cs.CL

TL;DR: 本文介绍了查理大学在IWSLT 2025多语言同时语音翻译任务上的系统，采用Whisper与AlignAtt提升翻译质量，在多语言方向显著提升BLEU成绩，并提出新的延迟评估方法。


<details>
  <summary>Details</summary>
Motivation: 提升IWSLT 2025同期语音翻译任务系统性能，克服现有系统在多语言和延迟方面的局限。

Method: 采用Whisper离线语音模型，通过直接或级联系统进行多语种（共四种语言对）同时翻译，并采用AlignAtt前沿同时翻译策略优化模型。在级联系统中结合EuroLLM实现无界同时翻译，并通过prompt方法注入领域专属术语并对上下文进行处理。同时提出新的语音识别延迟评测方法。

Result: 与组织者基线系统相比，捷克语到英语提升2个BLEU，英语到德语、中文、日语提升13到22个BLEU。还提出了新的时延评估指标。

Conclusion: 所提出系统在多语言同时语音翻译任务上显著超越基线，并能有效引入术语和上下文。还带来了更先进的延迟测评方法。

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [100] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Main category: cs.CL

TL;DR: Tower+模型通过创新训练流程，在不损失通用能力的前提下大幅提升了翻译等专精任务表现，小型模型已超越同类产品，最大模型在多项领域树立新标杆。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在微调实现任务上的出色表现往往以牺牲通用能力为代价，这限制了其在实际需要多能力的场景中的应用。论文旨在突破该困境，寻求提升模型专精任务与通用能力的最佳平衡。

Method: 基于Tower架构，采用了一套创新训练策略，包括持续预训练、监督微调、偏好优化和带可验证奖励的强化学习。在每一阶段精心构建数据集，提升翻译、代码生成、数学题求解和指令跟随等多项能力。模型开发涵盖2B、9B及72B多种规模，并在不同阶段均重视多能力兼顾。

Result: Tower+小型模型在多个任务上优于规模更大的开放或商用模型（如Llama 3 70B、GPT-4o），而最大模型在高资源语种翻译、Multilingual Arena Hard评测以及所新提出的IF-MT基准上均取得最佳成绩。

Conclusion: 该论文提出了一种同时提升大语言模型翻译能力和多语种通用能力的新方法，即无需在特定领域（如翻译）和通用推理、指令跟随等能力间做取舍。所开发的Tower+模型在多项任务上取得了优异成绩，证明了兼顾两类能力的可行性。

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [101] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Main category: cs.CL

TL;DR: 链式思维提示虽然能减少大模型幻觉，但也会干扰幻觉检测能力，提示应用需权衡二者影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在生成内容时常见“幻觉”现象，即生成事实错误或语义无关的内容。尽管链式思维（Chain-of-Thought, CoT）提示能够通过鼓励逐步推理来减少幻觉，但其对幻觉检测的影响尚未被充分研究。为此，作者希望系统性地评估CoT对幻觉检测方法的影响。

Method: 作者首先进行了一项试点实验，证明CoT推理显著影响了LLM的内部状态和token概率分布。随后，针对主流幻觉检测方法，在经过指令调优和推理优化的LLMs上，系统比较不同CoT提示方式的影响。评估角度包括幻觉分数分布变化、检测准确率变化及检测置信度变化。

Result: 研究发现，虽然CoT提示能减少幻觉发生频率，但也会削弱用于检测幻觉的关键信号，从而降低多种检测方法的有效性。

Conclusion: 在利用推理减缓幻觉的同时，可能会影响幻觉检测性能，提示该领域存在需要权衡的新挑战。

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [102] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文提出PILS方法，基于多步生成token概率分布，极大提升了大语言模型提示词逆推准确率，暴露出更严重的模型输出安全隐患。


<details>
  <summary>Details</summary>
Motivation: 近年来，大语言模型（LLM）的广泛应用带来了模型隐私与安全的新挑战，特别是在API接口保护下，系统提示词等隐私内容可能通过输出被推断泄露。如何从LLM输出逆推出隐藏提示词对于评估与增强模型安全性极为关键，但现有方法准确率有限。

Method: 作者提出了一种基于log概率序列的提示逆推方法（PILS），利用LLM多步生成过程中每一步的下一个token概率分布作为线索。核心见解在于LLM输出向量位于低维子空间中，因而能通过线性映射对多步生成的全部概率分布进行无损压缩，从而保留更多用于逆推的信息。该方法还能迁移至不同模型架构间。

Result: 新方法PILS在提示逆推准确率上显著优于以往技术，部分测试集的准确率由17%提升至60%，平均提升2--3.5倍，同时在更难的系统消息逆推任务上也有强表现。此外，实验显示训练时使用较少生成步数，预测时增加步数能额外提升准确率。同时，作者探讨了提示词重复现象对逆推效果的影响，并提出了跨模型家族迁移方法。

Conclusion: LLM的下一个token概率分布比以往认识的更易被逆推出输入提示词，提示词的保密性风险严重，PILS方法为逆推攻击及安全研究提供了更强工具。

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [103] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Main category: cs.CL

TL;DR: 本文系统梳理了大模型长文本上下文KV缓存的内存优化难题，提出并统一评价了KV footprint指标，改进现有方法并创新性提出PruLong，最终在不损伤性能的前提下显著降低了内存消耗，是长上下文推理领域的重要参考。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型处理的上下文越来越长，如图书摘要任务，导致KV缓存的内存需求迅速增加。当前已有许多方法试图区分或优化KV缓存，但这些方法往往适用于特定场景，并存在高峰值内存消耗与性能下降等隐患，同时缺乏统一的公平评估基准。

Method: 作者提出了一个统一的评估指标KV footprint，涵盖存储的KV条目数量及其在内存中的寿命。用这一指标在长上下文理解和生成任务（最长至128K tokens）下，系统评估现有KV缓存淘汰方法，并揭示其高内存峰值。同时，创新性地将无法在预填充期间淘汰KV的post-fill eviction方法进行适配，支持预填充期间淘汰，显著降低KV footprint。此外，提出了PruLong，一种优化recency eviction的方法，通过学习哪些attention head需保留完整KV缓存以进一步节省内存。

Result: 对比分析发现，许多现有KV淘汰方法有高峰值KV footprint，尤其是传统的post-fill eviction在预填充期间内存消耗大。经过改进，预填充淘汰机制有效降低了footprint。PruLong方法则在减少约12% KV footprint的同时保持了强大的长上下文性能，在高难度召回任务中未损失性能。

Conclusion: 引入KV footprint作为统一指标澄清了长上下文推理方法的内存性能优劣，创新的预填充淘汰和PruLong方法有效降低KV footprint，为KV缓存优化和长文本推理的未来研究提供了清晰方向。

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [104] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CL

TL;DR: 本文提出了CLEAR-3K数据集用于测试大模型对因果解释关系的识别能力，系统评测显示当前主流模型常混淆语义相关和因果关系，最优表现也有限，指出因果推理是亟待解决的技术难题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型可以进行复杂的语义推理，但对“因果解释”能力的评估手段不足，现有任务多偏重语义相关性而不是真正的因果推断。因此，研究者希望有新的基准用以区分模型对语义相关和因果关系的判断能力。

Method: 提出了CLEAR-3K数据集，包括3,000个声明-推理型问题，测试模型对于断言与原因是否存在因果解释关系的判别能力。基于该数据集，系统评估了21个参数量不同（0.5B-72B）的主流大语言模型。

Result: 发现主流模型常将语义相似和因果关系混淆，往往依赖词汇和语义重叠，而非实际推断因果解释关系。参数量提升后，模型判断因果关系的倾向性从‘过于怀疑’转为‘过于宽容’，但整体表现（Matthews Correlation Coefficient）在最优模型上也仅为0.55，出现性能瓶颈。

Conclusion: CLEAR-3K为检验和推动语言模型因果推理能力提供了关键的基准工具，说明当下模型对精准因果关系的识别仍具挑战，未来相关能力的提升对实际应用尤为重要。

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [105] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了由四个智能体协同工作的全新AI搜索架构，系统梳理了实现复杂人类级信息检索和决策支持的关键技术，旨在推动AI搜索系统的能力进化。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索系统在处理复杂任务和模拟人类决策信息处理方面存在明显不足，难以满足从简单事实到复杂推理等多样化信息需求。

Method: 提出了AI搜索范式（AI Search Paradigm），采用由四类大模型驱动的智能体（Master、Planner、Executor、Writer）组成的模块化架构。通过智能体协作，动态分工，实现任务规划、工具集成、推理执行、内容生成，并在基础设施层进行一系列算法和效率优化。

Result: 系统性阐述了实现上述范式的方法学，包括任务细分、工具协同、执行策略、检索增强生成及高效推理等关键要素。

Conclusion: 此范式为开发可信赖、可适配和可扩展的新一代AI搜索系统提供了基础蓝图和实践方向，有助于推动搜索系统向更接近人类能力进化。

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [106] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 微调大型语言模型会影响模型安全，即便数据无害。当前安全评估结果受实验细节影响波动大，需改进评估标准以保证可重复性和对比性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）在特定领域或任务上的微调变得普遍，但微调会削弱模型的安全对齐，即使微调数据无害。由于微调普及，用户对这种“攻击”不知情，这对模型部署安全构成严重威胁。此外，恶意行为者可以利用此漏洞绕过安全机制。因此，需要有效且可复现的安全性评估方法。

Method: 作者实证分析了当前用于评估LLM安全性的基准测试，考察其对实验设置微小变化和模型随机性的鲁棒性。通过控制微调过程中的细节变动，观察安全评估结果的波动情况。

Result: 实验发现，在微调设置发生一些看似无关紧要的变化时，安全性评测结果波动较大，表现出显著的方差。这暴露出现有安全性评估难以保持一致性和可比性的问题。

Conclusion: 研究表明，LLM安全性评估受实验细节影响极大，结果不易复现，威胁到后续研究和模型公开安全性声明的可信度。未来相关领域的研究需更加标准化实验流程，明确报告细节，以实现有意义的横向比较。

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>
