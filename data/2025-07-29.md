<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages](https://arxiv.org/abs/2507.19728)
*Lalita Na Nongkhai,Jingyun Wang,Takahiko Mendori*

Main category: cs.PL

TL;DR: 该论文提出利用本体论与Elo评分机制构建的ADVENTURE自适应编程学习支持系统，能够根据学习者水平动态调整练习难度。实验表明系统自适应模式显著优于随机模式，对提升编程学习效果具有实际帮助。


<details>
  <summary>Details</summary>
Motivation: 目前计算机编程学习中，如何根据个人能力水平为学习者推荐合适的编程练习仍具挑战性。现有系统缺乏针对性的自适应练习推荐机制，因此设计更智能的支持系统有助于提升学习效果。

Method: 采用本体论建模方法，构建跨多种编程语言的编程概念本体（CONTINUOUS），并应用在命名为ADVENTURE的自适应学习支持系统中。系统基于Elo评分机制实时评估学习者水平，并调整个性化练习难度。通过可视化、提示和概念推荐实现自适应支持。进行了自适应模式与随机模式的实验对比研究，基于1186份代码提交的6项特征进行分析。

Result: 实验结果显示，在分析的六项特征中有四项在两种模式下存在显著差异，特别是在正确答案提交和通过概念数量两项上，自适应模式相比随机模式显示出显著优势。

Conclusion: 自适应学习支持系统ADVENTURE在个性化推荐、难度匹配、帮助学习者完成编程练习方面具有效果。Elo评分机制有效提升学习支持的精准度。实验数据表明该系统优于随机推荐模式，有助于提高学习成效。

Abstract: This paper introduces an ontology-based approach within an adaptive learning
support system for computer programming. This system (named ADVENTURE) is
designed to deliver personalized programming exercises that are tailored to
individual learners' skill levels. ADVENTURE utilizes an ontology, named
CONTINUOUS, which encompasses common concepts across multiple programming
languages. The system leverages this ontology not only to visualize programming
concepts but also to provide hints during practice programming exercises and
recommend subsequent programming concepts. The adaptive mechanism is driven by
the Elo Rating System, applied in an educational context to dynamically
estimate the most appropriate exercise difficulty for each learner. An
experimental study compared two instructional modes, adaptive and random, based
on six features derived from 1,186 code submissions across all the experimental
groups. The results indicate significant differences in four of six analyzed
features between these two modes. Notably, the adaptive mode demonstrates a
significant difference over the random mode in two features, the submission of
correct answers and the number of pass concepts. Therefore, these results
underscore that this adaptive learning support system may support learners in
practicing programming exercises.

</details>


### [2] [The Power of Negation in Higher-Order Datalog](https://arxiv.org/abs/2507.20251)
*Angelos Charalambidis,Babis Kostopoulos,Christos Nomikos,Panos Rondogiannis*

Main category: cs.PL

TL;DR: 本文系统研究了高阶Datalog$^\neg$在不同语义下的表达能力，与多种复杂度类建立了严格对应关系，并揭示了提升阶数可显著增强程序表达能力，丰富了高阶逻辑编程理论。


<details>
  <summary>Details</summary>
Motivation: 探究高阶Datalog$^\neg$（带否定的Datalog）在不同语义下（well-founded和stable model semantics）的表达能力，以及其与复杂度类之间的关系。

Method: 对高阶Datalog$^\neg$进行理论分析，探讨其在well-founded语义下和stable model语义下与复杂度类（如k-EXP, k-NEXP, co-(k-NEXP)）的关系，根据程序的阶数进行分层，并讨论相关片段，不依赖于输入数据库的显示排序。

Result: 在well-founded语义下，(k+1)-阶Datalog$^\neg$捕捉k-EXP复杂度类。在stable model语义下，(k+1)-阶Datalog$^\neg$分别对应co-(k-NEXP)（cautious reasoning）和k-NEXP（brave reasoning）。这些表达能力在带有分层结构的片段中依然成立，并可通过丰富语言特性实现。论文还展示了随着阶数提升，表达能力显著增强，甚至优于低阶stable model语义。

Conclusion: 高阶Datalog$^\neg$的表达能力随着阶数与语义变化而增强，存在明确的表达能力层级，与复杂度理论紧密相关；高阶well-founded语义的程序具有超越低阶stable model语义程序的表达力。

Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both
the well-founded and the stable model semantics, establishing tight connections
with complexity classes. We prove that under the well-founded semantics, for
all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds
without explicit ordering of the input database. The proof of this fact can be
performed either by using the powerful existential predicate variables of the
language or by using partially applied relations and relation enumeration.
Furthermore, we demonstrate that this expressive power is retained within a
stratified fragment of the language. Under the stable model semantics, we show
that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning
and k-NEXP using brave reasoning, again with analogous results for the
stratified fragment augmented with choice rules. Our results establish a
hierarchy of expressive power, highlighting an interesting trade-off between
order and non-determinism in the context of higher-order logic programming:
increasing the order of programs under the well-founded semantics can surpass
the expressive power of lower-order programs under the stable model semantics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code](https://arxiv.org/abs/2507.19549)
*Nadeen Fathallah,Daniel Hernández,Steffen Staab*

Main category: cs.SE

TL;DR: 提出AccessGuru工具，结合大语言模型，实现对网页可访问性三类问题的自动检测和修复，平均违规得分降幅高达84%，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数网页未能遵守Web可访问性标准，导致不同能力的用户被排除在外。实现网页全面可访问性往往需要开发者投入额外精力和专门知识。该研究希望通过自动化检测和修正Web可访问性问题，降低开发者负担并促进包容性。

Method: 作者提出了可访问性违规的新分类法（语法类、语义类、布局类），据此开发了混合检测与修正方法——AccessGuru。AccessGuru结合了现有自动化测试工具与大型语言模型（LLMs），通过分类驱动的提示策略自动检测和修正三类可访问性违规。作者还构建了一个现实世界违规基准，量化评估方法效果。

Result: AccessGuru在实验基准上实现了平均84%的可访问性违规得分降低，显著优于当前最佳方法（最多50%）。该工具既能发现也能修复语法、语义与布局三类问题。

Conclusion: AccessGuru创新地将可访问性检测与自动修正结合，通过分类法与大模型技术大幅提升了网页可访问性问题自动修正的效果，对实际Web开发产生积极推动作用。

Abstract: The vast majority of Web pages fail to comply with established Web
accessibility guidelines, excluding a range of users with diverse abilities
from interacting with their content. Making Web pages accessible to all users
requires dedicated expertise and additional manual efforts from Web page
providers. To lower their efforts and promote inclusiveness, we aim to
automatically detect and correct Web accessibility violations in HTML code.
While previous work has made progress in detecting certain types of
accessibility violations, the problem of automatically detecting and correcting
accessibility violations remains an open challenge that we address. We
introduce a novel taxonomy classifying Web accessibility violations into three
key categories - Syntactic, Semantic, and Layout. This taxonomy provides a
structured foundation for developing our detection and correction method and
redefining evaluation metrics. We propose a novel method, AccessGuru, which
combines existing accessibility testing tools and Large Language Models (LLMs)
to detect violations and applies taxonomy-driven prompting strategies to
correct all three categories. To evaluate these capabilities, we develop a
benchmark of real-world Web accessibility violations. Our benchmark quantifies
syntactic and layout compliance and judges semantic accuracy through
comparative analysis with human expert corrections. Evaluation against our
benchmark shows that AccessGuru achieves up to 84% average violation score
decrease, significantly outperforming prior methods that achieve at most 50%.

</details>


### [4] [LastMerge: A language-agnostic structured tool for code integration](https://arxiv.org/abs/2507.19687)
*Joao Pedro Duarte,Paulo Borba,Guilherme Cavalcanti*

Main category: cs.SE

TL;DR: 提出了一种新型、通用的结构化合并工具LastMerge，在准确率和性能上与专用工具相当，显著降低了支持多语言的开发门槛，有望推动结构化合并在行业内广泛应用。


<details>
  <summary>Details</summary>
Motivation: 现有的基于抽象语法树(AST)的结构化合并工具拥有更高的合并准确率，但由于其针对特定语言、开发和维护成本高，导致在实际中难以推广，许多语言并没有对应的结构化合并工具。本研究希望提供一种能跨语言、适应性强的通用结构化合并工具。

Method: 提出了一种名为LastMerge的通用结构化合并工具，提供简洁的接口以降低对新语言的适配难度；通过与两种Java专用工具（jDime和Spork）及其通用对应工具（LastMerge和Mergiraf）对比，在实际的合并场景下评估工具的合并准确率、性能与差异表现。

Result: 实验显示，通用结构化合并工具在合并准确率上与语言专用工具差异不大，约有10%的差异主要来源于实现细节，并非通用性本身造成。LastMerge比jDime少了15%的误报，Mergiraf比Spork少了42%的漏报，两款通用工具在运行时间上与专用工具相当。

Conclusion: 通用结构化合并工具能无显著损失地取代语言特定工具，为结构化合并技术在各类编程语言和实际工业场景的普及铺平了道路。

Abstract: Unstructured line-based merge tools are widely used in practice. Structured
AST-based merge tools show significantly improved merge accuracy, but are
rarely used in practice because they are language specific and costly,
consequently not being available for many programming languages. To improve
merge accuracy for a wide range of languages, we propose LastMerge, a generic
structured merge tool that can be configured through a thin interface that
significantly reduces the effort of supporting structured merge. To understand
the impact that generic structured merge might have on merge accuracy and
performance, we run an experiment with four structured merge tools: two Java
specific tools, jDime and Spork, and their generic counterparts, respectively
LastMerge and Mergiraf. Using each tool, we replay merge scenarios from a
significant dataset, and collect data on runtime, behavioral divergences, and
merge accuracy. Our results show no evidence that generic structured merge
significantly impacts merge accuracy. Although we observe a difference rate of
approximately 10% between the Java specific tools and their generic
counterparts, most of the differences stem from implementation details and
could be avoided. We find that LastMerge reports 15% fewer false positives than
jDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic
tools exhibit comparable runtime performance to the state of the art language
specific implementations. These results suggest that generic structured merge
tools can effectively replace language-specific ones, paving the way for
broader adoption of structured merge in industry.

</details>


### [5] [Refactoring $\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis](https://arxiv.org/abs/2507.19714)
*Feifei Niu,Junqian Shao,Christoph Mayr-Dorn,Liguo Huang,Wesley K. G. Assunção,Chuanyi Li,Jidong Ge,Alexander Egyed*

Main category: cs.SE

TL;DR: 本文证明忽略代码重构会降低JIT缺陷预测方法的性能，提出CAT分析能提升数据准确性和模型指标，强调整合重构信息是提升预测效果的重要途径。


<details>
  <summary>Details</summary>
Motivation: 现有JIT-DP研究在提升预测准确性的同时，大多忽略了代码重构行为，尤其是在学习与评估阶段。由于重构经常与修复bug以及引入bug的变更纠缠在一起，忽视重构会导致模型训练与评估出现偏差。

Method: 作者提出了Code chAnge Tactics (CAT) 分析方法，对代码重构及其传播进行分类，并提升数据集标注的准确性。然后分析六种主流JIT-DP方法在未考虑和整合重构信息前后的性能变化，并通过整合重构信息对六种基线方法进行了改进。

Result: 实验发现数据集中忽略重构信息，对模型性能有显著负面影响，语义模型的F1-score能降低18.6%到37.3%。整合重构信息后，基线方法的召回率和F1-score分别最高提升43.2%和32.5%。CAT分析能广泛应用于重构及其传播的研究。

Conclusion: 研究证明在JIT-DP领域，重构信息在方法和数据集标注中都十分重要，忽略重构会显著损害模型效果，建议后续研究充分考虑重构影响。

Abstract: Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of
code changes resulting in software defects at an early stage. Although code
change metrics and semantic features have enhanced prediction accuracy, prior
research has largely ignored code refactoring during both the evaluation and
methodology phases, despite its prevalence. Refactoring and its propagation
often tangle with bug-fixing and bug-inducing changes within the same commit
and statement. Neglecting refactoring can introduce bias into the learning and
evaluation of JIT-DP models. To address this gap, we investigate the impact of
refactoring and its propagation on six state-of-the-art JIT-DP approaches. We
propose Code chAnge Tactics (CAT) analysis to categorize code refactoring and
its propagation, which improves labeling accuracy in the JIT-Defects4J dataset
by 13.7%. Our experiments reveal that failing to consider refactoring
information in the dataset can diminish the performance of models, particularly
semantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose
integrating refactoring information to enhance six baseline approaches,
resulting in overall improvements in recall and F1-score, with increases of up
to 43.2% and 32.5%, respectively. Our research underscores the importance of
incorporating refactoring information in the methodology and evaluation of
JIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring
and its propagation for software maintenance.

</details>


### [6] [Clean Code In Practice: Challenges and Opportunities](https://arxiv.org/abs/2507.19721)
*Dapeng Yan,Wenjie Yang,Kui Liu,Zhiming Liu,Zhikuang Cai*

Main category: cs.SE

TL;DR: 本文分析了可靠性、安全性与安全性在软件系统中的关系，提出结合评估模型和实用指南，可提升现代软件系统的安全和健壮性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统复杂度不断提升，单一从可靠性角度评估已无法全面应对实际中的安全与安全性威胁，因此有必要探究三者的关系并提升综合预测和防御能力。

Method: 对工业中用于可靠性预测的主要度量和测量技术进行了全面分析，并提出了一个结合安全性和安全性要素的威胁评估框架和可操作性指南。

Result: 提出了影响软件可靠性的关键威胁和综合威胁评估框架，结果显示，与安全和安全性结合的可靠性指标能提升系统鲁棒性，并为业界提供了改进模型的具体建议。

Conclusion: 集成可靠性、安全性和安全性指标可以显著提升软件系统的健壮性；结合这些因素能更好地应对实际应用中的威胁。

Abstract: Reliability prediction is crucial for ensuring the safety and security of
software systems, especially in the context of industry practices. While
various metrics and measurements are employed to assess software reliability,
the complexity of modern systems necessitates a deeper understanding of how
these metrics interact with security and safety concerns. This paper explores
the interplay between software reliability, safety, and security, offering a
comprehensive analysis of key metrics and measurement techniques used in the
industry for reliability prediction. We identify critical threats to software
reliability and provide a threat estimation framework that incorporates both
safety and security aspects. Our findings suggest that integrating reliability
metrics with safety and security considerations can enhance the robustness of
software systems. Furthermore, we propose a set of actionable guidelines for
practitioners to improve their reliability prediction models while
simultaneously addressing the security and safety challenges of contemporary
software applications.

</details>


### [7] [Defining ethically sourced code generation](https://arxiv.org/abs/2507.19743)
*Zhuolin Xu,Chenglin Li,Qiushi Li,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 本文系统梳理了代码生成模型的伦理问题，提出了“伦理来源代码生成”的概念和11个关键维度，结合文献与实践者调研呼吁业界更多关注社会伦理。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型的普及，对其伦理问题（如数据来源、隐私、许可、公平和环境影响）的关注持续上升。本研究旨在系统梳理这些问题，提出伦理代码生成的完整框架，并从理论和实践两个层面推动社区重视此议题。

Method: 采用两阶段文献回顾（阅读803篇论文，筛选出71篇相关文献建立初始10个维度），随后通过对32位实践者（包含真实受影响开发者）的调研，进一步完善与补充ES-CodeGen的维度。

Result: 最终建立了包含11个ES-CodeGen维度的分类法（新增了代码质量维度），识别了相关后果、产物和涉及阶段。调研显示，虽然ES-CodeGen有助于提升实践者对伦理问题的认识，但社会相关维度依然被忽视。

Conclusion: 本研究提出并详细阐述了“伦理来源代码生成”(ES-CodeGen)的概念，发现了其关键维度和涉及的具体因素，并强调了业界实践者普遍忽视社会相关伦理维度的现象。呼吁更多关注相关伦理问题。

Abstract: Several code generation models have been proposed to help reduce time and
effort in solving software-related tasks. To ensure responsible AI, there are
growing interests over various ethical issues (e.g., unclear licensing,
privacy, fairness, and environment impact). These studies have the overarching
goal of ensuring ethically sourced generation, which has gained growing
attentions in speech synthesis and image generation. In this paper, we
introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to
refer to managing all processes involved in code generation model development
from data collection to post-deployment via ethical and sustainable practices.
To build a taxonomy of ES-CodeGen, we perform a two-phase literature review
where we read 803 papers across various domains and specific to AI-based code
generation. We identified 71 relevant papers with 10 initial dimensions of
ES-CodeGen. To refine our dimensions and gain insights on consequences of
ES-CodeGen, we surveyed 32 practitioners, which include six developers who
submitted GitHub issues to opt-out from the Stack dataset (these impacted users
have real-world experience of ethically sourcing issues in code generation
models). The results lead to 11 dimensions of ES-CodeGen with a new dimension
on code quality as practitioners have noted its importance. We also identified
consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey
reflection showed that most practitioners tend to ignore social-related
dimensions despite their importance. Most practitioners either agreed or
strongly agreed that our survey help improve their understanding of ES-CodeGen.
Our study calls for attentions of various ethical issues towards ES-CodeGen.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Graded Quantitative Narrowing](https://arxiv.org/abs/2507.19630)
*Mauricio Ayala-Rincón,Thaynara Arielly de Lima,Georg Ehling,Temur Kutsia*

Main category: cs.LO

TL;DR: 该论文提出了一种结合量化度量的新型重写与统一理论（分级量化重写与定量缩窄），在理论上扩展了等式推理和统一的能力，并证明了其正确性及在部分情形下的完备性。


<details>
  <summary>Details</summary>
Motivation: 传统重写系统缺乏对项之间距离或复杂度等量化度量的表达能力，因此亟需引入新的理论框架来支持这类度量，为等式推理与重写提供更灵活和丰富的基础。

Method: 提出了分级量化重写（Graded Quantitative Rewriting）理论，并提出定量缩窄（quantitative narrowing）方法，将量化重写中的匹配过程扩展为统一过程，实现同时实例化与重写；并在Lawvere量子化理论下，研究了其在定量等式理论下的统一问题求解能力，给出其正确性证明并讨论了完备性条件。

Result: 证明了定量缩窄在定量等式理论下的正确性，且在特定条件下可保证完备性，并展示该方法能够处理早前方法难以覆盖的更丰富理论中的定量方程。

Conclusion: 分级量化重写及定量缩窄为结合度量特性的等式推理提供了理论基础和工具，拓展了等式推理与统一在更广泛理论中的适用范围。

Abstract: The recently introduced framework of Graded Quantitative Rewriting is an
innovative extension of traditional rewriting systems, in which rules are
annotated with degrees drawn from a quantale. This framework provides a robust
foundation for equational reasoning that incorporates metric aspects, such as
the proximity between terms and the complexity of rewriting-based computations.
Quantitative narrowing, introduced in this paper, generalizes quantitative
rewriting by replacing matching with unification in reduction steps, enabling
the reduction of terms even when they contain variables, through simultaneous
instantiation and rewriting. In the standard (non-quantitative) setting,
narrowing has been successfully applied in various domains, including
functional logic programming, theorem proving, and equational unification.
Here, we focus on quantitative narrowing to solve unification problems in
quantitative equational theories over Lawverean quantales. We establish its
soundness and discuss conditions under which completeness can be ensured. This
approach allows us to solve quantitative equations in richer theories than
those addressed by previous methods.

</details>


### [9] [Scroll nets](https://arxiv.org/abs/2507.19689)
*Pablo Donato*

Main category: cs.LO

TL;DR: 本文提出了一种源于Peirce的图形逻辑的新工具scroll nets，能高效直观地表示命题逻辑证明，并支持类似cut-elimination的简化操作，理论和计算应用上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统的命题逻辑证明系统在表达、归纳等方面受到某些限制。作者希望借助Peirce的图形逻辑设想和现代类型理论技术，建立更加直观且强表达力的证明表示方法，提升对推理流程的理解和处理能力。

Method: 采用C. S. Peirce的'卷轴'符号为基础，从本体直观出发抽象成图论结构，结合Curry-Howard对应，将推理规则内化于判断中，并通过对冗余路径（detour）分析，设计了类似cut-elimination的消除过程。最后通过对λ演算的归一化模拟，说明了scroll nets的表达能力。

Result: 成功提出scroll nets，并给出其数学定义，分析了detour-elimination过程，模拟了λ-演算归一化，证明了新系统在逻辑推理和计算中的可行性和高度表达力。

Conclusion: 作者提出了一种新的命题逻辑证明表达形式——scroll nets，并验证其在逻辑和计算表达能力上的有效性。

Abstract: We introduce a new formalism for representing proofs in propositional logic
called "scroll nets". Its fundamental construct is the "scroll", a topological
notation for implication proposed by C. S. Peirce at the end of the 19th
century as the basis for his diagrammatic system of existential graphs (EGs).
Scroll nets are derived from EGs by following the Curry-Howard methodology of
internalizing inference rules inside judgments, just as terms in type theory
internalize natural deduction rules. We focus on the intuitionistic implicative
fragment of EGs, starting from a natural diagrammatic representation of scroll
nets, and then distilling their combinatorial essence into a purely
graph-theoretic definition. We also identify a notion of detour, that we use to
sketch a detour-elimination procedure akin to cut-elimination. We illustrate
how to simulate normalization in the simply typed $\lambda$-calculus,
demonstrating both the logical and computational expressivity of our framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: 本研究系统评估Transformer与LSTM方法在心理健康文本分类的表现，证实Transformer模型（尤其是RoBERTa）具有更高准确率。LSTM+BERT嵌入也极具竞争力，适用于计算资源有限场景，对数字心理健康监测具有现实应用意义。


<details>
  <summary>Details</summary>
Motivation: 随着心理健康障碍发病率的上升，亟需开发强大的自动化工具，实现早期检测与监测。近年来NLP，特别是Transformer架构在文本分析方面显示出巨大潜力，这为心理健康自动识别提供了新方向。

Method: 对比主流Transformer模型（BERT、RoBERTa、DistilBERT、ALBERT、ELECTRA）和基于LSTM的不同文本嵌入技术，就Reddit上的心理健康障碍分类任务展开全面评估。构建了大规模标注数据集，并通过统计与主题建模进行了可靠性验证。

Result: 实验结果显示，Transformer模型整体优于传统深度学习方法。其中RoBERTa取得了99.54%的F1分数（保留测试集）和96.05%（外部测试集）。LSTM结合BERT嵌入也表现突出，F1分数超过94%，且计算资源消耗更低。

Conclusion: Transformer模型在心理健康文本自动识别方面极具优势，有望实现实时、可扩展的心理健康监测。LSTM+BERT嵌入为资源受限场景提供可行方案。论文还探讨了此类模型在临床和数字健康干预的实际应用前景。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [11] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 本文提出了基于合成意图的新数据集和LLM编辑优化手段，系统性提升了文献对比schema自动生成和质量优化，为规模化学术文献整理带来新方法。


<details>
  <summary>Details</summary>
Motivation: 学术文献数量激增，研究者需要对文献集合进行有组织地比较和对照。以往用大语言模型（LLM）生成文献对比维度（schema）存在评估歧义大和缺乏编辑优化手段两大难题。

Method: 1. 设计方法用合成意图增强无标注表格语料，生成带有明确信息需求的数据集，用以研究在既定信息需求下的schema生成问题，减少评估歧义。2. 用这个数据集分析加入表格意图对基线模型重构对照schema的提升效果。3. 系统性比较多种单步schema生成方法（提示式LLM、微调模型等）；4. 提出基于LLM的schema编辑技术，提升生成schema的质量。

Result: （1）合成意图数据集能显著提升基线模型重构参考schema的能力；（2）开源小模型微调后在schema生成任务上可接近甚至媲美最先进的提示式LLM；（3）所提出的LLM编辑技术可进一步优化原本生成的schema。

Conclusion: 本文首次系统性解决了schema生成中参照歧义与缺乏编辑技术两大难题：提出新数据集减少歧义，并用LLM方法提升schema生成及后续编辑优化能力。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [12] [Product-Congruence Games: A Unified Impartial-Game Framework for RSA ($φ$-MuM) and AES (poly-MuM)](https://arxiv.org/abs/2507.20087)
*Satyam Tyagi*

Main category: cs.DM

TL;DR: 本文通过Product-Congruence Game统一描述了RSA的指数约简与AES S盒求逆操作，提出统一的理论模型，给出了结构性定理，并解释了相关变体的结构性质。


<details>
  <summary>Details</summary>
Motivation: RSA指数约简和AES S盒求逆虽属于不同密码体系，但其核心操作有着共同的组合原理。作者希望揭示和分析这一隐藏的共性，并建立更统一的理论框架。

Method: 提出了Product-Congruence Game（PCG，积同余游戏）作为统一模型，并通过具体实例（如 $phi$-MuM 和 poly-MuM 游戏）对RSA和AES核心计算进行建模；进一步证明了一般PCG的结构性定理。

Result: 将RSA指数运算和AES S盒求逆统一视为同一种组合游戏，并给出具体对应的游戏模型。提出了四个结构性定理，并揭示了为何部分变体会简化为整体聚合。

Conclusion: RSA和AES的某些核心代数过程可通过同一类型的积同余组合博弈加以统一解释。该方法不仅统一了对两种主流密码构造的理解，也带来了全新的结构与塌缩定理。

Abstract: RSA exponent reduction and AES S-box inversion share a hidden commonality:
both are governed by the same impartial combinatorial principle, which we call
a Product-Congruence Game (PCG). A Product-Congruence Game tracks play via the
modular or finite-field product of heap values, providing a single invariant
that unifies the algebraic cores of these two ubiquitous symmetric and
asymmetric cryptosystems. We instantiate this framework with two companion
games. First, $\phi$-MuM, in which a left-associated "multi-secret" RSA
exponent chain compresses into the game of Multiplicative Modular Nim,
PCG($k,\{1\}$), where $k = ord_N(g)$. The losing predicate then factorizes via
the Chinese remainder theorem, mirroring RSA's structure. Second, poly-MuM, our
model for finite-field inversion such as the AES S-box. For poly-MuM we prove
the single-hole property inside its threshold region, implying that the
Sprague-Grundy values are multiplicative under disjunctive sums in that region.
Beyond these instances, we establish four structural theorems for a general
Product-Congruence Game PCG($m,R$): (i) single-heap repair above the modulus,
(ii) ultimate period $m$ per coordinate, (iii) exact and asymptotic losing
densities, and (iv) confinement of optimal play to a finite indeterminacy
region. An operation-alignment collapse principle explains why some variants
degenerate to a single aggregate while MuM, $\phi$-MuM and poly-MuM retain rich
local structure. All ingredients (multiplicative orders, the Chinese remainder
theorem, finite fields) are classical; the contribution is the unified
aggregation-compression viewpoint that embeds both RSA and AES inside one
impartial-game framework, together with the structural and collapse theorems.

</details>


### [13] [Ternary Binomial and Trinomial Bent Functions in the Completed Maiorana-McFarland Class](https://arxiv.org/abs/2507.20715)
*Tor Helleseth,Alexander Kholosha,Niki Spithaki*

Main category: cs.DM

TL;DR: 作者提出两类新的三元四次Bent函数，给出显式构造与判定准则，丰富了Bent函数理论。


<details>
  <summary>Details</summary>
Motivation: 研究三元（域上）Bent函数，特别是寻找具有高代数次数且表示形式简单的新Bent函数，对于密码学、安全通信等领域具有重要意义。本文旨在拓展这类Bent函数的已知类，丰富理论基础。

Method: 通过分析在完备Maiorana-McFarland类中的单变量四次Bent函数，作者使用迹函数表达新的二项式和三项式Bent函数，同时提出了一个新判定准则，通过分析选定$n/2$维子空间方向上的一、二阶导数特性来验证Bent性。

Result: 发现了两个类的新型三元四次Bent函数：一类为$	ext{GF}(3^{4k})$上的Binomial型，另一类为$	ext{GF}(3^{2k})$上的Trinomial型，两类函数的系数均有明确构造。部分Binomial例子可由例外多项式表示，具体函数与参数给出。

Conclusion: 本文扩展了已知三元Bent函数家族，提出了一种新的Bent性判定方法，为后续三元域上Bent函数及其应用研究提供了理论基础和构造工具。

Abstract: Two classes of ternary bent functions of degree four with two and three terms
in the univariate representation that belong to the completed
Maiorana-McFarland class are found. Binomials are mappings
$\F_{3^{4k}}\mapsto\fthree$ given by $f(x)=\Tr_{4k}\big(a_1 x^{2(3^k+1)}+a_2
x^{(3^k+1)^2}\big)$, where $a_1$ is a nonsquare in $\F_{3^{4k}}$ and $a_2$ is
defined explicitly by $a_1$. Particular subclasses of the binomial bent
functions we found can be represented by exceptional polynomials over
$\fthreek$. Bent trinomials are mappings $\F_{3^{2k}}\mapsto\fthree$ given by
$f(x)=\Tr_n\big(a_1 x^{2\cdot3^k+4} + a_2 x^{3^k+5} + a_3 x^2\big)$ with
coefficients explicitly defined by the parity of $k$. The proof is based on a
new criterion that allows checking bentness by analyzing first- and
second-order derivatives of $f$ in the direction of a chosen $n/2$-dimensional
subspace.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [14] [Self-Verifying Predicates in Büchi Arithmetic](https://arxiv.org/abs/2507.19717)
*Mazen Khodier,Luke Schaeffer,Jeffrey Shallit*

Main category: cs.FL

TL;DR: 基于Angluin算法自动生成用于Büchi算术一阶逻辑的自动机，理论及实验显示方法大幅提升效率并减少空间，占优于直接构建方案。


<details>
  <summary>Details</summary>
Motivation: 现有为一阶逻辑公式构建有限自动机的方法效率较低且空间占用大，需寻找更高效省空间的替代方案。

Method: 采用基于Angluin算法的自动机生成技术，对比以往直接生成自动机的方法，在理论上和实践中都进行了分析与实验。

Result: 提出的方法比传统直接构建自动机的方式在速度和空间利用上都有显著优势，并用Walnut软件进行了实验验证。

Conclusion: 基于Angluin算法的方法能更高效地自动生成用于Büchi算术一阶逻辑公式的有限自动机，且空间占用更低。

Abstract: We discuss a technique, based on Angluin's algorithm, for automatically
generating finite automata for various kinds of useful first-order logic
formulas in B\"uchi arithmetic. Construction in this way can be faster and use
much less space than more direct methods. We discuss the theory and we present
some empirical data for the free software Walnut.

</details>
