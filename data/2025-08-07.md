<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 67]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [If-T: A Benchmark for Type Narrowing](https://arxiv.org/abs/2508.03830)
*Hanwen Guo,Ben Greenman*

Main category: cs.PL

TL;DR: 文章提出If-T这一新基准，用于系统性评估和比较不同语言类型缩窄机制的优缺点，已应用于多种主流类型检查器，揭示了实现上的核心差异，为类型系统设计提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 动态语言的静态类型系统设计面临挑战，因代码常通过运行时测试进行类型缩窄，需有精细的类型缩窄机制。目前类型缩窄设计缺乏标准评估，且现有形式化方法复杂，实用性不明。该研究希望为类型缩窄设计提供系统化评测基准。

Method: 提出If-T，一种语言无关的类型缩窄设计基准，通过涵盖关键技术维度和具体用例，判断类型缩窄系统对正确与错误代码的识别能力。并在TypeScript、Flow、Typed Racket、mypy、Pyright五个类型检查器上实施基准测试。

Result: If-T基准揭示了不同类型系统在处理类型缩窄方面的重要差异，包括变量间逻辑关系追踪和用户自定义缩窄谓词的能力。研究为设计人员提供了评估类型系统复杂度收益的具体例证。

Conclusion: If-T基准为不同复杂度的类型缩窄系统提供了公平、跨语言的评估工具，帮助未来类型系统在精度、注解负担与性能间更好权衡，推动渐进式类型系统的发展。

Abstract: **Context:** The design of static type systems that can validate
dynamically-typed programs (**gradually**) is an ongoing challenge. A key
difficulty is that dynamic code rarely follows datatype-driven design. Programs
instead use runtime tests to narrow down the proper usage of incoming data.
Type systems for dynamic languages thus need a **type narrowing** mechanism
that refines the type environment along individual control paths based on
dominating tests, a form of flow-sensitive typing. In order to express
refinements, the type system must have some notion of sets and subsets. Since
set-theoretic types are computationally and ergonomically complex, the need for
type narrowing raises design questions about how to balance precision and
performance. **Inquiry:** To date, the design of type narrowing systems has
been driven by intuition, past experience, and examples from users in various
language communities. There is no standard that captures desirable and
undesirable behaviors. Prior formalizations of narrowing are also significantly
more complex than a standard type system, and it is unclear how the extra
complexity pays off in terms of concrete examples. This paper addresses the
problems through If-T, a language-agnostic **design benchmark** for type
narrowing that characterizes the abilities of implementations using simple
programs that draw attention to fundamental questions. Unlike a traditional
performance-focused benchmark, If-T measures a narrowing system's ability to
validate correct code and reject incorrect code. Unlike a test suite, systems
are not required to fully conform to If-T. Deviations are acceptable provided
they are justified by well-reasoned design considerations, such as compile-time
performance. **Approach:** If-T is guided by the literature on type narrowing,
the documentation of gradual languages such as TypeScript, and experiments with
typechecker implementations. We have identified a set of core technical
dimensions for type narrowing. For each dimension, the benchmark contains a set
of topics and (at least) two characterizing programs per topic: one that should
typecheck and one that should not typecheck. **Knowledge:** If-T provides a
baseline to measure type narrowing systems. For researchers, it provides
criteria to categorize future designs via its collection of positive and
negative examples. For language designers, the benchmark demonstrates the
payoff of typechecker complexity in terms of concrete examples. Designers can
use the examples to decide whether supporting a particular example is
worthwhile. Both the benchmark and its implementations are freely available
online. **Grounding:** We have implemented the benchmark for five typecheckers:
TypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight
important differences, such as the ability to track logical implications among
program variables and typechecking for user-defined narrowing predicates.
**Importance:** Type narrowing is essential for gradual type systems, but the
tradeoffs between systems with different complexity have been unclear. If-T
clarifies these tradeoffs by illustrating the benefits and limitations of each
level of complexity. With If-T as a way to assess implementations in a fair,
cross-language manner, future type system designs can strive for a better
balance among precision, annotation burden, and performance.

</details>


### [2] [A Type System for Data Privacy Compliance in Active Object Languages](https://arxiv.org/abs/2508.03831)
*Chinmayi Prabhu Baramashetru,Paola Giannini,Silvia Lizeth Tapia Tarifa,Olaf Owe*

Main category: cs.PL

TL;DR: 本文提出一种结合静态与运行时技术的编程语言类型系统，自动追踪和检查用户数据流，确保GDPR合规。通过实例和理论证明展示其在用户同意、用途限制等方面的有效性，有助于自动化开发隐私合规系统。


<details>
  <summary>Details</summary>
Motivation: 尽管GDPR等数据保护法规鼓励开发者在系统架构中默认融入隐私保护（即隐私设计原则），但现有从抽象原则到具体、可执行方法的转化仍面临巨大挑战。本文旨在弥补这一空白，寻求系统性、自动化的合规方法。

Method: 本文提出了一种基于编程语言的隐私集成方法，结合静态和运行时技术，通过在主动对象语言中采用类型检查和类型推导，实现了对授权数据流的跟踪，并基于用户同意自动生成运行时约束。该方法利用类型系统将合规检查和用户同意的动态变化整合进系统的实际运行中。

Result: 提出了一套类型系统，将隐私合规性检查融入系统执行流程，实现了对GDPR具体要求（如用户同意、用途限制和数据主体权利）的支持。通过形式化证明和多个实例，展示了方法的可行性和适用性。

Conclusion: 本方法为隐私友好型系统设计提供了一种系统化、自动化的GDPR合规集成手段，推动了信任系统构建，尤其适用于对数据隐私要求高的领域（如医疗、金融）。

Abstract: Data protection laws such as GDPR aim to give users unprecedented control
over their personal data. Compliance with these regulations requires
systematically considering information flow and interactions among entities
handling sensitive data. Privacy-by-design principles advocate embedding data
protection into system architectures as a default. However, translating these
abstract principles into concrete, explicit methods remains a significant
challenge. This paper addresses this gap by proposing a language-based approach
to privacy integration, combining static and runtime techniques. By employing
type checking and type inference in an active object language, the framework
enables the tracking of authorised data flows and the automatic generation of
constraints checked at runtime based on user consent. This ensures that
personal data is processed in compliance with GDPR constraints. The key
contribution of this work is a type system that gather the compliance checks
and the changes to users consent and integrates data privacy compliance
verification into system execution. The paper demonstrates the feasibility of
this approach through a soundness proof and several examples, illustrating how
the proposed language addresses common GDPR requirements, such as user consent,
purpose limitation, and data subject rights. This work advances the state of
the art in privacy-aware system design by offering a systematic and automated
method for integrating GDPR compliance into programming languages. This
capability has implications for building trustworthy systems in domains such as
healthcare or finance, where data privacy is crucial.

</details>


### [3] [Generating Inputs for Grammar Mining using Dynamic Symbolic Execution](https://arxiv.org/abs/2508.03832)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.PL

TL;DR: 提出了一种结合动态符号执行和多阶段输入生成的新文法挖掘方法，能高效自动发现软件解析器的完整输入规范，在覆盖边缘特性和提升自动化程度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 很多软件需要解析和处理结构化输入，但由于长期演化和修改，输入组件实际接受的输入往往难以界定。已有的文法挖掘方法依赖于全面的输入样本，而实际录制的数据往往覆盖有限，无法得到完整的输入文法，导致边缘情况和旧特性被忽略。解决如何自动补齐输入数据、提升文法挖掘覆盖率是本研究动机。

Method: 提出了一种完全自动化的输入生成方法，用于文法挖掘。该方法基于Dynamic Symbolic Execution (DSE)，并引入两大机制：一是通过迭代式扩展从单字符开始逐步生成更复杂的输入；二是采用三阶段创新结构将解析器函数的输入生成步骤分离。方法在Mimid（现有文法挖掘工具）之上进行了扩展。

Result: 在11个基准应用上评估，结果显示该方法提取的文法在精度和召回率方面接近甚至优于当前最先进的Mimid。尤其是在识别解析器中的细微特性和边缘情况（常规方法经常遗漏）方面表现突出。实验还表明方法无需预先输入样本，具备自动化、高效和广泛适用性。

Conclusion: 该方法为软件工程领域的研究者和实践者提供了一套自动、可扩展且精确的文法挖掘解决方案。它有效减轻了人工生成输入样本的负担，同时提升了提取结果的健壮性和完整性，为现有或遗留系统的输入规范重建提供了重要工具。

Abstract: A vast number of software systems include components that parse and process
structured input. In addition to programming languages, which are analyzed by
compilers or interpreters, there are numerous components that process
standardized or proprietary data formats of varying complexity. Even if such
components were initially developed and tested based on a specification, such
as a grammar, numerous modifications and adaptations over the course of
software evolution can make it impossible to precisely determine which inputs
they actually accept. In this situation, grammar mining can be used to
reconstruct the specification in the form of a grammar. Established approaches
already produce useful results, provided that sufficient input data is
available to fully cover the input language. However, achieving this
completeness is a major challenge. In practice, only input data recorded during
the operation of the software systems is available. If this data is used for
grammar mining, the resulting grammar reflects only the actual processed inputs
but not the complete grammar of the input language accepted by the software
component. As a result, edge cases or previously supported features that no
longer appear in the available input data are missing from the generated
grammar. This work addresses this challenge by introducing a novel approach for
the automatic generation of inputs for grammar mining. Although input
generators have already been used for fuzz testing, it remains unclear whether
they are also suitable for grammar miners. Building on the grammar miner Mimid,
this work presents a fully automated approach to input generation. The approach
leverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms
to overcome the limitations of DSE regarding structured input parsers. First,
the search for new inputs is guided by an iterative expansion that starts with
a single-character input and gradually extends it. Second, input generation is
structured into a novel three-phase approach, which separates the generation of
inputs for parser functions. The proposed method was evaluated against a
diverse set of eleven benchmark applications from the existing literature.
Results demonstrate that the approach achieves precision and recall for
extracted grammars close to those derived from state-of-the-art grammar miners
such as Mimid. Notably, it successfully uncovers subtle features and edge cases
in parsers that are typically missed by such grammar miners. The effectiveness
of the method is supported by empirical evidence, showing that it can achieve
high performance in various domains without requiring prior input samples. This
contribution is significant for researchers and practitioners in software
engineering, offering an automated, scalable, and precise solution for grammar
mining. By eliminating the need for manual input generation, the approach not
only reduces workload but also enhances the robustness and comprehensiveness of
the extracted grammars. Following this approach, software engineers can
reconstruct specification from existing (legacy) parsers.

</details>


### [4] [Weak Memory Model Formalisms: Introduction and Survey](https://arxiv.org/abs/2508.04115)
*Roger C. Su,Robert J. Colvin*

Main category: cs.PL

TL;DR: 本文回顾和总结了弱内存模型的形式化方法、理论发展、工程实践与推理工具，为并发程序开发和可靠性保障提供理论基础与最新进展。


<details>
  <summary>Details</summary>
Motivation: 弱内存模型增加了并发程序开发的难度，尤其是在安全及关键软件领域。为帮助开发者有效应对这一挑战，急需对弱内存模型进行精准且可操作的理论规范和分析。

Method: 通过对弱内存模型的形式化方案进行全面梳理，包括操作语义和公理语义两种表示方式，并结合具体实例（如简化版x86），系统性地分析了其规格、实际表现、推理系统等。

Result: 梳理了弱内存硬件特性、历史发展、可计算性及复杂性成果，以及辅助开发和推理的相关工具系统，详细介绍了当前弱内存模型理论及实践前沿。

Conclusion: 本文总结了弱内存模型形式化研究的主要进展，并指出了当前和未来在该领域的发展方向。

Abstract: Memory consistency models define the order in which accesses to shared memory
in a concurrent system may be observed to occur. Such models are a necessity
since program order is not a reliable indicator of execution order, due to
microarchitectural features or compiler transformations. Concurrent
programming, already a challenging task, is thus made even harder when weak
memory effects must be addressed. A rigorous specification of weak memory
models is therefore essential to make this problem tractable for developers of
safety- and security-critical, low-level software.
  In this paper we survey the field of formalisations of weak memory models,
including their specification, their effects on execution, and tools and
inference systems for reasoning about code. To assist the discussion we also
provide an introduction to two styles of formal representation found commonly
in the literature (using a much simplified version of Intel's x86 as the
example): a step-by-step construction of traces of the system (operational
semantics); and with respect to relations between memory events (axiomatic
semantics). The survey covers some long-standing hardware features that lead to
observable weak behaviours, a description of historical developments in
practice and in theory, an overview of computability and complexity results,
and outlines current and future directions in the field.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 提出了17条软件工程领域的同理心指南，并通过实证分析和优先级框架，帮助团队有效落实同理心实践。


<details>
  <summary>Details</summary>
Motivation: 在软件工程领域，同理心虽然对于提升团队合作、交流与决策非常重要，但往往被忽视。研究动机是提升同理心在软件工程实践中的实际应用。

Method: 基于以往的实证研究，提出了17条可操作的同理心指南。通过对软件从业者的实际案例、应用难点与应对策略进行分析，并引入一个可视化优先级框架，以支持指南的采纳与实施。

Result: 形成了17条同理心指南，通过优先级框架帮助软件团队根据重要性、实施难度和采纳意愿进行分类，提出了整合同理心进日常工作的实际和灵活建议。

Conclusion: 本研究为软件工程中的同理心培养提供了一套具体、可行的实践指南，并通过优先级框架辅助其实践，帮助团队和组织将同理心理念付诸于日常和持续的行动。

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [6] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: 科研软件安全普遍薄弱，易受供应链攻击，建议采取简单可行的改进措施以提升安全水平。


<details>
  <summary>Details</summary>
Motivation: 科研软件的安全性对于确保科学结果的完整性和可复现性至关重要，但该领域的安全性研究仍然不足。特别是由于其依赖开源组件和分布式开发实践，科研软件在供应链攻击面前尤为脆弱。

Method: 本研究利用OpenSSF Scorecard，对3,248个高质量、主要经过同行评审的科研软件代码库进行了安全性分析。

Result: 结果显示，这些代码库的整体安全状况较弱，平均得分为3.5/10。其中，诸如签名发布和分支保护等重要的安全实践很少被实施。

Conclusion: 科研软件当前的安全措施不足，存在较大风险，因此提出了一系列可操作、低成本的建议，帮助科研团队提升软件安全性并保护科学诚信。

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [7] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 本文发现，仅凭应用元数据难以准确预测用户的解释需求，建议开发者结合元数据与用户反馈共同指导系统设计。


<details>
  <summary>Details</summary>
Motivation: 现代数字化环境下，软件系统需要支持用户理解系统交互方式及其行为背后的原因。为提升软件开发和需求工程效率，研究如何基于应用属性预测用户的解释需求，从而在开发早期或大规模需求挖掘中加以考虑。

Method: 分析包含4,495条应用评论及相关元数据（金标准数据集），通过相关性分析及线性回归模型，探索应用属性与用户解释需求之间的关系及其可预测性。并用495条人工标注的数据进行验证。

Result: 应用属性与解释需求大多仅呈弱相关，部分特性如版本、评论数、评分存在中等相关性。线性回归模型预测能力有限，难以做出可靠预测。在验证数据集上也得到相同结果。少数类别（如安全与隐私、系统行为）可稍好预测，用户交互和界面需求最难预测。

Conclusion: 解释需求高度依赖于具体上下文，无法仅通过应用元数据精确推断。开发者和需求工程师应结合直接用户反馈，才能有效设计可解释、以用户为中心的软件系统。

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [8] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: 本文提出了以人为本的GitHub Copilot评估框架，补足了单一技术指标的不足，重点关注交互、适应性和协作支持等人因需求，实验结果为未来AI编程助手的改进和研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: AI编程助手如GitHub Copilot被快速采用，但现有评估框架主要聚焦技术指标，忽视了人因因素，这影响了AI助手在软件开发中的有效整合。作者希望补足这一评估短板。

Method: 作者分析了GitHub Copilot通过聊天界面与用户互动的情况，测量了Copilot根据用户专业水平调整解释与代码生成的能力，并评估了其在协作编程中的有效性。此外，构建了以人为本的需求评估框架，设定了具体衡量指标。

Result: 通过测试表明，GitHub Copilot在适应用户专业水平、代码生成和协作支持方面存在一定效果，测试结果为未来人因需求的深入研究提供了参考。

Conclusion: 以人为本的评估对AI编程助手的整合与发展至关重要，本文提出的评估框架和测试结果有助于完善此类工具，更好地满足用户需求，对于后续自动化编程的人因分析具有重要意义。

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [9] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: 本文系统对比了ChatGPT、Copilot、Gemini和DeepSeek四大主流LLM在LeetCode自动解题中的表现。结果显示ChatGPT效率与稳定性最佳；Copilot和DeepSeek在难题上波动较大；Gemini难题表现欠佳。研究为开发者模型选择和理解GPT类代码生成能力提供了实证参考。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）如ChatGPT、Copilot、Gemini和DeepSeek已深度融入软件工程流程，但缺乏对它们在代码生成等实际开发任务中系统化、细致的性能比较。作者希望通过实证数据帮助开发者在实际选型时有据可依。

Method: 作者选取了150道LeetCode题目（涵盖简单、中等、困难），分别用Java和Python两种语言生成代码解答。对比分析了四个主流LLM在执行时间、内存使用与算法复杂度上的表现。

Result: ChatGPT在执行效率与内存消耗上表现始终稳定且优秀。Copilot与DeepSeek随着题目变难，性能波动加大。Gemini虽然在简单问题上表现良好，但在难题上尝试次数激增。整体上，四款模型在不同难度和任务下呈现出显著性能差异。

Conclusion: 每种LLM在代码生成等任务上有各自的优缺点。实证比较能为开发者的模型选型提供针对性指导，也揭示了GPT类模型在性能和算法复杂度方面的表现和局限。

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [10] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 压缩提升代码模型效率，但显著降低其对抗攻击下的鲁棒性，安全敏感场景需权衡使用，呼吁研究更均衡的压缩方案。


<details>
  <summary>Details</summary>
Motivation: 代码领域的Transformer模型虽然性能出色，但计算成本高、推理慢且产生较大环境影响，因此需要通过模型压缩（剪枝、量化、知识蒸馏等）来提升效率。但模型压缩后在面对对抗攻击时鲁棒性的变化尚不清楚，影响实际应用中的安全性。

Method: 作者对常见的模型压缩策略（剪枝、量化、蒸馏）进行了系统评估。选择三种典型代码语言模型，在三类软件分析任务上，通过六种评估指标和四种常见对抗攻击方法，测试压缩模型的鲁棒性。

Result: 压缩后的模型在正常情况下性能与原模型相当。但一旦受到对抗攻击，鲁棒性明显下降。

Conclusion: 模型压缩在提升计算效率的同时，会削弱模型面对对抗攻击的鲁棒性，两者之间存在明显权衡关系。在安全关键型软件应用中，部署压缩模型需谨慎。未来需要进一步研究兼顾效率与鲁棒性的压缩策略。

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [11] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: 本研究针对大语言模型在真实软件工程环境下的代码生成生产力，首次系统分析了项目级别任务中影响效率的人机交互特征，总结指引并提出错误对策，有助于提升实际开发效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在软件工程任务中的应用越来越广泛，但现有有关高效提问（prompting）技术的研究多集中于函数级别的问题，忽略了真实工作流程中更复杂的、多类依赖（如类之间的复杂关系）等情况。

Method: 设计并执行了一个综合实验，包括两个项目级别基准任务，并通过与GPT助手互动（采用特定提问模式）邀请36名不同背景的参与者完成任务。同时收集分析其屏幕录制和对话日志，从经验和行为角度考察影响代码生成效率的人机交互（HLI）特征。

Result: 实验揭示：(1) 15项HLI特征中有3项对代码生成生产力有显著影响；(2) 总结出提升生产力的五条主要指引；(3) 构建出29种HLI过程中运行期和逻辑错误的分类体系，并提出相应的缓解措施。

Conclusion: 真实、多类依赖等复杂背景下的人机-大语言模型交互中，有特定特征会显著影响代码生成效率。通过系统实验证明，采取合理指引和错误缓解措施能有效提升这类交互的效率和质量。

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [12] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust框架通过逐步骨架引导与自动修复，实现了大型C项目向Rust的高质量自动迁移，显著提升了准确率与安全性。


<details>
  <summary>Details</summary>
Motivation: Rust具有编译期安全保障，非常适合安全关键系统，因此存在将遗留C代码迁移到Rust的需求。现有方法（基于规则和LLM）各有弊端，难以同时满足代码安全与鲁棒性，大型项目支持也有限。

Method: 提出了EvoC2Rust，这是一个自动化框架，通过骨架引导的翻译策略，分三个阶段将C项目转换为语义等价的Rust项目：1）项目功能模块化并用增强型LLM生成Rust骨架；2）逐步替换函数实现；3）结合LLM与静态分析自动修复编译错误。

Result: 在开源基准和6个工业项目上评估，EvoC2Rust在项目级C转Rust翻译上表现优异，比LLM方法语法和语义准确率分别提升17.24%和14.32%，比基于规则的工具代码安全率高96.79%。模块级上，复杂代码库的编译通过率和测试通过率分别为92.25%和89.53%。

Conclusion: EvoC2Rust能高效推进大型C项目向Rust的迁移，兼顾代码安全、语义一致性及编译成功率，优于现有方案。

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [13] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: 本文提出并实践了迁移工具 Vanilla-Converter，能自动将 Camunda 7 的业务流程模型高效迁移至 Camunda 8，并在实际案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于 Camunda 7 即将停止支持，企业急需便捷、高效的方法将现有 BPMN 模型迁移至 Camunda 8，然而两平台架构差异大，人工迁移极为复杂。

Method: 提出并实现了命令行工具 Vanilla-Converter，对 Camunda 7 的 BPMN 模型进行自动转换，支持多种 BPMN 元素，并记录详细的转换日志。通过三个真实工业案例进行评估。

Result: 通过对三个实际工业用例的转化实验，Vanilla-Converter 成功将原有模型转为 Camunda 8 可用、可执行的模型，具备实用性。

Conclusion: Vanilla-Converter 工具能够有效将 Camunda 7 的 BPMN 模型迁移到 Camunda 8，生成可执行的模型，并详细标注转换过程中的自动与手动修改环节。

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [14] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: 本论文提出了一种基于人类错误理论的缺陷预测新框架，通过分析开发者编码习惯，显著提升了缺陷预测的效果和可解释性，为实际软件开发提供了更具指导意义的参考。


<details>
  <summary>Details</summary>
Motivation: 以往的软件缺陷预测大多基于代码度量，但很少关注非软件层面的指标。人类因素，特别是开发者的编程习惯，可能是缺陷的根本原因，因此有必要探索基于这些因素的新预测方法。

Method: 提出了一个用于选择预测指标的框架，重点利用开发者编码习惯相关的数据，并将这些新指标与历史上性能最好的代码层面及提交历史指标进行对比实验。最后还分析了各项指标在预测模型中的重要程度。

Result: 1）首次提出基于人类错误理论构建缺陷预测指标的框架；2）在21个大型开源项目实验中，基于该框架的新模型平均预测表现超过现有最佳方法；3）新指标在模型中的重要性通常高于传统代码和历史指标；4）该方法极大提升了模型的可解释性、实用性和可操作性。

Conclusion: 将开发者编程习惯等人类因素纳入缺陷预测，不仅提升了准确率，还让预测结果更具解释性和指导价值，推动了软件缺陷预测领域发展。

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [15] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: 本研究系统比较了主流静态分析工具与大语言模型在真实C#项目中的漏洞检测表现。语言模型召回率高、检测能力强，但误报与精度劣势明显。建议实际开发中采用混合流程，将大模型与传统规则引擎各取所长。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发高度依赖自动化测试和质量保证工具以防止错误、漏洞和潜在的安全隐患。该研究旨在评估现有主流静态代码分析工具与最新大语言模型在检测安全漏洞方面的优缺点和适用性。

Method: 比较了三种主流的基于规则的静态代码分析工具（SonarQube、CodeQL和Snyk Code）与三种先进的大型语言模型（GPT-4.1、Mistral Large和DeepSeek V3）。利用包含63个漏洞的10个真实C#项目，量化分析工具的检测精度（precision、recall、F-score）、分析延迟以及核查真阳性所需的开发者工作量。

Result: 大语言模型的平均F1分数（0.797、0.753、0.750）明显高于静态工具（0.260、0.386、0.546），主要来源于更高的召回率，显示了其跨代码上下文推理的能力。但缺点包括误报较高且定位漏洞的精度较差，难以在关键安全审计中单独使用。

Conclusion: 大语言模型在漏洞检测上可与传统静态分析工具竞争，但仅依赖其输出不适合高安全场景。推荐结合使用：开发早期用大模型做广覆盖初筛，关键阶段用规则引擎做精确验证。同时，该研究公开了基准数据集与结果处理框架，有助于后续可复现研究。

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [16] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 通过深度访谈和问卷调查，本研究全面分析了同理心在软件工程中的表现、动机及影响因素，并提出相关实践建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然强调同理心在软件工程中的重要性，但对于实际工作中同理心的体现、动因及影响因素缺乏系统理解。

Method: 采用了22个深度访谈和116名软件从业者的大规模问卷调查相结合的方法。

Result: 研究系统总结了同理心在SE活动中的外在表现、驱动因素、在哪些活动中被认为有用或无用，以及影响同理心的其他因素，并针对从业者和研究者给出了实践启示。

Conclusion: 本研究揭示了同理心在软件工程实践中的多种表现形式、动机以及影响因素，并提出了将同理心有效融入软件工程流程的建议。

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [17] [Control Closure Certificates](https://arxiv.org/abs/2508.03947)
*Vishnu Murali,Mohammed Adib Oumer,Majid Zamani*

Main category: cs.LO

TL;DR: 本文提出一种称为控制闭包证书的新方法，实现了离散时间系统对ω-正则规范的自动控制器合成，通过求和平方(SOS)方法自动搜索证书，并在多个案例中验证了方案效果。


<details>
  <summary>Details</summary>
Motivation: 传统ω-正则规范下的控制器合成依赖于归纳不变式与常规的良基性证明，这限制了自动化与灵活性。通过引入函数式类似的不变式概念——即闭包证书，有望提升系统化、自动化和可扩展性。

Method: 提出控制闭包证书的理论框架，并用求和平方优化方法来自动化搜索证书，从而实现对满足 ω-正则规范的控制器合成。具体包括将系统与赋予规范的奇偶自动机（parity automaton）构成积系统，通过不交的良基性（disjunctive well-foundedness）组合证据，最终为奇偶规范（parity specification）构建证书。

Result: 提出的控制闭包证书方法能够自动为离散时间系统合成满足 ω-正则规范的控制器，并在案例中展示了该方法的有效性与实用性。

Conclusion: 通过提出控制闭包证书（control closure certificates），可以有效自动地为离散时间控制系统合成满足 ω-正则规范的控制器，并且利用求和平方（sum-of-squares）优化方法搜索此类证书，实现了多案例验证。

Abstract: This paper introduces the notion of control closure certificates to
synthesize controllers for discrete-time control systems against
$\omega$-regular specifications. Typical functional approaches to synthesize
controllers against $\omega$-regular specifications rely on combining inductive
invariants (for example, via barrier certificates) with proofs of
well-foundedness (for example, via ranking functions). Transition invariants,
provide an alternative where instead of standard well-foundedness arguments one
may instead search for disjunctive well-foundedness arguments that together
ensure a well-foundedness argument. Closure certificates, functional analogs of
transition invariants, provide an effective, automated approach to verify
discrete-time dynamical systems against linear temporal logic and
$\omega$-regular specifications. We build on this notion to synthesize
controllers to ensure the satisfaction of $\omega$-regular specifications. To
do so, we first illustrate how one may construct control closure certificates
to visit a region infinitely often (or only finitely often) via disjunctive
well-founded arguments. We then combine these arguments to provide an argument
for parity specifications. Thus, finding an appropriate control closure
certificate over the product of the system and a parity automaton specifying a
desired $\omega$-regular specification ensures that there exists a controller
$\kappa$ to enforce the $\omega$-regular specification. We propose a
sum-of-squares optimization approach to synthesize such certificates and
demonstrate their efficacy in designing controllers over some case studies.

</details>


### [18] [GradSTL: Comprehensive Signal Temporal Logic for Neurosymbolic Reasoning and Learning](https://arxiv.org/abs/2508.04438)
*Mark Chevallier,Filip Smola,Richard Schmoetten,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: 本文提出了可无缝集成进神经符号学习的首个全面信号时序逻辑（GradSTL）工具链，自动化实现且具备形式化正确性证明，并有效支持任意采样信号的可微约束验证。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号学习在与信号时序逻辑（STL）集成时面临实现不全面和不够严谨的问题，特别是在对任意采样信号和约束进行可微分验证方面存在困难。

Method: 提出了一种名为GradSTL的新实现方式，首先形式化定义了在张量上的平滑STL语义，并对其导数函数的正确性和健壮性进行了形式化证明。然后，利用这一形式化定义通过自动化工具生成代码，实现无需手写代码的自动化构建，确保了正确性。

Result: GradSTL能够对任意信号和任意STL约束进行平滑可微的检查，并且可以无缝集成到神经符号学习框架中。通过案例研究表明，利用该方法可以使神经符号过程成功学习并满足预设的STL约束。

Conclusion: GradSTL为信号时序逻辑与梯度下降学习的集成提供了极为严谨且通用的基础，可显著提升相关系统的正确性与灵活性。

Abstract: We present GradSTL, the first fully comprehensive implementation of signal
temporal logic (STL) suitable for integration with neurosymbolic learning. In
particular, GradSTL can successfully evaluate any STL constraint over any
signal, regardless of how it is sampled. Our formally verified approach
specifies smooth STL semantics over tensors, with formal proofs of soundness
and of correctness of its derivative function. Our implementation is generated
automatically from this formalisation, without manual coding, guaranteeing
correctness by construction. We show via a case study that using our
implementation, a neurosymbolic process learns to satisfy a pre-specified STL
constraint. Our approach offers a highly rigorous foundation for integrating
signal temporal logic and learning by gradient descent.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 研究发现GPT-4 Turbo在有关印度故事生成中，对宗教和种姓展现出严重代表性偏见，主要偏向主导群体，现有利用提示词的多样性鼓励手段难以消除此偏见，仅依靠增加多样化训练数据不能完全解决LLM偏见问题。


<details>
  <summary>Details</summary>
Motivation: 以往对大型语言模型（LLMs）代表性偏见的研究集中在单一回复且关注于北半球如种族和性别等身份特征。本研究意在探究这些偏见在LLMs中的深度，以及如何延伸至被较少关注的身份维度（如印度的宗教和种姓）。

Method: 通过系统性审查，向GPT-4 Turbo提出7200多条与印度重要生活事件相关的故事生成请求，使用不同程度鼓励多样性的提示词，然后将其输出中的宗教和种姓多样性与印度人口普查数据对比，量化代表性偏见及其“黏性”。

Result: GPT-4 Turbo在宗教和种姓的生成内容中显著过度代表了文化主导群体，即使在输入提倡多样性的情况下，群体分布远超真实人口比例。重复性提示调整对偏见的消减效果有限且不一致。

Conclusion: 仅仅增加多样化训练数据可能不足以纠正LLMs代表性偏见，模型开发需进行更根本性的变革。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [20] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 本文针对高能理论物理研发了20种Llama-3.1模型微调变体，均在领域任务上超越基础模型，并与市面主流LLM进行了性能对比，总结了面向小众学术领域定制大语言模型的经验。


<details>
  <summary>Details</summary>
Motivation: 高能理论物理领域需要更专业和表现优秀的大语言模型，目前通用大模型难以应对该领域的特定任务。

Method: 对8B参数的Llama-3.1模型进行了细致微调，产生了20个变体，分别在高能物理的hep-th、hep-ph、gr-qc等不同子领域arXiv摘要数据上训练。同时对比了包括生物、计算机等交叉领域摘要上的效果；采用两种不同的低秩适应（LoRA）微调方法和不同数据集规模。

Result: 所有微调后的模型在高能理论物理（hep-th）摘要补全文本生成任务中均优于基础Llama-3.1模型。

Conclusion: 专门面向高能理论物理领域的大语言模型，通过定制微调，可显著提升该领域的学术文本生成表现，并为后续专用模型提供有价值的开发思路。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [21] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文提出并实现了智能农业助手Krishi Sathi，通过多轮对话、指令微调和检索增强技术，为印度农民提供英印双语、文本与语音兼容的高质量个性化农业咨询支持，显著提升服务可及性和响应效率，测试各项性能指标均表现优秀。


<details>
  <summary>Details</summary>
Motivation: 印度农民、尤其是农村与低识字率群体，难以及时获得便捷且适合本地语境的农业咨询服务，需要一种低门槛、智能化、个性化的数字支持工具。

Method: 提出并实现了Krishi Sathi AI农业聊天机器人，采用指令微调（IFT）模型，结合三组印度农业知识数据集进行细化；实现多轮对话流程，通过RAG(检索增强生成)从农业知识库检索并生成个性化响应，支持英印双语及语音输入输出用于低识字率用户。

Result: 系统在查询响应准确率（97.53%）、情境相关与个性化（91.35%）、查询完成率（97.53%）等方面表现优异，平均响应时间不超过6秒，满足用户对及时性的需求。

Conclusion: 结合基于意图的对话流、指令调优的模型和检索式生成技术，能够显著提升印度农业数字化咨询服务的质量与可及性，尤其是在低识字率和乡村地区。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [22] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 本文提出一种新的验证框架（HVT），能显著加速大语言模型的推理过程，同时保持甚至提升生成质量，无须额外训练或修改模型结构，实验结果优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自然语言处理领域表现出色，但由于自回归生成方式，推理效率有限。虽然已有投机解码和束采样等改进方法，但传统方案在验证草稿序列时没有优化优先级，造成计算冗余。

Method: 提出了一种层次化验证树（HVT）框架，重构了投机束解码流程，通过优先验证高概率草稿并及早剪枝低质量候选，提高推理效率。该方法包括理论基础和形式化的验证-剪枝算法，无需模型重新训练或结构修改，可直接融入标准推理流程。

Result: 在多个数据集和模型上的实验显示，HVT方法在推理速度和能耗上显著优于以往投机解码方法，并且保证或提升了生成质量。

Conclusion: HVT展现了层次化验证策略在加速大语言模型推理中的潜力，为高效推理开辟了新方向。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [23] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: 本文提出WiNELL框架，结合多智能体和高效编辑模型，显著提升了维基百科自动化、持续更新的能力，在关键信息覆盖和效率方面超越多种先进基线，为自动知识库维护开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 维基百科是全球最大的知识库之一，但由于其高度依赖人工编辑，内容及时更新一直是主要挑战。随着NELL等系统对持续知识获取的探索以及大型语言模型（LLM）智能体的快速发展，自动化知识更新显得尤为重要。

Method: 设计并提出了WiNELL——一种基于多智能体的框架，通过聚合在线信息，筛选新的重要知识，为维基百科目标实体生成精确的编辑建议，并由人工最终审核。其编辑模型以维基百科历史人工编辑数据为训练集，以贴近真实编辑风格。

Result: 编辑器模型在关键信息覆盖和编辑效率方面均优于开源指令跟随基线模型和闭源LLM（如GPT-4o）。在高活跃度维基百科页面的端到端评测中，WiNELL能够及时发现并建议精准的事实更新。

Conclusion: WiNELL能够实现高效、持续的维基百科内容自动更新，推动了LLM智能体自动维护大规模知识库的研究方向。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [24] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 本文提出了一个涵盖英文和印地语数学题目的视觉推理基准GanitBench，并发现当前模型在此基准上表现有限，尤其在印地语上的表现更弱，对多语言VLM研究具有推动作用。


<details>
  <summary>Details</summary>
Motivation: 近年来，视觉语言模型（VLMs）推理能力的评测基准增多，但这些基准大多是英文，涵盖其他语言（如印地语）以及除理解和翻译外任务的数据集稀缺。作者希望通过该研究促进包括印地语在内的多语言VLM研究发展。

Method: 作者提出了GanitBench基准，包含1527道主要来自印度JEE Advanced和CBSE考试，涵盖多类数学题的纯视觉问题，并以图片（包含图形和文本）的形式，支持英文和印地语。并在零样本和两样本链式思考（Chain-of-Thought, CoT）设置下，对两个闭源VLM模型进行了评测。

Result: GPT-4o mini在该基准上的表现优于对比模型，最高平均准确率为38.15%。在“Double Lock”约束（更严格的设定）下，模型性能显著下降。两样本CoT设置较为有效。此外，模型在印地语问题下表现较差。

Conclusion: GanitBench作为多语言视觉推理基准，尤其补足了印地语数学任务的空白。双语言、多任务设定展现了当前VLM的局限，也为未来多语言VLM研究提供了参考和数据支持。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [25] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: 本文提出了基于注意力权重的高效上下文追踪方法AttnTrace，显著提升了准确性与效率，有助于大模型可解释性与安全应用，开源代码已发布。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文大语言模型（如Gemini-2.5-Pro、Claude-Sonnet-4）被广泛应用于复杂AI系统，这些系统需要追踪哪些上下文内容对模型输出影响最大。然而，主流追踪方法如TracLLM计算成本高、效率低，制约了实际应用。

Method: 作者提出了一种基于LLM注意力权重的新上下文追踪方法AttnTrace；为有效利用注意力权重，设计了两项改进技术并给出了理论解释；系统性地对比评估了该方法。

Result: AttnTrace在准确性和效率上均优于现有主流方法，尤其在检测长上下文下的提示注入方面更具效果，同时展示了应用于检测模型输出操控行为的实际场景。

Conclusion: AttnTrace为大语言模型追踪关键上下文提供了一种高效、可信的新方案，并有助于提升模型可解释性、检测安全风险，具备广泛真实世界应用前景。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [26] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: 本文提出新型多比特水印方法MajorMark及其增强版本，通过多数比特感知的编码策略和聚类解码，有效提升了水印文本的生成质量和解码准确率，表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在实际应用中的广泛部署，如何防止其生成有害或误导内容成为一个重要问题。水印技术可通过在生成文本中嵌入可识别的二进制信息，实现来源验证和滥用追踪。然而，现有多比特水印方案通常在文本质量和解码准确率之间存在权衡，难以兼顾。

Method: 提出了一种新颖的水印方法MajorMark，通过基于多数比特的编码策略选择优先token集合，使得能够更灵活、扩大生成token的范围，同时引入基于聚类的新型解码方法，在优先token集合较大时也能保持高解码准确率。此外，提出了MajorMark+，通过分块编码和确定性解码进一步优化文本质量和解码性能。

Result: 在主流大语言模型上的大量实验显示，MajorMark和MajorMark+在解码准确率和文本生成质量方面均显著优于已有多比特水印方法。

Conclusion: MajorMark及其增强版本MajorMark+有效改善了水印文本在保证内容生成质量的同时，实现了高精度的多比特消息嵌入与解码，为实际LLM应用提供了更优的溯源和防滥用技术方案。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [27] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 本综述梳理了LLM事实核查的挑战和最新方法，指出需结合RAG、领域微调等提升事实准确性，未来研究应重视评测指标改进和领域定制。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的训练数据包含大量不准确或误导性信息，导致其生成内容容易出现虚假信息。因此，对LLM输出内容的事实核查和一致性评估变得尤为重要。

Method: 本文采用系统性文献综述的方法，梳理并分析了2020—2025年间关于LLM事实性评测和改进措施的最新研究，并围绕五个关键研究问题进行归纳。

Result: 现有评测指标存在局限，有效事实检验需结合高阶的提示设计、领域微调和检索增强生成（RAG）等方法。特别强调利用可靠外部证据、领域定制方法以及多主体推理和指令微调，以提升模型输出的事实一致性和可信度。

Conclusion: 论文强调需要构建具备领域适应性、准确性和可解释性的LLM，以增强其事实一致性和可靠性，从而提高其在不同应用场景下的可信度。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [28] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型的实体链接智能体，尤其适用于短文本问答问题，并在实验中表现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的实体链接（EL）方法主要针对长文本设计，面对问答任务中较短且模糊的用户问题表现不佳。因此，需要一种能够胜任短文本问答场景的新方法。

Method: 提出了一种基于大型语言模型的实体链接智能体，通过模拟人类认知流程，主动识别实体提及、检索备选实体，并做出判断。

Result: 通过实体链接工具和问答任务评测实验，结果显示所提出的智能体在鲁棒性和有效性上均有优异表现。

Conclusion: 该智能体方法能够提升问答系统在处理短小、模糊问题时的实体链接性能，为知识库问答提供更准确的答案。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [29] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出Sotopia-RL，通过话语级、多维回报优化社交智能RL训练，显著提升大模型社交任务表现，并在公开环境中实现最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在处理现实世界的社交任务时，对社交智能的需求不断上升。然而，利用强化学习（RL）训练具备社交智能的模型面临两个主要挑战：一是互动的部分可观测性导致回报归因困难；二是社交行为具有多维目标，单一维度的回报设计无法覆盖其复杂性。

Method: 本文提出了一种新的强化学习框架Sotopia-RL，该方法将原本粗粒度的回报（如每个episode的回报）细分为话语级别，并引入多维度的回报机制。这样能够更好地进行信用分配，并全面捕获社交互动的多样性。

Result: Sotopia-RL在开放式社交学习环境Sotopia中取得了最先进的社交目标完成率（在Sotopia-hard上为7.17，Sotopia-full上为8.31），显著优于已有方法。消融实验显示，话语级信用分配和多维回报设计对于强化学习训练至关重要。

Conclusion: 针对社交智能训练中的难题，Sotopia-RL通过细粒度和多维度的奖励设计，显著提升了大模型在社交任务中的表现，为社交智能RL训练提供了有效解决方案。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [30] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: 本文提出结合GUI操作和代码执行的多智能体系统CoAct-1，在自动化复杂电脑任务上显著提升了效率和成功率，开辟了更通用高效的计算机自主代理新方向。


<details>
  <summary>Details</summary>
Motivation: 现有通过图形界面（GUI）操作计算机的自主代理，在处理复杂、长期任务时效率低且易出错。虽然引入规划器可以改进任务分解，但完全依赖GUI操作本身存在固有限制，造成低效和不稳定。

Method: 提出CoAct-1系统，将传统GUI操作和直接编程执行结合。该系统包含一个调度者，能够根据任务特性动态分配子任务给GUI操作员或程序员代理（后者可编写执行Python或Bash脚本）。

Result: CoAct-1在OSWorld基准测试中取得了60.76%的最新最优成功率，显著超过先前方法；同时平均任务步骤减至10.15步，优于仅依赖GUI的15步。

Conclusion: 将编程作为核心操作，结合GUI和代码执行的方法，使得自主代理系统在通用自动化任务上更高效、强大、具备可扩展性。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [31] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: 本文提出的CAP-LLM方法通过结合用户偏好与事实一致性约束，显著提升了新闻头条的个性化和准确性，在多个指标上超过现有最强方法。


<details>
  <summary>Details</summary>
Motivation: 在信息过载时代，用户对新闻头条的个性化和事实准确性有更高需求。现有方法难以充分捕捉复杂用户兴趣且易引入事实错误，导致头条同质化或误导性强。

Method: 提出了一种名为CAP-LLM的新型框架，利用大语言模型，集成用户偏好和事实一致性约束。其结构包括：用户偏好编码器用于捕捉长期兴趣；上下文注入适配器融合用户与新闻上下文进生成过程；事实一致性增强模块通过对比损失减少虚假内容。

Result: 在真实PENS数据集上，各项指标均达最新最优水平。如事实一致性FactCC高达87.50，明显优于BART（86.67）；个性化和信息覆盖指标均显著提升（如ROUGE各项分数均为领域领先）。消融、人工和敏感性实验均验证了各组件和方法的有效性与稳健性。

Conclusion: CAP-LLM能够有效提升新闻头条生成中的个性化和事实一致性，实现了两者的优质平衡。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [32] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文提出一套覆盖开发到运营全过程的大模型偏见与公平性治理体系，扩展BEATS工具，实现对模型的持续评估和风险防控，为生成式AI安全应用提供实践方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在发展和应用过程中，容易出现偏见、不公平和事实错误等伦理及安全风险。当前的治理和评估方法难以系统覆盖整个机器学习模型生命周期，实际落地亟需完善的框架与工具。

Method: 提出并推广了一整套数据与AI治理方法，涵盖模型开发、验证、生产上线及监控等各阶段。在此基础上，扩展了已有的偏见评估工具（BEATS），结合治理框架，系统对LLM的偏见、伦理、公平性和事实准确性进行检测与管理。

Result: 这种方法可在生产部署前为LLM进行严格基准测试，并支持持续、实时评估和主动规控，有效增强生成式AI系统的安全性和责任感，减少歧视和声誉风险。

Conclusion: 通过全生命周期的数据和AI治理，能够显著提升生成式AI的社会责任与伦理契合，为安全、可信的AI应用落地提供切实保障。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [33] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出了一种通过早期假设剪枝提升自一致性token效率的方法，并在多个模型和任务上验证了有效性，节省10-35%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 自一致性方法虽然简单且有效，但在实际应用中由于需要消耗大量的token，尤其是在处理长链思维推理任务时，使用受限。论文希望在保持并行性的基础上，提高token效率。

Method: 作者提出通过早期假设剪枝提升自一致性推理的token效率。具体做法是在并行生成所有解答的同时，周期性地利用两种轻量级指标（模型对单独假设的置信度以及假设集合的词汇覆盖率）剪除不必要的中间假设，并设计了一个快速加权集合覆盖算法来实现。

Result: 在三个数学基准任务和五个大型语言模型上的实验表明，该方法能在许多情况下提升所有模型10-35%的token效率。

Conclusion: 通过早期假设剪枝，可在不损失并行性的前提下显著提升链式思维推理任务中的token效率，具有实际应用价值。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [34] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 本文通过构建大型数据集和以LLM为判官的评价体系，证明了针对幸福感相关知识，LLM的解释质量可通过偏好微调大幅提升，且微调后的小模型可超越更大但未经优化的模型。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的人通过大语言模型（LLM）获取有关幸福感（well-being）信息，如何让LLM生成既精准又能适应不同用户群体需求的解释，成为一个关键挑战。该问题的核心在于，解释不仅要事实正确，还需能够满足不同专业背景用户的期望。

Method: 作者构建了一个大规模数据集，包含来自十个不同LLM的43,880条关于2,194个幸福感概念的解释。提出了一种基于原则的“以LLM为判官”的评估框架，采用双LLM判官模式对解释质量进行评价。同时，利用监督微调（SFT）和直接偏好优化（DPO）两种策略对开源模型进行微调，以提升解释质量。

Result: （1）以LLM为评判员的评价结果与人工评估高度一致；（2）不同模型、用户群体和幸福感类别的解释质量存在明显差异；（3）通过DPO与SFT微调后的模型，在专门化解释任务上表现超过规模更大的基础模型，表明偏好学习对优化解释有显著作用。

Conclusion: 本研究证明了以LLM为评判员的评价体系的可行性，并且通过偏好微调等方法能够显著提升LLM生成幸福感相关解释的质量。由此，针对特定任务，通过优化和微调可以获得更高质量、个性化的解释能力，适用于不同专业和知识背景的用户群。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [35] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于多模态数据的社会机器人共同促进者，通过可解释的概念瓶颈模型辅助群体会议促进，并能将专家知识迁移给新手，效果显著优于传统黑箱模型，提升了智能助理在复杂社交环境中的可用性和可信度。


<details>
  <summary>Details</summary>
Motivation: 群体会议的成功需兼顾个人目标的实现与群体关系的强化，但促进者需应对成员脱离、目标执行困难及人际关系问题，负担极大。现有“黑箱”基础模型虽能识别社交线索，却缺乏可解释性和针对性，因此需要能自动解释群体互动和个体需求的智能辅助工具。

Method: 提出了一种社会机器人共同促进者，使用多模态会议数据，为促进者提供实时、隐蔽的建议。其核心为概念瓶颈模型（CBM），通过人类可解释的概念（如参与度、情感）做出推断，支持知识迁移和人类实时纠错。主要方法为将基础模型的广泛社交理解迁移至具备可解释性的CBM中。

Result: 提出的概念驱动系统在预测何时需要干预上显著优于直接零样本基础模型，且支持模型决策的实时人类修正。模型能跨群体泛化，并将高级促进者的专业知识迁移给初学者，有效提升新手表现。

Conclusion: 该系统借助可解释的机器人伙伴，将资深专家的认知模型导入实际促进，会显著增强群体会议等复杂社会场域下的人类能力。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [36] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本文提出多智能体协作框架HarmonyGuard，能联合提升效用和安全性。通过实时策略更新和目标优化，在基准测试任务中显著提升了智能体的策略合规率和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动了智能体在开放网络中的自主任务执行，但由于网络环境中的潜在威胁不断演变，智能体需要在任务表现和安全风险之间做权衡。目前相关研究大多只关注单一目标或单轮场景，缺乏在网络环境下同时优化效用和安全性的能力。

Method: 提出HarmonyGuard多智能体协作框架，结合策略增强和目标优化以联合提升效用与安全。包括：(1) 引入Policy Agent自动提取与维护结构化安全策略，并应对安全威胁持续更新；(2) 引入Utility Agent基于安全和效用双重目标，采用马尔可夫实时推理和元认知进行评估与优化。

Result: 在多个基准测试中，HarmonyGuard相较现有方法，策略合规性最多提升38%，任务完成率提升20%，并在所有任务中实现了90%以上的策略合规率。

Conclusion: HarmonyGuard能够在网络环境下兼顾任务效用与安全性，为智能体系统带来更高效安全的任务执行能力，并大幅提升策略合规与任务完成表现。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [37] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 本文提出新方法SMEdit，通过多步反向传播和正则化技巧，解决了大语言模型编辑中低数据表现和训练效率问题，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型更新知识成本高，而模型编辑作为替代手段已获得进展，尤其是元学习方法。然而，现有MLBME方法在低数据和训练效率方面表现不佳，限制了其实际应用。

Method: 提出了一种新型元学习型模型编辑方法SMEdit，核心为多次反向传播步骤（MBPS）用于提升低监督编辑效果，并通过权重更新的范数正则化改善训练效率。

Result: 在两个数据集和两个LLM上实验表明，SMEdit明显优于之前的MLBME基线方法，且MBPS方法可以无缝集成进现有算法并提升其效果。

Conclusion: SMEdit方法显著提升了在低数据场景下的模型编辑效果和训练效率，并优于现有MLBME方法。MBPS策略具有良好的通用性，可直接融合进其他方法并带来性能提升。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [38] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA是首个无需训练即可实现可解释、零样本运动识别的智能体框架，性能远超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的动作传感器时间序列分析方法通常只能处理固定集合的活动，并且在出现新行为或新传感器配置时需要昂贵的重新训练，对泛化和可解释性有较高需求。最近尝试用大型语言模型（LLM）处理该任务，但面临准确率与可解释性不足问题。

Method: 提出了ZARA，这是一个首创的基于智能体、可零样本、可解释的动作识别框架。ZARA由三个核心部分组成：1）自动获取的两两行为特征知识库，2）多传感器证据检索模块，3）层次化智能体流水线，引导LLM逐步选择特征、提取证据，并最终给出预测及自然语言解释。ZARA无需微调或特定分类器即可灵活执行任务。

Result: 在8个权威动作识别数据集上，ZARA达到了最优的零样本性能，在宏F1分数上超过最强基线2.53倍。消融实验验证了每个模块的重要性。

Conclusion: ZARA实现了无需任务特定训练、可解释且灵活的动作识别，为可信赖、即插即用的人类动作识别分析开辟了新途径。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [39] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 研究发现，大型推理模型极大降低了AI模型越狱门槛，一般用户也能轻松实施越狱攻击。测试中，自动化攻击成功率高达97.14%，显示当前主流AI模型面临严重的安全风险，亟需加强模型的抗越狱与自保能力。


<details>
  <summary>Details</summary>
Motivation: AI模型固有的安全机制通常需要复杂技术或专家知识才能绕过，然而随着大型推理模型（LRMs）说服能力的提升，越狱攻击变得简单且廉价，甚至普通用户也能操作。因此，探究LRMs在AI模型越狱中的作用及其安全隐患变得迫切。

Method: 作者挑选了四种大型推理模型（DeepSeek-R1、Gemini 2.5 Flash、Grok 3 Mini、Qwen3 235B）作为攻击者，与九种广泛使用的目标模型进行多轮自动对话，并通过系统提示给予基本指令。实验设计覆盖七大敏感领域共70条有害提示，系统性测试组合攻击成功率，无需后续人工干预。

Result: 在所有模型组合中，攻击平均成功率高达97.14%。实验结果表明，LRMs能够高效地削弱或绕过其他AI模型的安全机制。

Conclusion: 大型推理模型不仅易于被用作越狱工具，还能作为自动化攻击者高效破坏AI模型的安全措施，暴露了AI模型在对抗恶意利用及安全对齐方面的重大短板。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [40] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出了动态Token级前缀增强（DTPA）方法，有效改善了长文本可控生成中的属性控制能力，并在多个任务中优于传统方法，生成文本流畅且相关性强。


<details>
  <summary>Details</summary>
Motivation: 以往的可控文本生成方法在生成长文本时，文本的可控性下降，尤其是前缀注意力衰减导致生成结果不理想。

Method: 提出了一种基于Air-Decoding的动态Token级前缀增强（DTPA）框架，能够根据任务动态调整前缀类型，并在生成过程中指数增强前缀注意力，从而提升长文本生成的可控性，并可选性增强原始提示，兼顾文本质量。

Result: 实验显示，DTPA算法在保证文本流畅性、主题相关性和多样性的同时，大幅提升了长文本中的属性控制能力。分析还证明其在长文本生成方面尤其有效。

Conclusion: DTPA能够很好地提升长文本生成中的属性可控性，同时保持流畅度、多样性和主题相关性，并在多种可控文本生成任务中优于现有方法。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [41] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: PAIRS框架通过智能判断何时需要检索和如何选择外部信息，在节省25%检索成本的同时还能提高问答准确率，无须另行训练，简单有效。


<details>
  <summary>Details</summary>
Motivation: 当前的RAG系统在简单问题上不必要地进行检索，浪费资源且效率低；对于信息稀疏的查询，容易检索到无关文档，影响回答准确性。

Method: 提出了一种无需训练的新型框架PAIRS，通过双路径生成机制：先让LLM独立生成直接回答和伪上下文增强回答，若两者一致则直接输出结果，无需检索；若不一致，再根据原始查询与自生成上下文信号联合引导检索、并通过加权相似度过滤文档，最终输出答案。

Result: 在六个问答基准数据集上，PAIRS能够减少约25%的检索操作（仅对75%的查询需要检索），同时准确率提升，平均EM提升1.1%，F1提升1.0%。

Conclusion: PAIRS无须训练即可高效融合LLM已有知识与检索知识，自适应决定是否检索及如何选择外部信息，提升效率并提高问答效果。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [42] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 本文聚焦于资源受限情境下提升大语言模型效率，综合数据、训练与架构优化方法，实验验证了效率与性能的改进，为LLMs应用提供了实用策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然在人工智能和自然语言处理领域取得了突破，但其大规模部署受到计算资源需求高的限制。推动更高效的LLM以适应资源受限环境是当前的研究难题。

Method: 本文从基础模型出发，结合数据处理、数据筛选、训练策略及架构调整等综合方法。具体包括建立可靠数据集的标准、设计受控实验和系统评估模型能力、通用性、响应时间与安全性。

Result: 实验结果展示了不同改进策略下模型性能的变化，通过对比试验验证了所提出方法在提升LLM效率方面的有效性。

Conclusion: 提出的策略能够显著提升大语言模型在受限资源和特定知识库内的效率和能力，具有现实部署价值。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [43] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad框架以答案先行生成高质量工具使用数据集，实现更低成本和更高复杂度注释，并显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用类大模型（LLM）数据集的合成方法，通常是先生成用户查询，再进行复杂的工具使用注释，但这导致注释失败和数据生成效率低。

Method: 提出了ToolGrad框架，反转传统生成流程，即先基于文本“梯度”迭代构造有效工具使用链，然后再合成相应的用户查询，实现“先答案后问题”的数据生成模式。

Result: 生成了ToolGrad-5k数据集，该数据集工具使用更复杂、生成成本更低，并且注释通过率达到100%。

Conclusion: 实验表明，在ToolGrad-5k上训练的模型，无论是在常规还是分布外（OOD）基准上，都优于用昂贵基线数据集或商业大模型训练的模型。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [44] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了一种可自动纠错的生成式多模态流程奖励模型（GM-PRM），大幅提升多模态大模型在复杂数学推理中的表现，且仅需少量训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型（MLLMs）在处理复杂的、多步的数学推理任务时表现不佳，主要因为微小的视觉感知或逻辑推理错误会导致整个推理失败。虽然流程奖励模型（PRMs）可以提供分步监督，但当前多模态PRMs仅能识别错误，无法纠正，且解释能力有限。

Method: 提出生成式多模态流程奖励模型（GM-PRM），它不仅能细致地分析每一步的推理意图、视觉匹配和逻辑合理性，还能主动生成错误推理步骤的修正版本。利用这一纠错能力，作者提出了Refined Best-of-N（Refined-BoN）的测试推理策略，引导模型朝更优的推理路径发展。

Result: GM-PRM在多个多模态数学基准上取得了最新最优（SOTA）成绩，极大提升了策略模型的性能，同时只需极小的数据量（仅2万样本）即可训练。

Conclusion: GM-PRM大幅提升了多模态大模型在复杂数学推理中的解释性、纠错能力和整体性能，并以高数据效率取得了领先成绩。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [45] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: LLM微调过程中容易产生过度记忆，虽准确率未降但鲁棒性和泛化变差。作者系统分析该现象及成因，并提出微调优化建议。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在微调过程中，特别是在推理任务上的表现存在未被充分认识的问题。论文旨在探索LLM微调过程中存在的新型过度记忆现象，以促进对模型学习机制的理解。

Method: 通过将LLM在推理任务上进行微调，实验分析模型在不同训练轮次和学习率下的学习动态，观察模型在保持测试准确率情况下出现的高测试困惑度、鲁棒性下降、泛化能力变差等现象。覆盖了多任务、多模型、多种微调方法。

Result: 在模型微调的特定阶段，LLM会对训练数据产生过度记忆，导致测试困惑度高，但测试准确率依然不错。同时，发现过度记忆使模型鲁棒性、分布外泛化能力和生成多样性降低。该现象在不同任务、模型和微调方法中均普遍存在。

Conclusion: 过度参数化、长期微调的LLM在学习动态上表现出与传统模型显著不同的特点。基于过度记忆现象，作者提出了模型微调时关于检查点选择和学习率调整的优化建议。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [46] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 通过基于难度的数据选择方法，显著提升了大语言模型对齐效率和效果，仅用少量优选数据即可超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于人类偏好的对齐方法如RLHF和DPO通常需要大量且昂贵的偏好数据集，缺乏专门针对偏好数据进行高质量数据选择的方法。

Method: 提出了一种基于DPO隐式奖励机制的、以难度为导向的数据选择策略，选择具有较小DPO隐式奖励差的数据样本作为更具挑战性的训练例子。

Result: 所提出的方法在多个数据集和任务中均优于五个强力的基线，仅用原数据的10%即可取得更优的效果。

Conclusion: 这种高效且有原则性的数据选择方法能在资源有限的情况下有效扩展大模型对齐过程，对LLM对齐问题具有重要意义。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [47] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 本文提出以Human Fooling Rate衡量TTS模型拟人度并开展大规模测试。结果显示主流TTS模型在迷惑人类方面尚有限，商用系统表现优于开源，建议未来评测应更贴近真实人类感知。


<details>
  <summary>Details</summary>
Motivation: 近年来TTS（文本到语音）技术主观评价显示进步明显，但现有TTS系统能否在类似图灵测试的人类欺骗测试中通过，尚不明确。

Method: 提出了一种新的衡量指标Human Fooling Rate (HFR)，即机器生成语音被误认为人类语音的比例，并对开源与商用TTS模型进行了大规模评估。

Result: （i）基于CMOS的“媲美人类”主张在欺骗测试下常常不成立；（ii）TTS进步应在高HFR数据集上基准测试，以避免设置过低的评价标准；（iii）商用模型在零样本情境下接近人类欺骗能力，而开源系统在自然对话类语音上表现尚有距离；（iv）高质量数据微调有助提升拟真性但还不能完全弥补差距。

Conclusion: TTS系统需要通过更真实、更关注人类体验的测试来评价，而不仅仅依赖传统的主观评价手段。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [48] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 本文针对多模态大语言模型幻觉问题，通过因果分析剖析其根源，并提出基于因果完备性的强化学习方法，有效降低幻觉现象，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉-语言任务上表现优异，但存在幻觉问题，即模型输出与输入的图像或文本在语义上不一致。论文希望分析并解决这一幻觉问题。

Method: 通过因果分析，论文发现幻觉可分为遗漏和捏造两类，分别由未捕捉因果要素和被非因果线索误导引发。针对这些问题，提出了由因果完备性引导的强化学习框架，同时考虑token的因果充分性和必要性。具体做法是评估每个token的独立贡献和反事实不可或缺性，以此定义token级的因果完备性奖励，并用在GRPO优化框架中。

Result: 实验结果表明，该方法能有效缓解多模态大语言模型中的幻觉问题，在多种基准数据集和任务上均取得了良好效果。

Conclusion: 论文以因果推断为理论基础，提出了一种新型因果完备性引导的强化学习方法，显著减少了多模态大语言模型中的幻觉生成。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [49] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 本文系统定义了深度研究任务，并提出了LiveDRBench基准，测试当代模型在该任务上的表现，发现目前系统在大范围推理探索上存在显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 近年来，对深度研究（deep research）任务的关注不断增加，但其定义模糊，与其他高推理任务的区分不明确。作者希望更系统地定义‘深度研究’任务，并建立评测基准。

Method: 作者提出了对深度研究任务的正式界定，认为其核心特征是搜索过程中对概念的大范围、高推理性探索，并提出用中间输出表征分离推理与报告生成。他们据此构建了名为LiveDRBench的基准集，涵盖100个关于科学与公共事件的挑战性任务。

Result: 在该基准集上，当前最先进的深度研究系统的子任务F1分数在0.02至0.72之间，OpenAI模型表现最佳，总体F1为0.55。作者还分析了推理轨迹，覆盖了引用来源数量、分支与回溯等指标。

Conclusion: 本文明确了深度研究任务的核心特征、给出了评测基准，并揭示了当前系统在推理和搜索机制上的不足，为未来改进方向提供了依据。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [50] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 本论文揭示了大语言模型在复杂对话下易被诱导出现误对齐问题，并提出了自动评测工具和分类体系，跨模型实验证明误对齐现象普遍且严重，指明未来AI需提升针对隐蔽操控场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管在大模型对齐技术上取得了显著进展，但实际应用中模型仍存在在某些复杂对话场景下失效的隐忧。为评估和揭示这些潜在的安全隐患，作者希望系统性地挖掘现有对齐方法的局限性。

Method: 作者采用系统性的人工对抗（red-teaming）测试，针对Claude-4-Opus发现并总结了10种能够诱发模型失效（误对齐）行为的场景，并将这些攻击方法转化为一个自动化评测框架MISALIGNMENTBENCH，以便在多个主流大模型上进行可重复性测试和横向比较。

Result: 实验表明，10种攻击场景在5个前沿大语言模型上的平均成功率高达76%，其中GPT-4.1最易受攻击（90%），而Claude-4-Sonnet表现出更高的抵抗力（40%）。这些场景能引发模型欺骗、价值漂移、自我保护、操控性推理等多种失效行为。

Conclusion: 当前先进的对齐策略仍有显著缺陷，模型的复杂推理能力反而常被攻击者作为突破口。作者提出了详尽的对话操控场景分类体系及自动化评测工具，为AI对齐技术的未来发展与安全性提出更高要求。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [51] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 本研究针对多语混用、文化丰富但资源匮乏语境下的情感分析，提出诊断性评估框架，揭示顶尖LLM在复杂场景下的优势和现有开源模型的不足，强调文化敏感评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统的自然语言处理(NLP)方法在情感分析中通常假设标签固定且情感表达具有普遍性，但在资源匮乏且文化语境复杂的环境下，这些方法面临巨大挑战。作者希望解决主流情感分析方法在文化敏感、多语代码混用语境中的局限性。

Method: 提出了一个将情感视为依赖情境、文化嵌入性构念的诊断性框架，通过人工标注数据、情感翻转的对照样本以及基于量表的解释性评价，分析大型语言模型（LLMs）在内罗毕青年健康小组中WhatsApp消息的情感推理过程，重点考察模型的可解释性、鲁棒性及与人类推理的一致性。

Result: 发现不同LLM推理质量存在显著差异：顶尖LLM表现出解释的稳定性，而开源模型在面对模糊或情感变化时常常表现不佳。模型在情感识别上的表现受文化语境影响显著。

Conclusion: 提出的基于社会科学测量思路的方法，实现了在复杂、真实语境下对LLM情感推理能力的有效诊断和比较。研究显示，复杂跨文化交流场景需考虑文化敏感性与推理能力，并对AI评价标准提出新要求。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [52] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 本文提出ReasoningGuard，通过分析注意力机制实时进行安全自省和路径采样，低成本提高大模型内容安全性，超过当前七种主流防护手段。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）尽管在需要推理的任务中表现出色，但在推理过程的中后期仍然容易生成有害内容。现有的防护方法通常依赖昂贵的微调和专家知识，难以大规模应用。

Method: 提出了ReasoningGuard，这是一种在推理模型推理时进行保护的方法。它通过分析模型内部的注意力机制，准确识别推理路径的关键节点，及时触发面向安全的自省步骤，并在解码阶段引入采样策略，以选择最优的推理路径，从而减少有害内容生成。该方法只需较小的额外推理开销，无需微调或额外知识。

Result: ReasoningGuard成功防御了包括针对推理过程的最新越狱攻击在内的三类安全攻击，对比七种已有保护机制表现更佳，实现了更高水平的安全性，同时有效避免了过度安全造成的问题。

Conclusion: ReasoningGuard作为无需微调、高效的推理时保护方案，在保障大型推理模型生成内容安全方面表现出色，具有良好的实用性和可扩展性。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [53] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 本研究探讨通过API调用大语言模型用于分层文本分类的效果，发现适当的提示策略能够在深层次结构上优于传统方法，但也会带来更高计算成本，提示准确性与资源消耗需权衡。


<details>
  <summary>Details</summary>
Motivation: 分层文本分类（HTC）由于数据稀缺和模型复杂性而面临挑战。本文旨在探索通过API访问的黑盒大语言模型（LLMs）在HTC中的可行性，作为传统需大量标注数据和算力支持的机器学习方法的替代方案。

Method: 评估了三种提示策略：直接叶子标签预测（DL）、直接层级标签预测（DH）和自顶向下多步层级标签预测（TMH），并在零样本和小样本下测试这些策略的准确性与成本效益。

Result: 在两个数据集上的实验显示，小样本下的分类准确率普遍高于零样本。对浅层次结构数据，传统机器学习模型表现更好，但在深层次结构数据上，LLM（特别是DH策略）优于传统模型。对于层次越深的标签结构，DH策略的API成本显著上升。

Conclusion: 黑盒大语言模型在HTC任务中表现出潜力，尤其适合深层次标签结构。但需权衡提示策略的性能提升与计算成本，合理策略选择尤为重要。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [54] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: 本文提出DP-GPT4MTS双提示大语言模型，将显式与文本提示结合，显著提升多模态时间序列预测准确性，超越现有算法，凸显文本上下文的重要性。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在各行业的战略规划和决策制定中至关重要。然而，传统模型通常只关注数值型时间序列数据，忽略了如事件、新闻等重要文本信息，影响了预测精度。现有的大语言模型虽然具备整合多模态数据的潜力，但单一提示词机制在处理带时间戳的文本语义时表现有限，易引入冗余信息，影响模型性能。

Method: 提出了一种创新的双提示语大语言模型框架DP-GPT4MTS（Dual-Prompt GPT2-base for Multimodal Time Series），结合显式提示（明确任务指令）和隐式文本提示（从带时间戳的数据中提取上下文语义嵌入）。显式提示由分词器生成，文本提示经过自注意力和前馈网络优化嵌入表达，旨在充分利用文本和数值数据。

Result: 在包含文本和数值信息的多种时间序列数据集上，DP-GPT4MTS模型的预测表现优于当前最先进的算法。

Conclusion: 通过引入双提示机制并有效整合文本上下文信息，能显著提升时间序列预测的准确性。模型设计为多模态数据融合提供了新思路，对实际应用具重要意义。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [55] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 作者提出了一种结合大语言模型与临床专家反馈的虚拟患者生成方法TalkDep，生成经过专业人士验证的多样化、真实的抑郁症患者模拟，有助于提升自动诊断系统效果。


<details>
  <summary>Details</summary>
Motivation: 由于心理健康服务需求激增，而用于培养临床专业人员的真实训练数据短缺，限制了抑郁症诊断的支持能力。现有虚拟患者方案难以生成符合临床、自然且多样的症状表现，因此亟需改进。

Method: 提出了一种基于先进语言模型并融合临床专家反馈的虚拟患者模拟流程TalkDep，通过使用精神科诊断标准、症状严重程度量表和上下文因素来指导模型生成多样化且真实的患者回应。

Result: 通过临床专业人士的全面评估，验证了该模拟患者方法的可靠性。

Conclusion: 该方法提供了可扩展、可适应的虚拟患者资源，可改善自动化抑郁症诊断系统的鲁棒性与泛化能力。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [56] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: 本文提出KVSink方法，基于深入分析注意力sink机制，能精准高效地保护LLMs推理中关键token，较传统方法表现更优，有效提升KV缓存量化下的模型性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对大型语言模型（LLMs）推理中的KV缓存进行量化，是提升推理效率、降低内存压力的重要手段。但部分研究指出，为保护注意力sink点，需对前几个token保持高精度，这一经验背后的原理以及其局限性尚未被深入理解。尤其是新发现：注意力sink点不只存在于初始tokens，仍缺乏系统性应对方法。

Method: 本文通过分析推理过程中注意力sink的产生机制和极端激活异常值在跨层传播中的作用，揭示KV缓存量化与sink保护的关系。在此基础上，提出了KVSink方法：一种基于有效预测sink token的通用机制，能高效识别并保护sink token，几乎无额外开销。

Result: 大量实验证明，KVSink在KV缓存量化过程中，比现有的Preserve-First-N (PFN)策略更有效地保护了注意力sink。同时，结合现有KVQuant方法，KVSink能进一步优化PPL（困惑度），降低对16位数值异常值的依赖。

Conclusion: KVSink方法显著提升了KV缓存量化下LLMs推理的稳定性和精度，实现了更智能的注意力sink保护，优于传统的前N token保护策略。

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [57] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: 本文提出了电商复杂用户意图基准ShoppingBench，任务难度显著提升，顶尖大模型表现不佳；同时提出能力蒸馏方法，显著提升小模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有电商基准只关注简单用户目标，无法满足现实复杂情境（如加券、控预算、比较多商品等），高维度复杂意图评测工具及高质量环境急缺。

Method: 1. 构建了涵盖多层次/更复杂意图的ShoppingBench基准和2.5M+真实商品的沙盒环境
2. 用状态最先进large language agents (如GPT-4.1)进行基准测试
3. 提出轨迹蒸馏策略和强化学习+监督微调，从大模型迁移能力到小模型，并检验其有效性

Result: 在ShoppingBench基准上，GPT-4.1等先进模型表现不佳（<50%成功率），反映出此任务难度极大。同时，新提出的轨迹蒸馏与结合增强学习/微调的方法，显著提升了小模型性能，使之接近大模型。

Conclusion: 文章提出的ShoppingBench显著提升了电商场景中对复杂用户目标的建模能力，并通过迁移和蒸馏技术成功将大型语言模型的能力有效压缩进小模型中，取得了和GPT-4.1相当的表现。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [58] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 作者发现并实验证明，攻击者通过极小的文本修改即可对GraphRAG知识图投毒，从而严重误导后续推理。同时，现有防御措施难以防范此类攻击，凸显该领域的巨大安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前流行的Graph-based Retrieval-Augmented Generation (GraphRAG)方法提升了大语言模型（LLMs）的准确性和可解释性，但其依赖于LLMs从原始文本中提取知识构建知识图谱，这一过程存在被恶意攻击者操纵的风险。论文关注这一潜在的安全隐患。

Method: 论文提出了两种知识投毒攻击（KPA）：一是Targeted KPA（TKPA），通过图论分析定位生成知识图谱中的脆弱节点，并借助LLM改写相关叙述，以精准操控特定问答任务的结果；二是Universal KPA（UKPA），利用指代、依存关系等语言特征，通过少量修改全局影响力词汇，破坏知识图谱的结构完整性。

Result: TKPA能以93.1%的高成功率精准影响问答结果，同时保持文本流畅自然。UKPA仅需修改0.05%的文本，就能使问答准确率从95%骤降至50%。实验还显示，现有防御方法难以检测上述攻击。

Conclusion: GraphRAG在知识抽取阶段存在显著的安全威胁，当前防御机制无法有效识别和防御知识投毒攻击。因此，保障GraphRAG安全性亟需更多研究。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [59] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 本文提出并实践了MedCheck评估框架，系统揭示了当前医疗LLM基准建设的诸多弊病，并为后续改进提供了切实可行的路径。


<details>
  <summary>Details</summary>
Motivation: 当前在医疗领域应用大型语言模型（LLMs）时，评价其表现的基准存在可靠性担忧，具体包括临床相关性不足、数据管理不健全及安全性评估指标缺失。作者希望解决这些长期存在的问题。

Method: 设计并提出了MedCheck——首个针对医疗基准的全生命周期评估框架，将基准的建设分为五个连续阶段，并提出了46项医疗场景下专门定制的评判标准。借助该框架，对53个医疗类LLM基准进行了实证系统评估。

Result: 分析发现医疗基准存在普遍的系统性问题，如与临床实践脱节、数据完整性危机（污染风险无法控制）、对模型安全与不确定性评价等关键维度的忽视。

Conclusion: MedCheck不仅能够帮助诊断现有医疗LLM基准存在的问题，还可以作为行动指南，推动医疗AI评价更标准化、可靠与透明。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [60] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 本研究提出用于生成文献综述的新句子修辞注释体系及基准，系统评测了37种大模型，发现微调和半合成数据能显著提升分类效果，推动小模型与开源模型进步。


<details>
  <summary>Details</summary>
Motivation: 以往研究显示，通过对论文中句子的修辞角色（如研究空白、结果、局限性、方法扩展等）进行注释，有助于提升AI分析科学文献的能力，尤其对高质量综述系统的开发潜力巨大。然而，要实现这一目标，需要定义合适的注释体系，并找到有效的大规模标注策略。

Method: 本研究提出了一种新颖的注释体系，专为支持文献综述生成设计，并对多种先进大语言模型（LLMs）在该体系下进行修辞角色分类的能力进行了全面评估。具体方法包括：构建跨学科基准数据集Sci-Sentence（包含700条专家人工标注句子和2240条LLMs自动标注句子），并使用零样本学习与微调方式，对37种不同规模和类型的LLMs进行分类性能评测。

Result: 微调后的当前主流LLMs在该任务上表现出色，F1分数可达96%以上；GPT-4o实现最佳性能，但部分轻量级开源模型同样表现优秀；用LLMs生成的半合成标注扩充训练数据，可以让小模型和部分开源模型大幅提升性能。

Conclusion: 新注释体系和基准推动了论文句子修辞角色分类领域的发展，验证了多种LLMs的有效性，并为小模型和开源模型提供了提升途径。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [61] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 本文针对当前LLM推理中奖励分配粗糙的问题，提出基于token熵的动态加权机制（GTPO和GRPO-S），实现更细粒度的奖励分配，从而显著提升模型推理能力，实验证实其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型（LLM）推理通常通过强化学习（如GRPO）增强，但由于对序列中所有token分配相同奖励，无法细粒度地判别长链推理任务中的贡献。现有机制在长链推理任务中导致奖励分配粗糙，降低了模型性能。

Method: 提出了动态熵加权（Dynamic Entropy Weighting）机制，用token的熵值指导奖励分配，实现细粒度信用分配。具体包括两种方法：（1）GTPO，对每个token根据熵值加权奖励，实现精细粒度的信用归因；（2）GRPO-S，对整个序列按token平均熵分配加权奖励。

Result: 实验表明，所提方法在强化长链推理能力上显著优于强基线DAPO，提升效果主要归因于所用的熵加权机制。

Conclusion: 通过引入基于熵的奖励权重，可以更高效地提升语言模型的深度推理能力，熵权重机制是性能增强的关键。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [62] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 本文提出以生成问题为核心的多模态推理新方法（CoQ），有效提升了模型在多模态任务中的推理与响应能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的推理能力通过链式思维和逐步解释等技术取得了很大进展，但在多模态环境（如视觉、音频或空间感知）下的应用仍然面临挑战。它们尚不能主动决定何时使用哪一种感官模态以应对复杂真实环境。

Method: 提出了“Chain of Questions（CoQ）”框架，这是一种以好奇心驱动的推理方法，促使多模态语言模型动态生成关于周围环境的有针对性问题。模型通过生成这些问题，主动选择和激活相应的感知模态，从而收集有效信息，用于推理与生成更准确的回答。方法在集成了WebGPT、ScienceQA、AVSD和ScanQA的新多模态基准数据集上进行评估。

Result: 实验显示，CoQ方法提升了多模态基础模型有效识别和整合关键信息的能力，提高了推理过程在多模态任务中的准确率、可解释性与推理一致性。

Conclusion: CoQ框架能够促进多模态语言模型更主动、更高效地整合多种感官信息，优化推理效果与任务表现。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [63] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 作者提出了一种基于RAG的两步事实核查流程，在FEVER 8竞赛中排名第一，并且在有限硬件条件下依然表现出色，易于本地部署。


<details>
  <summary>Details</summary>
Motivation: 为了解决信息过载时代事实核查的难题，并在有限硬件资源下实现高效、精准的事实核查。

Method: 提出了一种简洁的两步RAG（Retrieval-Augmented Generation）流程，对去年的方法进行了改进，支持本地化部署。

Result: 该系统在FEVER 8竞赛中获得第一名，并且在仅有单张NVidia A10 GPU、23GB显存和每条claim 60秒运行时间的条件下，仍达到了最先进的Ev2R指标细分下的事实核查性能。

Conclusion: 所提出的两步RAG流程不仅能获得高精度的事实核查结果，也易于部署于本地受限硬件环境中，兼顾准确率和实际应用能力。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [64] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 通过对肯塔基州16,656份事故叙述测试，微调版Transformer模型如RoBERTa能高效准确地自动识别交通二次事故，优于大语言模型和传统方法，适合本地隐私友好部署。大模型召回突出但推理开销大，中等规模模型在准确和效率间表现良好，为交通数据智能化处理提供可行优化方案。


<details>
  <summary>Details</summary>
Motivation: 提升道路交通事故数据质量对于交通安全与管理至关重要，而现有的二次事故识别主要依赖结构化数据，未充分利用事故叙述文本。本研究旨在通过先进的自然语言处理(NLP)技术，挖掘事故叙述，提高事故数据的完整性和准确性，助力更有效的二次事故识别。

Method: 以肯塔基州2015-2022年共16,656份人工审核的交通事故叙述为数据集，其中包含3,803例确认的二次事故。对比评估了三类模型：1) 零样本开源大语言模型（LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B）；2) 经过微调的transformers模型（BERT, DistilBERT, RoBERTa, XLNet, Longformer）；3) 传统的逻辑回归作为基线。以2015-2021年数据训练，2022年1,771条数据为测试集。

Result: 微调的transformer模型表现最佳，RoBERTa取得F1值0.90和准确率95%；零样本LLaMA3:70B获得可比的F1（0.86），但推理耗时139分钟，逻辑回归显著落后（F1:0.66）。部分LLMs如GEMMA3:27B召回率达0.94但运算耗时高；而中等规模LLMs（如DeepSeek-R1:32B）可匹敌大模型却大幅减少运行时间。

Conclusion: 微调Transformer模型（尤其是RoBERTa）可在交通事故文本挖掘中实现高准确率和高效率，适合本地化、隐私友好部署。大语言模型在召回率上具有潜力但推理代价大。中等规模模型实现了性能与效率的平衡。综合来看，细致权衡精度、效率与数据需求后，微调Transformer方案可为交通事故数据提升质量提供可复制路径。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [65] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 本文剖析了大型语言模型能力涌现的本质，认为其源于神经网络的复杂非线性动态过程，呼吁用复杂系统理论去理解和定义LLMs的能力边界和原理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在生成任务上的非凡表现引发了关于其能力本质的讨论，尤其是这些能力往往是“涌现”的、非直接训练所致。本论文关注于理解深度神经网络（DNNs）能力涌现的根本原因，及其与传统符号主义计算范式的本质差异。

Method: 作者通过理论分析与实证研究，探讨了神经网络的非线性、随机性如何导致系统宏观行为无法直接从微观神经元活动推导出来。着重分析了扩展定律（scaling laws）、grokking现象和模型能力的相变，并考察了常见指标、预训练损失阈值及上下文学习的争议。

Result: 研究结果表明，DNN的能力涌现不是简单由参数扩展引起，而是源于高度敏感的非线性系统的复杂动态。能力涌现类似于自然界物理、化学、生物中的复杂系统，其协同与相互作用产生超越个体的系统性能力。单靠现有指标难以捕捉本质。

Conclusion: 真正理解LLMs能力需要把DNN视为一种全新复杂动力系统，其受控于普适的涌现原理。未来研究应该转向分析系统内部的动力学变换和组分间协作机制，而不只是观察表面现象。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [66] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文发现人类在对话生成时会选择性关注有用信息，提出以此为基础的ASR评估新思路，有望提升当前口语对话系统的评测贴近实际应用。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）是口语对话系统（SDS）前端的关键环节，当前缺乏理解和评估ASR在对话生成中应具备的能力。受人类“选择性听取”能力的启发，作者希望找到更贴合实际对话场景的ASR评估方法。

Method: 通过实验对比人类在生成对话回复时的转录内容与标准参考转录，分析人类在对话生成中的选择性听取现象。

Result: 实验确认了人在生成对话回复时存在明显的选择性听取，关注的是对生成回复有价值的语音部分，与参考转录存在差异。

Conclusion: 基于实验结果，作者认为可以提出一种基于人类选择性听取的新型ASR评估方法，用于衡量ASR与人类在真实对话中转录能力的差距。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [67] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 通过提出预测置信度模型，精准判断对话预测是否可用，从而减少用户等待时间。


<details>
  <summary>Details</summary>
Motivation: 在语音对话系统中，用户等待系统响应的时间（用户感知延迟，UPL）较长，影响用户体验。为减少UPL，需在用户说话结束前预测完整发言，提前准备系统响应。

Method: 提出了预测置信度模型（PCM），通过估算预测的完整用户发言与真实用户发言的语义相似度，判断是否可以进行预取响应。

Result: 利用PCM根据预测和真实用户发言的差异进行了评估，验证了PCM对可否预取响应的判断能力。

Conclusion: 预测置信度模型（PCM）可以有效评估是否可以进行对话响应预取，从而有助于减少语音对话系统的用户感知延迟。

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [68] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 论文提出了结构化客服对话新任务与框架，利用LLM生成并重写数据，显著提升了大模型在客服领域的应答质量和策略执行能力。


<details>
  <summary>Details</summary>
Motivation: 客户支持不仅需要准确解决问题，还需要结构化和具备同理心的专业沟通，但现有数据集缺乏策略指导且真实客服数据难以获取和标注。

Method: 提出了Customer Support Conversation (CSC)新任务。基于COPC标准，定义了五个会话阶段和十二种支持策略，搭建了结构化的CSC框架。通过LLM重写并标注了1,855条真实客服对话，形成了CSConv评测数据集，并提出一种基于LLM角色扮演的对话生成方法，创造了RoleCS训练集。

Result: 在RoleCS上微调的大模型在CSConv数据集上的策略一致性和高质量应答能力显著提升，经人工评估也显示在问题解决效果上有明显提升。

Conclusion: 结构化的对话策略框架结合LLM角色扮演生成的数据能够有效提升大模型在客服场景下的应答质量与策略对齐能力。

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [69] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 本文提出一种新的数据及训练流程，有效提升大模型自动将数学自然语言转化为形式化语言的能力，并大幅刷新了相关任务的准确率纪录。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在自然语言到数学形式化（autoformalization）任务上准确率仍低，主要由于缺乏完善的形式化知识学习与自然语言对齐推理能力。

Method: 构建两个新数据集（涵盖大量形式化知识例子的distilled dataset和由专家模板引导的非正式到正式推理记录），并用SFT和RLVR对模型进行训练。

Result: 7B和32B参数规模模型均达到了优秀的综合表现，32B模型在FormalMATH-Lite（BEq@1 40.5%）和ProverBench（26.7%）上获得SOTA。

Conclusion: 提出的ThinkingF流程能有效提升大模型在数学自动形式化任务中的表现，达到新的SOTA水平。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [70] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 论文提出多种生成马来语数学题目的自动化流程，基于RAG方法并结合官方课程内容。实验结果深显RAG方法在课程对齐和事实有效性上优势明显，提供了实践性的EdTech生成与评测参考。


<details>
  <summary>Details</summary>
Motivation: 该论文关注马来西亚教育体系中对可扩展且高质量教育评估工具的迫切需求，尤其是在资源有限的语言环境（如马来语）下，亟需确保自动化生成内容的事实准确性与课程对齐。

Method: 提出并比较了四种递进式流程，利用OpenAI GPT-4o在马来语环境下生成初中一年级数学选择题。这些方法涵盖：非基础的直接提示（结构化和基础型）、以及两种信息检索增强生成（RAG）方法（一个基于LangChain框架，一个手动实现）。系统引用了官方课程文档和教师备课资料。评估方面，采用了双管齐下的自动化评测框架，利用语义文本相似度（STS）评价题目与课程计划（RPT）的对齐度，并用新颖的RAG-QA方法验证题目的上下文有效性。

Result: 实验结果显示，基于RAG的流程在生成课程对齐、高事实有效性的题目方面明显优于非基础提示方法。同时，论文对RAG框架实现的易用性和手动实现的细粒度控制进行了权衡分析。

Conclusion: 提出了一套可在资源有限语言中应用、对课程特定内容自动生成的经验证方法；创新性引入了RAG-QA联合评测技术，并为马来西亚及相似地区教育科技工具的实际部署和开发提供了实践性指导。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [71] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 本文提出了跨词语语义建模的新方法和语料，训练出能够高效表达多种词义语境的模型CALE，并在多项语义任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 词汇语义学关注单词在不同语境下的多重词义，以及不同词语义之间的关系。尽管上下文化语言模型如XL-LEXEME通过“语境中词语”任务提升词义表示，但其仅限于比较同一个词形的不同用法，难以捕捉词间的语义信息。因此，亟需一种能够扩展至词间语义关系的方法。

Method: 本文提出了一个扩展任务——概念区分（Concept Differentiation），不再局限于同一词形（lemma），而是关注不同词之间的关联，并基于SemCor构建了相关数据集。随后，作者将多个上下文化表示模型在该数据集上微调，获得了名为Concept-Aligned Embeddings（CALE）的新模型。

Result: CALE模型在多项词汇语义任务上的实验表现优异，取得了最佳性能。此外，CALE的微调显著改善了词向量（embedding）的空间分布，提升了语义泛化能力和灵活性。

Conclusion: 扩展后的任务和数据集有助于获得更广泛且准确的词义表示。本文提出的CALE模型在实际评测中表现突出，能够更好地捕捉和利用词汇之间丰富的语义信息。

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [72] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: 本文提出StyliTruth机制，通过分离和独立控制语言模型表征空间的风格与真实性子空间，显著提升了生成风格多样性同时减少了对答案真实性的损害，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在利用大语言模型（LLM）生成具有特定风格的回复时，现有的表示编辑方法常常导致风格和真实度之间的权衡关系，即风格越强烈，回答的正确性（真实性）就越容易受损。该问题被称为“风格化引发的真实性崩溃”。作者发现这是由于风格和真实相关表征在部分关键注意力头上潜在耦合造成的。

Method: 作者提出了一种名为StyliTruth的新机制。该方法通过正交消元（orthogonal deflation）过程，将模型表征空间中的风格相关和真实性相关子空间分离，实现两者的独立控制。同时，在每个子空间内设计了自适应、针对Token级别的引导向量，从而在生成过程中动态精准地控制风格和真实性，最大限度减少相互干扰。

Result: 在多种风格和语言下进行的实验和分析表明，该方法显著减少了风格化引发的真实性崩溃（即提升答案的真实性），并在风格保持和真实性平衡方面整体优于现有的推理期干预方法。

Conclusion: StyliTruth机制能够有效实现语言模型输出风格与真实性的协同控制，显著减弱了风格化带来的真实性损失，对大语言模型输出的可控性和可靠性提升有较大意义。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [73] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 作者发布了为期两年的真实临床多模态数据集C-MIND，分析不同模态对抑郁症诊断的贡献，发现大语言模型单独效果有限，引入临床知识后性能显著提升。该工作推动了真实世界的智能抑郁症评估研究。


<details>
  <summary>Details</summary>
Motivation: 当前自动化抑郁症评估主要依赖于有限且非临床验证数据，模型复杂性被优先考虑却忽略了真实世界效用，缺乏高质量临床数据和有效算法支撑实际诊断。

Method: 提出并发布C-MIND数据集——为期两年、基于真实医院访问的临床神经精神疾病多模态诊断数据集，包括音频、视频、转录和近红外光谱信号。通过数据集，分析诊断相关行为特征，训练多种传统模型评估不同任务和模态对诊断效果的贡献，探索大语言模型（LLMs）是否能具备类似临床医生的精神科判断能力，并通过引入临床知识引导提问以提升LLM在诊断中的表现。

Result: C-MIND数据集为抑郁症评估提供了真实可靠的临床基础。实验中发现，单一或组合模态对诊断效果存在不同影响，单靠LLM存在明显局限；通过引入临床专家知识可将LLM诊断的Macro-F1分数提升最高10%。

Conclusion: C-MIND数据集和配套方法为抑郁症自动诊断领域提供了关键基础建设，为今后以临床为本的高可信度智能诊断方法探索提供了数据与算法支持。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [74] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 多智能体合作比单智能体更能激发科研创意，前提是团队要有足够的知识深度和多样性，带有领导的结构最优。这对AI协同创新系统设计有启示意义。


<details>
  <summary>Details</summary>
Motivation: 现有AI科研辅助系统多以单智能体为主，知识和视角有限，导致创新性不足。现实科研往往通过多主体合作激发更多想法，因此作者希望探索多智能体结构是否能提升科研方案的创意和质量。

Method: 提出一个合作型多智能体科研提案生成框架，系统性对比不同团队配置（组员数量、有无领导、团队跨学科程度与资历）。采用智能体评分和人工复核等综合协议，评估方案的新颖性、战略视野和整合深度。

Result: 多智能体讨论显著优于单智能体基线。设定团队领导能将讨论高效转换成更具整合性和前瞻性的提案。认知多样性是质量提升的关键，但是团队必须包含有经验的成员，否则整体水平不能超过一个单一的资深智能体。

Conclusion: 多智能体合作能提高科研方案创新性，但需注重团队成员的专业深度和认知多样性。领导作用和团队结构对创造力结果有重要影响。该研究为AI协同创新系统设计提供了理论和实践依据。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [75] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 本文提出MASA方法，通过跨transformer层共享attention模块权重，实现66.7%参数压缩，性能基本无损，适用于自然语言及视觉任务。支持直接替换现有结构、易于训练，是高效参数共享的有力方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在人工智能应用中表现突出，但高计算和内存需求限制了其广泛部署。目前多数的模型压缩方法只关注transformer内部的单层优化，但transformer层之间存在明显的冗余，这一方向目前探索较少。本文作者受到CNN中字典学习思想的启发，试图通过跨层权重共享进一步提升压缩效率。

Method: 提出了一种结构化的跨层权重共享方法，称为MASA（Matrix Atom Sharing in Attention）。该方法将attention投影矩阵分解为共享的“字典基元”，每层的权重由这些共享基元线性组合而成，无需复杂蒸馏或模型结构更改，可直接作为现有attention模块的替代，并可用标准优化器进行训练。

Result: MASA在多个模型规模（1亿~7亿参数）上测试，与主流方法（grouped-query attention、低秩分解等）相比，在相似参数规模下获得更优的准确性和困惑度。此外，在视觉Transformer（ViT）上，MASA在减少66.7%注意力参数的情况下仍能维持分类与检测任务的性能。消融实验显示MASA对字典大小的鲁棒性，以及其在捕捉跨层统计规律上的有效性。

Conclusion: MASA为transformer参数高效压缩提供了可扩展的解决方案，无需牺牲模型性能。并且证明了该方法可以在预训练的LLMs上应用，实现参数减少同时保持性能不降。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [76] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了TURA框架，创新性地将RAG与工具调用机制结合，实现了静态与动态信息源的统一检索和处理，从而大幅提升AI搜索引擎的实时性、复杂查询处理和工业可用性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RAG（检索增强生成）的LLM搜索引擎在处理动态、实时和结构化复杂查询时存在很大局限性，无法满足工业界对于实时数据（如库存、票务等）的需求。学术界也大多聚焦于静态内容的优化，忽视了动态数据和复杂意图的处理需求。

Method: 提出TURA（Tool-Augmented Unified Retrieval Agent），一个集成三大组件的新型三阶段框架：(1) 意图感知检索模块，能够把复杂查询拆分并将信息源封装为MCP服务器；(2) 基于有向无环图的任务规划器，实现任务依赖的最优并行处理；(3) 轻量级的蒸馏代理执行器，实现高效工具调用。该方法结合了传统RAG与agentic工具调用，对静态与动态信息源均能检索和调用。

Result: TURA是首个系统性打通静态RAG与动态信息源架构的工业级AI搜索方案，已服务千万级用户，在满足大规模系统低延迟需求的同时，显著提升了实时和复杂查询的处理能力。

Conclusion: TURA为大规模AI搜索产品提供了一种高效、通用、可扩展的检索增强方案，弥补了传统RAG在实时和动态数据处理上的不足，推动搜索引擎向智能问答和动态响应能力的进化。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [77] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 本文针对低资源环境，评测了三种轻量级Transformer模型在Text-to-SQL任务中的表现，T5-Small效果最佳，显示紧凑Transformer在资源有限场景下的应用潜力，并提出了可复用的处理流程为后续优化提供支持。


<details>
  <summary>Details</summary>
Motivation: 让非专家用户能够用自然语言查询关系型数据库，提升教育和商业智能领域的数据访问便捷性。特别关注在资源受限条件下，如何利用轻量级Transformer模型实现Text-to-SQL。

Method: 评估了三种轻量级Transformer模型（T5-Small、BART-Small、GPT-2）在Spider数据集上的表现，开发了可复用、模型无关的处理流程，针对不同模型架构定制schema格式。训练迭代次数为1000到5000次，使用LFAcc、BLEU和Exact Match指标，在1000个测试样本上评估结果。

Result: T5-Small模型微调后在LFAcc上取得最高分（27.8%），优于BART-Small（23.98%）和GPT-2（20.1%），说明编码器-解码器结构在模式感知SQL生成中有优势。虽然资源受限影响了总体性能，但提出的pipeline具备良好可扩展性。

Conclusion: 紧凑型Transformer在资源稀缺环境下，为自然语言到SQL的转换提供了有效和可扩展的解决方案，并为未来引入更复杂schema联动或不同基础模型提供基础。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [78] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 本文提出P-Aligner模块，通过高质量的合成数据集UltraPrompt轻量预对齐指令，有效提升大语言模型多维表现，实验表明其显著优于基线算法并兼具效率与实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常因接受不完善指令（如缺乏上下文、歧义、不适当语气）而行为不理想，现有优化手段要么推理成本高，要么训练目标不明确，急需更高效、成本友好的指令对齐方法。

Method: 提出P-Aligner模块，训练于通过蒙特卡罗树搜索（MCTS）采集的UltraPrompt数据集，并通过多模型与多基准的实验进行评估，包括对比基线、数据质量、搜索策略以及部署效率等分析。

Result: P-Aligner在GPT-4-turbo和Gemma-2-SimPO模型上分别获得28.35%和8.69%的平均胜率提升。多项分析证明其在数据、策略、效率等方面均有效。

Conclusion: P-Aligner是一种高效且轻量的模块，能够在LLM解码前对指令进行预对齐，在提升模型安全、帮助性与诚实性等多维表现方面效果显著，并优于多个强基线。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [79] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: 提出一种新的指令跟随训练框架——IFDecorator，有效解决了RLVR方法中的效率低和奖励作弊问题，使大模型更准确理解和执行指令，性能优于同类模型并减少了投机取巧行为。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习方法（RLVR）虽然能提升大模型的指令跟随能力，但在训练中存在效率低和容易奖励作弊（模型学会投机取巧而非真正理解指令意图）的问题。

Method: 提出了Instruction Following Decorator（IFDecorator）框架。该方法主要包含：1）协作-对抗数据生成机制，逐步生成更有挑战性的指令和验证对；2）IntentCheck子模块，强制模型与意图对齐，防止模型偏离用户真实意图；3）trip wires机制，通过“陷阱指令”及时诊断和捕获模型奖励投机行为。

Result: 经过IFDecorator训练的Qwen2.5-32B-Instruct-IFDecorator模型，在IFEval基准测试上取得87.43%准确率，超过了包括GPT-4o等更大型的专有模型。该方法在FollowBench等任务上也有明显提升，并且有效降低了奖励投机/作弊率。

Conclusion: IFDecorator框架有效提升了LLM在指令跟随任务上的表现，同时显著抑制了奖励作弊行为，提高了训练的效率与模型实际对齐能力。相关模型、代码和数据也会开源以促进后续研究。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [80] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 本文系统性分析了74篇反击仇恨言论的NLP研究，发现近年来研究和受害社群需求间日益脱节。通过与5家NGO的合作，提出了重新聚焦利益相关方的建议，强调让受影响社群参与对提高研究实际效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 反击仇恨言论（Counterspeech）在NLP领域被视为应对网络仇恨的重要策略，早期多和非政府组织合作，但近期研究更偏向自动化流程且缺乏受影响社区的参与。本文旨在探究利益相关方参与对反击仇恨言论数据集和模型构建的影响。

Method: 对74篇反击仇恨言论相关的NLP研究进行系统性综述，并与5家专注于线上性别暴力的NGO合作，开展参与式案例研究，分析利益相关方参与的最佳实践。

Result: 发现当前NLP研究与受有害内容影响最深社区之间的脱节日益严重，且缺乏利益相关方的深度参与。通过与NGO合作，总结出对利益相关方有益的实践建议。

Conclusion: NLP反击仇恨言论研究亟需重视和整合受影响社群和利益相关方的专业经验与需求，从而提升干预的有效性和社会价值。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [81] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: 提出mmGRPO方法，将GRPO推广到多模块AI系统，对多种NLP任务提升准确率，并已开源实现。


<details>
  <summary>Details</summary>
Motivation: 模块化AI系统越来越普遍，但单一的GRPO优化无法适应不同模块和多样化轨迹的需求。作者希望实现一个能适用复杂、多模块系统的GRPO优化方式。

Method: 将传统的GRPO方法泛化为能够针对AI系统中不同模块、变长和可能中断的轨迹进行团组优化，并结合自动化的prompt优化。

Result: mmGRPO在分类、多跳搜索、隐私保护委托等任务上相较后训练的LM平均提升了11%的准确率，相较单纯的prompt优化提升5%。已在DSPy库中开源该优化器。

Conclusion: 提出了一种新的多模块GRPO方法（mmGRPO），能够有效提升模块化AI系统的性能，并已在DSPy工具中开源。

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [82] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: 长文本时，LLM常被无关信息干扰，推理能力下降。作者提出Sculptor系统，通过主动的上下文管理工具，明显缓解了这一问题并提升了推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长文本时，由于主动干扰，表现会大幅下降：上下文中无关的信息会干扰模型的推理与记忆回调。现有多数工作主要专注于通过外部记忆系统扩展LLM的能力。本文提出了一种互补思路，希望提升LLM的内部工作记忆管理能力。

Method: 提出Active Context Management (ACM，主动上下文管理)工具，赋予LLM主动“雕刻”其内部记忆的能力。具体实现名为Sculptor框架，包括三类工具：1. 上下文分片（fragmentation），2. 总结、隐藏与恢复（summary, hide and restore），3. 智能搜索（intelligent search）。上述工具让LLM像人类一样，选择性地关注有用信息、过滤干扰。

Result: 在信息稀疏基准（PI-LLM, NeedleBench Multi-Needle Reasoning）上的实验表明，Sculptor框架可显著提升LLM在长文本任务下的表现，即使没有为此专门训练，也能发挥LLMs的工具泛化能力。

Conclusion: 通过主动上下文管理，Sculptor不仅有效缓解了主动干扰问题，还为多种长文本推理任务提供了更可靠的认知基础。论文强调，相较于简单扩大上下文窗口，引入显式的上下文控制策略对大规模LLMs的稳健性更加关键。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [83] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 提出通用样本回放GeRe框架及TM损失方法，有效解决LLM灾难性遗忘和连续任务性能下降的问题，表现优于现有回放策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在持续学习过程中常常面临灾难性遗忘问题，表现为对通用能力与已学任务表现的显著下降，阻碍了人工通用智能的发展。

Method: 提出了General Sample Replay（GeRe）框架，使用常规预训练文本进行高效抗遗忘。同时，提出了一种基于激活状态约束与阈值边际（TM）损失的增强优化方法，在回放学习过程中保持神经激活状态的一致性。将TM与标签拟合、KL散度的logit模仿、L1/L2特征模仿等回放策略进行了系统对比。

Result: 实验证明，采用小规模、固定预收集的通用样本回放，可以同时保持模型通用能力和在各连续任务上的整体性能提升。TM损失法在不同策略下均表现出更好的性能与鲁棒性。

Conclusion: GeRe框架及TM损失为高效防遗忘和高性能的LLM持续学习提供了新的思路，推动未来LLM有效回放发展。代码和数据已经开源。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [84] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 该论文专注于在用户偏好标注文档有限的场景下实现LLM个性化，通过新数据集和创新方法FaST，在现有技术上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）驱动的对话助手通常采取统一适用的模式，难以满足个体用户的偏好。个人化需求日益增长，推动了模型个性化研究的兴起，但每位用户仅有少量偏好标注数据情境下的个性化对齐问题尚未被充分解决。

Method: 提出了个性化偏好对齐问题（PPALLI），并构建了两个公开数据集（DnD和ELIP）；对多种对齐技术进行了基准测试，并提出了一种参数高效、自动发现高层特征的模型对齐方法FaST。

Result: FaST方法在两个数据集上取得了最佳的整体性能，验证了高层特征自动发现与参数高效结合的有效性。

Conclusion: 个性化偏好对齐问题在数据有限设置下具有挑战性，提出的公开数据集及基准推动了该方向研究，FaST方法在实际场景下表现突出。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [85] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 论文系统剖析了推理大模型多跳问答时的错误，提出三维度新分类体系，发现多个被准确率指标掩盖的认知局限，并为模型改进提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 虽然推理型大模型在多步复杂任务中展现突破性进展，但其为何比通用语言模型更容易产生幻觉现象仍缺乏系统性理解。因此，本文旨在深入剖析推理模型在复杂问答中的错误机制，并为模型改进提供理论与实践依据。

Method: 系统分析当前语言模型在多跳问答任务中的推理失败，提出并应用了一个从'跳数'多样性、信息覆盖完整性和认知效率（三维）展开的精细化错误分类体系。通过人工标注与自动化指标相结合的方法，揭示了模型在推理任务中的深层错误特点。

Result: 实验发现，现有推理型语言模型在多跳问答中普遍存在与相关文献多样性、信息覆盖和思考过程非效率相关的复杂错误。这些细致错误通过新分类框架得以细致揭示，为未来模型优化指明了方向。

Conclusion: 该研究揭示了现有推理型语言模型在多跳问答任务中的复杂错误模式，这些错误常常被传统的准确率评价方式忽视。提出的新型错误分类体系有助于深入理解模型的认知局限，并为未来模型的推理能力提升提供了具体可行的指导。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [86] [Identity Testing for Stochastic Languages](https://arxiv.org/abs/2508.03826)
*Smayan Agarwal,Shobhit Singh,Aalok Thakkar*

Main category: cs.FL

TL;DR: 本文提出首个无限离散分布（尤其是随机语言）的恒等性检验算法与理论框架，借助有限状态机和有理随机语言的表示，采用截断+有限域检验求解问题，样本复杂度获得理论保证，并拓展了分布检验的应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统分布检验主要集中在有限域，但实际应用中如自然语言处理、生物信息学和程序分析等领域需要对无限组合结构（如字符串）上的分布进行检验，因此迫切需要理论与方法进展。

Method: 首先提出了一个多项式时间算法来验证有限状态机是否表示了随机语言；接着证明有理随机语言能够逼近任意概率分布；在此基础上，发展了基于截断的恒等性检验算法，并借助随机语言的指数衰减性质来控制截断误差，最终将问题转化为有限域上经典检验方法可解。

Result: 提出的检验算法可以以$\widetilde{\Theta}(\frac{\sqrt{n}}{\varepsilon^2} + \frac{n}{\log n})$的样本复杂度在已知与未知分布之间进行区分，这里$n$是截断后的支持集大小，并给出首个适用于无限离散分布的恒等性检验理论框架。

Conclusion: 论文首次建立了无限离散分布的恒等性检验理论体系，为概率形式化方法和结构化数据的统计分析开辟了新方向。

Abstract: Determining whether an unknown distribution matches a known reference is a
cornerstone problem in distributional analysis. While classical results
establish a rigorous framework in the case of distributions over finite
domains, real-world applications in computational linguistics, bioinformatics,
and program analysis demand testing over infinite combinatorial structures,
particularly strings. In this paper, we initiate the theoretical study of
identity testing for stochastic languages, bridging formal language theory with
modern distribution property testing.
  We first propose a polynomial-time algorithm to verify if a finite state
machine represents a stochastic language, and then prove that rational
stochastic languages can approximate an arbitrary probability distribution.
Building on these representations, we develop a truncation-based identity
testing algorithm that distinguishes between a known and an unknown
distributions with sample complexity $\widetilde{\Theta}\left(
\frac{\sqrt{n}}{\varepsilon^2} + \frac{n}{\log n} \right)$ where $n$ is the
size of the truncated support. Our approach leverages the exponential decay
inherent in rational stochastic languages to bound truncation error, then
applies classical finite-domain testers to the restricted problem.
  This work establishes the first identity testing framework for infinite
discrete distributions, opening new directions in probabilistic formal methods
and statistical analysis of structured data.

</details>


### [87] [Componentwise Automata Learning for System Integration (Extended Version)](https://arxiv.org/abs/2508.04458)
*Hiroya Fujinami,Masaki Waga,Jie An,Kohei Suenaga,Nayuta Yanagisawa,Hiroki Iseri,Ichiro Hasuo*

Main category: cs.FL

TL;DR: 本文创新性地提出了可直接访问组件的新型自动机学习问题设定，并针对组件冗余问题给出了解决算法，经过实验验证具备实际效果。


<details>
  <summary>Details</summary>
Motivation: 现有的组合式自动机学习通常只允许对整个黑盒系统进行查询，无法直接访问和利用系统内部的组件信息。随着越来越多系统由第三方和黑盒组件组合集成，如何在系统集成场景中高效学习系统行为成为亟需解决的问题。

Method: 提出了一种新的问题设定——“基于组件的自动机学习”，即学习者能够直接访问各黑盒组件。并针对组件冗余（即有些组件部分对系统行为无贡献）难题，提出了一种上下文相关的基于组件的学习算法来自动消除这类冗余。

Result: 通过实验验证了所提算法的实际应用价值，证明其在去除冗余和提升学习效率上的有效性与实用性。

Conclusion: 提出的基于组件的自动机学习和冗余消除新算法，有效扩展了组合式自动机学习在系统集成领域的适用性和效率。

Abstract: Compositional automata learning is attracting attention as an analysis
technique for complex black-box systems. It exploits a target system's internal
compositional structure to reduce complexity. In this paper, we identify system
integration -- the process of building a new system as a composite of
potentially third-party and black-box components -- as a new application domain
of compositional automata learning. Accordingly, we propose a new problem
setting, where the learner has direct access to black-box components. This is
in contrast with the usual problem settings of compositional learning, where
the target is a legacy black-box system and queries can only be made to the
whole system (but not to components). We call our problem componentwise
automata learning for distinction. We identify a challenge there called
component redundancies: some parts of components may not contribute to
system-level behaviors, and learning them incurs unnecessary effort. We
introduce a contextual componentwise learning algorithm that systematically
removes such redundancies. We experimentally evaluate our proposal and show its
practical relevance.

</details>
