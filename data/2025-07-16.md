<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.LO](#cs.LO) [Total: 9]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.DM](#cs.DM) [Total: 3]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: 论文用更简单的同态方法理论化和优化了泛化的数据流/流函数，展现了其在多种复杂系统中的应用可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 现有的数据流程序优化语义框架条件复杂，限制了表达能力和推理简便性。作者希望找到一种既简明又有表达力的理论基础。

Method: 作者通过定义幺半群同态，用理论证明和具体案例说明了方法的有效性，包括分区数据库连接、分层否定和TCP简化模型。

Result: 提出的同态框架能有效支持顺序组合、并行组合和反馈等操作，且通过数据库、逻辑和网络的例子验证了其实用性。

Conclusion: 论文证明了一类确定性流函数可以作为同态映射到某种“状态”幺半群中，并且该同态法则更简单，且依然支持丰富的等式推理和数据流程序优化。

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


### [2] [The downgrading semantics of memory safety](https://arxiv.org/abs/2507.11282)
*René Rydhof Hansen,Andreas Stenbæk Larsen,Aslan Askarov*

Main category: cs.PL

TL;DR: 作者提出用‘渐进式分配器独立性’理论，借助信息流安全技术，改善和精确定义内存安全的语义基础，促进了低层分配器和程序安全性的严格分析。


<details>
  <summary>Details</summary>
Motivation: 现有的内存安全定义多以消极方式（防止坏事发生）表述，既缺乏原则性，也未能明确捕捉分配器相关的安全语义，因此需要一种更加精细、语义明确的内存安全理论模型。

Method: 作者在一个底层语言模型下（具有malloc和free原语，基于扁平内存模型），将指针简单地视为整数，并基于信息流理论设计和形式化“渐进式分配器独立性”，通过区分资源耗尽和指针类型转换，结合信息流安全和降级机制，给出精确定义和技术处理方式。

Result: 文中提出的“渐进式分配器独立性”理论能够描述和验证多种分配器实现下程序的安全语义，突破了原有内存安全定义的局限，也为处理资源耗尽和指针转换等实际问题提供了分析框架。

Conclusion: 该工作提出了“渐进式分配器独立性”的概念，为捕捉基于分配器的内存安全提供了新的理论基础，从而能更准确地描述和分析不同分配器下的内存安全性。

Abstract: Memory safety is traditionally characterized in terms of bad things that
cannot happen, an approach that is often criticized as unprincipled. Prior work
suggest a connection between memory safety and noninterference, but no
satisfactory semantic notion of memory safety is currently known.
  This work proposes a notion of gradual allocator independence that accurately
captures many allocator-specific aspects of memory safety. We consider a
low-level language with access to an allocator that provides malloc and free
primitives in a flat memory model. Pointers are just integers, and as such it
is trivial to write memory-unsafe programs. The basic intuition of gradual
allocator independence is that of noninterference, namely that allocators must
not influence program execution. This intuition is refined in two important
ways to account for the allocators running out-of-memory and for programs to
have pointer-to-integer casts. The key insight of the definition is to treat
these extensions as forms of downgrading and give them satisfactory technical
treatment using the state-of-the-art information flow machinery.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: 本文提出了大规模多源代码数据集DroidCollection，并开发了跨语言、跨领域能力更强的AI生成代码检测器DroidDetect。实验验证新方法在对抗性样本和多样分布下性能优异，推动了AI代码检测研究的进步。


<details>
  <summary>Details</summary>
Motivation: 当前的代码检测器在面对不同编程领域和语言时泛化能力不足，且容易被针对性对抗样本绕过，缺乏大规模、丰富的公开数据集限制了相关方法的研究和评估。

Method: 作者构建了一个包含百余万代码样本、七种编程语言、43种AI模型生成输出、涵盖多个真实场景的开源代码数据集DroidCollection，并开发了基于多任务目标训练的编码器检测器DroidDetect。此外，作者还引入了对抗样本、度量学习和基于不确定性的重采样等方法提升检测效果。

Result: 实验发现，已有检测器在不同编程领域和语言下表现大幅下降，也容易被“人化”处理的输出绕过。而基于少量对抗样本训练后，检测器鲁棒性可显著提升；同时，度量学习与不确定性重采样进一步优化了检测性能。

Conclusion: 该研究为AI生成代码检测领域提供了最全面公开的数据集，并提出更具泛化性和鲁棒性的检测方法，为后续研究与评测提供了坚实基础。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [4] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: 本论文提出ARPaCCino系统，自动将自然语言政策需求转化为正式政策代码并验证、修正IaC配置，验证了其在多平台和小型模型情况下的有效性，推动政策即代码的自动化与普及。


<details>
  <summary>Details</summary>
Motivation: 当前政策即代码（PaC）在基础设施即代码（IaC）领域具有自动化管理安全和合规性的巨大潜力，但因政策语言复杂、配置容易出错等问题，实际应用受限。

Method: 提出ARPaCCino系统，结合大型语言模型（LLM）、检索增强生成（RAG）和基于工具的验证方法。系统接受自然语言描述的政策需求，自动生成Rego格式政策规则，检测和修正IaC配置，支持多种IaC框架。其架构模块化，可集成外部工具和知识库。

Result: 通过Terraform案例实验，该系统能有效生成语法和语义正确的政策，发现不合规配置并完成修正。即便采用较小、开源LLM模型，依然能达到较好效果。

Conclusion: ARPaCCino展示了代理型RAG架构在提升PaC自动化、可靠性和易用性方面的潜力，可适用于多种IaC场景和政策语言需求。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [5] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: 该论文针对语言模型在满足多个竞争要求时反复失败的问题，提出Meta Self-Refining 框架，通过元纠错层辅助主模型走出循环，有效提升解决效率。


<details>
  <summary>Details</summary>
Motivation: 面对在语言模型管道中满足多个相互竞争软约束时，传统机制常陷入效率低下的反复回退循环。现有手段难以自动平衡这些约束，因此需要一种更高层次的自我修正方法。

Method: 提出在LM管道中增加一个meta-corrective层。当系统检测到反复失败的震荡循环时，调用一个meta-repairer LM，分析整体回溯状态，并生成自我修正指令，引导主模型走出无效循环，达到最终成功输出。

Result: 实验证明，Meta Self-Refining 能够有效打破失败循环，使LM程序更加高效地满足复杂约束。

Conclusion: Meta Self-Refining 方法能够有效修复语言模型管道在处理相互竞争的软约束时产生的失败循环，提高模型执行效率。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [6] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: ToolRegistry让大语言模型用工具变得更简单高效，代码量少了，速度提了，还兼容主流协议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）应用越来越多地依赖外部工具，以扩展其超越文本生成的功能。然而，目前的工具集成方法存在分散、协议受限和实现复杂等问题，导致开发成本较高。

Method: 本文提出了ToolRegistry，这是一种与协议无关的工具管理库，通过统一接口简化工具注册、表示、执行和生命周期管理。

Result: 评估结果显示，ToolRegistry可将工具集成代码量减少60-80%，通过并发执行实现高达3.1倍的性能提升，并100%兼容OpenAI函数调用标准。实际案例研究显示，不同集成场景下开发效率和代码可维护性均有显著提升。

Conclusion: ToolRegistry显著降低了LLM应用工具集成的开发门槛，提高了效率和兼容性，是一个开源的统一工具管理库。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [7] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: 本文针对社交应用评论中的隐私类请求和BUG自动分类，提出了SENSOR工具和GRACE模型，实验在16000条评论上，准确率高达95%，显著助力开发者更高效挖掘和解决隐私相关问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体应用的广泛使用引发了严重的隐私担忧，用户评论中频繁反映这些问题。虽然这些评论为开发者提供了改进的线索，但由于评论量大且内容复杂，开发者难以手动识别和优先处理隐私相关事项。过去虽有研究尝试利用机器学习自动分类用户评论类别，但针对“隐私相关功能请求”、“隐私相关漏洞报告”或“与隐私无关”这几个细分类还缺乏深入研究。

Method: 本文提出了SENtinel SORt (SENSOR)自动在线注释工具，同时设计了GRACE注释模型。GRACE结合GRU（门控循环单元）、CBOW词嵌入和注意力机制，实现对约16000条热门社交应用用户评论的自动注释和分类。手工标注者对所有评论进行了分类，获得了高一致性（Cohen's Kappa=0.87），为机器学习模型提供了优质训练数据。

Result: GRACE模型在面临类别不平衡的情况下表现突出，获得了宏F1分数0.9434、宏ROC-AUC分数0.9934和准确率95.10%。SENSOR工具展现了辅助开发者从评论提取并处理隐私相关问题的巨大潜力。

Conclusion: SENSOR和GRACE大幅提升了自动化识别、分类和标注用户评价中隐私相关需求和问题的能力，有助于开发者更好地保护用户隐私、增强用户信任。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [8] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 作者发现直接预训练的大模型难以真正理解代码语义。通过专门针对代码理解任务的大规模微调，可以明显提升模型对代码深层语义的把握，显著改善代码调试与优化能力。实验中，微调后模型在相关理解任务上准确率整体提升，最高可达87%以上。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码生成和补全等任务上表现优异，但由于其预训练目标多为下一个Token预测，主要学习了代码的表层语法模式，并不一定能理解代码的深层语义。因此，它们在需要语义理解的任务（如代码调试和优化）上常常表现较差。为了解决这一问题，作者提出对模型进行专门针对于代码理解任务的微调。

Method: 作者采用了大规模数据集对不同尺寸的代码模型进行了微调，使模型更好地理解代码的语义，并在一系列代码理解任务（尤其是语义层面的任务）上进行评估，例如主观性分级任务。对比微调前后的性能变化，从而判断方法有效性。

Result: 通过微调后，所有模型在代码理解任务上的表现均有提升，尤以QWQ-32B模型为甚，其在主观性分级任务上的准确率从70%提升至83.47%。而经过DPO微调的Codestral-22B模型则在同一任务中获得了最高的87.66%微平均准确率。

Conclusion: 定向微调能显著提升大模型对代码深层语义的理解能力，超越仅凭语法特征获得的基础水平，对于提升调试、优化等需要综合理解的代码任务尤为有效。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [9] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: 提出了多轮、真实项目环境下自动化编程助理评测框架CodeAssistBench（CAB），发现现有大模型在真实复杂项目场景中的解决率远低于常规模块题，指出编程助手评测应进一步贴近实际开发环境。


<details>
  <summary>Details</summary>
Motivation: 现有的针对编程助手的大模型基准测试多聚焦在单轮代码生成任务，无法覆盖多轮交互、整体项目环境中的真实编程任务评估。近期的相关工作虽借助Stack Overflow数据拓展了测试场景，但仍局限于孤立的问题、人工筛选繁重且难以代表完整软件项目。

Method: 提出了CodeAssistBench（CAB）基准框架，能在真实项目环境下评测多轮编程助理，自动从GitHub相关issue生成大规模多语言数据集，并对完整代码库容器化，自动化评估模型在用户模拟操作下对大型实际代码库的支持能力。

Result: 利用CAB构建了覆盖七种编程语言、231个仓库、3,286个真实问题的测试集。测试显示主流LLMs在Stack Overflow单问上的成功率为70-83%，但在CAB中近期问题的解决率仅16.49%，反映目前模型在复杂、项目特定环境中能力差距巨大。

Conclusion: CAB有效填补了多轮、真实项目场景下编程助手能力评测的空白，揭示了当前大模型在复杂真实应用中的瓶颈和进步空间，推动更符合实际的软件开发AI评估标准发展。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [10] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: 该论文评估了JIT漏洞预测技术在真实大规模数据集上的效果，发现目前主流方法在极度不平衡的数据下表现极差，常见的不平衡应对手段也无效，强调需要开发专项应对策略和使用贴近实际环境的数据进行评估。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统越来越复杂，确保其质量面临重大挑战。及时漏洞预测（JIT-VP）作为一种主动预测安全风险的方法，在实际评估中常采用理想化数据集，这些数据集只包括引入漏洞和修复漏洞的提交，忽略了现实场景中更多的中性提交。研究动机在于揭示理想化评估与真实评估之间的差距，推动更可靠的安全评估方法发展。

Method: 本研究构建了包含超过100万次提交的FFmpeg和Linux内核的大规模公共数据集，评估八种主流JIT-VP技术在真实环境（包含漏洞相关和中性提交）下的表现，并尝试使用定制化损失函数、过采样与欠采样等常见方法缓解数据不平衡问题。

Result: 在真实条件下，JIT-VP技术的预测性能显著下降。例如，Linux项目的平均PR-AUC从0.805骤降至0.016，下降了98%。主因在于真实数据集中，漏洞引入型提交占比极低。常见的克服数据不平衡技术（如调整损失函数、过采样、欠采样）在JIT-VP场景下并未显著改善结果。

Conclusion: JIT-VP技术在真实、严重不平衡的场景下预测性能大幅下降，现有普遍数据不平衡处理技术难以缓解此问题。需发展面向领域的专门不平衡处理方法，并坚持在真实场景下进行评估。

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [11] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: 本研究开发了集成AI的Jira插件，用于自动检测和处理backlog中的任务冗余与不规范问题，实现了准确高效的backlog管理，显著提升了流程效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着产品待办事项（backlog）规模和复杂性持续增长，团队面临冗余、过时或描述不清任务堆积的问题，导致优先级排序和决策更加困难。研究动机在于寻找自动化手段以提升backlog管理的效率和效果。

Method: 采用设计科学（Design Science）方法，开发了一个Jira插件，通过将backlog议题嵌入向量数据库，用余弦相似度检测重复项，并利用GPT-4o模型自动提出合并、删除或新建议题的建议。

Result: AI辅助backlog梳理实现了100%的准确率，并将完成时间缩短了45%。该工具能够优化backlog精炼流程，提升用户体验。

Conclusion: 生成式AI可以在不降低准确性和透明度的前提下自动化敏捷项目中的backlog梳理，有助于提高决策效率和团队协作。

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [12] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: 本文通过国际研讨会总结敏捷开发领域研究与实际应用之间的差距及弥补策略，促进双方合作。


<details>
  <summary>Details</summary>
Motivation: 敏捷开发虽受欢迎，但其理论与实际应用之间存在显著鸿沟，需要加强研究与实践的结合。

Method: 组织并分析了首届国际研讨会的讨论成果，总结了主要主题与影响因素。

Result: 研讨会识别了导致研究与实践脱节的因素，提出了弥补该差距的策略，并指出了需要进一步研究的挑战。

Conclusion: 本文强调了研究与实践之间的差距，提出通过合作可以推动敏捷开发的发展。

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [13] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 作者实证分析了六种LLM在代码中软件库推荐的行为，发现它们偏向推荐第三方库但缺乏依赖使用支持，对开发者带来额外负担，指出需提升LLM依赖推荐的可用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着开发者越来越多地使用大型语言模型（LLM）辅助编程任务，有必要了解这些模型在推荐软件库时的表现及其影响。

Method: 作者对六个主流（包括专有和开源）LLM进行了实证研究，利用Stack Overflow上的实际Python编程问题对其进行测试，分析其推荐和引入的库类型、库的特性及推荐结果的可用性。

Result: 发现LLM更倾向于推荐第三方库而非标准库，且推荐的库通常是成熟、流行且许可宽松的。同时，存在可用性不足的问题：4.6%的推荐库因导入名与可安装包名不一致而无法自动解析，仅有两款模型会给出安装指导，导致依赖处理的负担落在用户身上。

Conclusion: LLM生成的代码虽然技术上有效，但在依赖使用时缺乏支持，影响了代码的可用性。研究为开发者和研究者提出了改进建议，呼吁增强LLM代码生成在软件依赖方面的可靠性和易用性。

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [14] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: 本文综述了自适应AI驱动的会话型智能体在软件开发中的应用及挑战，认为该技术可极大提升开发效率，但仍需关注隐私和伦理等问题。


<details>
  <summary>Details</summary>
Motivation: 随着对开发效率和协作能力的需求提升，越来越多的会话型智能体（如聊天机器人和虚拟助手）被用于软件开发场景。本论文旨在探究这些智能会话系统在开发流程中的作用与价值。

Method: 本文主要通过综述的方式，回顾会话型智能体（包括传统规则系统与自适应AI系统）的发展演进，分析当前先进AI驱动工具（如GitHub Copilot和Microsoft Teams bots），并讨论其集成到开发流程时面临的挑战，如数据隐私和伦理问题。

Result: 研究发现，自适应AI驱动的会话智能体能够提供更加动态、个性化和上下文感知的辅助，相较于传统的规则系统更具灵活性。尽管存在隐私、伦理等现实挑战，但这些工具已在提高开发效率与自动化方面展现了巨大潜力。

Conclusion: 自适应AI会话型智能体有望彻底革新软件开发流程，为开发者提供实时、定制化的支持，显著促进开发效率提升。未来应用前景广阔，但需持续关注其局限性与潜在风险。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [15] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: 大模型结合特定提示设计，可以高效且接近人工水平地评价提交信息质量，优于传统指标，是自动化评价的有力工具。


<details>
  <summary>Details</summary>
Motivation: 在软件开发过程中，提交信息对于记录和解释代码变更至关重要，但实际中提交信息的质量常常不高。自动生成提交信息的技术已经取得进展，然而对于自动生成的提交信息的评价仍然存在挑战，现有的参考型自动评价指标（如BLEU、ROUGE-L和METEOR）有显著局限性，迫使研究者依赖耗时的人为评估。

Method: 本文系统性地实验了不同提示策略和多种先进大模型（LLMs），采用Chain-of-Thought推理结合few-shot示范，探索LLMs作为提交信息自动评估工具的可行性，并将评价效果与传统自动评价指标进行对比。

Result: 实验发现，结合Chain-of-Thought推理和few-shot示范的LLMs在提交信息评估中表现出接近人工评估的能力，大幅度优于传统的自动评价指标，同时在可复现性、鲁棒性和公平性方面也表现良好，虽然存在一定的变异性。

Conclusion: LLMs可作为提交信息质量评价的有效自动化工具，在评价效果和可扩展性上为替代人工评价提供了新的可行途径。

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [16] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: 本文提出并实现了SWE-MERA数据集，从源头和流程上规避了SWE-bench等旧数据集的数据污染问题，并能动态持续更新，提升了对大模型评测的公正性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程领域应用迅速，但现有如SWE-bench数据集存在严重缺陷，包含解决方案泄漏和测试用例不足等污染问题，影响模型评测公正性。

Method: 提出SWE-MERA，一个动态、持续更新的基准数据集，通过自动化收集真实GitHub问题并进行严格质量验证，构建了可靠的数据收集与筛选流程，最大限度减少数据污染。

Result: 目前已筛选出大约10,000个潜在任务，300条样本已开放。采用Aider编码代理进行评估，结果显示SWE-MERA能有效区分不同大模型的能力，已对一批最新LLM进行了性能测试。

Conclusion: SWE-MERA比现有基准更可靠、实时且能规避污染问题，为评估LLM在软件工程中的能力提供了更高质量测试平台。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [17] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: 本文提出基于变形测试的MT4DP框架，能高效检测深度学习代码搜索模型中的数据投毒攻击，实验表现大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对深度学习代码搜索模型的数据投毒攻击检测方法效果有限，需要更有效的检测机制，以提升代码搜索模型的安全性。

Method: 提出了MT4DP框架，通过生成语义等价的变形关系（SE-MR），对查询和检索结果进行差异分析，从而检测数据投毒攻击。具体包括识别高频关键词、生成等价检索、重新排序及方差计算。

Result: MT4DP在平均F1分数上比最优基线方法高191%，平均精确率提高265%。

Conclusion: MT4DP显著提升了对深度学习代码搜索模型中数据投毒攻击的检测能力，在F1分数和精确率上均优于现有最佳方法。

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [18] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: 本文提出结合自动机学习与测试的新方法，生成简洁且信息量大的bug描述，并实验证明该方法有效提升了缺陷检测与理解效率。


<details>
  <summary>Details</summary>
Motivation: 调试复杂系统是一个重要但非常耗时的任务，因此需要更高效的方法帮助开发者理解和定位故障。

Method: 提出利用自动机学习和测试技术，并引入Failure Explanations (FE)、Eventual Failure Explanations (EFE) 和 Early Detection (ED) 概念，总结和提炼失败行为模式，剔除无关信息。

Result: 在多种测试模式和实际基准上评估了方法，能够生成简明和富有信息的错误描述，提升故障检测与理解效率。

Conclusion: 本文方法能有效生成紧凑且有意义的bug描述，为调试复杂系统带来便利，提高了bug检测和理解效率。

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [19] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 该论文通过Fisher精确检验提出了新的变异体杀死标准，解决了原有方法在测试集扩展时单调性丧失的问题，提高了深度神经网络测试有效性的评估质量。


<details>
  <summary>Details</summary>
Motivation: 评估深度神经网络测试集有效性需要强有力的标准，现有的DeepCrime方法利用统计方法衡量模型变异体间差异，但存在单调性问题，扩大测试集反而可能导致已判定被杀死的变异体恢复。为解决这一矛盾，作者提出了改进方法。

Method: 提出基于Fisher精确检验（Fisher exact test）的统计变异体杀死标准，新方法既保持统计严谨性，又保证了单调性。

Result: 新方法不仅保留了原有统计方法的严格性，而且有效解决了单调性问题，改进了变异体评估的公正性和稳定性。

Conclusion: 采用基于Fisher精确检验的新标准为神经网络变异体杀死判定带来了理论与实际的进步，有助于提高测试集评价的准确性和可靠性。

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [20] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: 本研究提出了MARAUS系统，在越南高校招生真实场景中实现了高效、低成本、高准确率的智能咨询，优于单一LLM方案，为低资源教育环境部署多智能体RAG系统提供了经验。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高等教育咨询领域虽有应用潜力，但现有方案多为原型或在合成测试下运行，缺乏真实环境的大规模实际部署。因此，迫切需要一种可在真实大学招生环境中部署的智能咨询系统。

Method: 提出并实现了MARAUS（多智能体与检索增强大学招生系统），结合了混合检索、多智能体编排与基于LLM的生成。与越南河内运输技术大学合作，进行了包括技术开发和真实环境评估的两阶段测试，共计处理6000多次用户交互。

Result: 系统在六类实际咨询任务中，平均准确率达到92%；幻觉率从传统LLM方案的15%降至1.45%；平均响应时间低于4秒；两周部署成本仅为11.58美元（使用GPT-4o mini）。

Conclusion: MARAUS在真实高校招生场景中显著提升了咨询自动化效率与准确性，成本低，响应快，展示了在低资源教育环境下推广智能RAG系统的巨大潜力。

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [21] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: 该论文提出利用通用大模型进行代码重构检测，开发RefModel工具，评估主流模型在Java及真实开源项目上的表现。结果显示大模型方案准确率高、泛化性强，使用简单，有望取代依赖复杂规则的传统工具。


<details>
  <summary>Details</summary>
Motivation: 现有的自动重构检测工具如ReExtractor+、RefactoringMiner、RefDiff依赖于复杂的规则定义和静态分析，难以扩展和推广到其他编程语言。为了解决这一局限，作者探索是否可以利用大模型来简化重构检测过程。

Method: 作者通过开发RefModel工具，将主流大模型Phi4-14B和Claude 3.5 Sonnet应用于858个人工生成的Java，涵盖典型重构类型。同时，进一步评估了Gemini 2.5 Pro和o4-mini-high在从四个开源项目抽取的44个真实重构实例上的表现，并与传统工具进行了比较。

Result: 大模型驱动的RefModel在某些情况下甚至优于传统工具。在真实项目场景中，Claude 3.5 Sonnet和Gemini 2.5 Pro联手检测出97%的重构，优于所有静态分析工具。此外，模型在Python和Golang上也展现良好泛化能力，并且能生成自然语言解释，仅需一句话定义重构类型。

Conclusion: 基于大模型的方法在重构检测任务上表现出较好竞争力，具有跨语言扩展性和简洁性，有望取代部分复杂的静态分析工具。

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [22] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: 本文通过对22位软件从业者的访谈，发现安全债务管理实践存在显著差异，强调在开发全流程中更好整合安全措施，并平衡进度与安全任务，从而改进安全防护。


<details>
  <summary>Details</summary>
Motivation: 随着软件和自动化依赖度提升，项目常因紧迫的交付期限、资源有限和功能优先于安全而导致不安全的编码实践，从而积累安全债务（SD）。当前学界尚缺乏开发实践者在真实环境下对安全债务认知、管理及沟通的实证研究。

Method: 本文采用定性实证研究方法，通过对来自不同角色、组织和国家的22名软件从业者进行半结构化访谈，分析其对安全债务的认知、行为、管理工具与策略及其团队内部和对决策者的沟通方式。

Result: 研究发现，不同从业者在处理安全债务时存在差异：部分以交付速度优先，部分则始终重视安全。团队在安全实践、缓解措施、资源和安全任务之间的平衡方面亦有改进空间。研究强调需在软件开发生命周期中强化安全实践、统一缓解策略并提升“保密性-完整性-可用性（CIA）”三元组的关注。

Conclusion: 现有实际开发中安全债务管理和沟通不够一致，需加强安全实践的全面整合和缓解措施的一致性，同时平衡安全、进度与资源，提升团队应对安全债务的整体能力。

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [$ε$-Distance via Lévy-Prokhorov Lifting](https://arxiv.org/abs/2507.10732)
*Josée Desharnais,Ana Sokolova*

Main category: cs.LO

TL;DR: 论文分析了基于ε-bisimulation的伪度量（ε-距离），证明其不仅易于理解和计算，还与范畴论中的函子和余代数结构密切相关，为概率过程间行为距离的度量提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 现有概率过程的伪度量主要基于Kantorovich距离，但存在对直观性和计算易性的需求。

Method: 分析ε-距离（ε-bisimulation）作为伪度量的理论属性，探究它是否具备最大不动点和函子结构，通过将Kantorovich距离替换为Lévy-Prokhorov距离提升函子，并刻画ε-耦合与ε-互模拟的余代数特性。

Result: 证明了ε-距离是某个泛函的最大不动点，并能提供一个函子，这个函子通过Lévy-Prokhorov距离实现提升；同时展现了ε-耦合与ε-互模拟的余代数描述。

Conclusion: ε-距离不仅具备理论和计算优势，还拥有良好的余代数和函子化结构，使其成为衡量概率过程行为距离的一种有力选择。

Abstract: The most studied and accepted pseudometric for probabilistic processes is one
based on the Kantorovich distance between distributions. It comes with many
theoretical and motivating results, in particular it is the fixpoint of a given
functional and defines a functor on (complete) pseudometric spaces.
  Other notions of behavioural pseudometrics have also been proposed, one of
them ($\epsilon$-distance) based on $\epsilon$-bisimulation.
$\epsilon$-Distance has the advantages that it is intuitively easy to
understand, it relates systems that are conceptually close (for example, an
imperfect implementation is close to its specification), and it comes equipped
with a natural notion of $\epsilon$-coupling. Finally, this distance is easy to
compute.
  We show that $\epsilon$-distance is also the greatest fixpoint of a
functional and provides a functor. The latter is obtained by replacing the
Kantorovich distance in the lifting functor with the L\'evy-Prokhorov distance.
In addition, we show that $\epsilon$-couplings and $\epsilon$-bisimulations
have an appealing coalgebraic characterization.

</details>


### [24] [Reasoning about Medical Triage Optimization with Logic Programming](https://arxiv.org/abs/2507.10781)
*Jaikrishna Manojkumar Patil,Adam Chapman,Richard Knuszka,John Chapman,Paulo Shakarian*

Main category: cs.LO

TL;DR: 本文提出基于逻辑编程的优化结果推理框架，在医疗后送场景中成功提升资源调度效率和可解释性，显著降低了伤亡率，具有较强的实用和推广价值。


<details>
  <summary>Details</summary>
Motivation: 在前线医疗后送（MEDEVAC）等任务关键性领域中，需实现资源的高效分配和决策支持，尤其是在“合适的病人、合适的平台、合适的护送、合适的时间、合适的目的地”原则下，传统优化方法难以同时兼顾灵活性、可解释性和操作性。因此，亟需新方法提升决策效率和质量。

Method: 提出一个逻辑编程框架，能够协调多种优化问题的变体，并对其结果进行推理，通过将优化解转化为逻辑事实，进一步进行符号推理，支持资源高效分配。该框架被集成于GuardianTwin决策支持系统中，用于前线医疗后送应用。

Result: 在实验中，该框架相较于标准基线方法，实现了平均35.75%的伤亡减少。同时，系统界面便于用户交互，提供可解释的决策分析，有助于提升关键时刻的决策能力。

Conclusion: 逻辑编程可作为模块化、可解释与实用的优化基础方法，在任务关键领域（如MEDEVAC）中有效提升资源分配效率和决策质量。该方法不仅优化了结果，还增强了系统的可解释性和用户体验。

Abstract: We present a logic programming framework that orchestrates multiple variants
of an optimization problem and reasons about their results to support
high-stakes medical decision-making. The logic programming layer coordinates
the construction and evaluation of multiple optimization formulations,
translating solutions into logical facts that support further symbolic
reasoning and ensure efficient resource allocation-specifically targeting the
"right patient, right platform, right escort, right time, right destination"
principle. This capability is integrated into GuardianTwin, a decision support
system for Forward Medical Evacuation (MEDEVAC), where rapid and explainable
resource allocation is critical. Through a series of experiments, our framework
demonstrates an average reduction in casualties by 35.75 % compared to standard
baselines. Additionally, we explore how users engage with the system via an
intuitive interface that delivers explainable insights, ultimately enhancing
decision-making in critical situations. This work demonstrates how logic
programming can serve as a foundation for modular, interpretable, and
operationally effective optimization in mission-critical domains.

</details>


### [25] [Execution and monitoring of HOA automata with HOAX](https://arxiv.org/abs/2507.11126)
*Luca Di Stefano*

Main category: cs.LO

TL;DR: 本文提出了Hoax工具，支持{omega}-自动机的高效运行监控，突破了HOA格式和不可监控性下的问题，并通过实验验证了实际效果。


<details>
  <summary>Details</summary>
Motivation: 目前缺少能够高效运行和监控各种（非奇偶）{omega}-自动机的工具，尤其是能支持HOA通用格式的工具。监控不可监控自动机和识别特殊前缀的问题尚未得到较好解决。

Method: 提出了一种新工具Hoax，利用trap sets理论，可以运行监控HOA格式的{omega}-自动机。对于不可监控性情况，工具也能识别出“ugly prefixes”，判断后续观察是否有意义。并通过与PyContract工具在实际场景中进行对比实验。

Result: Hoax工具能支持HOA格式下多种（非parity）接受条件的自动机运行监控，即使在不可完全监控情况下也能输出有用结果。在锁获取实际场景中效果与同类工具PyContract进行了比较。

Conclusion: Hoax是一个高效、可配置、支持HOA格式和多种接受条件的{omega}-自动机运行监控工具，解决了部分不可监控性问题，并在实际应用中表现优良。

Abstract: We present a tool called Hoax for the execution of {\omega}-automata
expressed in the popular HOA format. The tool leverages the notion of trap sets
to enable runtime monitoring of any (non-parity) acceptance condition supported
by the format. When the automaton is not monitorable, the tool may still be
able to recognise so-called ugly prefixes, and determine that no further
observation will ever lead to a conclusive verdict. The tool is open-source and
highly configurable. We present its formal foundations, its design, and compare
it against the trace analyser PyContract on a lock acquisition scenario.

</details>


### [26] [Interpolation and Quantifiers in Ortholattices](https://arxiv.org/abs/2507.11141)
*Simon Guilloud,Sankalp Gambhir,Viktor Kunčak*

Main category: cs.LO

TL;DR: 本文探究了orthologic中量词和插值的性质，证明了插值总存在且可高效计算，补足了该逻辑在算法应用中的基础。


<details>
  <summary>Details</summary>
Motivation: 研究量词和插值性质在orthologic（一种非分配性经典逻辑弱化）的表现。这种逻辑对公式有效性来说与经典逻辑一致，但有更高效率的判定算法。

Method: 提出了一个基于序列的带量词orthologic证明系统，并证明该系统对于所有完备ortholattice是可靠且完备的。

Result: 证明了orthologic不支持普遍的量词消去，但总能在orthologic中构造插值项，并且给出了高效计算插值项的算法。

Conclusion: 该研究推进了orthologic在理论和算法验证中的应用，特别是在判定结果不可达场景的高效性方面有应用前景。

Abstract: We study quantifiers and interpolation properties in \emph{orthologic}, a
non-distributive weakening of classical logic that is sound for formula
validity with respect to classical logic, yet has a quadratic-time decision
procedure. We present a sequent-based proof system for quantified orthologic,
which we prove sound and complete for the class of all complete ortholattices.
We show that orthologic does not admit quantifier elimination in general.
Despite that, we show that interpolants always exist in orthologic. We give an
algorithm to compute interpolants efficiently. We expect our result to be
useful to quickly establish unreachability as a component of verification
algorithms.

</details>


### [27] [LISA -- A Modern Proof System](https://arxiv.org/abs/2507.11167)
*Simon Guilloud,Sankalp Gambhir,Viktor Kunčak*

Main category: cs.LO

TL;DR: LISA是一个用Scala开发的面向一阶逻辑与集合论的高效化证明助手，结合命题证明自动缩简与集合论形式化实践，为友好和高效证明构造提供平台。


<details>
  <summary>Details</summary>
Motivation: 现有通用证明助手系统多关注逻辑通用性与实用性间的折衷，如何兼顾高效自动检查、灵活定理抽象、命题顺序无关处理及用户友好构造工具仍是挑战。LISA旨在结合效率、友好性、灵活性，并便于形式化集合论等数学领域。

Method: 开发了一套以一阶逻辑等式及示意性谓词符号、函数符号为核的多项式时间证明核，采用正交格公理，提升连接词顺序无关性和命题推理效率；配套特定领域语言(Domain-Specific Language)及Scala工具链，支持开发与使用证明战术（如针对命题永真假言的证明生成器）。

Result: LISA证明系统已实现示意性一阶逻辑与集合论初步形式化（覆盖康托定理），支持高效命题证明生成与缩简，核查多项式时间内完成，实践展示语言抽象与自动化效率相兼容。

Conclusion: LISA系统实现了面向示意性一阶逻辑和公理集合论的证明与证明辅助，具备多项式时间证明检查、支持不展开证明的定理，支持谓词定义及唯一存在证明。LISA语言与Scala集成，证明构造友好，并在集合论（如康托定理）上有初步形式化成果。

Abstract: We present LISA, a proof system and proof assistant for constructing proofs
in schematic first-order logic and axiomatic set theory. The logical kernel of
the system is a proof checker for first-order logic with equality and schematic
predicate and function symbols. It implements polynomial-time proof checking
and uses the axioms of ortholattices (which implies the irrelevance of the
order of conjuncts and disjuncts and additional propositional laws). The kernel
supports the notion of theorems (whose proofs are not expanded), as well as
definitions of predicate symbols and objects whose unique existence is proven.
A domain-specific language enables construction of proofs and development of
proof tactics with user-friendly tools and presentation, while remaining within
the general-purpose language, Scala. We describe the LISA proof system and
illustrate the flavour and the level of abstraction of proofs written in LISA.
This includes a proof-generating tactic for propositional tautologies,
leveraging the ortholattice properties to reduce the size of proofs. We also
present early formalization of set theory in LISA, including Cantor's theorem.

</details>


### [28] [Cancellative Convex Semilattices](https://arxiv.org/abs/2507.11186)
*Ana Sokolova,Harald Woracek*

Main category: cs.LO

TL;DR: 本文将经典凸代数的可约性刻画推广到了带半格结构的凸半格，给出了其可约等价于Riesz空间中的凸子集的同构定理，对概率与非确定性领域的代数基础研究有重要意义。


<details>
  <summary>Details</summary>
Motivation: 凸半格（convex semilattice）作为同时具备凸代数和半格结构、并且满足分配律的代数结构，近年来因其在概率和非确定性领域的应用受到了关注。尤其，这些代数结构在分布子单子（distributions monad）非空有限生成凸子集的Eilenberg-Moore代数中具有重要作用。

Method: 作者首先回顾了凸代数可约性（cancellativity）由Stone和Kneser的经典刻画，然后推广这一思路，通过数学证明，将同类问题扩展到凸半格，具体地，研究凸半格的可约性条件，并寻找与向量空间中的凸子集的类似刻画。

Result: 证明了一个与Stone/Kneser定理类似的结论：一个凸半格可约当且仅当其同构于某个Riesz空间（即格序向量空间）中的凸子集，操作为标准的凸半格运算。

Conclusion: 该工作扩展了对凸半格结构的理解，特别是将既有关于凸代数的分类理论推广到了包含半格结构的更一般情形，丰富了概率和非确定性理论中的代数工具箱。

Abstract: Convex semilattices are algebras that are at the same time a convex algebra
and a semilattice, together with a distributivity axiom. These algebras have
attracted some attention in the last years as suitable algebras for probability
and nondeterminism, in particular by being the Eilenberg-Moore algebras of the
nonempty finitely-generated convex subsets of the distributions monad.
  A convex semilattice is cancellative if the underlying convex algebra is
cancellative. Cancellative convex algebras have been characterized by M. H.
Stone and by H. Kneser: A convex algebra is cancellative if and only if it is
isomorphic to a convex subset of a vector space (with canonical convex algebra
operations).
  We prove an analogous theorem for convex semilattices: A convex semilattice
is cancellative if and only if it is isomorphic to a convex subset of a Riesz
space, i.e., a lattice-ordered vector space (with canonical convex semilattice
operations).

</details>


### [29] [Complexity of some modal logics of density (extended version)](https://arxiv.org/abs/2507.11238)
*Philippe Balbiani,Olivier Gasquet*

Main category: cs.LO

TL;DR: 该文将单模密度逻辑的可满足性复杂度定为EXPTIME，双模弱密度逻辑定为PSPACE，并给出相应证明方法。


<details>
  <summary>Details</summary>
Motivation: 研究模态逻辑中的密度及弱密度逻辑的可满足性问题，以确定这些逻辑的复杂度归属。

Method: 采用选择性过滤（selective filtration）方法分析单模密度逻辑；采用类似表方法（tableau-like approach）分析双模弱密度逻辑。

Result: 证明了单模密度逻辑的可满足性问题属于EXPTIME复杂度类，双模弱密度逻辑的可满足性问题属于PSPACE复杂度类。

Conclusion: 通过不同技术手段界定了两种相关逻辑可满足性问题的复杂度，提高了对模态逻辑复杂性分类的理解。

Abstract: By using a selective filtration argument, we prove that the satisfiability
problem of the unimodal logic of density is in $EXPTIME$. By using a
tableau-like approach, we prove that the satisfiability problem of the bimodal
logic of weak density is in $PSPACE$.

</details>


### [30] [Path-filtration for modal logics applied to revisiting quasi-dense logics](https://arxiv.org/abs/2507.11258)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 纠正了已有文献的致命错误，创新性地采用路径过滤法，首次将quasi-dense模态逻辑的判定复杂度上界收敛至NEXPTIME。


<details>
  <summary>Details</summary>
Motivation: 先前LICS'24会议论文声称解决了quasi-dense模态逻辑判定性及EXPSPACE上界，但证明存在重大漏洞，导致该问题悬而未决。

Method: 引入了一种针对quasi-dense模态逻辑判定性的基于路径的典型模型过滤新方法，较之前复杂的不可修正证明更为简单直接。

Result: 作者给出了正确有效的证明方法，使该问题归入NEXPTIME复杂度类别。

Conclusion: 本文提出了一种新的基于典型模型路径的过滤方法变体，证明了quasi-dense模态逻辑的判定性问题，并将复杂度上界改进为NEXPTIME。

Abstract: In https://arxiv.org/pdf/2405.10094 (also published at LICS'24 conference),
Lyon and Ostropolski-Nalewaja answer the question of the decidability of
quasi-dense modallogics, and give an upper bound in EXPSPACE. Unfortunately,
their intricate proof contains a major flaw that cannot be fixed, leaving the
question wide open. In this paper we provide a correct and rather simple and
direct proof of it by introducing a new variant of the well-know filtration
method based on paths in a canonical model and improve the hypothetical
membership to membership NEXPTIME.

</details>


### [31] [SC-TPTP: An Extension of the TPTP Derivation Format for Sequent-Based Calculus](https://arxiv.org/abs/2507.11349)
*Julie Cailler,Simon Guilloud*

Main category: cs.LO

TL;DR: 论文提出了SC-TPTP格式，扩展并细化了TPTP推理格式，实现了一阶逻辑证明在不同证明工具间的格式迁移和协作，并开发了支持SC-TPTP的各类工具。


<details>
  <summary>Details</summary>
Motivation: 现有各自动定理证明系统之间的证明迁移存在格式障碍，尤其是从一阶自动定理证明器（ATP）到交互式定理证明器（ITP）的迁移难度较大。

Method: 提出了一种对现有TPTP推理格式的扩展——SC-TPTP，采用过度规范化的方法，专注于序列式形式体系，提升了细节表达和格式兼容性。还实现了相关工具库，支持格式的解析、重构、打印与验证，并可导出至Coq文件。

Result: SC-TPTP格式能详细、准确地描述一阶逻辑证明，兼容现有多个工具（尤其是分析树策略），并已应用于使Lisa证明助理能调用Goéland自动定理证明器。所开发的工具支持对SC-TPTP证明的各项操作，包括与Coq的交互。

Conclusion: SC-TPTP为不同证明系统间的证明迁移和协作提供了详细、一致的格式基础，有助于自动与交互式定理证明工具的集成与互操作。

Abstract: Motivated by the transfer of proofs between proof systems, and in particular
from first order automated theorem provers (ATPs) to interactive theorem
provers (ITPs), we specify an extension of the TPTP derivation text format to
describe proofs in first-order logic: SC-TPTP. To avoid multiplication of
standards, our proposed format over-specifies the TPTP derivation format by
focusing on sequent formalisms. By doing so, it provides a high level of
detail, is faithful to mathematical tradition, and cover multiple existing
tools and in particular tableaux-based strategies. We make use of this format
to allow the Lisa proof assistant to query the Go\'eland automated theorem
prover, and implement a library of tools able to parse, print and check SC-TPTP
proofs, export them into Coq files, and rebuild low-level proof steps from
advanced ones.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 本文提出的AI系统结合事实核查与评论互动，有效提升了辨别和纠正YouTube错误信息的能力，为网络平台健康生态提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，错误信息经常通过像YouTube这样的平台迅速传播，威胁网络信息的真实性。现有方法大多只专注于事实核查，而缺乏对用户互动和错误叙事挑战的主动性。本文旨在探索更高效的AI介入方式来解决这一问题。

Method: 本文提出了一个AI驱动的系统，包含Truth Sleuth和Trend Bender两个主要代理。Truth Sleuth负责从YouTube视频中提取声明，利用RAG（检索增强生成）方法结合多种权威信息源进行事实核查并输出报告。Trend Bender则参考事实核查报告和相关语料，生成富有洞见与说服力的评论，推动评论区的理性讨论，并通过自我评估循环不断优化输出。

Result: 系统通过基准数据集和YouTube真实环境下验证，展示了较高的事实核查准确性和用户参与度，证实了AI干预在打击错误信息与促进理性讨论方面的潜力。

Conclusion: AI驱动的多代理系统不仅能够高效核查YouTube视频中的信息真实性，还可主动引导用户积极参与，挑战误导叙事，从而帮助构建更健康的信息生态。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [33] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: 本论文提出并实现了一个可完全本地离线运行的智能手机心理支持App（EmoSApp），通过专门微调和量化的大语言模型，在保障隐私和低资源设备下，展现了良好的对话与推理能力，在人工和标准测试中均表现优异，为AI驱动心理健康服务指明方向。


<details>
  <summary>Details</summary>
Motivation: 精神健康与个人福祉密切相关。现有数字平台虽能拓展心理健康支持，但存在用户可及性差、网络连接受限、数据隐私等问题。因此，研究亟需一种可离线、基于智能手机的解决方案。

Method: 作者提出EmoSApp：一个完全离线运行的智能手机对话式心理健康支持App。其基于大语言模型（LLMs），通过Torchtune和Executorch对LLaMA-3.2-1B-Instruct模型进行微调、量化，并在资源受限设备本地部署，推理全在手机端完成。模型在14582条心理健康QA对及多轮对话数据上微调，结合定性人工评估及九项标准推理和常识基准测试。

Result: EmoSApp在学生群体中的人工评估显示，其能连贯、富有同理心地回应，维持互动对话，并对用户心理问题给出相关建议。在九项常识和推理基准测试中，经过微调和量化的模型在低资源环境下表现出较好效果。

Conclusion: EmoSApp通过设备本地化推理和领域专用适配，在实现隐私安全、无网络、便携化心理健康支持方面提供了创新蓝图，对未来AI驱动心理健康解决方案的研发具指导意义。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [34] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 作者构建了一套本地运行、注重隐私的开源文本处理工具链，能将敏感、异构的法律文档匿名化并标准化为可用于嵌入分析的摘要，支持大规模、自动化的内容分析，为社会科学和公共健康等领域的文本数据利用打开新局面。


<details>
  <summary>Details</summary>
Motivation: 法律、医疗等领域的非结构化文本数据虽然丰富，但受制于隐私保护和异构性难以用于大规模研究。亟需有效的自动化处理工具以突破上述瓶颈。

Method: 提出并实现了一套基于本地开源大模型的模块化处理工具链；包括LLM提示词标准化、摘要、英译，结合LLM、命名实体识别和规则的方法实现匿名化，并通过嵌入向量实现文档分析。

Result: 在处理10,842份瑞典法庭决定（LVM法案下）时，工具链能有效剥除个人身份信息并保留语义内容，经过人工与自动审核验证通过；进一步将嵌入向量应用于预测模型，展现了半自动化内容分析的规模化能力。

Conclusion: 本论文提出的模块化工具链能够有效保障隐私，促进对敏感文档的大规模分析应用，有助于开启相关领域的新研究可能性。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [35] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 该论文基于XAI文献，提出了一个专为自然语言解释（NLEs）设计的三维分类体系，旨在为AI治理中的可解释性与透明性建立系统分析和提升工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，自然语言解释成为阐释AI模型行为的关键方式，而目前对NLEs特性及治理影响缺乏系统梳理和标准化框架。

Method: 作者借鉴了可解释性人工智能（XAI）领域的既有成果，构建出一个针对NLEs的三维分类法，包括：1）语境（任务、数据、受众和目标），2）生成与展现（生成方式、输入、交互性、输出、形式），3）评价（内容、展现、以用户为中心的属性及评价环境）。

Result: 提出并详细描述了一个面向NLEs的新型XAI三维分类法，为相关利益相关者提供了实践和设计的理论工具框架。

Conclusion: 该论文提出了一个更新的可解释性人工智能（XAI）分类法，适用于基于提示的自然语言解释（NLEs），以支持AI系统行为的结构化治理和验证。该分类法有助于研究者、审计者和政策制定者更好地设计和优化透明的AI系统。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [36] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: 本文提出了AutoRAG-LoRA框架，利用LoRA适配器、KL正则和自动化检索机制，大幅降低了LLM幻觉，提升了模型输出的事实准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常出现“幻觉”（即事实不准确），影响其在真实场景下的应用与可信度。需要有效且高效的方法减少LLM的幻觉问题。

Method: 提出AutoRAG-LoRA模块化框架，结合检索增强生成（RAG）、LoRA轻量适配器和KL正则化训练。包含自动化提示重写、混合检索、低秩适配器微调，以及结合分类器与自评的幻觉检测模块，可反馈纠正模型输出，提高事实一致性。

Result: AutoRAG-LoRA框架显著减少大语言模型的事实漂移（幻觉），同时保持模型的效率和模块化特性。

Conclusion: 通过引入模块化、可反馈的自适应架构和训练机制，AutoRAG-LoRA有效减轻了LLM的幻觉问题，为真实应用提供更高可信度的输出。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [37] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 本文指出，LLM输出如果能仿效人类真实且个性化地表达不确定性，将有助于提升信息可信度。作者系统梳理了人类不确定性交流的研究，分析了现有模型在表达不确定性时的缺陷与偏见，并提出加强“拟人”特征作为未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着人类用户越来越多地依赖与大型语言模型（LLMs）的自然语言交互，LLMs输出内容时常表现出高度自信，即使其准确性存疑，导致用户对其信任度和合法性感受到挑战，因此有必要让模型向用户表达自身的不确定性，以促进人机合作并降低潜在风险。

Method: 本文对人类不确定性交流的研究进行了全面回顾，梳理了当前相关研究，并对语言模型在表达不确定性时可能出现的数据偏见进行了实证分析。

Result: 研究分析揭示，当前自然语言处理领域在处理不确定性表达时往往忽略了人类语言中表达不确定性的细致差异和隐藏的偏见。

Conclusion: 作者提出“拟人不确定性（anthropomimetic uncertainty）”，主张LLM等模型的可信和直观的不确定性表达应仿效人类沟通方式，兼具语言真实性与用户个性化，并对未来NLP领域在人机不确定性交流方面的研究方向进行了拆解与展望。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [38] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: 本文提出了一种快速、无需扰动的LLM解释方法PLEX，与LIME/SHAP效果一致或更优，大幅提升了效率，非常适合大模型的解释场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在文本分类上表现优异，但由于其复杂性，难以解释模型的预测依据。现有主流解释方法如LIME和SHAP依赖大量扰动生成，计算负担极重，尤其在LLM上表现尤甚。因此，亟需一种高效、低开销的本地可解释方法。

Method: 提出了一种扰动免疫的本地解释方法PLEX，通过提取LLM的上下文嵌入并用“孪生网络”训练来拟合特征重要性，训练完成后可直接对新样本解释，无需进一步扰动，显著提升效率。

Result: 在四项文本分类任务（情感、假新闻、COVID-19假新闻、抑郁）上，PLEX与LIME、SHAP解释结果有92%以上一致性，并能准确识别关键特征词，对模型表现有相似影响。部分情况下，PLEX对特征影响的捕捉能力优于现有方法。在效率上，解释速度和计算消耗分别缩减2和4个数量级。

Conclusion: PLEX无需扰动即可高效准确实现LLM文本分类的本地解释，大幅提升解释速度和资源利用，表现优于或媲美现有方法。适合实际大模型应用场景。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [39] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 本研究显示，LLMs在输出中会自然形成与心理学理论一致的情感层次结构，但存在针对边缘群体的系统性错误。通过借鉴认知理论，可提升模型评价和伦理治理。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在对话智能体中的应用增多，理解其对用户情感状态的建模方式对于模型的伦理和有效部署至关重要。作者欲探究LLMs情感识别能力及其偏见。

Method: 以心理学情感轮理论为基础，分析语言模型输出中情感状态之间的概率依赖关系，并结合人类研究比较LLMs的情感识别结构和偏差。

Result: LLMs能够自发形成与人类心理学模型一致的分层情感树，较大的模型构建出的情感层次更为复杂。但在不同社会经济群体、尤其交叉的边缘群体中，存在系统性情感识别偏差。人类实验也发现了相似的社会感知反映。作者认为，基于认知理论可以改进模型评价方法。

Conclusion: 大型语言模型（LLMs）在情感建模方面能够自然形成接近心理学理论的情感层次结构，且具有人类认知偏见。其行为反映了社会认知和感知偏差，这对于模型评估和伦理部署具有重要启示。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [40] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 本文针对性贩运检测的ASW广告复杂文本，提出高效自定义Transformer模型，在多个指标上优于BERT等主流模型，推动了ASW文本分析的发展，并可在多任务和实际场景下应用。


<details>
  <summary>Details</summary>
Motivation: 打击性交易和性贩运时，成人服务网站（ASW）因被用作广告受害者的平台，因此对其数据分析变得重要。然而，ASW广告文本因大量使用表情符号、语法差及故意混淆，导致文本分析极具挑战性。

Method: 本文综合评估了针对ASW文本分析的多种语言建模方法，包括简单的信息检索法、预训练Transformer模型以及自定义Transformer模型。实验培训自定义Transformer模型，并与BERT-base、RoBERTa和ModernBERT等主流编码型Transformer进行了比较。

Result: 自定义Transformer模型在准确率、召回率、F1分数和ROC AUC等指标上全面优于上述主流模型，并能在普通消费级硬件上高效推理。

Conclusion: 本文提出的自定义Transformer模型代表了ASW文本分析的重大进步，能被用于分解ASW图结构、聚类广告文本、及理解非法语境下的表情符号应用等任务，并有潜力在相关领域深入应用和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [41] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练语言模型将文本信息嵌入属性图，在不改变图结构的前提下，显著提升了节点分类和关系预测等任务的表现及分析可解释性。


<details>
  <summary>Details</summary>
Motivation: 带有标签的属性图经常包含丰富的文本属性，如果能有效利用，这些信息将有助于提升分析任务的效果。

Method: 利用预训练文本嵌入模型，将节点与边上的文本属性进行嵌入，然后将语言模型生成的语义信息无缝集成到属性图分析流程中，无需改变原有图结构。

Result: 所提出方法能够在节点分类、关系预测等下游任务中显著提升模型的语境理解能力和分析的准确性、可解释性。

Conclusion: 将文本语义有效嵌入属性图，显著提升了相关分析任务的表现和可解释性，验证了语言模型嵌入在图分析中的有效性。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [42] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出并构建了首个针对科研论文示意图理解的多模态问答基准MISS-QA，评测了18种前沿模型，发现它们与人类专家在该任务上仍有显著差距，并分析了模型的具体错误及改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在科学文献的示意图理解和信息检索任务上缺乏系统性评价，缺少专门针对该任务的基准数据集，亟需衡量和推动模型在领域科学文献理解上的进展。

Method: 构建了MISS-QA基准数据集，包含1,500个人工标注的样例，覆盖465篇科学论文。通过评测18种前沿多模态基础模型在该基准上的表现，并分析其在无法回答的问题上的表现和错误类型。

Result: MISS-QA推动了多模态模型在科学文献领域的系统评测，揭示了当下模型在示意图理解及复杂信息合成方面的不足，并通过详细的误差分析为模型提升提供了方向和建议。

Conclusion: 当前多模态基础模型在解释科研论文中的示意图并回答相关问题的能力，与人类专家存在显著差距，这揭示了模型理解多模态科学文献的不足之处。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [43] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 作者分析了Parler上的1.1亿条帖子，发现仇恨言论获得的社会认可（如点赞）并不会促使同一用户短期或长期内产生更多仇恨发言，甚至在有些情况下呈负相关。因此，网络仇恨与社会认可的关系在小众平台上可能并不如既有理论所说的那样简单。


<details>
  <summary>Details</summary>
Motivation: 研究者希望探讨网络仇恨言论是否会因为获得其他用户的社会认可（如“点赞”或“支持”）而加剧，从而验证和拓展Walther的线上仇恨社会认同理论的核心假设。

Method: 作者利用Parler这一小众社交媒体平台（2018-2021年）的1.1亿条帖子数据，统计分析了仇恨言论获得的“点赞数”与后续仇恨言论产出的关系，关注短期（下一条、接下来一周）和长期（三个月、六个月）两个层面的关联。

Result: 研究发现，在个人层面上，仇恨帖子获得的社会认可（如点赞数）与下一次发帖及之后各时间段的仇恨言论产出之间并无显著相关性。另外，整体上在单个帖子层面观察到社会认可与仇恨言论产出呈现负相关关系，但在其它时间区间则关系复杂、结果不一。

Conclusion: 该研究未能证实在小众社交平台Parler上，社会认可会推动个体发布更多或更极端的仇恨言论。社会认可对网络仇恨的促发机制在不同社交媒体平台上可能大不相同。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [44] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 本文系统评估了16款LLM在司法场景下的公平性，通过新框架和大数据集揭示模型普遍存在严重司法不公、偏见和结果不均衡。作者还开源了评测工具和数据集，以推动未来研究和改进。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在影响权利与公平的高风险领域被广泛应用，其司法公正性与社会正义影响尚缺乏深入研究。本文旨在系统性地评估LLMs在司法领域的公平性，确保这些模型具备可信赖性。

Method: 基于司法公正理论，作者构建了一套完备的LLM公平性评测框架，涵盖65个标签和161个对应值，并据此编制包含177,100个案件事实的数据集JudiFair。还提出了三项评估指标：不一致性、偏见、不均衡不准确性，同时开发用于评估多模型整体公平性的方法。

Result: 实验涉及16款LLM模型，结果显示这些模型存在普遍且严重的不一致性、偏见和不均衡不准确性，尤其在人口统计学标签上偏见更为显著，而在实质性标签上的偏见略低于程序性标签。发现不一致性越高，偏见反而减少；但更高的准确性会加剧偏见。参数温度调整可影响公平性，但模型规模、发布时间、国家来源对司法公平无显著影响。

Conclusion: LLMs在司法场景下公正性堪忧，表现出广泛的偏见、不一致和结果不均衡。相关公开数据和工具包将推动此领域的后续研究与改进。研究为审慎应用LLMs于高风险决策领域提供了重要参考。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [45] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文构建了包含主观与客观风格相似性评价的数据集，发现主观风格相似性更能反映用户偏好，并强调两种评估方式的区别。


<details>
  <summary>Details</summary>
Motivation: 之前研究指出用户与系统之间的风格相似性可能提升用户体验，但主观与客观相似性的区分常被忽略。为此，需深入探究二者对用户偏好的影响差异。

Method: 构建了一个包含用户偏好、用户主观风格相似性评价与第三方客观风格相似性标注的对话数据集，对数据进行相关性分析。

Result: 实验发现主观风格相似性与用户偏好高度相关，而用户主观测量与第三方客观测量存在显著的评判差异，强调了评估方式的差异性。

Conclusion: 主观风格相似性与用户偏好有较强的正相关性，且主观与客观风格相似性存在明显差异，因此在对话风格相似性与用户偏好关系的研究中，需区分主观与客观评估。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [46] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 本文针对韩语大模型难以处理同音异义词的挑战，提出HanjaBridge注入汉字语义，并引入知识蒸馏防止遗忘，显著提升了韩语及中-韩跨语义理解，且无需增加推理时间开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在低资源语言（如韩语）上表现较差，部分原因是存在同音异义词等语义歧义问题，且韩文表音文字难以区分这类词。

Method: 提出HanjaBridge方法，将所有可能的汉字候选作为同形异义词的语义注入模型，通过持续预训练框架引导模型学会基于上下文消歧。同时，配合token级知识蒸馏防止灾难性遗忘。

Result: HanjaBridge显著提升了韩语理解能力，在KoBALT基准上实现了21%的相对提升。通过汉字强化韩中语义对齐，带来显著跨语言迁移优势。而且即使推理时不增加汉字，性能提升仍然可保留，无需额外推理开销。

Conclusion: HanjaBridge为韩语中的语义歧义（如同音异义词）提供有效消歧方案，显著提升大型语言模型在韩语和跨中韩场景下的泛化能力，兼顾实用性和效率。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [47] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 本文评估了主流大型语言模型在故事类比任务上的推理能力，发现它们在一定程度上接近人类，但推理分布和表现仍有差距，大模型与显式提示能改善性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能在类比问题上提取相似性，但缺乏稳健的人类式推理能力。本研究旨在更细致地比较LLM与人类在类比推理中的表现差异，理解其作为人类推理模型的潜力。

Method: 采用故事类比任务，利用句子嵌入方法评估LLM对类比源文本-目标文本相似性与源文本-干扰文本差异性的表达能力；并通过显式提示让模型解释类比来评价推理表现；对不同模型参数规模（8B与70B）和主流架构（GPT-4、LLaMA3）进行实验比较，并细致分析个体类比项的推理能力。

Result: LLM对故事类比具有一定的语义理解和区分类比的能力，显式提示能提升类比解释的表现，且更大规模和更先进的模型表现更优，但模型推理分布与人类仍不完全一致。

Conclusion: 大型语言模型（LLMs）在类比推理任务上的表现与人类有一定相似之处，在某些维度上已接近人类认知能力，但仍存在与人类推理差异。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [48] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: 作者通过提示工程让多种LLM自动检测对话中的抑郁症状，并获得了eRisk 2025挑战赛第二名。方法基于BDI-II标准，借助一致性评估，在无标签环境下表现优秀。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的发展，其在情感和心理健康检测领域的潜力日益受到关注。尤其是在对话场景中自动检测抑郁症状，可以辅助心理健康早期干预。因此，作者团队参加了eRisk 2025关于会话性抑郁症检测的挑战，希望探索如何用LLM实现有效检测。

Method: 作者采用了提示工程的方法，让多种LLM基于BDI-II（Beck抑郁量表）标准自动评估对话内容，并生成结构化JSON输出数据。由于没有真实标签，团队通过模型间一致性和内部一致性对结果进行评估。

Result: 团队最优模型在官方排行榜上取得了第二名，DCHR=0.50，ADODL=0.89，ASHR=0.27。

Conclusion: 通过合理的提示工程，LLM能较好地对会话内容进行抑郁症状检测。团队方法在无标签场景下，借助模型间一致性等方式实现了效果评估，并取得了优异成绩。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [49] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型进行手语生成的新方法，将手语作为自然语言处理，通过微调和分步提示，提升了模型在手语与口语规则对齐和生成任务上的效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在许多AI任务中表现优异，但由于手语表达的复杂性和独特性，其在手语生成领域的应用仍然有限。

Method: 将手语视为另一种自然语言，通过微调大语言模型，使其学习文本与手语之间的对应关系，并采用分步提示策略从大语言模型中提取内在的手语知识，支撑学习与生成过程。

Result: 在How2Sign和Phoenix14T数据集上的实验结果证明，所提出的方法能有效提升大语言模型在手语生成任务上的表现，促进手语与口语之间的知识对齐。

Conclusion: 本文的方法能够有效利用大语言模型的手语知识和推理能力，实现手语和口语在分布与语法规则上的对齐。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [50] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种面向多语言推特性别歧视检测的高效微调方法，通过在Llama 3.1 8B上实施层级化LoRA，对所有线性层进行适配。方法利用多语言统一训练，使单模型具备跨语种能力，仅用极少可训练参数，实现性能提升、资源消耗极低，并在各项任务上均表现优良。


<details>
  <summary>Details</summary>
Motivation: 目前在推特等社交平台上，性别歧视性文本自动检测是一个重要且具有挑战性的问题，尤其是在多语言环境下。现有模型在效率、泛化能力、跨语言适应性方面存在不足。

Method: 采用Llama 3.1 8B基础模型，并基于层级化的Low-Rank Adaptation（LoRA）进行微调，通过条件适配器路由明确建模标签间的层级依赖。不同于以往LoRA只作用于注意力层，这里扩展到所有线性变换层。分别针对三个子任务（性别歧视二分类、意图检测、多标签分类）训练单独的LoRA适配器，利用统一的多语言训练，充分发挥Llama 3.1的双语能力。同时，采用了参数高效的训练范式（仅1.67%的可训练参数）和极简的数据预处理。

Result: 多语言训练实现了跨语言迁移，相较于单语训练F1提升1.7-2.4%；训练时间缩短75%，模型存储减少98%；在各子任务上获得了具有竞争力的表现，如性别歧视二分类ICM-Hard F1为0.6774，意图检测F1为0.4991，多标签分类F1为0.6519。

Conclusion: 通过层级化适配以及参数高效的训练方法，在无需复杂预处理和集成的前提下，实现了强大的多语言性别歧视检测效果，并大幅提升了训练和部署效率。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [51] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2通过证据摘要、答案重述、量化优化和最新LM集成，取得了效率与准确性的双提升，在AVeriTeC公开任务中表现优异，并已开源。


<details>
  <summary>Details</summary>
Motivation: FEVER-25的AVeriTeC任务鼓励开发更高效、准确的事实核查系统。HerO 2旨在克服现有系统在证据质量、计算开销和模型性能上的瓶颈，推动事实核查技术实用化。

Method: HerO 2在HerO模型基础上进行增强，具体做法包括：通过文档摘要与答案重述提升证据质量；通过模型量化在计算受限环境下优化真实性预测能力；集成更新后的语言模型主干提升系统整体表现。

Result: HerO 2在排行榜中获得第二名，并在前三名系统中实现了最短运行时间，兼具高效率和高准确性。

Conclusion: HerO 2有效提升了事实核查中对证据处理的质量、真实性判断的准确性和系统运行的效率，显示出在实际应用场景推广的潜力。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [52] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 论文提出了韩语新闻立场检测数据集K-News-Stance和JoA-ICL模型，通过分段立场预测和聚合方法，提高了长篇新闻的立场检测准确率，有助于减少信息茧房、提升观点多样性、揭示媒体偏见。


<details>
  <summary>Details</summary>
Motivation: 随着在线新闻消费的增长，个性化推荐系统变得重要，但这些系统可能强化“信息茧房”和政治极化，因为缺乏多样化观点。立场检测（stance detection）可以通过实现观点感知推荐和数据驱动的媒体偏见分析来缓解这一问题。然而，现有研究多集中在短文本和高资源语言，缺乏对长文本和低资源语言的研究。

Method: 提出了K-News-Stance，这是第一个针对韩语、面向新闻文章级立场检测的数据集，包含2000篇带有文章级和段落级（19650段）立场标注的新闻，覆盖47个社会议题。同时，本文提出了一种新的Journalism-guided Agentic In-Context Learning ( 93JoA-ICL 94)方法，利用语言模型对新闻结构内关键段落（如导语、引用等）进行分段立场预测，并聚合为整体文章立场。

Result: 实验表明，JoA-ICL方法优于现有立场检测模型，能够更好捕捉长新闻文档的总体立场。同时有两个案例研究，说明该方法在提升新闻推荐多元化和分析媒体偏见方面具有应用价值。

Conclusion: 本研究提出了首个韩语新闻文章级立场检测数据集，并创新性地提出了基于语言模型和分段预测聚合的JoA-ICL方法，有效提升了长文本立场识别的准确性，并对新闻推荐与媒体偏见分析有积极作用。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [53] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 本研究提出了利用大语言模型优化的临床文本处理方法，显著提升了心血管疾病风险预测的准确性和实用性，为智能化临床决策支持提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有心血管疾病（CVD）风险预测模型主要利用结构化数据，然而非结构化的临床文本包含了许多宝贵的早期风险指标，值得深入挖掘。

Method: 提出了一种新的大语言模型（LLM）增强的临床自然语言处理（NLP）方法，利用领域自适应LLM对自由文本中的症状进行抽取、上下文推理和相关性分析。该方法结合了心血管专用微调、基于提示的推理、实体感知推理，并采用提示工程和混合规则验证来应对情境幻觉和时序模糊等挑战。

Result: 在MIMIC-III和CARDIO-NLP两个数据集上，该方法在精准度、召回率、F1值和AUROC方面表现更好。同时，经临床专家评估，具有较高的临床相关性（kappa=0.82）。

Conclusion: LLM增强的NLP方法能够有效提升临床决策支持系统（CDSS）的能力，推进早期预警和将患者叙述信息转化为可操作风险评估。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [54] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 本研究开发了一种融合多种BERT模型的混合情感分析框架，针对孟加拉国七月革命期间的社交媒体评论，显著提升了孟加拉语情感识别的准确率，验证了深度学习和机器学习联合方法在低资源语言社会情绪分析中的强大能力。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国的七月革命期间，社交媒体在放大公众情绪和形成社会舆论中发挥了重要作用，研究者希望通过技术手段揭示社交媒体评论中蕴含的群体观点。由于孟加拉语资源有限，开发适用于该语言的情感分析方法尤为迫切。

Method: 研究提出了一种混合式基于transformer的情感分析框架，融合了BanglaBERT、mBERT、XLM-RoBERTa与新的XMB-BERT模型，结合主成分分析（PCA）进行降维处理，并通过十一种传统及先进的机器学习分类器进行情感识别。

Result: 采用混合XMB-BERT与投票分类器的方法，在4200条孟加拉语社交媒体评论数据集上，实现了83.7%的高准确率，优于其他模型组合。

Conclusion: 该方法有效提升了低资源语言（如孟加拉语）社交情感分析的准确性，凸显了机器学习技术在重大社会事件中的应用潜力。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [55] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 跨境金融实体识别需处理复杂匹配需求。大语言模型在此任务中，比传统方法更准确且误报率低，推荐在西班牙金融系统进行集成应用。


<details>
  <summary>Details</summary>
Motivation: 由于跨境金融活动日益频繁，金融系统中准确识别和分类外国实体变得极为重要，这是风险管理、合规和防止金融不当行为的必要前提。但实体匹配过程复杂，传统算法在面对语言差异、特殊字符、实体变动等实际难题时表现不佳。

Method: 对比了传统匹配算法（如Jaccard、余弦、Levenshtein距离）、基于Hugging Face的LLMs以及基于接口的LLMs（如微软Copilot、阿里巴巴Qwen 2.5），在包含65个葡萄牙公司案例的数据集上进行性能测试。

Result: 传统方法准确率超过92%，但误报率高（20-40%）；接口型LLMs表现更优，准确率高于93%，F1分数超96%，误报率更低（40-80%）。

Conclusion: 接口型的大语言模型在跨境实体识别和匹配任务中，相较于传统算法表现更为优越，能够更好处理上下文、缩写和法律实体变动等实际应用难题。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [56] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出针对扩散式大语言模型的首个系统性越狱攻击DIJA，展现出dLLMs标准对齐机制下重大安全弱点，实验大幅超越现有攻击手段，强调需重新思考此类模型的安全对策。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）因其高效推理和交互性，成为自回归LLM的有力替代者。然而，现有对齐机制无法防范针对上下文感知、掩码输入的对抗性提示，暴露出新的安全隐患。

Method: 提出DIJA框架，通过构造交错的掩码-文本对抗提示，利用dLLM的双向建模和并行解码机制系统性地发起越狱攻击，揭示dLLM在标准对齐机制下的特有脆弱性。

Result: DIJA方法大幅优于现有越狱攻击方式，在多个基准测试中表现突出：Dream-Instruct实现高达100%关键词ASR，JailbreakBench评估器ASR较强基线提升78.5%，StrongREJECT分数高37.7点，无需修改或隐藏越狱提示中的有害内容。

Conclusion: dLLM架构存在被忽视的重大安全漏洞，标准对齐机制在对抗有害提示时往往失效，亟需针对该模型类型重新设计对齐和安全机制。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [57] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 大语言模型不仅可被单一后门攻击，同时能隐藏多个互不干扰的后门，风险比预期更严重。作者提出了一种简便又有效的检测和移除多后门的方法，对提升LLMs安全有重要意义。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明大语言模型（LLMs）易受数据投毒攻击，但大多数研究仅关注单一触发短语的攻击有效性，对于触发机制及多触发器相互作用缺乏深入理解。本文提出希望揭示多触发机制在同一模型中如何工作并带来什么风险。

Method: 提出一个用于研究LLMs中数据投毒的分析框架。通过实验证明多个不同的后门触发器可以共存于单一模型，彼此之间不会干扰。此外，研究了高嵌入相似度的多个触发器在被修改或间隔较长token时依然能有效激活后门行为。最后，提出了一种基于分层权重差异分析的后处理恢复方法，仅对部分模型参数进行重训练，以去除后门行为。

Result: （1）证实多个后门触发器可同时存在且互不影响，带来更广泛和持久的安全威胁；（2）使用高嵌入相似度的多触发器，可实现更稳健的后门激活；（3）提出的分层权重差异分析重训练方法能高效去除多重触发后门，且只需很少参数更新。

Conclusion: LLMs面临比想象中更广泛且持久的数据投毒威胁。攻击者可嵌入多个互不干扰的后门触发器，增强攻击隐蔽性和鲁棒性。文中提出的恢复方法为抵御多触发器投毒攻击提供了有效手段。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [58] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 多模态多语言推理挑战中，通过结合多个轻量版视觉-语言模型和严格提示策略，显著优于端到端大模型，系统在权威比赛中多语种表现拔尖。


<details>
  <summary>Details</summary>
Motivation: 随着多语言多模态推理在教育等高要求场景中需求的增长，现有的端到端大模型虽然强大但很重，可能效率低下。因此，探索轻量级集成方法、优化提示策略以及跨语言增强，期望获得更优性能。

Method: 搭建了一个集成系统，集成Gemini系列（2.5 Flash、1.5 Pro、2.5 Pro）模型，分别在图片描述、字幕润色、一致性检查、最终推理决策等环节协作。通过few-shot和zero-shot的提示工程对多语言数据进行推理，并设计了严格的格式化及输入限制。进行了多种大模型（Phi 4, Gemma 3, Mistral等）的消融实验，数据涵盖原始英语以及多语种增强。

Result: 设计的系统在ImageCLEF 2025 EXAMS V大赛中多语言赛道夺得第一，总准确率81.4%，11/13种语言赛道领先，如克罗地亚语和意大利语表现优异（分别95.07%和92.12%）。同时，零样本的Gemini 2.5 Flash模型优于有监督训练模型，优化的提示工程可提升英文集验证准确率自55.9%至61.7%。

Conclusion: 轻量级OCR-视觉语言模型（VLM）集成，配合精准的提示和跨语种数据增强，可在高要求多语言教育场景下优于重型端到端模型。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [59] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 作者提出一种面向个体的人格信息记忆量化方法和数据集（WikiMem），为识别和删除LLM中已记忆的个人信息提供平台，有助于满足GDPR等法规对被遗忘权的要求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）会记忆并泄露个人信息，引发了关于如何符合欧盟GDPR法规、尤其是“被遗忘权”相关条款的担忧。已有的“机器遗忘”方法假设要遗忘的数据已知，却没有方法判别模型中实际存储了哪些个人相关事实的信息。现有隐私审核只适用于群体或有限的标识符，难以满足个体级别的数据查询需求。

Method: 作者提出WikiMem数据集，包含5000多个来自Wikidata、涉及243个人属性的“自然语言canary句子”。他们还提出了一个与模型无关的度量方法，通过对同一内容的不同表述进行校准的负对数似然度量，将真实属性与对立事实进行对比排序，从而量化LLM中有关个体的事实性记忆。实验覆盖15个规模不同的LLM，并对200名个体进行了测试。

Result: 结果显示：LLM记忆的个人信息与主体在网络上的活跃程度（web presence）和模型规模正相关。提出的方法能够在个体级别识别LLM所存储的个人事实，有助于动态打造“需被遗忘的数据集”，以支持机器遗忘和实现GDPR被遗忘权请求。

Conclusion: 本文为找出LLM中被记忆的个人数据提供了有效基础工具，能够支持更精确和动态的机器遗忘、以及对GDPR被遗忘权的响应。该工具提升了个人数据安全和隐私保护的可操作性。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [60] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 用大语言模型模拟多智能体协作编码，发现多数情况下单模型编码更优，多人格代理难以提升准确率，但有助于发现编码争议点和完善编码手册。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索多智能体系统（MAS）在大语言模型（LLMs）驱动下对定性研究领域，特别是在大规模编码和数据标注中的作用及其相比单智能体的优势。目前关于MAS相较于单智能体编码的具体好处尚不明确，因此需要系统性研究。

Method: 作者设计了模拟人类归纳编码流程的MAS系统，通过结构化群体探讨和共识仲裁实现多代理协作。实验涵盖6种开源LLM（参数规模3-32B），18种参数配置，对77,000余条对话编码决策进行对比分析，全部以人类标注的转录文本作为金标准。同时，考察了多种代理人格特征（如中立、坚定、共情）与temperature参数对共识建立及编码准确性的影响。

Result: 温度（temperature）对所有LLM达成共识的可能性及速度有显著影响；多种人格的MAS在大部分模型下显著延缓共识达成，而较高温度则削弱这一延缓效果。整体来看，无论是温度还是人格搭配，都未能显著提升编码准确性；大多数情况下，单一代理的表现优于MAS共识。只有OpenHermesV2:7B模型某一编码类别在温度≤0.5并包含至少一位坚定人格代理时，MAS推理结果超过单体水平，且主要体现在界定模糊编码应用场景。

Conclusion: 多智能体系统（MAS）并非在LLM驱动的定性编码任务中普遍优于单智能体，多样化代理人格也不会稳定提升性能。MAS主要价值可能体现在帮助细化编码规则、辅助人机协作而非直接提升准确率。作者还开源了MAS及其实验代码，以促进后续研究。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [61] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 本文提出了面向西班牙语和加泰罗尼亚语的社会偏见问答基准评测数据集，并发现现有LLM在准确率提升的同时，社会偏见风险也加剧，强调多语种公平性研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 以往文献表明，大型语言模型（LLM）会延续其训练数据中的社会偏见，但英文以外的社会偏见评估资源及美国以外的社会语境研究稀缺。该文旨在填补西班牙语和加泰罗尼亚语社会偏见评估工具的空白。

Method: 基于原有的BBQ基准，作者提出了针对西班牙语和加泰罗尼亚语的Bias Benchmark for Question Answering（EsBBQ和CaBBQ）数据集，覆盖10个社会类别，并将多项选择问答场景适应西班牙及相关语言及社会语境。随后，作者对多种不同类型和规模的LLM进行了评测。

Result: 评测结果显示，不同LLM在不明确（模棱两可）场景下普遍难以选择正确答案。同时，模型在问答准确率高的情况下，往往更加依赖社会偏见。

Conclusion: 高准确率的LLM可能牺牲公平性而过度依赖社会刻板印象，西班牙语及加泰语场景下的社会偏见测评工具有望推动多语种偏见研究。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [62] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: 本文针对FSM抽取存在的难题，提出了FlowFSM框架，将LLM与多步提示和推理结合，实现了从RFC文档中高精度提取FSM，实验结果突出，显示了此方法用于协议分析和网络安全的前景。


<details>
  <summary>Details</summary>
Motivation: 现有的有限状态机（FSM）抽取技术存在可扩展性不足、覆盖不全以及自然语言规范产生歧义等问题，影响网络协议逻辑建模和安全性分析。

Method: 本文提出FlowFSM，一个结合了大型语言模型（LLMs）、提示链（prompt chaining）和思维链推理（chain-of-thought reasoning）的创新代理式框架，实现从原始RFC协议文档中自动抽取准确FSM。该框架通过系统化流程处理协议规范，识别状态转移，串联代理产出，构建结构化规则书。

Result: 在FTP和RTSP协议实验中，FlowFSM实现了高精度的FSM抽取，有效减少了错误（幻觉）转移，取得了有希望的结果。

Conclusion: 基于代理的LLM系统在协议分析和FSM推理方面具有巨大潜力，为网络安全和逆向工程提供了新工具。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [63] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出了利用稀疏自编码器和特征激活概率方法，识别并解释大型语言模型中的语言特定特征。这些特征有助于模型多语言处理，提高可解释性，并在语言识别任务上取得与fastText相当的效果。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLMs）处理多语言的机制具有重要意义，但目前的研究多聚焦于单个神经元，难以分离语言特定的单元。模型内部存在多义性结构，单独关注神经元无法揭示跨语言的具体与抽象特征。

Method: 本工作探索了稀疏自编码器（SAEs）在学习LLM中具体与抽象的、多语言语义特征方面的能力，并提出了SAE-LAPE方法，通过特征激活概率在前馈神经网络层内识别语言特定特征。

Result: 发现大量语言特定特征主要出现在模型中间到后期层，这些特征可解释且显著影响模型的多语言表现和输出。同时，这些特征可用于语言识别，性能与fastText相当但拥有更强可解释性。

Conclusion: 通过SAE-LAPE方法可以有效发现和解释大语言模型中的语言特定特征，有助于提升模型对多语言处理机制的理解，并可用于提升模型可解释性和实际任务表现。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [64] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: 提出了 KV-Latent 方案，对 Transformer LLM 的 KV Cache 进行降采样，有效减少内存与带宽消耗，提升推理效率。方法简单实用，仅需极少再训练，且在不同模型上均表现良好，并发布了开源代码。


<details>
  <summary>Details</summary>
Motivation: Transformer Decoder 架构的 LLMs 虽然性能优越，但推理阶段 KV Cache 空间和带宽消耗成为效率瓶颈。需要更高效的 KV Cache 机制以降低内存和带宽占用。

Method: 提出 KV-Latent 方法，将 Key-Value 向量降采样到潜在空间，从而大幅减少 KV Cache 占用，并仅需极少量再训练。同时改进低维向量的旋转位置编码的频率采样方式，以增强稳定性。

Result: 在有无 Grouped Query Attention 的模型上实验均取得令人满意的效果。还对单独减少 Key 和 Value 的尺寸对模型性能影响进行了比较分析。

Conclusion: KV-Latent 大幅提升了 LLM 的 KV Cache 效率，降低内存和带宽占用，提升推理速度，实现了高效的推理系统。为高效 LLM 和 KV Cache 节省提供了新的可能性。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [65] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 作者提出了基于大模型和错误反馈的自动形式化方法，打造了一个高质量的自然语言与Lean形式化对齐的数据集，并验证了其对自动定理证明器的挑战性及其作为基准的价值。


<details>
  <summary>Details</summary>
Motivation: 自动将大量自然语言的数学题目转化为形式化语言数据集，有助于推动自动数学推理领域发展。现有方法有限，亟需高效、准确的自动形式化方法。

Method: 提出基于大语言模型和错误反馈机制的管线，实现了全自动且无需训练的自然语言到形式语言的转换。利用该方法构建了一个与Lean形式化对齐的奥数级别数据集。

Result: 构建了包含3922道自然语言数学题和9787个Lean格式问题的数据集，其中64.46%的结果被评为品质中上，适合作为自动定理证明系统的基准测试集。同时实验证明，few-shot学习、错误反馈以及增加采样次数可提升自动形式化效果。

Conclusion: 提出的管线可高效实现自动形式化，生成的数据集难度较高且具有重要基准意义。经验证，该数据集对多种自动定理证明器均构成挑战，能促进形式化自动推理领域进步。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [66] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本论文提出了首个span级中文仇恨言论数据集，并通过整合词表的方法显著提升了检测和解释能力，为中国语境下的仇恨言论识别提供了重要资源和方法。


<details>
  <summary>Details</summary>
Motivation: 中文仇恨言论检测研究相较于英文领域存在滞后，主要受到两方面挑战：缺乏细粒度标注数据集，难以深度理解仇恨语义；缺乏对隐晦仇恨表达的识别和解释手段，影响模型的可解释性。

Method: （1）构建并发布了首个面向中文、具备span级标签的仇恨言论数据集STATE ToxiCN；（2）首次系统性研究了中文隐晦仇恨表达术语，并用大模型评估其语义解释能力；（3）提出将标注词表融合到模型方法中，以提升检测能力。

Result: （1）STATE ToxiCN填补了中文细粒度仇恨言论标注数据集的空白，可评测现有模型的语义理解能力。（2）系统分析了中文隐晦仇恨表达及大模型的解释能力。（3）所提词表集成方法显著提升了仇恨言论检测性能。

Conclusion: 本工作丰富了中文仇恨言论检测资源和方法，极大推动了仇恨言论可解释性与检测精度的提升，为后续相关研究奠定了基础。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [67] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot多智能体大模型系统为罗马尼亚医生提供交流方式改进建议，显著提升远程医疗文本回应的表达质量，经实际部署后取得良好成效。


<details>
  <summary>Details</summary>
Motivation: 尽管基于文本的远程医疗日益普及，医生与患者之间的医疗建议往往是根据交流方式而非医学准确性来评判的，这影响了医疗服务的质量。

Method: 提出了Dr.Copilot系统，该系统由三个大型语言模型（LLM）智能体组成，通过DSPy自动优化提示词，不评估医学准确性，而是从17个可解释维度对医生的书面回复表达质量进行反馈和提升。该系统使用较低资源的罗马尼亚语数据，部署于远程医疗平台，并采用开源权重模型。

Result: 系统的实证评估和在41名医生中的实际部署显示，用户评价与回复质量均有显著提升，系统在罗马尼亚医疗环境实现了LLM的首次真实应用。

Conclusion: Dr.Copilot通过提升医生书面建议的表达质量，显著改善了远程医疗服务的互动效果和用户满意度，为低资源语言环境下LLM的实际医疗应用提供了范例。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [68] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出一种新方法ConVA，能在不影响模型性能的情况下，有效实现LLM对多种人类价值的高一致性对齐，且对恶意输入具备鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型（LLMs）与人类价值越来越受到重视，因为这样有助于提升模型的透明度、适应性和明确性。作者希望有效地对齐LLM的内部价值表达，确保模型在不同场景下能够稳定反映一致的人类价值观。

Method: 提出Controlled Value Vector Activation（ConVA）方法，直接在LLM的隐层表征中对价值进行解释和调整；为提升解释的准确性和无偏性，提出上下文控制的价值向量识别方法；为保证模型性能不受损，采用门控的价值向量激活方法，实现最小化控制下的有效一致性。

Result: 实验表明，该方法在不影响模型表现和流畅性的前提下，在10种基础价值观上的控制成功率最高，并能应对具有相反甚至恶意导向输入的情况，实现预期价值的一致性表达。

Conclusion: ConVA方法证明了可以在不牺牲LLM性能的前提下，有效、稳定地控制其价值输出，实现了LLM与人类目标价值的高效对齐。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [69] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文针对学术论文新颖性评估局限，提出结合LLM和人工专家知识、使用稀疏注意力融合模块辅助PLMs的方法，并在实验中取得显著优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前学术论文的新颖性常由专家主观判断或基于引用组合判定，但这两种方法均存在局限。专家知识有限，引用方法有效性存疑，且无法确定独特引用是否真的代表新颖性。

Method: 作者提出结合大语言模型（LLM）的丰富知识和人类专家的判断能力，辅助预训练语言模型（如BERT）预测论文方法的新颖性。主要做法包括：（1）提取评审报告中与新颖性相关的句子；（2）用LLM总结论文方法部分；（3）用于微调PLMs，并设计了稀疏注意力的文本引导融合模块，实现知识有效整合。

Result: 与大量基线方法对比，作者提出的方法在论文方法新颖性评估任务上取得了优越的表现。

Conclusion: 融合LLM与专家知识、并通过新颖的稀疏注意力机制与PLMs结合，能够有效提升学术论文方法新颖性自动评估的准确性和实用性。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [70] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次系统比较了9种流程模型表示在LLM流程建模任务中的效果，构建了PMo数据集并进行实证评估。发现Mermaid整体最优，BPMN文本生成模型结构最接近原流程。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来LLMs在流程建模中的应用日益普及，但针对流程模型表现（PMRs）的实际对比还未有系统性研究，而不同的PMRs在结构、复杂性和可用性等方面差异显著，且PMG的评估标准和生成方式各异，导致缺乏统一比较框架。基于此，作者希望为后续LLM驱动的流程建模提供基础和路线指引。

Method: 作者新构建了PMo数据集，包含55个流程描述及其对应的九种不同PMRs模型。论文通过实证实验，从LLM支持的PMo适用性和PMG性能两个维度，对各种PMRs进行系统性比较。

Result: 六项PMo评价标准下，Mermaid表现最佳，总分最高；但在流程元素相似性（PMG关键指标）方面，BPMN文本最优。该研究为基于LLM的流程模型表示选择提供了实证依据。

Conclusion: 此论文首次对多种流程模型表示（PMRs）在利用大语言模型（LLMs）进行流程建模（PMo）任务中的表现进行了实证评估。结果显示，Mermaid在六项PMo准则中总体得分最高，而BPMN文本在流程元素相似度方面则实现了最佳流程模型生成（PMG）效果。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [71] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 本文提出在Transformer情感检测模型中使用动态加权损失函数来解决类别不均衡，实验表明该方法提升了高频类别检测效果，但对低频类别改进有限。


<details>
  <summary>Details</summary>
Motivation: 多标签情感检测任务中数据类别分布极度不均，常规的采样方法计算开销大，如何提升少数类别的识别效果成为挑战。

Method: 在Transformer模型（如BERT、RoBERTa、BART）中引入动态加权损失函数，通过调整类权重来应对数据不平衡问题，而非采用传统的重采样方法。

Result: 动态加权损失函数能改善高频情感类别的检测表现，但对少数类别的提升有限。

Conclusion: 加权损失函数方法在多标签不均衡情感检测上有效提升主流类别表现，但难以显著改进少数类，显示这一方法的局限性和后续改进空间。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [72] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 本文提出DCR框架，有效检测和量化大模型基准数据污染，通过调整准确率提升评测公正性，平均误差低于4%，可广泛集成于LLM评测流程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）迅速发展，但模型在评测时可能记忆评测数据，从而造成基准数据污染（BDC），导致模型表现虚高，无法真正评估其泛化能力。因此，亟需有效工具检测和量化数据污染，提升评测公正性和可信度。

Method: 本文提出了数据污染风险（DCR）框架，这是一种轻量级、可解释的流程管道，可从语义、信息、数据、标签四个细粒度水平检测和量化污染。通过模糊推理系统汇总各级分数，生成统一的DCR因子，用于调整原始准确率，反映受污染影响修正后的模型表现。

Result: DCR框架在9个不同规模（0.5B-72B）大模型、涵盖情感分析、虚假新闻检测及算数推理等任务上进行验证，能准确诊断污染严重程度，并用DCR因子调整后表现与无污染基线对比，三大基准下平均误差在4%以内。

Conclusion: DCR框架高效、透明，能实用地集成至日常评估流程，为LLM基准评测提供污染检测和修正，增强了评测的公正性和可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [73] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0新模型兼顾强推理、多语种、工具调用能力，并在开源同类模型中性能突出，便于下载应用。


<details>
  <summary>Details</summary>
Motivation: 在AI迈向Agent化时代的趋势下，需要既具备强推理能力又便于实际应用，且能灵活支持工具调用和多语种环境的大模型。

Method: 通过在EXAONE 3.5的基础上引入推理模式，并集成Agent工具使用接口，开发了两种模型规模以满足不同部署需求，同时扩大了多语种支持。

Result: EXAONE 4.0的两个模型（32B高性能版、1.2B端侧版）在同类开源模型中性能优越，在与前沿模型的对比中亦具竞争力，并已开放下载。

Conclusion: EXAONE 4.0综合了非推理和推理两种模式，兼顾了易用性与高级推理能力，并具备多语种、Agent工具调用等特性，在同类开源模型中表现优异。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [74] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 提出了一种可自动提取的因果链式思维图，并构建了相应大数据集，详细实验证明大模型推理时本质依赖于这些结构，为研究其推理机制指明了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（Chain-of-thought, CoT）可以提升大语言模型在推理任务中的表现，但其提升机制尚未达成共识。为更加深入理解这一机制，作者提出了新的因果图模型。

Method: 提出因果CoT图（CCG），通过自动地从推理链条中抽取有向无环图以建模大语言模型输出中的因果依赖关系。作者利用1671道数学推理题（覆盖MATH500、GSM8K、AIME）和这些题目对应的CCGs，构建了KisMATH数据集，并用15个开源LLM进行了实证分析。

Result: 发现CCG中的推理节点是最终答案的中介，对推理过程是必要的；大模型在推理过程中会侧重于CCG所给的推理路径，说明模型内部已自然形成类似的结构。KisMATH还可用于受控的、图对齐的推理干预。

Conclusion: 本研究通过引入因果CoT图和KisMATH数据集，证实了链式思维能被转化为明确的因果路径，LLM内部隐式具有相关结构，为后续关于大模型推理机制的研究提供了基础和工具。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [75] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 作者开源同配方训练的encoder-only与decoder-only大模型套件，发现二者在各自任务（分类/生成）表现最佳，迁移适配并非优选，完整数据和训练过程助力后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）社区主要关注于decoder-only的语言模型，因为它们更适合文本生成任务。但实际中，许多任务（如分类、检索）仍大量依赖encoder-only模型。以往对于encoder-only与decoder-only架构的对比研究，往往模型参数量、训练方法和数据集都不相同，难以得出公平结论。本文的动机是设计一个配对、统一训练环境下的encoder-only和decoder-only模型套件，以公平、系统地评估两类架构的任务表现。

Method: 提出并训练了Ettin开源模型套件，包含参数量从1700万到10亿的encoder-only和decoder-only模型，训练数据规模达2万亿token，且二者完全用一致的配方（数据、方法、超参数等）训练。系统地在分类、检索和生成等任务上进行对比评估，并通过双向迁移学习（将decoder适配encoder任务、或反向）进一步分析架构优势。所有训练数据、流程与200+训练检查点全部开源。

Result: 1. 使用相同训练策略后，两类架构在各自任务上都取得SOTA表现：encoder-only超过ModernBERT（同类编码器），decoder-only优于Llama 3.2、SmolLM2（同类解码器）。2. encoder-only在分类、检索任务上优势显著，而decoder-only在生成任务表现更优。3. 将decoder-only适配为encoder任务（或反向）效果有限，不如直接用合适架构：如400M encoder on MNLI超过1B decoder，生成任务则反之。4. 充分开源数据与过程，为后续研究提供资源。

Conclusion: 在相同训练条件下，encoder-only与decoder-only架构各自对特定任务有独特优势；迁移适配不如选择合适架构。统一训练和全面开源有助于公平比较和促进后续LLM研究。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [76] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 该论文发现通过提示词可以控制LLM推理策略，但没有单一最佳策略，引导模型自适应选择合适策略可提升推理表现。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，大型语言模型（LLMs）往往倾向采用单一推理策略，这可能限制其在应对多样化推理问题时的表现。该研究旨在探究是否可以通过提示词来控制LLM的推理策略，并评估其对逻辑问题解决能力的影响。

Method: 通过对不同推理策略下，利用提示词控制LLMs的实验，分析不同策略和策略选择方法对推理准确性的影响。同时，提出引导LLMs选择策略的新方法。

Result: 实验显示，没有任何一种单一策略能够在所有情况下稳定提升准确率。如果模型能够自适应地选择最优策略，其性能将进一步提升。

Conclusion: 通过控制LLMs的推理策略可以在一定程度上提升推理能力，但关键在于实现灵活自适应的策略选择机制。研究提出了引导LLMs选择最佳推理策略的新方法，为提升其推理表现开辟了新的方向。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [77] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 本文介绍了针对香港本地需求开发的基础大模型HKGAI-V1。模型通过多维度微调与检索增强，优化了对香港多语言、多元法律文化环境的适应性。其实验结果表明，HKGAI-V1能更好处理本地敏感问题，并通过新开发的本地价值测评工具，验证其与本地规范的高度一致性。该成果为本地AI自主与治理提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 面向香港本地化需求，开发符合香港价值观和法律体系的AI基础设施。该地区拥有独特的多语言环境（粤语、普通话和英语）及“一国两制”下的社会法律背景，因此需要符合本地文化和规范的基础大模型。

Method: 基于DeepSeek架构，对模型进行全参数多维度本地化微调，并系统性地与本地规范对齐。同时引入检索增强生成（RAG）系统以提供事实支撑和实时信息。提出了区分本地化AI对齐与安全框架，并自主开发了Adversarial HK Value Benchmark，用以严格评估模型在香港伦理法律标准下的表现。

Result: 成功研发出HKGAI-V1模型，在处理香港本地及敏感问题时效果优于通用大模型，实现了“嵌入治理式”的数字主权。模型已可应用于本地公共服务、法律与教育等关键领域。同时，建立的评测基准工具能够可靠评估模型与本地价值观的契合度。

Conclusion: HKGAI-V1模型不仅推动了香港AI自主可控进程，也为其他有区域特色的AI系统开发提供了可复制蓝图。该研究兼具技术创新与社会实践价值。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [78] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 评估酒店摘要的忠实性时，传统简单指标与人工判断相关性良好，复杂方法或LLM自动评测并不总是更优。实际业务中，应重视错误和难验证信息带来的风险。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLM）生成酒店亮点摘要时的忠实性非常重要，因为这些摘要用于展现住宿独特卖点，准确性对实际业务影响重大。当前评估方法（如传统评价指标或训练方法）在复杂性与效果上的表现尚不清晰，有必要系统比较。

Method: 采用人工评估，进行类别错误识别及片段级标注，对比传统指标、可训练方法及“LLM评判员”三种评估方式。进一步分析在真实业务中的影响与众包评估面临的挑战。

Result: 结果显示，像词重叠这样简单的指标与人工评估相关性很高（斯皮尔曼秩相关0.63），在跨领域场景下有时甚至优于复杂模型。LLM自身虽然能生成高质量摘要，但用于摘要评测时不可靠，易出现严重的过度或不足标注。发现不正确和不可查证信息对业务有最大风险。

Conclusion: 简单传统指标的表现也很强，某些情况下胜过复杂的方法；LLM生成内容用于自我评估应谨慎；业务上需警惕不准确与不可查证内容；众包评估亦存在挑战。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [79] [Metrics on Permutation Families Defined by a Restriction Graph](https://arxiv.org/abs/2507.10569)
*Danylo Tymoshenko,Leonhard Nagel*

Main category: cs.DM

TL;DR: 本文研究了通过有向限制图约束的排列集的极端度量距离以及相关算法，揭示了Kendall-Tau度量与偏序集维数的密切关系，并对组合统计应用给出了新结果和高效算法。


<details>
  <summary>Details</summary>
Motivation: 理解排列家族的度量结构对组合学具有基础意义，并且在社会选择理论、生物信息学和编码理论中有应用。本文研究由限制图定义的排列家族，这些限制图是决定排列中元素相对次序的有向图。

Method: 针对任意限制图，作者分析了在最大距离（$[0;35m\ell_\infty[0m$-度量）下两排列之间的最远距离，并给出了一个显式的算法来构造最优排列对。同时，文章刻画了Kendall-Tau度量在何种条件下能达到组合上界，并分析了其与集合有向序维数之间的关系。研究还应用到经典的排列统计量，包括下降集和Hessenberg簇。

Result: 1. 给出了限制图下排列的最大距离的确定方法和相应构造算法；2. 证明了Kendall-Tau距离达到组合上界当且仅当限制图所诱导的偏序集的维数至多为2；3. 揭示了度量几何和偏序集维数理论之间的深刻联系；4. 对经典排列统计量如下降集，给出了度量直径的显式公式和高效算法。

Conclusion: 本文系统性地研究了受限制图约束的排列家族的度量结构，为理解排列的极端距离、度量与偏序理论的联系提供了理论支撑，并为相关算法应用铺垫了基础。

Abstract: Understanding the metric structure of permutation families is fundamental to
combinatorics and has applications in social choice theory, bioinformatics, and
coding theory. We study permutation families defined by restriction
graphs--oriented graphs that constrain the relative order of elements in valid
permutations. For any restriction graph $G$, we determine the maximum distance
achievable by two permutations under the $\ell_\infty$-metric and provide an
explicit algorithm that constructs optimal permutation pairs. Our main
contribution characterizes when the Kendall-Tau metric achieves its
combinatorial upper bound: this occurs if and only if the poset induced by $G$
has dimension at most 2. When this condition holds, the extremal permutations
form a minimal realizer of the poset, revealing a deep connection between
metric geometry and poset dimension theory. We apply these results to classical
permutation statistics including descent sets and Hessenberg varieties,
obtaining explicit formulas and efficient algorithms for computing metric
diameters.

</details>


### [80] [Rapid Mixing of Glauber Dynamics for Monotone Systems via Entropic Independence](https://arxiv.org/abs/2507.11031)
*Weiming Feng,Minji Yang*

Main category: cs.DM

TL;DR: 本文通过新颖的方法，将Glauber动力学在单调系统上的混合时间上界推进至更优水平，并结合高维扩展与经典不等式，理论和应用上均取得突破。


<details>
  <summary>Details</summary>
Motivation: 研究Glauber动力学在单调系统上的混合时间，并改进相关模型的混合时间上界，从而拓展对于统计物理和组合优化中采样收敛速度的理解。

Method: 结合经典的审查(筛选)不等式中的随机主导(stochastic dominance)思想与最新高维扩展子(high-dimensional expanders)工具，提出单调系统Glauber动力学与场动力学间的新比较结果。

Result: 对于满足熵独立条件(entropic independence)的单调系统，给出了Glauber动力学混合时间新的比较定理。具体应用上，获得了铁磁Ising模型诱导的随机簇模型在偏置外场下一致$	ilde{O}(n)$混合时间，以及二分硬核模型在单侧唯一性条件下$	ilde{O}(n^2)$混合时间，均优于已有最佳结果。

Conclusion: 本文提升了单调系统Glauber动力学混合时间的理论界，证明了新的混合时间比较工具，并在具体模型中获得了更优的混合时间上界，推动了相关领域的研究进展。

Abstract: We study the mixing time of Glauber dynamics on monotone systems. For
monotone systems satisfying the entropic independence condition, we prove a new
mixing time comparison result for Glauber dynamics. For concrete applications,
we obtain $\tilde{O}(n)$ mixing time for the random cluster model induced by
the ferromagnetic Ising model with consistently biased external fields, and
$\tilde{O}(n^2)$ mixing time for the bipartite hardcore model under the
one-sided uniqueness condition, where $n$ is the number of variables in
corresponding models, improving the best known results in [Chen and Zhang,
SODA'23] and [Chen, Liu, and Yin, FOCS'23], respectively.
  Our proof combines ideas from the stochastic dominance argument in the
classical censoring inequality and the recently developed high-dimensional
expanders. The key step in the proof is a novel comparison result between the
Glauber dynamics and the field dynamics for monotone systems.

</details>


### [81] [Lower bounds for dominating set reconfiguration on sparse (directed) graphs](https://arxiv.org/abs/2507.11446)
*Jona Dirks,Alexandre Vigny*

Main category: cs.DM

TL;DR: 本文研究了支配集Token Sliding重新配置问题，证明其在多种参数组合下为W[2]-hard或NP-hard，比相关问题复杂性更高，揭示了该领域若干难解性结果。


<details>
  <summary>Details</summary>
Motivation: 本论文关注于图的支配集重新配置问题，尤其是在“Token Sliding”机制下的复杂性。虽然某些变体（如Token Jumping）在多数图类别上有高效算法，但Token Sliding版本在理论与实际应用中更具挑战性。作者希望探索在不同参数化（如支配集大小、路径宽度等）下，此问题的参数化复杂性，以揭示该问题的边界与难解性。

Method: 作者从参数化算法复杂性角度出发，通过构建复杂性证明，将DSR-TS与已知的W[2]-hard问题进行了归约。同时，他们引入了新的参数化（如转换序列的迭代次数），并研究该参数对问题复杂性的影响。此外，研究还扩展到了有向图的变体，并综合考虑路径宽度、树宽、深度等参数。

Result: 作者证明了DSR-TS问题在参数k、路径宽度、以及重构迭代次数的组合下为W[2]-hard，这比Token Jumping和独立集变体要难。即使不限定迭代次数，DSR-TS仍在有限树宽或路径宽度的图上是W[2]-hard。对于有向图变体DSR-DTS，作者证明其在树宽5的DAG上为NP-hard，以及在某些参数化组合下是W[2]-hard，而与独立集变体相比，复杂性更高。

Conclusion: 支配集的Token Sliding重新配置问题，在多种常见参数化下均展现出更高的理论复杂性，特别是在树宽、路径宽度、迭代次数等参数下，对比其它相关问题表现出W[2]-hard乃至NP-hard的难度。这为相关算法设计与理论研究指明了问题的边界。

Abstract: In a graph, a vertex dominates itself and its neighbors, and a dominating set
is a set of vertices that together dominate the entire graph. Given a graph and
two dominating sets of equal size $k$, the {\em Dominating Set Reconfiguration
with Token sliding} (DSR-TS) problem asks whether one can, by iteratively
replacing a vertex by an adjacent one, transform the first set into the second
one, while ensuring that every set during the reconfiguration process is a
dominating set.
  The token jumping variant, where a vertex can be replaced by a non-adjacent
one, is known to be efficiently solvable on many graph classes such as planar,
bounded treewidth, and the very broad notion of nowhere-dense classes of
graphs. Alternatively, some algorithms also exist for the reconfiguration of
independent sets in the token sliding paradigm for graph classes with bounded
degree or large girth.
  We show that DSR-TS is W[2]-hard when parameterized $k$, the pathwidth of the
instance, and the iteration of the reconfiguration sequence (a recently
introduced parameter). This is a setting where both the token jumping and the
independent set variants are fixed parameter tractable. Not restricting the
iteration yields W[2] hardness already on graphs with treewidth 9 and pathwidth
13.
  In the directed variant (DSR-DTS), we are only allowed to replace a vertex
with an out-neighbor. We show that DSR-DTS is NP-hard on DAGs of treewidth 5
and W[2]-hard for both the case of DAGs of depth 3 parameterized by $k$, and
the case of DAGs when parameterized by $k$ and the pathwidth of the instance
(independent set reconfiguration is again FPT in both settings).

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [82] [A Decision Procedure for Probabilistic Kleene Algebra with Angelic Nondeterminism](https://arxiv.org/abs/2507.10980)
*Shawn Ong,Dexter Kozen*

Main category: cs.FL

TL;DR: 本文提出并论证了一种针对概率Kleene代数结合天使式非确定性的等式理论的决策算法。


<details>
  <summary>Details</summary>
Motivation: 概率Kleene代数与天使式非确定性结合后，其等值判定具有理论与实践意义。缺乏这方面的有效决策机制。本文工作旨在填补这一领域空白。

Method: 本文提出了一个决策过程（decision procedure），并给出了其正确性的证明。

Result: 得到了针对该等式理论的决策过程及其正确性证明，为后续研究和实际应用提供了理论基础。

Conclusion: 本文为概率Kleene代数带有天使式非确定性的等式理论提供了决策过程，并证明了其正确性。

Abstract: We give a decision procedure and proof of correctness for the equational
theory of probabilistic Kleene algebra with angelic nondeterminism introduced
in Ong, Ma, and Kozen (2025).

</details>


### [83] [Polynomial Complementation of Nondeterministic 2-Way Finite Automata by 1-Limited Automata](https://arxiv.org/abs/2507.11209)
*Bruno Guillon,Luca Prigioniero,Javad Taheri*

Main category: cs.FL

TL;DR: 本文证明了2NFA可以用1-LA补操作，只需多项式状态增长，进而说明对1-LA，补操作的必要和充分状态扩展为单指数级。


<details>
  <summary>Details</summary>
Motivation: 2NFA补操作的状态复杂性一直是理论计算机科学中的重要问题，补操作通常会导致状态爆炸。本论文旨在探索2NFA补操作的有效机理，并寻求仅需可控（多项式/单指数）状态增长的补操作方法。

Method: 作者构造了一种方法，将2NFA用1-LA进行补操作，并进一步证明该补机器实际上是带共同猜测的2NFA，并具有自验证特性。通过这一构造，分析补操作所需的状态增长界限。

Result: 所有2NFA都可以在多项式状态增长下被1-LA补操作，补机器属于2NFA带共同猜测的自验证子类型。对1-LA而言，补操作需要且仅需要单指数状态增长。

Conclusion: 只需多项式增长的状态数，每个不受限的二向非确定有限自动机（2NFA）都能通过1-限制自动机（1-LA）完成补操作，并且补结果是一类被称为“带共同猜测的2NFA”的自验证自动机。对于1-LA，补操作所需的状态增长是单指数级，且这是充分且必要的。

Abstract: We prove that, paying a polynomial increase in size only, every unrestricted
two-way nondeterministic finite automaton (2NFA) can be complemented by a
1-limited automaton (1-LA), a nondeterministic extension of 2NFAs still
characterizing regular languages. The resulting machine is actually a
restricted form of 1-LAs -- known as 2NFAs with common guess -- and is
self-verifying. A corollary of our construction is that a single exponential is
necessary and sufficient for complementing 1-LAs.

</details>
