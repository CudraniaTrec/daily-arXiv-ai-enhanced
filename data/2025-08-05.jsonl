{"id": "2508.01255", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01255", "abs": "https://arxiv.org/abs/2508.01255", "authors": ["Cuong Chi Le", "Cuong Duc Van", "Tung Duy Vu", "Thai Minh Pham Vu", "Hoang Nhat Phan", "Huy Nhat Phan", "Tien N. Nguyen"], "title": "TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models", "comment": null, "summary": "Regression testing ensures that code changes do not unintentionally break\nexisting functionality. While recent advances in large language models (LLMs)\nhave shown promise in automating test generation for regression testing, they\noften suffer from limited reasoning about program execution, resulting in\nstagnated coverage growth - a phenomenon known as the coverage plateau. In this\npaper, we present TestWeaver, a novel LLM-based approach that integrates\nlightweight program analysis to guide test generation more effectively.\nTestWeaver introduces three key innovations: (1) it reduces hallucinations and\nimproves focus by supplying the LLM with the backward slice from the target\nline instead of full program context; (2) it identifies and incorporates close\ntest cases - those that share control-flow similarities with the path to the\ntarget line - to provide execution context within the LLM's context window; and\n(3) it enhances LLM's reasoning with execution in-line annotations that encode\nvariable states as comments along executed paths. By equipping LLMs with these\ntargeted and contextualized inputs, TestWeaver improves coverage-guided test\ngeneration and mitigates redundant explorations. Empirical results demonstrate\nthat TestWeaver accelerates code coverage growth and generates more effective\nregression test cases than existing LLM-based approaches."}
{"id": "2508.01199", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01199", "abs": "https://arxiv.org/abs/2508.01199", "authors": ["Avinash Malik"], "title": "Efficient compilation and execution of synchronous programs via type-state programming", "comment": null, "summary": "Synchronous programs are used extensively in implementation of safety\ncritical embedded software. Imperative synchronous programming languages model\nmultiple Finite State Machines (FSMs) executing in lockstep at logical clock\nticks. The synchronous view of time along with the FSM based design enables\neasier formal verification. The synchronous composition of multiple FSMs,\nduring compilation, results in the well known state space explosion problem.\nHence, efficiently compiling imperative synchronous programs into small and\nfast executables is challenging. This paper introduces a novel linear time\ncompilation technique for automata based compilation of synchronous programs.\nGraph based rewrite rules for kernel programming constructs are introduced. A\nlinear time algorithm applies these rules to produce a FSM. The FSM is then\nencoded into a type-state program using template meta-programming in C++.\nExperimental results show that the compilation time and generated binary size\nis comparable, while the execution times are on average 31-60% faster than\ncurrent state-of-the-art compilers."}
{"id": "2508.01821", "categories": ["cs.FL", "F.4.3"], "pdf": "https://arxiv.org/pdf/2508.01821", "abs": "https://arxiv.org/abs/2508.01821", "authors": ["Dana Fisman", "Elina Sudit"], "title": "Runtime Consultants", "comment": null, "summary": "In this paper we introduce the notion of a runtime consultant. A runtime\nconsultant is defined with respect to some value function on infinite words.\nSimilar to a runtime monitor, it runs in parallel to an execution of the system\nand provides inputs at every step of the run. While a runtime monitor alerts\nwhen a violation occurs, the idea behind a consultant is to be pro-active and\nprovide recommendations for which action to take next in order to avoid\nviolation (or obtain a maximal value for quantitative objectives). It is\nassumed that a runtime-controller can take these recommendations into\nconsideration. The runtime consultant does not assume that its recommendations\nare always followed. Instead, it adjusts to the actions actually taken (similar\nto a vehicle navigation system). We show how to compute a runtime consultant\nfor common value functions used in verification, and that almost all have a\nruntime consultant that works in constant time. We also develop consultants for\n$\\omega$-regular properties, under both their classical Boolean semantics and\ntheir recently proposed quantitative interpretation."}
{"id": "2508.01067", "categories": ["cs.LO", "cs.AI", "F.4.1; F.1.1; I.2.0"], "pdf": "https://arxiv.org/pdf/2508.01067", "abs": "https://arxiv.org/abs/2508.01067", "authors": ["Veeti Ahvonen", "Maurice Funk", "Damian Heiman", "Antti Kuusisto", "Carsten Lutz"], "title": "Expressive Power of Graph Transformers via Logic", "comment": null, "summary": "Transformers are the basis of modern large language models, but relatively\nlittle is known about their precise expressive power on graphs. We study the\nexpressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and\nGPS-networks by Ramp\\'asek et al. (2022), both under soft-attention and average\nhard-attention. Our study covers two scenarios: the theoretical setting with\nreal numbers and the more practical case with floats. With reals, we show that\nin restriction to vertex properties definable in first-order logic (FO),\nGPS-networks have the same expressive power as graded modal logic (GML) with\nthe global modality. With floats, GPS-networks turn out to be equally\nexpressive as GML with the counting global modality. The latter result is\nabsolute, not restricting to properties definable in a background logic. We\nalso obtain similar characterizations for GTs in terms of propositional logic\nwith the global modality (for reals) and the counting global modality (for\nfloats)."}
{"id": "2508.01337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01337", "abs": "https://arxiv.org/abs/2508.01337", "authors": ["Wei Liu", "Linqiang Guo", "Yi Wen Heng", "Chenglin Li", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "Screencast-Based Analysis of User-Perceived GUI Responsiveness", "comment": null, "summary": "GUI responsiveness is critical for a positive user experience in mobile\napplications. Even brief delays in visual feedback can frustrate users and lead\nto negative reviews. However, detecting and quantifying such user-perceived\ndelays remains challenging, especially in industrial testing pipelines that\nevaluate thousands of apps daily across diverse devices and OS versions.\nExisting techniques based on static analysis or system metrics, while useful,\nmay not accurately capture user-perceived issues or scale effectively.\n  In this experience paper, we present \\tool, a lightweight and black-box\ntechnique that measures GUI responsiveness directly from mobile screencasts --\nvideo recordings captured during automated GUI testing. \\tool detects user\ninteractions and visual delays, helping developers identify GUI performance\nissues that affect the user experience. It uses computer vision to detect user\ninteractions and analyzes frame-level visual changes to compute two key\nmetrics: response time (from user action to first visual feedback) and finish\ntime (until visual feedback stabilizes). We evaluate \\tool on a manually\nannotated benchmark of 2,458 interactions from 64 popular Android apps. \\tool\nachieves 0.96 precision and 0.93 recall in detecting interactions, and measures\nresponse and finish times within 50\\,ms and 100\\,ms error, respectively, for\nover 89\\% of interactions. The tool has been deployed in an industrial testing\npipeline and analyzes thousands of screencasts daily, uncovering responsiveness\nissues missed by traditional tools and improving performance debugging\nefficiency."}
{"id": "2508.02305", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02305", "abs": "https://arxiv.org/abs/2508.02305", "authors": ["Rose Bohrer"], "title": "Proceedings 14th International Workshop on Trends in Functional Programming in Education", "comment": null, "summary": "The goal of TFPIE is to gather researchers, teachers and professionals that\nuse, or are interested in the use of, functional programming in education.\nTFPIE aims to be a venue where novel ideas, classroom-tested ideas and\nwork-in-progress on the use of functional programming in education are\ndiscussed. The one-day workshop will foster a spirit of open discussion by\nhaving a review process for publication after the workshop."}
{"id": "2508.01535", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.01535", "abs": "https://arxiv.org/abs/2508.01535", "authors": ["Yeonseok Lee", "Koji Nakazawa"], "title": "Relative Completeness of Incorrectness Separation Logic", "comment": "This is an extended version of a paper that appeared in the Asian\n  Symposium on Programming Languages and Systems (APLAS) 2024: Lee, Y.,\n  Nakazawa, K. \"Relative Completeness of Incorrectness Separation Logic.\" In:\n  Kiselyov, O. (eds) Programming Languages and Systems. Lecture Notes in\n  Computer Science, vol 15194. Springer, Singapore. DOI:\n  10.1007/978-981-97-8943-6_13", "summary": "Incorrectness Separation Logic (ISL) is a proof system that is tailored\nspecifically to resolve problems of under-approximation in programs that\nmanipulate heaps, and it primarily focuses on bug detection. This approach is\ndifferent from the over-approximation methods that are used in traditional\nlogics such as Hoare Logic or Separation Logic. Although the soundness of ISL\nhas been established, its completeness remains unproven. In this study, we\nestablish relative completeness by leveraging the expressiveness of the weakest\npostconditions; expressiveness is a factor that is critical to demonstrating\nrelative completeness in Reverse Hoare Logic. In our ISL framework, we allow\nfor infinite disjunctions in disjunctive normal forms, where each clause\ncomprises finite symbolic heaps with existential quantifiers. To compute the\nweakest postconditions in ISL, we introduce a canonicalization that includes\nvariable aliasing."}
{"id": "2508.01357", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01357", "abs": "https://arxiv.org/abs/2508.01357", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Guwen Lyu", "Zhe Cui"], "title": "HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection", "comment": null, "summary": "Code clone detection is a critical task in software engineering, aimed at\nidentifying duplicated or similar code fragments within or across software\nsystems. Traditional methods often fail to capture functional equivalence,\nparticularly for semantic clones (Type 4), where code fragments implement\nidentical functionality despite differing syntactic structures. Recent advances\nin large language models (LLMs) have shown promise in understanding code\nsemantics. However, directly applying LLMs to code clone detection yields\nsuboptimal results due to their sensitivity to syntactic differences. To\naddress these challenges, we propose a novel two-stage framework that combines\nLLM-based screening with execution-based validation for detecting semantic\nclones in Python programs. In the first stage, an LLM evaluates code pairs to\nfilter out obvious non-clones based on semantic analysis. For pairs not\nidentified as clones, the second stage employs an execution-based validation\napproach, utilizing LLM-generated test inputs to assess functional equivalence\nthrough cross-execution validation. Our experimental evaluation demonstrates\nsignificant improvements in precision, recall, and F1-score compared to direct\nLLM-based detection, highlighting the framework's effectiveness in identifying\nsemantic clones. Future work includes exploring cross-language clone detection\nand optimizing the framework for large-scale applications."}
{"id": "2508.01974", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01974", "abs": "https://arxiv.org/abs/2508.01974", "authors": ["Jiahao Zhang", "Xiao Cheng", "Yuxiang Lei"], "title": "Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis", "comment": null, "summary": "Flow-sensitive pointer analysis constitutes an essential component of precise\nprogram analysis for accurately modeling pointer behaviors by incorporating\ncontrol flows. Flow-sensitive pointer analysis is extensively used in alias\nanalysis, taint analysis, program understanding, compiler optimization, etc.\nExisting flow-sensitive pointer analysis approaches, which are conducted based\non control flow graphs, have significantly advanced the precision of pointer\nanalysis via sophisticated techniques to leverage control flow information.\nHowever, they inevitably suffer from computational inefficiencies when\nresolving points-to information due to the inherent complex structures of\ncontrol flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph\n(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of\ncontrol-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to\nleverage the structural advantages of set-constraint graphs (which are commonly\nused in flow-insensitive pointer analysis) while keeping the flow sensitivity\nof variable definitions and uses, allowing the incorporation of sophisticated\ngraph optimization and dynamic solving techniques. In this way, CG-FSPTA\nachieves significant efficiency improvements while keeping the precision of\nflow-sensitive analysis. Experimental evaluations on benchmark programs\ndemonstrate that CG-FSPTA, significantly reduces both memory usage and\nexecution time while maintaining precision. In particular, by solving in the\nFSConsG, CG-FSPTA achieves an average memory reduction of 33.05\\% and\naccelerates flow-sensitive pointer analysis by 7.27x compared to the\nstate-of-art method. These experimental results underscore the efficacy of\nCG-FSPTA as a scalable solution to analyze large-scale software systems,\nestablishing a robust foundation for future advancements in efficient program\nanalysis frameworks."}
{"id": "2508.01758", "categories": ["cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.01758", "abs": "https://arxiv.org/abs/2508.01758", "authors": ["Pinaki Chakraborty", "Tristan Caulfield", "David Pym"], "title": "Causality and Decision-making: A Logical Framework for Systems and Security Modelling", "comment": "28 pages", "summary": "Causal reasoning is essential for understanding decision-making about the\nbehaviour of complex `ecosystems' of systems that underpin modern society, with\nsecurity -- including issues around correctness, safety, resilience, etc. --\ntypically providing critical examples. We present a theory of strategic\nreasoning about system modelling based on minimal structural assumptions and\nemploying the methods of transition systems, supported by a modal logic of\nsystem states in the tradition of van Benthem, Hennessy, and Milner, and\nvalidated through equivalence theorems. Our framework introduces an\nintervention operator and a separating conjunction to capture actual causal\nrelationships between component systems of the ecosystem, aligning naturally\nwith Halpern and Pearl's counterfactual approach based on Structural Causal\nModels. We illustrate the applicability through examples of of decision-making\nabout microservices in distributed systems. We discuss localized\ndecision-making through a separating conjunction. This work unifies a formal,\nminimalistic notion of system behaviour with a Halpern--Pearl-compatible theory\nof counterfactual reasoning, providing a logical foundation for studying\ndecision making about causality in complex interacting systems."}
{"id": "2508.01358", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01358", "abs": "https://arxiv.org/abs/2508.01358", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "An Empirical Validation of Open Source Repository Stability Metrics", "comment": null, "summary": "Over the past few decades, open source software has been continuously\nintegrated into software supply chains worldwide, drastically increasing\nreliance and dependence. Because of the role this software plays, it is\nimportant to understand ways to measure and promote its stability and potential\nfor sustainability. Recent work proposed the use of control theory to\nunderstand repository stability and evaluate repositories' ability to return to\nequilibrium after a disturbance such as the introduction of a new feature\nrequest, a spike in bug reports, or even the influx or departure of\ncontributors. This approach leverages commit frequency patterns, issue\nresolution rate, pull request merge rate, and community activity engagement to\nprovide a Composite Stability Index (CSI). While this framework has theoretical\nfoundations, there is no empirical validation of the CSI in practice. In this\npaper, we present the first empirical validation of the proposed CSI by\nexperimenting with 100 highly ranked GitHub repositories. Our results suggest\nthat (1) sampling weekly commit frequency pattern instead of daily is a more\nfeasible measure of commit frequency stability across repositories and (2)\nimproved statistical inferences (swapping mean with median), particularly with\nascertaining resolution and review times in issues and pull request, improves\nthe overall issue and pull request stability index. Drawing on our empirical\ndataset, we also derive data-driven half-width parameters that better align\nstability scores with real project behavior. These findings both confirm the\nviability of a control-theoretic lens on open-source health and provide\nconcrete, evidence-backed applications for real-world project monitoring tools."}
{"id": "2508.01866", "categories": ["cs.LO", "D.3.1; F.3.1; F.3.2; F.3.3; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.01866", "abs": "https://arxiv.org/abs/2508.01866", "authors": ["Berend van Starkenburg", "Henning Basold", "Chase Ford"], "title": "Separation Logic of Generic Resources via Sheafeology", "comment": "55 pages including appendix", "summary": "Separation logic was conceived in order to make the verification of pointer\nprograms scalable to large systems and it has proven extremely effective. The\nkey idea is that programs typically access only small parts of memory, allowing\nfor local reasoning. This idea is implemented in separation logic by extending\nfirst-order logic with separating connectives, which inspect local regions of\nmemory. It turns that this approach not only applies to pointer programs, but\nalso to programs involving other resource structures. Various theories have\nbeen put forward to extract and apply the ideas of separation logic more\nbroadly. This resulted in algebraic abstractions of memory and many variants of\nseparation logic for, e.g., concurrent programs and stochastic processes.\nHowever, none of the existing approaches formulate the combination of\nfirst-order logic with separating connectives in a theory that could\nimmediately yield program logics for different resources. In this paper, we\npropose a framework based on the idea that separation logic can obtained by\nmaking first-order logic resource-aware. First-order logic can be understood in\nterms of categorical logic, specifically fibrations. Our contribution is to\nmake these resource-aware by developing categorical logic internally in\ncategories of sheaves, which is what we call sheafeology. The role of sheaves\nis to model views on resources, through which resources can be localised and\ncombined, which enables the scalability promised by separation logic. We\ncontribute constructions of an internal fibration in sheaf categories that\nmodels predicates on resources, and that admits first-order and separating\nconnectives. Thereby, we attain a general framework of separation logic for\ngeneric resources, a claim we substantiate by instantiating our framework to\nvarious memory models and random variables."}
{"id": "2508.00864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00864", "abs": "https://arxiv.org/abs/2508.00864", "authors": ["Margarita Bugueño", "Gerard de Melo"], "title": "Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches", "comment": "7 pages, 3 figures, 3 tables. Appendix starts on page 10", "summary": "In document classification, graph-based models effectively capture document\nstructure, overcoming sequence length limitations and enhancing contextual\nunderstanding. However, most existing graph document representations rely on\nheuristics, domain-specific rules, or expert knowledge. Unlike previous\napproaches, we propose a method to learn data-driven graph structures,\neliminating the need for manual design and reducing domain dependence. Our\napproach constructs homogeneous weighted graphs with sentences as nodes, while\nedges are learned via a self-attention model that identifies dependencies\nbetween sentence pairs. A statistical filtering strategy aims to retain only\nstrongly correlated sentences, improving graph quality while reducing the graph\nsize. Experiments on three document classification datasets demonstrate that\nlearned graphs consistently outperform heuristic-based graphs, achieving higher\naccuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness\nof the statistical filtering in improving classification robustness. These\nresults highlight the potential of automatic graph generation over traditional\nheuristic approaches and open new directions for broader applications in NLP."}
{"id": "2508.01430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01430", "abs": "https://arxiv.org/abs/2508.01430", "authors": ["Kaveh Shahedi", "Matthew Khouzam", "Heng Li", "Maxime Lamothe", "Foutse Khomh"], "title": "From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool", "comment": null, "summary": "System tracing has become essential for understanding complex software\nbehavior in modern systems, yet sophisticated trace analysis tools face\nsignificant adoption gaps in industrial settings. Through a year-long\ncollaboration with Ericsson Montr\\'eal, developing TMLL (Trace-Server Machine\nLearning Library, now in the Eclipse Foundation), we investigated barriers to\ntrace analysis adoption. Contrary to assumptions about complexity or automation\nneeds, practitioners struggled with translating expert knowledge into\nactionable insights, integrating analysis into their workflows, and trusting\nautomated results they could not validate. We identified what we called the\nExcellence Paradox: technical excellence can actively impede adoption when\nconflicting with usability, transparency, and practitioner trust. TMLL\naddresses this through adoption-focused design that embeds expert knowledge in\ninterfaces, provides transparent explanations, and enables incremental\nadoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's\nintegration, and a survey of 40 industry and academic professionals revealed\nconsistent patterns: survey results showed that 77.5% prioritize quality and\ntrust in results over technical sophistication, while 67.5% prefer\nsemi-automated analysis with user control, findings supported by qualitative\nfeedback from industrial collaboration and external peer review. Results\nvalidate three core principles: cognitive compatibility, embedded expertise,\nand transparency-based trust. This challenges conventional capability-focused\ntool development, demonstrating that sustainable adoption requires\nreorientation toward adoption-focused design with actionable implications for\nautomated software engineering tools."}
{"id": "2508.02301", "categories": ["cs.LO", "68Q60, 68Q45", "F.3.1; D.3.1"], "pdf": "https://arxiv.org/pdf/2508.02301", "abs": "https://arxiv.org/abs/2508.02301", "authors": ["Marek Chalupa", "Thomas A. Henzinger", "Ana Oliveira da Costa"], "title": "Monitoring Hyperproperties over Observed and Constructed Traces", "comment": null, "summary": "We study the problem of monitoring at runtime whether a system fulfills a\nspecification defined by a hyperproperty, such as linearizability or variants\nof non-interference. For this purpose, we introduce specifications with both\npassive and active quantification over traces. While passive trace quantifiers\nrange over the traces that are observed, active trace quantifiers are\ninstantiated with \\emph{generator functions}, which are part of the\nspecification. Generator functions enable the monitor to construct traces that\nmay never be observed at runtime, such as the linearizations of a concurrent\ntrace. As specification language, we extend hypernode logic with trace\nquantifiers over generator functions and interpret these hypernode formulas\nover possibly infinite domains. We present a corresponding monitoring\nalgorithm, which we implemented and evaluated on a range of hyperproperties for\nconcurrency and security applications. Our method enables, for the first time,\nthe monitoring of asynchronous hyperproperties that contain alternating trace\nquantifiers."}
{"id": "2508.00889", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00889", "abs": "https://arxiv.org/abs/2508.00889", "authors": ["Hagyeong Shin", "Binoy Robin Dalal", "Iwona Bialynicka-Birula", "Navjot Matharu", "Ryan Muir", "Xingwei Yang", "Samuel W. K. Wong"], "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts", "comment": "Accepted for an oral presentation at Agentic & GenAI Evaluation KDD\n  2025: KDD workshop on Evaluation and Trustworthiness of Agentic and\n  Generative AI Models", "summary": "Large language models (LLMs) are known to hallucinate, producing natural\nlanguage outputs that are not grounded in the input, reference materials, or\nreal-world knowledge. In enterprise applications where AI features support\nbusiness decisions, such hallucinations can be particularly detrimental. LLMs\nthat analyze and summarize contact center conversations introduce a unique set\nof challenges for factuality evaluation, because ground-truth labels often do\nnot exist for analytical interpretations about sentiments captured in the\nconversation and root causes of the business problems. To remedy this, we first\nintroduce a \\textbf{3D} -- \\textbf{Decompose, Decouple, Detach} -- paradigm in\nthe human annotation guideline and the LLM-judges' prompt to ground the\nfactuality labels in linguistically-informed evaluation criteria. We then\nintroduce \\textbf{FECT}, a novel benchmark dataset for \\textbf{F}actuality\n\\textbf{E}valuation of Interpretive AI-Generated \\textbf{C}laims in Contact\nCenter Conversation \\textbf{T}ranscripts, labeled under our 3D paradigm.\nLastly, we report our findings from aligning LLM-judges on the 3D paradigm.\nOverall, our findings contribute a new approach for automatically evaluating\nthe factuality of outputs generated by an AI system for analyzing contact\ncenter conversations."}
{"id": "2508.01443", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01443", "abs": "https://arxiv.org/abs/2508.01443", "authors": ["Jingzhi Gong", "Rafail Giavrimis", "Paul Brookes", "Vardan Voskanyan", "Fan Wu", "Mari Ashiga", "Matthew Truscott", "Mike Basios", "Leslie Kanthan", "Jie Xu", "Zheng Wang"], "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective", "comment": "Submitted to ASE'25 Industry Showcase", "summary": "There is a growing interest in leveraging large language models (LLMs) for\nautomated code optimization. However, industrial platforms deploying multiple\nLLMs face a critical challenge: prompts optimized for one LLM often fail with\nothers, requiring expensive model-specific prompt engineering. This cross-model\nprompt engineering bottleneck severely limits the practical deployment of\nmulti-LLM optimization systems in production environments. To address this, we\nintroduce Meta-Prompted Code Optimization (MPCO), a framework that\nautomatically generates high-quality, task-specific prompts across diverse LLMs\nwhile maintaining industrial efficiency requirements. MPCO leverages\nmeta-prompting to dynamically synthesize context-aware optimization prompts by\nintegrating project metadata, task requirements, and LLM-specific contexts, and\nit seamlessly deploys on the ARTEMIS industrial platform for automated\nvalidation and scaling.\n  Our comprehensive evaluation on five real-world codebases with 366 hours of\nruntime benchmarking demonstrates MPCO's effectiveness: it achieves overall\nperformance improvements up to 19.06% with the best statistical rank across all\nsystems compared to baseline methods. Analysis shows that 96% of the\ntop-performing optimizations stem from meaningful edits. Through systematic\nablation studies and meta-prompter sensitivity analysis, we identify that\ncomprehensive context integration is essential for effective meta-prompting,\nand that all three major LLMs can serve effectively as meta-prompters,\nproviding actionable insights for industrial practitioners."}
{"id": "2302.06506", "categories": ["cs.FL", "cs.DS", "cs.LO"], "pdf": "https://arxiv.org/pdf/2302.06506", "abs": "https://arxiv.org/abs/2302.06506", "authors": ["Nicola Cotumaccio"], "title": "A Myhill-Nerode Theorem for Generalized Automata, with Applications to Pattern Matching and Compression", "comment": null, "summary": "The model of generalized automata, introduced by Eilenberg in 1974, allows\nrepresenting a regular language more concisely than conventional automata by\nallowing edges to be labeled not only with characters, but also strings.\nGiammaresi and Montalbano introduced a notion of determinism for generalized\nautomata [STACS 1995]. While generalized deterministic automata retain many\nproperties of conventional deterministic automata, the uniqueness of a minimal\ngeneralized deterministic automaton is lost.\n  In the first part of the paper, we show that the lack of uniqueness can be\nexplained by introducing a set $ \\mathcal{W(A)} $ associated with a generalized\nautomaton $ \\mathcal{A} $. By fixing $ \\mathcal{W(A)} $, we are able to derive\nfor the first time a full Myhill-Nerode theorem for generalized automata, which\ncontains the textbook Myhill-Nerode theorem for conventional automata as a\ndegenerate case.\n  In the second part of the paper, we show that the set $ \\mathcal{W(A)} $\nleads to applications for pattern matching and data compression. Wheeler\nautomata [TCS 2017, SODA 2020] are a popular class of automata that can be\ncompactly stored using $ e \\log \\sigma (1 + o(1)) + O(e) $ bits ($ e $ being\nthe number of edges, $ \\sigma $ being the size of the alphabet) in such a way\nthat pattern matching queries can be solved in $ \\tilde{O}(m) $ time ($ m $\nbeing the length of the pattern). In the paper, we show how to extend these\nresults to generalized automata. More precisely, a Wheeler generalized automata\ncan be stored using $ \\mathfrak{e} \\log \\sigma (1 + o(1)) + O(e + rn) $ bits so\nthat pattern matching queries can be solved in $ \\tilde{O}(r m) $ time, where $\n\\mathfrak{e} $ is the total length of all edge labels, $ r $ is the maximum\nlength of an edge label and $ n $ is the number of states."}
{"id": "2508.00924", "categories": ["cs.CL", "68T05, 68T50", "I.2.6; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.00924", "abs": "https://arxiv.org/abs/2508.00924", "authors": ["Ernesto L. Estevanell-Valladares", "Suilan Estevez-Velarde", "Yoan Gutiérrez", "Andrés Montoyo", "Ruslan Mitkov"], "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML", "comment": "17 pages, 10 figures, 7 tables. Preprint. Under review at EMNLP 2025.\n  This is not the final version", "summary": "Experts in machine learning leverage domain knowledge to navigate decisions\nin model selection, hyperparameter optimisation, and resource allocation. This\nis particularly critical for fine-tuning language models (LMs), where repeated\ntrials incur substantial computational overhead and environmental impact.\nHowever, no existing automated framework simultaneously tackles the entire\nmodel selection and HPO task for resource-efficient LM fine-tuning. We\nintroduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past\nexperiences to optimise discriminative and generative LM fine-tuning pipelines\nefficiently. XAutoLM learns from stored successes and failures by extracting\ntask- and system-level meta-features to bias its sampling toward fruitful\nconfigurations and away from costly dead ends. On four text classification and\ntwo question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak\nF1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error\nratios by up to sevenfold, and uncovers up to 50% more pipelines above the\nzero-shot Pareto front. In contrast, simpler memory-based baselines suffer\nnegative transfer. We release XAutoLM and our experience store to catalyse\nresource-efficient, Green AI fine-tuning in the NLP community."}
{"id": "2508.01472", "categories": ["cs.SE", "68N99", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.01472", "abs": "https://arxiv.org/abs/2508.01472", "authors": ["Lukas Kirschner", "Ezekiel Soremekun"], "title": "Directed Grammar-Based Test Generation", "comment": "21 pages, 10 figures, 13 tables, submitted to IEEE Transactions on\n  Software Engineering, for replication package, see\n  https://tinyurl.com/FDLoop-V3", "summary": "To effectively test complex software, it is important to generate\ngoal-specific inputs, i.e., inputs that achieve a specific testing goal.\nHowever, most state-of-the-art test generators are not designed to target\nspecific goals. Notably, grammar-based test generators, which (randomly)\nproduce syntactically valid inputs via an input specification (i.e., grammar)\nhave a low probability of achieving an arbitrary testing goal. This work\naddresses this challenge by proposing an automated test generation approach\n(called FdLoop) which iteratively learns relevant input properties from\nexisting inputs to drive the generation of goal-specific inputs. Given a\ntesting goal, FdLoop iteratively selects, evolves and learn the input\ndistribution of goal-specific test inputs via test feedback and a probabilistic\ngrammar. We concretize FdLoop for four testing goals, namely unique code\ncoverage, input-to-code complexity, program failures (exceptions) and long\nexecution time. We evaluate FdLoop using three (3) well-known input formats\n(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,\nFdLoop outperforms all five tested baselines namely the baseline grammar-based\ntest generators (random, probabilistic and inverse-probabilistic methods),\nEvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best\nbaseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that\nthe main components of FdLoop (i.e., input mutator, grammar mutator and test\nfeedbacks) contribute positively to its effectiveness. Finally, our evaluation\ndemonstrates that FdLoop effectively achieves single testing goals (revealing\nerroneous behaviors, generating complex inputs, or inducing long execution\ntime) and scales to multiple testing goals across varying parameter settings."}
{"id": "2508.01005", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01005", "abs": "https://arxiv.org/abs/2508.01005", "authors": ["Yiqun Chen", "Erhan Zhang", "Lingyong Yan", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Jiaxin Mao"], "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation", "comment": null, "summary": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG."}
{"id": "2508.01489", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01489", "abs": "https://arxiv.org/abs/2508.01489", "authors": ["SK. Golam Saroar", "Waseefa Ahmed", "Elmira Onagh", "Maleknaz Nayebi"], "title": "GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development", "comment": "SANER 2025 journal first paper", "summary": "GitHub, a central hub for collaborative software development, has\nrevolutionized the open-source software (OSS) ecosystem through its GitHub\nMarketplace, a platform launched in 2017 to host automation tools aimed at\nenhancing the efficiency and scalability of software projects. As the adoption\nof automation in OSS production grows, understanding the trends,\ncharacteristics, and underlying dynamics of this marketplace has become vital.\nFurthermore, despite the rich repository of academic research on software\nautomation, a disconnect persists between academia and industry practices. This\nstudy seeks to bridge this gap by providing a systematic analysis of the GitHub\nMarketplace, comparing trends observed in industry tools with advancements\nreported in academic literature, and identifying areas where academia can\ncontribute to practical innovation."}
{"id": "2508.01006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01006", "abs": "https://arxiv.org/abs/2508.01006", "authors": ["Farah Adeeba", "Brian Dillon", "Hassan Sajjad", "Rajesh Bhatt"], "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu", "comment": null, "summary": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages."}
{"id": "2508.01492", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01492", "abs": "https://arxiv.org/abs/2508.01492", "authors": ["Angel C. Chavez-Moreno", "Cristina L. Abad"], "title": "OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications", "comment": "8 pages, 7 figures, 13th IEEE International Conference on Cloud\n  Engineering (IC2E 2025, accepted, to appear)", "summary": "Function-as-a-Service (FaaS) is at the core of serverless computing, enabling\ndevelopers to easily deploy applications without managing computing resources.\nWith an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless\nFramework use YAML configurations to define and deploy APIs, tasks, workflows,\nand event-driven applications on cloud providers, promoting zero-friction\ndevelopment. As with any rapidly evolving ecosystem, there is a need for\nupdated insights into how these tools are used in real-world projects. Building\non the methodology established by the Wonderless dataset for serverless\ncomputing (and applying multiple new filtering steps), OpenLambdaVerse\naddresses this gap by creating a dataset of current GitHub repositories that\nuse the Serverless Framework in applications that contain one or more AWS\nLambda functions. We then analyze and characterize this dataset to get an\nunderstanding of the state-of-the-art in serverless architectures based on this\nstack. Through this analysis we gain important insights on the size and\ncomplexity of current applications, which languages and runtimes they employ,\nhow are the functions triggered, the maturity of the projects, and their\nsecurity practices (or lack of). OpenLambdaVerse thus offers a valuable,\nup-to-date resource for both practitioners and researchers that seek to better\nunderstand evolving serverless workloads."}
{"id": "2508.01096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01096", "abs": "https://arxiv.org/abs/2508.01096", "authors": ["Michael Farag", "Patrick Halina", "Andrey Zaytsev", "Alekhya Munagala", "Imtihan Ahmed", "Junhao Wang"], "title": "Cross-Domain Web Information Extraction at Pinterest", "comment": null, "summary": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives."}
{"id": "2508.01523", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01523", "abs": "https://arxiv.org/abs/2508.01523", "authors": ["Ningzhi Tang", "Emory Smith", "Yu Huang", "Collin McMillan", "Toby Jia-Jun Li"], "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification", "comment": null, "summary": "This paper presents a study of using large language models (LLMs) in\nmodifying existing code. While LLMs for generating code have been widely\nstudied, their role in code modification remains less understood. Although\n\"prompting\" serves as the primary interface for developers to communicate\nintents to LLMs, constructing effective prompts for code modification\nintroduces challenges different from generation. Prior work suggests that\nnatural language summaries may help scaffold this process, yet such approaches\nhave been validated primarily in narrow domains like SQL rewriting. This study\ninvestigates two prompting strategies for LLM-assisted code modification:\nDirect Instruction Prompting, where developers describe changes explicitly in\nfree-form language, and Summary-Mediated Prompting, where changes are made by\nediting the generated summaries of the code. We conducted an exploratory study\nwith 15 developers who completed modification tasks using both techniques\nacross multiple scenarios. Our findings suggest that developers followed an\niterative workflow: understanding the code, localizing the edit, and validating\noutputs through execution or semantic reasoning. Each prompting strategy\npresented trade-offs: direct instruction prompting was more flexible and easier\nto specify, while summary-mediated prompting supported comprehension, prompt\nscaffolding, and control. Developers' choice of strategy was shaped by task\ngoals and context, including urgency, maintainability, learning intent, and\ncode familiarity. These findings highlight the need for more usable prompt\ninteractions, including adjustable summary granularity, reliable summary-code\ntraceability, and consistency in generated summaries."}
{"id": "2508.01159", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01159", "abs": "https://arxiv.org/abs/2508.01159", "authors": ["Liam G. McCoy", "Fateme Nateghi Haredasht", "Kanav Chopra", "David Wu", "David JH Wu", "Abass Conteh", "Sarita Khemani", "Saloni Kumar Maharaj", "Vishnu Ravi", "Arth Pahwa", "Yingjie Weng", "Leah Rosengaus", "Lena Giang", "Kelvin Zhenghao Li", "Olivia Jee", "Daniel Shirvani", "Ethan Goh", "Jonathan H. Chen"], "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates", "comment": null, "summary": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication."}
{"id": "2508.01550", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01550", "abs": "https://arxiv.org/abs/2508.01550", "authors": ["Zhilong Chen", "Chengzong Zhao", "Boyuan Chen", "Dayi Lin", "Yihao Chen", "Arthur Leung", "Gopi Krishnan Rajbahadur", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale", "comment": null, "summary": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks."}
{"id": "2508.01161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01161", "abs": "https://arxiv.org/abs/2508.01161", "authors": ["Jiyu Chen", "Necva Bölücü", "Sarvnaz Karimi", "Diego Mollá", "Cécile L. Paris"], "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages", "comment": "In Proceedings of the 19th International Workshop on Semantic\n  Evaluation (SemEval-2025), Vienna, Austria. Association for Computational\n  Linguistics", "summary": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language."}
{"id": "2508.01974", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01974", "abs": "https://arxiv.org/abs/2508.01974", "authors": ["Jiahao Zhang", "Xiao Cheng", "Yuxiang Lei"], "title": "Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis", "comment": null, "summary": "Flow-sensitive pointer analysis constitutes an essential component of precise\nprogram analysis for accurately modeling pointer behaviors by incorporating\ncontrol flows. Flow-sensitive pointer analysis is extensively used in alias\nanalysis, taint analysis, program understanding, compiler optimization, etc.\nExisting flow-sensitive pointer analysis approaches, which are conducted based\non control flow graphs, have significantly advanced the precision of pointer\nanalysis via sophisticated techniques to leverage control flow information.\nHowever, they inevitably suffer from computational inefficiencies when\nresolving points-to information due to the inherent complex structures of\ncontrol flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph\n(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of\ncontrol-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to\nleverage the structural advantages of set-constraint graphs (which are commonly\nused in flow-insensitive pointer analysis) while keeping the flow sensitivity\nof variable definitions and uses, allowing the incorporation of sophisticated\ngraph optimization and dynamic solving techniques. In this way, CG-FSPTA\nachieves significant efficiency improvements while keeping the precision of\nflow-sensitive analysis. Experimental evaluations on benchmark programs\ndemonstrate that CG-FSPTA, significantly reduces both memory usage and\nexecution time while maintaining precision. In particular, by solving in the\nFSConsG, CG-FSPTA achieves an average memory reduction of 33.05\\% and\naccelerates flow-sensitive pointer analysis by 7.27x compared to the\nstate-of-art method. These experimental results underscore the efficacy of\nCG-FSPTA as a scalable solution to analyze large-scale software systems,\nestablishing a robust foundation for future advancements in efficient program\nanalysis frameworks."}
{"id": "2508.01198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios."}
{"id": "2508.02023", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02023", "abs": "https://arxiv.org/abs/2508.02023", "authors": ["Huashan Lei", "Guanping Xiao", "Yepang Liu", "Zheng Zheng"], "title": "PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades", "comment": "52 pages, 33 figures", "summary": "Python third-party libraries (TPLs) are essential in modern software\ndevelopment, but upgrades often cause compatibility issues, leading to system\nfailures. These issues fall into two categories: version compatibility issues\n(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect\ndependency conflicts but overlook code-level incompatibilities, with no\nsolution fully automating the inference of compatible versions for both VCIs\nand CCIs. To fill this gap, we propose PCREQ, the first approach to\nautomatically infer compatible requirements by combining version and code\ncompatibility analysis. PCREQ integrates six modules: knowledge acquisition,\nversion compatibility assessment, invoked APIs and modules extraction, code\ncompatibility assessment, version change, and missing TPL completion. PCREQ\ncollects candidate versions, checks for conflicts, identifies API usage,\nevaluates code compatibility, and iteratively adjusts versions to generate a\ncompatible requirements.txt with a detailed repair report. To evaluate PCREQ,\nwe construct REQBench, a large-scale benchmark with 2,095 upgrade test cases\n(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%\ninference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and\nLLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each\ncase from REQBench in 60.79s on average, demonstrating practical efficiency.\nPCREQ significantly reduces manual effort in troubleshooting upgrades,\nadvancing Python dependency maintenance automation."}
{"id": "2508.01213", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01213", "abs": "https://arxiv.org/abs/2508.01213", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level."}
{"id": "2508.02144", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02144", "abs": "https://arxiv.org/abs/2508.02144", "authors": ["Yusaku Kato", "Norihiro Yoshida", "Erina Makihara", "Katsuro Inoue"], "title": "BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games", "comment": "4 pages, 5 figures", "summary": "Open-world video games present a broader search space than other games,\nposing challenges for test automation. Fuzzing, which generates new inputs by\nmutating an initial input, is commonly used to uncover failures. In this study,\nwe proposed BiFuzz, a two-stage fuzzer designed for automated testing of\nopen-world video games, and investigated its effectiveness. The results\nrevealed that BiFuzz mutated the overall strategy of gameplay and test cases,\nincluding actual movement paths, step by step. Consequently, BiFuzz can detect\n`stucking' failures. The tool and its video are at\nhttps://github.com/Yusaku-Kato/BiFuzz."}
{"id": "2508.01222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01222", "abs": "https://arxiv.org/abs/2508.01222", "authors": ["Ethan Hsu", "Hong Meng Yam", "Ines Bouissou", "Aaron Murali John", "Raj Thota", "Josh Koe", "Vivek Sarath Putta", "G K Dharesan", "Alexander Spangher", "Shikhar Murty", "Tenghao Huang", "Christopher D. Manning"], "title": "WebDS: An End-to-End Benchmark for Web-based Data Science", "comment": "14 pages", "summary": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science."}
{"id": "2508.02167", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02167", "abs": "https://arxiv.org/abs/2508.02167", "authors": ["Yuxuan Wang", "Cristian Tirelli", "Giovanni Ansaloni", "Laura Pozzi", "David Atienza"], "title": "An MLIR-based Compilation Framework for Control Flow Management on CGRAs", "comment": null, "summary": "Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility\nand efficiency, making them well-suited for the acceleration of intensive\nworkloads. Nevertheless, a key barrier towards their widespread adoption is\nposed by CGRA compilation, which must cope with a multi-dimensional space\nspanning both the spatial and the temporal domains. Indeed, state-of-the-art\ncompilers are limited in scope as they mostly deal with the data flow of\napplications, while having little or no support for control flow. Hence, they\nmostly target the mapping of single loops and/or delegate the management of\ncontrol flow divergences to ad-hoc hardware units.\n  Conversely, in this paper we show that control flow can be effectively\nmanaged and optimized at the compilation level, allowing for a broad set of\napplications to be targeted while being hardware-agnostic and achieving high\nperformance. We embody our methodology in a modular compilation framework\nconsisting of transformation and optimization passes, enabling support for\napplications with arbitrary control flows running on abstract CGRA meshes. We\nalso introduce a novel mapping methodology that acts as a compilation back-end,\naddressing the limitations in available CGRA hardware resources and\nguaranteeing a feasible solution in the compilation process. Our framework\nachieves up to 2.1X speedups over state-of-the-art approaches, purely through\ncompilation optimizations."}
{"id": "2508.01245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01245", "abs": "https://arxiv.org/abs/2508.01245", "authors": ["Yue Chen", "Minghua He", "Fangkai Yang", "Pu Zhao", "Lu Wang", "Yu Kang", "Yifei Dong", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework", "comment": null, "summary": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability."}
{"id": "2508.02176", "categories": ["cs.SE", "cs.HC", "D.2.3; D.2.6; D.2.5; H.5.2"], "pdf": "https://arxiv.org/pdf/2508.02176", "abs": "https://arxiv.org/abs/2508.02176", "authors": ["Andrew Tropin"], "title": "Highly Interactive Testing for Uninterrupted Development Flow", "comment": "12 pages, ICFP-2025", "summary": "Highly interactive development environments (HIDEs) enable uninterrupted\ndevelopment flow through continuous program evolution and rapid hypothesis\nchecking. However, traditional testing approaches -- typically executed\nseparately via CLI -- isolate tests from HIDE tooling (interactive debuggers,\nvalue and stack inspectors, etc.) and introduce disruptive delays due to coarse\nexecution granularity and lack of runtime context. This disconnect breaks\ndevelopment flow by exceeding critical attention thresholds. In this paper we\npresent a library that provides runtime representation for tests, allowing\ntight integration with HIDEs, and enabling immediate access to HIDE tooling in\nthe context of test failure. We then describe development workflows enhanced\nwith testing and demonstrate how they achieve subsecond test reexecution times\ncrucial for maintaining developer focus."}
{"id": "2508.01263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives."}
{"id": "2508.02233", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02233", "abs": "https://arxiv.org/abs/2508.02233", "authors": ["Vincenzo De Martino", "Joel Castaño", "Fabio Palomba", "Xavier Franch", "Silverio Martínez-Fernández"], "title": "A Methodological Framework for LLM-Based Mining of Software Repositories", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research."}
{"id": "2508.01290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01290", "abs": "https://arxiv.org/abs/2508.01290", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Yanyan Wang", "Hongye Tan", "Jiye Liang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information."}
{"id": "2508.02279", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02279", "abs": "https://arxiv.org/abs/2508.02279", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "title": "Dialogue Systems Engineering: A Survey and Future Directions", "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering."}
{"id": "2508.01302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01302", "abs": "https://arxiv.org/abs/2508.01302", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment."}
{"id": "2508.02335", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.02335", "abs": "https://arxiv.org/abs/2508.02335", "authors": ["Matteo Cancellieri", "Martin Docekal", "David Pride", "Morane Gruenpeter", "David Douard", "Petr Knoth"], "title": "Interoperable verification and dissemination of software assets in repositories using COAR Notify", "comment": "8 pages. Presented at the 20th International Conference on Open\n  Repositories, June 15-18 2025, Chicago, Illinois, USA", "summary": "The discoverability, attribution, and reusability of open research software\nare often hindered by its obscurity within academic manuscripts. To address\nthis, the SoFAIR project (2024-2025) introduces a comprehensive workflow\nleveraging machine learning tools for extracting software mentions from\nresearch papers. The project integrates repository systems, authors, and\nservices like HAL and Software Heritage to ensure proper archiving, citation,\nand accessibility of research software in alignment with FAIR principles. To\nenable interoperable communication across the various systems we present an\nintegration of the COAR Notify Protocol, which facilitates automated,\ninteroperable communication among repositories and authors to validate and\ndisseminate software mentions. This paper outlines the SoFAIR workflow and the\nimplementation of the COAR Notify Protocol, emphasising its potential to\nenhance the visibility and credibility of research software as first-class\nbibliographic records."}
{"id": "2508.01309", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01309", "abs": "https://arxiv.org/abs/2508.01309", "authors": ["Weibo Zhou", "Lingbo Li", "Shangsong Liang"], "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation", "comment": null, "summary": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains."}
{"id": "2508.02338", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02338", "abs": "https://arxiv.org/abs/2508.02338", "authors": ["Jiahui Wu", "Chengjie Lu", "Aitor Arrieta", "Shaukat Ali", "Thomas Peyrucain"], "title": "Vision Language Model-based Testing of Industrial Autonomous Mobile Robots", "comment": null, "summary": "Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,\nwarehouses, retail spaces, and offices), where they work alongside humans.\nGiven that human behavior can be unpredictable and that AMRs may not have been\ntrained to handle all possible unknown and uncertain behaviors, it is important\nto test AMRs under a wide range of human interactions to ensure their safe\nbehavior. Moreover, testing in real environments with actual AMRs and humans is\noften costly, impractical, and potentially hazardous (e.g., it could result in\nhuman injury). To this end, we propose a Vision Language Model (VLM)-based\ntesting approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.\nBased on the functional and safety requirements, RVSG uses the VLM to generate\ndiverse human behaviors that violate these requirements. We evaluated RVSG with\nseveral requirements and navigation routes in a simulator using the latest AMR\nfrom PAL Robotics. Our results show that, compared with the baseline, RVSG can\neffectively generate requirement-violating scenarios. Moreover, RVSG-generated\nscenarios increase variability in robot behavior, thereby helping reveal their\nuncertain behaviors."}
{"id": "2508.01317", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01317", "abs": "https://arxiv.org/abs/2508.01317", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales."}
{"id": "2508.02397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02397", "abs": "https://arxiv.org/abs/2508.02397", "authors": ["Lida Zhao", "Chaofan Li", "Yueming Wu", "Lyuye Zhang", "Jiahui Wu", "Chengwei Liu", "Sen Chen", "Yutao Hu", "Zhengzi Xu", "Yi Liu", "Jingquan Ge", "Jun Sun", "Yang Liu"], "title": "JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis", "comment": null, "summary": "While reusing third-party libraries (TPL) facilitates software development,\nits chaotic management has brought great threats to software maintenance and\nthe unauthorized use of source code also raises ethical problems such as\nmisconduct on copyrighted code. To identify TPL reuse in projects, Software\nComposition Analysis (SCA) is employed, and two categories of SCA techniques\nare used based on how TPLs are introduced: clone-based SCA and\npackage-manager-based SCA (PM-based SCA). Although introducing TPLs by clones\nis prevalent in Java, no clone-based SCA tools are specially designed for Java.\nAlso, directly applying clone-based SCA techniques from other tools is\nproblematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA\ntool that aims to accurately and comprehensively identify instances of TPL\nreuse introduced by source code clones in Java projects. JC-Finder achieves\nboth accuracy and efficiency in identifying TPL reuse from code cloning by\ncapturing features at the class level, maintaining inter-function\nrelationships, and excluding trivial or duplicated elements. To evaluate the\nefficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as\nreference data and tested the TPL reuse of 1,000 GitHub projects. The result\nshows that JC-Finder achieved an F1-score of 0.818, outperforming the other\nfunction-level tool by 0.427. The average time taken for resolving TPL reuse is\n14.2 seconds, which is approximately 9 times faster than the other tool. We\nfurther applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code\nclones in 789 projects (about 9.89% of all projects) and identifying a total of\n2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not\nexplicitly declared in package managers."}
{"id": "2508.01326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01326", "abs": "https://arxiv.org/abs/2508.01326", "authors": ["Xuemiao Zhang", "Chengying Tu", "Can Ren", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "Large-Scale Diverse Synthesis for Mid-Training", "comment": null, "summary": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale."}
{"id": "2508.02407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02407", "abs": "https://arxiv.org/abs/2508.02407", "authors": ["Xinyi Wang", "Qinghua Xu", "Paolo Arcaini", "Shaukat Ali", "Thomas Peyrucain"], "title": "Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots", "comment": null, "summary": "Robots are increasingly becoming part of our daily lives, interacting with\nboth the environment and humans to perform their tasks. The software of such\nrobots often undergoes upgrades, for example, to add new functionalities, fix\nbugs, or delete obsolete functionalities. As a result, regression testing of\nrobot software becomes necessary. However, determining the expected correct\nbehavior of robots (i.e., a test oracle) is challenging due to the potentially\nunknown environments in which the robots must operate. To address this\nchallenge, machine learning (ML)-based test oracles present a viable solution.\nThis paper reports on the development of a test oracle to support regression\ntesting of autonomous mobile robots built by PAL Robotics (Spain), using\nquantum machine learning (QML), which enables faster training and the\nconstruction of more precise test oracles. Specifically, we propose a hybrid\nframework, QuReBot, that combines both quantum reservoir computing (QRC) and a\nsimple neural network, inspired by residual connection, to predict the expected\nbehavior of a robot. Results show that QRC alone fails to converge in our case,\nyielding high prediction error. In contrast, QuReBot converges and achieves 15%\nreduction of prediction error compared to the classical neural network\nbaseline. Finally, we further examine QuReBot under different configurations\nand offer practical guidance on optimal settings to support future robot\nsoftware testing."}
{"id": "2508.01370", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01370", "abs": "https://arxiv.org/abs/2508.01370", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights."}
{"id": "2508.02455", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02455", "abs": "https://arxiv.org/abs/2508.02455", "authors": ["Daniele Cipollone", "Egor Bogomolov", "Arie van Deursen", "Maliheh Izadi"], "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs", "comment": null, "summary": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance."}
{"id": "2508.01401", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01401", "abs": "https://arxiv.org/abs/2508.01401", "authors": ["Ahmad Rezaie Mianroodi", "Amirali Rezaie", "Niko Grisel Todorov", "Cyril Rakovski", "Frank Rudzicz"], "title": "MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs", "comment": "7 pages excluding references and appendices", "summary": "Physicians spend significant time documenting clinical encounters, a burden\nthat contributes to professional burnout. To address this, robust automation\ntools for medical documentation are crucial. We introduce MedSynth -- a novel\ndataset of synthetic medical dialogues and notes designed to advance the\nDialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.\nInformed by an extensive analysis of disease distributions, this dataset\nincludes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We\ndemonstrate that our dataset markedly enhances the performance of models in\ngenerating medical notes from dialogues, and dialogues from medical notes. The\ndataset provides a valuable resource in a field where open-access,\nprivacy-compliant, and diverse training data are scarce. Code is available at\nhttps://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available\nat https://huggingface.co/datasets/Ahmad0067/MedSynth."}
{"id": "2508.02473", "categories": ["cs.SE", "cs.LG", "68N30", "D.2.3; D.1.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2508.02473", "abs": "https://arxiv.org/abs/2508.02473", "authors": ["Xinfang Chen", "Siyang Xiao", "Xianying Zhu", "Junhong Xie", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li", "Peng Di"], "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs", "comment": "13 pages", "summary": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY."}
{"id": "2508.01411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01411", "abs": "https://arxiv.org/abs/2508.01411", "authors": ["Rania Al-Sabbagh"], "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations", "comment": null, "summary": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts."}
{"id": "2508.02487", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02487", "abs": "https://arxiv.org/abs/2508.02487", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Commit Stability as a Signal for Risk in Open-Source Projects", "comment": null, "summary": "Open source software (OSS) generates trillions of dollars in economic value\nand has become essential to technical infrastructures worldwide. As\norganizations increasingly depend on OSS, understanding project evolution is\ncritical. While existing metrics provide insights into project health, one\ndimension remains understudied: project resilience -- the ability to return to\nnormal operations after disturbances such as contributor departures, security\nvulnerabilities, and bug report spikes. We hypothesize that stable commit\npatterns reflect underlying project characteristics such as mature governance,\nsustained contributors, and robust development processes that enable\nresilience. Building on the Composite Stability Index (CSI) framework, we\nempirically validate commit frequency patterns across 100 highly ranked\nrepositories. Our findings reveal that only 2\\% of repositories exhibit daily\nstability, 29\\% achieve weekly stability, and 50\\% demonstrate monthly\nstability, while half remain unstable across all temporal levels. Programming\nlanguages and blockchain applications were the most stable. We identified two\nexemplary repositories that achieved stability at all three granularities,\nwhose governance models, CI cadence, and release policies could serve as\nreference frameworks. We observed that large yearly commit throughput does not\nnecessarily correlate with stability. Beyond commits, stability can be enriched\nwith issue-resolution times, PR merge rates, and community-engagement metrics\nto broaden resilience assessment and sharpen stability-based risk evaluation."}
{"id": "2508.01412", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01412", "abs": "https://arxiv.org/abs/2508.01412", "authors": ["Jinhao Pan", "Chahat Raj", "Ziwei Zhu"], "title": "Discovering Bias Associations through Open-Ended LLM Generations", "comment": null, "summary": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation"}
{"id": "2508.02497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02497", "abs": "https://arxiv.org/abs/2508.02497", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation", "comment": null, "summary": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation."}
{"id": "2508.01424", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01424", "abs": "https://arxiv.org/abs/2508.01424", "authors": ["Haonan Bian", "Yutao Qi", "Rui Yang", "Yuanxi Che", "Jiaqian Wang", "Heming Xia", "Ranran Zhen"], "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches."}
{"id": "2508.02541", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02541", "abs": "https://arxiv.org/abs/2508.02541", "authors": ["Peter Hamfelt", "Ricardo Britto", "Lincoln Rocha", "Camilo Almendra"], "title": "Automatic Identification of Machine Learning-Specific Code Smells", "comment": null, "summary": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment."}
{"id": "2508.01450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01450", "abs": "https://arxiv.org/abs/2508.01450", "authors": ["Xinlin Zhuang", "Feilong Tang", "Haolin Yang", "Ming Hu", "Huifa Li", "Haochen Xue", "Yichen Li", "Junjun He", "Zongyuan Ge", "Ying Qian", "Imran Razzak"], "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data", "comment": "preprint, under review", "summary": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ."}
{"id": "2508.02611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02611", "abs": "https://arxiv.org/abs/2508.02611", "authors": ["Vali Tawosia", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "Meta-RAG on Large Codebases Using Code Summarization", "comment": null, "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance."}
{"id": "2508.01473", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01473", "abs": "https://arxiv.org/abs/2508.01473", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks."}
{"id": "2508.01480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01480", "abs": "https://arxiv.org/abs/2508.01480", "authors": ["Dimitra Panou", "Alexandros C. Dimopoulos", "Manolis Koubarakis", "Martin Reczko"], "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach", "comment": null, "summary": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4."}
{"id": "2508.01486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01486", "abs": "https://arxiv.org/abs/2508.01486", "authors": ["Vallabhaneni Raj Kumar", "Ashwin S", "Supriya Manna", "Niladri Sett", "Cheedella V S N M S Hema Harshitha", "Kurakula Harshitha", "Anand Kumar Sharma", "Basina Deepakraj", "Tanuj Sarkar", "Bondada Navaneeth Krishna", "Samanthapudi Shakeer"], "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu", "comment": "work under review", "summary": "In the Indian subcontinent, Telugu, one of India's six classical languages,\nis the most widely spoken Dravidian Language. Despite its 96 million speaker\nbase worldwide, Telugu remains underrepresented in the global NLP and Machine\nLearning landscape, mainly due to lack of high-quality annotated resources.\nThis work introduces TeSent, a comprehensive benchmark dataset for sentiment\nclassification, a key text classification problem, in Telugu. TeSent not only\nprovides ground truth labels for the sentences, but also supplements with\nprovisions for evaluating explainability and fairness, two critical\nrequirements in modern-day machine learning tasks. We scraped Telugu texts\ncovering multiple domains from various social media platforms, news websites\nand web-blogs to preprocess and generate 26,150 sentences, and developed a\ncustom-built annotation platform and a carefully crafted annotation protocol\nfor collecting the ground truth labels along with their human-annotated\nrationales. We then fine-tuned several SOTA pre-trained models in two ways:\nwith rationales, and without rationales. Further, we provide a detailed\nplausibility and faithfulness evaluation suite, which exploits the rationales,\nfor six widely used post-hoc explainers applied on the trained models. Lastly,\nwe curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate\nfairness of Telugu sentiment and emotion related NLP tasks, and provide a\nfairness evaluation suite for the trained classifier models. Our experimental\nresults suggest that training with rationales may improve model accuracy,\nreduce bias in models, and make the explainers' output more aligned to human\nreasoning."}
{"id": "2508.01491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01491", "abs": "https://arxiv.org/abs/2508.01491", "authors": ["Zhivar Sourati", "Alireza S. Ziabari", "Morteza Dehghani"], "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought", "comment": null, "summary": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability."}
{"id": "2508.01503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01503", "abs": "https://arxiv.org/abs/2508.01503", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "comment": null, "summary": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction."}
{"id": "2508.01541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01541", "abs": "https://arxiv.org/abs/2508.01541", "authors": ["Sara Câmara", "Eduardo Luz", "Valéria Carvalho", "Ivan Meneghini", "Gladston Moreira"], "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "comment": "8 pages", "summary": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length."}
{"id": "2508.01554", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01554", "abs": "https://arxiv.org/abs/2508.01554", "authors": ["Yujia Zheng", "Tianhao Li", "Haotian Huang", "Tianyu Zeng", "Jingyu Lu", "Chuangxin Chu", "Yuekai Huang", "Ziyou Jiang", "Qian Xiong", "Yuyao Ge", "Mingyang Li"], "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "comment": null, "summary": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP."}
{"id": "2508.01630", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01630", "abs": "https://arxiv.org/abs/2508.01630", "authors": ["Maziyar Panahi"], "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "comment": null, "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct."}
{"id": "2508.01656", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2508.01656", "abs": "https://arxiv.org/abs/2508.01656", "authors": ["Lucio La Cava", "Dominik Macko", "Róbert Móro", "Ivan Srba", "Andrea Tagarelli"], "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios."}
{"id": "2508.01674", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements."}
{"id": "2508.01682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01682", "abs": "https://arxiv.org/abs/2508.01682", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "title": "The Bidirectional Process Reward Model", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling."}
{"id": "2508.01696", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01696", "abs": "https://arxiv.org/abs/2508.01696", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bin Qin"], "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks."}
{"id": "2508.01708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01708", "abs": "https://arxiv.org/abs/2508.01708", "authors": ["Berkay Köprü", "Mehrzad Mashal", "Yigit Gurses", "Akos Kadar", "Maximilian Schmitt", "Ditty Mathew", "Felix Burkhardt", "Florian Eyben", "Björn W. Schuller"], "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "comment": null, "summary": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate."}
{"id": "2508.01710", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01710", "abs": "https://arxiv.org/abs/2508.01710", "authors": ["Raviraj Joshi", "Rakesh Paul", "Kanishk Singla", "Anusha Kamath", "Michael Evans", "Katherine Luna", "Shaona Ghosh", "Utkarsh Vaidya", "Eileen Long", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "comment": null, "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models."}
{"id": "2508.01739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01739", "abs": "https://arxiv.org/abs/2508.01739", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues."}
{"id": "2508.01754", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01754", "abs": "https://arxiv.org/abs/2508.01754", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Luodan Zhang", "Zhen Lin", "Guangsheng Bao", "Yue Zhang"], "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "comment": null, "summary": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection."}
{"id": "2508.01781", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.01781", "abs": "https://arxiv.org/abs/2508.01781", "authors": ["Manuel Cossio"], "title": "A comprehensive taxonomy of hallucinations in Large Language Models", "comment": "55 pages, 16 figures, 3 tables", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications."}
{"id": "2508.01812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01812", "abs": "https://arxiv.org/abs/2508.01812", "authors": ["Amir DN Cohen", "Hilla Merhav", "Yoav Goldberg", "Reut Tsarfaty"], "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "comment": null, "summary": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs."}
{"id": "2508.01815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01815", "abs": "https://arxiv.org/abs/2508.01815", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning."}
{"id": "2508.01832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01832", "abs": "https://arxiv.org/abs/2508.01832", "authors": ["Rubin Wei", "Jiaqi Cao", "Jiarui Wang", "Jushi Kai", "Qipeng Guo", "Bowen Zhou", "Zhouhan Lin"], "title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory", "comment": null, "summary": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future."}
{"id": "2508.01858", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01858", "abs": "https://arxiv.org/abs/2508.01858", "authors": ["Yuhan Guo", "Cong Guo", "Aiwen Sun", "Hongliang He", "Xinyu Yang", "Yue Lu", "Yingji Zhang", "Xuntao Guo", "Dong Zhang", "Jianzhuang Liu", "Jiang Duan", "Yijia Xiao", "Liangjian Wen", "Hai-Ming Xu", "Yong Dai"], "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents", "comment": "Our code and data is open sourced at\n  https://github.com/Gnonymous/Web-CogReasoner", "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner"}
{"id": "2508.01862", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01862", "abs": "https://arxiv.org/abs/2508.01862", "authors": ["Yijun Feng"], "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism."}
{"id": "2508.01918", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01918", "abs": "https://arxiv.org/abs/2508.01918", "authors": ["Jaskaranjeet Singh", "Rakesh Thakur"], "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language", "comment": null, "summary": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP"}
{"id": "2508.01930", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.01930", "abs": "https://arxiv.org/abs/2508.01930", "authors": ["Tom S. Juzek", "Zina B. Ward"], "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research."}
{"id": "2508.01943", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01943", "abs": "https://arxiv.org/abs/2508.01943", "authors": ["Philip Schroeder", "Ondrej Biza", "Thomas Weng", "Hongyin Luo", "James Glass"], "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "comment": null, "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io"}
{"id": "2508.01959", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01959", "abs": "https://arxiv.org/abs/2508.01959", "authors": ["Junjie Wu", "Jiangnan Li", "Yuqing Li", "Lemao Liu", "Liyan Xu", "Jiwei Li", "Dit-Yan Yeung", "Jie Zhou", "Mo Yu"], "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension", "comment": "Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding", "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications."}
{"id": "2508.01977", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01977", "abs": "https://arxiv.org/abs/2508.01977", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models", "comment": null, "summary": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine."}
{"id": "2508.01990", "categories": ["cs.CL", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.01990", "abs": "https://arxiv.org/abs/2508.01990", "authors": ["Praveen Tangarajan", "Anand A. Rajasekar", "Manish Rathi", "Vinay Rao Dandin", "Ozan Ersoy"], "title": "Contextually Aware E-Commerce Product Question Answering using RAG", "comment": "6 pages, 1 figure, 5 tables. Preprint under review", "summary": "E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations."}
{"id": "2508.01999", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01999", "abs": "https://arxiv.org/abs/2508.01999", "authors": ["Md Badsha Biswas", "Özlem Uzuner"], "title": "Prompting Large Language Models to Detect Dementia Family Caregivers", "comment": null, "summary": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub."}
{"id": "2508.02013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02013", "abs": "https://arxiv.org/abs/2508.02013", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "comment": null, "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field."}
{"id": "2508.02018", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02018", "abs": "https://arxiv.org/abs/2508.02018", "authors": ["Wanqi Yang", "Yanda Li", "Yunchao Wei", "Meng Fang", "Ling Chen"], "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models", "comment": null, "summary": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks."}
{"id": "2508.02037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02037", "abs": "https://arxiv.org/abs/2508.02037", "authors": ["Huihan Li", "You Chen", "Siyuan Wang", "Yixin He", "Ninareh Mehrabi", "Rahul Gupta", "Xiang Ren"], "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time", "comment": null, "summary": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks."}
{"id": "2508.02038", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02038", "abs": "https://arxiv.org/abs/2508.02038", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Voice Technical Report", "comment": "Technical Report", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis."}
{"id": "2508.02045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02045", "abs": "https://arxiv.org/abs/2508.02045", "authors": ["Soyeon Kim", "Jindong Wang", "Xing Xie", "Steven Euijong Whang"], "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models", "comment": null, "summary": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git."}
{"id": "2508.02053", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02053", "abs": "https://arxiv.org/abs/2508.02053", "authors": ["Zhentao Xu", "Fengyi Li", "Albert Chen", "Xiaofeng Wang"], "title": "ProCut: LLM Prompt Compression via Attribution Estimation", "comment": null, "summary": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts."}
{"id": "2508.02074", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02074", "abs": "https://arxiv.org/abs/2508.02074", "authors": ["Gustaf Ahdritz", "Anat Kleiman"], "title": "The SMeL Test: A simple benchmark for media literacy in language models", "comment": null, "summary": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it."}
{"id": "2508.02087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02087", "abs": "https://arxiv.org/abs/2508.02087", "authors": ["Jin Li", "Keyu Wang", "Shu Yang", "Zhuoran Zhang", "Di Wang"], "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems."}
{"id": "2508.02094", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.02094", "abs": "https://arxiv.org/abs/2508.02094", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection."}
{"id": "2508.02189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02189", "abs": "https://arxiv.org/abs/2508.02189", "authors": ["David Demitri Africa", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining", "comment": null, "summary": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released."}
{"id": "2508.02193", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02193", "abs": "https://arxiv.org/abs/2508.02193", "authors": ["Yuxuan Song", "Zheng Zhang", "Cheng Luo", "Pengyang Gao", "Fan Xia", "Hao Luo", "Zheng Li", "Yuehang Yang", "Hongli Yu", "Xingwei Qu", "Yuwei Fu", "Jing Su", "Ge Zhang", "Wenhao Huang", "Mingxuan Wang", "Lin Yan", "Xiaoying Jia", "Jingjing Liu", "Wei-Ying Ma", "Ya-Qin Zhang", "Yonghui Wu", "Hao Zhou"], "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference", "comment": "Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion", "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models."}
{"id": "2508.02208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02208", "abs": "https://arxiv.org/abs/2508.02208", "authors": ["Yebo Peng", "Zixiang Liu", "Yaoming Li", "Zhizhuo Yang", "Xinye Xu", "Bowen Ye", "Weijun Yuan", "Zihan Wang", "Tong Yang"], "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems", "comment": "14 pages, 5 figures", "summary": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems."}
{"id": "2508.02241", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02241", "abs": "https://arxiv.org/abs/2508.02241", "authors": ["Danial Namazifard", "Lukas Galke"], "title": "Isolating Culture Neurons in Multilingual Large Language Models", "comment": "18 pages, 13 figures", "summary": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons ."}
{"id": "2508.02256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02256", "abs": "https://arxiv.org/abs/2508.02256", "authors": ["Belen Alastruey", "João Maria Janeiro", "Alexandre Allauzen", "Maha Elbayad", "Loïc Barrault", "Marta R. Costa-jussà"], "title": "Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders", "comment": null, "summary": "In this paper, we present a comprehensive study of language interference in\nencoder-only Transformer models across 83 languages. We construct an\ninterference matrix by training and evaluating small BERT-like models on all\npossible language pairs, providing a large-scale quantification of\ncross-lingual interference. Our analysis reveals that interference between\nlanguages is asymmetrical and that its patterns do not align with traditional\nlinguistic characteristics, such as language family, nor with proxies like\nembedding similarity, but instead better relate to script. Finally, we\ndemonstrate that the interference matrix effectively predicts performance on\ndownstream tasks, serving as a tool to better design multilingual models to\nobtain optimal performance."}
{"id": "2508.02260", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02260", "abs": "https://arxiv.org/abs/2508.02260", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning", "comment": "7 pages, 20 figures", "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs."}
{"id": "2508.02268", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02268", "abs": "https://arxiv.org/abs/2508.02268", "authors": ["Serry Sibaee", "Omer Nacar", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System", "comment": null, "summary": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of \\textbf{4.01 out of 5.0}\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication."}
{"id": "2508.02271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02271", "abs": "https://arxiv.org/abs/2508.02271", "authors": ["Kenneth Enevoldsen", "Kristian Nørgaard Jensen", "Jan Kostkan", "Balázs Szabó", "Márton Kardos", "Kirten Vad", "Andrea Blasi Núñez", "Gianluca Barmina", "Jacob Nielsen", "Rasmus Larsen", "Peter Vahlstrup", "Per Møldrup Dalum", "Desmond Elliott", "Lukas Galke", "Peter Schneider-Kamp", "Kristoffer Nielbo"], "title": "Dynaword: From One-shot to Continuously Developed Datasets", "comment": null, "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution."}
{"id": "2508.02290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02290", "abs": "https://arxiv.org/abs/2508.02290", "authors": ["Malik Marmonier", "Benoît Sagot", "Rachel Bawden"], "title": "A French Version of the OLDI Seed Corpus", "comment": null, "summary": "We present the first French partition of the OLDI Seed Corpus, our submission\nto the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its\ncreation process, which involved using multiple machine translation systems and\na custom-built interface for post-editing by qualified native speakers. We also\nhighlight the unique translation challenges presented by the source data, which\ncombines highly technical, encyclopedic terminology with the stylistic\nirregularities characteristic of user-generated content taken from Wikipedia.\nThis French corpus is not an end in itself, but is intended as a crucial pivot\nresource to facilitate the collection of parallel corpora for the\nunder-resourced regional languages of France."}
{"id": "2508.02296", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02296", "abs": "https://arxiv.org/abs/2508.02296", "authors": ["Ilias Triantafyllopoulos", "Renyi Qu", "Salvatore Giorgi", "Brenda Curtis", "Lyle H. Ungar", "João Sedoc"], "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks", "comment": null, "summary": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance."}
{"id": "2508.02308", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02308", "abs": "https://arxiv.org/abs/2508.02308", "authors": ["Sikui Zhang", "Guangze Gao", "Ziyun Gan", "Chunfeng Yuan", "Zefeng Lin", "Houwen Peng", "Bing Li", "Weiming Hu"], "title": "LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training", "comment": "13 pages, 9 figures", "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE."}
{"id": "2508.02317", "categories": ["cs.CL", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.02317", "abs": "https://arxiv.org/abs/2508.02317", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."}
{"id": "2508.02322", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02322", "abs": "https://arxiv.org/abs/2508.02322", "authors": ["Yuzhuang Xu", "Xu Han", "Yuanchi Zhang", "Yixuan Wang", "Yijun Liu", "Shiyu Ji", "Qingfu Zhu", "Wanxiang Che"], "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis", "comment": "16 pages, 9 figures, 7 tables", "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU."}
{"id": "2508.02360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02360", "abs": "https://arxiv.org/abs/2508.02360", "authors": ["Jiayi Zhang", "Shu Yang", "Junchao Wu", "Derek F. Wong", "Di Wang"], "title": "Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models", "comment": null, "summary": "Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization."}
{"id": "2508.02401", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02401", "abs": "https://arxiv.org/abs/2508.02401", "authors": ["Xiaolin Lin", "Jingcun Wang", "Olga Kondrateva", "Yiyu Shi", "Bing Li", "Grace Li Zhang"], "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation", "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."}
{"id": "2508.02426", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02426", "abs": "https://arxiv.org/abs/2508.02426", "authors": ["Linyu Li", "Zhi Jin", "Yuanpeng He", "Dongming Jin", "Yichi Zhang", "Haoran Duan", "Nyima Tash"], "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding", "comment": null, "summary": "Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models."}
{"id": "2508.02430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02430", "abs": "https://arxiv.org/abs/2508.02430", "authors": ["Robin Nowak", "Patrick Figge", "Carolin Haeussler"], "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications", "comment": null, "summary": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation."}
{"id": "2508.02452", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02452", "abs": "https://arxiv.org/abs/2508.02452", "authors": ["Mateusz Bystroński", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "LatentPrompt: Optimizing Promts in Latent Space", "comment": null, "summary": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks."}
{"id": "2508.02498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02498", "abs": "https://arxiv.org/abs/2508.02498", "authors": ["Md Tasin Abir", "Arpita Chowdhury", "Ashfia Rahman"], "title": "Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity", "comment": "10 pages, 9 figures", "summary": "This study investigates how Facebook shaped collective identity during the\nJuly 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.\nDuring government repression, protesters turned to Facebook as a central space\nfor resistance, where multimodal expressions, images, memes, videos, hashtags,\nand satirical posts played an important role in unifying participants. Using a\nqualitative approach, this research analyzes visual rhetoric, verbal discourse,\nand digital irony to reveal how shared symbols, protest art, and slogans built\na sense of solidarity. Key elements included the symbolic use of red, the\nironic metaphorical use of the term \"Razakar\", and the widespread sharing of\nvisuals representing courage, injustice, and resistance. The findings show that\nthe combination of visual and verbal strategies on Facebook not only mobilized\npublic sentiment, but also built a strong collective identity that challenged\nauthoritarian narratives. This study tries to demonstrate how online platforms\ncan serve as powerful tools for identity construction and political\nmobilization in the digital age."}
{"id": "2508.02502", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02502", "abs": "https://arxiv.org/abs/2508.02502", "authors": ["Shuzhou Yuan", "Zhan Qu", "Mario Tawfelis", "Michael Färber"], "title": "From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition."}
{"id": "2508.02513", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02513", "abs": "https://arxiv.org/abs/2508.02513", "authors": ["Tanja Baeumel", "Daniil Gurgurov", "Yusser al Ghussin", "Josef van Genabith", "Simon Ostermann"], "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit", "comment": null, "summary": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks."}
{"id": "2508.02515", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02515", "abs": "https://arxiv.org/abs/2508.02515", "authors": ["Zhan Qu", "Shuzhou Yuan", "Michael Färber"], "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs", "comment": null, "summary": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts."}
{"id": "2508.02527", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02527", "abs": "https://arxiv.org/abs/2508.02527", "authors": ["Jack Merullo", "Arjun Khurana", "Oliver McLaughlin"], "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2", "comment": null, "summary": "Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so."}
{"id": "2508.02532", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02532", "abs": "https://arxiv.org/abs/2508.02532", "authors": ["Karan Reddy", "Mayukha Pal"], "title": "Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction", "comment": null, "summary": "Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications."}
{"id": "2508.02540", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02540", "abs": "https://arxiv.org/abs/2508.02540", "authors": ["Anastasia Zhukova", "Terry Ruas", "Felix Hamborg", "Karsten Donnay", "Bela Gipp"], "title": "What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)", "comment": "published in the Proceedings of the 2023 ACM/IEEE Joint Conference on\n  Digital Libraries", "summary": "In a world overwhelmed with news, determining which information comes from\nreliable sources or how neutral is the reported information in the news\narticles poses a challenge to news readers. In this paper, we propose a\nmethodology for automatically identifying bias by commission, omission, and\nsource selection (COSS) as a joint three-fold objective, as opposed to the\nprevious work separately addressing these types of bias. In a pipeline concept,\nwe describe the goals and tasks of its steps toward bias identification and\nprovide an example of a visualization that leverages the extracted features and\npatterns of text reuse."}
{"id": "2508.02555", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.02555", "abs": "https://arxiv.org/abs/2508.02555", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Building and Aligning Comparable Corpora", "comment": "27 pages, 11 figures", "summary": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level."}
{"id": "2508.02556", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02556", "abs": "https://arxiv.org/abs/2508.02556", "authors": ["Ali Noori", "Pratik Devkota", "Somya Mohanty", "Prashanti Manda"], "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks", "comment": null, "summary": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment."}
{"id": "2508.02558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02558", "abs": "https://arxiv.org/abs/2508.02558", "authors": ["Yuerong Song", "Xiaoran Liu", "Ruixiao Li", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction", "comment": "11 pages, 6 figures", "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."}
{"id": "2508.02573", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02573", "abs": "https://arxiv.org/abs/2508.02573", "authors": ["Jérémie Dentan", "Davide Buscaldi", "Sonia Vanier"], "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs", "comment": null, "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization."}
{"id": "2508.02574", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.02574", "abs": "https://arxiv.org/abs/2508.02574", "authors": ["Eman Alamoudi", "Ellis Solaiman"], "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare", "comment": null, "summary": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling."}
{"id": "2508.02584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02584", "abs": "https://arxiv.org/abs/2508.02584", "authors": ["Ming Pok Ng", "Junqi Jiang", "Gabriel Freedman", "Antonio Rago", "Francesca Toni"], "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification", "comment": null, "summary": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs."}
{"id": "2508.02591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02591", "abs": "https://arxiv.org/abs/2508.02591", "authors": ["Omri Uzan", "Yuval Pinter"], "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks", "comment": null, "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks."}
{"id": "2508.02618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02618", "abs": "https://arxiv.org/abs/2508.02618", "authors": ["Jianxiang Zang", "Meiling Ning", "Shihan Dou", "Jiazheng Zhang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation", "comment": null, "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM."}
{"id": "2508.02631", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02631", "abs": "https://arxiv.org/abs/2508.02631", "authors": ["Zixi Li"], "title": "Pointer: Linear-Complexity Long-Range Modeling without Pre-training", "comment": "Submitted to Nordic AI Meet 2025", "summary": "We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies."}
{"id": "2508.02635", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02635", "abs": "https://arxiv.org/abs/2508.02635", "authors": ["Kranti Chalamalasetti", "Gabriel Bernier-Colborne", "Yvan Gauthier", "Sowmya Vajjala"], "title": "Test Set Quality in Multilingual LLM Evaluation", "comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total", "summary": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues."}
{"id": "2508.02279", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02279", "abs": "https://arxiv.org/abs/2508.02279", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "title": "Dialogue Systems Engineering: A Survey and Future Directions", "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering."}
