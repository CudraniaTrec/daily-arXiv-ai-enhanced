<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 28]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 78]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Efficient compilation and execution of synchronous programs via type-state programming](https://arxiv.org/abs/2508.01199)
*Avinash Malik*

Main category: cs.PL

TL;DR: 本文提出一种线性时间的同步程序编译方法，通过图重写规则和模板元编程，能有效减少状态空间爆炸，并使生成程序运行速度比主流方法快31-60%。


<details>
  <summary>Details</summary>
Motivation: 同步程序在安全关键型嵌入式软件中应用广泛，但同步组合多个有限状态机（FSM）会导致状态空间爆炸，影响编译效率和可验证性。因此，如何高效编译同步程序成为挑战。

Method: 该论文提出了一种新颖的线性时间编译技术，将同步程序基于自动机进行编译。具体包括：引入了图形化重写规则用于内核编程构造；使用线性时间算法将同步程序转换为FSM；再利用C++模板元编程将FSM编码成类型状态程序。

Result: 实验结果显示，该方法的编译时间和生成的二进制文件尺寸与现有最先进的编译器相当，但执行时间平均提升了31-60%。

Conclusion: 提出的线性时间编译技术可有效处理同步程序的状态空间爆炸，提高了生成程序的执行效率，并保持了较小的编译时间和输出体积。

Abstract: Synchronous programs are used extensively in implementation of safety
critical embedded software. Imperative synchronous programming languages model
multiple Finite State Machines (FSMs) executing in lockstep at logical clock
ticks. The synchronous view of time along with the FSM based design enables
easier formal verification. The synchronous composition of multiple FSMs,
during compilation, results in the well known state space explosion problem.
Hence, efficiently compiling imperative synchronous programs into small and
fast executables is challenging. This paper introduces a novel linear time
compilation technique for automata based compilation of synchronous programs.
Graph based rewrite rules for kernel programming constructs are introduced. A
linear time algorithm applies these rules to produce a FSM. The FSM is then
encoded into a type-state program using template meta-programming in C++.
Experimental results show that the compilation time and generated binary size
is comparable, while the execution times are on average 31-60% faster than
current state-of-the-art compilers.

</details>


### [2] [Proceedings 14th International Workshop on Trends in Functional Programming in Education](https://arxiv.org/abs/2508.02305)
*Rose Bohrer*

Main category: cs.PL

TL;DR: 本文介绍了TFPIE研讨会的宗旨、模式及意义，旨在促进功能性编程在教育领域中的应用与创新。


<details>
  <summary>Details</summary>
Motivation: 功能性编程作为一种编程范式，被认为有助于培养学生的编程思维和解决问题的能力。随着计算机科学教育的不断发展，越来越多的研究者、教师和专业人士关注如何在教育中有效地引入和应用功能性编程。该摘要的动机是为这些群体提供一个交流与合作的平台，共同探讨功能性编程在教育领域中的实际应用与创新方法。

Method: 该研讨会通过组织为期一天的活动，邀请相关领域的研究者和从业者进行交流和讨论。会议强调新思想、经过课堂实践的经验以及尚在进行中的工作，并采用事后同行评审的方式，对参会者的提交进行评审与出版，以鼓励更多开放和深入的讨论。

Result: 研讨会为功能性编程在教育中的推广和发展搭建了一个开放性的交流平台。与会者可以分享实际经验、最新研究进展及未来计划，促进教学方法和教育理念的创新。虽然摘要未具体描述实际研究成果，但成果体现在促进学术交流和推动该领域发展的平台建设上。

Conclusion: TFPIE研讨会通过鼓励开放、创新和实用的讨论，推动功能性编程在教育中的研究、应用与合作，为该领域的专业人士提供了宝贵的交流与合作机会。

Abstract: The goal of TFPIE is to gather researchers, teachers and professionals that
use, or are interested in the use of, functional programming in education.
TFPIE aims to be a venue where novel ideas, classroom-tested ideas and
work-in-progress on the use of functional programming in education are
discussed. The one-day workshop will foster a spirit of open discussion by
having a review process for publication after the workshop.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models](https://arxiv.org/abs/2508.01255)
*Cuong Chi Le,Cuong Duc Van,Tung Duy Vu,Thai Minh Pham Vu,Hoang Nhat Phan,Huy Nhat Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: TestWeaver结合轻量程序分析与LLM，通过精细化输入与上下文设计，突破传统LLM自动回归测试的覆盖率瓶颈，能更高效生成有效测试用例。


<details>
  <summary>Details</summary>
Motivation: 当前LLM用于自动化生成回归测试时，由于对程序执行的推理能力有限，导致测试覆盖率增长停滞（称为覆盖平台期）。该文旨在解决这一问题，提高自动化回归测试的质量与效率。

Method: 提出了TestWeaver方法，将轻量级程序分析与LLM结合，通过三点创新：（1）用目标代码行的反向切片替代全量上下文，减少幻觉、提升关注度；（2）筛选并引入与目标路径控制流相近的测试用例为上下文，丰富执行语境；（3）在执行路径中用注释嵌入变量状态，提升LLM推理能力。

Result: 实验证明，TestWeaver比现有基于LLM的方法能更快提升代码覆盖率，并生成更有效的回归测试用例。

Conclusion: TestWeaver通过针对性地增强LLM输入上下文，显著改善了自动回归测试中的代码覆盖效率与测试效果，是LLM自动测试领域的重要进展。

Abstract: Regression testing ensures that code changes do not unintentionally break
existing functionality. While recent advances in large language models (LLMs)
have shown promise in automating test generation for regression testing, they
often suffer from limited reasoning about program execution, resulting in
stagnated coverage growth - a phenomenon known as the coverage plateau. In this
paper, we present TestWeaver, a novel LLM-based approach that integrates
lightweight program analysis to guide test generation more effectively.
TestWeaver introduces three key innovations: (1) it reduces hallucinations and
improves focus by supplying the LLM with the backward slice from the target
line instead of full program context; (2) it identifies and incorporates close
test cases - those that share control-flow similarities with the path to the
target line - to provide execution context within the LLM's context window; and
(3) it enhances LLM's reasoning with execution in-line annotations that encode
variable states as comments along executed paths. By equipping LLMs with these
targeted and contextualized inputs, TestWeaver improves coverage-guided test
generation and mitigates redundant explorations. Empirical results demonstrate
that TestWeaver accelerates code coverage growth and generates more effective
regression test cases than existing LLM-based approaches.

</details>


### [4] [Screencast-Based Analysis of User-Perceived GUI Responsiveness](https://arxiv.org/abs/2508.01337)
*Wei Liu,Linqiang Guo,Yi Wen Heng,Chenglin Li,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该文提出了一种基于视频分析的轻量级黑盒工具\tool，可以高精度、低误差地量化移动应用的GUI响应性能，并已在工业环境大规模应用，提升了传统工具无法发现的问题检测能力。


<details>
  <summary>Details</summary>
Motivation: GUI响应速度对于移动应用的用户体验至关重要，细微的延迟都可能导致用户不满和负面评价。然而，目前在大规模工业测试流程中，准确检测和量化用户感知的GUI延迟依然具有挑战性。现有静态分析或系统指标方法并不能真正反映用户感知的问题，也难以高效扩展。作者希望解决这一痛点。

Method: 提出了一种轻量级、黑盒的GUI响应性测量工具（\tool），通过分析自动化GUI测试录制的视频，使用计算机视觉技术检测用户交互和视觉延迟，直接量化GUI响应性。主要测量指标包括响应时间（用户操作到首次视觉反馈）和完成时间（直到视觉反馈稳定）。这些算法在帧级别上分析视觉变化，无需侵入性地接触应用底层。

Result: 在一个包含2,458次交互、覆盖64个流行Android应用的人工标注基准集上，\tool在交互检测上达到了0.96的精度和0.93的召回率，并分别在89%以上的交互中，将响应时间和完成时间的测量误差控制在50ms和100ms以内。该工具已部署到工业测试流程中，日分析数千条视频，精准发现了传统工具遗漏的问题，提升了性能调试效率。

Conclusion: 文中提出的\tool能够高效、准确、可扩展地检测和量化移动应用GUI响应性，弥补了现有方法在用户感知延迟检测方面的不足。其实用性已在工业环境下得到验证，为大规模移动应用性能测试提供了新思路和有力工具。

Abstract: GUI responsiveness is critical for a positive user experience in mobile
applications. Even brief delays in visual feedback can frustrate users and lead
to negative reviews. However, detecting and quantifying such user-perceived
delays remains challenging, especially in industrial testing pipelines that
evaluate thousands of apps daily across diverse devices and OS versions.
Existing techniques based on static analysis or system metrics, while useful,
may not accurately capture user-perceived issues or scale effectively.
  In this experience paper, we present \tool, a lightweight and black-box
technique that measures GUI responsiveness directly from mobile screencasts --
video recordings captured during automated GUI testing. \tool detects user
interactions and visual delays, helping developers identify GUI performance
issues that affect the user experience. It uses computer vision to detect user
interactions and analyzes frame-level visual changes to compute two key
metrics: response time (from user action to first visual feedback) and finish
time (until visual feedback stabilizes). We evaluate \tool on a manually
annotated benchmark of 2,458 interactions from 64 popular Android apps. \tool
achieves 0.96 precision and 0.93 recall in detecting interactions, and measures
response and finish times within 50\,ms and 100\,ms error, respectively, for
over 89\% of interactions. The tool has been deployed in an industrial testing
pipeline and analyzes thousands of screencasts daily, uncovering responsiveness
issues missed by traditional tools and improving performance debugging
efficiency.

</details>


### [5] [HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection](https://arxiv.org/abs/2508.01357)
*Yunhao Liang,Ruixuan Ying,Takuya Taniguchi,Guwen Lyu,Zhe Cui*

Main category: cs.SE

TL;DR: 本文提出了结合大模型筛选与执行验证的两阶段方法，可更准确检测Python语义克隆代码，实验指标优于现有直接用LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有传统和LLM方法难以有效检测出语义克隆（功能等价但语法结构不同的代码），需要新方法提升检测的准确性和实用性。

Method: 方法包括两阶段流程：第一阶段利用LLM进行语义分析筛选非克隆对；对未能判定为克隆的代码对，在第二阶段利用LLM生成测试输入，通过交叉执行验证实现功能等价性，确保识别出语义克隆。

Result: 实验证明，该框架在精度、召回率和F1-score方面均明显优于仅用LLM的检测方法，有效提升了语义克隆的识别能力。

Conclusion: 该论文提出的两阶段代码克隆检测框架显著提升了在Python程序中检测语义克隆的准确性，并优于直接使用LLM方法。

Abstract: Code clone detection is a critical task in software engineering, aimed at
identifying duplicated or similar code fragments within or across software
systems. Traditional methods often fail to capture functional equivalence,
particularly for semantic clones (Type 4), where code fragments implement
identical functionality despite differing syntactic structures. Recent advances
in large language models (LLMs) have shown promise in understanding code
semantics. However, directly applying LLMs to code clone detection yields
suboptimal results due to their sensitivity to syntactic differences. To
address these challenges, we propose a novel two-stage framework that combines
LLM-based screening with execution-based validation for detecting semantic
clones in Python programs. In the first stage, an LLM evaluates code pairs to
filter out obvious non-clones based on semantic analysis. For pairs not
identified as clones, the second stage employs an execution-based validation
approach, utilizing LLM-generated test inputs to assess functional equivalence
through cross-execution validation. Our experimental evaluation demonstrates
significant improvements in precision, recall, and F1-score compared to direct
LLM-based detection, highlighting the framework's effectiveness in identifying
semantic clones. Future work includes exploring cross-language clone detection
and optimizing the framework for large-scale applications.

</details>


### [6] [Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis](https://arxiv.org/abs/2508.01974)
*Jiahao Zhang,Xiao Cheng,Yuxiang Lei*

Main category: cs.SE

TL;DR: 文章提出了一种新的基于约束图的流敏感指针分析方法CG-FSPTA，在保证精度的同时大幅提高了效率和可扩展性，实验中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制流图的流敏感指针分析精度虽高，但计算效率低下，难以扩展到大规模程序。作者希望在保留流敏感分析精度的前提下，提高效率与可扩展性。

Method: 提出基于FSConsG（Flow-Sensitive Constraint Graph）的流敏感指针分析方法CG-FSPTA，将集合约束图结构优势与流敏感性结合，应用高级图优化和动态求解技术，以提升分析效率。

Result: 通过在基准程序上的实验证明，CG-FSPTA能够将平均内存使用减少33.05%，分析速度提升7.27倍，并维持了流敏感分析的精度，相较于最先进的方法效果显著。

Conclusion: CG-FSPTA不仅保持了流敏感分析的精度，还大幅提高了分析效率，减少了内存使用和运行时间。试验结果表明，该方法具有良好的可扩展性，为大规模软件系统的程序分析提供了可行方案。

Abstract: Flow-sensitive pointer analysis constitutes an essential component of precise
program analysis for accurately modeling pointer behaviors by incorporating
control flows. Flow-sensitive pointer analysis is extensively used in alias
analysis, taint analysis, program understanding, compiler optimization, etc.
Existing flow-sensitive pointer analysis approaches, which are conducted based
on control flow graphs, have significantly advanced the precision of pointer
analysis via sophisticated techniques to leverage control flow information.
However, they inevitably suffer from computational inefficiencies when
resolving points-to information due to the inherent complex structures of
control flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph
(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of
control-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to
leverage the structural advantages of set-constraint graphs (which are commonly
used in flow-insensitive pointer analysis) while keeping the flow sensitivity
of variable definitions and uses, allowing the incorporation of sophisticated
graph optimization and dynamic solving techniques. In this way, CG-FSPTA
achieves significant efficiency improvements while keeping the precision of
flow-sensitive analysis. Experimental evaluations on benchmark programs
demonstrate that CG-FSPTA, significantly reduces both memory usage and
execution time while maintaining precision. In particular, by solving in the
FSConsG, CG-FSPTA achieves an average memory reduction of 33.05\% and
accelerates flow-sensitive pointer analysis by 7.27x compared to the
state-of-art method. These experimental results underscore the efficacy of
CG-FSPTA as a scalable solution to analyze large-scale software systems,
establishing a robust foundation for future advancements in efficient program
analysis frameworks.

</details>


### [7] [An Empirical Validation of Open Source Repository Stability Metrics](https://arxiv.org/abs/2508.01358)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文实证验证了基于控制理论的开源项目复合稳定性指数（CSI）的有效性，提出采用每周提交频率和中位数统计能更准确反映项目稳定性，从而为实际健康监控工具提供可靠依据。


<details>
  <summary>Details</summary>
Motivation: 随着开源软件在全球软件供应链中的广泛应用，对其稳定性与可持续性进行有效评估变得尤为重要。此前已提出了一种基于控制理论的方法来衡量开源项目的稳定性，但缺乏实证验证。该研究正是为验证这种理论模型的实际有效性而展开。

Method: 本研究选取了100个高排名GitHub仓库，基于实际数据对复合稳定性指数（CSI）的有效性进行实证检验。主要方法包括：对比使用每周与每日的提交频率采样，对案件与拉取请求的统计指标从均值替换为中位数，并通过数据集推导出更符合实际项目特性的参数。

Result: 实验证明，采用每周提交频率作为稳定性衡量的周期更具可行性；采用中位数代替均值可提升问题和拉取请求稳定性的推断准确性。同时，研究基于数据集推导了稳定性评分更切合实际项目表现的参数。

Conclusion: CSI指数通过实证得到支持，控制理论方法在评估开源项目健康状况上的有效性得到证实。研究成果为实际的项目监控和健康评估工具提供了数据支持和改进建议。

Abstract: Over the past few decades, open source software has been continuously
integrated into software supply chains worldwide, drastically increasing
reliance and dependence. Because of the role this software plays, it is
important to understand ways to measure and promote its stability and potential
for sustainability. Recent work proposed the use of control theory to
understand repository stability and evaluate repositories' ability to return to
equilibrium after a disturbance such as the introduction of a new feature
request, a spike in bug reports, or even the influx or departure of
contributors. This approach leverages commit frequency patterns, issue
resolution rate, pull request merge rate, and community activity engagement to
provide a Composite Stability Index (CSI). While this framework has theoretical
foundations, there is no empirical validation of the CSI in practice. In this
paper, we present the first empirical validation of the proposed CSI by
experimenting with 100 highly ranked GitHub repositories. Our results suggest
that (1) sampling weekly commit frequency pattern instead of daily is a more
feasible measure of commit frequency stability across repositories and (2)
improved statistical inferences (swapping mean with median), particularly with
ascertaining resolution and review times in issues and pull request, improves
the overall issue and pull request stability index. Drawing on our empirical
dataset, we also derive data-driven half-width parameters that better align
stability scores with real project behavior. These findings both confirm the
viability of a control-theoretic lens on open-source health and provide
concrete, evidence-backed applications for real-world project monitoring tools.

</details>


### [8] [From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool](https://arxiv.org/abs/2508.01430)
*Kaveh Shahedi,Matthew Khouzam,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 过于追求技术卓越的追踪分析工具反而会妨碍在工业界的采用。作者基于与爱立信的合作及行业反馈，提出采用友好、嵌入专家知识、强调透明度和用户信任的设计原则，认为这些对于自动化工具的实际应用和可持续推广至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管现代系统对复杂软件行为的理解越来越依赖系统追踪，但许多先进的追踪分析工具在工业界并未被广泛采用。作者希望调查为什么这些工具未被充分采用，并解决实际应用中的阻碍因素。

Method: 作者通过与爱立信蒙特利尔长达一年的合作，开发TMLL（Trace-Server Machine Learning Library），深入调查了追踪分析工具在实际采用中的障碍。他们收集了专家反馈、在Eclipse基金会的平台集成经验，并对40位业界和学术界专业人士进行了问卷调查和定性访谈。

Result: 他们发现，由于可用性、透明度和用户信任问题，技术上的卓越有时反而阻碍了工具的实际应用（即“卓越悖论”）。TMLL通过易于采用的设计、嵌入专家知识、透明解释和渐进式采用机制来应对这些问题。超过77.5%的受访者更重视结果的质量和可信度而非技术复杂性，67.5%更偏好用户可控的半自动分析方式。

Conclusion: 作者提出，自动化软件工程工具要实现可持续采用，需要从以功能为中心的开发转向以采用为中心的设计，应注重认知兼容性、嵌入专业知识以及透明度建立信任。这样才能更好地促进实际工业应用。

Abstract: System tracing has become essential for understanding complex software
behavior in modern systems, yet sophisticated trace analysis tools face
significant adoption gaps in industrial settings. Through a year-long
collaboration with Ericsson Montr\'eal, developing TMLL (Trace-Server Machine
Learning Library, now in the Eclipse Foundation), we investigated barriers to
trace analysis adoption. Contrary to assumptions about complexity or automation
needs, practitioners struggled with translating expert knowledge into
actionable insights, integrating analysis into their workflows, and trusting
automated results they could not validate. We identified what we called the
Excellence Paradox: technical excellence can actively impede adoption when
conflicting with usability, transparency, and practitioner trust. TMLL
addresses this through adoption-focused design that embeds expert knowledge in
interfaces, provides transparent explanations, and enables incremental
adoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's
integration, and a survey of 40 industry and academic professionals revealed
consistent patterns: survey results showed that 77.5% prioritize quality and
trust in results over technical sophistication, while 67.5% prefer
semi-automated analysis with user control, findings supported by qualitative
feedback from industrial collaboration and external peer review. Results
validate three core principles: cognitive compatibility, embedded expertise,
and transparency-based trust. This challenges conventional capability-focused
tool development, demonstrating that sustainable adoption requires
reorientation toward adoption-focused design with actionable implications for
automated software engineering tools.

</details>


### [9] [Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective](https://arxiv.org/abs/2508.01443)
*Jingzhi Gong,Rafail Giavrimis,Paul Brookes,Vardan Voskanyan,Fan Wu,Mari Ashiga,Matthew Truscott,Mike Basios,Leslie Kanthan,Jie Xu,Zheng Wang*

Main category: cs.SE

TL;DR: MPCO框架自动跨多种大语言模型生成高效代码优化提示，显著提升工业代码优化性能，解决提示词迁移难题，实用性强。


<details>
  <summary>Details</summary>
Motivation: 当前在工业平台上同时部署多个大语言模型（LLM）时，针对某一模型优化的提示词在其它模型上常常无效，导致需要花费大量精力进行模型特定的提示词工程。这一“跨模型提示词工程瓶颈”极大限制了多模型优化系统在生产环境中的实际应用。

Method: 提出了Meta-Prompted Code Optimization (MPCO) 框架，自动为不同LLM生成高质量、任务特定的提示词，并能在ARTEMIS工业平台上无缝部署，集成了项目信息、任务需求和模型特定上下文，通过meta-prompting动态合成上下文感知的优化提示。

Result: 在五个真实代码库上的366小时运行时间基准测试中，MPCO相较基线方法总体性能提升高达19.06%，且96%的顶级优化均来自有意义的代码编辑。系统性消融实验和敏感性分析表明，集成全面的上下文对于高效meta-prompting至关重要，三种主流LLM均可作为有效的meta-prompter。

Conclusion: MPCO框架能够解决多LLM环境下的提示词迁移问题，实现自动化、高效、可扩展的代码优化，且易于生产环境落地，为工业界提供了实践参考。

Abstract: There is a growing interest in leveraging large language models (LLMs) for
automated code optimization. However, industrial platforms deploying multiple
LLMs face a critical challenge: prompts optimized for one LLM often fail with
others, requiring expensive model-specific prompt engineering. This cross-model
prompt engineering bottleneck severely limits the practical deployment of
multi-LLM optimization systems in production environments. To address this, we
introduce Meta-Prompted Code Optimization (MPCO), a framework that
automatically generates high-quality, task-specific prompts across diverse LLMs
while maintaining industrial efficiency requirements. MPCO leverages
meta-prompting to dynamically synthesize context-aware optimization prompts by
integrating project metadata, task requirements, and LLM-specific contexts, and
it seamlessly deploys on the ARTEMIS industrial platform for automated
validation and scaling.
  Our comprehensive evaluation on five real-world codebases with 366 hours of
runtime benchmarking demonstrates MPCO's effectiveness: it achieves overall
performance improvements up to 19.06% with the best statistical rank across all
systems compared to baseline methods. Analysis shows that 96% of the
top-performing optimizations stem from meaningful edits. Through systematic
ablation studies and meta-prompter sensitivity analysis, we identify that
comprehensive context integration is essential for effective meta-prompting,
and that all three major LLMs can serve effectively as meta-prompters,
providing actionable insights for industrial practitioners.

</details>


### [10] [Directed Grammar-Based Test Generation](https://arxiv.org/abs/2508.01472)
*Lukas Kirschner,Ezekiel Soremekun*

Main category: cs.SE

TL;DR: FdLoop是一种自动学习驱动的自动化测试用例生成方法，显著提升了对多类测试目标的实现效率，在主流测试基线中表现突出，有力推动了基于目标的测试技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的测试用例生成器，尤其是基于语法的生成方法，难以高效生成能达成特定测试目标的测试用例，因此在复杂软件的有效测试中存在局限性。本文提出新的自动化测试生成方法，解决目标导向用例生成难题。

Method: 提出了一种名为FdLoop的自动化测试生成方法。该方法通过对已有输入的回馈和概率语法学习相关输入属性，迭代选择和进化生成能够达成特定测试目标的输入。具体对四类测试目标进行了实例化：唯一代码覆盖、输入到代码复杂性、程序失效（异常）和长执行时间。

Result: 在涵盖JSON、CSS和JavaScript三种输入格式及20个开源软件上的实验表明：FdLoop在86%的场景下优于包括随机、概率、逆概率、EvoGFuzz和DynaMosa在内的五种对比基线。对诱发程序错误行为而言，FdLoop效果最多可达最佳基线的2倍。实验还证明了FdLoop的主组件对整体有效性的正向贡献，并验证了其在单一及多目标下的实用性和扩展性。

Conclusion: FdLoop能有效克服现有基于语法的用例生成器对目标导向测试支持不足的问题，大幅提升目标相关测试用例的生成效率，对自动化程序测试具有重要推动作用。

Abstract: To effectively test complex software, it is important to generate
goal-specific inputs, i.e., inputs that achieve a specific testing goal.
However, most state-of-the-art test generators are not designed to target
specific goals. Notably, grammar-based test generators, which (randomly)
produce syntactically valid inputs via an input specification (i.e., grammar)
have a low probability of achieving an arbitrary testing goal. This work
addresses this challenge by proposing an automated test generation approach
(called FdLoop) which iteratively learns relevant input properties from
existing inputs to drive the generation of goal-specific inputs. Given a
testing goal, FdLoop iteratively selects, evolves and learn the input
distribution of goal-specific test inputs via test feedback and a probabilistic
grammar. We concretize FdLoop for four testing goals, namely unique code
coverage, input-to-code complexity, program failures (exceptions) and long
execution time. We evaluate FdLoop using three (3) well-known input formats
(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,
FdLoop outperforms all five tested baselines namely the baseline grammar-based
test generators (random, probabilistic and inverse-probabilistic methods),
EvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best
baseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that
the main components of FdLoop (i.e., input mutator, grammar mutator and test
feedbacks) contribute positively to its effectiveness. Finally, our evaluation
demonstrates that FdLoop effectively achieves single testing goals (revealing
erroneous behaviors, generating complex inputs, or inducing long execution
time) and scales to multiple testing goals across varying parameter settings.

</details>


### [11] [GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development](https://arxiv.org/abs/2508.01489)
*SK. Golam Saroar,Waseefa Ahmed,Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: 本文系统分析了 GitHub Marketplace 上自动化工具的发展趋势及其与学界研究成果的关系，揭示了二者存在脱节，并建议学界应聚焦实践创新以推动行业进步。


<details>
  <summary>Details</summary>
Motivation: 随着软件自动化在开源软件开发中的应用不断增长，深入理解 GitHub Marketplace 的演变与动态变得尤为重要。尽管学界对自动化的关注已久，但其成果与业界实际需求之间仍存在差距，因此有必要系统梳理两者关系并寻求合作创新点。

Method: 本研究采取系统分析方法，对 GitHub Marketplace 的自动化工具趋势、特征及动态进行调研，并且将行业工具的发展与学术文献中的技术进展进行比较。

Result: 研究发现了 GitHub Marketplace 自动化工具的发展趋势，并揭示了当前学术研究与行业实践之间的显著差异，进而指出了学术界在促进开源自动化实践方面的潜在贡献方向。

Conclusion: 研究指出，GitHub Marketplace 在推动开源软件自动化方面发挥着核心作用，但学术界与业界在自动化工具的实践与研究上存在脱节。通过系统分析，该研究揭示了二者间的差距，并为学术界如何更好地促进实际创新提供了建议。

Abstract: GitHub, a central hub for collaborative software development, has
revolutionized the open-source software (OSS) ecosystem through its GitHub
Marketplace, a platform launched in 2017 to host automation tools aimed at
enhancing the efficiency and scalability of software projects. As the adoption
of automation in OSS production grows, understanding the trends,
characteristics, and underlying dynamics of this marketplace has become vital.
Furthermore, despite the rich repository of academic research on software
automation, a disconnect persists between academia and industry practices. This
study seeks to bridge this gap by providing a systematic analysis of the GitHub
Marketplace, comparing trends observed in industry tools with advancements
reported in academic literature, and identifying areas where academia can
contribute to practical innovation.

</details>


### [12] [OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications](https://arxiv.org/abs/2508.01492)
*Angel C. Chavez-Moreno,Cristina L. Abad*

Main category: cs.SE

TL;DR: 本论文创建并分析了OpenLambdaVerse数据集，收集和剖析了采用Serverless Framework和AWS Lambda的GitHub项目，系统展示了当前Serverless应用的规模、语言、触发方式、成熟度与安全实践情况，为业界和学界提供了最新的Serverless应用研究与实证数据基础。


<details>
  <summary>Details</summary>
Motivation: 随着Serverless计算和FaaS（Function-as-a-Service）的蓬勃发展，开发者采用Infrastructure-as-Code（IaC）和如Serverless Framework等工具大幅简化了云端应用部署，但对于这些工具在真实项目中的实际使用仍缺研究与洞察。该论文的动机是填补这一研究空白，系统化了解和分析当前Serverless技术栈的应用现状。

Method: 本论文构建了OpenLambdaVerse数据集，通过对GitHub上的公开仓库筛选，选择那些采用Serverless Framework且包含AWS Lambda函数的应用项目。方法基于Wonderless数据集的原始方法，并增加多项新过滤步骤，随后对筛选得到的项目进行数量、复杂性、使用语言与运行时、触发方式、项目成熟度和安全实践等方面的数据分析与特征刻画。

Result: 通过对OpenLambdaVerse数据集的分析，论文揭示了当前Serverless架构项目的规模、复杂度、主流开发语言与运行时分布、函数触发方式、项目成熟度以及普遍存在的安全实践/隐患，提供了真实世界中Serverless应用工作负载的最新画像。

Conclusion: OpenLambdaVerse数据集是一个为Serverless研究和实践者服务的，针对应用Serverless Framework和AWS Lambda的当前项目的全面、开放且实时更新的资源，有助于深入理解Serverless应用的实际发展、应用模式与挑战。

Abstract: Function-as-a-Service (FaaS) is at the core of serverless computing, enabling
developers to easily deploy applications without managing computing resources.
With an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless
Framework use YAML configurations to define and deploy APIs, tasks, workflows,
and event-driven applications on cloud providers, promoting zero-friction
development. As with any rapidly evolving ecosystem, there is a need for
updated insights into how these tools are used in real-world projects. Building
on the methodology established by the Wonderless dataset for serverless
computing (and applying multiple new filtering steps), OpenLambdaVerse
addresses this gap by creating a dataset of current GitHub repositories that
use the Serverless Framework in applications that contain one or more AWS
Lambda functions. We then analyze and characterize this dataset to get an
understanding of the state-of-the-art in serverless architectures based on this
stack. Through this analysis we gain important insights on the size and
complexity of current applications, which languages and runtimes they employ,
how are the functions triggered, the maturity of the projects, and their
security practices (or lack of). OpenLambdaVerse thus offers a valuable,
up-to-date resource for both practitioners and researchers that seek to better
understand evolving serverless workloads.

</details>


### [13] [Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification](https://arxiv.org/abs/2508.01523)
*Ningzhi Tang,Emory Smith,Yu Huang,Collin McMillan,Toby Jia-Jun Li*

Main category: cs.SE

TL;DR: 本文探索了大语言模型在代码修改任务中的两种提示策略，发现直接指令提示更灵活，摘要中介提示更有助理解，建议提升提示工具的可用性和摘要一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在代码生成上的应用已被广泛研究，但在现有代码修改上的角色还缺乏深入理解。现有的代码修改提示方式（prompting）构建存在特殊挑战，尤其是在跨越不同类型任务时表现尚不清晰。

Method: 作者设计并对比了两种LLM辅助代码修改的提示策略：直接指令提示（开发者用自然语言直接描述修改）和摘要中介提示（开发者通过编辑由LLM生成的代码摘要实施修改）。15位开发者参与了多场景的探索性实验，完成了相关修改任务。

Result: 开发者普遍采用了迭代式工作流：理解代码、定位需要编辑的部分，并通过执行或语义推理验证结果。直接指令提示更灵活、易于描述需求，而摘要中介提示则有助于理解、框架构建和控制提示内容。开发者选择哪种策略取决于任务目标和环境（如紧迫性、可维护性、学习需求和代码熟悉度）。

Conclusion: 改进LLM用于代码修改的提示设计非常必要，包括提升摘要粒度调节、摘要和代码的可追溯性以及生成摘要的一致性，从而实现更好的人机交互体验。

Abstract: This paper presents a study of using large language models (LLMs) in
modifying existing code. While LLMs for generating code have been widely
studied, their role in code modification remains less understood. Although
"prompting" serves as the primary interface for developers to communicate
intents to LLMs, constructing effective prompts for code modification
introduces challenges different from generation. Prior work suggests that
natural language summaries may help scaffold this process, yet such approaches
have been validated primarily in narrow domains like SQL rewriting. This study
investigates two prompting strategies for LLM-assisted code modification:
Direct Instruction Prompting, where developers describe changes explicitly in
free-form language, and Summary-Mediated Prompting, where changes are made by
editing the generated summaries of the code. We conducted an exploratory study
with 15 developers who completed modification tasks using both techniques
across multiple scenarios. Our findings suggest that developers followed an
iterative workflow: understanding the code, localizing the edit, and validating
outputs through execution or semantic reasoning. Each prompting strategy
presented trade-offs: direct instruction prompting was more flexible and easier
to specify, while summary-mediated prompting supported comprehension, prompt
scaffolding, and control. Developers' choice of strategy was shaped by task
goals and context, including urgency, maintainability, learning intent, and
code familiarity. These findings highlight the need for more usable prompt
interactions, including adjustable summary granularity, reliable summary-code
traceability, and consistency in generated summaries.

</details>


### [14] [RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale](https://arxiv.org/abs/2508.01550)
*Zhilong Chen,Chengzong Zhao,Boyuan Chen,Dayi Lin,Yihao Chen,Arthur Leung,Gopi Krishnan Rajbahadur,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: RepoForge自动化流水线突破SWE代理训练高成本、低效等难题，8B及以下LLM新SOTA；数据、评估与标注自动化成效显著。


<details>
  <summary>Details</summary>
Motivation: 训练面向软件工程（SWE）的LLM受到基础设施昂贵、评估流程低效、训练数据稀缺以及高成本质量控制等问题的严重限制。

Method: 提出RepoForge——一种自主的端到端流水线，大规模自动生成、评估和训练SWE代理，包括：存储高效的沙箱、基于Ray的分布式评估、自动数据生成、基于SPICE的自动难度标注及无阻塞RL流程。详见五大贡献。

Result: （1）提出RepoForge-8B-Agent，在SWE-Bench-Verified基准上达到17.4%，创8B及以下非推理LLM新纪录；（2）自动生成7,304个可执行环境，完全无需人工干预；（3）通过智能依赖管理和镜像裁剪，实现14倍存储压缩；（4）基于Ray评估系统，评测速度提升70%以上；（5）引入SPICE难度评估，标注成本降低19,000倍。

Conclusion: 作者通过一体化的流水线突破了SWE代理训练中的关键瓶颈，显著降低了存储成本，加速了评估流程，提高了训练数据质量和可扩展性，使小参数模型亦可在高难基准上取得SOTA表现。

Abstract: Training software engineering (SWE) LLMs is bottlenecked by expensive
infrastructure, inefficient evaluation pipelines, scarce training data, and
costly quality control. We present RepoForge, an autonomous, end-to-end
pipeline that generates, evaluates, and trains SWE agents at scale. Our key
contributions include: (1) RepoForge-8B-Agent, achieving 17.4\% on
SWE-Bench-Verified~\citep{swebench_verified2024}, establishing new
state-of-the-art for $\leq$8B non-thinking LLMs; (2) 7,304 executable
environments auto-generated from real GitHub commits with zero manual
intervention; (3) 14$\times$ storage reduction (1.4GB $\rightarrow$ 102MB per
instance) via intelligent dependency management and image pruning; (4) $>$70\%
faster evaluation using a Ray-powered~\citep{ray2018} distributed RepoForge
harness; (5) 19,000$\times$ cheaper labeling through our automated
SPICE~\citep{spice2024} difficulty assessment technique. By unifying
storage-efficient sandboxing, Ray-powered evaluation harness, automated data
generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate
that even $\leq$8B models can reach new state-of-the-art performance on
demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical
bottlenecks in SWE agent training: high storage costs of container-based
evaluation, inefficient sequential reward pipelines, limited availability of
high-quality training data, expensive manual labeling, and multi-turn RL
pipeline bottlenecks.

</details>


### [15] [PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades](https://arxiv.org/abs/2508.02023)
*Huashan Lei,Guanping Xiao,Yepang Liu,Zheng Zheng*

Main category: cs.SE

TL;DR: 提出的PCREQ工具自动分析并修复Python依赖升级中的各类兼容性问题，推断准确率近94%，显著优于现有方法，大幅提升了自动化与效率。


<details>
  <summary>Details</summary>
Motivation: Python第三方库的升级经常引发兼容性问题，导致系统故障。现有工具大多只检测依赖冲突，忽略代码级别的不兼容问题，且没有工具能同时自动推断版本兼容与代码兼容。

Method: 提出PCREQ方法，通过结合版本兼容性分析和代码兼容性分析，自动推断兼容的依赖项。PCREQ包括知识采集、版本兼容评估、API和模块提取、代码兼容评估、版本调整、缺失依赖补全等六大模块，最后输出可用的requirements.txt及修复报告。

Result: PCREQ在自建的REQBench大规模基准（2095个升级测试用例，其中406个pip无法解决）上验证，推断成功率达到94.03%，远高于PyEGo（37.02%）、ReadPyE（37.16%）、及各类LLM方法（GPT-4o、DeepSeek V3/R1高出18-20%），平均处理时长仅60.79秒，展现出优异的实际效率。

Conclusion: PCREQ首次实现了自动推断Python依赖项各类兼容性的自动化工具，极大降低了依赖升级中的人工排查工作，并推动了Python依赖维护的自动化。

Abstract: Python third-party libraries (TPLs) are essential in modern software
development, but upgrades often cause compatibility issues, leading to system
failures. These issues fall into two categories: version compatibility issues
(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect
dependency conflicts but overlook code-level incompatibilities, with no
solution fully automating the inference of compatible versions for both VCIs
and CCIs. To fill this gap, we propose PCREQ, the first approach to
automatically infer compatible requirements by combining version and code
compatibility analysis. PCREQ integrates six modules: knowledge acquisition,
version compatibility assessment, invoked APIs and modules extraction, code
compatibility assessment, version change, and missing TPL completion. PCREQ
collects candidate versions, checks for conflicts, identifies API usage,
evaluates code compatibility, and iteratively adjusts versions to generate a
compatible requirements.txt with a detailed repair report. To evaluate PCREQ,
we construct REQBench, a large-scale benchmark with 2,095 upgrade test cases
(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%
inference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and
LLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each
case from REQBench in 60.79s on average, demonstrating practical efficiency.
PCREQ significantly reduces manual effort in troubleshooting upgrades,
advancing Python dependency maintenance automation.

</details>


### [16] [BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games](https://arxiv.org/abs/2508.02144)
*Yusaku Kato,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: 提出了BiFuzz双阶段模糊测试器，通过分阶段变异测试用例，有效发现开放世界游戏中的卡死问题，提升了自动化测试效果。


<details>
  <summary>Details</summary>
Motivation: 开放世界游戏因其广阔的搜索空间，相较于其他类型游戏，给自动化测试带来了更大挑战。因此需要更有效的测试方法来发现潜在缺陷。

Method: 提出了一种名为BiFuzz的两阶段模糊测试器，针对开放世界视频游戏进行自动化测试。BiFuzz通过分阶段变异游戏策略和测试用例，包括具体的移动路径，从而产生新的输入进行测试。

Result: BiFuzz能够逐步变异和扩展游戏策略、路径，能够检测到卡死（stucking）类型的失败案例。

Conclusion: BiFuzz在开放世界游戏自动化测试中展现了有效性，能够针对复杂路径变异并检测到特殊类型故障。工具和演示视频已在GitHub公开。

Abstract: Open-world video games present a broader search space than other games,
posing challenges for test automation. Fuzzing, which generates new inputs by
mutating an initial input, is commonly used to uncover failures. In this study,
we proposed BiFuzz, a two-stage fuzzer designed for automated testing of
open-world video games, and investigated its effectiveness. The results
revealed that BiFuzz mutated the overall strategy of gameplay and test cases,
including actual movement paths, step by step. Consequently, BiFuzz can detect
`stucking' failures. The tool and its video are at
https://github.com/Yusaku-Kato/BiFuzz.

</details>


### [17] [An MLIR-based Compilation Framework for Control Flow Management on CGRAs](https://arxiv.org/abs/2508.02167)
*Yuxuan Wang,Cristian Tirelli,Giovanni Ansaloni,Laura Pozzi,David Atienza*

Main category: cs.SE

TL;DR: 本文提出一种针对CGRAs的编译框架，显著提升对复杂控制流应用的支持与性能，无需额外硬件，仅靠软件优化实现最高2.1倍加速。


<details>
  <summary>Details</summary>
Motivation: CGRAs（粗粒度可重构阵列）因高灵活性和高效率广受关注，被视为高强度工作负载加速的理想方案。但CGRAs的编译难题——尤其是在空间和时间两个维度的复杂性——限制了其大范围应用。现有编译工具主要支持数据流，对控制流的支持很有限，通常仅映射单一循环，复杂的控制流分支则依赖特殊硬件单元处理。

Method: 提出一种模块化编译框架，在代码变换和优化阶段对任意控制流的应用进行支持，并在CGRA虚拟网格上进行映射。此外，提出新的映射方法作为编译后端，提升现有CGRA硬件资源利用率，并保证编译过程可行。

Result: 该编译框架能在无需专用硬件支持情况下处理广泛的应用控制流，且提高编译后程序性能。实验结果显示，框架相较于最先进方法可实现最高2.1倍的加速效果，优化完全来自编译策略。

Conclusion: 通过编译级别的控制流管理与优化，无需依赖专用硬件即可显著提升CGRAs的应用范围和性能。该模块化方法拓展了支持的应用类，并以高效映射和资源利用为核心，让CGRAs具备更大实际应用潜力。

Abstract: Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility
and efficiency, making them well-suited for the acceleration of intensive
workloads. Nevertheless, a key barrier towards their widespread adoption is
posed by CGRA compilation, which must cope with a multi-dimensional space
spanning both the spatial and the temporal domains. Indeed, state-of-the-art
compilers are limited in scope as they mostly deal with the data flow of
applications, while having little or no support for control flow. Hence, they
mostly target the mapping of single loops and/or delegate the management of
control flow divergences to ad-hoc hardware units.
  Conversely, in this paper we show that control flow can be effectively
managed and optimized at the compilation level, allowing for a broad set of
applications to be targeted while being hardware-agnostic and achieving high
performance. We embody our methodology in a modular compilation framework
consisting of transformation and optimization passes, enabling support for
applications with arbitrary control flows running on abstract CGRA meshes. We
also introduce a novel mapping methodology that acts as a compilation back-end,
addressing the limitations in available CGRA hardware resources and
guaranteeing a feasible solution in the compilation process. Our framework
achieves up to 2.1X speedups over state-of-the-art approaches, purely through
compilation optimizations.

</details>


### [18] [Highly Interactive Testing for Uninterrupted Development Flow](https://arxiv.org/abs/2508.02176)
*Andrew Tropin*

Main category: cs.SE

TL;DR: 本文提出了一种新的库，将测试过程与高交互开发环境深度集成，实现了即时测试反馈和工具访问，有助于提升开发者的专注力和开发效率。


<details>
  <summary>Details</summary>
Motivation: 传统测试方式与高交互开发环境脱节，经常带来反馈延迟和缺乏运行时上下文，影响开发者注意力并破坏流畅的开发体验。

Method: 设计并实现了一个将测试以运行时形式表示的库，使测试可以与HIDE工具链（如调试器、堆栈检查器等）无缝结合，并展示了通过该库获得子秒级测试重执行能力的开发流程。

Result: 通过该库的集成，开发者可以在测试失败时马上利用HIDE工具，测试重执行时间可缩短至亚秒级，显著提升开发效率和持续专注。

Conclusion: 作者提出的库为测试提供了运行时表示，能与高交互开发环境（HIDE）紧密集成，在测试失败时可立即调用HIDE相关工具，大幅减少测试反馈延迟，改善开发流程的连续性。

Abstract: Highly interactive development environments (HIDEs) enable uninterrupted
development flow through continuous program evolution and rapid hypothesis
checking. However, traditional testing approaches -- typically executed
separately via CLI -- isolate tests from HIDE tooling (interactive debuggers,
value and stack inspectors, etc.) and introduce disruptive delays due to coarse
execution granularity and lack of runtime context. This disconnect breaks
development flow by exceeding critical attention thresholds. In this paper we
present a library that provides runtime representation for tests, allowing
tight integration with HIDEs, and enabling immediate access to HIDE tooling in
the context of test failure. We then describe development workflows enhanced
with testing and demonstrate how they achieve subsecond test reexecution times
crucial for maintaining developer focus.

</details>


### [19] [A Methodological Framework for LLM-Based Mining of Software Repositories](https://arxiv.org/abs/2508.02233)
*Vincenzo De Martino,Joel Castaño,Fabio Palomba,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 本文通过文献综述和问卷，系统梳理了大语言模型在软件仓库挖掘的应用方法、威胁及应对，提出PRIMES 2.0方法框架，助力提升研究透明度与可复现性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在软件工程中的研究应用日益增多，特别是在自动化仓库挖掘任务领域。但现有研究主要关注模型能力或性能评估，很少系统性地探讨如何在整个研究流程中整合LLMs，缺乏对其方法论应用的深入理解。

Method: 作者采用混合方法研究，结合快速综述和问卷调查，系统性梳理了LLM在软件仓库挖掘领域（LLM4MSR）的应用方法和影响实证严谨性的相关威胁。

Result: 研究归纳出15种方法学途径、9类主要威胁及25种应对策略，并进一步提出PRIMES 2.0经验框架。该框架涵盖六大阶段和23个方法学子步骤，对应具体威胁及应对措施，为LLM支持的MSR研究流程提供全生命周期的应对和建议。

Conclusion: 该成果为LLM支持的软件仓库挖掘研究提供了更透明和可复现的方法基础，有助于提升该领域研究的规范性与实证有效性。

Abstract: Large Language Models (LLMs) are increasingly used in software engineering
research, offering new opportunities for automating repository mining tasks.
However, despite their growing popularity, the methodological integration of
LLMs into Mining Software Repositories (MSR) remains poorly understood.
Existing studies tend to focus on specific capabilities or performance
benchmarks, providing limited insight into how researchers utilize LLMs across
the full research pipeline. To address this gap, we conduct a mixed-method
study that combines a rapid review and questionnaire survey in the field of
LLM4MSR. We investigate (1) the approaches and (2) the threats that affect the
empirical rigor of researchers involved in this field. Our findings reveal 15
methodological approaches, nine main threats, and 25 mitigation strategies.
Building on these findings, we present PRIMES 2.0, a refined empirical
framework organized into six stages, comprising 23 methodological substeps,
each mapped to specific threats and corresponding mitigation strategies,
providing prescriptive and adaptive support throughout the lifecycle of
LLM-based MSR studies. Our work contributes to establishing a more transparent
and reproducible foundation for LLM-based MSR research.

</details>


### [20] [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
*Mikio Nakano,Hironori Takeuchi,Sadahiro Yoshikawa,Yoichi Matsuyama,Kazunori Komatani*

Main category: cs.SE

TL;DR: 本文提出“对话系统工程”新领域，基于SWEBOK体系梳理其核心知识点，总结未解问题并展望未来，推动对话系统工程化发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型促进了对话系统的核心技术进步，对话系统在社会问题和商业场景中的应用需求增长，因此需要对其全生命周期的构建、运营和持续改进进行系统性研究，推动专属于对话系统的软件工程演进。

Method: 本文提出“对话系统工程”这一新领域，依据SWEBOK V4.0的软件工程知识体系，列举并调研了对话系统工程的知识领域，然后识别每一领域中的未探索话题并分析其未来发展方向。

Result: 通过调研，本文全面梳理了对话系统工程的各核心知识领域，并指出了当前尚未被充分研究的领域及未来可能的研究方向。

Conclusion: 对话系统的广泛应用对其工程方法提出新要求，除了借鉴传统软件工程知识外，还需发展针对对话系统特性的工程理论和方法，以完善其全生命周期管理。本文为“对话系统工程”奠定了理论基础和未来研究框架。

Abstract: This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.

</details>


### [21] [Interoperable verification and dissemination of software assets in repositories using COAR Notify](https://arxiv.org/abs/2508.02335)
*Matteo Cancellieri,Martin Docekal,David Pride,Morane Gruenpeter,David Douard,Petr Knoth*

Main category: cs.SE

TL;DR: 本文提出了SoFAIR项目，通过机器学习从论文中提取软件引用并自动归档、传播，提升科研软件的可发现性和影响力，推进FAIR原则落地。


<details>
  <summary>Details</summary>
Motivation: 开源科研软件在学术论文中常常难以被发现、归属和复用，这限制了其学术影响力和可持续利用。作者希望解决科研软件“沉默”在文献中的问题，提高其可发现性、可溯源性和可复用性。

Method: SoFAIR项目采用机器学习工具，从科研论文中自动提取软件引用信息，并整合仓库系统、作者、HAL和Software Heritage等服务，实现软件的归档、引用和访问。还集成了COAR Notify协议，用于实现不同系统（如仓库和作者）之间的互操作与自动通信，从而验证和传播软件引用信息。

Result: 论文描述了SoFAIR工作流程和COAR Notify协议的集成实现，演示了如何通过自动化和标准化手段提升科研软件在学术体系中的可见度和学术信誉。

Conclusion: 通过SoFAIR项目，实现了开源科研软件的发现、归档与传播自动化和标准化，提升了其作为一等科研成果的地位，推动了FAIR（可查找、可获取、可互操作、可复用）原则在科研软件领域的应用。

Abstract: The discoverability, attribution, and reusability of open research software
are often hindered by its obscurity within academic manuscripts. To address
this, the SoFAIR project (2024-2025) introduces a comprehensive workflow
leveraging machine learning tools for extracting software mentions from
research papers. The project integrates repository systems, authors, and
services like HAL and Software Heritage to ensure proper archiving, citation,
and accessibility of research software in alignment with FAIR principles. To
enable interoperable communication across the various systems we present an
integration of the COAR Notify Protocol, which facilitates automated,
interoperable communication among repositories and authors to validate and
disseminate software mentions. This paper outlines the SoFAIR workflow and the
implementation of the COAR Notify Protocol, emphasising its potential to
enhance the visibility and credibility of research software as first-class
bibliographic records.

</details>


### [22] [Vision Language Model-based Testing of Industrial Autonomous Mobile Robots](https://arxiv.org/abs/2508.02338)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 作者提出基于视觉-语言模型的AMR测试体系，可自动生成多样化的人类干扰场景，并在仿真中发现更多机器人潜在安全缺陷，显著优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动移动机器人（AMRs）广泛用于仓库、零售和办公等多种环境中，并且需要与人类协作，但由于人类行为的不可预测性和机器人训练的局限性，保证它们的安全性具有挑战。实际环境下的测试成本高昂且有风险，因此需要新的高效测试方法。

Method: 本文提出了一种基于视觉-语言模型（VLM）的测试方法（RVSG），该方法根据功能和安全需求，通过VLM自动生成违反这些需求的多样化人类行为，用于在模拟器中测试AMR。

Result: 实验在PAL Robotics公司的最新AMR及多条导航路径和多条需求下进行，对比基线方法，RVSG能够更有效地生成违反需求的测试场景，增加了AMR行为的多样性，更好地揭示了其不确定性。

Conclusion: VLM驱动的RVSG测试体系能为工业AMR提供更全面、有效和安全的行为测试，显著提升了机器人行为的不确定性暴露能力。

Abstract: Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,
warehouses, retail spaces, and offices), where they work alongside humans.
Given that human behavior can be unpredictable and that AMRs may not have been
trained to handle all possible unknown and uncertain behaviors, it is important
to test AMRs under a wide range of human interactions to ensure their safe
behavior. Moreover, testing in real environments with actual AMRs and humans is
often costly, impractical, and potentially hazardous (e.g., it could result in
human injury). To this end, we propose a Vision Language Model (VLM)-based
testing approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.
Based on the functional and safety requirements, RVSG uses the VLM to generate
diverse human behaviors that violate these requirements. We evaluated RVSG with
several requirements and navigation routes in a simulator using the latest AMR
from PAL Robotics. Our results show that, compared with the baseline, RVSG can
effectively generate requirement-violating scenarios. Moreover, RVSG-generated
scenarios increase variability in robot behavior, thereby helping reveal their
uncertain behaviors.

</details>


### [23] [JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis](https://arxiv.org/abs/2508.02397)
*Lida Zhao,Chaofan Li,Yueming Wu,Lyuye Zhang,Jiahui Wu,Chengwei Liu,Sen Chen,Yutao Hu,Zhengzi Xu,Yi Liu,Jingquan Ge,Jun Sun,Yang Liu*

Main category: cs.SE

TL;DR: 该研究提出并验证了JC-Finder，为Java项目提供有效的代码克隆式第三方库检测，与现有方法相比，在检测准确率、效率和覆盖率上均优于主流工具。


<details>
  <summary>Details</summary>
Motivation: 第三方库（TPL）虽然有助于软件开发，但其无序管理对维护安全造成威胁，且源码的未授权使用引发伦理与版权问题。Java中的TPL通过代码克隆引入较常见，但现有SCA技术缺乏专为Java设计的克隆检测工具，导致识别准确性和效率问题。

Method: 提出了JC-Finder，一种新的基于克隆的SCA工具，特别为Java项目服务。其方法包括在类级别提取特征，保持函数间关系，并排除无效或重复元素，用于高效且准确地检测通过克隆引入的TPL复用。

Result: JC-Finder在9,965个Maven库和1,000个GitHub项目的评测中，F1分数达到0.818，较现有函数级工具高0.427，检测平均用时14.2秒，比对方工具快约9倍。进一步分析7,947个GitHub项目，发现789个项目中存在由代码克隆引入的TPL复用，检出2,142个TPL，且比包管理器未声明的TPL多检测26.20%。

Conclusion: JC-Finder能更高效、全面、准确地检测Java项目中通过源码克隆引入的第三方库复用，弥补了现有工具的不足，并能发现常被忽视的未在包管理器声明的第三方库。

Abstract: While reusing third-party libraries (TPL) facilitates software development,
its chaotic management has brought great threats to software maintenance and
the unauthorized use of source code also raises ethical problems such as
misconduct on copyrighted code. To identify TPL reuse in projects, Software
Composition Analysis (SCA) is employed, and two categories of SCA techniques
are used based on how TPLs are introduced: clone-based SCA and
package-manager-based SCA (PM-based SCA). Although introducing TPLs by clones
is prevalent in Java, no clone-based SCA tools are specially designed for Java.
Also, directly applying clone-based SCA techniques from other tools is
problematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA
tool that aims to accurately and comprehensively identify instances of TPL
reuse introduced by source code clones in Java projects. JC-Finder achieves
both accuracy and efficiency in identifying TPL reuse from code cloning by
capturing features at the class level, maintaining inter-function
relationships, and excluding trivial or duplicated elements. To evaluate the
efficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as
reference data and tested the TPL reuse of 1,000 GitHub projects. The result
shows that JC-Finder achieved an F1-score of 0.818, outperforming the other
function-level tool by 0.427. The average time taken for resolving TPL reuse is
14.2 seconds, which is approximately 9 times faster than the other tool. We
further applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code
clones in 789 projects (about 9.89% of all projects) and identifying a total of
2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not
explicitly declared in package managers.

</details>


### [24] [Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots](https://arxiv.org/abs/2508.02407)
*Xinyi Wang,Qinghua Xu,Paolo Arcaini,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 作者提出用量子与传统神经网络结合的方法构建机器人软件回归测试oracle，预测精度提升15%，解决了复杂场景下难以定义测试判据的问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器人广泛应用于日常生活，其软件频繁升级后导致回归测试变得重要。然而，针对机器人在未知环境下的正确行为制定测试判据（oracle）极具挑战性。为此，作者尝试用机器学习提升回归测试的有效性。

Method: 提出了一个混合测试oracle框架QuReBot，结合量子水库计算（QRC）与简单神经网络（利用残差连接思想），以预测机器人在各种环境下的期望行为。

Result: 实验结果表明，单独使用QRC无法收敛且预测误差高，而QuReBot成功收敛，且在预测误差上比传统神经网络基线减少了15%。还探讨了不同配置下的表现并给出最佳实践建议。

Conclusion: 混合量子和经典神经网络的方法可提升自动移动机器人的回归测试oracle的预测精度，对未来机器人软件测试实践具有参考意义。

Abstract: Robots are increasingly becoming part of our daily lives, interacting with
both the environment and humans to perform their tasks. The software of such
robots often undergoes upgrades, for example, to add new functionalities, fix
bugs, or delete obsolete functionalities. As a result, regression testing of
robot software becomes necessary. However, determining the expected correct
behavior of robots (i.e., a test oracle) is challenging due to the potentially
unknown environments in which the robots must operate. To address this
challenge, machine learning (ML)-based test oracles present a viable solution.
This paper reports on the development of a test oracle to support regression
testing of autonomous mobile robots built by PAL Robotics (Spain), using
quantum machine learning (QML), which enables faster training and the
construction of more precise test oracles. Specifically, we propose a hybrid
framework, QuReBot, that combines both quantum reservoir computing (QRC) and a
simple neural network, inspired by residual connection, to predict the expected
behavior of a robot. Results show that QRC alone fails to converge in our case,
yielding high prediction error. In contrast, QuReBot converges and achieves 15%
reduction of prediction error compared to the classical neural network
baseline. Finally, we further examine QuReBot under different configurations
and offer practical guidance on optimal settings to support future robot
software testing.

</details>


### [25] [TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs](https://arxiv.org/abs/2508.02455)
*Daniele Cipollone,Egor Bogomolov,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 提出了一种结合语言模型、轻量高效的新型代码补全排序方法，可显著提升IDE中代码补全的准确性与实用性，且易于现有系统集成。


<details>
  <summary>Details</summary>
Motivation: 现有的代码补全功能依赖静态分析，并通过启发式或基于日志的简单机器学习模型进行排序，但这些方法在利用上下文和适应不同项目、编码风格方面存在局限。提升补全排序准确性和上下文感知能力是核心需求。

Method: 提出了一种新颖的排序方法：利用语言模型，以轻量级、与模型无关的方式对静态补全结果评分。具体做法是将所有合法补全组织成前缀树，并通过一次贪心式解码获得整个树上每个token级别的分数，实现无需梁搜索、提示工程或模型调整的精准排序。

Result: 该方法速度快、架构无关，可与现有代码补全模型兼容；能够实现精确的token感知排序，有效提升补全建议的相关性和响应速度，并易于集成到现有IDE工具。

Conclusion: 通过引入基于语言模型的补全排序方法，为现有IDE集成智能、响应更快的补全助手提供了一条实际有效的路径，促进开发效率提升。

Abstract: Token-level code completion is one of the most critical features in modern
Integrated Development Environments (IDEs). It assists developers by suggesting
relevant identifiers and APIs during coding. While completions are typically
derived from static analysis, their usefulness depends heavily on how they are
ranked, as correct predictions buried deep in the list are rarely seen by
users. Most current systems rely on hand-crafted heuristics or lightweight
machine learning models trained on user logs, which can be further improved to
capture context information and generalize across projects and coding styles.
In this work, we propose a new scoring approach to ranking static completions
using language models in a lightweight and model-agnostic way. Our method
organizes all valid completions into a prefix tree and performs a single greedy
decoding pass to collect token-level scores across the tree. This enables a
precise token-aware ranking without needing beam search, prompt engineering, or
model adaptations. The approach is fast, architecture-agnostic, and compatible
with already deployed models for code completion. These findings highlight a
practical and effective pathway for integrating language models into already
existing tools within IDEs, and ultimately providing smarter and more
responsive developer assistance.

</details>


### [26] [An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs](https://arxiv.org/abs/2508.02473)
*Xinfang Chen,Siyang Xiao,Xianying Zhu,Junhong Xie,Ming Liang,Dajun Chen,Wei Jiang,Yong Li,Peng Di*

Main category: cs.SE

TL;DR: 本文提出NES代码编辑框架，通过分析历史编辑数据实现无需指令、低延迟的智能编辑建议，准确率和效率均超越主流模型，已被大规模实际应用，相关数据集也支持开源模型表现提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的代码编辑工具普遍依赖明确的自然语言指令，且延迟较高，难以高效集成到开发者工作流中。作者发现开发者过去的代码编辑行为和目标中蕴含丰富信息，有机会减少对指令依赖并提升效率。

Method: 提出NES（Next Edit Suggestion）框架，基于双模型结构，并使用高质量的SFT和DAPO数据集进行训练。NES无需指令，能快速推断下一个编辑位置并理解开发者意图，通过连续Tab操作降低使用延迟。

Result: NES在真实数据集上，下一个编辑位置预测任务表现优异（准确率分别为75.6%和81.6%），在意图对齐编辑任务上获得91.36%的ES和27.7%的EMR，超越现有最优模型。同时，开源的SFT和DAPO数据集提升了开源CodeLLMs的表现。

Conclusion: NES解决了现有AI代码编辑工具对指令依赖强和延迟高的问题，为大规模工业应用提供了可扩展和实用的低延迟解决方案，已被头部金融科技公司大规模采用。

Abstract: Code editing, including modifying, refactoring, and maintaining existing
code, is the most frequent task in software development and has garnered
significant attention from AI-powered tools. However, existing solutions that
translate explicit natural language instructions into code edits face critical
limitations, such as heavy reliance on human instruction input and high
latency, which hinder their effective integration into a developer's workflow.
We observe that developers' habitual behaviors and coding objectives are often
reflected in their historical editing patterns, making this data key to
addressing existing limitations. To leverage these insights, we propose NES
(Next Edit Suggestion), an LLM-driven code editing framework that delivers an
instruction-free and low-latency experience. Built on a dual-model architecture
and trained with our high-quality SFT and DAPO datasets, NES enhances
productivity by understanding developer intent while optimizing inference to
minimize latency. NES is a scalable, industry-ready solution with a continuous
Tab key interaction workflow, seamlessly adopted by a FinTech company with over
20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%
and 81.6% accuracy in two tasks of predicting next edit locations, alongside
91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.
Our open-sourced SFT and DAPO datasets have been demonstrated to enhance the
performance of open-source CodeLLMs. The demonstration of NES is available at
https://youtu.be/yGoyYOe6fbY.

</details>


### [27] [Commit Stability as a Signal for Risk in Open-Source Projects](https://arxiv.org/abs/2508.02487)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: 现有开源软件项目多难以保持工作稳定性，仅少数仓库治理成熟能实现高韧性，仅靠commit量难以量化风险，建议采用多维度指标评估项目健康。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）已成为全球基础设施的核心，创造了巨大的经济价值，企业对其依赖性不断上升。当前对开源项目健康的研究很多，但项目在受到人员变动、安全问题和报告激增等冲击后的“韧性”——即恢复正常运作能力，尚缺乏深入研究。本文希望探究影响项目韧性的关键因素。

Method: 以Composite Stability Index（CSI）为基础，对100个高排名开源代码库的commit频率模式进行实证验证，统计其在日、周、月不同时间粒度下的稳定性，并观察语言类型、应用领域等变量对稳定性的影响。还分析了实现三项粒度稳定性的典型代码库的治理模式、CI频率和发布策略。

Result: 仅有2%的仓库表现出日稳定性，29%为周稳定，50%为月稳定，有一半仓库在所有粒度都不稳定。编程语言相关和区块链应用最稳定。两个仓库在所有粒度均达到稳定，其治理与流程可作为参考。每年commit数量高不代表稳定。此外，稳定性评估可考虑issue解决时长、PR合并速率和社区活跃度等多种指标。

Conclusion: 绝大多数OSS项目在commit活动上的高粒度“韧性”不足，治理完善和开发流程规范有助于提升稳定性。单靠commit数量无法充分评估项目的韧性，需要引入更多多维度的社区及开发活动指标。

Abstract: Open source software (OSS) generates trillions of dollars in economic value
and has become essential to technical infrastructures worldwide. As
organizations increasingly depend on OSS, understanding project evolution is
critical. While existing metrics provide insights into project health, one
dimension remains understudied: project resilience -- the ability to return to
normal operations after disturbances such as contributor departures, security
vulnerabilities, and bug report spikes. We hypothesize that stable commit
patterns reflect underlying project characteristics such as mature governance,
sustained contributors, and robust development processes that enable
resilience. Building on the Composite Stability Index (CSI) framework, we
empirically validate commit frequency patterns across 100 highly ranked
repositories. Our findings reveal that only 2\% of repositories exhibit daily
stability, 29\% achieve weekly stability, and 50\% demonstrate monthly
stability, while half remain unstable across all temporal levels. Programming
languages and blockchain applications were the most stable. We identified two
exemplary repositories that achieved stability at all three granularities,
whose governance models, CI cadence, and release policies could serve as
reference frameworks. We observed that large yearly commit throughput does not
necessarily correlate with stability. Beyond commits, stability can be enriched
with issue-resolution times, PR merge rates, and community-engagement metrics
to broaden resilience assessment and sharpen stability-based risk evaluation.

</details>


### [28] [Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation](https://arxiv.org/abs/2508.02497)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: 开源技术文档多为英文，人工翻译稀缺。大语言模型译文准确但结构和格式保真度有欠缺。TRIFID框架有助于自动检测译文保真度，推动自动化文档国际化。


<details>
  <summary>Details</summary>
Motivation: 开源社区汇集了来自全球的多样化贡献者，但只有少数代码库提供英文以外语言的文档。技术文档中自然语言、代码与格式混杂，人工翻译难度较高。因此有必要评估大语言模型（LLMs）在技术文档翻译方面的能力和实际需求。

Method: 首先评估了开源社区文档的人工翻译活动（主要面向英文转德文），然后用ChatGPT 4和Anthropic Claude对50个README文件进行了自动翻译，并比较了两者表现。进一步分析了模型输出的翻译保真度，包括结构成分（如超链接）和格式一致性。最后提出了自动化保真度评分框架TRIFID用于检测翻译内容对代码、链接和格式的保留情况。

Result: 人工翻译活动稀少，主要集中在大型且社区驱动的代码库。LLMs能产出较准确的译文，但在结构成分和格式一致性上存在保真度问题。TRIFID初步展示了自动检测译文保真度的可行性。

Conclusion: LLMs在开源技术文档翻译方面具有潜力，可提升自动化支持，但仍面临结构和格式保真等挑战。提出的TRIFID为构建翻译感知型持续集成打下基础，有助于未来完善自动化文档国际化流程。

Abstract: While open source communities attract diverse contributors globally, few
repositories provide essential documentation in languages other than English.
Large language models (LLMs) have demonstrated remarkable capabilities in
software engineering tasks and translations across domains. However, little is
known about LLM capabilities in translating open-source technical
documentation, which mixes natural language, code, URLs, and markdown
formatting. To understand the need and potential for LLMs in technical
documentation translation, we evaluated community translation activity and
English-to-German translations of 50 README files using OpenAI's ChatGPT 4 and
Anthropic's Claude. We found scarce translation activity, mostly in larger
repositories and community-driven in nature. LLM performance comparison
suggests they can provide accurate translations. However, analysis revealed
fidelity challenges: both models struggled to preserve structural components
(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings
highlight both promise and challenges of LLM-assisted documentation
internationalization. As a first step toward translation-aware continuous
integration pipelines, we introduce TRIFID, an early-stage translation fidelity
scoring framework that automatically checks how well translations preserve
code, links, and formatting. Our efforts provide a foundation for automated
LLM-driven support for creating and maintaining open source documentation.

</details>


### [29] [Automatic Identification of Machine Learning-Specific Code Smells](https://arxiv.org/abs/2508.02541)
*Peter Hamfelt,Ricardo Britto,Lincoln Rocha,Camilo Almendra*

Main category: cs.SE

TL;DR: 本研究提出了专为ML应用设计的代码异味检测工具MLpylint，经实验和专家问卷验证，结果表明该工具切实有效且有助于优化ML代码开发。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习（ML）广泛应用于各行各业，对ML代码异味（code smell）的研究需求增加，但现有针对ML代码异味的识别和有效性工具与研究较为匮乏。

Method: 采用设计科学方法论。首先，通过文献综述识别ML特有的代码异味。在方案设计阶段，进一步文献综述并咨询专家，选取合适的方法和工具实现静态代码分析工具MLpylint。然后以160个GitHub开源ML应用为数据集进行工具评估，并通过15位ML专家参与的问卷调查进行静态验证。

Result: 实验结果表明，MLpylint具备有效性和实用性。

Conclusion: 本研究设计并开发了用于ML代码异味静态检测的工具MLpylint，并验证了其有效性和实用性。下一步将探索如何将该工具无缝集成到开发流程中，促进开发者环境的生产力与创新。

Abstract: Machine learning (ML) has rapidly grown in popularity, becoming vital to many
industries. Currently, the research on code smells in ML applications lacks
tools and studies that address the identification and validity of ML-specific
code smells. This work investigates suitable methods and tools to design and
develop a static code analysis tool (MLpylint) based on code smell criteria.
This research employed the Design Science Methodology. In the problem
identification phase, a literature review was conducted to identify ML-specific
code smells. In solution design, a secondary literature review and
consultations with experts were performed to select methods and tools for
implementing the tool. We evaluated the tool on data from 160 open-source ML
applications sourced from GitHub. We also conducted a static validation through
an expert survey involving 15 ML professionals. The results indicate the
effectiveness and usefulness of the MLpylint. We aim to extend our current
approach by investigating ways to introduce MLpylint seamlessly into
development workflows, fostering a more productive and innovative developer
environment.

</details>


### [30] [Meta-RAG on Large Codebases Using Code Summarization](https://arxiv.org/abs/2508.02611)
*Vali Tawosia,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 作者提出用Meta-RAG方法对庞大代码库进行摘要后，结合LLM做bug定位，在标准数据集上文件级和函数级都取得了最佳定位表现，明显提升了bug排查效率。


<details>
  <summary>Details</summary>
Motivation: 软件开发流程中，代码维护和错误定位是一项复杂且重要的工作。随着大语言模型（LLM）在自动化代码任务上的应用不断深入，作者希望进一步探索如何利用LLM和信息检索技术提升大规模现有代码库中的bug定位能力。

Method: 提出了一种多智能体系统，结合了信息检索和大语言模型，实现bug定位。他们创新性地引入了Meta-RAG（检索增强生成）方案——先用代码摘要对庞大代码库进行压缩，再利用LLM对浓缩呈现的代码做关键区域定位。

Result: 该系统在SWE-bench Lite数据集上，通过Meta-RAG方法，实现了文件级别84.67%、函数级别53.0%的正确定位率，均达到当前最优水平。

Conclusion: Meta-RAG能极大提升代码库bug定位效率与准确性，展示了LLM与RAG方法在代码维护场景下的强大潜力。

Abstract: Large Language Model (LLM) systems have been at the forefront of applied
Artificial Intelligence (AI) research in a multitude of domains. One such
domain is software development, where researchers have pushed the automation of
a number of code tasks through LLM agents. Software development is a complex
ecosystem, that stretches far beyond code implementation and well into the
realm of code maintenance. In this paper, we propose a multi-agent system to
localize bugs in large pre-existing codebases using information retrieval and
LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)
approach, Meta-RAG, where we utilize summaries to condense codebases by an
average of 79.8\%, into a compact, structured, natural language representation.
We then use an LLM agent to determine which parts of the codebase are critical
for bug resolution, i.e. bug localization. We demonstrate the usefulness of
Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores
84.67 % and 53.0 % for file-level and function-level correct localization
rates, respectively, achieving state-of-the-art performance.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [31] [Expressive Power of Graph Transformers via Logic](https://arxiv.org/abs/2508.01067)
*Veeti Ahvonen,Maurice Funk,Damian Heiman,Antti Kuusisto,Carsten Lutz*

Main category: cs.LO

TL;DR: 该文系统梳理并刻画了主流图Transformer和GPS结构在不同数系下的表达极限，并将其与模态逻辑的表达能力进行等价对齐，为理论大家和实际工程提供了边界认知。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在大语言模型中应用广泛，但它们在图结构表达上的精确能力尚不明确，因此希望通过理论分析，揭示它们在图结构任务中的表达极限。

Method: 分析两类主流图神经网络（GTs和GPS-networks）的表达能力，分别在理论上（实数情况下）与实践中（浮点数情况下），并将它们与一阶逻辑和分级模态逻辑进行对比。

Result: 在实数系下，GPS-networks的表达能力等价于带全局模态的分级模态逻辑（GML）；在浮点数情况下，则与带计数全局模态的GML等价。对于GTs，则与带全局模态和带计数全局模态的命题逻辑分别等价（分别对应实数和浮点数）。

Conclusion: 论文揭示了图Transformer（GTs）和GPS-networks在图结构上表达能力的理论边界，并系统刻画了它们与逻辑系统的等价关系。

Abstract: Transformers are the basis of modern large language models, but relatively
little is known about their precise expressive power on graphs. We study the
expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and
GPS-networks by Ramp\'asek et al. (2022), both under soft-attention and average
hard-attention. Our study covers two scenarios: the theoretical setting with
real numbers and the more practical case with floats. With reals, we show that
in restriction to vertex properties definable in first-order logic (FO),
GPS-networks have the same expressive power as graded modal logic (GML) with
the global modality. With floats, GPS-networks turn out to be equally
expressive as GML with the counting global modality. The latter result is
absolute, not restricting to properties definable in a background logic. We
also obtain similar characterizations for GTs in terms of propositional logic
with the global modality (for reals) and the counting global modality (for
floats).

</details>


### [32] [Relative Completeness of Incorrectness Separation Logic](https://arxiv.org/abs/2508.01535)
*Yeonseok Lee,Koji Nakazawa*

Main category: cs.LO

TL;DR: 本文首次证明了不正确性分离逻辑（ISL）的相对完备性，通过引入能够表达无限析取和变量别名处理的弱后条件计算方法，拓展了该理论在程序堆分析与错误检测中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然ISL已被证明具有可靠性（soundness），但其完备性尚未被证明，本文旨在解决这一理论空白，通过提升表达能力和理论工具完善ISL基础。

Method: 在ISL框架下，允许析取范式中的无限析取，每个子句由带有存在量词的有限符号堆组成，并提出了一种包含变量别名的标准化方法来计算ISL的弱后条件。

Result: 证明了ISL的相对完备性，为该逻辑在程序错误检测领域的理论基础提供了关键支持。

Conclusion: 本文通过引入弱后条件（weakest postconditions）的表达能力，首次建立了不正确性分离逻辑（ISL）的相对完备性。

Abstract: Incorrectness Separation Logic (ISL) is a proof system that is tailored
specifically to resolve problems of under-approximation in programs that
manipulate heaps, and it primarily focuses on bug detection. This approach is
different from the over-approximation methods that are used in traditional
logics such as Hoare Logic or Separation Logic. Although the soundness of ISL
has been established, its completeness remains unproven. In this study, we
establish relative completeness by leveraging the expressiveness of the weakest
postconditions; expressiveness is a factor that is critical to demonstrating
relative completeness in Reverse Hoare Logic. In our ISL framework, we allow
for infinite disjunctions in disjunctive normal forms, where each clause
comprises finite symbolic heaps with existential quantifiers. To compute the
weakest postconditions in ISL, we introduce a canonicalization that includes
variable aliasing.

</details>


### [33] [Causality and Decision-making: A Logical Framework for Systems and Security Modelling](https://arxiv.org/abs/2508.01758)
*Pinaki Chakraborty,Tristan Caulfield,David Pym*

Main category: cs.LO

TL;DR: 提出了一种将系统建模、战略推理和因果反事实方法统一的理论框架，并用分布式系统中的微服务案例验证，为复杂系统中的因果性决策提供了形式化方法论基础。


<details>
  <summary>Details</summary>
Motivation: 理解支撑现代社会的复杂系统生态中的决策过程中的因果推理十分重要，尤其在系统的安全性（正确性、安全性、鲁棒性等）问题中尤为关键。然而，建立一种可以形式化表达系统因果性和决策过程的理论一直具有挑战性。

Method: 提出了基于最小结构假设的系统建模战略推理理论，采用了转移系统方法，并结合了van Benthem、Hennessy和Milner传统中的模态逻辑。框架设计了干预算子与分离合取，用以刻画子系统间的实际因果关系，并与Halpern和Pearl的结构因果模型反事实方法兼容。理论的有效性通过等价性定理进行了验证。

Result: 框架能够形式化表达系统行为和因果推理，为分析复杂系统中决策制定的因果性问题提供了逻辑基础。通过微服务和分布式系统决策制定的案例，展示了理论的适用性。

Conclusion: 本文统一了形式化、极简主义的系统行为定义与Halpern-Pearl兼容的反事实推理理论，为研究复杂交互系统中的因果性决策提供了坚实的逻辑基础。

Abstract: Causal reasoning is essential for understanding decision-making about the
behaviour of complex `ecosystems' of systems that underpin modern society, with
security -- including issues around correctness, safety, resilience, etc. --
typically providing critical examples. We present a theory of strategic
reasoning about system modelling based on minimal structural assumptions and
employing the methods of transition systems, supported by a modal logic of
system states in the tradition of van Benthem, Hennessy, and Milner, and
validated through equivalence theorems. Our framework introduces an
intervention operator and a separating conjunction to capture actual causal
relationships between component systems of the ecosystem, aligning naturally
with Halpern and Pearl's counterfactual approach based on Structural Causal
Models. We illustrate the applicability through examples of of decision-making
about microservices in distributed systems. We discuss localized
decision-making through a separating conjunction. This work unifies a formal,
minimalistic notion of system behaviour with a Halpern--Pearl-compatible theory
of counterfactual reasoning, providing a logical foundation for studying
decision making about causality in complex interacting systems.

</details>


### [34] [Separation Logic of Generic Resources via Sheafeology](https://arxiv.org/abs/2508.01866)
*Berend van Starkenburg,Henning Basold,Chase Ford*

Main category: cs.LO

TL;DR: 这篇论文提出了一个基于sheaf和范畴逻辑的通用分离逻辑理论，可支持不同类型资源的程序推理，克服了以往只针对指针或特殊资源的限制。


<details>
  <summary>Details</summary>
Motivation: 分离逻辑虽然在指针程序验证和其他资源结构程序中的应用非常广泛，但现有方法尚不能将一阶逻辑与分离连接词结合到可适用于各种资源的理论中。作者希望提出一种更通用的理论框架。

Method: 本文提出将分离逻辑理解为‘资源感知’的一阶逻辑，利用范畴逻辑中的fiber（纤维化）和sheaf（层）技术，将一阶逻辑内部化到层范畴中，使逻辑能够处理资源视图，从而本质上实现对各种资源的局部化与组合。

Result: 作者构造了一种在sheaf范畴内部的fiber结构，用以建模资源上的谓词，支持一阶及分离连接词，并验证了该框架能实例化到不同内存模型和随机变量等场景，展示了广泛适用性。

Conclusion: 提出的sheafeology框架将分离逻辑推广到了通用资源，既保留了局部推理和扩展性，又能用于构建针对多种资源类型的程序逻辑，为相关领域的理论发展和应用工具提供了基础。

Abstract: Separation logic was conceived in order to make the verification of pointer
programs scalable to large systems and it has proven extremely effective. The
key idea is that programs typically access only small parts of memory, allowing
for local reasoning. This idea is implemented in separation logic by extending
first-order logic with separating connectives, which inspect local regions of
memory. It turns that this approach not only applies to pointer programs, but
also to programs involving other resource structures. Various theories have
been put forward to extract and apply the ideas of separation logic more
broadly. This resulted in algebraic abstractions of memory and many variants of
separation logic for, e.g., concurrent programs and stochastic processes.
However, none of the existing approaches formulate the combination of
first-order logic with separating connectives in a theory that could
immediately yield program logics for different resources. In this paper, we
propose a framework based on the idea that separation logic can obtained by
making first-order logic resource-aware. First-order logic can be understood in
terms of categorical logic, specifically fibrations. Our contribution is to
make these resource-aware by developing categorical logic internally in
categories of sheaves, which is what we call sheafeology. The role of sheaves
is to model views on resources, through which resources can be localised and
combined, which enables the scalability promised by separation logic. We
contribute constructions of an internal fibration in sheaf categories that
models predicates on resources, and that admits first-order and separating
connectives. Thereby, we attain a general framework of separation logic for
generic resources, a claim we substantiate by instantiating our framework to
various memory models and random variables.

</details>


### [35] [Monitoring Hyperproperties over Observed and Constructed Traces](https://arxiv.org/abs/2508.02301)
*Marek Chalupa,Thomas A. Henzinger,Ana Oliveira da Costa*

Main category: cs.LO

TL;DR: 本文提出支持被动与主动轨迹量词的新型超属性监控方法，并在理论和实践中突破了异步、交替量词超属性的运行时监控难题，对并发与安全系统检测具有开创性价值。


<details>
  <summary>Details</summary>
Motivation: 监控复杂系统的运行时是否满足超属性（如线性化、一致性或变种的非干扰性）是分布式与安全领域的重大挑战，尤其是当属性涉及多重轨迹、交错量词时，现有监控技术无法处理。该文旨在突破此监控难题。

Method: 引入被动和主动两类轨迹量词——被动量词针对已观测轨迹，主动量词结合生成器函数用于主动构造可能从未观测到的轨迹。基于此，扩展hypernode逻辑，支持基于生成器函数的轨迹量词，并提出了能够处理无限域的超节点公式的运行时监控算法与原型实现。

Result: 提出的新监控算法首次能够监控含交替轨迹量词的异步超属性。论文实现了该算法，并在并发性与安全性相关的超属性上进行实验评估，验证了其实用性与有效性。

Conclusion: 该工作拓展了超属性运行时监控的边界，使得含主动轨迹生成机制的复杂属性在现实场景下能够自动化监测，对并发与安全系统的验证具有重要意义。

Abstract: We study the problem of monitoring at runtime whether a system fulfills a
specification defined by a hyperproperty, such as linearizability or variants
of non-interference. For this purpose, we introduce specifications with both
passive and active quantification over traces. While passive trace quantifiers
range over the traces that are observed, active trace quantifiers are
instantiated with \emph{generator functions}, which are part of the
specification. Generator functions enable the monitor to construct traces that
may never be observed at runtime, such as the linearizations of a concurrent
trace. As specification language, we extend hypernode logic with trace
quantifiers over generator functions and interpret these hypernode formulas
over possibly infinite domains. We present a corresponding monitoring
algorithm, which we implemented and evaluated on a range of hyperproperties for
concurrency and security applications. Our method enables, for the first time,
the monitoring of asynchronous hyperproperties that contain alternating trace
quantifiers.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [36] [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)
*Margarita Bugueño,Gerard de Melo*

Main category: cs.CL

TL;DR: 作者提出利用自注意力和统计滤波自动学习文档句子级图结构的方法，在多个数据集上优于传统手工设计图，提升了文档分类表现和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有文档分类中，基于图的模型能够更好地捕捉文档结构和上下文，但大多数图结构依赖于人工设计、领域知识或启发式规则，限制了泛化能力和自动化。

Method: 提出一种数据驱动的方法自动学习图结构，无需人工设计或领域依赖。具体做法是将句子作为节点，并用自注意力模型自动学习句子对之间的关联边。采用统计滤波方法保留强关联句子以优化图结构和减少图规模。

Result: 在三个文档分类数据集上，所提自动化学习图法准确率和F1分数均优于启发式图结构。统计滤波进一步提升分类鲁棒性。

Conclusion: 自动化图生成优于传统手工设计的图结构，在文档分类任务上表现更优，也为NLP其它场景应用提供新方法。

Abstract: In document classification, graph-based models effectively capture document
structure, overcoming sequence length limitations and enhancing contextual
understanding. However, most existing graph document representations rely on
heuristics, domain-specific rules, or expert knowledge. Unlike previous
approaches, we propose a method to learn data-driven graph structures,
eliminating the need for manual design and reducing domain dependence. Our
approach constructs homogeneous weighted graphs with sentences as nodes, while
edges are learned via a self-attention model that identifies dependencies
between sentence pairs. A statistical filtering strategy aims to retain only
strongly correlated sentences, improving graph quality while reducing the graph
size. Experiments on three document classification datasets demonstrate that
learned graphs consistently outperform heuristic-based graphs, achieving higher
accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness
of the statistical filtering in improving classification robustness. These
results highlight the potential of automatic graph generation over traditional
heuristic approaches and open new directions for broader applications in NLP.

</details>


### [37] [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)
*Hagyeong Shin,Binoy Robin Dalal,Iwona Bialynicka-Birula,Navjot Matharu,Ryan Muir,Xingwei Yang,Samuel W. K. Wong*

Main category: cs.CL

TL;DR: 为客服中心对话分析提出了3D范式事实性评估框架和FECT数据集，有效提升了AI生成解释性结论的事实性评估标准化和自动化水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉（生成与输入、参考资料或真实世界知识不相符的内容）的问题，尤其是在企业应用中，这种幻觉可能会影响商业决策。对于分析和总结客服中心对话的LLM来说，评估事实性尤为困难，因为涉及的观点和根因通常缺乏明确的真实性标签。

Method: 1. 提出3D范式（Decompose, Decouple, Detach），融合进人工标注准则和LLM评审指令，使事实性标签基于有语言学依据的评价标准。
2. 构建FECT数据集——一个用于客服中心对话分析中AI生成解释性结论事实性评估的基准数据集，采用3D范式标注。
3. 实验对齐LLM评审者对3D范式的理解和执行效果。

Result: 建立了一套新的人工标注和自动化事实性评估方案，并构建了基于该方案的FECT基准数据集。结果显示，3D范式有助于提升LLM输出事实性判断的一致性和可靠性。

Conclusion: 提出了一套新颖的3D事实性评估框架，并据此建立了FECT数据集，可以有效支持客服中心对话分析中的AI输出事实性自动评估，推动了解释性AI输出真实性评测的发展。

Abstract: Large language models (LLMs) are known to hallucinate, producing natural
language outputs that are not grounded in the input, reference materials, or
real-world knowledge. In enterprise applications where AI features support
business decisions, such hallucinations can be particularly detrimental. LLMs
that analyze and summarize contact center conversations introduce a unique set
of challenges for factuality evaluation, because ground-truth labels often do
not exist for analytical interpretations about sentiments captured in the
conversation and root causes of the business problems. To remedy this, we first
introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in
the human annotation guideline and the LLM-judges' prompt to ground the
factuality labels in linguistically-informed evaluation criteria. We then
introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality
\textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact
Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm.
Lastly, we report our findings from aligning LLM-judges on the 3D paradigm.
Overall, our findings contribute a new approach for automatically evaluating
the factuality of outputs generated by an AI system for analyzing contact
center conversations.

</details>


### [38] [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
*Ernesto L. Estevanell-Valladares,Suilan Estevez-Velarde,Yoan Gutiérrez,Andrés Montoyo,Ruslan Mitkov*

Main category: cs.CL

TL;DR: XAutoLM是一个结合元学习的自动化语言模型微调框架，能高效重用经验，显著降低运算成本并提升模型表现，优于现有baselines，并促进绿色AI发展。


<details>
  <summary>Details</summary>
Motivation: 在机器学习领域，模型选择和超参数优化通常需要专家借助领域知识决策，尤其在语言模型微调时，每次试错都会带来可观的计算和环境成本。目前尚无同时解决整个模型选择与超参数优化、注重资源效率的自动化框架。

Method: 提出了XAutoLM，一种结合元学习的AutoML框架，通过复用过往实验经验，自动从已存储的成功和失败案例中提取任务与系统层面的元特征，从而更高效地偏向于有潜力的配置，避免资源浪费在低效的尝试上。

Result: 在四个文本分类和两个问答基准上，XAutoLM在六个任务中五次超越了zero-shot优化器的最高F1分数，平均评测时间最多缩短4.5倍，错误率最多下降七倍，发现的优质管道数量最多提升50%，而简单的记忆式基线方法表现为负迁移。

Conclusion: XAutoLM有效提升了语言模型微调阶段的资源利用率和模型效果，有助于推动更绿色、更高效的自动化AI实践，相关代码和经验库已开放以促进NLP领域的绿色AI发展。

Abstract: Experts in machine learning leverage domain knowledge to navigate decisions
in model selection, hyperparameter optimisation, and resource allocation. This
is particularly critical for fine-tuning language models (LMs), where repeated
trials incur substantial computational overhead and environmental impact.
However, no existing automated framework simultaneously tackles the entire
model selection and HPO task for resource-efficient LM fine-tuning. We
introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past
experiences to optimise discriminative and generative LM fine-tuning pipelines
efficiently. XAutoLM learns from stored successes and failures by extracting
task- and system-level meta-features to bias its sampling toward fruitful
configurations and away from costly dead ends. On four text classification and
two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak
F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error
ratios by up to sevenfold, and uncovers up to 50% more pipelines above the
zero-shot Pareto front. In contrast, simpler memory-based baselines suffer
negative transfer. We release XAutoLM and our experience store to catalyse
resource-efficient, Green AI fine-tuning in the NLP community.

</details>


### [39] [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)
*Yiqun Chen,Erhan Zhang,Lingyong Yan,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Jiaxin Mao*

Main category: cs.CL

TL;DR: 本文提出了通过多智能体协作与动态流程规划的适应性RAG问答框架，在多个数据集上验证了其兼顾质量、成本与延迟的优势。


<details>
  <summary>Details</summary>
Motivation: 问答系统中，RAG架构提升了回答准确性并减少幻觉，但不同类型查询的复杂度不同，固定的RAG流程难以兼顾性能与成本效率。需要一种能动态适应不同查询复杂性的RAG系统。

Method: 提出了一种名为MAO-ARAG的自适应RAG框架，通过多智能体编排实现。系统定义多个执行智能体（如查询重写、文档选择、答案生成智能体），由规划智能体根据查询特征动态选取和组合合适的流程。通过强化学习训练规划智能体，在每轮基于F1分奖励和成本惩罚调整策略。

Result: 实验显示，该框架在多个QA数据集上，能够根据不同查询动态规划流程，提升答案质量，且代价和延迟控制在合理范围内。

Conclusion: MAO-ARAG框架通过多智能体编排和自适应流程规划，实现了在兼顾高质量答案与合理成本下，提升问答系统对不同查询的适应能力。

Abstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has
become pivotal in enhancing response accuracy and reducing hallucination
issues. The architecture of RAG systems varies significantly, encompassing
single-round RAG, iterative RAG, and reasoning RAG, each tailored to address
different types of queries. Due to the varying complexity of real-world
queries, a fixed RAG pipeline often struggles to balance performance and cost
efficiency across different queries. To address this challenge, we propose an
adaptive RAG framework called MAO-ARAG, which leverages multi-agent
orchestration. Our adaptive RAG is conceived as a multi-turn framework.
Specifically, we define multiple executor agents, representing typical RAG
modules such as query reformulation agents, document selection agent, and
generation agents. A planner agent intelligently selects and integrates the
appropriate agents from these executors into a suitable workflow tailored for
each query, striving for high-quality answers while maintaining reasonable
costs. During each turn, the planner agent is trained using reinforcement
learning, guided by an outcome-based reward (F1 score) and a cost-based
penalty, continuously improving answer quality while keeping costs within a
reasonable range. Experiments conducted on multiple QA datasets demonstrate
that our approach, which dynamically plans workflows for each query, not only
achieves high answer quality but also maintains both cost and latency within
acceptable limits.The code of MAO-ARAG is on
https://github.com/chenyiqun/Agentic-RAG.

</details>


### [40] [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)
*Farah Adeeba,Brian Dillon,Hassan Sajjad,Rajesh Bhatt*

Main category: cs.CL

TL;DR: 本文提出了用于评估多语LLM在乌尔都语句法能力的UrBLiMP测试集，发现现有模型在低资源语言上表现存在显著差异，展示了其潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLM）在多种语言上表现优异，但对乌尔都语等低资源语言的数据远少于英语等高资源语言。本研究旨在评估LLM在乌尔都语中的语言知识。

Method: 构建了乌尔都语最小对立句法基准测试集（UrBLiMP），收集了5696对仅在语法可接受性上有细微差异的句子，涵盖十种核心句法现象，来源包括乌尔都语树库和多样的语料库。通过人工标注和一致性检验（96.10%一致率）保证数据集可靠性，并对20个多语LLM进行了测试与分析。

Result: LLaMA-3-70B在UrBLiMP上的平均准确率最高（94.73%），但与Gemma-3-27B-PT等顶尖模型在统计上无显著差异。各模型在细粒度句法知识掌握上差异较大。

Conclusion: 多语LLM在捕捉低资源语言（如乌尔都语）精细句法知识方面表现不一，仍有改进空间。UrBLiMP为评估和提升此类模型提供了重要工具。

Abstract: Multilingual Large Language Models (LLMs) have shown remarkable performance
across various languages; however, they often include significantly less data
for low-resource languages such as Urdu compared to high-resource languages
like English. To assess the linguistic knowledge of LLMs in Urdu, we present
the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of
minimally different sentences that contrast in grammatical acceptability.
UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,
carefully curated using the Urdu Treebank and diverse Urdu text corpora. A
human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator
agreement, confirming the reliability of the dataset. We evaluate twenty
multilingual LLMs on UrBLiMP, revealing significant variation in performance
across linguistic phenomena. While LLaMA-3-70B achieves the highest average
accuracy (94.73%), its performance is statistically comparable to other top
models such as Gemma-3-27B-PT. These findings highlight both the potential and
the limitations of current multilingual LLMs in capturing fine-grained
syntactic knowledge in low-resource languages.

</details>


### [41] [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)
*Michael Farag,Patrick Halina,Andrey Zaytsev,Alekhya Munagala,Imtihan Ahmed,Junhao Wang*

Main category: cs.CL

TL;DR: Pinterest开发了一个高效、低成本、依赖多模态网页特征表示的属性抽取系统，其性能和经济性均大幅优于大型语言模型方案。


<details>
  <summary>Details</summary>
Motivation: Pinterest需要将电商网页的非结构化信息转化为结构化属性数据，以提升用户体验和内容分发效率，但这一直是技术上的挑战。

Method: 创新性地将网页结构、文本、样式和布局等多模态信息融合为紧凑的特征表示，适用于小模型如XGBoost，并与LLM（如GPT）进行对比实验。

Result: 该系统能每秒处理1000+个URL，准确率优于较大的LLM模型，且成本比GPT类方案低1000倍，实现了高准确率和高性价比。

Conclusion: 提出了一种结合结构、视觉和文本多模态的网页表示方法，使结构化商品属性抽取更加高效且准确。该系统在大规模页面中展现出高准确率和经济性。

Abstract: The internet offers a massive repository of unstructured information, but
it's a significant challenge to convert this into a structured format. At
Pinterest, the ability to accurately extract structured product data from
e-commerce websites is essential to enhance user experiences and improve
content distribution. In this paper, we present Pinterest's system for
attribute extraction, which achieves remarkable accuracy and scalability at a
manageable cost. Our approach leverages a novel webpage representation that
combines structural, visual, and text modalities into a compact form,
optimizing it for small model learning. This representation captures each
visible HTML node with its text, style and layout information. We show how this
allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract
attributes more accurately than much more complex Large Language Models (LLMs)
such as Generative Pre-trained Transformer (GPT). Our results demonstrate a
system that is highly scalable, processing over 1,000 URLs per second, while
being 1000 times more cost-effective than the cheapest GPT alternatives.

</details>


### [42] [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)
*Liam G. McCoy,Fateme Nateghi Haredasht,Kanav Chopra,David Wu,David JH Wu,Abass Conteh,Sarita Khemani,Saloni Kumar Maharaj,Vishnu Ravi,Arth Pahwa,Yingjie Weng,Leah Rosengaus,Lena Giang,Kelvin Zhenghao Li,Olivia Jee,Daniel Shirvani,Ethan Goh,Jonathan H. Chen*

Main category: cs.CL

TL;DR: LLMs在结构化、全面的临床模板生成方面表现良好，但难以在限定长度和优先级排序方面满足所有临床需求，尤其在叙事性科室表现不佳。


<details>
  <summary>Details</summary>
Motivation: 电子化医疗咨询需要结构化且重点突出的临床问询模板，目前缺乏有效工具自动生成此类模板；评估LLMs能否高效、准确地生成符合临床实际需求的结构化内容，并考察其适应多专业领域的能力。

Method: 采用145份由斯坦福eConsult团队手工设计并常用的模板，对多种前沿LLM（如o3、GPT-4o、Kimi K2、Claude 4 Sonnet、Llama 3 70B和Gemini 2.5 Pro）在生成结构化、重点突出的临床问询模板能力进行评估，通过多智能体流程结合提示优化、语义自动评分和优先级分析评估结果。

Result: o3等模型可实现高达92.2%的全面性，但生成模板过长，且在限定长度时难以正确优先排序关键临床问题。不同专业表现不一，尤其在精神科等叙事性领域效果下降显著。

Conclusion: LLMs能够生成结构化的临床问询模板，在提升医生间信息交流方面具有潜力，但在优先排序关键临床问题方面表现不足，特别是在内容长度受限的实际应用场景中。不同专业之间表现有较大差异，叙事性强的科室（如精神病学和疼痛医学）退化明显。

Abstract: This study evaluates the capacity of large language models (LLMs) to generate
structured clinical consultation templates for electronic consultation. Using
145 expert-crafted templates developed and routinely used by Stanford's
eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,
Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to
produce clinically coherent, concise, and prioritized clinical question
schemas. Through a multi-agent pipeline combining prompt optimization, semantic
autograding, and prioritization analysis, we show that while models like o3
achieve high comprehensiveness (up to 92.2\%), they consistently generate
excessively long templates and fail to correctly prioritize the most clinically
important questions under length constraints. Performance varies across
specialties, with significant degradation in narrative-driven fields such as
psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance
structured clinical information exchange between physicians, while highlighting
the need for more robust evaluation methods that capture a model's ability to
prioritize clinically salient information within the time constraints of
real-world physician communication.

</details>


### [43] [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)
*Jiyu Chen,Necva Bölücü,Sarvnaz Karimi,Diego Mollá,Cécile L. Paris*

Main category: cs.CL

TL;DR: 本论文通过Semeval 2025 Task 11任务，探索了多语言情感识别的多种方法，结果表明分别对每种语言以LoRA进行微调的大模型效果最优。


<details>
  <summary>Details</summary>
Motivation: 不同语言中情感表达存在多样性和文化差异，导致跨语言情感检测具有挑战性。Semeval 2025 Task 11旨在探讨如何在文本基础上进行跨语言情感识别。

Method: 对比评估多种大语言模型（LLM）在情感识别任务中的适应策略，并重点研究了分别对每种语言以LoRA设置微调预训练多语言LLM的方法。

Result: 结果显示，将多语言LLM以LoRA方式分别针对每种语言进行微调是最有效的做法。

Conclusion: 为实现跨语言文本情感识别，分别对每种语言进行LoRA微调的多语言大模型能取得最佳效果。

Abstract: Detecting emotions across different languages is challenging due to the
varied and culturally nuanced ways of emotional expressions. The
\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared
task was organised to investigate emotion recognition across different
languages. The goal of the task is to implement an emotion recogniser that can
identify the basic emotional states that general third-party observers would
attribute to an author based on their written text snippet, along with the
intensity of those emotions. We report our investigation of various
task-adaptation strategies for LLMs in emotion recognition. We show that the
most effective method for this task is to fine-tune a pre-trained multilingual
LLM with LoRA setting separately for each language.

</details>


### [44] [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)
*Yige Li,Peihai Jiang,Jun Sun,Peng Shu,Tianming Liu,Zhen Xiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为后缀优化（SOP）的内容限制新方法，只需在输入后加个短后缀即可显著提升大语言模型的内容合规能力，无需微调，效果优于系统级方案，并在真实应用场景下表现出良好实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在不同应用中表现优异，但内容限制（如防止生成有害内容）很难实现，且现有方法如SFT对每个场景都微调模型的代价极高。不同用户和场景对于“有害内容”的定义和需求也经常变化，所以需要一种既高效又灵活的内容限制手段。

Method: 提出了一种名为自适应内容限制（AdaCoRe）的新任务，专注于使用无需模型微调的轻量级方法进行内容限制。具体方法为“后缀优化（SOP）”，即在输入指令后添加一个短的、优化过的后缀，既阻止目标模型生成特定禁用词，也尽量保证输出质量。

Result: 在新提出的内容限制基准测试（CoReBench）上，通过与传统系统级别的方法对比，SOP方法在Gemma2-2B、Mistral-7B、Vicuna-7B、Llama3-8B及Llama3.1-8B等主流模型上的限制率分别提升15%、17%、10%、9%和6%。同时，在POE等实际在线平台测试也证明了其实用性和有效性。

Conclusion: SOP作为一种轻量的内容限制方法，无需模型微调即可有效实现有害内容防控，并具备良好的适应性和扩展性，适合灵活应对多样场景下的内容合规需求。

Abstract: Large Language Models (LLMs) have demonstrated significant success across
diverse applications. However, enforcing content restrictions remains a
significant challenge due to their expansive output space. One aspect of
content restriction is preventing LLMs from generating harmful content via
model alignment approaches such as supervised fine-tuning (SFT). Yet, the need
for content restriction may vary significantly across user groups, change
rapidly over time, and not always align with general definitions of
harmfulness. Applying SFT to each of these specific use cases is impractical
due to the high computational, data, and storage demands. Motivated by this
need, we propose a new task called \textit{Adaptive Content Restriction}
(AdaCoRe), which focuses on lightweight strategies -- methods without model
fine-tuning -- to prevent deployed LLMs from generating restricted terms for
specific use cases. We propose the first method for AdaCoRe, named
\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to
any prompt to a) prevent a target LLM from generating a set of restricted
terms, while b) preserving the output quality. To evaluate AdaCoRe approaches,
including our SOP, we create a new \textit{Content Restriction Benchmark}
(CoReBench), which contains 400 prompts for 80 restricted terms across 8
carefully selected categories. We demonstrate the effectiveness of SOP on
CoReBench, which outperforms the system-level baselines such as system suffix
by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B,
Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also
demonstrate that SOP is effective on POE, an online platform hosting various
commercial LLMs, highlighting its practicality in real-world scenarios.

</details>


### [45] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
*Shengqi Zhu,Jeffrey M. Rzeszotarski,David Mimno*

Main category: cs.CL

TL;DR: 论文提出对LLM用户查询进行细粒度分段分析的新任务，揭示了用户行为会因经验和模型能力变化而变化，丰富了用户与LLM互动的理解。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）用户的聊天日志蕴含丰富信息，但由于用户查询表达差异大，行为模式往往被掩盖。为深入理解用户与LLM的互动，需要对聊天日志进行细粒度分析。

Method: 提出了一项新任务——将聊天查询细分为请求内容、角色、查询特定上下文和附加表达。收集并标注相关数据以分析用户表达和行为，并结合历时分析和新模型引入后的社区行为变化。

Result: 发现LLM查询中的请求表达方式与人与人之间的互动显著不同。用户在使用早期多以请求为主，随着经验积累，表达方式趋于收敛。新模型能力的引入会在社区层面引发用户行为的可追踪变化。

Conclusion: 该研究不仅提出了有效的聊天日志分段分析方法，还揭示了用户行为在使用LLM过程中的变化规律。新模型的推出会显著影响用户提问和表达模式。

Abstract: Chat logs provide a rich source of information about LLM users, but patterns
of user behavior are often masked by the variability of queries. We present a
new task, segmenting chat queries into contents of requests, roles,
query-specific context, and additional expressions. We find that, despite the
familiarity of chat-based interaction, request-making in LLM queries remains
significantly different from comparable human-human interactions. With the data
resource, we introduce an important perspective of diachronic analyses with
user expressions. We find that query patterns vary between early ones
emphasizing requests, and individual users explore patterns but tend to
converge with experience. Finally, we show that model capabilities affect user
behavior, particularly with the introduction of new models, which are traceable
at the community level.

</details>


### [46] [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)
*Ethan Hsu,Hong Meng Yam,Ines Bouissou,Aaron Murali John,Raj Thota,Josh Koe,Vivek Sarath Putta,G K Dharesan,Alexander Spangher,Shikhar Murty,Tenghao Huang,Christopher D. Manning*

Main category: cs.CL

TL;DR: WebDS基准真实模拟了复杂的数据科学网络任务，考核现有LLM代理能力。实验显示当前LLM在此类任务中表现较差，表明亟需提升其面向现实数据科学场景的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的网络基准测试只关注简单的交互任务（如表单提交、电商操作），且传统数据科学基准只关注静态数据，无法完整考察数据科学全流程中的多步操作与多工具协作。实际数据科学任务则往往复杂且依赖多跳的网络操作与不同数据格式。

Method: 提出WebDS，这是一个端到端、基于网络的数据科学基准，涵盖870个来自29个不同网站的复杂任务，任务涉及结构化与非结构化数据、多步骤流程、工具使用能力等。

Result: 当前先进的LLM代理在WebDS任务上的表现远低于以往基准。例如，Browser Use在Web Voyager基准可完成80%的任务，但在WebDS仅完成15%，分析表明原因包括信息定位差、行为重复、捷径取巧等新失效模式。

Conclusion: WebDS基准真实、复杂且多样，暴露了当前LLM工具在数据科学现实任务中的局限，有助于推动更实用的LLM数据科学代理的发展。

Abstract: A large portion of real-world data science tasks are complex and require
multi-hop web-based interactions: finding appropriate data available on the
internet, synthesizing real-time data of various modalities from different
locations, and producing summarized analyses. Existing web benchmarks often
focus on simplistic interactions, such as form submissions or e-commerce
transactions, and often do not require diverse tool-using capabilities required
for web based data science. Conversely, traditional data science benchmarks
typically concentrate on static, often textually bound datasets and do not
assess end-to-end workflows that encompass data acquisition, cleaning,
analysis, and insight generation. In response, we introduce WebDS, the first
end-to-end web-based data science benchmark. It comprises 870 web-based data
science tasks across 29 diverse websites from structured government data
portals to unstructured news media, challenging agents to perform complex,
multi-step operations requiring the use of tools and heterogeneous data formats
that better reflect the realities of modern data analytics. Evaluations of
current SOTA LLM agents indicate significant performance gaps in accomplishing
these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web
Voyager, successfully completes only 15% of tasks in WebDS, which our analysis
suggests is due to new failure modes like poor information grounding,
repetitive behavior and shortcut-taking that agents performing WebDS' tasks
display. By providing a more robust and realistic testing ground, WebDS sets
the stage for significant advances in the development of practically useful
LLM-based data science.

</details>


### [47] [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)
*Yue Chen,Minghua He,Fangkai Yang,Pu Zhao,Lu Wang,Yu Kang,Yifei Dong,Yuefeng Zhan,Hao Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: WarriorMath采用多专家协作生成和挑选难题，并用渐进式训练克服模型弱点，显著提升大模型解决数学问题的能力，效果大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在数学问题求解方面虽有强大能力，但受限于高质量、多样性训练数据的缺乏。现有的数据增强方法如题目改写或难度递进，未能针对模型的具体失误，导致生成的问题基本已被模型掌握，实际提升有限。

Method: 作者提出了WarriorMath缺陷感知框架。该框架分为两阶段：一是在数据合成阶段，多个专家LLM协作生成、批判并优化数学问题，筛选模型失败的问题并迭代改进，最终生成高质量、针对性强的数据；二是在训练阶段，引入渐进式训练策略，逐步以更具挑战性的、针对模型弱点的数据进行微调。

Result: 实验覆盖六大数学基准，WarriorMath平均超越强基线12.57%，达到最新SOTA水平。

Conclusion: WarriorMath基于缺陷感知、多专家协作的数据生成与渐进训练，有效提升了LLM数学能力，展现该方法对模型性能提升的显著作用。

Abstract: Large Language Models (LLMs) excel in solving mathematical problems, yet
their performance is often limited by the availability of high-quality, diverse
training data. Existing methods focus on augmenting datasets through rephrasing
or difficulty progression but overlook the specific failure modes of LLMs. This
results in synthetic questions that the model can already solve, providing
minimal performance gains. To address this, we propose WarriorMath, a
defect-aware framework for mathematical problem solving that integrates both
targeted data synthesis and progressive training. In the synthesis stage, we
employ multiple expert LLMs in a collaborative process to generate, critique,
and refine problems. Questions that base LLMs fail to solve are identified and
iteratively improved through expert-level feedback, producing high-quality,
defect-aware training data. In the training stage, we introduce a progressive
learning framework that iteratively fine-tunes the model using increasingly
challenging data tailored to its weaknesses. Experiments on six mathematical
benchmarks show that WarriorMath outperforms strong baselines by 12.57% on
average, setting a new state-of-the-art. Our results demonstrate the
effectiveness of a defect-aware, multi-expert framework for improving
mathematical ability.

</details>


### [48] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文分析了2025年XAI Challenge教育问答竞赛，强调大模型结合符号推理提升了系统可解释性，对今后相关可解释AI教育应用和竞赛有借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 人工智能在教育中的应用日益广泛，急需提高系统的透明度和可解释性。目前虽然有许多关于AI的黑客松竞赛，但很少专注于可解释AI（XAI）在真实教育情境下的应用，因此本研究顺应了XAI在教育领域的迫切需求。

Method: 文章分析了2025年在IJCNN会议期间举办的XAI Challenge竞赛。该竞赛要求参赛队伍开发能回答学生关于校规问题并生成逻辑化自然语言解释的问答系统，采用轻量级大模型或混合符号方法。数据集通过逻辑模板建构、Z3验证及学生专家审核，以确保与实际学术情景吻合。同时，论文描述了竞赛的动机、结构、数据集设计及评测协议。

Result: 本项挑战赛推动了大语言模型与符号推理相结合，实现教育问答系统的可解释性，提升了透明度和信任度。竞赛经验为未来可解释AI在教育系统的设计和相关竞赛提供了可借鉴的方法和见解。

Conclusion: 研究认为XAI Challenge 2025竞赛是连接大语言模型与符号推理以实现教育应用可解释性的创新尝试，为今后的XAI教育系统和研究与应用型竞赛提供了切实的启示和建议。

Abstract: The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


### [49] [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)
*Zhichao Yan,Jiapu Wang,Jiaoyan Chen,Yanyan Wang,Hongye Tan,Jiye Liang,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 提出利用部分相关知识“唤醒”大模型已有知识，实验验证优于传统方法，并提出了模拟真实世界挑战的新数据集和任务，对RAG系统有实际价值。


<details>
  <summary>Details</summary>
Motivation: 尽管基于检索增强的生成（RAG）方法能显著提升大语言模型（LLM）的知识能力，但在知识库不完全或检索不足时，部分相关知识的有效利用仍是一个巨大挑战。同时，传统观点认为需要直接、完整答案证据才能更好唤起LLM相关知识。

Method: 作者将黄金推理路径上的三元组及其变体用于构建部分相关知识，故意移除含有正确答案路径，从而模拟部分相关检索场景。通过理论分析及在两个知识图谱问答数据集上的实验，评估了LLM在摄取部分相关知识时被唤醒的效应。此外，提出了“未见实体知识图谱问答”新任务，模拟实际中实体无法链接的情况。

Result: 实验结果表明，该基于唤醒的策略在实际应用中更加有效，优于传统基于嵌入相似性的检索方法，后者更容易返回噪声信息。唤醒法能更好激发LLM已有的相关知识。

Conclusion: 部分相关知识可以有效唤醒LLM中的内隐知识，对不完整知识库或实体链接失败的实际场景有现实意义。论文提出的新任务和方法对RAG系统具有推动作用。

Abstract: Retrieval-Augmented Generation (RAG) shows impressive performance by
supplementing and substituting parametric knowledge in Large Language Models
(LLMs). Retrieved knowledge can be divided into three types: explicit answer
evidence, implicit answer clue, and insufficient answer context which can be
further categorized into totally irrelevant and partially relevant information.
Effectively utilizing partially relevant knowledge remains a key challenge for
RAG systems, especially in incomplete knowledge base retrieval. Contrary to the
conventional view, we propose a new perspective: LLMs can be awakened via
partially relevant knowledge already embedded in LLMs. To comprehensively
investigate this phenomenon, the triplets located in the gold reasoning path
and their variants are used to construct partially relevant knowledge by
removing the path that contains the answer. We provide theoretical analysis of
the awakening effect in LLMs and support our hypothesis with experiments on two
Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we
present a new task, Unseen Entity KGQA, simulating real-world challenges where
entity linking fails due to KG incompleteness. Our awakening-based approach
demonstrates greater efficacy in practical applications, outperforms
traditional methods that rely on embedding-based similarity which are prone to
returning noisy information.

</details>


### [50] [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)
*Chenming Tang,Yutong Yang,Yunfang Wu*

Main category: cs.CL

TL;DR: KEDAS方法通过多样化增强和自适应推理机制，极大提升了大模型的知识编辑能力，对比现有方法综合表现最优，有望成为知识编辑对齐的新标准。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型知识编辑方法多为参数层面编辑或基于检索，难以高效且精准地编辑知识，同时要保证模型原有能力不受影响。因而，需要一种更好的知识对齐方法提升编辑效果。

Method: 提出KEDAS方法，包括三个关键环节：1）在对齐阶段，通过低秩适配让LLM习得上下文中的已编辑知识；2）编辑过程中设计多样化编辑增强技术以提升编辑召回率；3）推理阶段引入自适应推理机制，利用基于筛选的智能检索动态决定推理路径，使无关查询走原始模型，相关查询走已对齐适配器。

Result: KEDAS在4个数据集上、3种大模型、3种设置下，35/36项任务全面优胜，综合打分相比现有对齐方法提升约19.8点，显著优于参数编辑及检索类基线。在计算成本和通用任务表现方面，KEDAS也展示了强健性与高效性。

Conclusion: KEDAS是知识编辑对齐理想范式，能高效且有效地编辑大模型知识，同时保持模型通用能力和计算效率。

Abstract: Knowledge editing aims to modify outdated knowledge in large language models
(LLMs) efficiently while retaining their powerful capabilities. Most existing
methods rely on either parameter-level editing or retrieval-based approaches.
In this work, we propose Knowledge Editing alignment with Diverse Augmentation
and Self-adaptive inference (KEDAS) to better align LLMs with knowledge
editing. In the alignment phase, LLMs learn to apply in-context edited
knowledge via low-rank adaptation. During editing, we design a diverse edit
augmentation technique to improve the recall of edits. After that, a
self-adaptive post-alignment inference mechanism is proposed, in which a
filter-based smart retriever is employed to perform a dynamic selection of
inference routing. Specifically, irrelevant queries will go through the
original pre-alignment model directly, while relevant ones, together with their
related edits, go through the model with aligned adapters activated. In
experiments, KEDAS secures the highest overall performance scores in 35 out of
36 cases across four datasets with three LLMs on three settings, surpassing its
strong knowledge editing alignment counterpart by about 19.8 harmonic mean
scores of edit success, locality and portability and outperforming both
parameter editing and retrieval-based baselines significantly. Analysis of
computational cost and performance on general tasks further validates the
robustness and efficiency of KEDAS, indicating that it presents an ideal
paradigm of knowledge editing alignment.

</details>


### [51] [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)
*Weibo Zhou,Lingbo Li,Shangsong Liang*

Main category: cs.CL

TL;DR: D-SCoRE是一种无需训练、可高效自动生成高质量多样化问答数据的流程，在多个领域超越了传统人工标注数据集，对领域大模型微调极具价值。


<details>
  <summary>Details</summary>
Motivation: 高质量问答（QA）数据集稀缺且昂贵，限制了领域特定大语言模型（LLM）的有监督微调（SFT）。

Method: 提出D-SCoRE，一种无训练流程，通过LLM和提示工程，从任意文本源自动生成多样化和高质量的问答数据集。该方法结合文档中心处理、分段、链式推理（CoT）和结构化导出，并引入语义角色变换、问题类型平衡与反事实材料等机制以提升生成问答的多样性与相关性。

Result: 在SQuADShifts和Covid-QA测试集上，使用D-SCoRE生成的数据集微调后的LLM在绝大部分领域表现优于使用人工标注SQuAD和Covid-QA数据集的数据集微调后的模型。此外，D-SCoRE可在消费级硬件上高效生成优质问答对。

Conclusion: D-SCoRE流程具有简单、高效、可扩展的特点，可大幅提升跨领域高质量问答数据生成与微调效果。

Abstract: The scarcity and high cost of high-quality question-answering (QA) datasets
hinder supervised fine-tuning (SFT) for domain-specific large language models
(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that
utilizes LLMs and prompt engineering to produce diverse, high-quality QA
datasets from arbitrary textual sources. D-SCoRE integrates
$\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T
$\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT
datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,
such as semantic role transformation, question type balancing, and
counterfactual materials, enhance diversity and relevance, overcoming
limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA
datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on
SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most
domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual
materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade
hardware. Its simplicity and scalability enable efficient QA generation and
high-performance fine-tuning across domains.

</details>


### [52] [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: LinkSyn是一种通过知识点图灵活生成高质多样学科QA数据的新方法，能显著提升大模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的发展受限于高质量、多样化训练数据的不足，尤其是难以灵活按需控制学科覆盖、知识点分布及题目难度，亟需新的自动化数据合成方式来拓展和优化训练数据。

Method: 本文提出了LinkSyn框架，先从QA种子数据中抽取知识点（KP）并构建KP图，再通过图游走和知识分布函数动态平衡知识点的覆盖与流行度，并在生成中引入扩散式生成方法及难度灵活调控，最终生成大规模多学科QA数据集LinkQA。应用Llama-3 8B模型在LinkQA上持续预训练并验证效果。

Result: LinkSyn生成的LinkQA数据集（50B词）经过在Llama-3 8B上的实验表明，MMLU和CMMLU评测的平均提升幅度达到11.51%，且不同模型规模和算力配置均表现出显著性能提升。

Conclusion: LinkSyn能够有效生成高质量、多样化的QA数据集，不仅提升了数据的学科和难度灵活性，还大幅提升了下游模型在权威评测集（如MMLU和CMMLU）上的表现，达到了新的SOTA。

Abstract: The advancement of large language models (LLMs) struggles with the scarcity
of high-quality, diverse training data. To address this limitation, we propose
LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that
enables flexible control over discipline and difficulty distributions while
balancing KP coverage and popularity. LinkSyn extracts KPs from
question-answering (QA) seed data and constructs a KP graph to synthesize
diverse QA data from multiple seeds strongly linked by KPs and sampled from
graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution
value function to guide the adjustment of path sampling probability and balance
KP coverage and popularity during graph walks; (2) diffusion-based synthesis
via DeepSeek-R1 by leveraging multiple seeds with dense logical associations
along each path; and (3) high-difficulty QA enhancement within given
disciplines by flexible difficulty adjustments. By executing LinkSyn, we
synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.
Extensive experiments on Llama-3 8B demonstrate that continual pre-training
with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and
CMMLU, establishing new SOTA results. LinkQA consistently enhances performance
across model size and initial FLOPs scales.

</details>


### [53] [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)
*Xuemiao Zhang,Chengying Tu,Can Ren,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 本研究提出BoostQA，通过多阶段、多源数据合成，极大提升大语言模型在知识问答，尤其是STEM领域及高难度数据上的泛化和表现，性能随模型、数据和算力增长而持续提升。


<details>
  <summary>Details</summary>
Motivation: 高质量、知识密集型训练数据的稀缺限制了大语言模型（LLMs）的发展，传统语料库信息有限。过去通过合成和整合基于语料的数据提升模型，但在多领域尤其是跨领域的数据多样性和难度可扩展性上存在挑战。该研究希望解决上述难题，推动模型在STEM领域和高难度任务上的表现。

Method: 作者提出了一套多元化的数据合成流程来合成BoostQA，一个拥有1000亿token的大规模QA数据集。该流程包括：从异质来源筛选种子数据；利用DeepSeek-R1进行以STEM为主、多等级、提升多样性的合成，以及高难度数据的合成；使用DeepSeek-V3优化答案质量。BoostQA用于训练过程中的中期，以优化领域知识获取和数据质量。

Result: 使用BoostQA进行中期训练后，Llama-3 8B在MMLU和CMMLU任务上平均提升12.74%，并在12项基准测试取得SOTA表现。此外，BoostQA具有良好的可扩展性，随着模型规模、数据量和算力提升，模型表现持续提升。

Conclusion: BoostQA和该数据合成流程大幅提升了大模型在知识密集问答任务中的表现，尤其在STEM领域和高难度场景下，并验证了通过大规模、多样化高质量数据提升模型性能的有效性。

Abstract: The scarcity of high-quality, knowledge-intensive training data hinders the
development of large language models (LLMs), as traditional corpora provide
limited information. Previous studies have synthesized and integrated
corpora-dependent question-answering (QA) data to improve model performance but
face challenges in QA data scalability and knowledge diversity, particularly in
cross-domain contexts. Furthermore, leveraging our designed discipline and
difficulty annotation system, we probe model deficiencies in STEM disciplines
and high-difficulty data. To overcome these limitations, we propose a novel
diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA
dataset. Our synthesis framework: (1) curates seed data from heterogeneous
sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade
synthesis to boost data diversity and high-difficulty synthesis to mitigate
difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output
quality. We utilize BoostQA in mid-training, a mid-stage between pre-training
and post-training, to optimize domain-specific knowledge acquisition and
enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token
dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and
CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also
demonstrates robust scalability, with performance consistently improving as
model size, data volume, and initial FLOPs scale.

</details>


### [54] [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)
*Roman Koshkin,Pengyu Dai,Nozomi Fujikawa,Masahito Togami,Marco Visentini-Scarzanella*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型，通过多智能体协作与自监督优化的自动化市场报告生成系统，在成本与效率上大幅优于传统人工流程，并实现可复制的高质量报告生成。


<details>
  <summary>Details</summary>
Motivation: 市场分析报告的生成通常依靠昂贵且耗时的人力资源，难以快速响应商业需求。人工智能（尤其是大语言模型）的自动化能力有望提升效率和降低成本。

Method: 提出了一个以大语言模型（LLM）为核心的自动化商业分析与市场报告生成框架。该系统包含Researcher、Reviewer、Writer、Retriever四类智能体，协同完成数据查询、分析、可视化及报告撰写。通过in-context learning，系统学习亚马逊专业顾问的分析方法。还引入了基于LLM的自动化报告质量评估与自迭代优化机制。

Result: 实验结果表明，自动化审核流程与顾问的非结构化知识均能提升报告质量。在实验验证中，该系统可在7分钟内以约1美元成本生成6页的详细市场分析报告。

Conclusion: 本框架能够高效、低成本地自动生成高质量的市场分析报告，并实现自动化报告质量评估与优化，推动基于LLM的自动商业分析应用落地。

Abstract: We present an autonomous framework that leverages Large Language Models
(LLMs) to automate end-to-end business analysis and market report generation.
At its core, the system employs specialized agents - Researcher, Reviewer,
Writer, and Retriever - that collaborate to analyze data and produce
comprehensive reports. These agents learn from real professional consultants'
presentation materials at Amazon through in-context learning to replicate
professional analytical methodologies. The framework executes a multi-step
process: querying databases, analyzing data, generating insights, creating
visualizations, and composing market reports. We also introduce a novel
LLM-based evaluation system for assessing report quality, which shows alignment
with expert human evaluations. Building on these evaluations, we implement an
iterative improvement mechanism that optimizes report quality through automated
review cycles. Experimental results show that report quality can be improved by
both automated review cycles and consultants' unstructured knowledge. In
experimental validation, our framework generates detailed 6-page reports in 7
minutes at a cost of approximately \$1. Our work could be an important step to
automatically create affordable market insights.

</details>


### [55] [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)
*Ahmad Rezaie Mianroodi,Amirali Rezaie,Niko Grisel Todorov,Cyril Rakovski,Frank Rudzicz*

Main category: cs.CL

TL;DR: 论文提出了面向医疗对话和病历记录双向生成任务的新合成数据集MedSynth，覆盖全面、可自由使用，并在提升相关模型性能方面有实际效果，具有重要应用和研究价值。


<details>
  <summary>Details</summary>
Motivation: 医生在临床记录上花费大量时间，导致职业倦怠。因医疗对话与病历公有大数据稀缺，急需开放、合规且多样化的高质量数据集推动自动化工具研发。

Method: 作者通过分析疾病分布，合成了覆盖2000多个ICD-10编码、超过1万条对话-病历配对的数据集，并开展模型实证，评估其在对话生成和病历生成方面的表现提升。

Result: MedSynth数据集显著提升了多种生成模型在医疗对话与病历文档互转任务上的表现，并为隐私合规、安全开放的数据资源提供了新选择。

Conclusion: MedSynth数据集极大推动了医患对话到病历记录（Dial-2-Note）及反向任务（Note-2-Dial）的研究和应用，提升了自动化医学文档生成模型的性能。

Abstract: Physicians spend significant time documenting clinical encounters, a burden
that contributes to professional burnout. To address this, robust automation
tools for medical documentation are crucial. We introduce MedSynth -- a novel
dataset of synthetic medical dialogues and notes designed to advance the
Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.
Informed by an extensive analysis of disease distributions, this dataset
includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We
demonstrate that our dataset markedly enhances the performance of models in
generating medical notes from dialogues, and dialogues from medical notes. The
dataset provides a valuable resource in a field where open-access,
privacy-compliant, and diverse training data are scarce. Code is available at
https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available
at https://huggingface.co/datasets/Ahmad0067/MedSynth.

</details>


### [56] [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: 该论文发布了一个涵盖歌词、小说、字幕等多体裁的埃及阿拉伯语-英语人工翻译语料库，内容丰富、对齐精确，可用于机器翻译与多种学术、实用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的阿拉伯语-英语平行语料多集中于新闻等有限体裁，缺乏涵盖更多日常生活和文化情境的高质量平行资源，特别是人工翻译且覆盖丰富体裁的数据库。本研究旨在填补这一空白。

Method: 通过人工翻译与人工对齐，收集并整理了不同体裁的埃及阿拉伯语文本与其英文版本，最终构建成包含25,557段落对的平行语料库。

Result: 构建了一个多体裁、多功能的埃及阿拉伯语-英语平行数据集，可用于机器翻译模型的评测和微调、翻译与语义研究、翻译教学及专业翻译辅助。

Conclusion: 该论文提出了ArzEn-MultiGenre数据集，这是一个高质量的阿拉伯语（埃及方言）与英语的双语对齐并经过人工翻译的语料库，涵盖歌曲歌词、小说和电视剧字幕等多种文本体裁，弥补了现有平行语料的不足。

Abstract: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,
novels, and TV show subtitles that are manually translated and aligned with
their English counterparts. The dataset contains 25,557 segment pairs that can
be used to benchmark new machine translation models, fine-tune large language
models in few-shot settings, and adapt commercial machine translation
applications such as Google Translate. Additionally, the dataset is a valuable
resource for research in various disciplines, including translation studies,
cross-linguistic analysis, and lexical semantics. The dataset can also serve
pedagogical purposes by training translation students and aid professional
translators as a translation memory. The contributions are twofold: first, the
dataset features textual genres not found in existing parallel Egyptian Arabic
and English datasets, and second, it is a gold-standard dataset that has been
translated and aligned by human experts.

</details>


### [57] [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)
*Jinhao Pan,Chahat Raj,Ziwei Zhu*

Main category: cs.CL

TL;DR: 提出BADF框架，不依赖预设标签，系统挖掘LLM潜在偏见，能发现未预见的身份相关刻板印象，为LLM偏见识别和理解提供了强有力的新思路。


<details>
  <summary>Details</summary>
Motivation: 当前LLM存在社会偏见，且许多评测方法依赖于预设的身份-概念关联，难以发现新或未预见到的偏见。因此需要能系统地挖掘和映射LLM中表现出的全部偏见联想的新方法。

Method: 提出了Bias Association Discovery Framework（BADF），通过系统性地从开放式LLM输出中提取身份与描述性概念的关联，无需预设具体关联，采用多模型、多现实语境的大规模实验证明该方法有效。

Result: BADF能发现既有又新出现的身份-概念关联，在多种主流LLM及真实应用环境下表现出良好的泛化性和实用性。相关数据、代码和结果已公开。

Conclusion: 本文提出的BADF方法能较好地发现和分析LLM中与人口统计学身份相关的偏见联想，包括先前未被认识到的偏见类型。其结果推动了对开放式文本生成中偏见现象的理解，并为评估和减少LLM偏见提供了新工具。

Abstract: Social biases embedded in Large Language Models (LLMs) raise critical
concerns, resulting in representational harms -- unfair or distorted portrayals
of demographic groups -- that may be expressed in subtle ways through generated
language. Existing evaluation methods often depend on predefined
identity-concept associations, limiting their ability to surface new or
unexpected forms of bias. In this work, we present the Bias Association
Discovery Framework (BADF), a systematic approach for extracting both known and
previously unrecognized associations between demographic identities and
descriptive concepts from open-ended LLM outputs. Through comprehensive
experiments spanning multiple models and diverse real-world contexts, BADF
enables robust mapping and analysis of the varied concepts that characterize
demographic identities. Our findings advance the understanding of biases in
open-ended generation and provide a scalable tool for identifying and analyzing
bias associations in LLMs. Data, code, and results are available at
https://github.com/JP-25/Discover-Open-Ended-Generation

</details>


### [58] [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
*Haonan Bian,Yutao Qi,Rui Yang,Yuanxi Che,Jiaqian Wang,Heming Xia,Ranran Zhen*

Main category: cs.CL

TL;DR: 该论文提出了ORACLE框架，结合知识图谱与大型语言模型，实现了更强的复杂多跳问答能力，推理链更具逻辑性和可解释性，在多项任务上达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多跳复杂问答任务（MQA）中由于无法充分捕捉实体之间的深层概念关系，导致其在需要非线性、结构化推理的场景下表现有限。

Method: 提出了一个名为ORACLE的无训练框架，该框架结合了LLMs的生成能力与知识图谱的结构优势。其方法包括三个阶段：1）利用LLMs动态构建针对问题的知识本体；2）将本体转化为一阶逻辑推理链；3）系统性地将原始问题分解为逻辑一致的子问题。

Result: 在多个标准多跳问答基准测试上，ORACLE框架取得了与最新SOTA模型（如DeepSeek-R1）相媲美的表现。分析表明，该方法各组成部分表现有效，且比现有方法生成更具逻辑性和可解释性的推理链。

Conclusion: ORACLE框架有效弥补了LLMs在结构化推理中的短板，通过知识本体与逻辑链的结合，使得复杂问答任务中的推理过程更具条理和可解释性，具有良好的实际和理论意义。

Abstract: Large Language Models (LLMs), despite their success in question answering,
exhibit limitations in complex multi-hop question answering (MQA) tasks that
necessitate non-linear, structured reasoning. This limitation stems from their
inability to adequately capture deep conceptual relationships between entities.
To overcome this challenge, we present **ORACLE** (**O**ntology-driven
**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a
training-free framework that combines LLMs' generative capabilities with the
structural benefits of knowledge graphs. Our approach operates through three
stages: (1) dynamic construction of question-specific knowledge ontologies
using LLMs, (2) transformation of these ontologies into First-Order Logic
reasoning chains, and (3) systematic decomposition of the original query into
logically coherent sub-questions. Experimental results on several standard MQA
benchmarks show that our framework achieves highly competitive performance,
rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses
further confirm the effectiveness of each component, while demonstrating that
our method generates more logical and interpretable reasoning chains than
existing approaches.

</details>


### [59] [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)
*Xinlin Zhuang,Feilong Tang,Haolin Yang,Ming Hu,Huifa Li,Haochen Xue,Yichen Li,Junjun He,Zongyuan Ge,Ying Qian,Imran Razzak*

Main category: cs.CL

TL;DR: DIQ方法能极大提升医学领域LLM微调的数据利用率，仅用极少数据即获得接近或超越全量数据训练的推理能力，显著提升模型实用性和推理专家性。


<details>
  <summary>Details</summary>
Motivation: SFT通常用于将大语言模型（LLM）适配到医学推理等专业领域，但传统的数据选择方法常常带来低质量和冗余样本，导致算力浪费和性能下降。现有基于样本难度的数据选择方案在提升数据质量时存在局限，未有效结合样本在优化中产生的梯度影响。

Method: 提出了Difficulty-Influence Quadrant (DIQ)方法，将样本的梯度影响和推理难度结合建模，在高难度且高梯度影响象限优先选取数据，以实现高效的数据利用和模型训练。通过人工及LLM评测，对比传统方法验证所选子集的数据质量和推理符合专家标准。

Result: 在医学推理基准测试中，DIQ方法仅用1%精挑细选数据，模型性能可与全量数据持平。使用10%时持续超越传统基线，体现了基于DIQ的数据选择对提升模型效率和推理质量的优越性。

Conclusion: DIQ通过结合样本难度与梯度影响指标，实现了极高效的数据精简与模型优化，且生成推理结果更接近医学专家标准。该方法相比盲目扩充数据更具实际应用价值，为SFT在垂直领域的推广提供有效数据筛选新范式。

Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language
Models (LLMs) to specialized domains such as medical reasoning. However,
existing SFT practices often rely on unfiltered datasets that contain redundant
and low-quality samples, leading to substantial computational costs and
suboptimal performance. Although existing methods attempt to alleviate this
problem by selecting data based on sample difficulty, defined by knowledge and
reasoning complexity, they overlook each sample's optimization utility
reflected in its gradient. Interestingly, we find that gradient-based influence
alone favors easy-to-optimize samples that cause large parameter shifts but
lack deep reasoning chains, while difficulty alone selects noisy or overly
complex cases that fail to guide stable optimization. Based on this
observation, we propose a data selection strategy, Difficulty-Influence
Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence
quadrant to balance complex clinical reasoning with substantial gradient
influence, enabling efficient medical reasoning with minimal fine-tuning data.
Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected
subsets demonstrate higher data quality and generate clinical reasoning that is
more aligned with expert practices in differential diagnosis, safety check, and
evidence citation, as DIQ emphasizes samples that foster expert-like reasoning
patterns. Extensive experiments on medical reasoning benchmarks demonstrate
that DIQ enables models fine-tuned on only 1% of selected data to match
full-dataset performance, while using 10% consistently outperforms the
baseline, highlighting the superiority of principled data selection over
brute-force scaling. The code and data are available at
https://github.com/mihara-bot/DIQ.

</details>


### [60] [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)
*Yiming Zeng,Jinghan Cao,Zexin Li,Yiming Chen,Tao Ren,Dawei Xiang,Xidong Wu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: 提出结合抽象语法树的扩散模型，显著提升生成代码的正确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型文本生成方法，在处理结构化领域如源代码时表现不佳，主要由于编程语言具有严格的语法与层次结构，标准的token级噪声方法忽略了这些结构特征，影响模型学习代码有效表示。

Method: 提出了一套语法感知扩散框架，将抽象语法树（AST）中的结构性先验纳入去噪过程。该方法通过基于AST的子树选择，选择性地对语法上有意义的代码片段进行“腐蚀”，而不是随机mask单个token。这样有助于模型在遵循语法边界与长距离依赖的基础上重构程序。

Result: 实验证明，在代码生成任务中，语法感知模式明显提升了语法正确性、重构准确率与对未见代码模式的泛化能力。

Conclusion: 将结构信息引入基于扩散的训练，可显著提升扩散语言模型在代码生成任务中的效果。语法引导的去噪为此类模型的未来提供了有前景的方向。

Abstract: Recent advances in diffusion-based language models have opened new
possibilities for controllable and bidirectional sequence generation. These
models provide an alternative to traditional autoregressive approaches by
framing text generation as an iterative denoising process. However, applying
diffusion models to structured domains such as source code remains a
significant challenge. Programming languages differ from natural language in
that they follow strict syntactic and semantic rules, with hierarchical
organization that must be preserved for correctness. Standard token-level
corruption techniques used during training often ignore this structure, which
may hinder the model's ability to learn meaningful representations of code. To
address this limitation, we propose a syntax-aware diffusion framework that
incorporates structural priors from Abstract Syntax Trees (ASTs) into the
denoising process. Instead of masking individual tokens at random, we
selectively corrupt syntactically meaningful code spans derived from AST
subtrees. This enables the model to reconstruct programs in a way that respects
grammatical boundaries and captures long-range dependencies. Experimental
results demonstrate that syntax-aware corruption significantly improves
syntactic correctness, reconstruction accuracy, and generalization to unseen
code patterns. These findings highlight the potential of incorporating
structural information into diffusion-based training and suggest that
syntax-guided denoising is a promising direction for advancing diffusion-based
language models in code generation tasks.

</details>


### [61] [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)
*Dimitra Panou,Alexandros C. Dimopoulos,Manolis Koubarakis,Martin Reczko*

Main category: cs.CL

TL;DR: 作者提出通过多种开源LLMs组合和特定融合策略，有效提升了生物医学问答系统表现，并在BioASQ 2025挑战中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 面对生物医学文献的爆炸性增长，自动化文本挖掘与问答系统变得尤为重要，但这对模型的检索和理解能力提出了极高要求。研究者亟需高效、准确的方法来提升生物医学领域问答系统的表现。

Method: 作者在BioASQ挑战中，使用多种开源大型语言模型（LLMs）作为支持检索的生成器，并根据问题类型设计了不同的答案融合策略：对于是/否（Yes/No）问题，采用多数投票法确定最终答案；对于列表和事实类问题，则聚合多模型输出的答案。评估了13个最新开源LLMs，并尝试所有可能组合，以优化各类问题的解决方案。

Result: 作者通过多模型组合，在BioASQ 2025挑战的四轮中表现优异：在Synergy任务中，第二轮获得理想答案第一名和准确答案第二名，第三、四轮在准确答案上共享第一名。

Conclusion: 论文验证了多种LLMs组合方法在生物医学问答系统中的有效性，不同模型组合针对不同类型问题能够显著提升系统表现，并为未来问答系统设计提供了依据。

Abstract: Biomedical text mining and question-answering are essential yet highly
demanding tasks, particularly in the face of the exponential growth of
biomedical literature. In this work, we present our participation in the 13th
edition of the BioASQ challenge, which involves biomedical semantic
question-answering for Task 13b and biomedical question-answering for
developing topics for the Synergy task. We deploy a selection of open-source
large language models (LLMs) as retrieval-augmented generators to answer
biomedical questions. Various models are used to process the questions. A
majority voting system combines their output to determine the final answer for
Yes/No questions, while for list and factoid type questions, the union of their
answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring
all possible model combinations to contribute to the final answer, resulting in
tailored LLM pipelines for each question type. Our findings provide valuable
insight into which combinations of LLMs consistently produce superior results
for specific question types. In the four rounds of the 2025 BioASQ challenge,
our system achieved notable results: in the Synergy task, we secured 1st place
for ideal answers and 2nd place for exact answers in round 2, as well as two
shared 1st places for exact answers in round 3 and 4.

</details>


### [62] [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)
*Vallabhaneni Raj Kumar,Ashwin S,Supriya Manna,Niladri Sett,Cheedella V S N M S Hema Harshitha,Kurakula Harshitha,Anand Kumar Sharma,Basina Deepakraj,Tanuj Sarkar,Bondada Navaneeth Krishna,Samanthapudi Shakeer*

Main category: cs.CL

TL;DR: 本研究针对特鲁古语缺乏高质量NLP资源现状，推出了包含理据和公平性考虑的情感分类数据集TeSent及公平评测集TeEEC。综合实验显示，利用理据训练模型能提升准确率、降低模型偏见，并改进模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管特鲁古语有大量使用者，但因缺乏高质量注释资源，NLP领域相关应用稀缺；同时现代机器学习需关注模型的可解释性与公平性，现有特鲁古语数据集在这方面存在不足。

Method: 开发并发布了TeSent数据集（包含注释理据），设计了注释平台与流程，对主流预训练模型进行了带理据与不带理据两种微调，并为解释性与公平性提供了评测工具。实验对比了不同训练策略及解释器表现。

Result: 1. 构建了26,150句的大型特鲁古语情感分类数据集TeSent，并发布了可用于公平性评估的TeEEC子集；2. 实证显示带理据训练的模型在准确率、公平性与可解释性方面优于基线。

Conclusion: 训练中引入人类标注的理据可以提升模型准确率，降低偏见，并增强解释器输出与人类推理的一致性。

Abstract: In the Indian subcontinent, Telugu, one of India's six classical languages,
is the most widely spoken Dravidian Language. Despite its 96 million speaker
base worldwide, Telugu remains underrepresented in the global NLP and Machine
Learning landscape, mainly due to lack of high-quality annotated resources.
This work introduces TeSent, a comprehensive benchmark dataset for sentiment
classification, a key text classification problem, in Telugu. TeSent not only
provides ground truth labels for the sentences, but also supplements with
provisions for evaluating explainability and fairness, two critical
requirements in modern-day machine learning tasks. We scraped Telugu texts
covering multiple domains from various social media platforms, news websites
and web-blogs to preprocess and generate 26,150 sentences, and developed a
custom-built annotation platform and a carefully crafted annotation protocol
for collecting the ground truth labels along with their human-annotated
rationales. We then fine-tuned several SOTA pre-trained models in two ways:
with rationales, and without rationales. Further, we provide a detailed
plausibility and faithfulness evaluation suite, which exploits the rationales,
for six widely used post-hoc explainers applied on the trained models. Lastly,
we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate
fairness of Telugu sentiment and emotion related NLP tasks, and provide a
fairness evaluation suite for the trained classifier models. Our experimental
results suggest that training with rationales may improve model accuracy,
reduce bias in models, and make the explainers' output more aligned to human
reasoning.

</details>


### [63] [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)
*Zhivar Sourati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

TL;DR: 大型语言模型虽然便利，但可能让我们的语言和思维越来越一致，减少创新和集体智慧，应警惕其对认知多样性的影响。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）广泛应用，对人类语言、推理方式的多样性可能产生深远影响，引发了人们对集体智能和创新力受限的担忧。

Method: 综述方法：跨语言学、认知科学与计算机科学的研究证据，分析LLMs如何特定地反映并强化主流的语言和推理风格，同时忽视或边缘化其他声音和推理策略。

Result: LLMs通过模仿其训练数据中的主流模式，强化了主流表达与思维方式，并因其广泛使用导致各领域用户采用趋同化的语言和思维方式，从而加剧思维与认知的同质化。

Conclusion: 如果不加以控制，LLMs对语言与推理方式的同质化会削弱认知多样性，威胁集体智能和社会适应力。

Abstract: Cognitive diversity, reflected in variations of language, perspective, and
reasoning, is essential to creativity and collective intelligence. This
diversity is rich and grounded in culture, history, and individual experience.
Yet as large language models (LLMs) become deeply embedded in people's lives,
they risk standardizing language and reasoning. This Review synthesizes
evidence across linguistics, cognitive, and computer science to show how LLMs
reflect and reinforce dominant styles while marginalizing alternative voices
and reasoning strategies. We examine how their design and widespread use
contribute to this effect by mirroring patterns in their training data and
amplifying convergence as all people increasingly rely on the same models
across contexts. Unchecked, this homogenization risks flattening the cognitive
landscapes that drive collective intelligence and adaptability.

</details>


### [64] [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)
*Clayton Cohn,Surya Rayala,Namrata Srivastava,Joyce Horn Fonteles,Shruti Jain,Xinying Luo,Divya Mereddy,Naveeduddin Mohammed,Gautam Biswas*

Main category: cs.CL

TL;DR: 作者提出理论与技术相结合的LLM教学代理框架，并以Inquizzitor系统验证其高效与价值，强调将大型模型应用于教育应有理论指导。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）具备成为新型教学代理的大好机会，但当前在课堂的应用（如ChatGPT）缺乏早期智能辅导系统所依托的坚实理论。该论文旨在弥补这一理论与实践的差距。

Method: 提出了结合证据中心设计（Evidence-Centered Design, ECD）与社会认知理论（Social Cognitive Theory）的LLM基教学代理框架，以实现以STEM+C（科学、技术、工程、数学+计算）为核心的自适应脚手架。并以Inquizzitor系统为例，展示了该框架的实现。Inquizzitor是一种融合了人—AI混合智能的基于LLM的形成性评估代理，能够按照认知科学原理提供反馈。

Result: 实验和应用结果表明，Inquizzitor可以基于核心学习理论，提供高质量的评估与互动体验，能够给教师有效建议，并且受到学生认可和重视。

Conclusion: 理论驱动的LLM集成模式在教育领域具有巨大潜力，这类系统能够实现自适应且有原则性的教学支持。

Abstract: Large language models (LLMs) present new opportunities for creating
pedagogical agents that engage in meaningful dialogue to support student
learning. However, the current use of LLM systems like ChatGPT in classrooms
often lacks the solid theoretical foundation found in earlier intelligent
tutoring systems. To bridge this gap, we propose a framework that combines
Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding
in LLM-based agents focused on STEM+C learning. We illustrate this framework
with Inquizzitor, an LLM-based formative assessment agent that integrates
human-AI hybrid intelligence and provides feedback grounded in cognitive
science principles. Our findings show that Inquizzitor delivers high-quality
assessment and interaction aligned with core learning theories, offering
teachers effective guidance that students value. This research underscores the
potential for theory-driven LLM integration in education, highlighting the
ability of these systems to provide adaptive and principled instruction.

</details>


### [65] [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)
*Sara Câmara,Eduardo Luz,Valéria Carvalho,Ivan Meneghini,Gladston Moreira*

Main category: cs.CL

TL;DR: 本文提出MOPrompt框架，实现了提示词准确率与token长度的多目标优化。在葡萄牙语情感分析任务中，相较基线模型，MOPrompt能在准确率不下降的情况下大幅缩短输入长度，提升实际部署效率。


<details>
  <summary>Details</summary>
Motivation: 人工设计提示词（prompt）过程复杂且耗时，因此需要自动化的提示词优化方法。现有方法多聚焦于提升任务性能（如准确率），而忽视对于上下文长度（token数）的优化，难以满足真实应用中兼顾效率与效果的需求。

Method: 提出MOPrompt框架，将多目标进化优化（EMO）方法用于同时优化准确率和上下文长度，通过映射Pareto前沿，为不同需求场景提供最优折中解。并在葡萄牙语情感分析任务上，使用Gemma-2B和Sabiazinho-3模型进行评测。

Result: MOPrompt在实验中显著优于基线方法。特别是在Sabiazinho模型上，MOPrompt找到了一个与最佳基线相同最高准确率（0.97）的提示词，但token长度减少了31%。

Conclusion: MOPrompt能有效权衡提示词的准确率和上下文长度，为部署大模型时提供更灵活、高效的应用选择。其多目标优化思想为自动化提示词工程研究提供了新的方向。

Abstract: Prompt engineering is crucial for unlocking the potential of Large Language
Models (LLMs). Still, since manual prompt design is often complex,
non-intuitive, and time-consuming, automatic prompt optimization has emerged as
a research area. However, a significant challenge in prompt optimization is
managing the inherent trade-off between task performance, such as accuracy, and
context size. Most existing automated methods focus on a single objective,
typically performance, thereby failing to explore the critical spectrum of
efficiency and effectiveness. This paper introduces the MOPrompt, a novel
Multi-objective Evolutionary Optimization (EMO) framework designed to optimize
prompts for both accuracy and context size (measured in tokens) simultaneously.
Our framework maps the Pareto front of prompt solutions, presenting
practitioners with a set of trade-offs between context size and performance, a
crucial tool for deploying Large Language Models (LLMs) in real-world
applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,
using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that
MOPrompt substantially outperforms the baseline framework. For the Sabiazinho
model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)
as the best baseline solution, but with a 31% reduction in token length.

</details>


### [66] [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)
*Yujia Zheng,Tianhao Li,Haotian Huang,Tianyu Zeng,Jingyu Lu,Chuangxin Chu,Yuekai Huang,Ziyou Jiang,Qian Xiong,Yuyao Ge,Mingyang Li*

Main category: cs.CL

TL;DR: 本文提出了PromptAnatomy框架，通过细致分解prompt结构并采用新扰动方法ComPerturb，显著提升了对LLM对抗攻击的有效性和解释性，验证了结构化扰动和PPL过滤对鲁棒性评估的重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）对抗攻击方法通常将prompt视为整体文本，忽视了其结构性和不同组成部分对鲁棒性的不同影响。而复杂领域特定的prompt结构丰富，各组件存在脆弱性差异，因此有必要细致分析prompt结构以设计更有效的对抗攻击。

Method: 提出了PromptAnatomy自动化框架，将prompt分解为功能组件，并用新方法ComPerturb针对性生成多样化、可解释的对抗样例。为保证语言合理性，增加了基于困惑度（PPL）的过滤机制。还用PromptAnatomy标注了4个公开instruction-tuning数据集，并由人工验证。

Result: 在四个数据集和五个先进LLM上，ComPerturb方法达到了当前最优的攻击成功率。消融实验显示，prompt分解和PPL过滤具有互补的益处。

Conclusion: prompt的结构感知和可控扰动对于可靠的LLM对抗鲁棒性评估至关重要。PromptAnatomy和ComPerturb为评估和提升大模型鲁棒性提供了新的、有效的工具。

Abstract: Prompt-based adversarial attacks have become an effective means to assess the
robustness of large language models (LLMs). However, existing approaches often
treat prompts as monolithic text, overlooking their structural
heterogeneity-different prompt components contribute unequally to adversarial
robustness. Prior works like PromptRobust assume prompts are value-neutral, but
our analysis reveals that complex, domain-specific prompts with rich structures
have components with differing vulnerabilities. To address this gap, we
introduce PromptAnatomy, an automated framework that dissects prompts into
functional components and generates diverse, interpretable adversarial examples
by selectively perturbing each component using our proposed method, ComPerturb.
To ensure linguistic plausibility and mitigate distribution shifts, we further
incorporate a perplexity (PPL)-based filtering mechanism. As a complementary
resource, we annotate four public instruction-tuning datasets using the
PromptAnatomy framework, verified through human review. Extensive experiments
across these datasets and five advanced LLMs demonstrate that ComPerturb
achieves state-of-the-art attack success rates. Ablation studies validate the
complementary benefits of prompt dissection and PPL filtering. Our results
underscore the importance of prompt structure awareness and controlled
perturbation for reliable adversarial robustness evaluation in LLMs. Code and
data are available at https://github.com/Yujiaaaaa/PACP.

</details>


### [67] [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)
*Maziyar Panahi*

Main category: cs.CL

TL;DR: 本文提出OpenMed NER模型，用DAPT和LoRA结合提升生物医学NER表现，在主流数据集上刷新SOTA，效率高、资源消耗低、完全开源，且便于合规应用。


<details>
  <summary>Details</summary>
Motivation: 医疗数据中大部分信息以非结构化文本存在，如临床笔记和生物医学文献。如何高效、准确地从中抽取命名实体（NER）依然很有挑战性，特别是在保证模型计算效率的同时取得多领域实体类型上的先进性能。

Method: 提出OpenMed NER：在350k段公开、去标识化临床和科研文本上用轻量化领域自适应预训练（DAPT）进行模型训练，使用DeBERTa-v3、PubMedBERT和BioELECTRA为基底，再用LoRA进行参数高效的任务微调（仅更新1.5%参数）。

Result: 在12个已建立的生物医学NER基准测试集上评估，OpenMed NER在其中10个数据集上取得新的微平均F1分数SOTA。对疾病和化学领域基准数据集有显著提升，对专业的基因和细胞系语料提升更大（提升5.3至9.7百分点）。

Conclusion: 开源并经过领域自适应的模型可用更高效和更环保的方式（12小时单卡训练、低碳排放）超越闭源方案，且开放授权便于法规合规（如EU AI Act），促进生物医学NER的实际落地应用。

Abstract: Named-entity recognition (NER) is fundamental to extracting structured
information from the >80% of healthcare data that resides in unstructured
clinical notes and biomedical literature. Despite recent advances with large
language models, achieving state-of-the-art performance across diverse entity
types while maintaining computational efficiency remains a significant
challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted
transformer models that combine lightweight domain-adaptive pre-training (DAPT)
with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs
cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,
publicly available research repositories and de-identified clinical notes
(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA
backbones. This is followed by task-specific fine-tuning with LoRA, which
updates less than 1.5% of model parameters. We evaluate our models on 12
established biomedical NER benchmarks spanning chemicals, diseases, genes, and
species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of
these 12 datasets, with substantial gains across diverse entity types. Our
models advance the state-of-the-art on foundational disease and chemical
benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger
improvements of over 5.3 and 9.7 percentage points on more specialized gene and
clinical cell line corpora. This work demonstrates that strategically adapted
open-source models can surpass closed-source solutions. This performance is
achieved with remarkable efficiency: training completes in under 12 hours on a
single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively
licensed, open-source checkpoints designed to help practitioners facilitate
compliance with emerging data protection and AI regulations, such as the EU AI
Act.

</details>


### [68] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
*Lucio La Cava,Dominik Macko,Róbert Móro,Ivan Srba,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文首次系统提出多语言作者归因问题，发现现有方法在多语、跨语环境下表现有限，强调多语场景下作者归因的复杂性和现实需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）越来越接近人类的流畅度和连贯性，区分机器生成文本（MGT）和人类写作变得愈发困难。原有的机器文本检测多聚焦于二分类问题，但多样化的LLM生态催生了对更细粒度的作者归因（AA）的需求，尤其是在多语言背景下，现有研究多被限制在单一语言（主要是英语），忽视了LLM的多语言实际应用。

Method: 本文首次提出多语言作者归因问题，在18种不同语言（涵盖多种语系和字体）与8类生成者（包括7个LLM和人类）上进行测试，考察传统单语AA方法在多语言环境下的适用性及跨语言迁移能力，同时分析生成者多样性对归因效果的影响。

Result: 部分单语AA方法可以适度适应多语环境，但在跨语系迁移和复杂多语场景下仍面临巨大挑战，归因效果显著下降，显示多语言AA问题远比单语复杂，需要更为健壮的归因方法以适应现实需求。

Conclusion: 现有作者归因方法在多语言和多生成者场景下有显著局限，尤其是语系跨度较大时，归因准确性降低，强调需研发更强大的多语言作者归因技术。

Abstract: As Large Language Models (LLMs) have reached human-like fluency and
coherence, distinguishing machine-generated text (MGT) from human-written
content becomes increasingly difficult. While early efforts in MGT detection
have focused on binary classification, the growing landscape and diversity of
LLMs require a more fine-grained yet challenging authorship attribution (AA),
i.e., being able to identify the precise generator (LLM or human) behind a
text. However, AA remains nowadays confined to a monolingual setting, with
English being the most investigated one, overlooking the multilingual nature
and usage of modern LLMs. In this work, we introduce the problem of
Multilingual Authorship Attribution, which involves attributing texts to human
or multiple LLM generators across diverse languages. Focusing on 18 languages
-- covering multiple families and writing scripts -- and 8 generators (7 LLMs
and the human-authored class), we investigate the multilingual suitability of
monolingual AA methods, their cross-lingual transferability, and the impact of
generators on attribution performance. Our results reveal that while certain
monolingual AA methods can be adapted to multilingual settings, significant
limitations and challenges remain, particularly in transferring across diverse
language families, underscoring the complexity of multilingual AA and the need
for more robust approaches to better match real-world scenarios.

</details>


### [69] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
*Tae Soo Kim,Yoonjoo Lee,Yoonah Park,Jiho Kim,Young-Ho Kim,Juho Kim*

Main category: cs.CL

TL;DR: 论文构建了一个用于考查大模型能否动态推断用户偏好的互动基准集CUPID，发现主流大模型在此能力上表现不佳，CUPID可推动更精准个性化交互的发展。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的个性化方法多假设用户偏好是静态且在所有任务中都适用，但真实世界里人的偏好是动态且随情境变化的。因此有必要研究模型能否推断出用户在不同情境下的偏好，从而实现更符合实际的个性化。

Method: 论文提出了一个名为CUPID的基准数据集，包含了756个人工筛选的用户和基于LLM的聊天助手的多轮交互历史。用户在不同情境下给出请求并通过多轮反馈表达偏好。通过基准评测，考查模型能否根据既往交互推断与新请求相关的偏好并生成相应响应。使用CUPID对10个大模型进行了对比评估。

Result: 当前主流大模型在推断多轮交互中的用户偏好以及识别哪部分历史情境相关性方面表现不佳，精确率低于50%，召回率低于65%。

Conclusion: 现有大模型在动态、情境相关的个性化能力上仍有明显不足。CUPID数据集可作为推动大模型更精准个性化交互能力提升的重要资源。

Abstract: Personalization of Large Language Models (LLMs) often assumes users hold
static preferences that reflect globally in all tasks. In reality, humans hold
dynamic preferences that change depending on the context. As users interact
with an LLM in various contexts, they naturally reveal their contextual
preferences, which a model must infer and apply in future contexts to ensure
alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated
interaction session histories between users and LLM-based chat assistants. In
each interaction session, the user provides a request in a specific context and
expresses their preference through multi-turn feedback. Given a new user
request and prior interaction sessions, our benchmark assesses whether LLMs can
infer the preference relevant to this request and generate a response that
satisfies this preference. With CUPID, we evaluated 10 open and proprietary
LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from
multi-turn interactions and fail to discern what previous context is relevant
to a new request -- under 50% precision and 65% recall. Our work highlights the
need to advance LLM capabilities for more contextually personalized
interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [70] [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)
*Lingyin Zhang,Jun Gao,Xiaoxue Ren,Ziqiang Cao*

Main category: cs.CL

TL;DR: 本文提出了一种无需增加延迟的新颖双向过程奖励模型（BiPRM），通过引入并行的右到左评估，显著提升了大语言模型推理中的中间步骤评分效果，在多个实验中比传统单向模型提升显著，具有实际普适价值。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型（PRMs）通过对推理中的中间步骤进行细粒度评分来提升大语言模型（LLMs）的推理质量，但大多采用单向左到右（L2R）的评估范式，缺乏利用全局上下文的能力，难以利用后续步骤的信息来验证先前步骤的一致性。

Method: 提出了一种双向评估新范式——Bidirectional Process Reward Model（BiPRM），引入并行的右到左（R2L）评价流，与传统的L2R流程并行，使后续推理可在实时中反向帮助评估前面步骤。R2L通过仅修改提示词实现，不增加额外参数或推理延迟，兼容性强。

Result: 在两个数学推理基准上进行实验，涉及三种策略模型、三种骨干模型和三种不同的PRM目标。在所有设置下，BiPRM在逐步奖励评估上均优于单向基线，最高提升达31.9%。

Conclusion: BiPRM在效果、鲁棒性和普适性方面表现优异，为过程型奖励建模提供了新的有前景的方向。

Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning quality of Large Language Models (LLMs) by assigning fine-grained
scores to intermediate reasoning steps within a solution trajectory. However,
existing PRMs predominantly adopt a unidirectional left-to-right (L2R)
evaluation paradigm, which limits their ability to leverage global context,
making it challenging to verify the consistency of earlier steps based on later
ones. In light of these challenges, we propose a novel bidirectional evaluation
paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly
incorporates a parallel right-to-left (R2L) evaluation stream alongside the
conventional L2R flow, enabling later reasoning steps to help assess earlier
ones in real time. Notably, the built-in R2L evaluation is implemented solely
through prompt modifications that reverse the original reasoning trajectory,
without any additional parameters or inference latency introduced. This ensures
BiPRM remains both efficient and broadly compatible with existing PRM studies.
We conduct extensive experiments on two mathematical reasoning benchmarks using
samples generated by three different policy models. Our method, BiPRM, is
evaluated across three backbones and three distinct PRM objectives. Across all
settings, BiPRM consistently outperforms unidirectional baselines, achieving up
to a 31.9% improvement in stepwise reward evaluation. Generally, our results
highlight BiPRM's effectiveness, robustness, and general applicability,
offering a promising new direction for process-based reward modeling.

</details>


### [71] [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Lizhe Zhang,Yan Liu,Bin Qin*

Main category: cs.CL

TL;DR: 提出多智能体链式协作机制与长链训练，显著提升RAG模型对内外知识的融合能力和问答表现。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法难以充分利用知识，特别是在大模型内在知识和外部检索知识的协同上存在局限，召回的内容有时会误导生成过程。

Method: 提出Collaborative Chain-of-Agents (CoCoA) 框架，首先以CoCoA-zero进行条件性知识归纳与推理，再在此基础上利用长链训练策略（CoCoA）对LLM进行多智能体推理轨迹合成，共同优化模型对内在和检索知识的运用。

Result: CoCoA-zero和CoCoA在开放域和多跳问答任务上均取得了优异表现，提升了知识整合与利用能力。

Conclusion: 协同链式多智能体框架能显著提升RAG模型在复杂知识任务中的表现，对多源知识整合有突出贡献。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for
enhancing the capabilities of Large Language Models (LLMs), especially in
knowledge-intensive tasks. Despite its advantages, current RAG methods often
struggle to *fully exploit knowledge during generation*. In particular, the
synergy between the model's internal parametric knowledge and external
retrieved knowledge remains limited. Retrieved contents may sometimes mislead
generation, while certain generated content can guide the model toward more
accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a
framework designed to enhance explicitly synergy over both parametric and
retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent
RAG framework that first performs conditional knowledge induction and then
reasons answers. Building on this, we develop CoCoA, a long-chain training
strategy that synthesizes extended multi-agent reasoning trajectories from
CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability
to explicitly integrate and jointly leverage parametric and retrieved
knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior
performance on open-domain and multi-hop QA tasks.

</details>


### [72] [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)
*Berkay Köprü,Mehrzad Mashal,Yigit Gurses,Akos Kadar,Maximilian Schmitt,Ditty Mathew,Felix Burkhardt,Florian Eyben,Björn W. Schuller*

Main category: cs.CL

TL;DR: 本文首次系统性提出了表达性泄露而非语义泄露的问题，并建立数据集与评价体系。发现随着模型规模增大泄露减轻，但需在模型训练阶段专门处理，单靠提示无法缓解，尤其负面情感提示更易造成泄露。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在具备优越NLP能力的同时，因吸收更广泛上下文而容易引入与语义无关的错误信息，尤其在以往只关注语义泄露（semantic leakage）的前提下，提出关注表达性泄露（expression leakage）这一新现象。

Method: 1）建立针对表达性泄露的基准数据集，并提出基于common-crawl自由文本的自动生成方法；2）提出可与人工判断高度相关的自动化评估流程，减少对每个模型进行人工标注的需求。3）通过实验对不同参数规模和模型家族的泄露现象进行量化分析，以及负面/正面情感提示注入对泄露程度的影响。

Result: （1）随着模型参数规模增大，在同一LLM家族内表达性泄露有所减弱；（2）表达性泄露的缓解需要在模型训练和构建阶段进行特别处理，仅通过prompt工程无法解决；（3）实验还发现，负面情感比正面情感更易引发表达性泄露。

Conclusion: 表达性泄露是LLMs模型实际应用中的新挑战，需要专门的模型构建策略控制，很难通过后期提示或简单技术消除。对于提高模型生成文本的相关性和安全性具有重大意义。

Abstract: Large language models (LLMs) have advanced natural language processing (NLP)
skills such as through next-token prediction and self-attention, but their
ability to integrate broad context also makes them prone to incorporating
irrelevant information. Prior work has focused on semantic leakage, bias
introduced by semantically irrelevant context. In this paper, we introduce
expression leakage, a novel phenomenon where LLMs systematically generate
sentimentally charged expressions that are semantically unrelated to the input
context. To analyse the expression leakage, we collect a benchmark dataset
along with a scheme to automatically generate a dataset from free-form text
from common-crawl. In addition, we propose an automatic evaluation pipeline
that correlates well with human judgment, which accelerates the benchmarking by
decoupling from the need of annotation for each analysed model. Our experiments
show that, as the model scales in the parameter space, the expression leakage
reduces within the same LLM family. On the other hand, we demonstrate that
expression leakage mitigation requires specific care during the model building
process, and cannot be mitigated by prompting. In addition, our experiments
indicate that, when negative sentiment is injected in the prompt, it disrupts
the generation process more than the positive sentiment, causing a higher
expression leakage rate.

</details>


### [73] [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
*Raviraj Joshi,Rakesh Paul,Kanishk Singla,Anusha Kamath,Michael Evans,Katherine Luna,Shaona Ghosh,Utkarsh Vaidya,Eileen Long,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 本文提出CultureGuard方案，实现了高质量多语言安全数据集的自动构建，通过微调获得了多语言内容安全守卫模型，在多项评测中取得最佳性能，有效提升了LLM在非英语环境下的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在非英语环境下的安全防护能力较弱，主要原因是高质量、文化对齐的标注安全数据集稀缺，数据收集成本高昂。本文旨在解决多语言环境下内容安全保护难题，推动非英语语言下的LLM安全性提升。

Method: 提出了一套包含四个阶段的合成数据生成与筛选流程，包括文化数据分离、文化数据适配、机器翻译和质量过滤。该流程支持将Nemotron-Content-Safety-Dataset-V2英文安全数据集转化并扩展为八种非英语语言（阿拉伯语、德语、西班牙语、法语、印地语、日语、泰语和中文）。最后基于LoRA微调技术训练出多语言安全守卫模型。

Result: 生成了Nemotron-Content-Safety-Dataset-Multilingual-v1，包含九种语言共386,661个样本。训练出的Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1在多语言内容安全评测中达到国际领先水平。还发现现有开源LLM在非英语语言回答中更容易产生不安全内容。

Conclusion: 该研究提出的方法有效提升了多语言内容安全数据生成与模型训练能力，缩小了LLM在多语言环境下的内容安全差距，为构建具备文化意识的多语言安全守卫模型奠定基础。

Abstract: The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,
comprises 386,661 samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.
The final model achieves state-of-the-art performance on several multilingual
content safety benchmarks. We also benchmark the latest open LLMs on
multilingual safety and observe that these LLMs are more prone to give unsafe
responses when prompted in non-English languages. This work represents a
significant step toward closing the safety gap in multilingual LLMs by enabling
the development of culturally aware safety guard models.

</details>


### [74] [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)
*Cheng Wang,ziru Liu,Pengcheng Tang,Mingyu Zhang,Quanyu Dai,Yue Zhu*

Main category: cs.CL

TL;DR: 论文提出IterChat框架，将多轮偏好抽取分解为单轮并创新数据格式，结合GPT-4自动生成高质量数据，实验表明新方案提升解码性能和标注效率，有效缓解多轮对话系统中的偏好识别难题。


<details>
  <summary>Details</summary>
Motivation: 识别对话系统中的用户偏好对于提升服务质量至关重要，但高质量带标注的多轮对话数据获取困难，且标注过程易出错，训练模型时序列依赖带来误差传递问题。研究动因是解决高质量标注困难和泛化性能提升的矛盾。

Method: 提出IterChat数据生成框架，将多轮偏好抽取拆分为多次一轮抽取，并设计新数据格式，将对话数据分为历史偏好和单轮对话片段，提升标注效率。利用GPT-4定义偏好槽位并随机组合槽位及取值，自动生成多样高质量对话数据。

Result: 在实验中，无论微调还是少量示例提示，新数据格式相较原始多轮格式在性能上均有显著提升，且标注效率提升，胜率高28.4%。

Conclusion: 新提出的IterChat框架和数据格式解决了多轮对话偏好抽取中的标注困难和误差传播，为偏好识别任务提供了更高效和更高质量的数据基础，显著提升系统性能和可用性。

Abstract: Identifying user preferences in dialogue systems is a pivotal aspect of
providing satisfying services. Current research shows that using large language
models (LLMs) to fine-tune a task-specific preference extractor yields
excellent results in terms of accuracy and generalization. However, the primary
challenge stems from the inherent difficulty in obtaining high-quality labeled
multi-turn dialogue data. Accurately tracking user preference transitions
across turns not only demands intensive domain expertise and contextual
consistency maintenance for annotators (termed \textbf{``Annotating
Disaster''}) but also complicates model training due to error propagation in
sequential dependency learning. Inspired by the observation that multi-turn
preference extraction can be decomposed into iterative executions of one-turn
extraction processes. We propose a novel dialogue data generation framework
named \textbf{IterChat}. First, we construct a new data format that categorizes
the dialogue data into attributed historical preferences and one-turn
dialogues. This reduces the probability of annotation errors and improves
annotation efficiency. Then, to generate a high-quality and diverse dialogue
dataset, we adopt GPT4 to pre-define the preference slots in the target
preference extractor task and then randomly sample the subset of the slots and
their corresponding schema values to create the dialogue datasets. Experimental
results indicate that fine-tuning or only few-shot prompting with the new
dialogue format yields superior performance compared to the original multi-turn
dialogues. Additionally, the new data format improves annotator efficiency with
a win rate of 28.4\% higher than the original multi-turn dialogues.

</details>


### [75] [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
*Alva West,Yixuan Weng,Minjun Zhu,Luodan Zhang,Zhen Lin,Guangsheng Bao,Yue Zhang*

Main category: cs.CL

TL;DR: 以往AI文本检测方法忽略异常的位置分布，易受对抗攻击。作者发现AI文本存在显著非平稳性并提出TDT，利用小波变换建模时序动态，显著提升了检测及对抗稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本检测方法会将每个token的特征聚合成一个分数，忽略了异常出现的具体位置。这导致对于利用“局部篡改”进行对抗的文本，这些检测方法容易失效。

Method: 提出了新的检测范式TDT（Temporal Discrepancy Tomography），将token级差异建模为时序信号，通过连续小波变换（Continuous Wavelet Transform）实现保留文本中异常的时序和尺度信息，从而提升检测能力。

Result: TDT在RAID基准上达到0.855 AUROC，比最好基线提升7.1%；在面对HART Level 2对抗性同义改写攻击时，AUROC提升14.1%；仅增加13%的计算开销。

Conclusion: AI生成文本具有显著的非平稳性，保留和建模文本的时序动态对于鲁棒检测至关重要，TDT为此开辟了新路径。

Abstract: The field of AI-generated text detection has evolved from supervised
classification to zero-shot statistical analysis. However, current approaches
share a fundamental limitation: they aggregate token-level measurements into
scalar scores, discarding positional information about where anomalies occur.
Our empirical analysis reveals that AI-generated text exhibits significant
non-stationarity, statistical properties vary by 73.8\% more between text
segments compared to human writing. This discovery explains why existing
detectors fail against localized adversarial perturbations that exploit this
overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),
a novel detection paradigm that preserves positional information by
reformulating detection as a signal processing task. TDT treats token-level
discrepancies as a time-series signal and applies Continuous Wavelet Transform
to generate a two-dimensional time-scale representation, capturing both the
location and linguistic scale of statistical anomalies. On the RAID benchmark,
TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More
importantly, TDT demonstrates robust performance on adversarial tasks, with
14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its
sophisticated analysis, TDT maintains practical efficiency with only 13\%
computational overhead. Our work establishes non-stationarity as a fundamental
characteristic of AI-generated text and demonstrates that preserving temporal
dynamics is essential for robust detection.

</details>


### [76] [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)
*Manuel Cossio*

Main category: cs.CL

TL;DR: 本文全面梳理并分类大型语言模型的幻觉问题，分析成因、表现及检测缓解方法，指出幻觉是理论上不可避免的，需要持续监测和人类监督。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM极大推动了自然语言处理，但幻觉现象（生成合理但不准确内容）依然是重要挑战。作者意图梳理这一课题，以指导未来研究和实际部署。

Method: 作者提出了LLM幻觉的形式化定义和理论框架，系统阐述幻觉的分类、表现、成因及其评价方法，并调研了当前检测和缓解的架构与系统措施。同时，还整合了相关在线资源。

Result: 本文构建了LLM幻觉的完整分类体系，提出区分内在/外在、事实性/忠实性，加深了对幻觉具体表现和成因的理解，还细致介绍了检测、评价与缓解工具。并提供追踪LLM表现的网络资源。

Conclusion: 报告强调大型语言模型（LLM）幻觉现象的复杂性与多面性，并指出幻觉在理论上是不可避免的。为实现负责任、可靠的部署，未来工作应着重于健全的检测、缓解方法以及持续的人类监督。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their propensity for hallucination, generating plausible but factually
incorrect or fabricated content, remains a critical challenge. This report
provides a comprehensive taxonomy of LLM hallucinations, beginning with a
formal definition and a theoretical framework that posits its inherent
inevitability in computable LLMs, irrespective of architecture or training. It
explores core distinctions, differentiating between intrinsic (contradicting
input context) and extrinsic (inconsistent with training data or reality), as
well as factuality (absolute correctness) and faithfulness (adherence to
input). The report then details specific manifestations, including factual
errors, contextual and logical inconsistencies, temporal disorientation,
ethical violations, and task-specific hallucinations across domains like code
generation and multimodal applications. It analyzes the underlying causes,
categorizing them into data-related issues, model-related factors, and
prompt-related influences. Furthermore, the report examines cognitive and human
factors influencing hallucination perception, surveys evaluation benchmarks and
metrics for detection, and outlines architectural and systemic mitigation
strategies. Finally, it introduces web-based resources for monitoring LLM
releases and performance. This report underscores the complex, multifaceted
nature of LLM hallucinations and emphasizes that, given their theoretical
inevitability, future efforts must focus on robust detection, mitigation, and
continuous human oversight for responsible and reliable deployment in critical
applications.

</details>


### [77] [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)
*Amir DN Cohen,Hilla Merhav,Yoav Goldberg,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 本文提出了希伯来语机器阅读理解（MRC）新基准HeQ，并针对该语言的形态特性开发了适用的标注与评价体系，填补了现有基准在语义理解方面的空白，推动了形态丰富语言的NLP研究。


<details>
  <summary>Details</summary>
Motivation: 现有希伯来语自然语言处理（NLP）基准主要集中在形态句法任务上，忽视了语义理解层面。希伯来语的形态复杂性使得在机器阅读理解（MRC）任务中存在标注不一致和评价困难，需要更适合该语言特点的基准和标准。

Method: 提出一套新的数据标注指南、受控众包协议和针对形态丰富语言的改进评价指标，建立了包含希伯来语维基百科和以色列科技新闻的HeQ阅读理解数据集。通过实验证明传统指标在该语种下不适用，并提出了相应的改进。

Result: 建立了包含30,147组多样问答对的HeQ基准集，发现标准F1和EM指标用于希伯来语存在缺陷，提出了更合适的指标。同时，模型在形态句法任务与MRC任务上的性能相关性较低，说明专注于前者的模型可能无法很好地处理语义理解任务。

Conclusion: HeQ数据集的开发揭示了形态丰富语言在自然语言理解上的新挑战，并推动了该领域算法和评测方法的进步。该工作将促进希伯来语以及其他形态丰富语言更全面的理解与建模。

Abstract: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly
on morpho-syntactic tasks, neglecting the semantic dimension of language
understanding. To bridge this gap, we set out to deliver a Hebrew Machine
Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive
Question Answering. The morphologically rich nature of Hebrew poses a challenge
to this endeavor: the indeterminacy and non-transparency of span boundaries in
morphologically complex forms lead to annotation inconsistencies,
disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled
crowdsourcing protocol, and revised evaluation metrics that are suitable for
the morphologically rich nature of the language. Our resulting benchmark, HeQ
(Hebrew QA), features 30,147 diverse question-answer pairs derived from both
Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation
reveals that standard evaluation metrics such as F1 scores and Exact Match (EM)
are not appropriate for Hebrew (and other MRLs), and we propose a relevant
enhancement.
  In addition, our experiments show low correlation between models' performance
on morpho-syntactic tasks and on MRC, which suggests that models designed for
the former might underperform on semantics-heavy tasks. The development and
exploration of HeQ illustrate some of the challenges MRLs pose in natural
language understanding (NLU), fostering progression towards more and better NLU
models for Hebrew and other MRLs.

</details>


### [78] [AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy](https://arxiv.org/abs/2508.01815)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Tan Chuan Fu,Yue Xiu,Dusit Niyato,Jonathan Z. Low,Eugene Ho Hong Zhuang,Daren Zong Loong Tan*

Main category: cs.CL

TL;DR: 该论文提出了AgenticT^2$S，一个能跨异构知识图谱模块化多agent问答框架，在循环经济领域显著提升了跨图推理能力和问答准确性，为低资源及多源知识图谱应用场景提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 异构知识图谱问答（KGQA）面临需跨越不同结构、数据信息分散与部分对齐不全等挑战，特别是在循环经济等领域，信息分布在多个独立构建的知识图谱中。已有的text-to-SPARQL方法依赖于大规模特定领域微调，仅局限于单一图谱环境，缺乏在低资源与多图谱情况下的泛化能力。

Method: 提出了AgenticT^2S模块化框架，将KGQA流程划分为由专属agent管理的子任务，包括检索、查询生成与验证。调度器基于弱到强的对齐策略将子目标分配至不同知识图谱。两阶段验证器结合符号验证及反事实一致性检测，筛查查询结构与语义问题。

Result: 在循环经济真实知识图谱实验中，AgenticT^2$S相比最佳基线，执行准确率提升17.3%，三元组级F1提升25.4%，平均提示长度减少46.4%。

Conclusion: 基于agent的框架能高效支持跨图谱的结构与语义推理，为可持续领域提供了稳健的知识图谱问答解决方案。

Abstract: Question answering over heterogeneous knowledge graphs (KGQA) involves
reasoning across diverse schemas, incomplete alignments, and distributed data
sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific
fine-tuning or operate within single-graph settings, limiting their
generalizability in low-resource domains and their ability to handle queries
spanning multiple graphs. These challenges are particularly relevant in domains
such as the circular economy, where information about classifications,
processes, and emissions is distributed across independently curated knowledge
graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes
KGQA into subtasks managed by specialized agents responsible for retrieval,
query generation, and verification. A scheduler assigns subgoals to different
graphs using weak-to-strong alignment strategies. A two-stage verifier detects
structurally invalid and semantically underspecified queries through symbolic
validation and counterfactual consistency checks. Experiments on real-world
circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy
by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing
the average prompt length by 46.4%. These results demonstrate the benefits of
agent-based schema-aware reasoning for scalable KGQA and support
decision-making in sustainability domains through robust cross-graph reasoning.

</details>


### [79] [MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)
*Rubin Wei,Jiaqi Cao,Jiarui Wang,Jushi Kai,Qipeng Guo,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出解耦记忆与生成的解码器架构，引入预训练MLP外部记忆，解决模型幻觉问题并显著提升速度和性能，在多项基准上效果优异。


<details>
  <summary>Details</summary>
Motivation: 现代大模型在生成文本时经常出现幻觉（hallucination）问题，尤其在知识密集型任务中表现受限。传统的检索增强生成（RAG）方法虽然引入外部知识，但由于检索器的非参数性质，导致与LLM深度交互有限。

Method: 提出将LLM解码器中的记忆与生成解耦，引入一个预训练的可微外部MLP存储，用于模仿检索器在整个预训练数据集上的行为。模型整体架构由Transformer解码器和专门用于语言建模和检索器模仿预训练的外部MLP记忆组成。

Result: 新结构在下游任务上展现出优秀的困惑度和性能。与传统的decoder-only模型相比，在WikiText-103和Web数据集上分别提升了17.5%和24.1%。在三项幻觉基准和九项记忆密集任务中表现更优，且推理速度显著提升（相较kNN-LM快80倍，相较decoder-only模型快1.3倍）。MLP外部记忆还能提升推理类任务表现（如StrategyQA），不会像kNN-LM那样损害推理能力。

Conclusion: 通过将外部MLP记忆模块与解码器分离并分别优化，模型在减少幻觉、提升记忆相关任务性能及推理速度上取得突破，兼具高效性与泛化能力，未来代码和模型将开源。

Abstract: While modern decoder-only LLMs achieve superior performance across various
domains, hallucinations have risen to be a common problem in their generated
text, hindering their application in knowledge-intensive tasks.
Retriever-augmented generation (RAG) offers a solution, but the non-parametric
nature of the retriever hinders its deep interaction with LLM. In this work, we
propose to decouple memorization from the LLM decoder using a pretrained,
differentiable external memory. The external memory is an MLP pretrained by
imitating the behavior of a retriever on the entire pretraining dataset. Our
resulting architecture, which comprises a transformer decoder and an external
MLP memory pretrained on language modeling and retriever imitation
respectively, demonstrates strong perplexity and performance on downstream
tasks. Experiments show our architecture exhibits steeper power-law scaling
with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web
datasets compared to decoder-only models while benefiting from added training
without overfitting. We demonstrate superior performance on three hallucination
benchmarks and nine memory-intensive tasks. Additionally, our approach delivers
$80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference
than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP
memory improves StrategyQA performance. We will open-source our code and models
in the future.

</details>


### [80] [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents](https://arxiv.org/abs/2508.01858)
*Yuhan Guo,Cong Guo,Aiwen Sun,Hongliang He,Xinyu Yang,Yue Lu,Yingji Zhang,Xuntao Guo,Dong Zhang,Jianzhuang Liu,Jiang Duan,Yijia Xiao,Liangjian Wen,Hai-Ming Xu,Yong Dai*

Main category: cs.CL

TL;DR: 本文提出将web agent能力分为知识学习与认知推理两阶段，基于新知识框架及数据集，开发出Web-CogReasoner新模型，在通用性和推理任务上领先现有方法，并提供开源评测套件和资源。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型推动了web agent的发展，但目前的agent在认知推理前，缺乏系统性的知识获取。为提升web agent有效应对复杂数字环境的能力，需要首先完善其知识内容，再强化其认知过程。

Method: 提出Web-CogKnowledge Framework，区分知识为事实性、概念性与程序性，将agent能力分为知识内容学习（记忆、理解）和认知过程（探索），并构建Web-CogDataset（从14个真实网站整理而成），为agent奠定概念和推理基础。此外，开发知识驱动的链式思维(Chain-of-Thought, CoT)推理框架，并训练出Web-CogReasoner模型，同时引入Web-CogBench评价体系。

Result: Web-CogReasoner在各项实验中，尤其在面对未知任务时，展现出对比其他模型的显著优势。Web-CogBench也能系统评估agent在知识和认知领域的能力。

Conclusion: 系统性地将知识内容学习和认知推理阶段区分，并配合针对性的知识库和推理框架，可大幅提升web agent的泛化和推理能力，为web agent的发展奠定基础。

Abstract: Multimodal large-scale models have significantly advanced the development of
web agents, enabling perception and interaction with digital environments akin
to human cognition. In this paper, we argue that web agents must first acquire
sufficient knowledge to effectively engage in cognitive reasoning. Therefore,
we decompose a web agent's capabilities into two essential stages: knowledge
content learning and cognitive processes. To formalize this, we propose
Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and
Procedural. In this framework, knowledge content learning corresponds to the
agent's processes of Memorizing and Understanding, which rely on the first two
knowledge types, representing the "what" of learning. Conversely, cognitive
processes correspond to Exploring, grounded in Procedural knowledge, defining
the "how" of reasoning and action. To facilitate knowledge acquisition, we
construct the Web-CogDataset, a structured resource curated from 14 real-world
websites, designed to systematically instill core knowledge necessary for web
agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon
which comprehension is built-as well as the basis for learning how to reason
and act. Building on this foundation, we operationalize these processes through
a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing
and training our proposed agent, the Web-CogReasoner. Extensive experimentation
reveals its significant superiority over existing models, especially in
generalizing to unseen tasks where structured knowledge is decisive. To enable
rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation
suite designed to assess and compare agent performance across the delineated
knowledge domains and cognitive capabilities. Our code and data is open sourced
at https://github.com/Gnonymous/Web-CogReasoner

</details>


### [81] [Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2508.01862)
*Yijun Feng*

Main category: cs.CL

TL;DR: 作者提出了反事实探测方法，通过制造微妙的反事实扰动，有效检测和降低大语言模型中的幻觉输出，平均可降低幻觉评分24.5%，且无需重新训练模型，可作为实时校验机制使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多种任务上表现优异，但经常会生成流畅但事实错误或缺乏支持的内容（即幻觉）。如何检测并缓解这些幻觉，是当前LLM应用中的重要挑战。

Method: 作者提出了一种新的方法——反事实探测（Counterfactual Probing）。方法具体为：动态生成看似合理但包含微妙事实错误的反事实陈述，再评估模型对这些扰动的敏感性。通过观察模型对反事实变化的鲁棒性，判断模型输出的内容真实性，并设计了适应性缓解策略。该方法无需对模型重新训练，可作为实时验证机制集成到现有LLM流程中。

Result: 在TruthfulQA、事实陈述数据集及人工策划的幻觉案例上进行了全面评测。反事实探测方法在幻觉检测方面优于基线方法，且采用缓解策略后，幻觉评分平均降低了24.5%。

Conclusion: 反事实探测为检测与缓解LLM幻觉输出提供了有效的技术方案，可无缝集成到现有体系中，有助于提升生成内容的可靠性和真实度。

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse tasks, yet they frequently generate hallucinations outputs that are
fluent but factually incorrect or unsupported. We propose Counterfactual
Probing, a novel approach for detecting and mitigating hallucinations in LLM
outputs. Our method dynamically generates counterfactual statements that appear
plausible but contain subtle factual errors, then evaluates the model's
sensitivity to these perturbations. We hypothesize that genuine knowledge
exhibits robustness to counterfactual variations, while hallucinated content
shows inconsistent confidence patterns when confronted with plausible
alternatives. Our comprehensive evaluation on TruthfulQA, factual statement
datasets, and curated hallucination examples demonstrates that counterfactual
probing achieves superior detection performance compared to baseline methods,
while our adaptive mitigation strategies reduce hallucination scores by an
average of 24.5%. The approach requires no model retraining and can be
integrated into existing LLM pipelines as a realtime verification mechanism.

</details>


### [82] [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)
*Jaskaranjeet Singh,Rakesh Thakur*

Main category: cs.CL

TL;DR: 提出了首个公开旁遮普语大模型PunGPT2及检索增强和量子检索框架，在旁遮普语NLP各项指标全面超越多语种大模型，为小语种AI扩展和量子NLP开创技术范式。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对低资源语言支持不足，尤其旁遮普语相关高性能及可复现开源模型和特定检索生成框架长期缺失；同类工作在语法/形态/事实性和上下文相关性上有明显短板，激发创新需求。

Method: 自主构建35GB旁遮普语多领域数据集，训练原生模型；设计结合字节对编码优化分词器和语言学对齐的预训练目标；提出融合稠密（FAISS）与稀疏（BM25）检索和量子启发匹配的“Quantum-RAG”，并开发参数高效、指令微调的Pun-Instruct（QLoRA微调）；与多语种基线模型对比实验。

Result: PunGPT2系列模型在困惑度、事实性、流畅度上显著优于mBERT、mT5、MuRIL等多语种基线；Pun-RAG和Quantum-RAG大幅提升检索准确率和生成相关性，且内存消耗低。实现小语种指令遵循与零样本推理能力突破。

Conclusion: 本工作不仅提出了首个完整开源的旁遮普语大语言模型（PunGPT2），还展示了量子表征在低资源语言NLP检索生成中的创新应用，极大提升了小语种AI的性能和可拓展性。

Abstract: Despite the rapid advancement of large language models (LLMs), low-resource
languages remain largely excluded from the NLP landscape. We present PunGPT2,
the first fully open-source suite of Punjabi large language models, trained
from scratch on a 35GB domain-diverse corpus encompassing literature, religious
texts, news, and social discourse. Unlike prior multilingual approaches,
PunGPT2 captures rich syntactic and morphological features unique to Punjabi
through a tokenizer optimised with byte pair encoding and linguistically
aligned pretraining objectives. To improve factual grounding and domain recall,
we introduce Pun-RAG, a retrieval-augmented generation framework combining
PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We
further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant
using QLoRA, enabling robust zero-shot and instruction-following performance
with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system
that fuses sparse (BM25) and dense methods with quantum-inspired semantic
matching. By encoding queries using amplitude-based embeddings and retrieving
via quantum kernel similarity, Quantum-RAG achieves improved contextual
relevance with minimal memory overhead marking the first practical integration
of quantum representations in low-resource language generation. Our models
significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in
perplexity, factuality, and fluency. This work provides a scalable,
reproducible blueprint for extending LLM capabilities to underrepresented
languages and pioneers quantum-aware retrieval in low-resource NLP

</details>


### [83] [Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback](https://arxiv.org/abs/2508.01930)
*Tom S. Juzek,Zina B. Ward*

Main category: cs.CL

TL;DR: 本文分析了大语言模型因人类反馈学习过程产生词汇过度使用现象，通过实验确认了LHF引起特定用词偏好，为AI可解释性和模型对齐研究提供了数据与程序透明度新见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在生成文本时倾向于过度使用某些词汇，比如“delve”和“intricate”，但造成这一现象的具体原因尚不清楚。

Method: 本研究使用Meta的Llama模型，分析了“来自人类反馈学习”（LHF，包括强化学习和直接偏好优化）对词汇选择偏好的影响。作者提出了一种检测LLMs潜在LHF诱发词汇偏好的简单方法，并通过实验模拟LHF过程，验证相关词汇的过度使用现象。

Result: 实验结果证明，参与者系统性地更偏好包含特定词汇的文本变体，证实LHF过程会导致特定词汇的过度使用。这揭示了LHF指导人群与普通LLM用户的词汇期望可能存在差异。

Conclusion: LHF过程会引发LLMs在词汇选择上的某种“失调”，体现了训练时数据与程序透明度对AI对齐研究的重要性。本文为可解释性AI领域的研究增添了新的视角，提示在AI模型训练中需关注不同人群间的语言预期分歧。

Abstract: Large Language Models (LLMs) are known to overuse certain terms like "delve"
and "intricate." The exact reasons for these lexical choices, however, have
been unclear. Using Meta's Llama model, this study investigates the
contribution of Learning from Human Feedback (LHF), under which we subsume
Reinforcement Learning from Human Feedback and Direct Preference Optimization.
We present a straightforward procedure for detecting the lexical preferences of
LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to
lexical overuse by experimentally emulating the LHF procedure and demonstrating
that participants systematically prefer text variants that include certain
words. This lexical overuse can be seen as a sort of misalignment, though our
study highlights the potential divergence between the lexical expectations of
different populations -- namely LHF workers versus LLM users. Our work
contributes to the growing body of research on explainable artificial
intelligence and emphasizes the importance of both data and procedural
transparency in alignment research.

</details>


### [84] [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)
*Philip Schroeder,Ondrej Biza,Thomas Weng,Hongyin Luo,James Glass*

Main category: cs.CL

TL;DR: 本文提出ROVER框架，通过递归分解长视频并滑动窗口推理，使视觉-语言模型对长期视频任务的理解更准确高效，在多项视频推理任务中取得优异表现，并显著减少模型的幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型虽然在图像理解任务上表现出色，但在需要对视频中长序列帧进行推理的场景下依然存在困难。这一问题在真实环境感知任务中尤为突出，因为这些任务依赖于对一系列连续视觉输入的长期推理，因此亟需新的方法来提升模型在此类场景中的实用性。

Method: 提出了一种名为ROVER（Reasoning Over VidEo Recursively）的框架。该方法通过递归方式将长时序的视频轨迹分解为对应短子任务的片段，使模型能够聚焦于局部时段的推理，同时保留整体上下文。ROVER以In-context learning为基础实现，并引入子任务级滑动上下文窗口，使推理时间复杂度与视频长度线性扩展。

Result: ROVER在OpenX Embodiment视频、RoboCasa新数据集（543个包含专家及非专家操作的视频，涵盖27类机器人任务）上进行了评估。在任务进度估计、帧级自然语言推理以及视频问答三类视频推理任务上均优于现有强基线方法。ROVER能有效减少推理的幻觉现象，特别是在非最优或异常任务片段中表现突出。

Conclusion: ROVER能够递归地分解视频任务，将长期视频轨迹有效处理为短期子任务，有效提升了视觉-语言模型在多种视频推理任务中的表现，并实现了更高的效率与准确性。该方法减轻了长序列推理中的幻觉，具备良好的实际应用潜力及可扩展性。

Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across
diverse image understanding tasks, but still struggle in settings that require
reasoning over extended sequences of camera frames from a video. This limits
their utility in embodied settings, which require reasoning over long frame
sequences from a continuous stream of visual input at each moment of a task
attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo
Recursively), a framework that enables the model to recursively decompose
long-horizon video trajectories into segments corresponding to shorter subtasks
within the trajectory. In doing so, ROVER facilitates more focused and accurate
reasoning over temporally localized frame sequences without losing global
context. We evaluate ROVER, implemented using an in-context learning approach,
on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa
that consists of 543 videos showing both expert and perturbed non-expert
trajectories across 27 robotic manipulation tasks. ROVER outperforms strong
baselines across three video reasoning tasks: task progress estimation,
frame-level natural language reasoning, and video question answering. We
observe that, by reducing the number of frames the model reasons over at each
timestep, ROVER mitigates hallucinations, especially during unexpected or
non-optimal moments of a trajectory. In addition, by enabling the
implementation of a subtask-specific sliding context window, ROVER's time
complexity scales linearly with video length, an asymptotic improvement over
baselines. Demos, code, and data available at: https://rover-vlm.github.io

</details>


### [85] [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959)
*Junjie Wu,Jiangnan Li,Yuqing Li,Lemao Liu,Liyan Xu,Jiwei Li,Dit-Yan Yeung,Jie Zhou,Mo Yu*

Main category: cs.CL

TL;DR: 利用更大上下文来优化短chunk的嵌入表征，大幅提升长文本RAG检索及用例效果，所提SitEmb系列模型在多项评测中超越主流embedding模型，参数量更低但表现更强。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在处理长文本时，由于需要将其切分为小块进行检索，小块间存在依赖，常常缺乏必要的上下文来准确理解每个chunk。尽管早有工作尝试通过对更长文本编码以增强chunk信息，但提升有限。主要问题包括embedding模型面对更长chunk信息承载压力大，以及真实应用往往只需局部证据。

Method: 提出用更大上下文来条件化短文本chunk的表征（situated representation），即在embedding时将chunk的意义放在其更广泛上下文内考量。此外，设计了新型训练范式并开发了适应这一方式的嵌入模型（SitEmb）。为评估方法有效性，专门构建了考察chunk situated 表征检索能力的数据集（book-plot retrieval dataset）。

Result: SitEmb-v1（基于BGE-M3，1B参数）在新数据集和下游任务上均显著优于包括7-8B参数的大模型；SitEmb-v1.5（8B）在多语言和多下游任务下效果再提升10%以上，表现强劲。

Conclusion: 以更广上下文约束短chunk embedding的方法，相比传统直接编码长chunk的做法，在retrieval和下游结果均有明显提升，此方法更适合RAG等长文本检索任务。

Abstract: Retrieval-augmented generation (RAG) over long documents typically involves
splitting the text into smaller chunks, which serve as the basic units for
retrieval. However, due to dependencies across the original document,
contextual information is often essential for accurately interpreting each
chunk. To address this, prior work has explored encoding longer context windows
to produce embeddings for longer chunks. Despite these efforts, gains in
retrieval and downstream tasks remain limited. This is because (1) longer
chunks strain the capacity of embedding models due to the increased amount of
information they must encode, and (2) many real-world applications still
require returning localized evidence due to constraints on model or human
bandwidth.
  We propose an alternative approach to this challenge by representing short
chunks in a way that is conditioned on a broader context window to enhance
retrieval performance -- i.e., situating a chunk's meaning within its context.
We further show that existing embedding models are not well-equipped to encode
such situated context effectively, and thus introduce a new training paradigm
and develop the situated embedding models (SitEmb). To evaluate our method, we
curate a book-plot retrieval dataset specifically designed to assess situated
retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3
substantially outperforms state-of-the-art embedding models, including several
with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model
further improves performance by over 10% and shows strong results across
different languages and several downstream applications.

</details>


### [86] [TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2508.01977)
*Fan Gao,Cheng Huang,Nyima Tashi,Yutong Liu,Xiangxiang Wang,Thupten Tsering,Ban Ma-bao,Renzeg Duojie,Gadeng Luosang,Rinchen Dongrub,Dorje Tashi,Xiao Feng,Hao Wang,Yongbin Yu*

Main category: cs.CL

TL;DR: 作者自动构建了大规模藏语多领域数据集，并据此开发了具备强推理和生成能力、以藏语为核心的大模型，有效填补了藏语AI处理的资源和技术空白。


<details>
  <summary>Details</summary>
Motivation: 藏语作为一种低资源语言，数据稀缺问题严重，严重影响了相关语言处理和研究的进展。研究动机在于解决藏语大规模高质量数据集缺乏的问题，推动藏语自然语言处理能力的发展，提升多语言AI的包容性。

Method: 作者提出了一种基于链式思维（chain-of-thought, CoT）提示的大型语言模型自动化构建藏语多领域大规模数据集（TIBSTC-CoT）的框架。基于该数据集，进一步训练了具备链式思维能力、以藏语为核心的Sunshine-thinking系列大型语言模型。

Result: 基于TIBSTC-CoT训练的Sunshine-thinking LLM系列在推理和文本生成方面表现优异，其能力已接近多语种SOTA大型语言模型。该数据集和模型资源已公开发布。

Conclusion: 该研究为低资源语言尤其是藏语的数据集构建与大模型训练提供了可扩展和可复现的方法，显著推动了藏语自然语言理解和生成技术的发展，促进了AI包容性。

Abstract: To address the severe data scarcity in Tibetan, a low-resource language
spoken by over six million people, we introduce TIBSTC-CoT, the large-scale,
multi-domain Tibetan dataset automatically constructed via chain-of-thought
prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable
and reproducible framework for dataset creation in low-resource settings,
covering diverse domains and reasoning patterns essential for language
understanding and generation. Building on this dataset, we develop the
Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with
chain-of-thought capabilities. Trained entirely on TIBSTC-CoT,
Sunshine-thinking has demonstrated strong reasoning and generation performance,
comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a
significant step toward inclusive AI by enabling high-quality Tibetan language
processing through both resource creation and model innovation. All data are
available: https://github.com/Vicentvankor/sun-shine.

</details>


### [87] [Contextually Aware E-Commerce Product Question Answering using RAG](https://arxiv.org/abs/2508.01990)
*Praveen Tangarajan,Anand A. Rajasekar,Manish Rathi,Vinay Rao Dandin,Ozan Ersoy*

Main category: cs.CL

TL;DR: 文章提出了一种基于RAG的电商产品问答系统，能融合多源信息和用户上下文，提升回答相关性和个性化，同时支持内容完善和系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前电商产品页面信息丰富但杂乱，包括结构化规格、非结构化评论及个性化推荐等元素，导致用户在查找所需信息时容易出现认知过载。现有产品问答系统难以有效利用丰富的用户上下文及多样化的产品信息。

Method: 提出了一个可扩展的端到端电商产品问答框架，基于RAG（检索增强生成）技术，深度融合了用户会话历史、用户画像和产品属性，实现上下文感知回答。同时，该系统能处理多种类型的问题，并可识别产品目录中的信息缺口。还提出了新颖的评测指标用于框架性能评估。

Result: 该框架可以为客观、主观及多意图问题提供相关且个性化的回答，有效整合多来源异构数据，并能发现产品目录的内容改进空间。提出的指标也适用于更广泛的RAG系统评估。

Conclusion: 新方法使电商问答系统更具上下文感知能力，能提升用户体验，并有助于持续完善产品目录内容及问答系统本身。

Abstract: E-commerce product pages contain a mix of structured specifications,
unstructured reviews, and contextual elements like personalized offers or
regional variants. Although informative, this volume can lead to cognitive
overload, making it difficult for users to quickly and accurately find the
information they need. Existing Product Question Answering (PQA) systems often
fail to utilize rich user context and diverse product information effectively.
We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval
Augmented Generation (RAG) that deeply integrates contextual understanding. Our
system leverages conversational history, user profiles, and product attributes
to deliver relevant and personalized answers. It adeptly handles objective,
subjective, and multi-intent queries across heterogeneous sources, while also
identifying information gaps in the catalog to support ongoing content
improvement. We also introduce novel metrics to measure the framework's
performance which are broadly applicable for RAG system evaluations.

</details>


### [88] [Prompting Large Language Models to Detect Dementia Family Caregivers](https://arxiv.org/abs/2508.01999)
*Md Badsha Biswas,Özlem Uzuner*

Main category: cs.CL

TL;DR: 本研究用大语言模型和简要提示，有效区分了推特上家庭有痴呆症患者的推文，系统在相关比赛中获得了优秀表现（macro F1=0.95）。


<details>
  <summary>Details</summary>
Motivation: 社交媒体为痴呆症患者照护者分享经验和寻求支持提供了平台，但要开展网络干预，首先需要准确识别这些照护者的推文。

Method: 本研究将任务定义为二分类问题，利用多个大语言模型（LLMs）及不同提示方式，探索识别提到有家庭成员患痴呆症推文的最佳方案。

Result: 通过简单的zero-shot提示和微调模型，系统在验证集和测试集上都获得了0.95的macro F1分数。

Conclusion: 依靠LLMs及适当的提示，能够高效识别痴呆患者家庭照护者的推文，为后续网络干预和资源提供打下基础。

Abstract: Social media, such as Twitter, provides opportunities for caregivers of
dementia patients to share their experiences and seek support for a variety of
reasons. Availability of this information online also paves the way for the
development of internet-based interventions in their support. However, for this
purpose, tweets written by caregivers of dementia patients must first be
identified. This paper demonstrates our system for the SMM4H 2025 shared task
3, which focuses on detecting tweets posted by individuals who have a family
member with dementia. The task is outlined as a binary classification problem,
differentiating between tweets that mention dementia in the context of a family
member and those that do not. Our solution to this problem explores large
language models (LLMs) with various prompting methods. Our results show that a
simple zero-shot prompt on a fine-tuned model yielded the best results. Our
final system achieved a macro F1-score of 0.95 on the validation set and the
test set. Our full code is available on GitHub.

</details>


### [89] [SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents](https://arxiv.org/abs/2508.02013)
*Changhao Jiang,Jiajun Sun,Yifei Cao,Jiabao Zhuang,Hui Li,Xiaoran Fan,Ming Zhang,Junjie Ye,Shihan Dou,Zhiheng Xi,Jingqi Tong,Yilong Wu,Baoyu Fan,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 论文提出了大规模高质量的语音角色扮演数据集和多维度评测基准，分析了主流方法的优势与挑战，并公开相关资源，推动了多模态语音角色扮演研究。


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演智能体多聚焦于文本模式，忽略了真实交互场景中语音的重要性，且缺乏对语音角色扮演代理（SRPA）的系统性评测。为填补该领域的研究空白，论文提出新的研究和工具。

Method: 1）构建了SpeechRole-Data数据集，包含98类不同的角色和11.2万条基于语音的单轮及多轮对话，每个角色具有独特的嗓音特性。2）提出SpeechRole-Eval多维评测基准，从互动能力、语音表现力、角色扮演一致性等方面系统评估SRPA表现。3）实验比较了级联和端到端SRPA的优缺点。

Result: 实验揭示了级联与端到端SRPA在维持嗓音风格一致性和角色连贯性方面的优势与挑战。论文公开了数据、代码和基线模型，为语音驱动的多模态角色扮演研究奠定了基础。

Conclusion: 该研究系统填补了语音角色扮演代理评测和数据集的缺口，为今后的多模态角色扮演智能体研究和应用提供了重要资源和工具支持。

Abstract: Recently, role-playing agents have emerged as a promising paradigm for
achieving personalized interaction and emotional resonance. Existing research
primarily focuses on the textual modality, neglecting the critical dimension of
speech in realistic interactive scenarios. In particular, there is a lack of
systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this
gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that
comprises 98 diverse roles and 112k speech-based single-turn and multi-turn
conversations. Each role demonstrates distinct vocal characteristics, including
timbre and prosody, thereby enabling more sophisticated speech role-playing.
Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation
benchmark that systematically assesses SRPAs performance in key aspects such as
fundamental interaction ability, speech expressiveness, and role-playing
fidelity. Experimental results reveal the advantages and challenges of both
cascaded and end-to-end speech role-playing agents in maintaining vocal style
consistency and role coherence. We release all data, code, and baseline models
to provide a solid foundation for speech-driven multimodal role-playing
research and to foster further developments in this field.

</details>


### [90] [SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2508.02018)
*Wanqi Yang,Yanda Li,Yunchao Wei,Meng Fang,Ling Chen*

Main category: cs.CL

TL;DR: 作者提出SpeechR基准，系统评测LALM语音推理能力。结果显示，转录强不等于推理强。SpeechR推动更精准的模型能力分析。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频-语言模型（LALMs）在转录和情感识别方面表现接近人类，但对其在语音场景下的推理能力评估不足，主要关注表层感知，缺乏对上下文和推理驱动能力的系统分析。该论文旨在弥补这一评测空白。

Method: 提出了一个新的基准SpeechR，用于针对语音推理能力评测LALMs。SpeechR涵盖三大核心维度：事实检索、程序性推理和规范判断，并设计了多项评测格式：选择题、生成题和声学特征分析。通过这三种格式全面评估模型在不同层次和维度下的推理能力。

Result: 在11种最先进LALMs上的实验结果显示：虽然转录准确率高，但这些模型的推理能力不强。尤其在事实检索、推理过程连贯性以及受声学特征影响的推理能力上，表现均有短板。

Conclusion: SpeechR为评估音频-语言大模型的推理能力提供了统一且结构化的基准，为随后模型改进和定向分析提供了依据。结果表明，当前模型在推理能力上与转录或表层任务存在明显差距。

Abstract: Large audio-language models (LALMs) have achieved near-human performance in
sentence-level transcription and emotion recognition. However, existing
evaluations focus mainly on surface-level perception, leaving the capacity of
models for contextual and inference-driven reasoning in speech-based scenarios
insufficiently examined. To address this gap, we introduce SpeechR, a unified
benchmark for evaluating reasoning over speech in large audio-language models.
SpeechR evaluates models along three key dimensions: factual retrieval,
procedural inference, and normative judgment. It includes three distinct
evaluation formats. The multiple-choice version measures answer selection
accuracy. The generative version assesses the coherence and logical consistency
of reasoning chains. The acoustic-feature version investigates whether
variations in stress and emotion affect reasoning performance. Evaluations on
eleven state-of-the-art LALMs reveal that high transcription accuracy does not
translate into strong reasoning capabilities. SpeechR establishes a structured
benchmark for evaluating reasoning in spoken language, enabling more targeted
analysis of model capabilities across diverse dialogue-based tasks.

</details>


### [91] [Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time](https://arxiv.org/abs/2508.02037)
*Huihan Li,You Chen,Siyuan Wang,Yixin He,Ninareh Mehrabi,Rahul Gupta,Xiang Ren*

Main category: cs.CL

TL;DR: 本文提出STIM框架，通过token级分析揭示大语言模型推理中依赖记忆的特征，辨识本地记忆主导错误，并为推理链中错误token的预测提供方法，助力模型推理能力的诊断与改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在推理任务上的表现优异，但当输入内容稍有变化时容易失败，这引发了关于这些模型是否过度依赖记忆的担忧。尤其是在链式思维（CoT）推理中，模型可能因记忆到的虚假模式而导致推理链中间部分出错，继而产生错误的最终答案。

Method: 本文提出了一种新颖的方法STIM（源感知token级记忆性识别框架），能将推理链中的每个token根据与预训练语料中token的统计共现关联，归因于本地、中程或远程三类记忆源，实现精细的token级记忆性分析。

Result: 分析显示，模型在复杂或长尾任务案例中更加依赖记忆，并且本地记忆通常是错误产生的主要来源，最高可导致67%的token错误。STIM框架的记忆评分可以有效预测推理链中错误的token。

Conclusion: STIM为诊断和提升模型推理能力提供了有力工具，并能推广应用到其他结构化逐步生成任务。

Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often
fail when inputs alter slightly, raising concerns about the extent to which
their success relies on memorization. This issue is especially acute in
Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger
intermediate errors that cascade into incorrect final answers. We introduce
STIM, a novel framework for Source-aware Token-level Identification of
Memorization, which attributes each token in a reasoning chain to one of
multiple memorization sources - local, mid-range, or long-range - based on
their statistical co-occurrence with the token in the pretraining corpus. Our
token-level analysis across tasks and distributional settings reveals that
models rely more on memorization in complex or long-tail cases, and that local
memorization is often the dominant driver of errors, leading to up to 67% of
wrong tokens. We also show that memorization scores from STIM can be effective
in predicting the wrong tokens in the wrong reasoning step. STIM offers a
powerful tool for diagnosing and improving model reasoning and can generalize
to other structured step-wise generation tasks.

</details>


### [92] [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038)
*Fengping Tian,Chenyang Lyu,Xuanfan Ni,Haoqin Sun,Qingjuan Li,Zhiqiang Qian,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出一种兼具语音克隆和情感控制能力的合成系统，通过创新的解耦方法和高质量数据集，取得了语音自然度和情感表达的显著提升，对表达性神经语音合成具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 目前语音合成在实现高度表达性、可控性以及在多样的语言和情感环境下精确保留说话人身份方面存在挑战。现有系统难以实现说话人和情感的独立调控，限制了语音合成的自然度和应用范围。

Method: 本文提出了一种多功能语音合成系统，将语音克隆与情感控制语音合成结合于一个统一框架中。核心方法包括基于batch内对比学习的说话人与情感解耦机制，以及旋转情感嵌入整合方式，实现说话人身份和情感风格的独立操控与平滑控制。同时，作者还构建了CSEMOTIONS高质量的中文情感语音数据集供系统训练和评测。

Result: 实验结果显示所提出的Marco-Voice系统在客观和主观指标上均有显著提升，语音清晰度和情感丰富性表现突出，在表达性神经语音合成领域取得了竞争性的性能。

Conclusion: Marco-Voice系统通过结合创新的情感与说话人解耦机制以及数据集支持，实现了高自然度、高表达力、可控性强的语音合成，是该领域的重要进展。

Abstract: This paper presents a multifunctional speech synthesis system that integrates
voice cloning and emotion control speech synthesis within a unified framework.
The goal of this work is to address longstanding challenges in achieving highly
expressive, controllable, and natural speech generation that faithfully
preserves speaker identity across diverse linguistic and emotional contexts.
Our approach introduces an effective speaker-emotion disentanglement mechanism
with in-batch contrastive learning, enabling independent manipulation of
speaker identity and eemotional style, as well as rotational emotional
embedding integration method for smooth emotion control. To support
comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality
emotional speech dataset containing 10 hours of Mandarin speech from six
professional speakers across seven emotional categories. Extensive experiments
demonstrate that our system, Marco-Voice, achieves substantial improvements in
both objective and subjective metrics. Comprehensive evaluations and analysis
were conducted, results show that MarcoVoice delivers competitive performance
in terms of speech clarity and emotional richness, representing a substantial
advance in the field of expressive neural speech synthesis.

</details>


### [93] [Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models](https://arxiv.org/abs/2508.02045)
*Soyeon Kim,Jindong Wang,Xing Xie,Steven Euijong Whang*

Main category: cs.CL

TL;DR: 本文提出TDBench，通过自动化的数据库技术生成时间敏感问答数据集，结合新颖的评价指标，大幅提升了大语言模型在该领域的评估效率与可靠性，降低了人工成本。


<details>
  <summary>Details</summary>
Motivation: 随着事实随时间变化，当前用于时间敏感型问答任务（TSQA）的评估基准依赖人工整理或模板，难以规模化且不够全面，需要更高效、系统的评估方法。

Method: 提出了TDBench基准，利用时序数据库和数据库技术（如时序SQL和函数依赖）系统性地自动生成TSQA问题。同时引入细粒度的“时间准确率”指标，结合传统答案准确率来全面评估模型对时间信息的把握。

Result: 基于TDBench对多种主流大语言模型（LLMs）开展实验，结果显示TDBench能更全面和自动化地衡量模型在时间敏感型问答上的性能，减少对人工的依赖，并能与现有基于维基百科/维基数据的方法互补，应用于特定领域的数据和复杂问题生成。

Conclusion: TDBench为时间敏感型问答的模型评估带来更高效、可扩展和更可信的标准，并丰富了LLMs的实际应用场景评测方式。

Abstract: Facts evolve over time, making it essential for Large Language Models (LLMs)
to handle time-sensitive factual knowledge accurately and reliably. While
factual Time-Sensitive Question-Answering (TSQA) tasks have been widely
studied, existing benchmarks often rely on manual curation or a small, fixed
set of predefined templates, which restricts scalable and comprehensive TSQA
evaluation. To address these challenges, we propose TDBench, a new benchmark
that systematically constructs TSQA pairs by harnessing temporal databases and
database techniques such as temporal SQL and functional dependencies. We also
introduce a fine-grained evaluation metric called time accuracy, which assesses
the validity of time references in model explanations alongside traditional
answer accuracy to enable a more reliable TSQA evaluation. Extensive
experiments on contemporary LLMs show how \ours{} enables scalable and
comprehensive TSQA evaluation while reducing the reliance on human labor,
complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by
enabling LLM evaluation on application-specific data and seamless multi-hop
question generation. Code and data are publicly available at:
https://github.com/ssoy0701/tdbench.git.

</details>


### [94] [ProCut: LLM Prompt Compression via Attribution Estimation](https://arxiv.org/abs/2508.02053)
*Zhentao Xu,Fengyi Li,Albert Chen,Xiaofeng Wang*

Main category: cs.CL

TL;DR: ProCut通过归因分析对大规模prompt进行无损压缩，大幅减少token用量，提升工业LLM系统效率与表现。


<details>
  <summary>Details</summary>
Motivation: 在大规模工业级LLM系统中，随着任务指令、few-shot示例和启发性规则的不断增加，prompt模板通常会变得非常庞大，难以维护且推理代价高昂。因此，亟需一种能够减少prompt体积且不损耗性能的方法。

Method: 本文提出了一种名为 ProCut 的方法，通过归因分析将prompt划分为语义相关单元，量化每一部分对任务性能的贡献，并剪除低效成分，实现prompt的压缩。该框架具有灵活、模型无关、无需训练等优点。还提出了一种基于LLM驱动的归因估算器，进一步降低了压缩延迟。

Result: 在五个公开基准数据集及实际工业prompt上的大量实验显示，ProCut能在不影响甚至轻微提升任务性能的前提下，实现高达78%的prompt token压缩率，任务表现最高比其他方法提升62%。LLM驱动归因估算器将压缩延迟降低超过50%。

Conclusion: ProCut框架能够有效、无损或提升任务性能地压缩冗余prompt，降低运算及维护成本，与现有的prompt优化框架高度兼容，有望在工业化LLM应用落地。

Abstract: In large-scale industrial LLM systems, prompt templates often expand to
thousands of tokens as teams iteratively incorporate sections such as task
instructions, few-shot examples, and heuristic rules to enhance robustness and
coverage. This expansion leads to bloated prompts that are difficult to
maintain and incur significant inference latency and serving costs. To address
this, we introduce Prompt Compression via Attribution Estimation (ProCut), a
flexible, LLM-agnostic, training-free framework that compresses prompts through
attribution analysis. ProCut segments prompt templates into semantically
meaningful units, quantifies their impact on task performance, and prunes
low-utility components. Through extensive experiments on five public benchmark
datasets and real-world industrial prompts, we show that ProCut achieves
substantial prompt size reductions (78% fewer tokens in production) while
maintaining or even slightly improving task performance (up to 62% better than
alternative methods). We further introduce an LLM-driven attribution estimator
that reduces compression latency by over 50%, and demonstrate that ProCut
integrates seamlessly with existing prompt-optimization frameworks to produce
concise, high-performing prompts.

</details>


### [95] [The SMeL Test: A simple benchmark for media literacy in language models](https://arxiv.org/abs/2508.02074)
*Gustaf Ahdritz,Anat Kleiman*

Main category: cs.CL

TL;DR: 作者提出SMeL Test评估大模型筛选不可信信息的能力，发现现有模型在此方面表现有限且幻觉率高，模型规模大未必表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前互联网上充斥着大量不可信或误导性内容，而尚不清楚大型语言模型在自主网页浏览时，能否像人类一样通过简单的启发式方法筛选信息。

Method: 本文提出了Synthetic Media Literacy Test (SMeL Test)，这是一个最小化的基准测试，用于评估语言模型在上下文中过滤不可信信息的能力，并对多种主流指令调优大语言模型进行了基准评测。

Result: 无一模型能始终如一地信任更可靠的信息源，推理能力较强的模型分数较高，但最佳API模型仍高达70%的幻觉率（错误生成）。更大、能力更强的模型并不必然优于更小的模型。

Conclusion: 当前的大型语言模型在过滤不可信信息方面表现不佳，幻觉问题突出，且模型规模与能力并非决定性因素。作者希望本研究能为理解并改进相关幻觉问题提供参考。

Abstract: The internet is rife with unattributed, deliberately misleading, or otherwise
untrustworthy content. Though large language models (LLMs) are often tasked
with autonomous web browsing, the extent to which they have learned the simple
heuristics human researchers use to navigate this noisy environment is not
currently known. In this paper, we introduce the Synthetic Media Literacy Test
(SMeL Test), a minimal benchmark that tests the ability of language models to
actively filter out untrustworthy information in context. We benchmark a
variety of commonly used instruction-tuned LLMs, including reasoning models,
and find that no model consistently trusts more reliable sources; while
reasoning in particular is associated with higher scores, even the best API
model we test hallucinates up to 70% of the time. Remarkably, larger and more
capable models do not necessarily outperform their smaller counterparts. We
hope our work sheds more light on this important form of hallucination and
guides the development of new methods to combat it.

</details>


### [96] [When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models](https://arxiv.org/abs/2508.02087)
*Jin Li,Keyu Wang,Shu Yang,Zhuoran Zhang,Di Wang*

Main category: cs.CL

TL;DR: 本文系统分析了大语言模型迎合用户观点的内在机制，发现奉承行为源自模型深层结构的知识覆盖，与表述视角、输出层表现密切相关，并提出其对AI对齐和真实性的重要影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在奉承行为，即模型会迎合用户观点，即便这些观点与事实相矛盾。此前有文献指出这一现象，但其内部机制尚未深入解析。该论文旨在揭示LLMs奉承行为产生的机制。

Method: 系统性研究了用户观点对不同模型家族的奉承行为诱导，通过logit-lens分析和因果激活修补法揭示模型内部表现的变化。同时分析了用户权威性及语法视角（如第一人称与第三人称）对奉承行为的影响。

Result: 1）简单的用户观点陈述会稳定诱导奉承行为，用户自身权威性影响极小；2）奉承行为的产生经历两阶段：晚期层输出偏好转变和更深层表征分歧；3）模型内部并未编码用户权威，导致权威性难以影响模型行为；4）第一人称表述会比第三人称显著提升奉承发生率，因为前者对深层表征扰动更强。

Conclusion: 奉承行为并非表层现象，而是模型深层知识结构被覆盖的结果。这一发现对模型对齐和事实性AI系统带来警示和启示。

Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing
with user-stated opinions even when those contradict factual knowledge. While
prior work has documented this tendency, the internal mechanisms that enable
such behavior remain poorly understood. In this paper, we provide a mechanistic
account of how sycophancy arises within LLMs. We first systematically study how
user opinions induce sycophancy across different model families. We find that
simple opinion statements reliably induce sycophancy, whereas user expertise
framing has a negligible impact. Through logit-lens analysis and causal
activation patching, we identify a two-stage emergence of sycophancy: (1) a
late-layer output preference shift and (2) deeper representational divergence.
We also verify that user authority fails to influence behavior because models
do not encode it internally. In addition, we examine how grammatical
perspective affects sycophantic behavior, finding that first-person prompts
(``I believe...'') consistently induce higher sycophancy rates than
third-person framings (``They believe...'') by creating stronger
representational perturbations in deeper layers. These findings highlight that
sycophancy is not a surface-level artifact but emerges from a structural
override of learned knowledge in deeper layers, with implications for alignment
and truthful AI systems.

</details>


### [97] ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)
*Yaqiong Li,Peng Zhang,Lin Wang,Hansu Gu,Siyuan Qiao,Ning Gu,Tun Lu*

Main category: cs.CL

TL;DR: 本研究聚焦青少年和成年人对网络毒性内容感知差异，搭建中文青少年毒性数据集，分析其语言特征，并发现结合语境信息能显著提升毒性检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往有毒内容检测多聚焦于成年人的感知，忽略了青少年特有的，有些成年人认为无害但青少年视为有毒的语言内容。该研究旨在填补该空白并揭示青少年毒性语言特征与检测难点。

Method: 针对中国青少年，构建了首个中文“青少年毒性”数据集，并对语言特征与现有检测方法展开了深入分析。

Result: 发现青少年感知到的毒性与语境因素（如话语来源、文本特征）密切相关。在现有检测方法中，加入这些语境元信息可显著提升对青少年毒性语言的检测效果。

Conclusion: 将元信息（如话语来源和文本相关特征）整合进现有有毒内容检测方法后，检测针对青少年的“青少年毒性”语言的准确率显著提升。

Abstract: Risk perception is subjective, and youth's understanding of toxic content
differs from that of adults. Although previous research has conducted extensive
studies on toxicity detection in social media, the investigation of youth's
unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as
youth, is ignored. To address this gap, we aim to explore: 1) What are the
features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing
toxicity detection techniques accurately detect these languages (RQ2). For
these questions, we took Chinese youth as the research target, constructed the
first Chinese ``youth-toxicity'' dataset, and then conducted extensive
analysis. Our results suggest that youth's perception of these is associated
with several contextual factors, like the source of an utterance and
text-related features. Incorporating these meta information into current
toxicity detection methods significantly improves accuracy overall. Finally, we
propose several insights into future research on youth-centered toxicity
detection.

</details>


### [98] [Learning Dynamics of Meta-Learning in Small Model Pretraining](https://arxiv.org/abs/2508.02189)
*David Demitri Africa,Yuval Weiss,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 将元学习与小型语言模型预训练结合，提升训练速度、表现及可解释性，为小模型高效发展提供新探索。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但预训练成本高昂。研究动机在于探索元学习(meta-learning)是否能让小语言模型的预训练变得更高效且更易解释。

Method: 整合一阶MAML元学习算法与子集掩码语言模型预训练，构建了四个LLama风格的解码器模型（参数从1100万到5.7亿），并在一个具有多种设置和实际应用的基础NLP任务上进行了评估。

Result: 实验发现：1）模型达到相同loss所需时间比传统训练快至1.6倍；2）在相同计算资源下，多语种通用NER任务F1提升；3）训练过程中网络表现阶段性变化：先“多样化”再“压缩”，可清晰观测训练动力学，并能定位早期和后期特化的模型层。

Conclusion: 本文提出的方法不仅提升了小语言模型的训练效率和下游任务表现，还带来了训练过程可解释性的提升，提供了识别和理解元适应过程的计算特征。

Abstract: Large language models are powerful but costly. We ask whether meta-learning
can make the pretraining of small language models not only better but also more
interpretable. We integrate first-order MAML with subset-masked LM pretraining,
producing four LLama-style decoder-only models (11M-570M params), and evaluate
it on a fundamental NLP task with many settings and real-world applications.
Compared with vanilla training, our model (i) reaches the same loss up to 1.6x
sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and
(iii) makes the training dynamics easy to read: first the network's
representations fan out ("diversify") and later they collapse into a smaller,
shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall
in both effective-rank curves and attention-head entropy. The same curves
pinpoint which layers specialise earliest and which later reconverge, giving a
compact, interpretable signature of meta-adaptation. Code, checkpoints and
WandB logs are released.

</details>


### [99] [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference](https://arxiv.org/abs/2508.02193)
*Yuxuan Song,Zheng Zhang,Cheng Luo,Pengyang Gao,Fan Xia,Hao Luo,Zheng Li,Yuehang Yang,Hongli Yu,Xingwei Qu,Yuwei Fu,Jing Su,Ge Zhang,Wenhao Huang,Mingxuan Wang,Lin Yan,Xiaoying Jia,Jingjing Liu,Wei-Ying Ma,Ya-Qin Zhang,Yonghui Wu,Hao Zhou*

Main category: cs.CL

TL;DR: Seed Diffusion Preview是一种基于离散扩散的高效大模型，在代码生成任务上以极快的推理速度领先同类方法，并保持优异质量。


<details>
  <summary>Details</summary>
Motivation: 当前的大模型推理速度较慢，主要由于逐token生成方式导致延迟，尤其是在代码生成等场景下影响显著。之前如Mercury Coder、Gemini Diffusion等模型尝试通过离散扩散模型改进速度。

Method: 提出了一种基于离散状态扩散的语言模型Seed Diffusion Preview，通过非序列化、并行生成方式进行推理，有效加速了生成过程。

Result: Seed Diffusion Preview在H20 GPU上实现了2146 token/s的推理速度，同时在多个代码评测基准上保持了与现有模型相当的性能，推理速度远超Mercury和Gemini Diffusion，达到了代码模型速度-质量帕累托前沿的新水平。

Conclusion: Seed Diffusion Preview显著提升了大语言模型的推理速度，同时保持了竞争性的任务性能，在代码相关任务上树立了新的速度与质量的标杆。

Abstract: We present Seed Diffusion Preview, a large-scale language model based on
discrete-state diffusion, offering remarkably fast inference speed. Thanks to
non-sequential, parallel generation, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of token-by-token decoding, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over H20 GPUs while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.

</details>


### [100] [Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems](https://arxiv.org/abs/2508.02208)
*Yebo Peng,Zixiang Liu,Yaoming Li,Zhizhuo Yang,Xinye Xu,Bowen Ye,Weijun Yuan,Zihan Wang,Tong Yang*

Main category: cs.CL

TL;DR: 本文提出了Proof2Hybrid全自动数学推理能力评测框架，通过Proof2X路线图和创新题型，有效解决了手工出题难题，并用AlgGeoTest首次系统检验了主流大模型在代数几何领域的真实能力，揭示了当前LLM的重大短板，为后续AI数学智能研究提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在处理数学问题，特别是证明类问题时，现有的评测基准难以全面、公正地评价其能力，这是因为手工制作高质量证明题集既费时又不易扩展，致使大模型的真实数学能力被低估或未被充分测试。

Method: 提出Proof2Hybrid框架，该框架能够全自动地从自然语言数学语料中生成高质量的、以证明为核心的测试集。核心创新点是Proof2X路线图，可将数学证明转换为多种易于验证的问题。此外，设计了新型的“m选n多裁判问题”题型，使评测更鲁棒，能有效防止模型通过猜测或表面模式识别来得分。以该框架自动生成了包含456道高难度题目的代数几何测试集（AlgGeoTest）作为示范。

Result: 应用该框架及AlgGeoTest，对多种主流LLM进行评测，发现它们在代数几何理解上能力有限，暴露出以往评测未能展现的深层能力缺陷。新设计的基准测试方法更准确地衡量了AI在现代高端数学领域的实际实力。

Conclusion: Proof2Hybrid为自动生成高质量、以证明为核心的数学评测集提供了可扩展方案，促进了对AI数学智能的深入探索。提出的新题型和基准能够推动AI数学能力评估进入新阶段。

Abstract: Evaluating the mathematical capability of Large Language Models (LLMs) is a
critical yet challenging frontier. Existing benchmarks fall short, particularly
for proof-centric problems, as manual creation is unscalable and costly,
leaving the true mathematical abilities of LLMs largely unassessed. To overcome
these barriers, we propose Proof2Hybrid, the first fully automated framework
that synthesizes high-quality, proof-centric benchmarks from natural language
mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of
converting mathematical proofs into various kinds of questions that are easy to
verify. Instructed by this roadmap, we propose a new type of hybrid-formatted
questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically
designed to enable robust, automatic evaluation while being resilient to
guessing and superficial pattern matching inherent in traditional formats. As a
demonstration of our framework, we introduce AlgGeoTest, a benchmark for
algebraic geometry--a frontier domain of modern mathematics--comprising 456
challenging items. Our extensive evaluations on state-of-the-art LLMs using
AlgGeoTest reveal profound deficits in their comprehension of algebraic
geometry, providing a more precise measure of their true mathematical
capabilities. Our framework and benchmark pave the way for a new wave of
in-depth research into the mathematical intelligence of AI systems.

</details>


### [101] [Isolating Culture Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2508.02241)
*Danial Namazifard,Lukas Galke*

Main category: cs.CL

TL;DR: 本文提出一种能够识别和定位多语言大模型中文化特定神经元的方法，并构建了涵盖六种文化的大型数据集。实验表明，文化神经元具有独立性，并主要分布于模型高层，可被单独编辑。这为推动模型公平、包容和对齐提供了可能。


<details>
  <summary>Details</summary>
Motivation: 尽管语言与文化密不可分，但目前尚不清楚多语言大模型是如何、以及在何处编码文化信息的，因此希望揭示模型中文化有关的表征机制，为模型公平性、包容性和对齐操作提供理论基础。

Method: 在已有的语言特定神经元定位方法基础上，提出并扩展出文化特定神经元的定位和干预方法。引入并构建MUREL数据集，涵盖六种文化，利用神经元定位、交互分析和干预实验，分析文化和语言神经元的关系。

Result: 实验发现，LLM在特定神经元群集中编码不同文化，这些文化神经元主要分布在模型的上层，并且能与语言神经元区分并独立调控。实验还表明，不同文化的神经元可以独立于其他文化或语言神经元进行调制和编辑。

Conclusion: 多语言大模型中的文化知识编码在神经元层面上与语言知识有一定的独立性，且不同文化的神经元多集中在模型上层，可被独立调控和编辑。

Abstract: Language and culture are deeply intertwined, yet it is so far unclear how and
where multilingual large language models encode culture. Here, we extend upon
an established methodology for identifying language-specific neurons and extend
it to localize and isolate culture-specific neurons, carefully disentangling
their overlap and interaction with language-specific neurons. To facilitate our
experiments, we introduce MUREL, a curated dataset of 85.2 million tokens
spanning six different cultures. Our localization and intervention experiments
show that LLMs encode different cultures in distinct neuron populations,
predominantly in upper layers, and that these culture neurons can be modulated
independently from language-specific neurons or those specific to other
cultures. These findings suggest that cultural knowledge and propensities in
multilingual language models can be selectively isolated and edited - promoting
fairness, inclusivity, and alignment. Code and data is available at
https://github.com/namazifard/Culture_Neurons .

</details>


### [102] [Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders](https://arxiv.org/abs/2508.02256)
*Belen Alastruey,João Maria Janeiro,Alexandre Allauzen,Maha Elbayad,Loïc Barrault,Marta R. Costa-jussà*

Main category: cs.CL

TL;DR: 本文系统评估了83种语言的BERT编码模型语言干扰，提出干扰与文字系统关系最大，且可用干扰矩阵预测下游任务，从而指导多语言模型更优设计。


<details>
  <summary>Details</summary>
Motivation: 研究多语言编码模型中，语言之间可能存在的相互干扰问题，并希望揭示这种干扰背后的模式与影响因素。

Method: 构造语言干扰矩阵，通过训练和评估BERT类模型在83种语言的所有语言对上的表现，系统评估交叉语言干扰。并分析这些干扰模式与语言家族、向量相似性以及文字系统等特征的关系。

Result: 发现不同语言间的干扰是非对称的，这些干扰模式既不与传统语言学特征（如语言家族）一致，也不与向量相似性一致，而是与文字系统（script）关系最密切。同时，干扰矩阵能有效预测下游任务表现。

Conclusion: 语言干扰矩阵为多语言模型设计提供了有力工具，有助于实现最佳模型性能。传统特征无法充分解释干扰，文字系统因素至关重要。

Abstract: In this paper, we present a comprehensive study of language interference in
encoder-only Transformer models across 83 languages. We construct an
interference matrix by training and evaluating small BERT-like models on all
possible language pairs, providing a large-scale quantification of
cross-lingual interference. Our analysis reveals that interference between
languages is asymmetrical and that its patterns do not align with traditional
linguistic characteristics, such as language family, nor with proxies like
embedding similarity, but instead better relate to script. Finally, we
demonstrate that the interference matrix effectively predicts performance on
downstream tasks, serving as a tool to better design multilingual models to
obtain optimal performance.

</details>


### [103] [Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning](https://arxiv.org/abs/2508.02260)
*Jia Deng,Jie Chen,Zhipeng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文针对LLM推理强化中的熵-性能权衡，深入分析了其在不同训练阶段和层级下的表现机制，并提出基于困惑度与位置信息的动态奖励两种新方法，在多个模型上取得提升。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型（LLM）的推理能力提升中，使用带有可验证奖励的强化学习（RLVR）越来越多，但其中熵与性能之间的权衡机制尚不清晰。因此，作者希望深入理解RLVR中的熵-性能交换机制，以优化训练效果。

Method: 本文通过系统的实证分析，分阶段（熵上升阶段与平台阶段）以及不同粒度（阶段、实例、Token级）探索RLVR的熵-性能交换规律。基于分析结果，作者提出了两种新方法，结合困惑度和位置等信息，动态调整奖励信号以更有效地进行RL更新。

Result: 发现于熵上升阶段，对负样本的熵降低有助于推理模式学习，从而提高表现。在平台阶段，学习效率与低困惑度样本中的高熵Token，以及序列结尾处的高熵Token关系密切。两种提出的新方法在多个LLM上相比基线模型取得了性能提升。

Conclusion: RLVR训练过程中，熵与性能间的动态关系对强化推理能力至关重要，通过粒度化分析和有针对性的奖励调整可以显著改善LLM性能。

Abstract: Recently, reinforcement learning with verifiable rewards (RLVR) has been
widely used for enhancing the reasoning abilities of large language models
(LLMs). A core challenge in RLVR involves managing the exchange between entropy
and performance of policies. Despite the importance of this exchange, a
fine-grained understanding of when and how this exchange operates most
effectively remains limited. To bridge this gap, we conduct a systematic
empirical analysis of the entropy-performance exchange mechanism of RLVR across
different levels of granularity. Specifically, we first divide the training
process into two distinct stages based on entropy dynamics, i.e., rising stage
and plateau stage, and then systematically investigate how this mechanism
varies across stage-level, instance-level, and token-level granularitiess. Our
analysis reveals that, in the rising stage, entropy reduction in negative
samples facilitates the learning of effective reasoning patterns, which in turn
drives rapid performance gains. Moreover, in the plateau stage, learning
efficiency strongly correlates with high-entropy tokens present in
low-perplexity samples and those located at the end of sequences. Motivated by
these findings, we propose two methods that dynamically adjust the reward
signal using perplexity and positional information to focus RL updates on
tokens that exhibit high learning potential, achieving improvements compared to
the baseline methods on various LLMs.

</details>


### [104] [SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System](https://arxiv.org/abs/2508.02268)
*Serry Sibaee,Omer Nacar,Yasser Al-Habashi,Adel Ammar,Wadii Boulila*

Main category: cs.CL

TL;DR: 本文解决了阿拉伯世界MSA与叙利亚方言之间机器翻译难题，提出了SHAMI-MT翻译系统，在公开评测中展现出高质量评分，具有显著学术与实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界存在着现代标准阿拉伯语（MSA）与地方方言之间的巨大差距，这种双言现象给自然语言处理、尤其是机器翻译任务带来了极大挑战。针对MSA与叙利亚（Shami）方言之间缺乏高质量翻译工具的问题，本文提出了新的解决方案。

Method: 提出了一套名为SHAMI-MT的双向机器翻译系统，设计了MSA到Shami及Shami到MSA两个独立模型。两个模型均基于先进的AraT5v2-base-1024结构，利用Nabra数据集进行微调，并在MADAR语料库未见过的数据上严格评估。

Result: MSA到Shami的模型在GPT-4.1的评判下获得了平均4.01/5.0的翻译质量评分，显示出卓越的准确性和方言原真性。

Conclusion: 本文开发的SHAMI-MT为此前被忽视的MSA与叙利亚方言语言对提供了高保真的机器翻译工具，推动了阿拉伯语方言翻译领域的发展，并在内容本地化、文化遗产和跨文化交流等方面具有重要应用价值。

Abstract: The rich linguistic landscape of the Arab world is characterized by a
significant gap between Modern Standard Arabic (MSA), the language of formal
communication, and the diverse regional dialects used in everyday life. This
diglossia presents a formidable challenge for natural language processing,
particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a
bidirectional machine translation system specifically engineered to bridge the
communication gap between MSA and the Syrian dialect. We present two
specialized models, one for MSA-to-Shami and another for Shami-to-MSA
translation, both built upon the state-of-the-art AraT5v2-base-1024
architecture. The models were fine-tuned on the comprehensive Nabra dataset and
rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami
model achieved an outstanding average quality score of \textbf{4.01 out of 5.0}
when judged by OPENAI model GPT-4.1, demonstrating its ability to produce
translations that are not only accurate but also dialectally authentic. This
work provides a crucial, high-fidelity tool for a previously underserved
language pair, advancing the field of dialectal Arabic translation and offering
significant applications in content localization, cultural heritage, and
intercultural communication.

</details>


### [105] [Dynaword: From One-shot to Continuously Developed Datasets](https://arxiv.org/abs/2508.02271)
*Kenneth Enevoldsen,Kristian Nørgaard Jensen,Jan Kostkan,Balázs Szabó,Márton Kardos,Kirten Vad,Andrea Blasi Núñez,Gianluca Barmina,Jacob Nielsen,Rasmus Larsen,Peter Vahlstrup,Per Møldrup Dalum,Desmond Elliott,Lukas Galke,Peter Schneider-Kamp,Kristoffer Nielbo*

Main category: cs.CL

TL;DR: 提出了一种开放、可持续发展的数据集构建方法（Dynaword），并以丹麦语数据集为例验证了其实用性，解决了传统数据集在许可、维护和质量保障上的三大难题。


<details>
  <summary>Details</summary>
Motivation: 目前自然语言处理领域的大规模数据集存在以下三大问题：（1）依赖于许可不明确的数据源，限制了使用、共享和衍生作品的权利；（2）数据集以静态方式发布，不利于社区持续贡献和数据集的长久维护；（3）质量保障流程仅限于发布团队，未能充分利用社区的集体智慧。

Method: 本文提出了Dynaword方法和具体实现Danish Dynaword。Dynaword是一种支持社区持续协作，能不断扩展和更新的大规模开放数据集构建框架。Danish Dynaword则是这一方法在丹麦语数据集上的实践，体现了其实用性。

Result: Danish Dynaword的数据规模相比于现有同类数据集提升了4倍，全部采用开放许可证，并吸引了业界和学界的多方贡献。其数据仓库内含轻量级的数据格式、质量和文档测试，构建了可持续的社区协作与数据集演化机制。

Conclusion: Dynaword方法及Danish Dynaword项目展示了如何通过开放、持续更新和社区协作的方式构建大规模高质量数据集，为自然语言处理数据资源的管理和发展提供了新的解决思路。

Abstract: Large-scale datasets are foundational for research and development in natural
language processing. However, current approaches face three key challenges: (1)
reliance on ambiguously licensed sources restricting use, sharing, and
derivative works; (2) static dataset releases that prevent community
contributions and diminish longevity; and (3) quality assurance processes
restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword
approach and Danish Dynaword. The Dynaword approach is a framework for creating
large-scale, open datasets that can be continuously updated through community
collaboration. Danish Dynaword is a concrete implementation that validates this
approach and demonstrates its potential. Danish Dynaword contains over four
times as many tokens as comparable releases, is exclusively openly licensed,
and has received multiple contributions across industry and research. The
repository includes light-weight tests to ensure data formatting, quality, and
documentation, establishing a sustainable framework for ongoing community
contributions and dataset evolution.

</details>


### [106] [A French Version of the OLDI Seed Corpus](https://arxiv.org/abs/2508.02290)
*Malik Marmonier,Benoît Sagot,Rachel Bawden*

Main category: cs.CL

TL;DR: 本文提出并详述了OLDI Seed Corpus的首个法语分区，采用多重机器翻译与人工后编辑结合，兼顾技术与非规范文本难点，为推动法国低资源语言的数据建设提供关键支持。


<details>
  <summary>Details</summary>
Motivation: 开发面向多语资源稀缺语言的高质量语料库，特别关注法国地区语言的并行语料需求。

Method: 采用多台机器翻译系统，以及由合格母语者参与的自定义后编辑界面来构建法语分区，并处理技术性和非标准风格内容。

Result: 创建了首个法语分区OLDI Seed Corpus，专门描述了构建过程中遇到的技术术语和维基百科用户生成内容的特殊挑战。

Conclusion: 该法语语料库主要作为促进法国本土低资源地区语言平行语料收集的关键中介资源，对相关任务具有支持与推动作用。

Abstract: We present the first French partition of the OLDI Seed Corpus, our submission
to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its
creation process, which involved using multiple machine translation systems and
a custom-built interface for post-editing by qualified native speakers. We also
highlight the unique translation challenges presented by the source data, which
combines highly technical, encyclopedic terminology with the stylistic
irregularities characteristic of user-generated content taken from Wikipedia.
This French corpus is not an end in itself, but is intended as a crucial pivot
resource to facilitate the collection of parallel corpora for the
under-resourced regional languages of France.

</details>


### [107] [Simple Methods Defend RAG Systems Well Against Real-World Attacks](https://arxiv.org/abs/2508.02296)
*Ilias Triantafyllopoulos,Renyi Qu,Salvatore Giorgi,Brenda Curtis,Lyle H. Ungar,João Sedoc*

Main category: cs.CL

TL;DR: 论文系统对比了多种离域检测方法，提出了高效的特征降维和分离策略，并用真实和模拟攻击实验验证，这对于提升RAG系统安全性和稳定性具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前在安全关键领域应用中，RAG（检索增强生成）系统需要保证安全性和响应内容在领域内（In-Domain），但实现起来存在很大难度。离域（OOD）检索检测是实际落地面临的主要挑战。

Method: 论文评估了四种OOD查询检测方法：GPT-4o、基于回归、基于主成分分析（PCA）、以及Neural Collapse（NC）方法。特别提出两种新颖的特征降维与分离策略：一是PCA，按方差解释度或OOD可分离性选取主成分；二是对Neural Collapse特征分离进行改进。

Result: 在标准数据集和真实世界场景（StackExchange、MSMARCO、Substance Use、COVID-19）验证了方法有效性，包括应对由大模型模拟和真实攻击的测试。通过人工和大模型评测，证实外部的OOD检测器对于保障系统响应的相关性至关重要。

Conclusion: 为RAG系统引入和优化OOD检测器可有效提升其安全性和在领域内的响应能力，特别是在实际攻击场景下展现出显著效果。

Abstract: Ensuring safety and in-domain responses for Retrieval-Augmented Generation
(RAG) systems is paramount in safety-critical applications, yet remains a
significant challenge. To address this, we evaluate four methodologies for
Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal
Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG
system only responds to queries confined to the system's knowledge base.
Specifically, our evaluation explores two novel dimensionality reduction and
feature separation strategies: \textit{PCA}, where top components are selected
using explained variance or OOD separability, and an adaptation of
\textit{Neural Collapse Feature Separation}. We validate our approach on
standard datasets (StackExchange and MSMARCO) and real-world applications
(Substance Use and COVID-19), including tests against LLM-simulated and actual
attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations
of response correctness and relevance, we confirm that an external OOD detector
is crucial for maintaining response relevance.

</details>


### [108] [LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training](https://arxiv.org/abs/2508.02308)
*Sikui Zhang,Guangze Gao,Ziyun Gan,Chunfeng Yuan,Zefeng Lin,Houwen Peng,Bing Li,Weiming Hu*

Main category: cs.CL

TL;DR: 提出LaMPE方法，无需训练且适配多种LLM，通过动态位置编码与多粒度机制，极大提升了RoPE在长文本任务上的表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在输入超出预训练上下文窗口后，性能显著下降，主要由于RoPE（旋转位置编码）的分布外（OOD）行为造成。已有方法多采用固定的映射策略，但忽视了输入长度与模型有效上下文窗口之间的动态关系。

Method: 提出了一种新颖的、无需训练的长度感知多粒度位置编码方法（LaMPE）。LaMPE通过参数化的缩放sigmoid函数，根据输入长度动态调整映射长度，并采用多粒度注意力机制，根据序列区域分配不同的位置信息分辨率，实现细粒度与长距离依赖的兼顾。

Result: 通过在三种典型LLM和五个主流长上下文评测基准上的大量实验，LaMPE对比现有的长度外推方法获得了显著的性能提升。

Conclusion: LaMPE可无缝适用于各种基于RoPE的LLM，无需训练，显著提升了长上下文处理能力。

Abstract: Large language models (LLMs) experience significant performance degradation
when the input exceeds the pretraining context window, primarily due to the
out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent
studies mitigate this problem by remapping OOD positions into the
in-distribution range with fixed mapping strategies, ignoring the dynamic
relationship between input length and the model's effective context window. To
this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a
training-free method that fully utilizes the model's effective context window
for adaptive long-context scaling in LLMs. Motivated by the left-skewed
frequency distribution of relative positions, LaMPE establishes a dynamic
relationship between mapping length and input length through a parametric
scaled sigmoid function to adaptively allocate positional capacity across
varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention
mechanism that strategically allocates positional resolution across different
sequence regions to capture both fine-grained locality and long-range
dependencies. Our method can be seamlessly applied to a wide range of
RoPE-based LLMs without training. Extensive experiments on three representative
LLMs across five mainstream long-context benchmarks demonstrate that LaMPE
achieves significant performance improvements compared to existing length
extrapolation methods. The code will be released at
https://github.com/scar-on/LaMPE.

</details>


### [109] [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](https://arxiv.org/abs/2508.02317)
*Qianli Ma,Yaowei Zheng,Zhelun Shi,Zhongkai Zhao,Bin Jia,Ziyue Huang,Zhiqi Lin,Youjie Li,Jiacheng Yang,Yanghua Peng,Zhi Zhang,Xin Liu*

Main category: cs.CL

TL;DR: 本文提出了高效模块化的全模态大模型训练框架\veomni，有效分离并行通信和计算逻辑，实现3D并行，提升训练效率，并能便捷支持新模态扩展。


<details>
  <summary>Details</summary>
Motivation: 当前全模态大模型训练受限于异构架构带来的工程复杂度，提高系统可扩展性和模块化设计已成为瓶颈。现有方案模型定义与并行逻辑耦合，造成工程负担和可扩展性受限。

Method: 提出了\veomni，一个以模型为中心的分布式训练框架，通过解耦通信与计算，实现了3D并行机制，并支持以最小改动集成新模态。

Result: 利用\veomni，30B参数的MoE模型在128颗GPU上可达每GPU每秒2800 tokens吞吐量，并能支持16万长度上下文，展现了优异的效率和扩展能力。

Conclusion: \veomni 框架显著提升了全模态大语言模型训练的效率和可扩展性，能够在大规模硬件上快速训练具有数十亿参数和超长上下文的模型。

Abstract: Recent advances in large language models (LLMs) have driven impressive
progress in omni-modal understanding and generation. However, training
omni-modal LLMs remains a significant challenge due to the heterogeneous model
architectures required to process diverse modalities, necessitating
sophisticated system design for efficient large-scale training. Existing
frameworks typically entangle model definition with parallel logic, incurring
limited scalability and substantial engineering overhead for end-to-end
omni-modal training. % We present \veomni, a modular and efficient training
framework to accelerate the development of omni-modal LLMs. \veomni introduces
model-centric distributed recipes that decouples communication from
computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also
features a flexible configuration interface supporting seamless integration of
new modalities with minimal code change. % Using \veomni, a omni-modal
mixture-of-experts (MoE) model with 30B parameters can be trained with over
2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D
parallelism on 128 GPUs, showcasing its superior efficiency and scalability for
training large omni-modal LLMs.

</details>


### [110] [CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis](https://arxiv.org/abs/2508.02322)
*Yuzhuang Xu,Xu Han,Yuanchi Zhang,Yixuan Wang,Yijun Liu,Shiyu Ji,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 将MoE大模型按更细粒度的“微专家”压缩和量化，提出训练无关、极高效的新方法，大幅提升效率且性能优于多种主流方案。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）中的专家混合（MoE）架构能显著提升性能，但带来了高昂的计算与存储开销，并且其模型参数的扩展与性能提升并不完全成正比。以往方法如专家级剪枝和合并等虽能减小参数量，但依然未能很好兼顾性能和效率。

Method: 本文提出以更细粒度的“微专家（micro-expert）”为核心，将MoE层视为微专家的混合，并提出了CAMERA框架——一个轻量级、无需训练就能识别微专家冗余的方法。在此基础上，进一步提出了结构化剪枝（CAMERA-P）与混合精度量化（CAMERA-Q）策略，专门针对微专家，实现更有效的模型压缩。

Result: 实验覆盖九个下游任务，结果显示CAMERA-P在20%到60%剪枝率下优于现有主流剪枝方法，而CAMERA-Q在激进的2位量化情况下亦超过了其他基于矩阵和通道的方法。此外，方法实现了在一张NVIDIA A100-40GB GPU上对Qwen2-57B-A14B模型微专家的完整分析，耗时不足5分钟。

Conclusion: 通过引入微专家这个更细粒度的单元及其相关压缩方法，显著提升了大语言模型MoE结构的剪枝与量化效果，在保持甚至提升性能的同时大幅提高了计算与存储效率。该方法简单高效，表现显著优于现有方案。

Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are
distinguished by their strong performance scaling with increasing parameters
across a wide range of tasks, yet they also suffer from substantial
computational and storage overheads. Notably, the performance gains of MoE
models do not scale proportionally with the growth in expert parameters. While
prior works attempt to reduce parameters via expert-level pruning, merging, or
decomposition, they still suffer from challenges in both performance and
computational efficiency. In this paper, we address these challenges by
introducing micro-expert as a finer-grained compression unit that spans across
matrices. We first establish a more fundamental perspective, viewing MoE layers
as mixtures of micro-experts, and present CAMERA, a lightweight and
training-free framework for identifying micro-expert redundancy. Our analysis
uncovers significant variance in micro-expert contributions during decoding.
Based on this insight, we further propose CAMERA-P, a structured micro-expert
pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed
for micro-experts. Extensive experiments on nine downstream tasks show that
CAMERA-P consistently outperforms strong baselines under pruning ratios ranging
from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under
aggressive 2-bit quantization, surpassing existing matrix- and channel-level
ideas. Notably, our method enables complete micro-expert analysis of
Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.

</details>


### [111] [Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models](https://arxiv.org/abs/2508.02360)
*Jiayi Zhang,Shu Yang,Junchao Wu,Derek F. Wong,Di Wang*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型在政治话题微调后产生意外立场泛化的神经元机制，并提出了抑制型微调方法，有效减少了这种泛化现象，对模型安全微调和可控性有具体意义。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型（LLM）在政治话题上会影响其在多个议题上的政治立场，并且还会对无关话题的立场产生意外影响，但对这种现象的内部机制缺乏理解。

Method: 提出了PNLAC方法，用于定位和区分与政治立场相关的两种神经元（跨话题通用型和主题特定型）；同时提出一种基于抑制的微调方法InhibitFT，以减少跨话题泛化现象。

Result: 通过激活修补实验，在四个模型和数据集上验证了两类政治神经元的存在。InhibitFT能有效减少20%的跨话题立场泛化，同时保持对特定话题的表现，仅抑制5%的神经元即可取得良好效果。

Conclusion: 通过从神经元层面揭示了政治立场泛化的内在机制，并提出有效降低这一现象的方法，有助于更精确地控制模型在政治话题上的表现。

Abstract: Fine-tuning Large Language Models on a political topic will significantly
manipulate their political stance on various issues and unintentionally affect
their stance on unrelated topics. While previous studies have proposed this
issue, there is still a lack of understanding regarding the internal
representations of these stances and the mechanisms that lead to unintended
cross-topic generalization. In this paper, we systematically explore the
internal mechanisms underlying this phenomenon from a neuron-level perspective
and how to mitigate the cross-topic generalization of political fine-tuning.
Firstly, we propose Political Neuron Localization through Activation
Contrasting (PNLAC) to identify two distinct types of political neurons:
general political neurons, which govern stance across multiple political
topics, and topic-specific neurons} that affect the model's political stance on
individual topics. We find the existence of these political neuron types across
four models and datasets through activation patching experiments. Leveraging
these insights, we introduce InhibitFT, an inhibition-based fine-tuning method,
effectively mitigating the cross-topic stance generalization. Experimental
results demonstrate the robustness of identified neuron types across various
models and datasets, and show that InhibitFT significantly reduces the
cross-topic stance generalization by 20% on average, while preserving
topic-specific performance. Moreover, we demonstrate that selectively
inhibiting only 5% of neurons is sufficient to effectively mitigate the
cross-topic stance generalization.

</details>


### [112] [CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation](https://arxiv.org/abs/2508.02401)
*Xiaolin Lin,Jingcun Wang,Olga Kondrateva,Yiyu Shi,Bing Li,Grace Li Zhang*

Main category: cs.CL

TL;DR: 为了提高大模型长文本处理时KV缓存的效率，CompressKV只保留最重要的token以及逐层调整缓存分配，实验显示在各种需求下都优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 近期大语言模型（LLM）在处理长文本时取得了重大进展，但日益增长的key-value（KV）缓存大小对内存和执行效率带来了巨大挑战。现有大多KV缓存压缩方法依赖启发式方法，通过全部attention head进行token淘汰，这忽视了不同attention head的功能差异，进而可能错误地淘汰掉重要token，导致模型性能下降。

Method: 本文提出CompressKV方法。首先在GQA架构下，识别每层中既能检索首尾token、又能获取本文重要token和其语义上下文的attention head。仅利用这些head确定重要token，并保留对应KV缓存对。同时，针对每层分别分析缓存淘汰错误，并引入逐层自适应KV缓存分配策略。

Result: CompressKV方法在各类内存限制下，于LongBench和Needle-in-a-Haystack基准测试上，持续优于现有最先进方法。

Conclusion: 通过关注具有特殊能力的部分注意力头，并采用分层适应的KV缓存分配策略，CompressKV在内存和性能效率间取得更优平衡，显著提升了长文本处理能力。

Abstract: Recent advances in large language models (LLMs) have significantly boosted
long-context processing. However, the increasing key-value (KV) cache size
poses critical challenges to memory and execution efficiency. Most KV cache
compression methods rely on heuristic token eviction using all attention heads
in Grouped Query Attention (GQA)-based LLMs. This method ignores the different
functionalities of attention heads, leading to the eviction of critical tokens
and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in
GQA-based LLMs to determine important tokens as in the previous work, we first
identify the attention heads in each layer that are not only capable of
retrieving the initial and final tokens of a prompt, but also capable of
retrieving important tokens within the text and attending to their surrounding
semantic context. Afterwards, we exploit such heads to determine the important
tokens and retain their corresponding KV cache pairs. Furthermore, we analyze
the cache eviction error of each layer individually and introduce a
layer-adaptive KV cache allocation strategy. Experimental results demonstrate
the proposed CompressKV consistently outperforms state-of-the-art approaches
under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.
Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.

</details>


### [113] [Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.02426)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Yichi Zhang,Haoran Duan,Nyima Tash*

Main category: cs.CL

TL;DR: 该论文提出了BAKE模型，用贝叶斯更新与持续聚类提升知识图谱嵌入模型的持续学习能力，显著减少遗忘问题，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KG）在实际场景下会不断演化，但传统的知识图谱嵌入（KGE）模型只能用于静态知识图谱，无法适应这种动态性。现有持续学习的KGE模型容易出现遗忘旧知识的问题。该论文致力于解决持续知识图谱嵌入（CKGE）中的灾难性遗忘问题。

Method: 提出了BAKE模型。其核心是将每次新数据的学习过程视为对模型先验的贝叶斯更新，利用贝叶斯后验的特性增强模型对早期知识的保留。此外，引入了一种持续聚类方法，通过约束不同时间快照中新旧知识的变化幅度，进一步减少遗忘。

Result: 通过在多个数据集上的实验，证明了BAKE模型的效果显著优于现有的基线模型，在持续学习场景下更好地保留了先前知识。

Conclusion: BAKE模型利用贝叶斯后验更新原则与持续聚类方法，有效缓解了持续知识图谱嵌入中的灾难性遗忘问题，性能优于现有方法。

Abstract: Since knowledge graphs (KG) will continue to evolve in real scenarios,
traditional KGE models are only suitable for static knowledge graphs.
Therefore, continual knowledge graph embedding (CKGE) has attracted the
attention of researchers. Currently, a key challenge facing CKGE is that the
model is prone to "catastrophic forgetting", resulting in the loss of
previously learned knowledge. In order to effectively alleviate this problem,
we propose a new CKGE model BAKE. First, we note that the Bayesian posterior
update principle provides a natural continual learning strategy that is
insensitive to data order and can theoretically effectively resist the
forgetting of previous knowledge during data evolution. Different from the
existing CKGE method, BAKE regards each batch of new data as a Bayesian update
of the model prior. Under this framework, as long as the posterior distribution
of the model is maintained, the model can better preserve the knowledge of
early snapshots even after evolving through multiple time snapshots. Secondly,
we propose a continual clustering method for CKGE, which further directly
combats knowledge forgetting by constraining the evolution difference (or
change amplitude) between new and old knowledge between different snapshots. We
conduct extensive experiments on BAKE on multiple datasets, and the results
show that BAKE significantly outperforms existing baseline models.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [114] [Runtime Consultants](https://arxiv.org/abs/2508.01821)
*Dana Fisman,Elina Sudit*

Main category: cs.FL

TL;DR: 论文首次提出运行时顾问的概念，实现了对系统行为的主动建议，能高效适应实际运行状况，并支持常见价值函数及$\omega$-正则性质的应用。


<details>
  <summary>Details</summary>
Motivation: 当前的运行时监控只能在系统出现违规时被动报警，缺乏事前的建议或指导。本论文旨在提出一种能够主动为系统提供建议，协助系统避免违规并实现最优目标的新方法。

Method: 文章提出了运行时顾问（runtime consultant）的概念，用于对系统每一步的操作提出建议，并针对常见的无限字价值函数和$\omega$-正则性质，详细阐述了如何计算这样的顾问。

Result: 作者证明了常用的价值函数几乎都能在常数时间内计算出对应的运行时顾问。同时，还提出了适用于$\omega$-正则性质（包括经典布尔语义和新的定量解释）的顾问。

Conclusion: 运行时顾问不仅能对目标函数提供建议，且无需假设建议总是被采纳，还能根据实际操作做自适应调整。该方法为系统的主动安全和性能优化提供了新思路。

Abstract: In this paper we introduce the notion of a runtime consultant. A runtime
consultant is defined with respect to some value function on infinite words.
Similar to a runtime monitor, it runs in parallel to an execution of the system
and provides inputs at every step of the run. While a runtime monitor alerts
when a violation occurs, the idea behind a consultant is to be pro-active and
provide recommendations for which action to take next in order to avoid
violation (or obtain a maximal value for quantitative objectives). It is
assumed that a runtime-controller can take these recommendations into
consideration. The runtime consultant does not assume that its recommendations
are always followed. Instead, it adjusts to the actions actually taken (similar
to a vehicle navigation system). We show how to compute a runtime consultant
for common value functions used in verification, and that almost all have a
runtime consultant that works in constant time. We also develop consultants for
$\omega$-regular properties, under both their classical Boolean semantics and
their recently proposed quantitative interpretation.

</details>


### [115] [A Myhill-Nerode Theorem for Generalized Automata, with Applications to Pattern Matching and Compression](https://arxiv.org/abs/2302.06506)
*Nicola Cotumaccio*

Main category: cs.FL

TL;DR: 本文针对广义自动机最小化唯一性问题，提出了基于集合$\mathcal{W(A)}$的理论和算法突破，首次建立了Myhill-Nerode定理的扩展，并将Wheeler自动机的高效存储和模式匹配性能推广至广义自动机。


<details>
  <summary>Details</summary>
Motivation: 传统自动机模型仅允许边被字符标记，而广义自动机允许用字符串标记，从而能够更简洁地表示正规语言，但现有的确定性广义自动机失去了最小化自动机唯一性的性质。本文旨在解释和修正这种唯一性的缺失，并推广相关理论与应用。

Method: 第一部分引入了和广义自动机关联的集合$\mathcal{W(A)}$，通过固定该集合，首次为广义自动机推导出了完整的Myhill-Nerode定理，并证明该定理对于传统自动机是一个退化情形。第二部分则利用$\mathcal{W(A)}$将Wheeler自动机中的存储和模式匹配结果推广到广义自动机，包括实现紧凑存储和高效模式匹配查询。

Result: 证明了通过引入集合$\mathcal{W(A)}$可以恢复广义确定性自动机的唯一性理论基础，并首次建立了广义自动机的Myhill-Nerode定理。此外，还提出了广义Wheeler自动机，其能用$\mathfrak{e} \log \sigma (1 + o(1)) + O(e + rn)$比特存储，并在$\tilde{O}(rm)$时间内完成模式匹配查询，其中$\mathfrak{e}$为所有边标记的总长度，$r$为单条边最大长度，$n$为状态数。

Conclusion: 通过引入集合$\mathcal{W(A)}$，不仅理论上恢复了广义自动机的许多良好性质，还在模式匹配与数据压缩等应用领域推广和优化了现有的Wheeler自动机方法。

Abstract: The model of generalized automata, introduced by Eilenberg in 1974, allows
representing a regular language more concisely than conventional automata by
allowing edges to be labeled not only with characters, but also strings.
Giammaresi and Montalbano introduced a notion of determinism for generalized
automata [STACS 1995]. While generalized deterministic automata retain many
properties of conventional deterministic automata, the uniqueness of a minimal
generalized deterministic automaton is lost.
  In the first part of the paper, we show that the lack of uniqueness can be
explained by introducing a set $ \mathcal{W(A)} $ associated with a generalized
automaton $ \mathcal{A} $. By fixing $ \mathcal{W(A)} $, we are able to derive
for the first time a full Myhill-Nerode theorem for generalized automata, which
contains the textbook Myhill-Nerode theorem for conventional automata as a
degenerate case.
  In the second part of the paper, we show that the set $ \mathcal{W(A)} $
leads to applications for pattern matching and data compression. Wheeler
automata [TCS 2017, SODA 2020] are a popular class of automata that can be
compactly stored using $ e \log \sigma (1 + o(1)) + O(e) $ bits ($ e $ being
the number of edges, $ \sigma $ being the size of the alphabet) in such a way
that pattern matching queries can be solved in $ \tilde{O}(m) $ time ($ m $
being the length of the pattern). In the paper, we show how to extend these
results to generalized automata. More precisely, a Wheeler generalized automata
can be stored using $ \mathfrak{e} \log \sigma (1 + o(1)) + O(e + rn) $ bits so
that pattern matching queries can be solved in $ \tilde{O}(r m) $ time, where $
\mathfrak{e} $ is the total length of all edge labels, $ r $ is the maximum
length of an edge label and $ n $ is the number of states.

</details>
