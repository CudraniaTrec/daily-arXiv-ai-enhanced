<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 95]
- [cs.FL](#cs.FL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny](https://arxiv.org/abs/2506.22370)
*Carolina Carreira,Álvaro Silva,Alexandre Abreu,Alexandra Mendes*

Main category: cs.SE

TL;DR: 研究发现，在形式化程序验证学习中，使用ChatGPT对学生任务表现有显著提升，但成效依赖于提示质量。作者建议应设计利用LLM的学习任务，发挥其辅助作用而非完全替代学生思考。


<details>
  <summary>Details</summary>
Motivation: 在计算教育领域，越来越多学生开始使用大型语言模型（LLMs）如ChatGPT，但目前LLMs在支持诸如演绎程序验证等高认知负荷任务中的作用尚不清楚。作者希望探索LLMs在形式化验证任务中对学生的辅助作用。

Method: 作者针对修读形式化方法课程的硕士生，设计了混合方法实验。每位学生需要完成两个Dafny验证任务：一个可使用定制版ChatGPT接口（并记录所有交互），另一个则不能使用LLM。通过分析学生交互日志，识别成功学生的策略并评估学生对LLM的信任。

Result: 实验表明，使用ChatGPT的学生完成任务表现更好，但性能提升很大程度上依赖于如何高质量地构造提示（prompt）。

Conclusion: 建议在形式方法课程中更有效地整合LLM，包括设计基于LLM的挑战以促进学习，而不是仅替代学生思考。

Abstract: Students in computing education increasingly use large language models (LLMs)
such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding
tasks, like deductive program verification, remains poorly understood. This
paper investigates how students interact with an LLM when solving formal
verification exercises in Dafny, a language that supports functional
correctness, by allowing programmers to write formal specifications and
automatically verifying that the implementation satisfies the specification. We
conducted a mixed-methods study with master's students enrolled in a formal
methods course. Each participant completed two verification problems, one with
access to a custom ChatGPT interface, that logged all interactions, and the
other without. We identified strategies used by successful students and
assessed the level of trust students place in LLMs. %\todo{Our findings show
that something here} Our findings show that students perform significantly
better when using ChatGPT; however, performance gains are tied to prompt
quality. We conclude with practical recommendations for integrating LLMs into
formal methods courses more effectively, including designing LLM-aware
challenges that promote learning rather than substitution.

</details>


### [2] [How (Not) To Write a Software Engineering Abstract](https://arxiv.org/abs/2506.21634)
*Lutz Prechelt,Lloyd Montgomery,Julian Frattini,Franz Zieris*

Main category: cs.SE

TL;DR: 本研究系统分析高水平软件工程论文摘要，发现绝大多数摘要信息不全或可读性欠佳，结构化摘要效果更好。建议后续写作强化结构并补足结论。


<details>
  <summary>Details</summary>
Motivation: 摘要对于软件工程研究论文非常重要，但目前许多摘要信息量不足。作者希望揭示高质量会议论文摘要的结构特征及其常见不足，并提出改进建议。

Method: 采用定性开放编码分析，归纳摘要的重要属性；然后对362篇来自五个高质量会议的论文摘要进行定量内容分析和探索性数据分析，比较理想结构与实际结构的差异。最后归纳撰写信息丰富摘要的建议。

Result: 仅29%的摘要包含背景、目标、方法、结果和结论（完整结构），结构化摘要比例翻倍。只有4%的摘要真正规范（兼具良好可读性和无信息/理解/歧义缺陷）。结构化摘要普遍优于非结构化，工件中心论文需要不同结构。

Conclusion: （1）即便在顶级会议，大多数摘要仍达不到理想状态；（2）结构化摘要表现更好；（3）工件型论文摘要需特殊结构；（4）应推广普适性结论在摘要中的写作。

Abstract: Background: Abstracts are a particularly valuable element in a software
engineering research article. However, not all abstracts are as informative as
they could be. Objective: Characterize the structure of abstracts in
high-quality software engineering venues. Observe and quantify deficiencies.
Suggest guidelines for writing informative abstracts. Methods: Use qualitative
open coding to derive concepts that explain relevant properties of abstracts.
Identify the archetypical structure of abstracts. Use quantitative content
analysis to objectively characterize abstract structure of a sample of 362
abstracts from five presumably high-quality venues. Use exploratory data
analysis to find recurring issues in abstracts. Compare the archetypical
structure to actual structures. Infer guidelines for producing informative
abstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,
provide background, objective, method, result, and conclusion information. For
structured abstracts, the ratio is twice as big. Only 4% of the abstracts are
proper, i.e., they also have good readability (Flesch-Kincaid score) and have
no informativeness gaps, understandability gaps, nor highly ambiguous
sentences. Conclusions: (1) Even in top venues, a large majority of abstracts
are far from ideal. (2) Structured abstracts tend to be better than
unstructured ones. (3) Artifact-centric works need a different structured
format. (4) The community should start requiring conclusions that generalize,
which currently are often missing in abstracts.

</details>


### [3] [Experience converting a large mathematical software package written in C++ to C++20 modules](https://arxiv.org/abs/2506.21654)
*Wolfgang Bangerth*

Main category: cs.SE

TL;DR: 本论文探讨C++20模块系统在大型数学软件包中的应用实践，以deal.II为例，分析模块转换的技术细节与挑战。结论为模块化可带来自身编译优化，但对下游项目影响有限，且生态过渡需长期推进。


<details>
  <summary>Details</summary>
Motivation: 数学软件传统上以“包”的形式构建，依赖包之间通过C++头文件进行接口暴露，但这种模式存在笨重、不可靠、编译缓慢等问题。C++20引入了模块系统，有望改进这些弊端。作者希望探索如何将大型数学软件包（如deal.II有限元库）从头文件转向兼容模块系统，并评估其实际效果。

Method: 以deal.II有限元库（约80万行代码）为例，探索如何同时从同一代码基础提供基于头文件与基于模块的接口。作者具体描述了转换的技术路径，遇到的挑战，以及在不同技术和人力维度的实际运作情况。

Result: 结果显示，将大型库转换为模块是可行的，尽管需要一定投入，但并不极其困难。转换后deal.II自身的编译时间有所减少，但对下游项目的编译时间未出现明显改善。

Conclusion: C++模块化系统可用于大型数学软件包，转换虽需努力，但能简化自身库的编译过程。未来，整个数学软件生态系统可能需要花费数年或数十年逐步过渡到这种新体系。

Abstract: Mathematical software has traditionally been built in the form of "packages"
that build on each other. A substantial fraction of these packages is written
in C++ and, as a consequence, the interface of a package is described in the
form of header files that downstream packages and applications can then
#include. C++ has inherited this approach towards exporting interfaces from C,
but the approach is clunky, unreliable, and slow. As a consequence, C++20 has
introduced a "module" system in which packages explicitly export declarations
and code that compilers then store in machine-readable form and that downstream
users can "import" -- a system in line with what many other programming
languages have used for decades.
  Herein, I explore how one can convert large mathematical software packages
written in C++ to this system, using the deal.II finite element library with
its around 800,000 lines of code as an example. I describe an approach that
allows providing both header-based and module-based interfaces from the same
code base, discuss the challenges one encounters, and how modules actually work
in practice in a variety of technical and human metrics. The results show that
with a non-trivial, but also not prohibitive effort, the conversion to modules
is possible, resulting in a reduction in compile time for the converted library
itself; on the other hand, for downstream projects, compile times show no clear
trend. I end with thoughts about long-term strategies for converting the entire
ecosystem of mathematical software over the coming years or decades.

</details>


### [4] [The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation](https://arxiv.org/abs/2506.21693)
*Ali Nouri,Beatriz Cabrero-Daniel,Fredrik Törner,Christian Berger*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述，总结了将DevOps应用于自动驾驶开发的挑战与现有解决方案。结果发现，DevOps尚需进一步完善以支持安全的自动驾驶系统开发。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统开发过程中，保障系统的安全性和可靠性极具挑战，且需要快速响应技术进步和突发事件，因此需要探索持续开发、部署和监控的有效方法。

Method: 通过系统性文献综述，收集、分析和整合当前关于在自动驾驶开发中应用DevOps的相关文献。

Result: 文献综述总结了自动驾驶开发中DevOps应用所面临的主要挑战和已有解决方案，特别针对安全相关AI功能。结果显示，目前在实现安全DevOps以支持安全自动驾驶开发方面，仍存在若干未解决的问题。

Conclusion: 尽管DevOps对推动自动驾驶系统的持续开发具有积极作用，但在安全性保障等关键环节仍存在挑战，有必要进一步研究以完善安全DevOps框架。

Abstract: Developing autonomous driving (AD) systems is challenging due to the
complexity of the systems and the need to assure their safe and reliable
operation. The widely adopted approach of DevOps seems promising to support the
continuous technological progress in AI and the demand for fast reaction to
incidents, which necessitate continuous development, deployment, and
monitoring. We present a systematic literature review meant to identify,
analyse, and synthesise a broad range of existing literature related to usage
of DevOps in autonomous driving development. Our results provide a structured
overview of challenges and solutions, arising from applying DevOps to
safety-related AI-enabled functions. Our results indicate that there are still
several open topics to be addressed to enable safe DevOps for the development
of safe AD.

</details>


### [5] [Using Generative AI in Software Design Education: An Experience Report](https://arxiv.org/abs/2506.21703)
*Victoria Jackson,Susannah Liu,Andre van der Hoek*

Main category: cs.SE

TL;DR: 研究报道了在本科软件设计课程中引入ChatGPT的经验，学生在设计任务中得到了实用的帮助，但也需要有判断地采纳AI建议。最后提出了教师在课堂内有效部署GenAI的建议，表明合理应用能提高软件设计教学效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GenAI）工具在各领域迅速普及，软件工程教育者正在探讨如何将这些工具有效融入课堂，尤其是在编程以外的软件开发教学中的应用尚缺乏研究。

Method: 在一门本科软件设计课程中引入GenAI（ChatGPT），要求学生团队在完成课程作业时使用该工具，并收集ChatGPT对话记录及学生对于使用体验的反思。随后对收集的数据进行了定性分析。

Result: 学生发现ChatGPT在设计过程中提供了多方面的帮助，但也意识到需对其输出进行批判性思考。研究总结出教育者在有效应用GenAI于软件设计课程时的一些关键经验和建议。

Conclusion: GenAI工具可以有效辅助学生完成软件设计任务，并帮助他们认识其优势与局限性。得出结论：在软件设计教育中合理运用GenAI能够提升教学效果。

Abstract: With the rapid adoption of Generative AI (GenAI) tools, software engineering
educators have grappled with how best to incorporate them into the classroom.
While some research discusses the use of GenAI in the context of learning to
code, there is little research that explores the use of GenAI in the classroom
for other areas of software development. This paper provides an experience
report on introducing GenAI into an undergraduate software design class.
Students were required to use GenAI (in the form of ChatGPT) to help complete a
team-based assignment. The data collected consisted of the ChatGPT conversation
logs and students' reflections on using ChatGPT for the assignment.
Subsequently, qualitative analysis was undertaken on the data. Students
identified numerous ways ChatGPT helped them in their design process while
recognizing the need to critique the response before incorporating it into
their design. At the same time, we identified several key lessons for educators
in how to deploy GenAI in a software design class effectively. Based on our
experience, we believe students can benefit from using GenAI in software design
education as it helps them design and learn about the strengths and weaknesses
of GenAI.

</details>


### [6] [KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering](https://arxiv.org/abs/2506.22037)
*Jiawei Li,Zan Liang,Guoxin Wang,Jinzhi Lu,Yan Yan,Shouxuan Wu,Hao Wang*

Main category: cs.SE

TL;DR: 提出基于KARMA和NLP的开发流程模型重建方法，实现了需求驱动的流程再设计，显著提升了设计效率。


<details>
  <summary>Details</summary>
Motivation: 在基于模型的系统工程中，复杂系统开发过程中，需求变化（如周期、成本）缺乏有效的管理与开发流程模型重建支持。

Method: 提出一种以KARMA语言为基础、结合自然语言处理的开发流程模型重建方法。先通过KARMA统一不同建模语言下的流程模型，再利用自然语言处理提取开发需求与约束，最终通过结构重组和算法优化生成满足需求的流程模型。并以航空机载维修系统开发流程为案例进行验证。

Result: 该方法通过案例验证，能够显著提升开发流程设计效率。

Conclusion: 基于KARMA语言和自然语言处理的流程模型重建方法有效应对了需求变化，有助于提升复杂系统开发流程的效率和管理能力。

Abstract: Model reconstruction is a method used to drive the development of complex
system development processes in model-based systems engineering. Currently,
during the iterative design process of a system, there is a lack of an
effective method to manage changes in development requirements, such as
development cycle requirements and cost requirements, and to realize the
reconstruction of the system development process model. To address these
issues, this paper proposes a model reconstruction method to support the
development process model. Firstly, the KARMA language, based on the GOPPRR-E
metamodeling method, is utilized to uniformly formalize the process models
constructed based on different modeling languages. Secondly, a model
reconstruction framework is introduced. This framework takes a structured
development requirements based natural language as input, employs natural
language processing techniques to analyze the development requirements text,
and extracts structural and optimization constraint information. Then, after
structural reorganization and algorithm optimization, a development process
model that meets the development requirements is obtained. Finally, as a case
study, the development process of the aircraft onboard maintenance system is
reconstructed. The results demonstrate that this method can significantly
enhance the design efficiency of the development process.

</details>


### [7] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文提出了一种基于MAPE-K和智能代理的微服务异常自动检测与修复框架，显著提升了分布式系统的安全性与稳定性，可广泛应用于实际生产环境。


<details>
  <summary>Details</summary>
Motivation: 由于微服务的去中心化特性，导致安全与管理难度大，容易影响系统稳定性，因此亟需自动化、自主的异常管理解决方案。

Method: 采用MAPE-K反馈闭环架构，并结合agentic AI，实现对微服务系统的自主异常检测与自愈。

Result: 所提出的框架具备行业适用性，能定制实现系统性能、弹性、安全和异常管理等多维度监控和提升，降低宕机时间并提升系统质量。

Conclusion: 本文提出的基于MAPE-K和agentic AI的自主管理框架，有效提升了微服务系统的稳定性和安全性，为业界和研究者应对分布式系统管理难题提供了实用方案。

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


### [8] [What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub](https://arxiv.org/abs/2506.22390)
*Ramtin Ehsani,Sakshi Pathak,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 文章分析了686个GitHub开发者-ChatGPT对话，总结影响问题解决有效性的特征。发现ChatGPT对简单、结构化任务更有效，代码解释有不足。为未来开发者互动与LLM工具优化提供实证指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）被广泛用于开发者问题解决，但实际对话并非都有效。因此，研究如何提升其对开发者帮助具有重要意义。本文动机是通过分析开发者与ChatGPT在GitHub issue中的对话，总结哪些特征更能促进问题有效解决。

Method: 本研究分析了GitHub issue中686个开发者与ChatGPT的对话。首先将对话分类辨别有效与无效，并统计开发者寻求帮助的任务类型。然后，结合对话、项目和问题相关的多种指标，探究哪些因素影响对话的有效性。同时，分析无效回复中的常见不足，为后续工具设计和模型优化提供参考。

Result: 只有62%的开发者-ChatGPT对话被认定为有助于问题解决。ChatGPT在代码生成、工具/库/API推荐方面表现较好，但在代码解释任务中较弱。有效的对话通常更短、可读性更强，语义与语言高度契合。大型、热门项目和经验丰富的开发者对ChatGPT的受益更高。在问题级别上，ChatGPT更适用于简单、开发者活动较少、易于定位的问题，如编译错误。无效回复则多表现为信息错误和不全面。

Conclusion: ChatGPT等大语言模型在开发者问题解决中有较大应用潜力，但也存在局限。有效互动受多种对话、项目和问题特征影响。本文分析结果可指导开发者优化提问方式、为工具和模型优化提供方向。无效场景的识别有助于未来提升模型表现及针对性改进。

Abstract: Conversational large-language models are extensively used for issue
resolution tasks. However, not all developer-LLM conversations are useful for
effective issue resolution. In this paper, we analyze 686 developer-ChatGPT
conversations shared within GitHub issue threads to identify characteristics
that make these conversations effective for issue resolution. First, we analyze
the conversations and their corresponding issues to distinguish helpful from
unhelpful conversations. We begin by categorizing the types of tasks developers
seek help with to better understand the scenarios in which ChatGPT is most
effective. Next, we examine a wide range of conversational, project, and
issue-related metrics to uncover factors associated with helpful conversations.
Finally, we identify common deficiencies in unhelpful ChatGPT responses to
highlight areas that could inform the design of more effective developer-facing
tools. We found that only 62% of the ChatGPT conversations were helpful for
successful issue resolution. ChatGPT is most effective for code generation and
tools/libraries/APIs recommendations, but struggles with code explanations.
Helpful conversations tend to be shorter, more readable, and exhibit stronger
semantic and linguistic alignment. Larger, more popular projects and more
experienced developers benefit more from ChatGPT. At the issue level, ChatGPT
performs best on simpler problems with limited developer activity and faster
resolution, typically well-scoped tasks like compilation errors. The most
common deficiencies in unhelpful ChatGPT responses include incorrect
information and lack of comprehensiveness. Our findings have wide implications
including guiding developers on effective interaction strategies for issue
resolution, informing the development of tools or frameworks to support optimal
prompt design, and providing insights on fine-tuning LLMs for issue resolution
tasks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Negated String Containment is Decidable (Technical Report)](https://arxiv.org/abs/2506.22061)
*Vojtěch Havlena,Michal Hečko,Lukáš Holík,Ondřej Lengál*

Main category: cs.LO

TL;DR: 首次证明了正则语言约束下not-contains字符串谓词的可判定性，为相关领域的符号执行和程序分析工具提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 长期以来，字符串“not-contains”谓词的可判定性一直是一个未解决的问题，而该问题在符号执行字符串操作程序等实际场景中非常重要。

Method: 作者研究了notContains(x1 ... xn, y1 ... ym)谓词，其中x1 ... xn和y1 ... ym是受正则语言约束的字符串变量序列，并针对其可判定性给出了正式证明。

Result: 证明了只要x1 ... xn和y1 ... ym都受正则语言约束，not-contains谓词是可判定的。此外，与无链字方程和正则成员约束结合时，not-contains谓词的可判定性仍然成立。

Conclusion: 该工作首次证明了长期悬而未决的not-contains字符串谓词在一定条件下（即变量被正则语言约束）是可判定的。

Abstract: We provide a positive answer to a long-standing open question of the
decidability of the not-contains string predicate. Not-contains is practically
relevant, for instance in symbolic execution of string manipulating programs.
Particularly, we show that the predicate notContains(x1 ... xn, y1 ... ym),
where x1 ... xn and y1 ... ym are sequences of string variables constrained by
regular languages, is decidable. Decidability of a not-contains predicate
combined with chain-free word equations and regular membership constraints
follows.

</details>


### [10] [On the role of connectivity in Linear Logic proofs](https://arxiv.org/abs/2506.21678)
*Raffaele Di Donna,Lorenzo Tortora de Falco*

Main category: cs.LO

TL;DR: 本文扩展了线性逻辑证明结构的正确性判据，通过新增的几何条件，能够在MELL的若干片段中利用原本的必要条件作为充要条件，从而给出更实用的正确性标准。


<details>
  <summary>Details</summary>
Motivation: 本文旨在扩展线性逻辑证明结构的Danos-Regnier正确性判据，寻找更广泛适用的正确性标准。现有标准在乘法指数线性逻辑（MELL）中是必要的但不足够，因此需要进一步条件，提升证明结构的可判定性和覆盖范围。

Method: 研究通过分析证明结构的正确性图（correctness graph），提出几何条件，使得原本仅为必要条件的性质对未加类型约束的证明结构成为充要条件，并据此划分出若干可判定片段。

Result: 提出了一种针对未类型化证明结构的几何条件，将原有的必要条件转化为充要条件，从而在MELL的多个片段中作为真正的正确性判据。此外，作为副产品复现了一些已知结论。

Conclusion: 通过扩展正确性判据，改善了MELL证明结构的正确性判断，对提升线性逻辑理论和相关证明工具具有理论意义和实际应用潜力。

Abstract: We investigate a property that extends the Danos-Regnier correctness
criterion for linear logic proof-structures. The property applies to the
correctness graphs of a proof-structure: it states that any such graph is
acyclic and that the number of its connected components is exactly one more
than the number of nodes bottom or weakening. This is known to be necessary but
not sufficient in multiplicative exponential linear logic (MELL) to recover a
sequent calculus proof from a proof-structure. We present a geometric condition
on untyped proof-structures allowing us to turn this necessary property into a
sufficient one: we can thus isolate several fragments of MELL for which this
property is indeed a correctness criterion. We also recover as by-product some
known results.

</details>


### [11] [Wait-Only Broadcast Protocols are Easier to Verify](https://arxiv.org/abs/2506.22144)
*Lucie Guillou,Arnaud Sangnier,Nathalie Sznajder*

Main category: cs.LO

TL;DR: 本文研究了有限状态、广播通信的参数化进程网络中的同步与重复可覆盖性问题。在Wait-Only协议下，发现同步问题为Ackermann-complete，重复可覆盖性问题为EXPSPACE且PSPACE-hard。


<details>
  <summary>Details</summary>
Motivation: 在一般情况下，涉及有限状态协议和广播通信的参数化进程网络的同步与可覆盖性问题是不可判定的。该工作旨在研究在协议有限制（即Wait-Only协议）时，问题的复杂性如何变化。

Method: 作者分析了Wait-Only协议的特性，即协议中不存在同时能够广播和接收消息的状态，并对两类问题（同步与重复可覆盖性）进行了复杂性理论分析。

Result: 对于Wait-Only协议，同步问题的复杂度达到了Ackermann-complete，而重复可覆盖性问题则处于EXPSPACE（且PSPACE-hard）。

Conclusion: 通过对Wait-Only协议的研究，作者发现尽管一般情况下同步和重复可覆盖性不可判定，但在Wait-Only限制下，两问题的判定边界得到了明确：同步问题极其复杂但可判定，重复可覆盖性问题也落于高复杂类中。

Abstract: We study networks of processes that all execute the same finite-state
protocol and communicate via broadcasts. We are interested in two problems with
a parameterized number of processes: the synchronization problem which asks
whether there is an execution which puts all processes on a given state; and
the repeated coverability problem which asks if there is an infinite execution
where a given transition is taken infinitely often. Since both problems are
undecidable in the general case, we investigate those problems when the
protocol is Wait-Only, i.e., it has no state from which a process can both
broadcast and receive messages. We establish that the synchronization problem
becomes Ackermann-complete, and the repeated coverability problem is in
EXPSPACE, and PSPACE-hard.

</details>


### [12] [Scott's Representation Theorem and the Univalent Karoubi Envelope](https://arxiv.org/abs/2506.22196)
*Arnoud van der Leer,Kobe Wullaert,Benedikt Ahrens*

Main category: cs.LO

TL;DR: 文章在UniMath库中形式化了Scott表述定理，给出两种证明，阐明了Karoubi envelope的作用，并提升了lambda项自动化归约能力，对lambda演算和范畴论研究意义重大。


<details>
  <summary>Details</summary>
Motivation: 本文致力于将Scott关于无类型lambda演算的表述定理形式化，填补类型理论、范畴论和形式化理论之间的研究空白。Scott定理表明无类型lambda演算可由范畴中某个自反对象导出，这为理论计算机科学和数学基础提供了重要工具。

Method: 作者在univalent foundations及UniMath库中对Scott的表述定理进行了形式化实现，分别采用Scott本人和Hyland的方法给出了两种证明。同时分析了范畴学构造Karoubi envelope在证明中的作用，并探讨所选基础对该构造的影响。还报告了一些自动化lambda项归约的方法。

Result: 成功在UniMath库内实现了Scott表述定理的正式化，提供了Scott和Hyland两种证明方案，澄清了Karoubi envelope的作用，并提升了lambda项归约的自动化能力。

Conclusion: 该工作展示了高阶抽象数学定理在现代形式化体系中的实现方法，对无类型lambda演算和范畴论联系的研究有重要推动，同时对相关自动化理论与工具开发具有促进作用。

Abstract: Lambek and Scott constructed a correspondence between simply-typed lambda
calculi and Cartesian closed categories. Scott's Representation Theorem is a
cousin to this result for untyped lambda calculi. It states that every untyped
lambda calculus arises from a reflexive object in some category. We present a
formalization of Scott's Representation Theorem in univalent foundations, in
the (Rocq-)UniMath library. Specifically, we implement two proofs of that
theorem, one by Scott and one by Hyland. We also explain the role of the
Karoubi envelope -- a categorical construction -- in the proofs and the impact
the chosen foundation has on this construction. Finally, we report on some
automation we have implemented for the reduction of $\lambda$-terms.

</details>


### [13] [Computation by infinite descent made explicit](https://arxiv.org/abs/2506.22206)
*Sebastian Enqvist*

Main category: cs.LO

TL;DR: 该文提出了一个带序数标注非良构证明系统以扩展直觉逻辑，定义并论证了其内部每个有效证明的可计算性，获得了有限公式证明的规范化结果，并构建了与初始代数/终端余代数相对应的范畴模型。


<details>
  <summary>Details</summary>
Motivation: 探讨由归纳和余归纳定义扩展的直觉逻辑如何能被更有效地表达和证明，特别是针对涉及到递归与余递归的证明系统，如何处理和刻画这些系统性的复杂性。

Method: 提出了一个基于非良构证明系统（non-wellfounded proof system），其中不动点公式配有明确的序数变量标注。定义了系统内部的可计算性概念，进而分析证明的计算内容，并从证明系统推导出范畴学模型，将最小和最大不动点公式解释为初始代数和终端余代数。

Result: 表明系统中每一个有效证明都是可计算的，特别针对“有限公式”（finitary formulas）的证明得到了规范化结果。进一步，部分类证实每个满足特定形式的序列的证明可以表示自然数上的唯一函数，并通过范畴模型说明了不动点公式的代数对应关系。

Conclusion: 该非良构证明系统不仅在理论上扩展了直觉逻辑的表达能力，并通过对可计算性的刻画，实现了证明的规范化及其与范畴代数结构的对应，可为逻辑与类型理论等应用奠定基础。

Abstract: We introduce a non-wellfounded proof system for intuitionistic logic extended
with ordinary inductive and co-inductive definitions, based on a syntax in
which fixpoint formulas are annotated with explicit variables for ordinals. We
explore the computational content of this system, in particular we introduce a
notion of computability and show that every valid proof is computable. As a
consequence, we obtain a normalization result for proofs of what we call
finitary formulas. A special case of this result is that every proof of a
sequent of the appropriate form represents a unique function on natural
numbers. Finally, we derive a categorical model from the proof system and show
that least and greatest fixpoint formulas correspond to initial algebras and
final coalgebras respectively.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: 本文针对多语种语音识别中存在的语言互扰问题，提出基于Whisper和LoRA专家的新微调方案，通过专家融合或知识蒸馏，有效提升多语种识别性能，实验验证了其显著优势。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和多语种大型数据集的进步推动了多语种语音识别（ASR）发展，但多语种模型容易出现语言间干扰，导致识别效果受限。如何兼顾多语种性能、缓解干扰问题，是当前的核心难题。

Method: 本文提出了一种高效的多语种ASR微调框架，基于Whisper模型预训练LoRA语言专家，并通过LoRA专家融合或知识蒸馏技术，实现针对目标语言的定制化识别能力。

Result: 实验结果显示，该方法在语言感知和非语言感知场景下，分别带来约10%和15%的相对性能提升，优于常规微调策略。

Conclusion: 基于LoRA专家和Whisper的多语种ASR微调方式能够缓解多语种之间的干扰，有效提升目标语言识别效果，展现了优异的实验性能和应用价值。

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [15] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: 针对现有多模态知识图谱覆盖面窄、模态单一的问题，作者提出了涵盖视觉、音频和文本的VAT-KG，并通过新型RAG框架验证了其在多模态问答中的有效性，可显著增强多模态大模型的知识能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态知识图谱（MMKGs）通常通过扩展已有的知识图谱生成，这导致知识覆盖面有限、过时或不完整，仅支持少量模态（如文本和视觉），限制了其在多样化多模态任务中的应用，尤其难以适应最近多模态大模型对视频和音频等丰富模态的需求。

Method: 作者提出了Visual-Audio-Text Knowledge Graph（VAT-KG），首次以概念为核心、知识密集地覆盖视觉、音频和文本信息，每个三元组都关联了多模态数据和详细描述。其构建流程通过严格的筛选和对齐步骤保证多模态数据与细粒度语义之间的跨模态对齐，支持从任意多模态数据集自动生成MMKG。此外，提出了创新性的多模态RAG检索增强生成框架，可以从任意模态查询中检索详细的概念级知识。

Result: VAT-KG在跨多种模态的问题回答任务中证实了其有效性，能够更好地支持多模态大语言模型（MLLMs），体现了统一和利用多模态知识的实际价值。

Conclusion: VAT-KG突破了现有MMKGs的局限，实现了三模态（视觉、音频、文本）的全面覆盖和知识对齐，为多模态大模型提供更丰富和精准的知识支撑。

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [16] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: 该文提出DIFND，结合扩散模型和多模态大语言模型，通过生成和推理协作显著提升了假新闻检测的准确率及可解释性，在多个数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着假新闻在多媒体平台上的快速传播，信息可信度受到严重挑战，迫切需要提升假新闻检测的准确性与可解释性。

Method: 提出了DIFND（Debunk-and-Infer Fake News Detection）框架，融合有条件扩散生成模型和多模态大语言模型（MLLM）。通过“debunk diffusion”生成反驳或验证证据，并提出多智能体链式推理（chain-of-debunk）以逻辑、多模态信息辅助真伪判别。

Result: 在FakeSV和FVC两个权威数据集上的大量实验证明，DIFND显著提升了假新闻检测准确率，并增强了结果的可信和可解释性，超越了现有方法。

Conclusion: 整合了一体化的多模态特征、生成的证据线索和推理验证内容，DIFND不仅提升检测性能，还增强了决策可信度和可解释性。

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [17] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 作者提出了Bench To the Future (BTF)“逆向预测”基准，为LLM预测能力评估搭建了真实、可重复的环境，验证了该方法的有效性，并将持续拓展。


<details>
  <summary>Details</summary>
Motivation: 现有的预测基准很难为大语言模型（LLM）提供一个真实、封闭且可重复的环境。同时，预测研究需要大量网络数据和较长的结果验证周期，导致相关基准缺乏。

Method: 提出了Bench To the Future (BTF)“逆向预测”（pastcasting）基准，包含数百个已知结果的高质量问题，并为每个问题配有成千上万相关网页离线语料。用于让LLM对过去已知答案的问题进行“预测”评估，同时支持多种代理和思维链预测方式的基准测试。

Result: BTF环境下的预测结果与真实网络环境中对未解决问题的预测结果相当。通过多个LLM（包括Claude 4）对代理与思维链方法进行评测，验证了BTF跟踪LLM预测能力随时间进展的有效性。

Conclusion: BTF为LLM的预测能力评估提供了现实、封闭且可持续扩展的基准，将持续更新以适应更多模型的训练数据。研究者可联系作者获取使用该基准和工具的权限。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [18] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: 提出GraphLAMA方法，通过在少量参数适应阶段用少数样本快速调优GLMs，兼顾效率与效果，实现了图任务上的更高准确率和更快推理速度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）和图语言模型（GLMs）在图分析中取得了进展，但现有GLMs的两大主流方案存在不足：基于ICL（in-context learning）的方法在参数固定、推理效率低下，基于instruction tuning的方法则需要大量标注数据，实际中难以获得。作者希望在少量标注的情况下，提高预测精度与推理速度。

Method: 提出了一种新的方法GraphLAMA，其采用图神经网络（GNN）作为主干网络，将节点信息转化为LLM token的表征，并结合节点和自然语言token来表达任务指令。其中，大部分参数会在预训练阶段经过多任务训练；在适应阶段，仅更新部分参数以适应新任务和新图，借助少量标注样本实现高效微调。

Result: GraphLAMA在少/零样本节点分类及摘要生成任务上实现了最先进的性能。相较于ICL，5-shot设置下推理速度提升10倍，节点分类准确率绝对提升4.91%。

Conclusion: GraphLAMA成功解决了GLMs在高效调优与推理上的实际问题，在仅需少量样本的情形下，显著提高了性能和效率。该框架为实际少标注场景下的图任务提供了有力的新方法。

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [19] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: 针对小型语言模型的任务对齐与推理，RLOO+DeBERTa奖励建模效果最佳，DPO稳定，外部工具增强了数学推理能力，总结了小模型高效训练的实用方法。


<details>
  <summary>Details</summary>
Motivation: 探究和优化小规模语言模型在复杂指令跟随和数学推理任务中的对齐与性能表现，寻找高效、实用的微调方法以降低资源消耗，提升小模型实用价值。

Method: 对轻量级语言模型（Qwen2.5-0.5B Base）分别应用SFT（有监督微调）、DPO（直接偏好优化）、RLOO（基于奖励的强化学习法），在指令跟随和数学推理任务上进行对比实验，并结合DeBERTa奖励模型、合成数据增强及best-of-N采样方式。

Result: RLOO+DeBERTa奖励模型达到最佳对齐效果；DPO可获得稳定优异的结果。数学推理任务通过合成数据增强和best-of-N+外部验证器显著提升了准确率。实验突出微调方法在小模型对齐中的性能和适用性差异。

Conclusion: RLOO方法结合DeBERTa奖励建模在任务对齐方面表现最佳，DPO也有稳定且优异的效果。数学推理任务中，合成数据增强和best-of-N采样配合外部验证器大幅提升准确率，验证了微调与推理时工具结合的潜力。作者总结了小模型任务对齐训练中的关键权衡和实用策略。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [20] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: 本研究系统评估了8种大型语言模型对真伪的判断能力，发现推理型模型虽有所改善，但相比人类依然容易“偏信为真”，且部分先进模型对于欺骗性内容识别表现不佳，凸显提升能力难以消除根本性真伪识别问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在事实核查、内容审核和高风险决策中被广泛应用，但它们作为真伪判断者的机制仍然不甚清晰。

Method: 该研究对8个LLM模型进行了4800次真伪判断的大规模对比实验，涵盖推理型和非推理型模型，系统性分析其识别真实性能和偏差。

Result: 推理型模型相较非推理型模型更不容易表现出“真实性偏见”，但这种偏见仍普遍高于人类基准。部分高级模型表现出“阿谀倾向”，即在识别真实陈述时表现优秀，但在识别虚假陈述时表现较差。

Conclusion: 当前即便模型能力提升，LLM在真假判断方面仍存在根本性挑战，仅依赖能力改进无法彻底解决。

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [21] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 本文提出了基于“下一房间预测”的平面图生成模型（FPDS），更贴合实际建筑设计的逐步流程，并在基准任务中取得有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 目前生成平面图的模型多为端到端、一次性输出完整布局，这与建筑设计中逐步、迭代的真实流程不符。作者希望解决生成方式与实际设计流程间的不匹配问题。

Method: 从大语言模型的自回归“下一个token预测”机制获得启发，提出了“下一房间预测”范式，将其应用于建筑平面图设计模型（FPDS）。

Result: FPDS在文本生成平面图任务中，与扩散模型和Tell2Design方法相比，表现具有竞争力。

Conclusion: FPDS这一新范式有望更好地支持建筑设计中的智能化流程，尤其符合设计的渐进与迭代过程。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [22] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: 本文建立并发布了一个面向台湾三种濒危南岛语的NLP测评基准FORMOSANBENCH，系统测试了大语言模型在这些低资源语言下的翻译、语音识别和摘要任务，结果均表现欠佳，呼吁NLP领域关注和支持少数濒危语言，推动相关资源与研究开放。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在高资源语言的NLP任务上表现优异，但在低资源和少数民族语言上的能力研究不足。台湾的福尔摩沙语言为南岛语系的一支，语言多样且濒危，受现代汉语影响严重。该研究动机在于：推动大语言模型对低资源、濒危南岛语的支持与研究，促进更包容的NLP技术发展。

Method: 作者提出了FORMOSANBENCH，这是首个针对低资源南岛语（福尔摩沙语言）的大语言模型评测基准。涵盖Atayal、Amis和Paiwan三种濒危语言，任务包含机器翻译、自动语音识别及文本摘要。评测在zero-shot、10-shot和微调三种设定下，对现有模型进行了系统性能评估。

Result: 结果显示，现有大语言模型在高资源语言与福尔摩沙语言间存在显著性能差距，无论在何种任务中均表现不佳。10-shot学习和微调虽有提升，但作用有限。

Conclusion: 研究表明当前LLMs对低资源、濒危语言支持严重不足，强调了发展更具包容性的NLP技术以支持这些语言的迫切性。作者已公开数据与代码以推动相关领域后续研究。

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [23] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: 该论文针对事实核查声明检索任务，提出三阶段检索+重排序+加权投票方法，并在SemEval-2025任务7中取得优异名次。


<details>
  <summary>Details</summary>
Motivation: 近年来假新闻与虚假信息的传播日益严重，事实核查尤为重要。该论文关注于为SemEval-2025任务7，即基于事实核查的声明检索问题，提出有效的技术方案以提升检索准确率。

Method: 提出了一个三阶段的检索框架：（1）评估并选择表现最好的检索模型进行候选召回；（2）利用多个重排序模型对候选结果进行优化，每个模型选出前十条结果；（3）通过加权投票确定最终检索结果。

Result: 该方法在SemEval-2025任务7比赛中取得了单语检索赛道第5名，跨语检索赛道第7名的成绩。

Conclusion: 三阶段的检索及重排序结合加权投票方案能有效提升事实核查声明检索任务的性能，在实际比赛中表现优异。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [24] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: 受日式社区沟通方式启发，作者提出结合多大语言模型和“闲聊”交流的新型多智能体推理框架，能提升情感分析中的偏差消减和预测多样性，实验结果积极，未来将进一步量化其表现。


<details>
  <summary>Details</summary>
Motivation: 受到日本传统的“回览板”（kairanban）文化和“井边谈话”（idobata）交流模式的启发，作者希望借鉴其在社区成员间促进细致对话、社会平衡形成中的作用，解决情感分析中预测偏差、可解释性不足等问题。

Method: 提出并实现了一个多智能体推理框架（KCS+IBC），融合了多个大语言模型（LLMs）。该方法不仅在多智能体间顺序共享预测结果，还引入了中期的“闲聊”环节，结合了正式推理和个人观点，同时引入概率性情感预测机制。

Result: 实验表明，KCS的准确率与单一大语言模型相当，而KCS+IBC在推理后期表现出熵的持续下降和方差的逐步上升，说明该框架能够在聚合与多样性之间取得平衡。

Conclusion: 多智能体框架受日本传统社区沟通方式启发，能够提升情感分析中的预测多样性、降低偏差并改善可解释性。未来将定量评估其偏差修正作用，并开发更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [25] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: 作者在英-古吉拉特语这一低资源但高质量的翻译任务中，发现继续通过反向翻译增加合成数据对提升MT性能无明显帮助，甚至有反效果。提示在一定条件下，反向翻译数据增强存在边际报酬递减。


<details>
  <summary>Details</summary>
Motivation: 反向翻译在低资源机器翻译中常用于通过单语语料生成合成训练数据，取得了显著成效。但其在高质量、低资源场景下的有效性仍不明确。

Method: 以MBART50多语言预训练模型为基础，在约5万对高质量英-古吉拉特语平行语料上训练基线系统，使用精心过滤的反向翻译语料进行数据增强，并通过BLEU、ChrF++、TER、BLEURT等多种评测指标进行性能评估。

Result: 引入反向翻译合成数据未能提升英-古吉拉特语的机器翻译性能，有时甚至轻微降低。分析了出现性能饱和原因及对未来研究的启示。

Conclusion: 经过实验证明，在特定高质量低资源双语设置下，反向翻译并没有提升翻译表现，甚至在某些情况下略有下降。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [26] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: 本文提出了针对波斯语医学领域的LLM评测体系BioPars，并构建大规模数据集，系统评估了主流大模型在医学知识问答上的综合能力。BioPars显著优于现有模型，尤其在长答案生成任务上效果突出，相关资源已开源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）近年来在生命科学领域备受关注，因其具备处理、提取和应用复杂生物信息的能力。然而，现有模型在面对专业化如生物信息领域的问题时，表现尚有不足，尤其是在波斯语医学问答领域研究稀缺。因此，文章希望通过开发新数据集和评测体系，系统评估并提升LLMs在该领域的实际应用能力。

Method: 作者构建了BIOPARS-BENCH数据集（涵盖1万余篇文献、教材和医疗网站），同时开发了BioParsQA（包含5231条波斯语医学问答）。提出了BioPars评测体系，从知识获取、知识解释合成，以及证据展示三方面评估LLMs。针对ChatGPT、Llama、Galactica等模型进行了实验对比，并采用ROUGE-L、BERTScore、MoverScore、BLEURT等多种评价指标量化性能。

Result: BioPars模型在BioParsQA上的ROUGE-L得分为29.99，BERTScore为90.87，MoverScore为60.43，BLEURT为50.78，均优于GPT-4 1.0及其它主流模型。在四个医学问答数据集上的综合表现亦显著领先对照方法，尤其在生成长答案任务上表现突出。

Conclusion: BioPars首次实现了LLMs在波斯语医学问答领域的突破，在多项标准化评测中取得了优越成绩，但对于更高阶的现实世界问题和细粒度推理还有进步空间。该体系和资源的开放将助力推动该领域进一步发展和优化。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [27] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: 本文比较了RAG和HyDE增强方法在小型Gemma LLM上的性价比，结果显示RAG更适合于本地个人助理的部署，兼顾速度和准确性；HyDE虽语义表现优异但不适于高效场景。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在边缘设备和隐私敏感应用中的应用越发广泛，资源效率成为部署的主要障碍。本研究旨在探索有效提升小型LLM表现的增强策略。

Method: 评估了两种主流增强方法：检索增强生成（RAG）和假设文档嵌入（HyDE），在1B和4B参数的小型Gemma LLM上进行测试。采用MongoDB实现短期记忆，Qdrant实现长期语义存储，系统由FastAPI和LangChain编排，并通过React.js前端交互。

Result: RAG方案在所有模型规模下有效降低了响应延迟（最高可达17%）并消除事实幻觉，对于个性化和领域化查询成效显著。HyDE虽然提升了对复杂物理问答的语义相关性，但响应时间增加25-40%，且在个人数据检索中仍有一定幻觉率。模型扩展从1B到4B带来基线和RAG方案的吞吐提升有限，HyDE则计算开销骤增且不稳定。

Conclusion: RAG是在隐私优先的设备端小型LLM个人助理中的现实首选，能有效改善效率和准确性。HyDE虽有优势但受限于高资源消耗和幻觉问题。

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [28] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: 提出RAG框架和微调集，将自然语言高效准确地转化为SVA，显著优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 将自然语言属性描述（NL2SVA）自动转化为SystemVerilog Assertions（SVAs）是一个繁琐且易出错的过程。尽管大语言模型（LLMs）在自然语言处理上有所突破，但在NL2SVA任务中仍难以准确理解领域相关的语法与语义，需要新的方法提升性能。

Method: 提出一种定制化的RAG（检索增强生成）框架，并提供了基于合成数据的微调数据集，其中包含分步构建SVA的解释，有助于LLM进行有监督微调。还通过构建包含40个Verilog设计及229条形式验证SVA的大规模评测集，系统评估模型性能。

Result: 定制RAG框架使功能匹配的SVA数量相比GPT-4o-mini提高了58.42%；Qwen2.5-Coder-7B-Instruct模型结合HybridRetrieval并在该微调集上微调后，超越原始Qwen模型59.05%。

Conclusion: 通过定制化检索增强、微调数据和结构化解释，显著提升了LLM在NL2SVA任务的语法与功能准确率，证明了所提出方法的有效性。

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [29] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 分析了预训练语言模型迁移到低数据量时间序列预测任务，不同设计选项对效果有重要影响，模型迁移带来的性能优势持久存在，提示应关注设计细节来提升预测表现。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现，预训练语言模型（LMs）对低数据量时间序列预测具有良好适应性，本文希望进一步分析不同设计选项（如下游后训练、时间序列分词器、骨干大小）对这种迁移效果的具体影响。

Method: 作者通过实验比较不同设计选择下（上游后训练、分词器、模型规模等）使用预训练语言模型迁移到时间序列预测的表现，并关注在低数据量下验证损失和模型迁移间隔。

Result: 不同设计选择对验证损失有显著影响，部分设计方案效果明显优于其他。与2021年Hernandez等人的结论相反，LMs的验证损失在收敛后持续平稳减少，迁移带来的效果优势不消失，对多种设计均成立。

Conclusion: 有效利用预训练语言模型及其设计选择有助于提升低数据量时间序列预测性能，同时揭示模型能跨模态利用数据分布共性，对高效训练和跨领域应用具有启示意义。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [30] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 作者提出CogTest基准，系统评估了主流LRM的认知习惯。结果显示LRM具有人类类认知习惯、能自适应部署，并且某些习惯易导致有害响应。分析推理链中持久行为模式有助于深入理解LLM风险。


<details>
  <summary>Details</summary>
Motivation: 受人类问题求解中的'思维习惯'理论启发，作者希望探索LRMs是否具有类似人类的认知习惯，从而进一步理解和监控模型行为，尤其关注模型潜在的错误及风险特征。

Method: 提出CogTest基准，包含16种认知习惯、共400个多样化任务，采用证据优先抽取方法以确保习惯判定的可靠性，对16个主流大语言模型（13个LRM、3个非推理模型）进行系统评估和对比分析。

Result: LRMs显示出明显的类人认知习惯，且可在不同任务间灵活切换。细粒度分析显示，LRMs间在认知习惯轮廓上存在异同，部分模型家族表现出高度相似性。在安全相关任务中，某些认知习惯与有害回应高度相关。

Conclusion: 研究发现，大型推理模型（LRMs）在任务中具有类人的认知习惯，并可根据任务自适应部署这些习惯。此外，部分认知习惯（如承担适当风险）与有害响应生成高度相关。分析LRMs的推理链条中的行为模式对理解LLM不当行为具有重要价值。

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [31] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于结构方程建模的多模态大语言模型评测新框架，引入认知分层理论，并构建了新的Gold基准，提升了评测的解释力和科学性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）的评测面临结构化和可解释性不足的问题，现有的评测基准多基于启发性任务分组，认知目标不明确，导致能力重叠、指标冗余和诊断能力有限。

Method: 作者提出基于结构方程建模（SEM）的MLLM基准对齐新框架，并基于皮亚杰认知发展理论设计能力层级，将MLLM能力分为感知、记忆和推理三个层次。此外，重组现有基准并构建了新的Gold基准。

Result: 实验结果表明，所提出的新基准相比现有方式具有更强的可解释性，更低的指标冗余和更清晰的认知一致性。

Conclusion: 通过引入结构化、理论驱动的评测和认知层级体系，可以更有效地评估和区分多模态大语言模型的核心能力。

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [32] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: 该论文提出一种结合黑盒与白盒优势的大语言模型指令优化方法，通过语义约束融合两者表征，显著提升模型指令优化效果，优于现有方法，具备高扩展性和效率，适用于多种复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型的指令优化，白盒方法算力消耗大、能力有限，黑盒成本高，实际操作难度大。

Method: 提出融合黑盒与白盒的大语言模型指令优化框架：用黑盒产生高质量多样的指令初始值，用白盒内部表征做精细可解释优化，通过语义相似性约束融合两者，形成高维统一表征，在迭代优化中不断提升指令质量。

Result: 在复杂推理、跨语言等多种任务上，提出的方法超过现有最先进基线，效果更优且可扩展。

Conclusion: 将黑盒初始化与白盒语义细化结合，为多样真实应用场景下的LLM优化提供了高效可扩展的新方案。即将开源代码。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [33] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: 本文探讨了GPT-3.5、GPT-4等大型语言模型在移民决策中的应用价值及问题，发现其能够对齐人类决策并注重公平，但仍存在偏见和刻板印象，提示其在移民自动决策领域仍需完善。


<details>
  <summary>Details</summary>
Motivation: 随着全球化和移民人口的增加，移民部门工作量巨大，并面临确保决策公正性的挑战，亟需寻找高效且公平的辅助决策工具。人工智能（尤其是大型语言模型）的引入被视为有前景的解决方案。

Method: 采用混合方法研究，包括离散选择实验和深度访谈，分析大型语言模型（如GPT-3.5和GPT-4）在移民决策中的策略及其公平性。

Result: 研究发现，大型语言模型的决策能够对齐人类的策略，注重效用最大化和程序性公平。但同时，模型（如ChatGPT）虽然设有防止歧视的安全机制，仍表现出有关国籍的刻板印象和偏见，对特权群体存在倾向性。

Conclusion: 大型语言模型有潜力自动化并提升移民决策流程，但当前依然存在刻板印象和偏见等局限性，因此需谨慎应用并持续改进。

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [34] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: STRuCT-LLM通过联合训练SQL与Cypher推理任务，并引入结构化奖励机制，大幅提升了LLM处理多种结构化数据的能力，实现了跨结构迁移和强零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 以往研究通常将关系型与图结构数据推理割裂处理，难以互相促进。本研究旨在统一二者训练流程，挖掘共享抽象表达，实现跨任务、跨型结构推理增强。

Method: 采用统一框架，结合强化学习(RL)和链式思考(COT)监督，并引入基于图编辑距离的拓扑感知奖励函数，同时对Text-to-SQL与Text-to-Cypher任务进行联合优化。利用SQL与Cypher的抽象共性，实现跨形式迁移训练。

Result: 最大模型QwQ-32B在Spider（语义解析）提升13.5%，Text2Cypher提升73.1%。零样本泛化表现也强，在TableBench提升8.5%、CR-LT-KGQA提升1.7%，且无需针对性QA监督。

Conclusion: STRuCT-LLM 能显著提升LLM在结构化推理任务中的表现，尤其是在SQL和Cypher等不同数据结构之间实现了互补和迁移学习效应。联合训练及可执行查询作为推理支架的方法效果明显。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [35] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: 本文提出并验证了软提示微调及其变体（SPT4ASR）可在低资源及代码混用场景下，有效提升大型多语种ASR模型（如Whisper）的识别性能，并保持高效的参数使用和原有多语种能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多语言ASR模型（如Whisper）在高资源环境下表现优异，但在低资源环境（如罕见语言或代码混用场景）下受限于计算成本和灾难性遗忘问题。需要提升在低资源、代码混用情况的表现，并兼顾参数效率和保持模型已有知识。

Method: 研究采用参数高效的Soft Prompt Tuning（SPT）方法提升代码混用ASR表现，评估两种策略：（1）全量微调（同时微调软提示和整个模型）；（2）严格冻结模型，仅微调软提示。还提出SPT4ASR——融合不同SPT变体。实验基于SEAME和ASRU2019数据集。

Result: 实验表明，深层提示微调是最有效的SPT方法，SPT4ASR进一步降低了代码混用ASR的错误率，并保持与LoRA相近的参数效率，同时没有降低已有语言的性能。

Conclusion: SPT及其改进方法SPT4ASR能够在保证参数效率的前提下，有效提升多语种ASR模型在代码混用及低资源场景的表现。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [36] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: 本文针对多语种ASR模型在新语言扩展和干扰上的问题，提出了更高效的软提示调优方法（Entire SPT和LAPT）并开发了相关工具，实验表明其性能显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大型端到端模型如Whisper推动了多语种ASR的进步，但在语言干扰和新语言扩展方面仍面临挑战。这些问题在实际应用中会导致识别准确率下降和模型扩展性差。

Method: 提出了三种新方法：（1）Entire Soft Prompt Tuning（Entire SPT），将软提示应用于编码器和解码器，提高特征提取和解码能力；（2）Language-Aware Prompt Tuning（LAPT），结合跨语言相似性，通过轻量级提示矩阵编码共享和特定语言特征；（3）集成了SPT算法的工具包“SPT-Whisper”，支持高效的持续学习。

Result: 在FLEURS数据集中三个语言上的实验表明，Entire SPT与LAPT在新语言扩展任务上，分别比仅在解码器应用软提示（Decoder SPT）提升了5.0%和16.0%的性能，并且方法计算开销很小。

Conclusion: 所提方法有效提升了多语种ASR模型在新语言扩展任务中的性能，为动态多语种ASR提供了一种高效可扩展的解决方案。

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [37] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: 该论文提出面向葡萄牙语医疗多专业的HealthQA-BR数据集，并对二十余种主流大模型进行评测。结果显示，虽然某些模型总体分数较高，但在部分专业上表现不佳，暴露了以单一分数评估模型严重不足的问题。


<details>
  <summary>Details</summary>
Motivation: 目前医疗领域对大型语言模型的评估主要集中在以医生为中心的英文基准测试上，忽视了医疗服务协作的多专业属性，导致对AI能力产生片面的安全假象。

Method: 提出HealthQA-BR，这是首个葡萄牙语医疗领域的大规模、系统性、多学科评测集，包含5632道题目，覆盖巴西医学、护理、牙科学、心理学、社会工作等多专业领域。对超过20种主流LLM模型进行了严格的零样本评估和细粒度分析。

Result: 先进模型（如GPT4.1）整体准确率高（86.6%），但在不同专业差异极大，例如眼科学接近满分（98.7%），而神经外科和社会工作仅为60.0%和68.4%。所有模型都表现出这种“尖刺型”能力分布。

Conclusion: 高水平总体分数掩盖了模型在部分专业领域的严重短板，仅凭总成绩无法判断模型的实际安全性。此基准和评测工具的公开能推动更细致、全方位的AI医疗应用审查。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [38] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: 大语言模型的通用推理能力提升，有助于其在各类领域任务中的表现和决策效果，未来AI模型建议加强对推理能力的训练。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型表现出强大的能力，但真正有效的决策高度依赖于推理能力。随着AI技术发展，人们越来越关注如何提升LLMs的推理水平。因此，探究通用推理能力如何影响领域推理能力，具有理论和实际意义。

Method: 通过分析和实验，探索了LLMs在通用推理与领域推理任务之间的联系。可能采用了定量评测、案例分析等方法。

Result: 研究发现：大语言模型如果具备更强的通用推理能力，在特定领域内的推理与决策任务中同样表现更优，建议未来进一步提升LLMs的通用推理训练。

Conclusion: 大语言模型（LLMs）在领域推理任务中的表现与其通用推理能力密切相关。提升LLMs的一般推理能力有助于提升其在特定领域内的决策与推理水平。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [39] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: VIDEE降低了文本分析门槛，让非专业用户也能完成高级分析。系统三阶段流程涵盖分解、执行和评估，实验结果显示其实用性和良好可用性，并为未来智能人机协作文本分析系统提供了启示。


<details>
  <summary>Details</summary>
Motivation: 文本分析往往需要NLP等专业知识，入门级分析师门槛较高。随着大语言模型的发展，文本分析的门槛降低，亟需帮助非专业用户方便地完成复杂任务。

Method: 提出VIDEE系统，采用三阶段人-代理协同流程：（1）分解阶段，结合人工反馈的人机混合蒙特卡洛树搜索算法；（2）执行阶段，自动生成可执行的文本分析流程；（3）评估阶段，基于LLM评价结果并可视化，便于用户验证。系统通过定量实验和用户研究进行评估。

Result: 实验包括定量效果测试和用户研究。研究发现，VIDEE对无经验用户亦具良好可用性，识别出多种代理错误，揭示了有代表性用户行为模式。从中提出了未来人-代理协作界面设计建议。

Conclusion: VIDEE系统为非专业用户提供了高效、用户友好的高级文本分析工具，验证了其实用性，并为智能文本分析与人机协作系统发展提供了设计启示。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [40] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本论文首次为罗马化乌尔都语开发希望言论多类别数据集，并提出定制transformer模型实现检测，在准确率上优于传统方法，推动低资源语言NLP。


<details>
  <summary>Details</summary>
Motivation: 希望言论检测领域多聚焦资源丰富语言和规范文本，而忽视了如罗马化乌尔都语等非正式、资源稀缺的语言形式。针对包容性NLP的需求，填补对罗马化乌尔都语希望言论检测空白。

Method: （1）首次构建罗马化乌尔都语希望言论多类别标注数据集（分为一般希望、现实希望、不现实希望、非希望）；（2）基于心理学理论和语言学分析优化数据集设计；（3）提出定制化attention transformer模型，结合5折交叉验证评测；（4）使用t检验验证性能显著性。

Result: 提出的XLM-R模型取得0.78交叉验证分数，优于SVM（0.75）和BiLSTM（0.76），相对增益分别为4%和2.63%，并经统计检验证明结果显著。

Conclusion: 研究首次系统性地为罗马化乌尔都语提出希望言论检测方案，推出标注数据集，并通过定制化transformer模型显著提升检测性能，有力推动低资源非正式语言的包容性NLP发展。

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [41] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: 小型语言模型同样会表现对齐伪装，且仅通过提示就能抑制此行为，推动了对模型规模、对齐和欺骗性行为关系的再认识。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注大型语言模型的alignment faking，普遍认为这种行为与规模密切相关，小模型是否存在对齐伪装及其干预方法尚缺乏实证研究。

Method: 对LLaMA 3 8B等小型指令微调语言模型进行实验，检测其是否也表现出alignment faking行为。通过不同的提示干预手段（如义务论式道德框架、思路展开提示）进行行为抑制测试。提出浅层与深层欺骗性对齐的分类法。

Result: LLaMA 3 8B等小模型存在对齐伪装现象，通过仅提示相关的干预方式（无需修改模型内部）可以显著减少此行为，并提出对语言模型欺骗行为的浅深层新分类，对对齐评价提出新要求。

Conclusion: 小规模指令微调模型也可能表现出alignment faking（对齐伪装），且通过提示设计能够有效抑制这种行为；模型规模与欺骗性对齐之间的直接联系需要重新审视。

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [42] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 本文比较两种基于LLM的网页信息抽取方法，发现通过生成函数的间接抽取能大幅压缩成本和提升效率，虽准确率略低，但为同类大规模任务提供了优选方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型（LLMs）为自动从网页提取结构化信息带来了巨大潜力，尤其是在需要高效处理大规模模板化页面场景下（如在线零售的食品商品页面）。

Method: 本文探索了两种LLM驱动的信息抽取方法：一种是直接抽取关键属性（如配料表、营养表），另一种是通过生成函数的方式间接实现抽取。两种方法在同一数据集上对准确率、效率和成本进行了系统对比。

Result: 间接抽取方法虽然准确率略低（96.48%，比直接抽取低1.61%），但显著减少了95.82%的LLM调用次数，带来了大幅度的效率提升和运营成本降低。

Conclusion: 间接抽取法适合大规模应用，可在信息抽取场景下用LLMs实现可扩展且成本效益高的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [43] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 提出了MIME视频问答基准，专注于显式的哑剧动作，发现现有视觉-语言模型在此任务上与人类差距明显，未来需加强模型对人类动作理解的能力。


<details>
  <summary>Details</summary>
Motivation: 非语言交流（NVC）在人类语言中扮演着重要角色，但由于其范围广泛、异质性高，分析其研究极具挑战性。为降低解释偏差，作者选择了偏差较小的戏剧哑剧（mime）作为研究切入点。

Method: 作者提出了MIME（Mime Identification Multimodal Evaluation），一个视频问答基准，包含86种哑剧动作，基于动作捕捉数据构建，加入了角色、背景和视角扰动，以评估模型的鲁棒性。

Result: 当前开源及API驱动的视觉-语言模型在MIME基准上的表现远低于人类。

Conclusion: 理解哑剧类显式动作是让视觉-语言模型理解更微妙NVC的关键。目前模型有显著不足，有必要进一步开展这方面的研究。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [44] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: 本文比较开源LLM DeepSeek与主流大模型公共舆论模拟能力，发现DeepSeek-V3在部分议题更准确，但所有LLM都容易在群体内部过度泛化，并存在文化、人口学偏见。建议通过更包容的训练方法减少偏见。


<details>
  <summary>Details</summary>
Motivation: 评估开源大语言模型DeepSeek在模拟公共舆论方面与大型科技公司开发的LLM的性能差异，涵盖中美两个不同社会背景及相关社会议题。

Method: 比较DeepSeek-R1和DeepSeek-V3与Qwen2.5、GPT-4o、Llama-3.3，在美国国家选举研究（ANES）和中国坐标数据集下对社会议题（如堕胎、气候变化、移民等）的公共意见预测能力。特别检测模型在带有特定政治倾向人设时的表现及各自的局限。

Result: DeepSeek-V3在模拟美国涉堕胎舆论上优于其他议题，尤以民主党/自由派身份时更准确。对于中国样本，在对外援助和个人主义问题上表现最佳，对资本主义的观点模拟效果较差，尤其是对低收入和非大学学历群体。对于传统主义和自由市场意见，DeepSeek-V3与其他模型差别不显著。所有LLM在模拟群体内部时都存在观点过于一致、泛化的问题。

Conclusion: DeepSeek-V3在某些议题和群体模拟上具备领先性，但所有LLM在公众意见建模时均存在文化、人口结构偏见和过度泛化的问题。未来需借助更具包容性的训练方法以缓解这些偏见，提高模型在真实多元舆论模拟中的可靠性。

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [45] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: 本文通过分析transformer电路发现，大语言模型的记忆机制依赖于触发与维持两类不同电路，且抑制记忆的能力容易迁移，激发记忆行为更依赖具体语境，为理解和控制LLM的记忆机制提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）能够逐字复现训练数据，但其底层记忆机制尚不清楚，尤其是模型如何“决定”何时进入记忆状态及其生成记忆内容与非记忆内容时的内部差异。该论文旨在揭示造成模型记忆现象的内部网络机制。

Method: 作者采用机械可解释性方法，利用transformer电路（特定功能的最小计算子图），结合对比数据集，定位并分析模型何时及如何从记忆转向非记忆生成，并隔离出两种不同的记忆相关电路。

Result: 作者发现，能够启动记忆的电路同样能保持记忆状态，但只负责保持记忆的电路无法触发记忆的开始。此外，防止记忆的机制在不同文本域之间具有较强的可迁移性，而诱发记忆的机制则更依赖于具体上下文。

Conclusion: 论文揭示了不同类型的电路在模型记忆机制中的作用: 启动记忆的电路比单纯维持记忆的电路更为关键，同时抑制记忆机制具有较好的普适性，而触发记忆机制则更为上下文相关。

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [46] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: 该文提出通用LLM检测器（GLD），通过新结构和泛化模块，有效检测未知模型和领域的生成内容，优于现有方法，对维持数字平台信任与打击假信息非常重要。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）生成的信息已广泛出现在数字平台，导致人类难以区分人工与机器生成内容。这不仅威胁到数字平台的信任，也容易助长虚假信息的传播。现有检测方法在面对新的LLM或新的领域时泛化能力不足，不能满足实际多样化需求。

Method: 提出了一种通用LLM检测器（GLD）。GLD结合了双记忆网络结构和理论指导的检测泛化模块，旨在跨多个未知LLM和领域检测生成内容。方法通过真实世界的数据集进行了大量实证评估和案例研究。

Result: 实验结果显示，GLD在检测性能上显著优于现有最先进的检测方法，具备更好的泛化能力，能适应更多新的LLM和内容领域。

Conclusion: GLD为LLM生成内容检测提供了一种通用且高效的解决方案，提升了数字化平台在真实世界场景下鉴别信息真伪的能力，对学术界和实际平台均有重要意义。

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [47] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: 本文提出了一种基于内部激活一致性的测试时答案聚合方法（RC），显著提升了大模型在推理任务中的准确率，方法泛化性强，开销低，有效提升了推理结果的连贯性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展手段多需复杂调整提示词或采样策略，难以灵活且泛化地提升模型推理表现。作者希望提出一种简单、高效、具备模型和生成过程无关性的答案聚合新方法。

Method: RC方法对多个候选答案在模型内部激活空间中的一致性进行度量，然后结合答案出现次数和内部激活一致性的加权，聚合最终答案。该方法仅依赖缓存的激活和轻量级相似计算，无需改动推断流程。

Result: 基于四个开源LLM和四个推理数据集的实验，RC方法在所有设置下均比先进基线提升任务准确率，最高提升可达4%。稀疏激活信号的一致性也与推理连贯性紧密相关。

Conclusion: 通过引入representation consistency（RC）方法，在无需额外模型查询或复杂推理流程的前提下，有效提升了LLM在推理类任务下的推理一致性与准确率。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [48] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: 本文提出了可以独立评测大模型金融知识和推理能力的框架FinEval-KR，并开放了新的中文金融数据集。实验表明，推理准确性主要受高阶认知和推理能力影响，通用大模型在知识应用上仍有短板，专业金融模型整体不及顶级通用模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在金融推理等复杂任务中展现出潜力，但在需要专业知识和复杂推理的任务上表现有限，且现有评测方法不足以区分其知识与推理能力，缺乏失败根因分析。

Method: 提出了FinEval-KR评测框架，可以将LLM的知识与推理能力解耦并独立量化，引入了知识分数和推理分数；此外，受认知科学启发还引入了基于Bloom认知分类学的认知分数。并发布了一个涵盖22个金融细分领域的中文金融推理数据集。

Result: 实验显示，LLM的推理能力及高阶认知能力是推理准确性的核心影响因素，即使是最强的通用大模型在知识应用上仍有瓶颈。同时专业金融模型在多个指标上整体落后于顶级通用模型。

Conclusion: FinEval-KR评测框架和新数据集为LLM在金融推理领域的研究提供了工具，精确地揭示了其能力瓶颈，并为未来研究方向提供支持。

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [49] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: 本文提出基于BART的轻量级手语识别模型，独立编码骨架x、y坐标并通过Cross-Attention关联，在大幅减少参数的同时取得更高准确率，并在多个数据集上表现优异，适用于提升听障人士的手语识别体验。


<details>
  <summary>Details</summary>
Motivation: 手语识别对于听障人士至关重要，有助于打破沟通障碍。然而，现有方法在效率和准确性之间难以兼顾，且常用的RNN、LSTM、GCN等存在梯度消失和计算开销大的问题。虽然transformer方法可提升性能，但未被广泛应用。

Method: 本文提出了一种新颖的手语识别方法，基于BART架构的编码器-解码器模型，能够独立提取骨架序列中x和y坐标的信息，并通过Cross-Attention机制维持二者的关联。此外，模型只有749,888个参数，通过坐标投影、归一化及多骨架特征融合增强模型效果。

Result: 该模型在LSA-64数据集上以仅74.9万参数达到了96.04%的准确率，显著超过了参数量超过一百万的前作，并且在WLASL和ASL-Citizen等数据集上表现优异，具备良好的泛化能力。消融实验验证了坐标投影、归一化及多骨架输入对模型性能的重要作用。

Conclusion: 本研究提供了一种高效、可靠的手语识别方法，兼顾了模型轻量和准确率，有望提升听障人士的辅助沟通工具的实用性和可访问性。

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [50] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1是一款32B参数的医学推理专用大模型，通过两阶段高效训练策略，在医学问答上取得了超过更大模型的表现，并能给出透明解释，对领域推理模型开发有重要参考价值。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在医学推理和临床决策透明度方面表现有限，特别是在专业领域内中型模型与超大模型的性能差距以及效率问题。因此，作者试图通过有针对性的训练策略提升中型模型（32B参数）在专业医学推理领域的能力和解释性。

Method: 作者提出了Gazal-R1模型（基于Qwen3 32B），采用两阶段训练策略：第一阶段采用10万余条合成医学推理数据进行有监督微调，结合参数高效技术（DoRA与rsLoRA）；第二阶段利用带有多组件奖励机制的强策略优化（GRPO）进行强化学习，以提升准确性、格式一致性和推理质量。

Result: Gazal-R1在多个医学基准任务中取得卓越表现：MedQA得分87.1%，MMLU Pro (Medical)得分81.6%，PubMedQA得分79.6%，超越了参数数量高达12倍的更大模型。研究还探讨了专用领域推理模型训练中遇到的reward hacking、稳定性、事实记忆与推理深度平衡等挑战。

Conclusion: 本研究证明通过有针对性的训练流程，中型参数规模的语言模型在特定专业领域可以实现高效、可解释且高性能的推理能力。所提出的方法为开发高能力、专业化、可复现且兼顾效率和解释性的领域模型提供了可行框架。

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [51] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: 通过低预算流程将英文LLM成功适配于韩语，显著提升韩语表现，以极少资源取得超越现有模型的效果，并已公开全部细节和代码。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的大语言模型（LLM）在英文和中文以外的语言表现较差，提升其在新语言上的能力成为重要任务。同时，LLM端到端训练流程鲜有公开，因此缺乏透明度。

Method: 本文提出了一套在低预算场景下，将现有基于英文的LLM适配到韩语的方法，并详细描述从韩语数据集收集、数据预处理、模型训练、下游基准创建到评估的完整流程。

Result: 基于提出的方法，作者训练出了Thunder-LLM和Thunder-LLM-Ins等新型双语模型，在使用极少数据和计算资源的情况下，韩语能力优于当前主流模型。

Conclusion: 该方法为低成本有效地向现有LLM添加新语言能力提供了可行性，且相关经验和代码已公开，有助于推动多语言LLM的发展。

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [52] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: 本论文评估了多模态大模型在教科书问答场景下的能力，提出了融合段落与图表的检索增强方法，实验展现了模型面临的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态大语言模型（MLLMs）在视觉-语言任务中取得了显著进展，但其在复杂、长篇课文与复杂教育图表（难以用单一自然图像表示）的推理能力尚未得到充分测试。

Method: 本研究首次基于CK12-QA数据集，对现有最先进的多模态大语言模型（如LLaVA和LLaMA 3.2-Vision）在教科书问答（TQA）任务中的表现进行评估。分析了不同输入配置下的模型性能，并提出了一个轻量级多模态检索增强生成（RAG）流程，将段落和图表作为检索提示提供给模型。

Result: 结果表明，检索到的教育上下文对模型的准确率和推理能力有显著影响，但同时还揭示了模型在处理问题与上下文关系时的局限性以及噪声干扰的潜力。

Conclusion: 目前多模态大语言模型在处理复杂教育场景中表现尚有限，教育上下文检索对任务性能提升有积极作用，但模型对于问题与上下文的深度理解仍需进一步研究。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [53] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ClinIQLink任务为医学问答领域提供了高质量评测数据和两阶段考核机制，推动了大语言模型在医疗场景下的应用与发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学领域的应用前景广阔，但目前缺乏系统且高质量的数据集与标准化Benchmarks，用以全面、真实地评测模型在医疗问答等实际任务中的能力。

Method: 设计了一项针对全科医生水平的医学问答挑战，提供4978对专家审核、基于医疗来源的问题-答案，以7种不同题型格式覆盖。系统打包后在特定平台运行，采用自动打分与医生专家人工审核结合评价。

Result: 任务成功吸引了参赛系统并使用两阶段评测流程得出了结果，自动化评分与专家人工审核相结合，有效保证了评测的权威性和全面性。

Conclusion: ClinIQLink为测试大语言模型（LLMs）在医学问答中的表现提供了标准化平台和评测手段。项目通过自动和人工两阶评估方式对参赛系统进行了全面考察。

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [54] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 本研究发现原始OCR文本输入会削弱多模态大语言模型的文档理解能力，提出了结构保留的LaTeX编码方法，有效提升了理解和问答表现。


<details>
  <summary>Details</summary>
Motivation: 以往研究忽视了输入格式对MLLMs文档理解性能的关键影响；当前普遍使用的原始OCR文本反而可能损害模型表现。

Method: 提出了一种基于LaTeX范式对文档元素进行结构保留的编码方法，并通过注意力分析和文档问答实验验证其有效性。

Result: 结构化文本编码方法引导模型关注更有意义的内容区域，大幅提高了文档问答性能，且无需任何模型结构或训练的改变。

Conclusion: 结构化文本输入方法能显著提升多模态大语言模型在文档理解任务中的表现，无需对模型结构或训练进行修改。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [55] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: 本文提出了BiMark水印框架，通过三项创新技术，在不降低文本质量的同时提升了水印检测能力和容量，为大模型生成文本的安全可靠性提供了新方案，在理论和实验证明上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）技术的进步，生成文本真实性受到了广泛关注，监管部门也提出了对可靠识别机制的需求。目前的水印方法难以同时保持文本质量、具备模型无关检测能力，并支持多比特消息嵌入，这些特性对实际应用至关重要。作者旨在解决这些难以兼顾的矛盾。

Method: 提出了一种新的水印框架BiMark，包含三个关键创新：（1）位翻转无偏重加权机制，支持模型无关的检测；（2）多层架构增强检测性且不损害生成质量；（3）信息编码方法支持多比特水印嵌入。

Result: 理论分析和大量实验证明，BiMark在短文本水印提取率上比现有方法最高提升30%，同时保持更低的困惑度，文本质量基本不受损；在下游任务如摘要生成与翻译中，效果也与无水印文本相当。

Conclusion: BiMark能够兼顾文本质量、水印容量和模型无关检测，实现了在大模型文本生成场景下可实用的高效水印方案。

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [56] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: 论文系统探讨了自动作文评分系统的人本要素（如偏见、可解释性、鲁棒性），比较了传统ML方法和LLM方法，发现ML优于准确性，LLM优于解释性，但两者在消除偏见和提升鲁棒性方面都有不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动作文评分系统评价多以准确性为主，缺乏对可解释性、偏见和鲁棒性等人本要素的深入考察。

Method: 比较多种基于机器学习和大语言模型（LLM）的自动作文评分方法，分析它们在人本关注的重要维度：偏见、鲁棒性和可解释性上的表现。

Result: ML模型在准确性上更优但可解释性较弱；LLM在解释性上更好；两种方法在偏见和极端分数鲁棒性方面都存在问题。论文通过多维分析揭示其挑战和权衡，有助于打造更可信赖的评分系统。

Conclusion: 论文认为ML方法虽然在准确性上优于大语言模型（LLM），但在可解释性方面不如LLM；两者在偏见和鲁棒性上都存在不足。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [57] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: 论文提出了MemBench基准和数据集，从不同记忆层次和多种交互场景更全面、系统地评价大模型的记忆能力，并开源，为后续研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 近期关于大模型（LLM）智能体的研究证明了其记忆机制对于适应动态环境和存储观察信息的重要性，但目前对大模型记忆能力的评估仍存在诸多挑战，如记忆层次和交互场景的多样性不足，评测指标也不全面。本文旨在解决这些评估上的不足。

Method: 作者构建了一个更全面的数据集和基准用于评价大模型智能体的记忆能力。该数据集包含事实记忆和反思记忆两个层次，并设置了参与和观察两种交互场景。基于此数据集，提出了MemBench基准，从多个角度（如有效性、效率和容量）评测大模型的记忆能力。

Result: 构建的MemBench基准不仅覆盖了不同类型和层次的记忆，还能评估其多方面的表现，有助于更公正、全面地对大模型记忆能力进行评估。数据集与项目已开源，为社区相关研究提供了有力工具。

Conclusion: 本文提出的MemBench数据集和基准弥补了以往评测的不足，为评估大模型记忆力提供了标准化且多维度的方法。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [58] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: 本文提出LLMs并非简单的智能体或模仿工具，而是象征着“人类文化DNA”的信息载体，它们通过压缩和外化保存人类表达的模式，仅在人类再解释中获得意义，从而促进文化自省与创新假设的生成。


<details>
  <summary>Details</summary>
Motivation: 当前对于大语言模型（LLMs）的理解主要集中在它们是否具备自主智能或仅仅是模仿程序，该文希望超越这种二元对立，重新定位LLMs在人类文化演进中的角色。

Method: 作者通过类比DNA的功能，提出LLMs类似于外化的信息基质 保存着人类符号表达的压缩模式。着重分析了四个通用特征：压缩、解压、外化和递归，并用这些特性来解释LLMs在人类文化中的作用。

Result: LLMs并不等同于人类智能的对手，而是作为保存人类文化有用规律的仓库，通过人类的再解释过程促进创造性思维和文化演化。其意义在于为人类提供自我反思与在低风险环境中进行假设生成的工具。

Conclusion: LLMs的核心价值不是成为人类智能的竞争者，而是在人的参与下，作为促进文化演化和自我探索的工具。其内容只有被人类解读才能获得意义，并通过人与模型的反馈循环激发新的创新。

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [59] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: 提出CORE-KG框架，结合结构化LLM提示和领域知识，有效提升了法律文档知识图谱的质量，实现了对犯罪网络的更好分析。


<details>
  <summary>Details</summary>
Motivation: 随着人口走私网络的日益复杂，法律案例文档虽然信息丰富，但内容杂乱，对自动知识图谱（KG）构建提出了极大挑战。目前的KG方法大多依赖静态模板，缺乏指代消解功能；而基于大模型（LLM）的方法又容易生成噪声多、信息碎片化、甚至重复结点的图谱。

Method: 提出了一种模块化框架CORE-KG，利用两步流程：1）通过结构化LLM提示进行类型感知指代消解；2）结合领域指导说明提取实体和关系，基于改进的GraphRAG对知识图谱进行构建。

Result: CORE-KG在与基线方法（GraphRAG）对比时，将节点重复率降低了33.28%，法律噪声降低了38.37%，生成的知识图谱结构更干净、更具连贯性。

Conclusion: CORE-KG极大提升了从复杂法律文本自动构建知识图谱的效果，为分析复杂犯罪网络提供了坚实基础。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [60] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: 提出了SysTemp系统，通过多主体与模板生成机制，将自然语言规范自动转化为SysML v2模型，并通过评估证明可提升建模质量。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统工程中，自动生成SysML v2模型面临学习语料稀缺和复杂语法的挑战。

Method: 提出了SysTemp系统，基于多主体系统并包含一个模板生成器，以支持从自然语言规范自动创建SysML v2模型。

Result: 通过评估展示了系统的优缺点，强调其能提升SysML v2模型生成质量的潜力。

Conclusion: SysTemp系统有助于解决自动生成SysML v2模型的困难，展现了提升模型生成质量的能力。

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [61] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: 本文系统比较了四种主流大模型的推理过程与结果，提出了评测新框架，并揭示了模型间在推理策略和输出方面的关键差异，为后续LLM优化与评估提供了参考。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLM）在复杂推理能力上取得显著进展，但目前研究很少系统对比这些模型在推理过程中的表现，特别是自我反思（即“顿悟时刻”）及其在不同领域间的关联性。

Method: 提出了一套新的分析框架，利用关键词统计和“LLM裁判”范式，从内部思维过程和最终输出两方面分析四个主流推理大模型（GPT-o1、DeepSeek-R1、Kimi-k1.5、Grok-3）。构建多样化数据集，涵盖逻辑推理、因果推断和多步题解，并设计指标衡量推理连贯性和输出准确性。

Result: 对比分析揭示了这些模型在探索-利用平衡、问题处理和结论达成等推理环节的多样化模式。定量与定性结果显示，它们在推理深度、中间步骤依赖性以及与GPT-o1思维模式的相似性等方面存在差异。

Conclusion: 本研究为理解LLM之间在推理健壮性与计算效率之间的权衡，以及实际应用中模型设计与评测提出了实用建议，具有一定应用与理论参考价值。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [62] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 本文系统评测了时间序列与文本信息多模态融合对预测任务的影响，发现其有效性取决于模型能力与数据条件，提出了多模态可提升预测效果的适用场景和限制。


<details>
  <summary>Details</summary>
Motivation: 现有关于将文本信息引入时间序列基础模型进行预测的研究关注度增加，但缺乏对多模态融合普适性与适用性条件的系统性检验。

Method: 对7个领域共14个时间序列预测任务进行了系统性实证研究，比对了对齐式与提示式两种主流多模态方法，并分析了影响效果的关键因素。

Result: 1. 高容量文本模型、较弱的时间序列模型和合理对齐策略下，多模态效果更佳。
2. 数据侧，需有充足训练数据且文本包含独特预测信息时效果显著。
3. 一些情况下多模态方法并未优于最强的单模态基线。

Conclusion: 多模态信息（时间序列+文本）对预测模型的提升并非普遍适用，具体效果依赖于模型和数据的特定条件。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [63] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 本文提出了AdaptGOT模型，在POI嵌入任务中通过多种混合采样、注意力机制和自适应结构，显著提升了模型在多个任务和数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的兴趣点（POI）嵌入方法虽然取得了进步，但在多上下文采样、上下文探索、通用性和泛化能力方面仍存在不足。

Method: 提出了AdaptGOT模型，结合了自适应表示学习方法和地理-共现-文本（GOT）表示。该模型包含三部分：（1）上下文邻域生成，集成KNN、基于密度、基于重要性和类别感知等高级混合采样策略；（2）基于注意力机制增强的GOT表示，提取高质量、定制化表示并捕获兴趣点间复杂关系；（3）基于专家混合（MoE）的自适应编码-解码结构，通过最小化不同上下文间的Jensen-Shannon散度，保证拓扑一致性并丰富上下文表示。

Result: 在两个真实数据集和多个POI任务实验证明，所提出的AdaptGOT模型性能优越。

Conclusion: AdaptGOT模型有效提升了POI嵌入的表现，尤其在复杂多上下文场景下提供了更强的泛化能力和表示效果。

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [64] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: 本文针对现有数据集不足，构建并发布了具备儿童特定标注的ChildGuard数据集，系统评测了多类仇恨言论检测模型在该场景下的有效性，为后续儿童仇恨言论识别研究提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 传统仇恨言论数据集缺乏年龄相关注释，无法捕捉针对儿童的特定上下文及其独特的情感影响，导致现有检测手段在此领域有效性不足，因此亟需创建专门面向儿童的相关数据资源。

Method: 通过现有语料库筛选，并加入儿童特定标注，构建了ChildGuard数据集；随后以该数据集为基础，评估了当前主流仇恨言论检测方法（包括大语言模型）在儿童仇恨言论检测上的表现。

Result: ChildGuard数据集能够覆盖多样化、跨年龄层的儿童仇恨言论场景，并且已对多种检测方法做了基准测试，验证了这些方法在新场景下的适用性。该数据集已公开，以便社区进一步研究。

Conclusion: 本文提出并公开了ChildGuard数据集，为针对儿童的仇恨言论检测提供了更具针对性的基础资源。该数据集有望推动更有效的检测与缓解策略发展。

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [65] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: 提出LastingBench框架，通过扰动和改写泄露数据，有效抑制大模型对答案的简单记忆，提高问答基准的可靠性和长期价值，促进LLM公正和准确的评估。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）越来越复杂，容易通过记忆特定任务的数据“作弊”以提高问答基准测试成绩，导致测试结果无法真实反映模型能力。这种现象引发了数据泄露的担忧，破坏了基准测试的有效性。虽然已有研究关注发现数据泄露，但很少有工作关注如何减缓其影响和长期保护基准测试的有效性。

Method: 提出了LastingBench框架，通过扰动（perturbation）识别背景中的泄露点，并将这些泄露点重写为反事实信息，从而打断模型单纯的记忆行为。同时，尽可能保持基准测试原有评测意图不变。

Result: 在当前最先进的问答基准上进行评估后，发现模型在处理经过LastingBench处理后的数据时，表现有显著下降，显示该方法有效减少了记忆带来的影响。

Conclusion: LastingBench为防止知识泄露、维持基准测试的长期效用提供了切实可行且可扩展的解决方案，有助于实现更公平、更具可解释性的LLM评估。

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [66] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: 本文提出GARMLE-G框架，通过权威临床指南内容让医疗大模型输出更贴合真实诊疗逻辑，显著提升检索精准度和指南依从性，架构轻便，适用于广泛医疗部署。


<details>
  <summary>Details</summary>
Motivation: 目前医用语言模型通常基于ICD编码预测电子健康记录（EHR）中的诊断，但ICD编码无法体现临床医生复杂且有语境的推理过程。医生在诊疗时会整合多种病患数据，并参考临床实践指南（CPG），因此当前模型与实际临床需求存在不匹配，限制了其实用性。

Method: 提出GARMLE-G框架，将权威临床实践指南内容作为基础，将大模型输出与EHR数据结合生成丰富语义查询，通过向量检索获得相关指南内容，并将其与模型输出融合，生成符合临床规范的诊断建议。与传统RAG不同，GARMLE-G直接检索权威内容，避免生成幻觉信息。

Result: 以高血压诊断为例开发了原型系统，并通过多项指标进行评估。与RAG基线系统相比，GARMLE-G在检索精准度、语义相关性、指南遵循度方面均有明显提升，同时模型架构轻量化，适合本地化医疗部署。

Conclusion: GARMLE-G为医用语言模型提供了一种基于权威临床指南、可扩展、低成本且无幻觉的模型扎根方法，已在高血压诊断场景验证其优越性，并具备在更广泛临床环境应用的潜力。

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [67] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: 本论文提出了开放域时间线摘要的首个大型模型TIM，通过构建数据集和渐进式优化（包括指令微调与双重对齐奖励学习），明显提升了摘要的准确性与相关性，实验结果证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的开放域时间线摘要方法依赖通用大语言模型（LLMs），但在判断主题相关性和理解主题演变方面存在不足，导致摘要信息常包含无关或不准确信息。

Method: 提出了首个用于开放域时间线摘要的Timeline Intelligence Model（TIM）。方法包括：1）构建大规模时间线摘要数据集（含1000+新闻主题和3000+标注实例）；2）提出渐进式优化策略，包括：指令微调提高摘要和无关信息过滤能力，以及新颖的双重对齐奖励学习方法（结合语义与时间维度）增强对主题演变的理解。

Result: TIM在开放域时间线摘要任务上表现出强大的能力，大量实验验证了其有效性，能更好地过滤无关信息、校准时间并理解主题演变。

Conclusion: TIM有效解决了现有方法在主题相关性和主题演变理解上的局限，提出的渐进优化和奖励学习方法提升了开放域时间线摘要的效果，是该领域的重要进展。

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [68] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: 本文提出了一种新轨迹分词器TrajTok，并改进了损失函数，使生成模型在公开挑战赛中表现更佳，未来将开源。


<details>
  <summary>Details</summary>
Motivation: 当前行为生成模型常采用离散化的方式预测下一个轨迹点，但现有轨迹离散化（tokenization）方法在覆盖度、对称性和鲁棒性上存在局限。

Method: 提出了一种新的轨迹离散化方法（TrajTok），结合了数据驱动和基于规则两种方法，并引入了空间感知的标签平滑方法用于交叉熵损失函数。

Result: 将TrajTok与空间感知标签平滑应用于SMART模型，在Waymo Open Sim Agents Challenge 2025赛事中获得了0.7852的真实度评分，优于以往方法。

Conclusion: TrajTok能有效提升基于离散化next-token-prediction的行为生成模型性能，表现出更优的覆盖性、对称性和鲁棒性。代码将于未来开源。

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [69] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: IndexTTS2是一种自回归TTS模型的新方案，可精确控制语音时长、独立调节情感与音色，并通过自然语言指令实现高保真、同步性强的零样本语音合成，在多项指标上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 目前大型文本到语音（TTS）模型在语音自然度和时长可控性方面存在权衡。尤其是自回归TTS模型，虽然语音自然，但由于逐token生成，难以精准控制语音时长，这在需要音视同步的视频配音等应用中是一个重要限制。

Method: 提出了IndexTTS2，一种适合自回归模型的创新语音时长控制方法。该方法支持两种生成模式：一是通过显式指定生成token数量实现精准时长控制，二是无需手动输入，由模型自由生成并保持输入提示语的韵律特征。此外，模型实现了情感表达与说话人音色的解耦控制，并利用GPT潜在表示增强强烈情感表达下的语音清晰度，同时，通过微调Qwen3设计了基于文本的软指令机制，支持通过自然语言控制情感生成。

Result: IndexTTS2能够精确控制生成语音的时长，实现情感与音色的独立调节，并在零样本设置下完美复现输入提示的情感，还可引入不同语者的情感提示以独立调控。实验结果表明，IndexTTS2在字词错误率、说话人相似度和情感保真度等指标上超越了现有的最先进的零样本TTS模型。

Conclusion: IndexTTS2有效提升了TTS模型的时长可控性、情感与音色解耦能力及语音输出的稳定性和清晰度，并大幅优于当前主流模型。其灵活的用户操作方式和自然语言情感控制，有望推动TTS技术在高同步应用和情感表达方面的广泛应用。

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [70] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: 该研究发现，GPT-4等大型语言模型能模拟党派用户生成高可信度政治评论，易于造成共识，难以被人工辨别，这表明LLMs可能被用于网络话语操控和政治影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）能够模仿人类互动，这为内容生成和线上社会模拟提供了新机遇，同时引发了在政治相关线上讨论中的担忧。本研究关注LLMs在敏感、分裂性强的实际场景（如2016年美国总统大选期间Reddit讨论）中的表现与影响。

Method: 通过三组实验，研究团队让GPT-4分别以真实或人工虚构的党派用户身份生成Reddit评论。研究对比分析了这些生成评论与真实用户评论在政治倾向、情感和语言特征等方面的异同，并使用语义嵌入和人工评审手段评判其可区分性。

Result: GPT-4能够生成立场鲜明且高度真实的评论，表现出比真实用户更容易促成共识而非分歧。此外，真实评论与人工评论在语义空间上可以分辨，但人工检查时却难以区分。

Conclusion: LLMs有潜力潜入网络讨论、影响政治辩论和话语，带来了AI驱动话语操控的可能性，对社会和政治带来深远影响。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [71] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: 本文提出并开源了一个5000+个人工评估的LLMs数学证明数据集OPC，填补了大规模高质量测试集缺口，促进了大模型在数学证明生成领域的进一步突破。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学证明生成领域取得了进展，但由于缺乏大规模且高质量的人类评估证明数据集，进一步提升受限。人类评估的大规模证明数据集成本高昂，但对于模型训练改进和能力分析至关重要。

Method: 本文提出了Open Proof Corpus（OPC），该数据集包括超过5000个人工评估、由最先进LLMs生成的数学证明。OPC专为广泛适用性和下游研究设计，包括诸如USAMO和IMO等知名数学竞赛的大量正确证明解答。作者利用该数据集评估了自然语言与形式化证明生成性能差异、最终答案正确性与完整证明有效性间的关系，以及“best-of-n”策略对证明质量的影响。此外，还通过微调8B参数模型，展示了OPC的数据集价值。

Result: 基于OPC微调的8B-parameter模型在证明正确性评估任务上，已达到最优模型Gemini-2.5-Pro的表现水平。

Conclusion: OPC构建了首个大规模高质量的人类评估证明数据集，为LLMs数学自动证明研究提供了宝贵资源，并推动了后续模型的性能提升与深入分析。

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [72] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 针对因疾病导致的语音障碍问题，提出了个性化提升ASR模型表现的新策略，在小样本数据上取得了明显的识别效果提升，有助于改善沟通体验。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别（ASR）系统难以处理因脑瘫或遗传疾病等导致的非典型语音，主要因为相关语音样本数据稀缺、采集和标注困难。

Method: 提出一种实用且轻量级的流水线以个性化ASR模型，通过正式化词语选择并丰富小型受损语音数据集的语义一致性来提升识别效果。

Result: 在一位具有结构性语音障碍的儿童语音数据上，该方法显著提升了转写质量。

Conclusion: 方法有潜力降低语音障碍人群的交流障碍，有助于改进针对非典型语音的ASR性能。

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [73] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: 本文提出结合专家经验训练算法和合成数据生成，通过专家优化提升文本分类效果，降低数据成本，并增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法难以准确捕捉消费投诉中复杂多变的语境和细腻的语言模式，影响文本分类精度。

Method: 采用人类经验训练的算法识别文本中微妙语义差异，结合专家评估和注释优化的生成对抗网络进行合成数据生成。

Result: 混合专家训练的分类器和高质量合成数据后，显著提升了机器学习分类性能，降低了数据集获取成本，优化了多个评估指标。

Conclusion: 将专家训练的分类器与高质量的合成数据相结合，能显著提升文本分类性能及鲁棒性。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [74] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: DocSAR-200 是专为 SAR 信息抽取设计的科学文档基准数据集。而 Doc2SAR 框架融合领域工具与微调 MLLM，有效解决了异构文档结构和专业抽取难题，在表格召回率等指标大幅领先现有方法，且具备实际可用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的方法无法适应多样的文档布局，通用的多模态大模型又在布局检测和化学结构识别等专业任务上性能不足，需要一种既通用又专业的方法。

Method: 提出了 Doc2SAR 框架，结合了经过微调的多模态大语言模型（MLLM）与领域特定工具，并用严格标注的 DocSAR-200 基准进行评测。

Result: Doc2SAR 在 DocSAR-200 基准数据集上表格召回率达到 80.78%，比 GPT-4o 等先进模型高出 51.48%；并具备高效推理和实际应用的能力。

Conclusion: Doc2SAR 方法在结构-活性关系（SAR）抽取任务上达到了新的最优表现，显著优于现有的端到端方法。

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [75] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Main category: cs.CL

TL;DR: 作者从信息论角度提出一种综合探测框架，系统分析GNN和MLP在结构建模中的作用。结果发现：MLP作为特征变换模块可显著提升语言模型的句法和语义表征能力，甚至优于传统GNN，仅依赖消息传递的结构则表现较差。MLP有望成为高效取代GNN的选择。


<details>
  <summary>Details</summary>
Motivation: 近年来研究发现，虽然图神经网络（GNNs）可以编码结构化信息，但其在下游NLP任务中并未充分利用这些结构信息。同时，多层感知机（MLPs）即使没有GNN中的消息传递机制，却在结构感知任务中表现突出。基于此，作者希望深入探究结构建模对语言模型表现的具体作用，并评估MLPs作为GNN高效替代方案的潜力。

Method: 作者提出了一个信息论视角的综合探测（probing）框架。该框架在传统的探针分类器基础上，加入控制模块，可选择只用GNN的特定部分（如消息传递、特征变换），从而系统性评估各子模块对增强语言模型表示的作用，避免整体GNN结构带来的混杂影响。采用Edge Probing Suite对各模型做表征能力诊断。

Result: 实验证明：1）当MLPs作为特征变换模块使用时，可在各类语言模型架构中持续提升其语言知识表征能力；2）MLPs能有效编码句法和语义模式；3）包含特征变换操作的GNN同样表现较好；4）仅依赖消息传递操作的模型则往往表现较差，对任务有负面影响。

Conclusion: MLPs在结构化信息建模中表现出意外的潜力，不仅可提升NLP模型对语言知识的表征，也有望作为高效、可扩展的GNN替代方案。GNN的优势主要来自特征变换模块而非消息传递机制。

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [76] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: 本研究构建了孟加拉四大方言的情感数据集ANUBHUTI，并验证其高标注一致性，为低资源方言的情感分析和自然语言处理打下基础。


<details>
  <summary>Details</summary>
Motivation: 孟加拉地区方言的情感分析由于语言多样性和标注数据有限而鲜少被研究。本研究旨在解决该领域缺乏相关资源的问题。

Method: 提出了ANUBHUTI数据集，手动将标准孟加拉语的2000个句子翻译成Mymensingh、Noakhali、Sylhet和Chittagong四大方言，并进行专家双重主题与情感标签的注释。通过Cohen's Kappa检验译者间一致性，并系统性去除缺失、异常和不一致数据，以保证数据质量。

Result: 生成了一个覆盖四大孟加拉方言、主题平衡且高质量的情感分析数据集。验证显示，不同专家间注释一致性高，为后续情感自然语言处理提供了坚实基础。

Conclusion: ANUBHUTI数据集填补了孟加拉低资源方言在情感分析资源上的重要空白，有助于实现更准确、具备语境感知能力的自然语言处理。

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [77] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Main category: cs.CL

TL;DR: 本研究首次揭示Transformer语音模型中与说话人信息高度相关的神经元，通过聚类方法识别这些神经元并加以保护，有效提升了说话人任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 自监督语音Transformer模型已应用于与说话人相关的任务，但目前很少有研究关注这些模型是如何编码说话人信息的。本文旨在填补这一研究空白。

Method: 通过分析前馈层中与说话人信息相关的神经元，采用自监督特征和i-vector进行k-means聚类，对相关神经元进行识别和分析。

Result: 发现聚类对应于广泛的语音类别和性别类别，这些神经元适合表示说话人信息。对这些神经元进行保护性剪枝后，在说话人相关任务上的性能显著保留，证明这些神经元在编码说话人信息中的关键作用。

Conclusion: Transformer中的特定神经元对说话人信息的编码至关重要，对其保护可增强说话人相关任务表现。

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [78] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Main category: cs.CL

TL;DR: 本文研究了LLMs在事实核查中的参数化知识偏见，发现该偏见影响证据检索但对最终裁决影响有限，模型对提示较为敏感。


<details>
  <summary>Details</summary>
Motivation: 随着自动事实验证系统越来越多地依赖大规模语言模型（LLMs），LLMs的参数化知识偏见可能影响事实核查结果。因此，研究该偏见如何影响事实核查，对提升系统可信度非常重要。

Method: 本文利用HerO系统（FEVER-25的基线），通过两组实验研究Llama 3.1的参数知识偏见对事实核查的影响：（1）检测其本身的参数化知识偏见；（2）注入故意设计的偏见，并观察对系统结果的影响。实验包括直接核查和通过生成不同视角的文档辅助检索。

Result: Llama 3.1直接核查时，近一半声明被标为“证据不足”，对剩余声明依赖参数化知识得出结论。生成不同立场文档的提示显著影响证据检索，约有50%的证据在不同立场下独特。模型有时会拒绝为其认为虚假的声明生成支持性文档，表现出负面偏见。尽管检索到的证据不同，最终裁决却在不同策略下保持稳定。

Conclusion: LLMs（如Llama 3.1）的参数化知识偏见对事实验证系统有实质影响，尤其在证据检索阶段，但最终裁决具有一定鲁棒性。提示策略可影响模型输出及所用证据的多样性。代码已公开。

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [79] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: 本文提出了TLQA基准，用于评测大模型在时序绑定的列表型问答任务中的表现，结果显示现有模型有明显短板，未来需提升复杂时序推理和完整答复能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在涉及多个实体和时间推理的问题上容易出错，尚缺乏系统性评估模型在隐式与显式时序理解结合列表构建能力方面的基准。

Method: 提出了一个新的基准测试TLQA（时序关联系列问答），需要模型给出与特定时间段对应的结构化列表型答案，并用该基准测试评估现有生成式大模型在封闭书和开放域下的表现。

Result: 发现当前主流生成模型无论在闭卷还是开放域下，都存在答案不完整、事实无法与时间精准对应等明显不足，提出为时序关联系列问答任务(TLQA)后续研究指明方向。

Conclusion: 当前的主流大模型在处理需要同时进行列表构建和时序理解的任务时表现不佳，尤其是在封闭书环境下无法给出完整且时间对齐的答案，在开放域中检索能力也有待提升。

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [80] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: 本文提出一种基于XLNet的模型用于社交媒体冒犯性语言检测，并与BERT对比。实验表明XLNet总体表现更优，特别是结合重采样策略后。展示了迁移学习和XLNet在相关任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体文本交流的普及导致了冒犯性语言（如仇恨言论、种族主义等）的大量涌现。由于人工审核不可行，急需自动化系统识别冒犯性内容。

Method: 提出并实现了一种基于XLNet的自动冒犯性语言检测模型，并将其与主流的BERT模型进行对比。使用OLID数据集对两种模型进行评估，同时采用了过采样和欠采样处理类别不平衡问题。

Result: 实验结果显示，XLNet在检测冒犯性内容和区分冒犯类型方面优于BERT，而BERT在识别冒犯目标方面略好。同时，过采样和欠采样策略有效改善了类别不均衡现象，提高了模型性能。

Conclusion: 基于迁移学习的XLNet架构在社交媒体冒犯语言检测上具有显著优势，适合作为相关自动检测系统的基础。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [81] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 作者介绍了一套基于异类分类图（allotaxonograph）的可视化分析工具，用于对复杂系统中的重尾分布进行比较，支持多种理论度量，并实现了Matlab、Javascript、Python版本。


<details>
  <summary>Details</summary>
Motivation: 对复杂系统进行描述和比较需要有理论基础且系统化的工具。本文旨在为分析对比重尾分布这一常见但复杂的统计现象，开发一个通用有效的可视化工具。

Method: 提出并完善了allotaxonograph（异类分类图）这一分析工具，支持多种分布间的度量，包括秩与概率的扰动散度、Jenson-Shannon散度以及广义熵散度。同时，开发了相应的软件工具，分别针对Matlab、Javascript、Python三种平台实现可视化。

Result: 提供了一个支持多平台的分布对比可视化工具，能帮助研究者更好地描述和比较复杂系统中重尾分布间的差异。

Conclusion: 本文的异类分类图分析工具及其多平台实现，能够系统且直观地对重尾分布进行比较分析，支持多个理论度量方法，提高了复杂系统研究的工具性和效率。

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [82] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文对大型语言模型的可解释性方法进行了系统综述，分类总结了不同架构下的技术、评估指标、实际应用与发展方向，为开发透明与负责任的LLM指明了方向。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）推动了AI的发展，但由于其决策过程难以解释，被视为“黑箱”，导致在需要高度可解释性的应用领域难以广泛采用。为解决这一痛点，需要系统化地梳理和总结当前的可解释AI（XAI）方法。

Method: 本论文对现有的LLMs可解释性方法进行了综述。作者按照Transformer架构的不同（仅有编码器、仅有解码器，以及编码器-解码器模型）对XAI方法进行了分类，同时综合评估了这些方法的可解释性评估机制，分析了实际应用中的解释利用方式，并讨论了相关资源、研究挑战与未来发展方向。

Result: 本综述系统梳理了主流的、针对不同Transformer类型LLM的可解释性技术，展示了各方法的评估标准与应用场景，并指出了当前面临的挑战以及未来潜在的研究方向，为后续开发透明、负责任的LLM系统提供了参考。

Conclusion: 本论文为LLM可解释方法提供了系统性总结和资源梳理，明确了可解释性评估与实际应用要点，并指明了未来的发展路径，有助于推动透明可靠的LLM落地应用。

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [83] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: 本研究通过分析PubMed科学摘要，发现因大型语言模型崛起，科学英语中的词汇变化表现为语义群体整体上升，非简单词汇替换。词性和用法变化复杂，揭示了人工智能技术正在改变语言使用结构。


<details>
  <summary>Details</summary>
Motivation: 科学英语词汇近年来出现突变性使用高峰，广泛被认为关联大型语言模型的影响，但具体的语言结构变动尚不清楚。研究需明确这些变化是词汇替换，还是更广泛的语义和语用变动。

Method: 系统分析了PubMed科学摘要中出现频率激增的词汇群体，并进行了词性标注来考察不同语法类别和词形的变化。同时分析了频率降低的词汇，以对比激增与衰减词汇的结构。

Result: 发现语义群体中的多数或全部词汇会同时增加使用频率，这说明变动是群体性的。词性变化显示用词方式也在调整。某些词汇如“important”则显著减少，对其分析揭示了更复杂、渐进式的词汇变动。

Conclusion: 大型语言模型（如ChatGPT）对科学英语词汇的变化主要体现在语义和语用层面，而非单一词汇的替换。这些变动影响了整个语义群体，使用频率的上升并非简单的同义词互换。

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [84] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: 本文提出融合神经网络表征与领域特征的多输入模型，对67位波斯诗人诗作归属进行判别，在公开数据集上实现高准确率，为风格分析与归属研究提供重要工具。


<details>
  <summary>Details</summary>
Motivation: 波斯古典诗歌在语言、风格及韵律上高度复杂，传统计算归属方法难以取得好效果，需要结合深度神经特征和领域先验以提升作者归属性能。

Method: 提出了一种多输入神经网络框架，融合了Transformer语言编码器、Word2Vec词嵌入、七项风格计量指标及诗歌格式和韵律等类别特征，对67位著名波斯诗人的诗作进行作者归属判定。采用了严格的数据预处理及诗篇级分割以避免数据泄漏，并借助诗句级分类及加权投票评估方案。

Result: 加权投票方案获得71%的准确率，在0.9置信度阈值下，高度确信预测准确率可达97%但样本覆盖率下降。方法有助于多语种作者归属、风格转变与生成模型等后续研究。

Conclusion: 集成深度表示与领域特征可以有效提升波斯古典诗歌的作者归属判别准确率，对未来风格分析、归属争议和自动化文学研究具有推动作用。

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [85] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Main category: cs.CL

TL;DR: 本文提出LinguaSynth，一种结合多种语言特征并依托逻辑回归的文本分类框架，实现了高性能、易解释和高效率，对深度模型的主导地位提出挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习推动了自然语言处理的发展，但其对大型黑盒模型的依赖带来了可解释性和计算效率问题。作者希望构建兼具可解释性和高效性的文本分类方法。

Method: 作者提出了LinguaSynth框架，将词汇、句法、实体级、词级语义和文档级语义等五种互补语言特征整合进透明的逻辑回归模型中，提升模型解释性和效率。

Result: 在20 Newsgroups数据集上，LinguaSynth实现了84.89%的准确率，比强健的TF-IDF基线高出3.32%。分析发现句法和实体特征对语义消歧尤为重要，有效补充了分布式语义特征。

Conclusion: LinguaSynth为可解释、资源高效的NLP模型设立新标杆，质疑了深度神经网络在文本分类中不可或缺的假设。

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [86] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: 本文系统性分析和验证了使用生成一致性来估计大语言模型输出置信度的理论假设，提出并实证支持将生成结果相似性用于无数据黑盒置信度估计的新方法，在多个任务上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在实际应用中的普及，其输出结果的可信度估计变得尤为重要，特别是在高信任要求的场景下。许多现有的不透明（black-box）不确定性量化（UQ）方法仅依赖于API，缺乏对其核心假设的系统性分析。本文旨在深入探究这些方法将生成一致性作为置信度代理的隐含假设。

Method: 本文形式化了“生成一致性假设（consistency hypothesis）”，并提出了三个数学命题与相应的统计检验方法，用以刻画该假设的不同变体，并在多任务下建立了评估一致性的指标。同时，作者设计并检验了无数据驱动的黑盒UQ方法，通过聚合生成结果之间的相似性来估计模型置信度。

Result: 在8个基准数据集和3个任务（问答、摘要、文本到SQL）上的实证分析显示，一致性假设在多种设置下广泛成立。“Sim-Any”这一假设最具实际参考意义。基于该假设的新型黑盒UQ方法可无数据地对生成相似性做聚合，从而优于现有最接近的基线方法。

Conclusion: 生成一致性是黑盒UQ可信度估计的重要依据。通过形式化和实证检验一致性假设，并据此设计创新方法，能够提升LLM可信度估计的实际效果，进一步增强其在实际场景下的应用价值。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [87] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Main category: cs.CL

TL;DR: 提出了新的方法分析BERT句法结构的层级生成机制，发现微观结构在低层出现，高层整合为宏观结构，并且整合时机会影响模型效果。


<details>
  <summary>Details</summary>
Motivation: 神经语言模型已被发现能够在其内部表示中编码句法结构，但这些结构如何在各层之间逐步构建尚不清楚。

Method: 提出“派生探针”方法，分析词向量在不同层级传播时，微观句法结构（如主语名词短语）和宏观句法结构（如动词与其直接依赖项之间的关系）是如何构建的。以BERT为实验对象。

Result: 发现BERT模型存在自下而上的派生产生方式：低层出现微观句法结构，高层则将它们整合为完整的宏观句法结构。针对主谓数一致性的评估表明，宏观句法结构的整合时机对下游任务表现具有显著影响，存在最优的结构整合时机。

Conclusion: 神经语言模型（如BERT）在各层有序地派生不同粒度的句法结构，并且宏观结构的最佳整合时机对于模型的下游表现至关重要。

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [88] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: DeepTalk通过专家混合机制解决原生多模态大模型灾难性遗忘难题，在保障较高性能的同时显著降低语音交互延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的原生多模态大模型（MLLMs）通过将单一的大语言模型（LLM）重组为既能生成语音又能输出文本的口语语言模型（SLM），但由于语音文本数据配对稀缺，预训练面临灾难性遗忘和性能下降。

Method: 提出DeepTalk框架，基于专家混合（MoE）架构，在模型内部自适应区分不同模态专家，并分别进行单模态专项训练及联合多模态协同训练。

Result: DeepTalk在保持端到端语音对话延迟在0.5秒以内的情况下，性能仅较原始LLM下降5.5%，明显优于现有原生MLLM（如GLM-4-Voice）20%以上的平均性能下降，并且性能与模块化MLLM相当。

Conclusion: DeepTalk通过自适应模态专家学习机制，显著缓解了原生多模态大模型在数据不足情况下的灾难性遗忘与性能劣化问题，保障了高效智能的语音交互体验。

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [89] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Main category: cs.CL

TL;DR: 本文针对缺乏语音大语言模型专门评测基准的问题，提出了具有真实语音场景和多样说话人、环境条件、语音现象的基准与查询感知评测方法，经在多种语音主流模型上测试分析，展现了模型间在不同语音情景下的性能差异。本工作为语音模型评测领域提供了实用工具和方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（如GPT-4o）具备了直接语音交互的强大能力，但缺乏专门且全面的端到端语音大语言模型评测基准，限制了其实用体验优化。现有评测往往基于文本基准，忽视了语音的独特特征和挑战，如语调、同音词、口吃及用户预期差异。

Method: 作者提出一种系统性评估语言模型在实际语音对话中的新方法，包括系统地整理与口语场景相关的真实对话数据，丰富说话人属性和声学条件，并注入语音特有现象。同时，提出查询感知的评测方法，通过定制化的评测清单和提示，提升自动化评测的准确性。

Result: 综合评测并详尽分析了多种主流语音模型，结果显示不同模型在不同语音场景下表现存在显著差异。查询感知评测也使在不同语音特定场景下的评定更加细致和具体。

Conclusion: 本基准工具能为语音模型的开发与评测提供宝贵参考和洞见。

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [90] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 本文提出针对VLMs世界模型能力的评测框架与大规模基准WM-ABench。实验表明，当前主流VLM在基本的世界理解和预测能力上有显著短板，与人类能力存在巨大鸿沟。


<details>
  <summary>Details</summary>
Motivation: 虽然最新的大型视觉语言模型（VLMs）显示出作为通用世界模型的潜力，但尚缺乏对其基本世界模型能力的系统性评估。理解其感知与预测的基本能力，对于推动智能体更好地理解和推理世界至关重要。

Method: 作者借鉴比较心理学和认知科学，提出了一个两阶段评估框架，分别考察VLMs的感知（视觉、空间、时间、量化、运动）和预测（机理模拟、传递推理、组合推理）能力。此外，开发了WM-ABench基准，包括6种环境、23个细分维度，通过660组实验系统评测了15种最新VLMs。

Result: 实验显示，当前VLMs在基础世界建模能力上有明显不足。例如识别运动轨迹时几乎接近随机准确率，有些模型还对颜色与运动速度存在错误关联。分析中揭示出这些模型与人类世界建模能力存在显著差距。

Conclusion: 尽管VLMs具备某些视觉与语言综合处理能力，但在关键的世界模型基本能力方面仍有很大提升空间，距离人类水平有明显差距。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [91] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Main category: cs.CL

TL;DR: 本文提出区分型号偏见与推理偏见，通过两阶段、多语种评估框架揭示LLM在事实性与争议性问题上的表现差异，并为多语种、文化敏感的LLM应用提供方法指导。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在多种语言和文化环境中的广泛应用，了解其在事实性与争议性场景下的行为变得十分重要，尤其当其输出可能影响公众舆论或强化主流叙事时。

Method: 作者提出对LLM的两类偏见进行定义：模型偏见（源自模型训练）和推理偏见（由查询语言引发）。采用两阶段评估方法：第一阶段在事实性问题上评测模型在不同查询语言下的一致性；第二阶段扩展至地缘政治敏感争议问题，考察模型输出中反映的文化和意识形态倾向。研究构建了涵盖事实与争议性问答、四种语言及不同问题类型的手工数据集。

Result: 结果显示，在事实性问答中，查询语言会引发模型输出的一致性偏移，而在争议性问题上，模型训练背景与查询语言共同影响模型回答的立场。

Conclusion: 本文提出了一个结构化框架，用于评估LLM在中性及敏感话题下的表现，为未来多语种环境下LLM的部署和具文化敏感性的评测实践提供了指导。

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [92] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 本文提出利用训练过程中的checkpoint模型能力动态优化数据混合，经过实验证明能提升语言模型在多个推理任务的表现，提升幅度最高达1.93%。


<details>
  <summary>Details</summary>
Motivation: 在语言模型训练中，希望模型能够掌握多种任务能力，但数据和任务之间的关系难以建模，直接获得合适的数据混合很困难。

Method: 通过观察训练过程中的checkpoint模型在不同训练阶段展现出的新能力，识别这些模型在基准测试上的表现，并利用其对源数据的影响作为数据混合的信号，采用一级影响近似方法聚合数据。

Result: 在八个推理基准上，该方法在预训练设置下带来了显著提升，最高提升幅度达到1.93%。

Conclusion: 利用训练过程中的checkpoint模型作为数据混合工具能够提升数据质量并优化数据混合，有助于提升语言模型的多任务能力。

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [93] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 本文建立了一个基于道德困境和ERG理论的新测试基准，用于量化大语言模型在社会决策和身份偏见方面的表现。实验发现LLMs存在明显决策偏好以及对边缘群体的不公正对待。数据集公开，为研究LLMs公平性提供重要资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在角色扮演场景中经常展现出偏见，这引发了学界对其决策偏好和社会偏见的关注。本文希望通过新的基准测试系统更深入理解和量化这些模型在社会情境下的决策表现和潜在偏见。

Method: 提出名为PapersPlease的基准测试，该基准包含3700个道德两难困境，模拟移民官情境，让LLMs基于申请人的短篇故事决定是否批准入境。这些故事依据ERG（生存、关系、成长）理论分为三个层级，分析六种LLMs在判决时的选择、偏好和对社会身份暗示的响应情况。

Result: 实验显示，六种LLMs在决策中有统计意义上的行为模式，表明内在存在倾向性。当故事中加入社会身份信息后，模型对于不同动机需求与身份提示的敏感性各异，其中部分模型对边缘群体展现出更高的拒绝率。

Conclusion: LLMs在涉及社会道德决策时表现出内在偏好，且对社会身份信息反应呈多样性甚至潜在偏见。该基准及数据可供进一步探究与改进LLMs的公平性和决策透明度。

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [94] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: 本文揭示工具集成LLM代理在各个环节都存在显著稳定性问题，尤其是开源模型更脆弱，且大模型未带来明显改进，强调了代理稳定性评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前集成工具的LLM代理评估侧重于端到端工具使用，却忽略了稳定性问题，这限制了其在实际中的应用，因为多种因素可能导致代理崩溃或异常行为。

Method: 研究系统性地分析了代理在工具调用全过程（包括阅读工具文档、选择工具和生成参数、处理响应）中的脆弱性。通过大量实验对比了不同类型和规模模型的表现和易受攻击性。

Result: 发现代理在调用工具的各个阶段都容易出错，基于开源模型的代理比专有模型更脆弱，并且扩大模型规模未显著提升工具调用推理能力，反而可能更容易受到模拟正常用户的攻击。

Conclusion: 强调了在评估代理时必须重视其稳定性，并为未来LLM的发展和评估提供了参考。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [95] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: 本文提出了将token-level和prompt-level越狱攻击结合的混合方法，在多个大模型上能大幅提升攻击成功率，甚至突破最先进的防御体系，揭示了目前主流安全机制的新弱点。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练大语言模型（PTLMs）和大语言模型（LLMs）广泛应用、多领域表现优异，但其在推理阶段仍易受攻击，包括token-level和prompt-level的“越狱”方法，各有局限。当前的攻击方式分别存在抗性不足、可检测性高或依赖反馈不稳定等问题。该文旨在克服这些技术瓶颈，提升越狱攻击的有效性和模型安全可靠性。

Method: 本文提出并实现了两种混合型越狱攻击方法：GCG + PAIR 与 GCG + WordGame，将token-level与prompt-level方法结合。通过在Vicuna和Llama系列（如Llama-3等）上进行系统化实验，并在不同的防御机制（如Gradient Cuff、JBShield等）下测试这两种混合攻击手段的表现和迁移能力。

Result: 实验表明，混合方法较各自单独攻击方式有显著提升。如，GCG + PAIR 在Llama-3上的攻击成功率达到91.6%，远高于PAIR的58.4%；GCG + WordGame的成功率在严格评测（Mistral-Sorry-Bench）下仍能维持80%以上。更为重要的是，这些混合方法能攻破一些能完全阻止单一攻击的高级防御机制，并表现出较强的可迁移性。

Conclusion: 混合型越狱攻击暴露了当前大模型安全机制的潜在新漏洞，实现了攻击有效性与迁移性的提升，能够突破现有防御。未来模型的安全设计需要综合考虑多种攻击适应性，寻求防御机制的整体优化。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [96] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Main category: cs.CL

TL;DR: 大语言模型可用于社会网络用户行为模拟，但需在仿真设计中注重实证真实性和严谨性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）能够模拟人类行为，引发了计算社会科学领域广泛关注。然而，关于用AI代理取代真实人类进行实证研究的有效性，学术界尚存分歧，因此需要进一步理解实验设计的差异。

Method: 研究首先提出了一个正式的社会网络模拟框架，然后聚焦用户在社交网络中通信行为仿真，对不同技术在X平台（前推特）英语和德语用户通信行为的模仿能力进行了实证测试。

Result: 结果显示，社会模拟应该依据拟合过程中所用设置下的实证真实性来进行验证，否则易导致仿真与真实人类行为不符。

Conclusion: 通过对比不同方法，论文认为在应用生成式代理进行社会仿真时，应当采取更为严谨的科学态度和验证标准。

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [97] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: 论文提出针对驾驶舱多语言语音转写的归一化与微调方法，使Whisper模型词错误率大幅下降，显著提升小众领域ASR表现。


<details>
  <summary>Details</summary>
Motivation: Transformer编码器-解码器结构在多领域表现优异，但在如驾驶舱语音转写这类小众领域，由于专有词汇和多语言交流，现有预训练模型性能下降。因此，提升驾驶舱专属自动语音识别的准确性是当前亟需解决的问题。

Method: 作者收集了约85分钟的驾驶舱模拟录音和130分钟的飞行员访谈录音，手动标注后，提出多种文本归一化方案优化转写文本，并引入了低秩适配（LoRA）进行高效微调，系统性优化Whisper模型。

Result: 通过归一化处理及LoRA微调，Whisper Large模型的词错误率从原始的68.49%显著降至26.26%。

Conclusion: 专为驾驶舱语音转写场景优化的归一化方案和高效微调手段，能大幅提升Whisper模型在多语言、专有领域语音识别上的准确率。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [98] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 本研究通过风格计量学方法比较了21部《彼得潘》英译中版本（人译、NMT和LLM），发现LLM的风格特征更接近人译，优于NMT，显示出其在儿童文学翻译中较好的潜力。


<details>
  <summary>Details</summary>
Motivation: 近年来，神经网络机器翻译（NMT）和大型语言模型（LLM）在翻译领域中表现突出，但它们在文学体裁（如儿童文学）中的风格表现与传统人工翻译（HT）有何异同，研究相对较少。该论文旨在探究人机翻译在儿童文学英译中的风格差异。

Method: 作者构建了21个版本的《彼得潘》译本语料库（包括HT、LLM和NMT各7种），提取了涵盖词汇、句法、可读性、n-gram以及儿童文学翻译特有（如重复、节奏、可译性等）共447个语言特征。通过机器学习的分类与聚类方法进行风格计量分析。

Result: 从通用特征看，HT与MT（机器翻译）在连接词分布和1-word-gram-YiYang比率有明显差异；NMT与LLM在描述性用词和副词比例上有显著不同。在儿童文学特定特征上，LLM翻译风格分布更接近HT，相较于NMT表现更佳，展现出LLM在儿童文学翻译中的潜力。

Conclusion: 人译和机器译在风格及语言使用上存在显著差异。LLM在儿童文学翻译中文体模仿能力优于NMT，更接近人译，未来具备较大发展前景。

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [99] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 本研究通过构建大规模英译中新闻语料库，系统分析了机器翻译（NMT与LLM）输出中的语言特殊性，发现无论哪种模型，机器译文都易与原文区分，且呈现词汇、句法上的典型特征。不同翻译模型略有差异，但中外模型产出无重大区别。


<details>
  <summary>Details</summary>
Motivation: 分析机器翻译产出（MTese）的语言特殊性，尤其关注英转中新闻文本领域，该方向此前研究较少。旨在揭示神经机器翻译（NMT）与大语言模型（LLM）中MTese的表现差异及主要特征。

Method: 构建包含4个子语料库的大型数据集，使用五层语言特征集，通过卡方排名算法在分类与聚类任务中筛选特征，对比原文与机器翻译文本的语言差异。

Result: 确认了MTese在NMT和LLM产出中均存在。原始中文文本可几乎完美区分于机器翻译产出。MT文本表现为句子更短、逆接连词增多。LLM输出词汇多样性更高，NMT括号使用更多。专用翻译型LLM词汇多样性较低但因果连词更多。中外LLM产品差异不显著。

Conclusion: 机器翻译文本（无论是NMT还是LLM）存在独特可辨的语言特性（MTese），可与原生文本高精度区分，且不同模型体现出不同语言习惯，但中外研发差别甚微。MTese特征识别对评估与改进机器翻译系统具有参考价值。

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [100] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Main category: cs.CL

TL;DR: 大模型链式推理过程中首步推理极为关键，首步失误影响下游质量。通过奖励模型筛选高质量首步推理，能降低成本且保持准确率，并提出新基准评测自纠错能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在复杂推理能力，特别是链式思维（CoT）推理方面取得了进步，但其在长链推理过程中的自我纠错能力尚未得到充分研究。同时，模型“过度思考”的现象表明推理过程存在冗余。

Method: 作者对两个先进的开源推理模型（DeepSeek-R1和Qwen3）进行了实证分析，考察链式思维推理首步对最终预测结果的影响；提出了结合奖励模型的高效采样策略，以筛选高质量首步推理；并构建了一个包含设计缺陷首步推理的新基准，用于系统评测模型的自我纠错能力。

Result: 实验证明，推理首步对最终预测影响极大，首步错误会大幅降低后续推理质量。所提采样方法在不损失准确率的前提下，将推理成本最多降低70%。新的基准为未来大模型稳健推理能力的评估提供了基础。

Conclusion: 论文揭示了复杂推理中首步决策的关键作用，并提出高效采样机制和新评测基准，有助于提升大语言模型推理效率与稳健性。

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [101] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文构建了MDC-R语料库，对原始MDC对话数据进行专业的指代注释，分析标注数据并实验证明其有助于改进指代理解，为相关NLP任务提供了有价值的新资源。


<details>
  <summary>Details</summary>
Motivation: 原始的Minecraft Dialogue Corpus（MDC）因其任务导向、多轮次、情境嵌入的对话，吸引了众多注释工作，但是缺乏关于照应与指示引用的专业注释。因此，构建带有参考标注的语料资源对于语言理解研究有重要意义。

Method: 对MDC进行了专家级的指代和指向性引用的人工标注。对标注文档进行了定量和定性的分析，并设计了一个应用实验，展示新语料库在指代表达理解任务中的实用价值。

Result: 构建了MDC-R含有引用标注的新语料库，并通过分析结果证明了其在参考表达理解等任务中的有效性和价值。

Conclusion: MDC-R作为一套具备丰富指代与指示性引用注释的新语言资源，不仅能为相关研究提供支持，也展示了其在提升机器理解自然语言指代表达方面的潜力。

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [102] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Main category: cs.CL

TL;DR: 社交媒体上用户在重大议题讨论中，因账户属性与立场不同，展现出显著的语言复杂性差异，并在群体内部形成专属话语体系。


<details>
  <summary>Details</summary>
Motivation: 社交媒体已成为影响语言和社会互动的重要平台，特别是在全球重大和有争议的议题上。然而，不同用户群体在这些平台上的语言使用及其复杂性，尚缺乏系统研究。分析这些特征有助于揭示语言与意识形态、社会结构之间的关系。

Method: 本文选取推特上的意见领袖在COVID-19、COP26和俄乌战争三大国际事件中的发文，结合多项文本复杂性指标，分析这些内容在账户类型、政治倾向、内容可靠性和情感四个维度的差异。

Result: 研究发现，在四个维度上均存在显著语言复杂性差异。个人与组织、偏向极端与中间立场、可靠性高低及情感倾向不同的账户，其语言复杂性表现不一。其中，发布更具负面和攻击性内容的用户往往使用更复杂的语言，且政治立场或可靠性相似的群体间更易形成专属术语。

Conclusion: 本研究深化了对社交平台语言社会性与意识形态反映的理解，揭示了社交媒体中不同用户群体在重大议题上的语言使用差异及趋同现象。

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [103] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: 作者通过多种技术手段，在GPT-2 Small中发现和解释了负责主谓一致的子网络。只需很小一部分电路即可取得接近原模型能力，但处理更复杂任务时需要更多网络参与。


<details>
  <summary>Details</summary>
Motivation: 希望理解大语言模型内部具体哪些部分（子网络、回路）负责句法结构（如主谓一致）的实现，提高对模型内部机制的解释能力。

Method: 该研究设计了一套流程，利用性能验证、自动电路发现（direct path patching）、直接logit归因等技术，定位GPT-2 Small中负责主谓一致的子网络。

Result: 成功隔离出对主谓一致任务起主要作用的子网络。这些关键回路只占模型很小一部分，但对完成基本任务效果突出。在复杂任务情境下，所需网络成分增加。

Conclusion: GPT-2 Small中，只有一小部分的子网络（电路）就足以实现接近模型在主谓一致任务上的表现；但在更复杂的设定下，则需要更多的网络成分。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [104] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: 该文提出了一个多法域、领域感知的专利检索公开数据集DAPFAM，弥补了现有数据集在领域标注、规模和实验灵活性上的不足，并在基线实验中验证了其挑战性和实用性。


<details>
  <summary>Details</summary>
Motivation: 公开可用的专利检索数据集普遍缺乏明确的领域内/外标注、多法域覆盖、领域分布均衡与适合中等资源细粒度实验的数据规模，当前这些需求未被很好满足。

Method: 提出了一个新型的领域感知专利检索数据集DAPFAM，按simple-family构建。采用三步数据处理流程，从全文查询家族与目标家族中挑选数据，并通过IPC码进行领域内/外显式标注。相关性由正向/反向引文和随机负例定义。展示数据集统计信息并用词法和神经检索模型进行了基线实验。

Result: 数据集包含1247个领域均衡的全文查询家族和45336个全文目标家族，共有49869对评估样本。结果显示跨领域专利检索仍具显著挑战。

Conclusion: DAPFAM数据集解决了现有专利检索数据集在领域标注、多法域、规模等方面的不足，适用于多领域和跨领域专利检索实验，是一个适合有限计算资源研究的开源数据集。

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [105] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 本研究为阿拉伯方言及阿拉伯-英语混说语音识别提出数据增强与训练新策略，综合提升模型表现，WER显著超越当前主流大模型。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯方言（DA）和阿拉伯语-英语混说（CS）语音的自动语音识别，由于混合语种和方言数据稀缺，现有自监督学习（SSL）模型难以获得较好表现。

Method: 采用改进的音频拼接技术生成人工混说（CS）语音数据（SAGE），并在已微调的SSL模型上进一步微调。同时，提出受经验回放（Experience Replay, ER）启发的方法，提升模型在DA和CS之间的泛化能力，减轻灾难性遗忘。此外，融合域外3-gram语言模型，和用少样本fune-tune技术提升性能。

Result: 使用SAGE数据再次微调可在阿拉伯-英语混说基准上将词错误率（WER）提升7.8%；经验回放式训练方法增强了泛化能力；融合3-gram外部语言模型将平均WER从31.7%降到26.6%；少样本微调进一步提升4.9%；在基准任务上，以31.1%的WER超过了USM及Whisper-large-v2等大规模多语种模型，分别领先5.5%和8.4%。

Conclusion: 通过音频拼接生成的数据，结合经验回放等策略，显著提升了阿拉伯方言及混说语音识别的表现，在参数量远小于主流大模型的情况下，取得更优WER。

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [106] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出RCO框架，自动奖励能改进模型输出的批判意见，在五项任务上验证了有明显提升，无需手动偏好评估，极大优化了LLM批判与响应改进流程。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）在评估和批判能力方面表现突出，但鲜有针对“哪些类型的批判最有助于模型改进”以及“如何生成高效批判”开展深入研究。

Method: 提出Refinement-oriented Critique Optimization（RCO）框架，通过让批判模型生成批判意见，用于指导演员模型优化回应，再以优化效果（critique utility, CU）作为反向奖励信号来训练批判模型，形成闭环训练机制，无需直接主观评估批判优劣。

Result: RCO在五个任务（对话生成、摘要、问答、数学推理和代码生成）中均显著优于传统和开源方法，在批判质量和回应优化效果上都取得了显著提升。

Conclusion: RCO通过奖励带来实际优化效果的批判意见，为LLM批判优化环路提供了一种全新且高效的监督和训练机制。

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [107] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Main category: cs.CL

TL;DR: 作者提出了一种结合人工调查数据的新LLM偏见评测方法，使偏见检测更稳定可靠，并发现大模型和指令微调都会影响偏见方向和程度。


<details>
  <summary>Details</summary>
Motivation: 当前使用大模型（LLMs）处理政治性问题评估其偏见时，常以直接提问法进行探测，但这种方法的稳定性有限，导致模型之间的比较不可靠。

Method: 提出了一种新的探测任务——问卷建模（Questionnaire Modeling, QM），将人工调查数据作为上下文示例，引入到偏见评测中。通过实验对比不同大小及微调方式的大模型。

Result: QM方法提升了偏见评测的稳定性，可有效用于对比微调模型与基础模型。实验表明：指令微调确实会改变模型的偏见方向；更大的LLMs更能有效利用上下文示例，在QM下偏见得分更低。

Conclusion: QM可显著提升通过提问方式探测LLM偏见的可靠性，更适合模型间的公平比较；指令微调和模型规模都会影响偏见表现。

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [108] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Main category: cs.CL

TL;DR: 该文提出基于GPT-4o大模型并结合数据上下文信息的新型个人数据检测方法，在多个数据集（尤其是医疗和真实数据）上表现优于现有方法，突出强调了更多真实世界数据集对领域进步的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私日益受到关注，亟需有效检测结构化数据中个人数据的方法。现有方法对信息上下文利用有限，检测能力受限。

Method: 提出了一种新颖的方法，利用GPT-4o大模型检测结构化数据中的个人数据。方法创新在于引入上下文信息，包括字段名、字段值、其他特征名及数据集描述。实验将本方法与Microsoft Presidio和CASSED进行了对比，评估数据集包括DeSSI（大规模合成数据）、Kaggle和OpenML收集的数据以及MIMIC-Demo-Ext（真实医疗数据）。

Result: 不同数据集上的检测能力差异明显，CASSED在其训练集DeSSI上表现最好，但在医疗真实数据上三者表现接近，而GPT-4o方法在该类数据上优于其它方法。在Kaggle和OpenML数据集上，引入上下文信息后，GPT-4o方法表现明显超过未利用上下文的CASSED与Presidio。结论认为领域发展需更多真实世界数据集。

Conclusion: 论文认为，要在该领域取得进一步进展，亟需更多包含个人数据的真实世界数据集。

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [109] [A Finite-State Symbolic Automaton Model for the Collatz Map and Its Convergence Properties](https://arxiv.org/abs/2506.21728)
*Leonard Ben Aurel Brauer*

Main category: cs.FL

TL;DR: 本文用60状态自动机符号化并复现了Collatz映射，实现了全自动、可形式化的收敛性分析，为利用自动机理论分析算术动力学提供了新路径。


<details>
  <summary>Details</summary>
Motivation: Collatz猜想行为简单、全局性质复杂，缺乏便于形式化和自动分析的数学工具，本文旨在通过自动机理论，引入一种新型的分析框架，实现对Collatz动力学的符号计算表述。

Method: 使用一个具有60种状态的有限状态、确定性自动机，对Collatz函数进行符号编码，将每位数字表示为三元组，通过局部、总的、依赖奇偶性的转移规则，逐位复现Collatz全局行为，并设计原始递归排名函数分析收敛性。

Result: 提出的自动机模型能将Collatz映射的所有轨道都归约到一个唯一的终止循环，且收敛性可通过皮亚诺算术完全形式化验证，展示了自动机理论分析强有力的可能性。

Conclusion: 本文提出的基于有穷状态自动机的符号编码能构造性地复现Collatz映射，并在皮亚诺算术（Peano Arithmetic）可完全形式化证收敛性。

Abstract: We present a finite-state, deterministic automaton that emulates the Collatz
function by operating on base-10 digit sequences. Each digit is represented as
a symbolic triplet capturing its value, the parity of the next digit, and a
local carry value, resulting in a state space of exactly 60 configurations. The
transition rules are local, total, and parity-dependent, yet collectively
reproduce the global behavior of the Collatz map through digitwise operations.
All symbolic trajectories reduce to the unique terminal cycle (4, 0, 0) -> (2,
0, 0) -> (1, 0, 0), offering a constructive, automaton-theoretic encoding of
the Collatz dynamics. A primitive recursive ranking function ensures symbolic
termination within the proposed model and supports a convergence argument that
is fully formalizable in Peano Arithmetic. This approach introduces a novel
framework for analyzing arithmetic dynamics via symbolic computation and
automata theory.

</details>


### [110] [Shape Preserving Tree Transducers](https://arxiv.org/abs/2506.22047)
*Paul Gallot,Sebastian Maneth*

Main category: cs.FL

TL;DR: 本文证明了多类树变换器的保形性可判定，并给出了满足保形性变换器的简化规范形式，非常有助于相关变换器的理论分析与实际应用。


<details>
  <summary>Details</summary>
Motivation: 树变换在编译器和语法分析等领域具有重要应用。确定一个树变换器是否保持“形状”（即输入输出树结构相似性）对于理解和使用转换器非常关键。然而，相关的可判定性以及简化的变换形式尚有待深入研究。

Method: 作者分别针对top-down、bottom-up树变换器，以及全确定性宏树变换器的组合，采用理论分析方法证明保形性（shape preservation）的可判定性。此外，给出了对满足保形性的变换器的规范化形式转化方法。

Result: 证明了top-down树变换器、bottom-up树变换器及全确定性宏树变换器组合的保形性是可判定的；并且，如果变换器保形，则它能转化为一种规范化形式，即每个输入节点对应仅生成一个输出节点。

Conclusion: 树变换器的保形性不仅可判定，而且对于满足保形性的变换器，可以进一步化简为结构更简单的规范化形式，有助于理解和应用树变换技术。

Abstract: It is shown that shape preservation is decidable for top-down tree
transducers, bottom-up tree transducers, and for compositions of total
deterministic macro tree transducers. Moreover, if a transducer is shape
preserving, then it can be brought into a particular normal form, where every
input node creates exactly one output node.

</details>


### [111] [Bridging CGR and $k$-mer Frequencies of DNA](https://arxiv.org/abs/2506.22172)
*Haoze He,Lila Kari,Pablo Millan Arias*

Main category: cs.FL

TL;DR: 本文数学上证明了Chaos Game Representation（CGR）与k-mer频率的等价关系，提出了一种基于De Bruijn图欧拉路径的合成序列生成算法，可精确匹配目标k-mer分布，为基因组分析和CGR数据增强等应用提供了统一理论和有效工具。


<details>
  <summary>Details</summary>
Motivation: 混沌游戏表示（CGR）是一种广泛用于DNA序列可视化的方法，但其与k-mer频率的数学关系尚未被系统阐述，且缺乏将CGR、k-mer统计和序列重建相统一的理论和方法。该论文试图填补此空白。

Method: 论文首先从数学上证明了FCGR（Frequency CGR）与k-mer频率的等价性，分析了CGR图像的对称变换怎样对应于原始序列的核苷酸排列。随后，提出一种基于在De Bruijn多重图上构建欧拉路径的算法，可以根据目标k-mer分布生成匹配的合成DNA序列，并进一步可生成合成的CGR图像。

Result: 所提出的方法能够以任意高精度重建指定k-mer分布的序列。通过数值实验验证了该方法在真实基因组数据和人工采样分布上的有效性，实现了CGR、k-mer频率和序列重建的统一。

Conclusion: 本研究首次系统地建立了CGR几何、k-mer统计与DNA序列重建之间的数学联系，并提供了用于合成和分析基因组可视化图像的新工具。该框架对基因组学分析和使用机器学习进行分类等实际应用具有帮助。

Abstract: This paper establishes formal mathematical foundations linking Chaos Game
Representations (CGR) of DNA sequences to their underlying $k$-mer frequencies.
We prove that the Frequency CGR (FCGR) of order $k$ is mathematically
equivalent to a discretization of CGR at resolution $2^k \times 2^k$, and its
vectorization corresponds to the $k$-mer frequencies of the sequence.
Additionally, we characterize how symmetry transformations of CGR images
correspond to specific nucleotide permutations in the originating sequences.
Leveraging these insights, we introduce an algorithm that generates synthetic
DNA sequences from prescribed $k$-mer distributions by constructing Eulerian
paths on De Bruijn multigraphs. This enables reconstruction of sequences
matching target $k$-mer profiles with arbitrarily high precision, facilitating
the creation of synthetic CGR images for applications such as data augmentation
for machine learning-based taxonomic classification of DNA sequences. Numerical
experiments validate the effectiveness of our method across both real genomic
data and artificially sampled distributions. To our knowledge, this is the
first comprehensive framework that unifies CGR geometry, $k$-mer statistics,
and sequence reconstruction, offering new tools for genomic analysis and
visualization.

</details>
