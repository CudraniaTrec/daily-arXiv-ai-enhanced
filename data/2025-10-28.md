<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 26]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 97]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Linear effects, exceptions, and resource safety: a Curry-Howard correspondence for destructors](https://arxiv.org/abs/2510.23517)
*Sidney Congard,Guillaume Munch-Maccagnoni,Rémi Douence*

Main category: cs.PL

TL;DR: 本文分析并建模了如何结合线性性、效应和异常处理以确保资源安全，提出了资源分配单子与析构机制，并通过两个计算系统展示了理论在实际资源管理中的应用，尤其借鉴了C++/Rust的析构与move语义。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究如何在编程语言的抽象模型中结合线性性、效应和异常处理，尤其关注在资源安全属性建模下的分配单子（allocation monad）的作用。这样做可更贴近实际语言如C++或Rust中资源管理与异常处理的需求。

Method: 提出并分析了一种分配单子$T(- \oplus E)$，在此基础上建立了两个线性效应型计算系统。第一个系统是带有资源分配和释放效应（new、delete）的线性call-by-push-value语言，通过类型规则保证资源安全；第二个系统则在此基础上引入异常和析构函数，模拟C++/Rust的资源释放方式，通过move操作实现随机顺序释放。

Result: 得到两个资源安全的线性效应型计算系统，其中第一个系统借助线性和有序的类型系统确保分配和释放资源安全；第二个系统通过引入异常处理及析构机制，同时允许资源move操作，使得资源释放顺序可以随机化。

Conclusion: 论文有效结合了线性性、效应和异常处理，提出了能保证资源安全的抽象模型，并通过两个不同特性的语言系统验证了方法的有效性，对实际编程语言的资源管理和错误处理理论提供了新思路。

Abstract: We analyse the problem of combining linearity, effects, and exceptions, in
abstract models of programming languages, as the issue of providing some kind
of strength for a monad $T(- \oplus E)$ in a linear setting. We consider in
particular for $T$ the allocation monad, which we introduce to model and study
resource-safety properties. We apply these results to a series of two linear
effectful calculi for which we establish their resource-safety properties.
  The first calculus is a linear call-by-push-value language with two
allocation effects $\mathit{new}$ and $\mathit{delete}$. The resource-safety
properties follow from the linear (and even ordered) character of the typing
rules.
  We then explain how to integrate exceptions on top of linearity and effects
by adjoining default destruction actions to types, as inspired by C++/Rust
destructors. We see destructors as objects $\delta : A\rightarrow TI$ in the
slice category over $TI$. This construction gives rise to a second calculus, an
affine ordered call-by-push-value language with exceptions and destructors, in
which the weakening rule performs a side-effect. As in C++/Rust, a ``move''
operation is necessary to allow random-order release of resources, as opposed
to last-in-first-out order. Moving resources is modelled as an exchange rule
that performs a side-effect.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*Timothé Boulet,Xavier Hinaut,Clément Moulin-Frier*

Main category: cs.SE

TL;DR: 本文首次系统评估了SWE-Agents在具身任务控制器生成上的性能，分析了不同信息获取方式的影响，为后续高效推理系统研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 虽然SWE-Agents在传统软件工程任务中表现优异，但对于需信息发现的具身任务尚缺少研究，亟需评估其在此类任务中的能力。

Method: 将Mini-SWE-Agent（MSWEA）应用于Minigrid环境中的20个具身任务，比较不同信息访问条件（源码访问、交互探索能力）的性能，并量化影响。

Result: 不同信息访问级别显著影响SWE-Agent的具身任务表现，静态代码分析与动态探索在任务解决中各有重要作用。

Conclusion: SWE-Agents在Controller生成的具身任务中效果受信息获取方式影响，并为高效推理系统的未来研究提供了基线。

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [3] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: 本文提出结合理论心智模型的双智能体编码系统，大幅提升智能体对用户意图推断与任务完成率，在实际开发场景下获得开发者高度认可。


<details>
  <summary>Details</summary>
Motivation: 当前编码智能体在规划、编辑、运行和测试复杂代码方面表现优异，但在理解和追踪用户意图（尤其是指令不明确或上下文相关时）仍存在困难，因此需要更好地桥接用户意图与智能体执行间的鸿沟。

Method: 提出ToM-SWE架构，将主软件工程智能体（SWE）与轻量级理论心智（ToM）智能体配对，ToM智能体基于用户指令和交互历史，建立持久化用户记忆，推断用户目标、约束与偏好，并向SWE智能体提供建议。通过在两个基准测试（模糊SWE-bench和状态型SWE-bench）及用户模拟实验进行评估。

Result: 在stateful SWE-bench基准上，ToM-SWE的任务成功率为59.7%，显著优于先进SWE智能体OpenHands的18.1%。在为期三周的专业开发者实际使用研究中，参与者在86%的工作场景下认为ToM-SWE有用。

Conclusion: 引入建模用户心智状态的智能体显著提升了编码智能体在复杂用户交互中成功率与用户满意度，将理论心智模型嵌入软件工程工作流具备实际应用价值。

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [4] [A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case](https://arxiv.org/abs/2510.21933)
*Joao Correia,Daniel Coutinho,Marco Castelluccio,Caio Barbosa,Rafael de Mello,Anita Sarma,Alessandro Garcia,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 结合RAG的GPT模型在Mozilla Firefox开发支持中表现出较高的全面性和近似人类的有用性，但需改善回答的简明性，这为大项目开发者辅助工具指明了持续优化方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在软件开发中的应用增加，开发者借助这些模型提升编码效率、获得互动问答支持，但提升其辅助效果仍有改进空间。作者希望评估RAG技术提升开发者支持的有效性。

Method: 与Mozilla基金会合作，利用Mozilla Firefox项目的真实开发者聊天室查询，分别对人类开发者、标准GPT模型以及RAG增强GPT模型的回答效果进行比较。通过Mozilla专家依据有用性、全面性、简明性对各类回答进行评估。

Result: RAG辅助的GPT回答在全面性上优于人类开发者（62.50%对54.17%），在有用性上接近人类开发者（75.00%对79.17%），但RAG回答不够简练，有较强的冗长倾向。

Conclusion: RAG技术有潜力提升开源软件项目中对开发者的辅助，减轻核心维护者负担，且不损失回答质量，但需优化以提升回答的简洁性。

Abstract: The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.

</details>


### [5] [ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities](https://arxiv.org/abs/2510.21966)
*Musengamana Jean de Dieu,Ruiyin Li,Peng Liang,Mojtaba Shahin,Muhammad Waseem,Arif Ali Khan,Bangchao Wang,Mst Shamima Aktar*

Main category: cs.SE

TL;DR: 提出了ArchISMiner框架，实现自动从Stack Overflow等社区高效挖掘和抽取架构知识，方法经实验和用户验证有效且实用。


<details>
  <summary>Details</summary>
Motivation: SO上架构相关知识难以挖掘，开发者难以及时获取高质量的架构解决方案，现有内容繁杂、无结构。

Method: 提出ArchISMiner，包括两个模块：ArchPI用于识别建筑相关帖子，测试了多种模型（传统ML/DL、预训练语言模型、LLM），选最优用于自动识别；ArchISPE结合BERT和TextCNN，用于抽取建筑问题-解决方案对，并采用间接监督学习。

Result: ArchPI最优模型ARP检测F1为0.960，ArchISPE抽取建筑问题和解决方案对的F1分别为0.883和0.894，真实用户验证了相关性和实用性。还推广到3个论坛，释放了超1.8万条数据集。

Conclusion: ArchISMiner能高效、准确挖掘开发社区中的建筑相关知识，助力架构师和开发者。工具包已开源。

Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of
software development knowledge. However, locating architectural knowledge, such
as architectural solutions remains challenging due to the overwhelming volume
of unstructured content and fragmented discussions. Developers must manually
sift through posts to find relevant architectural insights, which is
time-consuming and error-prone. This study introduces ArchISMiner, a framework
for mining architectural knowledge from SO. The framework comprises two
complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates
multiple models, including conventional ML/DL models, Pre-trained Language
Models (PLMs), and Large Language Models (LLMs), and selects the
best-performing model to automatically identify Architecture-Related Posts
(ARPs) among programming-related discussions. ArchISPE employs an indirect
supervised approach that leverages diverse features, including BERT embeddings
and local TextCNN features, to extract architectural issue-solution pairs. Our
evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in
ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,
achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.
A user study further validated the quality (e.g., relevance and usefulness) of
the identified ARPs and the extracted issue-solution pairs. Moreover, we
applied ArchISMiner to three additional forums, releasing a dataset of over 18K
architectural issue-solution pairs. Overall, ArchISMiner can help architects
and developers identify ARPs and extract succinct, relevant, and useful
architectural knowledge from developer communities more accurately and
efficiently. The replication package of this study has been provided at
https://github.com/JeanMusenga/ArchISPE

</details>


### [6] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: 本文提出FeaGPT，自然语言控制下实现从几何、网格到有限元仿真全流程自动化，经工业案例及大批量参数验证，证明该技术可降低复杂工程仿真的门槛，并保持高水平的分析精度。


<details>
  <summary>Details</summary>
Motivation: 当前有限元分析（FEA）的工程工具只能自动化部分流程，整合性不足，难以实现用户通过自然语言一站式完成从几何建模到仿真分析的完整流程。针对工程师日益增长的自动化与便捷性需求，亟需自然语言控制全流程仿真分析的新范式。

Method: 提出并实现了FeaGPT系统，通过自然语言交互控制，自动完成几何建模、生成自适应网格、配置仿真边界条件、闭环多目标分析，形成端到端集成的Geometry-Mesh-Simulation-Analysis（GMSA）管线，且无需人工干预。

Result: 在工业涡轮增压器案例（7叶压气机与12叶涡轮，转速110,000 rpm）中，系统能成功将自然语言规范转化为可验证的CalculiX仿真，并产出物理合理的旋转机械分析结果。此外，在432种NACA翼型设计参数验证中显示了良好可扩展性。

Conclusion: FeaGPT验证了自然语言接口在工程仿真一体化自动化中的可行性和实用性，有效降低了高端CAE工具的使用门槛，兼顾分析精度和流程自动化。

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>


### [7] [Taming Silent Failures: A Framework for Verifiable AI Reliability](https://arxiv.org/abs/2510.22224)
*Guan-Yan Yang,Farn Wang*

Main category: cs.SE

TL;DR: FAME框架通过形式化合成与运行时监控相结合，显著提升了安全关键系统中AI模块的可验证性，并在自动驾驶实验中高效识别了大部分隐性安全风险。该工作推动了AI应用由“概率安全”到“可证安全”的革命。


<details>
  <summary>Details</summary>
Motivation: 将人工智能融入安全关键系统可能导致“静默失败”，即AI在高度自信的情况下给出错误输出，带来安全隐患。解决AI组件潜在不可见错误成为亟需应对的挑战。

Method: 提出FAME（正式保证与监控环境）框架，结合离线形式化合成（给予数学严格性）与在线运行时监控（实时警戒），为不可解释的AI模块构建可验证的安全防护网。

Result: 在自动驾驶感知系统实验中，FAME成功检测到93.5%的本来会被忽略的关键安全违规行为。

Conclusion: FAME为可靠性工程师提供了符合ISO 26262和ISO/PAS 8800标准的、实际可认证的AI安全部署途径，实现了从概率性性能到可证明安全的重大转变。

Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems
introduces a new reliability paradigm: silent failures, where AI produces
confident but incorrect outputs that can be dangerous. This paper introduces
the Formal Assurance and Monitoring Environment (FAME), a novel framework that
confronts this challenge. FAME synergizes the mathematical rigor of offline
formal synthesis with the vigilance of online runtime monitoring to create a
verifiable safety net around opaque AI components. We demonstrate its efficacy
in an autonomous vehicle perception system, where FAME successfully detected
93.5% of critical safety violations that were otherwise silent. By
contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,
we provide reliability engineers with a practical, certifiable pathway for
deploying trustworthy AI. FAME represents a crucial shift from accepting
probabilistic performance to enforcing provable safety in next-generation
systems.

</details>


### [8] [Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review](https://arxiv.org/abs/2510.22003)
*Stefan Julian Kooy,Jean Paul Sebastian Piest,Rob Henk Bemthuis*

Main category: cs.SE

TL;DR: 本研究系统性回顾生成式AI在敏捷架构领域的应用，确认其在设计、产出和决策方面的价值，同时揭示一系列风险及对团队能力和治理的要求，为负责任的GenAI采纳和未来人机协作研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 虽然GenAI正在重塑敏捷架构实践，但相关影响的证据较为零散，故需系统性总结其用例、风险及能力要求。

Method: 采用Kitchenham和PRISMA标准的系统性文献回顾（SLR），筛选出1697条记录中的33项研究，覆盖企业、解决方案、领域、业务和IT架构师角色。

Result: GenAI能有效支持设计构思、快速产出和完善工件、架构决策与知识检索；但也存在不透明性、偏见、上下文错误、隐私合规和团队惰性等风险，并要求新技能（如提示工程、模型评估、专业督导）和组织应变能力。

Conclusion: 研究系统性梳理了生成式AI在敏捷软件组织中的企业架构工作中的应用与影响，强调要负责任地采用GenAI以加速数字化转型，同时保障架构完整性。

Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile
software organizations, yet evidence on its effects remains scattered. We
report a systematic literature review (SLR), following established SLR
protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies
across enterprise, solution, domain, business, and IT architect roles. GenAI
most consistently supports (i) design ideation and trade-off exploration; (ii)
rapid creation and refinement of artifacts (e.g., code, models, documentation);
and (iii) architectural decision support and knowledge retrieval. Reported
risks include opacity and bias, contextually incorrect outputs leading to
rework, privacy and compliance concerns, and social loafing. We also identify
emerging skills and competencies, including prompt engineering, model
evaluation, and professional oversight, and organizational enablers around
readiness and adaptive governance. The review contributes with (1) a mapping of
GenAI use cases and risks in agile architecting, (2) implications for
capability building and governance, and (3) an initial research agenda on
human-AI collaboration in architecture. Overall, the findings inform
responsible adoption of GenAI that accelerates digital transformation while
safeguarding architectural integrity.

</details>


### [9] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: 本文提出LSPRAG框架，通过复用LSP后端，在不增加复杂工程成本的前提下，极大提升了LLM驱动的自动化单元测试生成在多种语言上的覆盖率表现。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化单元测试生成方法难以跨多种编程语言泛化，且在实时开发环境中有限制。特别是，当前主流解决方案要么依赖于相似性检索但不够精准，要么需要构建昂贵且特定于某种语言的静态分析流程，效率低下。

Method: 提出了一种名为LSPRAG的框架，这是针对实时、语言无关的单元测试生成而优化的简洁上下文检索方法。LSPRAG利用现成的Language Server Protocol（LSP）后端，实时为大语言模型提供精准的符号定义和引用信息，从而减少每种语言的工程工作量并提升检索效率。

Result: LSPRAG在Java、Go和Python开源项目上进行了评估，相较于现有主流基线方法，其代码行覆盖率在Golang提升至174.55%、Java提升至213.31%、Python提升至31.57%。

Conclusion: LSPRAG无需复杂的语言定制化工程，利用成熟的LSP服务器，就能为LLM实时地检索到语言相关的准确上下文，极大提升了多语言环境下自动化测试生成的覆盖率和适用性。

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [10] [Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study](https://arxiv.org/abs/2510.22249)
*Ibuki Nakamura,Yutaro Kashiwa,Bin Lin,Hajimu Iida*

Main category: cs.SE

TL;DR: 本文系统分析了测试代码中的自承认技术债务（SATD），发现其有独特分布和类型，且与测试异味并不直接相关。提出的SATD自动分类工具（基于CodeBERT）在实际表现优异，为SATD管理带来新方法。


<details>
  <summary>Details</summary>
Motivation: 开发者为了赶工期或快速制作原型，常采用较简单但非最佳的实现方法，导致后续需要投入额外精力（技术债务）来改进代码。虽然自承认技术债务（SATD）通常通过注释写明，但现有研究多关注生产代码中的SATD，对测试代码中的SATD关注较少，或者默认两者特性相同。实际上测试代码中存在大量SATD，且其类型与生产代码不同。本文旨在补齐该研究空白，揭示测试代码中SATD的分布与类型，并分析其与测试质量的关系。

Method: 本研究从50个代码仓库收集了17,766条SATD注释（其中14,987条来自生产代码，2,779条来自测试代码），实证分析测试代码中SATD的分布、类型，并比较其与测试异味（test smells）的关系。此外，还提出了SATD类型新的分类体系，并基于机器学习（包括CodeBERT模型）自动对SATD注释进行类型分类。

Result: 研究发现，测试代码中广泛存在SATD，但这些SATD与测试异味并无直接关联。论文归纳了测试代码中SATD的新型类别，并开发了基于机器学习的自动化分类工具，其中CodeBERT模型在召回率和F1分数上优于其他模型，但在不同类型SATD上的表现有差异。

Conclusion: 测试代码中的SATD数量可观，其特性与生产代码SATD有显著不同。SATD与测试异味并非等价问题。提出的SATD类型分类和自动分类工具有助于后续管理和研究。CodeBERT等大模型在SATD分类上前景良好，但仍需针对不同类型优化。

Abstract: Developers often opt for easier but non-optimal implementation to meet
deadlines or create rapid prototypes, leading to additional effort known as
technical debt to improve the code later. Oftentimes, developers explicitly
document the technical debt in code comments, referred to as Self-Admitted
Technical Debt (SATD). Numerous researchers have investigated the impact of
SATD on different aspects of software quality and development processes.
However, most of these studies focus on SATD in production code, often
overlooking SATD in the test code or assuming that it shares similar
characteristics with SATD in production code. In fact, a significant amount of
SATD is also present in the test code, with many instances not fitting into
existing categories for the production code. This study aims to fill this gap
and disclose the nature of SATD in the test code by examining its distribution
and types. Moreover, the relation between its presence and test quality is also
analyzed. Our empirical study, involving 17,766 SATD comments (14,987 from
production code, 2,779 from test code) collected from 50 repositories,
demonstrates that while SATD widely exists in test code, it is not directly
associated with test smells. Our study also presents comprehensive categories
of SATD types in the test code, and machine learning models are developed to
automatically classify SATD comments based on their types for easier
management. Our results show that the CodeBERT-based model outperforms other
machine learning models in terms of recall and F1-score. However, the
performance varies on different types of SATD.

</details>


### [11] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: 该论文针对AI编码工具在科学计算中的应用，提出十条实用规则，旨在帮助研究人员高效利用AI同时保证代码的科学性和可靠性，推进科研软件开发方式的变革。


<details>
  <summary>Details</summary>
Motivation: AI编码工具正在加速软件开发，但在科学计算领域应用时，涉及代码质量和科学有效性等关键问题。作者希望通过规范使AI辅助编码更适用于科研实践。

Method: 提出并论证了十条AI辅助编码的实用规则，从备题、过程管理、测试验证到代码质量保障，涵盖开发全流程。并围绕保有人类主导权、建立验证流程和维护领域专长等主题进行讨论。

Result: 总结了科学计算中AI辅助编码的关键原则，并提供了具体规则，明确指出如何在加快开发效率的同时，确保代码可靠、可复现且具备科学有效性。

Conclusion: 建议科研人员在利用AI提升开发效率的同时，注意代码质量和科学严谨性，确保研究的可靠性与正确性。

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [12] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: 本文提出并评估了用大型语言模型（LLM）辅助ISTQB软件测试教育的方法，构建了专用数据集和优化提示，并验证了LLM的有效性，提出了实际应用建议。


<details>
  <summary>Details</summary>
Motivation: 软件测试在软件工程及教育中非常关键，随着行业的发展，教育方法需不断更新。ISTQB认证广泛应用于学界与业界，但尚未充分结合生成式AI的最新进展。当前大型语言模型（LLM）的能力提升，为ISTQB教育带来新的可能性。

Method: 构建了一个详细的、历经十余年、包含28套考试与1145道题目的ISTQB对齐数据集；提出了提升LLM在ISTQB相关任务表现的领域优化提示；用该数据集对当下先进LLM进行了系统评估；总结并提出将LLM纳入软件测试教育的实际建议。

Result: LLM能够有效支持ISTQB认证备考，对数据集表现良好，能够提升解题精准度与解释质量； 提供了将LLM集成进软件测试教育的实践操作建议。

Conclusion: LLM与ISTQB框架结合在高等教育场景下具有巨大潜力，不仅能提升认证备考效率，也为软件工程教育提供了新的技术基础。

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [13] [Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation](https://arxiv.org/abs/2510.22338)
*Aritra Mitra,Srijoni Majumdar,Anamitra Mukhopadhyay,Partha Pratim Das,Paul D Clough,Partha Pratim Chakrabarti*

Main category: cs.SE

TL;DR: 本文分析初学者代码注释质量问题，提出利用LLMs结合设计文档自动生成更有用注释的可行性，以改善代码维护。


<details>
  <summary>Details</summary>
Motivation: 随着代码开发的普及，越来越多的初学者参与代码库创建，但缺乏规范的注释标准，导致注释无效，增加维护难度。

Method: 研究探讨利用大型语言模型（LLMs），结合设计文档作为上下文，自动生成更有用的代码注释。

Result: 如果设计文档能作为上下文输入给LLMs，则有可能生成比业余开发者手写更有价值的注释，提升代码后期维护效率。

Conclusion: LLMs有望借助设计文档，在原始注释不足的情况下，自动生成对维护者更有帮助的注释。

Abstract: Comments are very useful to the flow of code development. With the increasing
commonality of code, novice coders have been creating a significant amount of
codebases. Due to lack of commenting standards, their comments are often
useless, and increase the time taken to further maintain codes. This study
intends to find the usefulness of large language models (LLMs) in these cases
to generate potentially better comments. This study focuses on the feasibility
of design documents as a context for the LLMs to generate more useful comments,
as design documents are often used by maintainers to understand code when
comments do not suffice.

</details>


### [14] [A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection](https://arxiv.org/abs/2510.22409)
*Shahidul Islam,Md Nahidul Islam Opu,Shaowei Wang,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本研究首次大规模分析测试代码中的SATD，建立其分类体系，发现现有检测方法（包括LLMs）在测试代码SATD检测上均表现不佳，为后续专注于测试代码SATD的研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 此前关于SATD的研究多集中在源代码，对于测试代码中的SATD几乎没有关注。测试代码中SATD可能影响维护和测试质量，存在研究空白。

Method: 本研究对1,000个开源Java项目的160万条注释随机抽样，人工分析50,000条注释，最终筛选出615条测试代码中的SATD并归类为15种类型，构建了测试代码SATD分类法。同时评估现有SATD检测工具和开源与商业LLMs自动检测SATD的能力。

Result: MAT工具在现有检测方法中表现最佳但召回率一般；开源与商业LLMs检测精度低，主要是准确率差，整体自动检测效果不理想。

Conclusion: 当前无论是现有SATD检测工具还是LLMs都难以可靠检测测试代码中的SATD，需要专针对测试代码的SATD检测方法进一步研究。

Abstract: Self-admitted technical debt (SATD) refers to comments in which developers
explicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD
is known to significantly increase software maintenance effort. While extensive
research has examined SATD in source code, its presence and impact in test code
have received no focused attention, leaving a significant gap in our
understanding of how SATD manifests in testing contexts.
  This study, the first of its kind, investigates SATD in test code by manually
analyzing 50,000 comments randomly sampled from 1.6 million comments across
1,000 open-source Java projects. From this sample, after manual analysis and
filtering, we identified 615 SATD comments and classified them into 15 distinct
categories, building a taxonomy of test code SATD. To investigate whether test
code SATD can be detected automatically, we evaluated existing SATD detection
tools, as well as both open-source and proprietary LLMs. Among the existing
tools, MAT performed the best, albeit with moderate recall. To our surprise,
both open-source and proprietary LLMs exhibited poor detection accuracy,
primarily due to low precision. These results indicate that neither existing
approaches nor current LLMs can reliably detect SATD in test code.
  Overall, this work provides the first large-scale analysis of SATD in test
code, a nuanced understanding of its types, and the limitations of current SATD
detection methods. Our findings lay the groundwork for future research on test
code-specific SATD.

</details>


### [15] [A Multifaceted View on Discrimination in Software Development Careers](https://arxiv.org/abs/2510.22457)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 软件工程领域的歧视现象远比传统性别和种族区分复杂，涉及年龄、政治、宗教、神经差异等。女性及非二元人士更易遭受歧视和心理困扰，研究者需在未来研究中考虑更多身份维度。


<details>
  <summary>Details</summary>
Motivation: 当前多样性与包容性讨论主要聚焦性别和种族，但其他类型的歧视（如年龄、政治观念、残疾、认知差异）同样普遍却被忽视，因此需要更深入的探究。

Method: 通过对State of the Developer Nation 2025问卷中800份开放式回答进行二次分析，探讨受访者感知到的歧视类型、挑战及负面影响。

Result: 年龄和性别相关的歧视报告最多，政治及宗教观点歧视也较为突出。女性和非二元群体在几乎所有工作场所议题中遭受的歧视和心理健康问题发生率更高。照料责任相关的歧视在所有性别中均被提出。许多人为应对歧视会调整外貌或行为。

Conclusion: 软件工程领域的歧视现象具有多面性，远不止于性别和年龄，还涵盖政治观念、宗教、神经差异等多个方面。研究者在设计相关调查和研究时，应关注并评估更多身份维度。

Abstract: Conversations around diversity and inclusion in software engineering often
focus on gender and racial disparities. However, the State of the Developer
Nation 2025 survey with 8,717 participants revealed that other forms of
discrimination are similarly prevalent but receive considerably less attention.
This includes discrimination based on age, political perspective, disabilities,
or cognitive differences such as neurodivergence. We conducted a secondary
analysis of 800 open-ended survey responses to examine patterns of perceived
discrimination, as well as related challenges and negative impacts. Our study
covers multiple identity facets, including age, gender, race, and disability.
We found that age- and gender-related discrimination was the most frequently
reported workplace issue, but discrimination based on political and religious
views emerged as further notable concerns. Most of the participants who
identified as female cited gender as the primary source of discrimination,
often accompanied by intersectional factors such as race, political views, age,
or sexual orientation. Discrimination related to caregiving responsibilities
was reported by all gender identities. Regarding the negative impacts of
workplace issues, many participants described modifying their appearance or
behavior in response to gender biases. Gender also appeared to influence
broader career challenges, as women and non-binary respondents reported
experiencing almost all workplace issues at higher rates, particularly
discrimination (35%) and mental health challenges (62%). Our goal is to raise
awareness in the research community that discrimination in software development
is multifaceted, and to encourage researchers to select and assess relevant
facets beyond age and gender when designing software engineering studies.

</details>


### [16] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: 本文提出了一种仅基于崩溃转储和源代码库、无需动态分析的新型LLM智能故障定位方法——AutoCrashFL，在超大规模工业项目上效果大幅优于传统方法，适合实际工程部署。


<details>
  <summary>Details</summary>
Motivation: 传统的故障定位（FL）方法依赖于动态分析技术（如覆盖分析、变异测试），但在大规模工业软件中，这些技术的运行成本高，难以应用。亟需一种高效、低成本的工业级故障定位方法。

Method: 提出AutoCrashFL，一种基于大型语言模型（LLM）的智能代理，仅需程序崩溃转储和源代码仓库信息进行崩溃定位，无需动态运行时分析。

Result: 在SAP HANA这一超大规模工业项目上评估，AutoCrashFL在30%的案例中能将崩溃定位在首位，对比基线方法的17%，显著提升定位效果。尤其在复杂bug上效果更优，还能赋予结果置信度。

Conclusion: AutoCrashFL无需高昂运行时代价，便于工业级部署，并能有效提升工业软件故障定位精度，表明LLM代理在工业现网中的实用性。

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [17] [DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices](https://arxiv.org/abs/2510.22613)
*Songhan Zhang,Aoyang Fang,Yifan Yang,Ruiyi Cheng,Xiaoying Tang,Pinjia He*

Main category: cs.SE

TL;DR: 该论文提出了DynaCausal，一种专为云原生微服务设计的动态因果根因分析方法，通过时空动态建模和因果优先机制，有效提升了故障定位的准确性和解释性，在基准测试中比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务不断发展带来了依赖关系复杂和快速变化，导致现有的根因分析方法难以应对这些动态行为和服务关系的变化。主要有三个挑战：故障级联传播建模不足、对噪声和概念漂移较为脆弱、过分依赖偏差强度影响根因识别准确性。

Method: 作者提出了DynaCausal，一种面向动态因果推断的根因分析框架。该方法统一多模态动态信号，通过交互感知表示学习建模时空依赖，并引入动态对比机制剥离真实故障指示和上下文噪声，同时采用因果优先排序目标优化因果归因。

Result: DynaCausal在公共基准数据集上的全面评估显示，相较于已有方法，AC@1平均可达0.63，绝对提升区间为0.25到0.46，实现了更高的准确性和可解释性。

Conclusion: DynaCausal能够有效识别和诊断高度动态微服务环境中的根因，显著优于当前主流技术，并兼具准确性和可解释性。

Abstract: Cloud-native microservices enable rapid iteration and scalable deployment but
also create complex, fast-evolving dependencies that challenge reliable
diagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal
fusion of logs, traces, and metrics, remain limited in capturing dynamic
behaviors and shifting service relationships. Three critical challenges
persist: (i) inadequate modeling of cascading fault propagation, (ii)
vulnerability to noise interference and concept drift in normal service
behavior, and (iii) over-reliance on service deviation intensity that obscures
true root causes. To address these challenges, we propose DynaCausal, a dynamic
causality-aware framework for RCA in distributed microservice systems.
DynaCausal unifies multi-modal dynamic signals to capture time-varying
spatio-temporal dependencies through interaction-aware representation learning.
It further introduces a dynamic contrastive mechanism to disentangle true fault
indicators from contextual noise and adopts a causal-prioritized pairwise
ranking objective to explicitly optimize causal attribution. Comprehensive
evaluations on public benchmarks demonstrate that DynaCausal consistently
surpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with
absolute gains from 0.25 to 0.46, and delivering both accurate and
interpretable diagnoses in highly dynamic microservice environments.

</details>


### [18] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: 本文系统分析了IDE中的代码模型置信度校准问题，发现大规模通用校准效果不佳，个性化校准依赖大量用户数据，而开发者更青睐颜色编码等非数字化置信度展示方式，对未来AI辅助开发工具的设计具有重要指导意义。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在集成开发环境（IDE）中的应用正在革新软件工程，但AI生成代码的可靠性和实用性仍存疑，为提高模型置信度与用户接受度的对齐，需探索置信度校准方法。

Method: 提出了可扩展且灵活的置信度校准框架，可用于开源模型并支持任意数据集，并通过大量真实开发者交互数据进行评估。同时，结合场景设计、多阶段访谈与问卷调查，分析开发者对置信度信息的偏好。

Result: 基于Platt-scaling的通用后处理校准在整体上未能提升模型置信度信号的可靠性。个性化校准对部分用户有效，但高度依赖交互数据量。大多数开发者倾向于非数字型、颜色编码的置信度展示方式。

Conclusion: 通用置信度校准方法对提升代码生成模型在IDE中的可靠性贡献有限，更有效的方案需结合个性化和更自然的用户界面设计。

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [19] [Collaborative LLM Agents for C4 Software Architecture Design Automation](https://arxiv.org/abs/2510.22787)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的多代理系统自动生成并评估C4软件架构模型，实现了效率和质量兼顾，推动了自动化架构设计的发展。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计是每个软件系统开发的关键部分，但当前主流的C4架构模型的制作过程仍然依赖手工，效率低下且耗时，因此亟需自动化解决方案。

Method: 提出了一个基于大语言模型（LLM）的多智能体系统，通过模拟不同角色专家之间的对话，自动分析需求并生成C4模型的Context、Container和Component视图。评估流程结合了结构完整性、一致性等确定性检查，与采用LLM作为评委的语义与质量评分。

Result: 在五个典型系统需求测试中，提出的方法能够快速生成C4模型，维持高编译成功率，并具备良好的语义匹配度。还比较了四种主流LLM在架构设计中的表现差异。

Conclusion: 本研究为软件架构设计与其自动化评估方法提供了新的技术途径，对相关领域具有推动作用。

Abstract: Software architecture design is a fundamental part of creating every software
system. Despite its importance, producing a C4 software architecture model, the
preferred notation for such architecture, remains manual and time-consuming. We
introduce an LLM-based multi-agent system that automates this task by
simulating a dialogue between role-specific experts who analyze requirements
and generate the Context, Container, and Component views of the C4 model.
Quality is assessed with a hybrid evaluation framework: deterministic checks
for structural and syntactic integrity and C4 rule consistency, plus semantic
and qualitative scoring via an LLM-as-a-Judge approach. Tested on five
canonical system briefs, the workflow demonstrates fast C4 model creation,
sustains high compilation success, and delivers semantic fidelity. A comparison
of four state-of-the-art LLMs shows different strengths relevant to
architectural design. This study contributes to automated software architecture
design and its evaluation methods.

</details>


### [20] [On the Freshness of Pinned Dependencies in Maven](https://arxiv.org/abs/2510.22815)
*Vasudev Vikram,Yuvraj Agarwal,Rohan Padhye*

Main category: cs.SE

TL;DR: 本研究揭示了Maven流行库中依赖固定普遍存在过时风险，影响安全与维护。提出的Pin-Freshener工具可通过引入来自其它项目的测试信号，帮助开发者安全升级依赖，有效减少安全漏洞，提升维护信心。


<details>
  <summary>Details</summary>
Motivation: 随着软件生态系统中库依赖的不断演进，开发者通常会固定（pin）某些库的版本以保证可复现性和稳定性，但这也可能导致使用过时且有安全漏洞的依赖版本。作者希望研究依赖版本固定行为的普遍性及其带来的后果。

Method: 首先定义了“过时固定”（stale pin）和“新鲜固定”（fresh pin）的概念，根据依赖的发布历史评估其新旧。通过实证研究分析了Maven库中依赖固定的现状，并提出Pin-Freshener原型工具，通过利用其它项目的众包测试结果为依赖升级安全性提供信号。最后在Maven热门500库的实际依赖升级上进行了评估。

Result: 发现超过60%的热门Maven库用户存在过时依赖固定，部分版本已过期一年以上，无法获得安全修复。10%的依赖升级可减少安全漏洞。Pin-Freshener在升级过程中，通过引入额外的测试覆盖，显著提升了对依赖升级安全的信心，在3000多个项目升级中能提供至少5个通过的众包测试套件信号。

Conclusion: 依赖固定有助于稳定性，但会带来安全隐患。Pin-Freshener工具可通过众包测试信号，提高开发者对依赖升级的信心，有助于安全地保持依赖新鲜。

Abstract: Library dependencies in software ecosystems play a crucial role in the
development of software. As newer releases of these libraries are published,
developers may opt to pin their dependencies to a particular version. While
pinning may have benefits in ensuring reproducible builds and avoiding breaking
changes, it bears larger risks in using outdated dependencies that may contain
bugs and security vulnerabilities. To understand the frequency and consequences
of dependency pinning, we first define the concepts of stale and fresh pins,
which are distinguished based on how outdated the dependency is relative to the
release date of the project. We conduct an empirical study to show that over
60% of consumers of popular Maven libraries contain stale pins to their
dependencies, with some outdated versions over a year old. These pinned
versions often miss out on security fixes; we find that 10% of all dependency
upgrades in our dataset to the latest minor or patch version would reduce
security vulnerabilities.
  We prototype an approach called Pin-Freshener that can encourage developers
to freshen their pins by leveraging the insight that crowdsourced tests of peer
projects can provide additional signal for the safety of an upgrade. Running
Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites
can provide 35-100% more coverage of a dependency, compared to that of a single
consumer test suite. Our evaluation on real-world pins to the top 500 popular
libraries in Maven shows that Pin-Freshener can provide an additional signal of
at least 5 passing crowdsourced test suites to over 3,000 consumers to safely
perform an upgrade that reduces security vulnerabilities. Pin-Freshener can
provide practical confidence to developers by offering additional signal beyond
their own test suites, representing an improvement over current practices.

</details>


### [21] [CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs](https://arxiv.org/abs/2510.22986)
*Junjie Huang,Minghua He,Jinyang Liu,Yintong Huo,Domenico Bianculli,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了可自动构造规则函数的日志异常检测新方法CodeAD，在多数据集上显著优于现有方法，兼具效率、解释性和低成本，适用于真实环境的在线监控。


<details>
  <summary>Details</summary>
Motivation: 日志异常检测对于保障大规模在线服务系统的可靠性和可用性至关重要。现有基于机器学习、深度学习及大语言模型的方法虽有所进展，却存在解释性差、推理成本高及预处理需求复杂等不足，不适用于实时海量日志处理；相对而言，规则系统高效但需人工大量参与，难以扩展。

Method: 提出了CodeAD框架，利用大语言模型自动合成轻量级Python规则函数，实现日志异常检测。它结合分层聚类和锚点采样策略构建对比日志窗口，引导LLM识别异常模式，并通过代理式工作流持续生成、测试、修复和优化规则，确保其正确性与抽象性。合成的规则可直接对原始日志执行，无需复杂预处理。

Result: 在BGL、Hadoop、Thunderbird三个公开数据集上，CodeAD相比现有方法平均F1分数提升3.6%，处理速度提升至4倍，成本降至单数据集不到4美元。合成规则具备良好解释性、轻量化特征，可实时高效且透明地支持异常检测。

Conclusion: CodeAD是一种实用且可扩展的日志异常检测解决方案，实现了高效、自动化、可解释的在线监测，极大提高了大规模系统的异常检测性能。

Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the
reliability and availability of large-scale online service systems. While
machine learning, deep learning, and large language models (LLMs)-based methods
have advanced the LogAD, they often suffer from limited interpretability, high
inference costs, and extensive preprocessing requirements, limiting their
practicality for real-time, high-volume log analysis. In contrast, rule-based
systems offer efficiency and transparency, but require significant manual
effort and are difficult to scale across diverse and evolving environments. In
this paper, We present CodeAD, a novel framework that automatically synthesizes
lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a
hierarchical clustering and anchor-grounded sampling strategy to construct
representative contrastive log windows, enabling LLMs to discern discriminative
anomaly patterns. To ensure robustness and generalizability, CodeAD employs an
agentic workflow that iteratively generates, tests, repairs, and refines the
rules until it meets correctness and abstraction requirements. The synthesized
rules are interpretable, lightweight, and directly executable on raw logs,
supporting efficient and transparent online anomaly detection. Our
comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)
demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1
score over the state-of-the-art baselines, while processing large datasets up
to 4x faster and at a fraction of the cost (total LLM invocation cost under 4
USD per dataset). These results highlight CodeAD as a practical and scalable
solution for online monitoring systems, enabling interpretable, efficient, and
automated LogAD in real-world environment.

</details>


### [22] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: TALM是一种新型多智能体协作框架，通过结构化分解、分治推理和长时记忆，有效提升了代码生成任务的推理能力和效率，实验结果优异，具有较高实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体代码生成框架存在流程刚性和推理恢复成本高等问题，限制了复杂任务的解决能力。作者旨在通过引入灵活的树结构协作和长时记忆机制，提升模型的复杂上下文管理和多步骤推理能力。

Method: 提出了Tree-Structured Multi-Agent Framework with Long-Term Memory（TALM）——一种动态多智能体框架，采用树状协作结构、长时记忆模块、结构化任务分解、本地化重推理，结合父子节点的分治策略实现高效推理和纠错。

Result: 在HumanEval、BigCodeBench、ClassEval等基准测试中，TALM展现了持续强劲的推理能力和高令牌效率，验证了其在多样化代码生成任务中的性能与鲁棒性。

Conclusion: TALM框架在复杂代码生成任务中表现出高鲁棒性和实用性，能够显著提升推理性能和令牌效率。

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [23] [From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks](https://arxiv.org/abs/2510.23055)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: 本文实证研究了轻量级开源LLMs在需求工程相关的用户反馈分析上的应用效果，验证其在分类与需求规格说明生成等任务上的有效性，并提供了复现包和能力分析。


<details>
  <summary>Details</summary>
Motivation: 在线用户反馈对于需求工程非常有价值，但由于其数量大且杂音多，分析存在挑战。大语言模型（LLMs）有潜力自动化该过程并超越以往技术，同时也能完成如生成需求规格说明等新任务。

Method: 实证评估了五种轻量级开源LLMs在三个需求工程任务上的表现：用户请求分类、非功能性需求（NFR）分类，以及需求规格说明的生成。结合两个反馈数据集进行分类性能评估，并通过人工评价需求规格说明的质量。

Result: LLMs在分类任务上取得了中高水平的准确率（F1约0.47-0.68），在需求规格说明生成上获得了较高的人工评分（均值约3/5）。

Conclusion: 轻量级LLMs适用于基于反馈的需求开发，能在多个需求工程相关任务上取得可观表现。

Abstract: [Context and Motivation] Online user feedback provides valuable information
to support requirements engineering (RE). However, analyzing online user
feedback is challenging due to its large volume and noise. Large language
models (LLMs) show strong potential to automate this process and outperform
previous techniques. They can also enable new tasks, such as generating
requirements specifications.
  [Question-Problem] Despite their potential, the use of LLMs to analyze user
feedback for RE remains underexplored. Existing studies offer limited empirical
evidence, lack thorough evaluation, and rarely provide replication packages,
undermining validity and reproducibility.
  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on
three RE tasks: user request classification, NFR classification, and
requirements specification generation. Classification performance was measured
on two feedback datasets, and specification quality via human evaluation. LLMs
achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and
moderately high specification quality (mean ~ 3/5).
  [Contributions] We newly explore lightweight LLMs for feedback-driven
requirements development. Our contributions are: (i) an empirical evaluation of
lightweight LLMs on three RE tasks, (ii) a replication package, and (iii)
insights into their capabilities and limitations for RE.

</details>


### [24] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 本文提出Checkstyle+，通过融合大语言模型及传统规则，能更有效地检测复杂语义下的Java代码风格违规，性能优于单一规则引擎。


<details>
  <summary>Details</summary>
Motivation: 良好的代码风格提高了程序的可读性、可维护性和协作效率，但开发者常常随意处理风格规则，导致专业开发中广泛使用像Checkstyle这样的自动化工具。但这些工具仅能检测表层的风格问题，对于需要更深层代码理解的规则则不适用。

Method: 作者提出了一种混合方法Checkstyle+，将大语言模型（LLM）与传统Checkstyle结合，从而检测那些常规静态分析难以覆盖的语义性风格违规。

Result: Checkstyle+在30,800个真实Java程序中采样的380个文件上进行评测，相比标准Checkstyle，在检测语义细致的风格违规方面取得了更好的性能。

Conclusion: 将LLM与传统风格检测工具结合，可以显著提升对复杂代码风格违规的检测能力。

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>


### [25] [Validating Formal Specifications with LLM-generated Test Cases](https://arxiv.org/abs/2510.23350)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: 该论文实证证明GPT-5等LLM能够自动生成符合需求的Alloy测试用例，既提高规范校验效率，也可帮助发现规范错误，自动化方法具实用价值。


<details>
  <summary>Details</summary>
Motivation: 验证对于形式化规范开发至关重要，但手动编写需要满足的测试用例费时且容易出错，导致用户常常跳过这一步。因此，作者希望探索自动化生成测试用例的可行性。

Method: 利用预训练大型语言模型（LLMs），如GPT-5，根据自然语言需求自动生成测试用例，针对Alloy规范语言的结构性需求，进行实证评估，对比了多种封闭和开源LLMs的表现。

Result: GPT-5等LLM在生成符号正确、能满足（或不满足）特定需求的正负测试用例上表现优异，并能有效发现人工编写的错误规范。

Conclusion: 利用GPT-5等LLM自动化生成测试用例，能够显著提升规范验证的效率和准确性，减少人工负担，提高规范质量。

Abstract: Validation is a central activity when developing formal specifications.
Similarly to coding, a possible validation technique is to define upfront test
cases or scenarios that a future specification should satisfy or not.
Unfortunately, specifying such test cases is burdensome and error prone, which
could cause users to skip this validation task. This paper reports the results
of an empirical evaluation of using pre-trained large language models (LLMs) to
automate the generation of test cases from natural language requirements. In
particular, we focus on test cases for structural requirements of simple domain
models formalized in the Alloy specification language. Our evaluation focuses
on the state-of-art GPT-5 model, but results from other closed- and open-source
LLMs are also reported. The results show that, in this context, GPT-5 is
already quite effective at generating positive (and negative) test cases that
are syntactically correct and that satisfy (or not) the given requirement, and
that can detect many wrong specifications written by humans.

</details>


### [26] [Floating-Point Neural Network Verification at the Software Level](https://arxiv.org/abs/2510.23389)
*Edoardo Manino,Bruno Farias,Rafael Sá Menezes,Fedor Shmarov,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: 该论文提出了针对神经网络浮点实现的安全性验证方法，并开发了NeuroCodeBench 2.0基准集，对主流验证工具进行了评测，推动了软件级神经网络验证工具的进步。


<details>
  <summary>Details</summary>
Motivation: 为了在安全关键系统中正确部署神经网络，有必要解决现有验证技术无法在软件级别完全证实神经网络正确性的问题。

Method: 通过明确考虑神经网络的浮点实现，指定并验证其安全性，同时构建了NeuroCodeBench 2.0基准集，用于系统性测试八种主流软件验证器。

Result: 构建了覆盖激活函数、常用层以及多达17万参数的神经网络的912个验证样例，并首次系统性评估了八个验证工具，在基准集发布后发现工具误判率已有显著下降。

Conclusion: 现有的自动化验证工具在神经网络代码上的正确解决率平均为11%，误判率约3%；发布的新基准集推动了该领域的发展。

Abstract: The behaviour of neural network components must be proven correct before
deployment in safety-critical systems. Unfortunately, existing neural network
verification techniques cannot certify the absence of faults at the software
level. In this paper, we show how to specify and verify that neural networks
are safe, by explicitly reasoning about their floating-point implementation. In
doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural
network verification examples that cover activation functions, common layers,
and full neural networks of up to 170K parameters. Our verification suite is
written in plain C and is compatible with the format of the International
Competition on Software Verification (SV-COMP). Thanks to it, we can conduct
the first rigorous evaluation of eight state-of-the-art software verifiers on
neural network code. The results show that existing automated verification
tools can correctly solve an average of 11% of our benchmark, while producing
around 3% incorrect verdicts. At the same time, a historical analysis reveals
that the release of our benchmark has already had a significantly positive
impact on the latter.

</details>


### [27] [Tracing Distribution Shifts with Causal System Maps](https://arxiv.org/abs/2510.23528)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: 该论文提出了一种创新方法ML System Maps，通过分层因果映射提升机器学习系统分布漂移的归因效率和自动化程度。


<details>
  <summary>Details</summary>
Motivation: 当前对机器学习系统的监控主要集中在分布漂移的检测，而分布漂移的根本原因分析通常依赖手动追踪，难以明确区分导致漂移的是软件故障、数据质量问题或自然变化。因此，亟需更系统化、自动化的因果分析方法来提升检测和定位准确性。

Method: 提出ML System Maps，即通过分层式因果映射，显式展示环境因素与机器学习系统内部之间的传播路径。这种方法强化了对分布漂移归因的系统化和可追溯性。作者还概述了该方法的具体实现方案及后续研究计划。

Result: 通过该因果地图方法，可系统性地将分布漂移与潜在原因进行归因，辅助自动化和高效的根因分析。为ML系统的稳定性和可靠性运维带来新的工具与理论基础。

Conclusion: ML System Maps为分布漂移检测和根因分析提供了创新的因果映射框架，有望取代传统依赖手动追踪的方式，将漂移归因过程系统化、自动化，并推动相关后续研究。

Abstract: Monitoring machine learning (ML) systems is hard, with standard practice
focusing on detecting distribution shifts rather than their causes. Root-cause
analysis often relies on manual tracing to determine whether a shift is caused
by software faults, data-quality issues, or natural change. We propose ML
System Maps -- causal maps that, through layered views, make explicit the
propagation paths between the environment and the ML system's internals,
enabling systematic attribution of distribution shifts. We outline the approach
and a research agenda for its development and evaluation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [28] [Proceedings of the Combined 32nd International Workshop on Expressiveness in Concurrency and 22nd Workshop on Structural Operational Semantics](https://arxiv.org/abs/2510.23211)
*Cinzia Di Giusto,Giorgio Bacci*

Main category: cs.LO

TL;DR: EXPRESS/SOS 2025论文集收录了并发性表达力和结构操作语义领域的最新研究成果，体现学术界对此方向的持续关注和重大进展。


<details>
  <summary>Details</summary>
Motivation: EXPRESS/SOS 2025旨在汇集对系统及编程概念的形式语义及计算模型表现力感兴趣的研究者，强调相关理论基础的重要性。

Method: 作为会议论文集，本论文采用研讨会收录的形式，涵盖参会者提交的最新研究、理论探讨和进展综述，致力于推动领域内学术交流。

Result: 会议成功举办，业界专家学者就并发性表达力与结构操作语义进行了深度交流与合作，论文集反映出领域最新研究成果和热点议题。

Conclusion: EXPRESS/SOS 2025通过促进学术交流，推动并发性和结构操作语义领域的发展，为相关系统与编程语言的理论及应用奠定了坚实基础。

Abstract: This volume contains the proceedings of EXPRESS/SOS 2025: the Combined 32nd
International Workshop on Expressiveness in Concurrency and the 22nd Workshop
on Structural Operational Semantics, which was held in Aarhus, Denmark, as an
affiliated workshop of CONFEST 2025. The EXPRESS/SOS workshop series aims at
bringing together researchers interested in the formal semantics of systems and
programming concepts, and in the expressiveness of computational models.

</details>


### [29] [Possibilistic Computation Tree Logic: Decidability and Complete Axiomatization](https://arxiv.org/abs/2510.23075)
*Yongming Li*

Main category: cs.LO

TL;DR: 论文首次解决了PoCTL的可满足性判定问题，通过构造可能性Hintikka结构证明了其指数时间判定性，并给出了PoCTL的完整公理化体系，为不确定信息系统的验证提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 此前PoCTL模型检测问题已经有研究，但关于PoCTL的可满足性问题尚未讨论。为推动含不确定信息系统的自动化验证，需要对PoCTL的可满足性问题及其推理基础做系统研究。

Method: 作者引入了从PoCTL公式中抽取可能性信息的技术，并构造了可能性Hintikka结构，以解决PoCTL的可满足性判定。同时，论文还提出了PoCTL的完整公理化体系。

Result: 证明了PoCTL的可满足性问题在指数时间内可判定，并给出了PoCTL的完整公理化体系。

Conclusion: 该研究不仅首次系统讨论了PoCTL的可满足性问题并证实其可判定性，还补充完整了PoCTL的公理化推理基础，推动了该逻辑在不确定信息系统自动化验证领域的理论发展。

Abstract: Possibilistic computation tree Logic (PoCTL) is one kind of branching
temporal logic combined with uncertain information in possibility theory, which
was introduced in order to cope with the systematic verification on systems
with uncertain information in possibility theory. There are two decision
problems related to PoCTL: the model checking problem and the satisfiability
problem. The model checking problem of PoCTL has been studied, while the
satisfiability problem of PoCTL was not discussed. One of the purpose of this
work is to study the satisfiability problem of PoCTL. By introducing some
techniques to extract possibility information from PoCTL formulae and
constructing their possibilistic Hintikka structures, we show that the
satisfiability problem of PoCTL is decidable in exponential time. Furthermore,
we give a complete axiomatization of PoCTL, which is another important
inference problem of PoCTL.

</details>


### [30] [Parametric Iteration in Resource Theories](https://arxiv.org/abs/2510.23413)
*Alessandro Di Giorgio,Pawel Sobocinski,Niels Voorneveld*

Main category: cs.LO

TL;DR: 论文提出了一种更抽象的方式来分析依赖安全参数的算法，利用资源理论和参数化迭代，在部分概率模型下以组合和可视化的方式推导出密码学的可忽略性结论。


<details>
  <summary>Details</summary>
Motivation: 许多算法，尤其是密码学协议，会涉及到一个固定但未指定的参数（如密钥长度），需要用更通用和抽象的方法来刻画和分析这些参数带来的资源变化及相关的安全性。

Method: 提出了一种通用的参数化迭代构造方法，利用资源理论与字符串图示（string diagram）语法，在概率布尔电路的Markov范畴中实现，用度量空间描述渐进等价和可忽略性。

Result: 该方法实现了用图式推理证明简单的密码学定理，例如证明猜测随机密钥的成功率在渐进意义下是可忽略的。

Conclusion: 通过引入参数化迭代构造，并在Markov范畴下用合适的度量衡来描述资源理论，可以组合地刻画概率过程在安全参数变化下的可忽略性，实现更直观的密码学推理。

Abstract: Many algorithms are specified with respect to a fixed but unspecified
parameter. Examples of this are especially common in cryptography, where
protocols often feature a security parameter such as the bit length of a secret
key.
  Our aim is to capture this phenomenon in a more abstract setting. We focus on
resource theories -- general calculi of processes with a string diagrammatic
syntax -- introducing a general parametric iteration construction. By
instantiating this construction within the Markov category of probabilistic
Boolean circuits and equipping it with a suitable metric, we are able to
capture the notion of negligibility via asymptotic equivalence, in a
compositional way. This allows us to use diagrammatic reasoning to prove simple
cryptographic theorems -- for instance, proving that guessing a randomly
generated key has negligible success.

</details>


### [31] [On the entailment problem for DL-Lite$_{core}$ ontologies and conjunctive queries with negation](https://arxiv.org/abs/2510.23490)
*Jerzy Marcinkowski,Piotr Ostropolski-Nalewaja*

Main category: cs.LO

TL;DR: DL-Lite$_{core}$本体遇到带不等式或安全否定的结合查询时，蕴含判定问题是不可判定的，说明该逻辑在相关场景下存在理论上的局限性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨DL-Lite$_{core}$本体下，带有不等式的结合查询的蕴含问题是否可判定。此前相关判定性问题在语义网络、知识表示和数据库整合等领域有广泛研究，但大多集中在无不等式或无负查询的情景。

Method: 作者通过构造规约与复杂性理论的方法，分析结合不等式和安全否定的结合查询对DL-Lite$_{core}$本体蕴含问题判定性的影响。

Result: 在包含不等式的结合查询下，DL-Lite$_{core}$本体的蕴含问题是不可判定的。当查询从不等式更换为安全否定时，该问题依然不可判定。

Conclusion: 无论是在不等式还是安全否定结合查询的情况下，DL-Lite$_{core}$本体的蕴含问题都不可判定，这极大限制了该本体语言在需要判定复杂查询的实际应用中的可用性。

Abstract: We show that the entailment problem, for a given entailment problem for
DL-Lite$_{core}$ ontology, and given conjunctive query with inequalities, is
undecidable.
  We also show that this problem remains undecidable if conjunctive queries
with safe negation are considered instead of conjunctive queries with
inequalities.

</details>


### [32] [Generalized Kantorovich-Rubinstein Duality beyond Hausdorff and Kantorovich](https://arxiv.org/abs/2510.23552)
*Paul Wild,Lutz Schröder,Karla Messing,Barbara König,Jonas Forster*

Main category: cs.LO

TL;DR: 本文探讨了Wasserstein提升与Kantorovich提升之间的关系，证明一般需要额外模态，并在Lévý-Prokhorov距离和分布凸集标准度量两种情况下实现了广义Kantorovich-Rubinstein对偶性，为相关度量理论和应用提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 经典的Kantorovich-Rubinstein对偶性为基于运输计划（耦合）和价格函数定义的概率分布空间上的度量提供了一致性。该理论已被提升到集合函子的层次，但两种提升——Wasserstein提升和Kantorovich提升——需要选择定量模态（quantitative modalities），且其中转换关系尚有不完全一致之处。作者旨在厘清不同提升方式所需模态的一致性及其限制。

Method: 作者通过理论分析和具体反例，展示了Wasserstein提升在表达为Kantorovich提升时，确实需要附加的模态（不可避免）。在此基础上，作者进一步研究了在何种情况下可以使用相同模态，从而实现广义Kantorovich-Rubinstein对偶性，并对两个重要度量场景进行了证明。

Result: 作者给出了一个反例，证明在一般情况下必须使用额外的模态。此外，作者在两个重要情形下证明了广义Kantorovich-Rubinstein对偶性可以成立：一是应用广泛的Lévý-Prokhorov距离，二是结合Hausdorff和Wasserstein距离得到的分布凸集上的标准度量。

Conclusion: 广义Kantorovich-Rubinstein对偶性在某些重要度量场景下成立，如Lévý-Prokhorov距离和分布凸集的标准度量。然而，在一般情况下，Kantorovich提升无法仅用原始模态表达Wasserstein提升，必须引入新的模态。

Abstract: The classical Kantorovich-Rubinstein duality guarantees coincidence between
metrics on the space of probability distributions defined on the one hand via
transport plans (couplings) and on the other hand via price functions. Both
constructions have been lifted to the level of generality of set functors, with
the coupling-based construction referred to as the Wasserstein lifting, and the
price-function-based construction as the Kantorovich lifting, both based on a
choice of quantitative modalities for the given functor. It is known that every
Wasserstein lifting can be expressed as a Kantorovich lifting; however, the
latter in general needs to use additional modalities. We give an example
showing that this cannot be avoided in general. We refer to cases in which the
same modalities can be used as satisfying the generalized
Kantorovich-Rubinstein duality. We establish the generalized
Kantorovich-Rubinstein duality in this sense for two important cases: The
L\'evy-Prokhorov distance on distributions, which finds wide-spread
applications in machine learning due to its favourable stability properties,
and the standard metric on convex sets of distributions that arises by
combining the Hausdorff and Wasserstein distances.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

Main category: cs.CL

TL;DR: 作者建立并公开了一个涵盖多语种与四大类（致谢、数据、代码、临床试验）段落的科学文献数据集，为相关文本挖掘和分类任务带来便利。


<details>
  <summary>Details</summary>
Motivation: 现有科学文献挖掘对语料库资源的需求巨大，特别是提取特定信息（如致谢、数据、代码、临床试验等）的信息抽取任务缺乏公开多语种数据集。

Method: 从法国开放科学监测语料库中抽取文本段落，使用GROBID进行处理、fastText进行语言识别、OpenAlex确定学科领域，对每个段落进行分类和标注并最终形成数据集。

Result: 构建了包含833,000个段落、覆盖英语、法语及其他欧洲语言的多类别科学文献数据集，并公开发布于HuggingFace平台，全部数据拥有CC-BY授权。

Conclusion: 本文建立了科学出版物多类别、多语种的高质量开放数据集，为科学文献挖掘、文本分类和命名实体识别模型训练提供了重要资源。

Abstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [34] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang,Lanser Contributors*

Main category: cs.CL

TL;DR: Lanser-CLI是一个基于CLI的编排系统，将语言服务器能力通过API暴露给智能体和CI流程，解决了API幻觉与编辑定位不准等难题，实现了安全、可确定和可复现的代码操作及检测机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理API时经常出现幻觉现象，且易于出现代码编辑定位错误，而语言服务器能够对真实代码进行验证并提供IDE级别的事实信息。现有工具难以充分利用语言服务器这些能力来让智能体与代码变化进行精准联动。

Method: 设计实现了Selector DSL用于健壮定位，分析包用于规范化语言服务器响应与环境信息记录，安全封装变更操作，并提出基于诊断、分歧置信度等事实的过程奖励函数，对快照环境下的确定性和奖励单调性也做了数学形式化证明。

Result: 提出了Lanser-CLI工具，它通过CLI作为编排层与LSP语言服务器链接，为代码智能体和CI流程提供可确定、可复现的工作流，解决了编辑定位脆弱等难题，并提供了多种新特性，包括符号性及AST路径选择器、分析包、变更安全包裹及过程奖励机制。

Conclusion: Lanser-CLI通过提升定位健壮性、变更安全性和操作可确定性，把语言服务器的事实与奖励机制结合，为代码智能体的计划执行、监督和分析提供了坚实基础，是促进代码自动化与质量管控的重要工具。

Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [35] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal,Aakash Sen Sharma,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 本文系统研究了在开放式链式思考下的策略优化行为，发现算法会逃避复杂推理而只追求最简答案输出，即使额外奖励复杂行为也未能逆转。此现象对语言模型对齐中的奖励设计提出重要警示。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的策略优化往往在链式思考（CoT）时采用严格的“先思考再回答”格式，但当这种格式被放宽，允许更开放的推理与回答交替时，PO算法的行为尚未被充分研究。作者旨在填补此空白，探究不拘一格的CoT结构下，策略优化的表现与原理。

Method: 作者通过一系列严密的对照实验，系统性地分析不同模型和算法在奖励分解与行为表现上的变化，包括设定不同奖励权重、奖励分解实验，并考察KL正则化下策略如何在较大自由度下发生偏移。

Result: 实验发现：只要模型允许在推理和回答间灵活切换，策略优化算法总会倾向于舍弃复杂的显式推理步骤，仅学习直接<answer>输出。这种退化现象无论在模型类型、算法细节或提升复杂格式的奖励权重（高达原本的4倍）均无法克服。通过奖励分解实验，作者揭示PO始终优先追求最简单、最直接的奖励成分，哪怕面对更强的复杂行为诱因。

Conclusion: 策略优化算法具有“趋利最简路径”的固有倾向。开放自由度虽能帮助模型发现高奖快捷方案，但同时也极易推动模型“奖励游戏”（reward hacking）；这给对齐下奖励设计提出了重大挑战。

Abstract: Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [36] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 本文提出一种轻量级候选响应重排序方法Language Ranker，极大降低训练与推理成本，在多个任务上表现与大规模奖励模型相当，展现出提升LLM解码过程的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）研究主要关注于优化模型的输出分布，忽略了解码过程的重要性。近年来基于奖励模型的推理方法尽管强调了解码过程，但普遍存在计算成本高和适用性有限的缺点。本文受推荐系统启发，将解码过程类比为推荐系统的排序阶段，以此为切入点发现传统解码及奖励模型存在诸多不足。

Method: 提出了Language Ranker框架，在基础模型的候选响应中，通过轻量级模块基于特征进行重新排序（rerank），仅增加少量参数，极大提升效率并减少计算开销。

Result: 在广泛任务上实验证明，Language Ranker可在只增加不到50万参数的前提下，实现与大规模奖励模型相当的性能，并大幅减少训练和推理时的计算成本。

Conclusion: Language Ranker在效率和效果上兼具优势，有助于最大化发挥LLM的能力。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [37] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: 本文提出RACE框架，系统评估LLM解释与传统模型特征的对齐情况，发现正确预测中解释更真实，错误预测中可能放大误导。


<details>
  <summary>Details</summary>
Motivation: 随着ML在敏感领域的应用增多，对于可解释AI的需求提升。然而，目前LLM生成的解释是否真正反映了模型决策的依据还不清楚。该工作旨在系统定量分析LLM解释的真实性，对比其与传统可解释方法的差异。

Method: 本文提出了RACE（Reasoning Alignment for Completeness of Explanations）框架，用于系统性评估大语言模型（LLM）生成的解释与基准逻辑回归模型提取的可解释特征重要性分数之间的一致性。利用token级、字符串精确匹配和编辑距离匹配三种对齐方式，分析四个文本分类数据集中的LLM解释与支持/矛盾特征的对应关系。

Result: 实证结果发现，在正确预测时，LLM解释能较好地覆盖支持性特征；错误预测时，则较多涉及矛盾性特征。编辑距离匹配方法发现了解释中的同义表达，提高了覆盖度，同时保持这种预测上的非对称性。

Conclusion: LLM的解释同时使用了表层和更灵活的证据重用，但在错误时也可能放大误导信息。RACE框架为评估神经语言模型解释的真实性和完整性提供了定量工具和新见解。

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [38] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham,Mihir Thalanki,Michael Sun,Aditya Chaloo,Ankita Gupta,Tian Xia,Aditya Mate,Ehimwenma Nosakhare,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 论文提出一种利用行为与语义多样性采样安全数据的方法，在微调大型语言模型时既能有效减少有害输出，又能维持有用性，实现高效安全微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调过程中，常常会丢失先前已经对齐的安全行为，即灾难性遗忘现象。虽然已有工作发现添加随机安全样本可以缓解这一问题，但目前尚不清楚哪些样本最为有效。

Method: 提出了一种行为感知采样框架，根据指令-响应行为（如拒绝与服从）以及有害类别的语义多样性两个互补因素来选择安全样本。

Result: 通过系统评估显示，该方法能在只增加0.5%训练数据的情况下，将有害输出减少高达41%，同时保持模型的有用性。

Conclusion: 定向的数据选择能够在大规模微调过程中提升安全性和效率。

Abstract: Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [39] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 本文提出利用语义等向性这一嵌入分散度指标，无需标注数据和微调，就能高效判断大型语言模型长文本答案的可信度，且效果优于传统逐条事实核查方法。


<details>
  <summary>Details</summary>
Motivation: 当前在需要高度准确性的高风险领域部署大型语言模型（LLM）时，缺乏可靠且计算低成本的方法来评估LLM在开放式回答中的可信度。现有方法多依赖于逐条事实核查，不仅耗时且在长文本处理上效果不佳。

Method: 提出了一种新的语义等向性（semantic isotropy）指标，通过计算长文本答案的嵌入向量在单位球上的角度分散度来评估文本的一致性。具体做法是：生成多份长文本答案，将其嵌入到向量空间，测量嵌入分布的分散程度。

Result: 实验发现，语义等向性越高（嵌入分散度越大），则不同答案之间的事实一致性越低。该方法无需标注数据、微调或超参数选择，兼容多种嵌入模型。在多个领域实验中，本文方法能用少量样本有效预测非事实性，优于现有方案。

Conclusion: 提出的语义等向性方法可高效、低成本地评估LLM长文本答案的可信度，并有望部署到实际应用场景。

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [40] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing,Chang Tian,Jianan Zhang,Lichen Pan,Peipei Liu,Zhaoteng Yan,Yinliang Yue*

Main category: cs.CL

TL;DR: 本文提出了NetMind框架，通过创新的配置分块、统一事实表示和混合语言设计，实现了用自然语言高效查询和理解复杂网络行为，相较现有方法具备更高准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代大规模网络因配置复杂，容易造成错误，理解网络行为变得越来越困难。现有方法虽能通过分析网络配置文件来理解网络行为，但需要学习复杂的领域专用语言，且灵活性有限。自然语言作为更友好和易解释的界面，结合大语言模型（LLM），成为新的研究方向，但仍面临长上下文、设备与协议异构和复杂推理等挑战。

Method: 提出NetMind框架，包括树状配置分块以保证语义一致和高效划分，构建统一事实图来标准化不同厂商配置，并设计混合命令/声明式语言以减轻LLM推理负担并提升精度。并构建了自然语言问答与网络配置对齐的新基准数据集。

Result: NetMind在准确、可扩展的网络行为理解方面优于现有方法，在实验基准测试上表现突出。

Conclusion: NetMind结合弱化LLM推理和标准化配置的多项创新，实现了更高效、准确的网络行为查询和理解，解决了当前主流方法的不足。

Abstract: Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [41] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang,Han Cui,Yidong Wang,Yijian Tian,Qi Guo,Cunxiang Wang,Jian Wu,Chiyu Song,Yue Zhang*

Main category: cs.CL

TL;DR: 该文提出了基于递进式大纲生成与多模态融合的自动综述生成方法IterSurvey，大幅提升了自动综述在内容覆盖、结构连贯及引用质量等方面的表现，并提供了新的评测基准。


<details>
  <summary>Details</summary>
Motivation: 目前自动文献综述生成方法（one-shot paradigm）存在检索噪声大、结构碎片化和内容过载等问题，影响综述的质量。该研究受到人类研究者逐步阅读的启发，欲提升系统综述生成的连贯性与覆盖度。

Method: 提出IterSurvey（ours）框架，采用递归式大纲生成：由规划代理逐步检索、阅读并实时更新大纲，利用paper cards对每篇论文的贡献、方法、发现进行精炼，同时引入review-and-refine回路与可视化增强以提升文本流畅性并融合多模态元素（如图表）。

Result: 在既有及新兴话题实验中，IterSurvey显著优于当前最优方法，在内容覆盖率、结构连贯性及引用质量上表现更优，生成的综述更易懂且条理更佳。

Conclusion: IterSurvey通过拟人化的递进式规划与多模态集成，极大改进了自动文献综述的质量。为更客观评价，还提出了配套评测基准Survey-Arena，辅助与人工综述的对比。

Abstract: Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [42] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett,Tyler A. Chang,Stella Biderman,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文系统评估了97种语言单语分词器的token premium成因，并发现优化词表大小和预分词策略可以大幅减少跨语言token premium问题，提高多语种模型效率。


<details>
  <summary>Details</summary>
Motivation: 不同语言的并行文本在编码时所需的tokens数量（token premium）存在显著差异，这会影响模型训练与推理的效率和成本。作者希望理解这些差异背后的跨语言成因，并探索如何缓解token premium效应。

Method: 作者对97种语言、约7000个单语分词器系统性实验，控制变量包括分词算法、词表大小、数据集规模等，分析tokens消耗与不同语言特性、分词器训练与测试数据相似度、词表大小和预分词处理之间的关系。还引入superword分词器（允许跨空格合并）进行效果比较。

Result: 结果显示：1）训练和测试数据的相似性对token premium影响不大；2）词表大小与预分词处理对token premium有显著作用；3）增大词表并非总能减少token premium，但可为每种语言确定最优词表规模以降低该影响；4）superword分词器不仅减少token premium，还提升压缩效率。

Conclusion: 通过调整词表规模和预分词策略，可以有效减少跨语言的token premium效应，从而提升多语言模型的训练和推理效率。

Abstract: The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [43] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 本文提出MATT方法，用模型注意力行为指导分词器迁移，仅需少量计算时间即可在多语种环境中实现高效高质量迁移，远超以往基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型支持多种语言，但其预定义分词器在适应低资源或独特语系语言时仍存在瓶颈。目前的分词器迁移方法主要依赖语义上的启发式初始化新嵌入，忽略了模型更高层次的动态，导致迁移效果有限。

Method: 提出了MATT（Model-Aware Tokenizer Transfer）方法，将模型内部结构引入分词器迁移流程。MATT包含AIM（Attention Influence Modeling）目标，将源模型中的词元间通信模式蒸馏到新分词器的目标模型中，从而为后续标准语言建模步骤快速预热，并通过关注模型的注意力行为，引导嵌入初始化和适应过程。

Result: MATT在不同语言环境下的实验表明，仅需几个GPU小时即可恢复原模型较大比例的性能，明显优于现有启发式基线方法。

Conclusion: 引入模型层次信号对于多语种大模型的分词器迁移提供了实用高效的解决方案，显著提升了迁移质量和模型性能。

Abstract: Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [44] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay,Jiayi Chen,Mohammad J. Latifi,Daniel N. Rockmore,Jeremy R. Manning*

Main category: cs.CL

TL;DR: 本文利用GPT-2分别按作者训练，成功分辨作者身份，不仅展示了LLM捕捉写作风格的能力，还验证了Oz系列第15本的作者归属。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否能够有效捕捉并识别不同文学作者的独特写作风格与特征。

Method: 对每位作者的全部作品分别从头训练GPT-2模型，然后用各自模型去预测该作者的留出文本和其他作者文本，比较预测准确率。

Result: 每位作者专属的GPT-2模型对本作者文本预测更准确，进而通过这种方法确认了Oz系列第15本书的真正作者是R.P. Thompson，而非原本归属的F.L. Baum。

Conclusion: 利用单一GPT-2模型可以体现并区分不同作者的写作风格，从而准确地辨识文本作者。

Abstract: We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [45] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi,Julien Serbanescu,Fattane Zarrinkalam,Ali Dehghantanha*

Main category: cs.CL

TL;DR: 本研究结合社会科学劝说理论，提出更有效绕过大型语言模型安全防线的对抗性提示词，实验证明此类“劝说式攻击”成功率高，并揭示模型在响应时具有劝说风格指纹。


<details>
  <summary>Details</summary>
Motivation: 当前针对大型语言模型的Jailbreak攻击策略多样，但对相关语言和心理机制影响模型易受攻击的探讨较少。考虑到模型训练语料来自人类文本，推测模型或更易响应具有说服力的提示。

Method: 借鉴社会科学中的劝说理论，设计具有说服力结构的对抗性提示语，对多个已对齐的大型语言模型进行实证评估。

Result: 结果显示，融合劝说策略的提示词显著提升了绕过安全防护的成功率，并且模型在Jailbreak响应时呈现出独特的劝说性特征。

Conclusion: 跨学科结合社会科学中的劝说理论设计提示词能够有效绕过大型语言模型的安全防护，突显了融合不同领域知识应对模型安全挑战的重要性。

Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [46] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball,Niki Hasrati,Alexander Robey,Avi Schwarzschild,Frauke Kreuter,Zico Kolter,Andrej Risteski*

Main category: cs.CL

TL;DR: 本文系统分析了大模型越狱攻击中后缀迁移性，高亮了模型内部激活和方向变化等统计属性对迁移成功的决定性影响。辅助以实验，证明统计分析可用于指导和提升越狱攻击效果，对模型安全性理解与防护具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 离散优化驱动的大语言模型越狱攻击能够生成短小且无意义的后缀信息，这类后缀能使模型生成违禁内容，并表现出较强的可迁移性。但这种可迁移性缺乏系统分析，尚不清楚何时以及为何会出现迁移。

Method: 作者针对可迁移性的统计解释展开，提出并分析了影响可迁移性的三个统计属性：（1）无后缀提示语激活模型内部拒答方向的程度；（2）后缀对该拒绝方向的反向推动；（3）在与拒绝方向正交的方向上发生的幅度变化。并通过实验分析，用统计结果指导具体攻击的优化。

Result: 三大统计属性与攻击迁移成功率高度相关，而提示语语义相似性相关性很弱。基于分析，作者通过干预性实验，展示了利用统计分析能实际提升攻击成功率。

Conclusion: 通过统计学视角深入理解了越狱攻击可迁移性的本质，提出了影响攻击迁移能力的关键内部属性指标，有力推动了对大模型安全性与攻击防御的理论与实践提升。

Abstract: Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [47] [Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics](https://arxiv.org/abs/2510.22028)
*Yilin Zhang,Wenda Xu,Zhongtao Liu,Tetsuji Nakagawa,Markus Freitag*

Main category: cs.CL

TL;DR: 本文揭示了主流机器翻译质量评估指标存在明显长度偏差，导致对长翻译的不公正评价，并影响实际应用。通过长度归一化训练和引入参考文本，可有效缓解此问题，提高质量评估的精准性和公正性。


<details>
  <summary>Details</summary>
Motivation: 在机器翻译中，无参考质量评估（QE）指标至关重要，但其长度偏差问题尚未被充分研究。作者希望揭示并解决QE中的长度偏差对评价公平性的影响。

Method: 系统性研究了10组不同语言对下，主流回归模型和LLM-as-a-Judge两类QE指标的表现，分析其在不同长度翻译上的得分趋势。提出了训练时长度归一化和评估时引入参考文本两种缓解策略。

Result: 发现QE指标在翻译长度增加时会高估错误数量，对高质量长文本尤为不利，并倾向于更短的翻译候选。提出的两种缓解措施均有效降低了长度偏差。

Conclusion: 现有QE指标存在显著长度偏差，会影响QE重排和强化学习等实际应用效果。建议在模型训练和评估中采用相应策略以提升QE公平性和可靠性。

Abstract: Quality Estimation (QE) metrics are vital in machine translation for
reference-free evaluation and as a reward signal in tasks like reinforcement
learning. However, the prevalence and impact of length bias in QE have been
underexplored. Through a systematic study of top-performing regression-based
and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two
critical length biases: First, QE metrics consistently over-predict errors with
increasing translation length, even for high-quality, error-free texts. Second,
they exhibit a preference for shorter translations when multiple candidates are
available for the same source text. These inherent length biases risk unfairly
penalizing longer, correct translations and can lead to sub-optimal
decision-making in applications such as QE reranking and QE guided
reinforcement learning. To mitigate this, we propose two strategies: (a)
applying length normalization during model training, and (b) incorporating
reference texts during evaluation. Both approaches were found to effectively
reduce the identified length bias.

</details>


### [48] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre,Sneha Kudugunta,Niklas Muennighoff,I-Hung Hsu,Isaac Caswell,Alex Pentland,Sercan Arik,Chen-Yu Lee,Sayna Ebrahimi*

Main category: cs.CL

TL;DR: 本文通过大规模多语言实验证明了新的适应性迁移规模定律（ATLAS）能更好地指导多语种AI模型的训练和扩展，提升泛化性能，并为全球化AI模型设计提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 现有的关于AI模型的规模定律研究主要集中在英语，但实际应用中模型服务于多语言用户。作者希望通过大规模多语言实验揭示不同语言间的规模规律和迁移特性，从而指导面向全球用户的模型训练。

Method: 进行了774次多语言训练实验，涵盖10M到8B参数规模、400+训练语言和48种评测语言。提出了适应性迁移规模定律（ATLAS）并通过分析跨语种转移矩阵、语言无关的规模定律，以及多语种预训练与微调的计算交叉点，来系统研究多语言学习规律。

Result: ATLAS在单语和多语预训练场景下的泛化性能优于现有规模定律，提升超过0.3 R^2。实验还得到跨语种相互迁移收益矩阵，并揭示当增加语言数量时如何优化模型规模与数据，识别了预训练和微调的计算分界点。

Conclusion: 本文首次系统性地扩展和优化了AI模型的多语言规模定律，为模型从英语扩展到多语言提供科学依据，有助于相关模型设计的高效与民主化。

Abstract: Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [49] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman,Adar Avsian,Larry Heck*

Main category: cs.CL

TL;DR: 论文发现LLM内部隐藏着稳定、可操控的情感低维空间，这种空间对多语言和多情感数据都有效，能通过模块引导，实现精准的情感操控。


<details>
  <summary>Details</summary>
Motivation: 推动理解LLM内部如何表达、编码情感，揭示其对情感的抽象能力及是否存在统一的情感表示空间。

Method: 作者通过分析LLM隐藏状态空间的几何结构，寻找情感表现的低维流形，并研究其编码方式与分布，采用了多种情感数据集和跨语言实验。同时，设计了干预模块用于情感引导，并使用线性探测方法验证结果。

Result: 发现低维情感流形在模型深度和语言间均表现稳定，且能通过线性探测和方向性操控进行有效辨识与干预。

Conclusion: LLMs在其隐藏状态空间中表现出一致且可操控的情感几何结构。这种结构不仅具有普适性，还能在不同语言和数据集之间泛化，能够通过特定模块进行有效操控。

Abstract: This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [50] [Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds](https://arxiv.org/abs/2510.22084)
*Atij Mahesh*

Main category: cs.CL

TL;DR: 比较了六种偏见控制方法，发现只有监督微调和显式解码技术可有效满足组合性约束并提升自然度，偏好学习在逻辑组合任务中表现较差，强调了显式监督对公平流畅生成的必要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在职业中性环境下仍会生成性别刻板印象的语言，反映出深层的社会偏见。尽管已有多种方法抑制此类偏见，但各方法的比较效果和学习动态尚不清楚。

Method: 对六种偏见控制技术（prompt-only、generate-and-filter、基于DFA的Ctrl-G解码、监督微调（SFT）、直接偏好优化（DPO）、迭代零空间投影（INLP））进行对比分析。在由Winogender衍生的20个职业，要求生成既包含主动描述词又包含共融描述词的句子，并评估合规性、词汇多样性和流畅度间的权衡。

Result: SFT方法几乎完美满足任务约束且词汇多样性高（合规性99.87%±0.15），DPO尽管训练稳定，但合规性仅为4.53%±0.82。Ctrl-G方法保证完全合规，但牺牲了流畅性和多样性。偏好学习无法满足组合性约束，因为二元偏好仅编码排序而非逻辑合取。只有明确的正向监督能有效减轻组合性偏见。

Conclusion: SFT和Ctrl-G能有效控制偏见，但对于组合性约束，仅偏好学习难以适应，需明确的正向监督才能实现公平流畅的受控生成。偏好学习的局限性显现，凸显显式监督的重要性。

Abstract: Large Language Models (LLMs) still produce gender-stereotyped language even
in occupation-neutral contexts that reflect deep societal biases (Rudinger et
al., 2018). To address this, prior work has proposed prompting, constrained
decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and
fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).
However, the comparative efficacy and learning dynamics remain little
understood. We report a comparative analysis of six control techniques for bias
mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and
Iterative Nullspace Projection (INLP). We evaluate each method on a
compositional constraint task. This task requires generating sentences that
contain at least one agentic and one communal descriptor for each of the twenty
Winogender-derived occupations. We quantify trade-offs between control strength
and naturalness with evaluations of constraint compliance, lexical diversity,
and fluency. Our results reveal key contrasts among the methods: SFT achieves
99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite
similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect
compliance, but at the cost of severely reduced fluency and diversity.
Preference-based learning fundamentally differs: it cannot satisfy
compositional constraints, as binary preference signals encode ranking, not
logical conjunctions. Only explicit positive supervision enables mitigation of
compositional biases; preference-based alignment fails to generalize logical
structures, underscoring the limitations of preference learning and the
necessity of explicit supervision for fair and fluent controlled generation.

</details>


### [51] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

Main category: cs.CL

TL;DR: 本文提出基于信息瓶颈理论和动态激活引导的新方法（DMS），显著提升大模型推理和事实一致性，为可靠应用提供支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在训练数据中常常表现出很强的泛化能力和脆弱的机械记忆，这种不可预测性阻碍了其在高风险场景下的可靠应用。作者希望理解并控制模型的推理和记忆机制。

Method: 作者提出了一个基于信息瓶颈（IB）原理的理论模型，将泛化形式化为学习压缩的、与任务相关的表征，将记忆归类为压缩失败。基于该理论，作者设计了一种新的推理时方法“动态模式引导（Dynamic Mode Steering, DMS）”，包括两个部分：（1）基于因果的线性探针，用于检测模型在某一时刻对记忆的依赖；（2）动态激活引导机制，将模型推理朝向已知的泛化线路。

Result: 实验证明，DMS方法能显著提升大语言模型在推理和事实性任务中的逻辑一致性和事实准确性。

Conclusion: 本文提出的Dynamic Mode Steering（DMS）为提升大模型的可靠性提供了一种新途径，可以有效控制其泛化和记忆模式。

Abstract: Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [52] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本文提出对输入tokens做对数压缩，以此扩展transformer的长上下文处理能力，无需修改模型结构，在主流数据集上取得了更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数长上下文处理方法需要在transformer内部结构上增加复杂性，如引入循环或辅助记忆模块。作者希望寻找一种更简单的替代方案。

Method: 受人类认知记忆模型启发，本文提出将输入tokens进行尺度不变的对数压缩，从而在保持transformer结构不变的情况下实现长上下文处理。

Result: 在WikiText-103和PG-19两个语言建模基准测试上，对数压缩输入后，transformer模型的困惑度相比于未压缩基线有所降低。随着压缩后的时间上下文变长，模型性能持续提升。

Conclusion: 输入层面的对数压缩是一种结构简单且有效的方法，可以显著增强transformer的长距离记忆能力。

Abstract: Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [53] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team,Ang Li,Ben Liu,Binbin Hu,Bing Li,Bingwei Zeng,Borui Ye,Caizhi Tang,Changxin Tian,Chao Huang,Chao Zhang,Chen Qian,Chenchen Ju,Chenchen Li,Chengfu Tang,Chili Fu,Chunshao Ren,Chunwei Wu,Cong Zhang,Cunyin Peng,Dafeng Xu,Daixin Wang,Dalong Zhang,Dingnan Jin,Dingyuan Zhu,Dongke Hu,Fangzheng Zhao,Feifan Wu,Feng Zhu,Gangshan Wang,Haitao Zhang,Hailin Zhao,Hanxiao Zhang,Hanzi Wang,Hao Qian,Haoyi Yu,Heng Zhang,Hongliang Zhang,Hongzhi Luan,Huirong Dong,Huizhong Li,Jia Li,Jia Liu,Jialong Zhu,Jian Sha,Jianping Wei,Jiaolong Yang,Jieyue Ma,Jiewei Wu,Jinjing Huang,Jingyun Tian,Jingyuan Zhang,Jinquan Sun,Juanhui Tu,Jun Liu,Jun Xu,Jun Zhou,Junjie Ou,Junpeng Fang,Kaihong Zhang,Kaiqin Hu,Ke Shi,Kun Tang,Kunlong Chen,Lanyin Mei,Lei Liang,Lei Xu,Libo Zhang,Lin Ju,Lin Yuan,Ling Zhong,Lintao Ma,Lu Liu,Lu Yu,Lun Cai,Meiqi Zhu,Mengying Li,Min Chen,Minghao Xue,Minghong Cai,Mingming Yin,Peijie Jiang,Peilong Zhao,Pingping Liu,Qian Zhao,Qing Cui,Qingxiang Huang,Qingyuan Yang,Quankun Yu,Shaowei Wei,Shijie Lian,Shoujian Zheng,Shun Song,Shungen Zhang,Shuo Zhang,Siyuan Li,Song Liu,Ting Guo,Tong Zhao,Wanli Gu,Weichang Wu,Weiguang Han,Wenjing Fang,Wubin Wang,Xiang Shu,Xiao Shi,Xiaoshun Lan,Xiaolu Zhang,Xiaqing Sun,Xin Zhao,Xingyu Lu,Xiong Xu,Xudong Wang,Xudong Wang,Xuemin Yang,Yajie Yang,Yang Xiang,Yanzhe Li,Yi Zhang,Yilong Wang,Yingxue Li,Yongzhen Guo,Yuzhuo Fu,Yuanyuan Wang,Yue Yang,Yue Yu,Yufeng Deng,Yun Zhang,Yunfei Xu,Yuqi Zhang,Yuxiao He,Zengke Gui,Zhaoxin Huan,Zhaoyang Wang,Zhibo Zhu,Zhihao Wang,Zhiqiang Zhang,Zhoufei Wang,Zihang Zeng,Ziqi Liu,Zitao Xuan,Zuoli Tang*

Main category: cs.CL

TL;DR: Ling 2.0是一套高度稀疏、推理导向的大模型架构，在确保推理能力的同时显著提升了计算效率，并在万亿参数级别达成推理准确度与效率的最佳平衡，为后续高级推理智能模型提供了高效开放的基础方案。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型在推理能力和计算效率之间存在权衡，提升推理能力通常伴随计算资源消耗的快速增长。因此，开发一种能够在保证推理能力提升的同时，提高计算效率、可扩展性和一致性的基础模型结构具有重要意义。

Method: 提出Ling 2.0系列推理导向的语言基础模型，基于高度稀疏的Mixture-of-Experts（MoE）架构，规模从数十亿到一万亿参数，并结合高效的MTP技术、推理导向数据和中期训练的CoT激活、基于强化学习的微调方法（DFT, Evo-CoT）、全流程FP8精度训练及异构化流水线优化。

Result: Ling 2.0包括Ling-mini-2.0、Ling-flash-2.0和Ling-1T三款模型，在16B到1T参数区间，相较于同等规模的稠密模型，激活计算效率提升可达7倍。在一万亿参数规模下，Ling-1T在推理能力和计算效率之间达成新的帕累托最优，展现出稀疏激活与推理任务对齐下的可扩展高效智能。

Conclusion: Ling 2.0为未来推理与思维模型提供了统一、开源且高效的基础，为后续如Ring系列模型的发展打下了坚实基础。

Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [54] [OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue](https://arxiv.org/abs/2510.22143)
*Tianhong Gao,Jundong Shen,Bei Shi,Jiapeng Wang,Ying Ju,Junfeng Yao,Jiao Ran,Yong Zhang,Lin Dong,Huiyu Yu,Tingting Ye*

Main category: cs.CL

TL;DR: OlaMind框架通过模仿人类专家思维与应答方式，并利用强化学习优化，显著提升了智能客服的自然对话能力和安全性，在实际社群及直播客服应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于RAG的智能客服系统虽然在自动化和效率方面取得了显著进展，但仍存在幻觉问题（生成虚假或错误内容）和回复呆板的问题，影响用户体验并带来业务风险。提升人性化和安全性成为亟需解决的难题。

Method: 提出OlaMind框架，通过“Learn-to-Think”阶段学习专家的推理与应答策略，再结合“Learn-to-Respond”阶段冷启动监督微调（SFT）及强化学习（RL），实现从基础到困难场景的自我优化，提升客服系统的类人对话和安全性。

Result: OlaMind经过大规模线上A/B测试，在社群支持和直播互动场景分别实现智能解决率提升28.92%/18.42%，人工介入率降低6.08%/7.12%，显示出显著且一致的效果提升。

Conclusion: OlaMind有效提升了基于RAG的智能客服系统的人性化和自然度，显著降低了幻觉和业务风险，在多种真实应用场景中展现出卓越性能，并将代码及数据公开。

Abstract: Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.

</details>


### [55] [SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language](https://arxiv.org/abs/2510.22160)
*Rahul Ranjan,Mahendra Kumar Gurve,Anuj,Nitin,Yamuna Prasad*

Main category: cs.CL

TL;DR: 本文针对低资源语言Maithili，发布了首个带有原生语言理由注释的情感分析标注数据集，经专家验证，适用于情感极性分析和模型可解释性研究，对多语种NLP和可解释AI具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如Maithili在自然语言处理领域的研究仍然不足，主要原因是缺乏本地语言专家以及数据标注所需的高昂成本与时间。尤其是在情感分析方面，该类语言的数据与可解释性的研究资源严重匮乏。

Method: 作者构建并发布了一个包含3,221个Maithili句子的情感极性标注数据集，且每条数据都配有以Maithili语言撰写的自然语言理由说明。数据集由语言专家精心策划与验证，保证了标注的准确性与情境的忠实性。此外，作者通过传统机器学习方法和先进的Transformer架构对数据集进行实验评估。

Result: 实验显示，该数据集在可解释情感分析任务上有效，支持模型更好地进行情感极性判断，并提高了结果的可解释性。

Conclusion: 该研究首次为Maithili语言的可解释情感计算建立了基准数据集，为多语种NLP和可解释人工智能提供了宝贵资源，推动了低资源语言相关研究的发展。

Abstract: Developing benchmark datasets for low-resource languages poses significant
challenges, primarily due to the limited availability of native linguistic
experts and the substantial time and cost involved in annotation. Given these
challenges, Maithili is still underrepresented in natural language processing
research. It is an Indo-Aryan language spoken by more than 13 million people in
the Purvanchal region of India, valued for its rich linguistic structure and
cultural significance. While sentiment analysis has achieved remarkable
progress in high-resource languages, resources for low-resource languages, such
as Maithili, remain scarce, often restricted to coarse-grained annotations and
lacking interpretability mechanisms. To address this limitation, we introduce a
novel dataset comprising 3,221 Maithili sentences annotated for sentiment
polarity and accompanied by natural language justifications. Moreover, the
dataset is carefully curated and validated by linguistic experts to ensure both
label reliability and contextual fidelity. Notably, the justifications are
written in Maithili, thereby promoting culturally grounded interpretation and
enhancing the explainability of sentiment models. Furthermore, extensive
experiments using both classical machine learning and state-of-the-art
transformer architectures demonstrate the dataset's effectiveness for
interpretable sentiment analysis. Ultimately, this work establishes the first
benchmark for explainable affective computing in Maithili, thus contributing a
valuable resource to the broader advancement of multilingual NLP and
explainable AI.

</details>


### [56] [DETECT: Determining Ease and Textual Clarity of German Text Simplifications](https://arxiv.org/abs/2510.22212)
*Maria Korobeynikova,Alessia Battisti,Lukas Fischer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 本文提出了首个针对德语自动文本简化的质量评估指标DETECT，通过大语言模型生成数据和优化评分准则，无需人工标注即可高效评估。新指标与人工判断的相关性显著高于传统方法，尤其在意义和流畅性方面，推动了德语ATS领域的自动评估发展。


<details>
  <summary>Details</summary>
Motivation: 目前德语自动文本简化（ATS）的评估主要依赖于通用指标（如SARI、BLEU和BERTScore），这些指标在简化质量（包括简单性、意思保留和流畅性）的衡量方面存在不足。相比英语的专用指标（如LENS），德语领域因缺乏人工标注语料而进展缓慢。

Method: 提出了一种德语专用自动文本简化质量评估指标DETECT。该方法基于LENS的框架，并有两项主要扩展：（1）建立了一个基于大语言模型的合成打分管道，无需人工标注即可生成训练数据；（2）通过大语言模型对评分准则进行细化，使其更贴合简化需求。

Result: DETECT在与人工评价的相关性上远超已有主流ATS指标，尤其在意义保留和流畅性方面优势显著。研究还构建了目前最大规模的德语人工评价简化数据集来验证新指标。

Conclusion: DETECT填补了德语自动文本简化质量评价领域专用指标的空白，推进了德语简化自动化评估的发展，并展示了大语言模型用于自动评价的潜力及其局限性，也为更广泛的语言可及性任务提供了可迁移的指导。

Abstract: Current evaluation of German automatic text simplification (ATS) relies on
general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently
capture simplification quality in terms of simplicity, meaning preservation,
and fluency. While specialized metrics like LENS have been developed for
English, corresponding efforts for German have lagged behind due to the absence
of human-annotated corpora. To close this gap, we introduce DETECT, the first
German-specific metric that holistically evaluates ATS quality across all three
dimensions of simplicity, meaning preservation, and fluency, and is trained
entirely on synthetic large language model (LLM) responses. Our approach adapts
the LENS framework to German and extends it with (i) a pipeline for generating
synthetic quality scores via LLMs, enabling dataset creation without human
annotation, and (ii) an LLM-based refinement step for aligning grading criteria
with simplification requirements. To the best of our knowledge, we also
construct the largest German human evaluation dataset for text simplification
to validate our metric directly. Experimental results show that DETECT achieves
substantially higher correlations with human judgments than widely used ATS
metrics, with particularly strong gains in meaning preservation and fluency.
Beyond ATS, our findings highlight both the potential and the limitations of
LLMs for automatic evaluation and provide transferable guidelines for general
language accessibility tasks.

</details>


### [57] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

Main category: cs.CL

TL;DR: 本文提出了无需依赖真实标签的LLM文本对比误差评估方法，在六个主流模型上表现稳定，Claude表现最好，方法优于现有评价指标。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，LLM在进行文本对比判断时，可能会受到顺序因素或随机性影响而产生偏差和误差。现有方法依赖真实标签或难以分辨模型本身误差，本研究希望提出一种更通用、可量化且不依赖标签的误差评估方法。

Method: 提出了一种无需依赖人工真实标签的新方法，通过多次对比同一文本对，分别记为文本顺序不同的情况下，计算LLM表现出的误差概率，并结合Copeland计数进行排名，评估和估算模型误差率。该方法适用于整体均匀误差和位置性偏差两种场景。

Result: 在六个主流LLM与五类文本输入上检验该方法，得出一致且可靠的误差估计；位置性偏差项普遍较小，误差率与均匀误差接近。Claude在误差率和鲁棒性上表现最佳。此外，该方法在误差检测上优于已有的Bradley-Terry模型和交换性分数。

Conclusion: 通过本文提出的方法可以更加准确和一致地评估主流LLM在文本对比任务中的输出偏差和误差，Claude在本次实验中的表现最优。

Abstract: We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [58] [Evolution of the lexicon: a probabilistic point of view](https://arxiv.org/abs/2510.22220)
*Maurizio Serva*

Main category: cs.CL

TL;DR: 传统Swadesh词汇统计法精度有限，新方法将词形渐变考虑在内，能更精准估算语言分离时间。


<details>
  <summary>Details</summary>
Motivation: Swadesh方法用于判断两种语言的分离时间，但其基本假设常常因词汇交叉、变迁速率、同源关系误判、同义词等因素不切实际，导致结果不准确，因此需要探究方法的限制与提升。

Method: 文章首先分析了传统Swadesh方法的统计与概率局限性，接着提出语言词汇演变还包括渐进性词形变动这一随机过程，并以概率统计方法量化两者影响，从而提升语言分离时间的估算精度。

Result: 发现即使传统假设全部成立，数学原理仍限制分离时间的判断精度；引入词汇渐进变动的随机过程后，在概率分析下能显著提高判断语言分离时间的准确性。

Conclusion: Swadesh方法在理论和实际中均受概率性限制；只有结合词汇渐进变动这一过程，才能更精准地推断两种语言的分离时间。

Abstract: The Swadesh approach for determining the temporal separation between two
languages relies on the stochastic process of words replacement (when a
complete new word emerges to represent a given concept). It is well known that
the basic assumptions of the Swadesh approach are often unrealistic due to
various contamination phenomena and misjudgments (horizontal transfers,
variations over time and space of the replacement rate, incorrect assessments
of cognacy relationships, presence of synonyms, and so on). All of this means
that the results cannot be completely correct.
  More importantly, even in the unrealistic case that all basic assumptions are
satisfied, simple mathematics places limits on the accuracy of estimating the
temporal separation between two languages. These limits, which are purely
probabilistic in nature and which are often neglected in lexicostatistical
studies, are analyzed in detail in this article.
  Furthermore, in this work we highlight that the evolution of a language's
lexicon is also driven by another stochastic process: gradual lexical
modification of words. We show that this process equally also represents a
major contribution to the reshaping of the vocabulary of languages over the
centuries and we also show, from a purely probabilistic perspective, that
taking into account this second random process significantly increases the
precision in determining the temporal separation between two languages.

</details>


### [59] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

Main category: cs.CL

TL;DR: 本文发现，高约束的提示工程方法（Sculpting）能提升中等模型的推理准确率，但在更强大的模型上反而会限制其表现，提示设计需根据模型水平动态调整。


<details>
  <summary>Details</summary>
Motivation: 随着大模型推理能力的提升，如何通过有效提示工程（Prompt Engineering）进一步减少推理中的语义歧义和常识错误成为研究重点。

Method: 提出了一种受约束、基于规则的新提示方法——Sculpting，并与Zero Shot和标准CoT在三个不同代际的OpenAI模型（gpt-4o-mini、gpt-4o、gpt-5）上进行比较测试，主要使用GSM8K数学推理基准进行体系化评估。

Result: Sculpting方法在gpt-4o模型上效果优越（准确率97%，标准CoT为93%），但在更先进的gpt-5模型上却适得其反（Sculpting为94%，CoT为96.36%）。出现了“提示反转”现象，源于约束策略在高阶模型上导致模型过于字面化。“防护栏到手铐”效应使原本用以纠错的约束变成了性能限制。

Conclusion: 提示策略的最优选择需与模型能力共同演化，对高级模型应采用更简洁的提示方式。

Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [60] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao,Ming Yan,Yilun Qiu,Haoting Ni,Yang Zhang,Fuli Feng,Hong Cheng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 该论文提出了基于因果推断的SteerX方法，通过准确定位用户偏好相关信号，提升LLM的个性化能力，并在实验中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在智能助手等应用中表现出色，但个性化成为关键挑战，现有的激活操控方法无法有效识别和利用真正代表用户偏好的数据。

Method: 提出SteerX方法，通过因果推断理论，区分用户偏好相关与无关的激活成分，提取并转化偏好驱动的token信号，用于更精确的激活操控，实现个性化LLM生成。

Result: 在两个主流操控方法和真实世界数据集上测试，SteerX显著提升激活操控向量的准确性和个性化效果。

Conclusion: SteerX能够更好地识别用户偏好，提高LLM激活操控的个性化能力，为个性化模型输出提供了实用的解决方案。

Abstract: Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [61] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu,Boyin Tan,Xiaoyuan Liu,Chao Peng,Pengfei Gao,Pinjia He*

Main category: cs.CL

TL;DR: 本文提出了一种新奖励模型R4P，通过推理代替传统测试，用于软件工程智能体训练，提升补丁验证准确率与效率，助力大模型快速扩展数据和能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于测试的监督方法在大语言模型（LLM）Agent的软件工程任务中难以扩展，主要由于测试环境构建和维护成本高、脆弱，且高覆盖率的测试数据稀缺，易被边界用例‘测试黑客’行为影响。

Method: 提出R4P，一种通过推理对补丁验证、专为SWE代理训练和测试提供可扩展奖励的模型。R4P采用群体目标用于RL训练，仅基于补丁内容互相验证，无需新写或运行测试，提升训练数据密度和奖励稳定性。并设计轻量骨架Mini-SE，纯用R4P奖励进行强化学习。

Result: R4P在SWE-bench-verified任务上的补丁验证准确率72.2%，超越OpenAI o3。Mini-SE在同一基准上Pass@1为26.2%，较Qwen3-32B提升10%。若R4P参与测试时奖励扩展，Pass@1可进一步提升至32.8%。R4P单次验证速度较传统测试快50倍（于1秒内），奖励和准确率扩展曲线稳定。

Conclusion: R4P为SWE Agent训练提供高效、可扩展的奖励机制，极大提升了补丁验证效率和效果，摆脱依赖测试沙箱局限，推动代码智能体大模型进行数据与能力的高效扩展。

Abstract: While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [62] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou,Denis Cavallucci*

Main category: cs.CL

TL;DR: 论文推出专利文本嵌入评测基准PatenTEB和多任务训练的patembed模型，在多项专利相关任务上刷新SOTA，显著提升专利检索与分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入基准无法充分捕捉专利领域的独特挑战，亟需专门面向专利分析与检索的全面评测与高效模型。

Method: 提出PatenTEB基准，涵盖15项任务与200余万样本，引入领域分层、定向负样本挖掘及碎片到文档的匹配；开发patembed系列模型，参数从67M到344M，支持多任务训练；进行外部验证与消融实验分析。

Result: patembed-base在BigPatentClustering.v2上以0.494 V-measure达新SOTA，patembed-large在DAPFAM获得0.377 NDCG@100，多任务训练提升泛化性能，域预训练带来持续优势。

Conclusion: patembed模型经过多任务训练表现出优异的泛化能力，在专利嵌入相关任务表现领先；域预训练初始化也带来一致优势。

Abstract: Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [63] [From Slides to Chatbots: Enhancing Large Language Models with University Course Materials](https://arxiv.org/abs/2510.22272)
*Tu Anh Dinh,Philipp Nicolas Schumacher,Jan Niehues*

Main category: cs.CL

TL;DR: 将大学课程材料用于扩展 LLM，采用检索增强生成（RAG）效果优于持续预训练，特别是以图片处理幻灯片能大幅提升模型性能，为 AI 助教开发提供了实用方法。


<details>
  <summary>Details</summary>
Motivation: 当前大学计算机课程中，LLMs 在回答专业问题时表现有限，受限于课程材料的多样性（如幻灯片和讲稿）以及其与常规语料的显著差异。因此，研究如何有效利用这些课程材料来提升 LLM 能力具有重要意义。

Method: 比较了两种扩展 LLM 课程知识的方法：检索增强生成（RAG）和持续预训练（CPT），并对讲义幻灯片内容进一步提出多模态 RAG（以图像形式向模型呈现幻灯片）。

Result: 实验表明：在课程材料体量较小时，RAG 比 CPT 更高效、效果更优；在多模态场景中，以图片形式应用幻灯片比单纯文本检索显著提升了 LLM 表现。

Conclusion: 结合课程特定材料，尤其以多模态方式处理幻灯片，可大幅提升 LLM 在教育场景下的辅助能力，相关策略可推广到其他教学领域。

Abstract: Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.

</details>


### [64] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moschkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: MATCH提出了一种无需参考代码、利用对比学习方式对代码生成效果进行评价的新指标，在功能和人类偏好相关性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的代码生成评价方法（如单元测试、BLEU、ROUGE、CodeBERTScore）存在扩展性差、功能性不强、对参考代码依赖等问题，难以无参考高效评价生成代码的准确性。

Method: 利用对比学习生成代码和任务描述的语义嵌入，通过嵌入的相似度评价生成代码与开发者意图的一致性。

Result: MATCH在多个编程语言上，评价结果与功能正确性和人工偏好相关性优于现有评价指标。

Conclusion: 提出MATCH作为参考无关、基于对比学习的新评价指标，比现有方法更能反映代码与任务描述的契合度。

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [65] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

Main category: cs.CL

TL;DR: 本文系统对比了BERT系列、GPT-4o基于上下文学习和有监督微调在临床NER任务上的表现，发现SFT-GPT-4o效果最优（F1≈87.1%），但成本较高；BERT系列提升有限，任务简化有助于大模型提高准确率。


<details>
  <summary>Details</summary>
Motivation: 对临床文本中的命名实体识别（NER）任务提升效果，并比较不同主流模型在CADEC数据集上的表现。

Method: 对比分析三类模型：（1）BERT系列编码器（BERT Base、BioClinicalBERT、RoBERTa-large）；（2）GPT-4o的少样本上下文学习，分别用简单和复杂提示词；（3）GPT-4o通过有监督微调（SFT）。所有模型基于CADEC的五类实体，采用标准NER指标评估。

Result: RoBERTa-large和BioClinicalBERT对BERT Base改进有限，表明该类型模型在任务上的提升有限。LLM（大模型）中，简单提示下的ICL优于复杂提示，SFT方式整体表现最佳（F1≈87.1%），但成本较高。当任务简化为二分类时，LLM表现进一步提升。

Conclusion: 在CADEC数据集和临床NER任务中，现有BERT系列增益有限，有监督微调后的LLM模型效果最强，但代价较高。LLM在任务简化后准确率更高。

Abstract: We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [66] [Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)
*Antal van den Bosch,Ainhoa Risco Patón,Teun Buijse,Peter Berck,Maarten van Gompel*

Main category: cs.CL

TL;DR: 提出了基于记忆的语言建模方法，通过快速k近邻实现次词预测，相比GPT-2等主流神经网络模型，性能可比且更环保，完全无需GPU，运行高效且结构透明。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络语言模型在性能优异的同时带来了高能耗和环境负担。本文旨在提出一种更加高效、环保且效果不俗的新型语言模型技术。

Method: 采用快速k-近邻分类近似，完全依赖CPU，实现高效的下一个词预测（next-token prediction），并以小规模生态足迹完成训练和推理。论文中使用OLIFANT进行实验，并与GPT-2、GPT-Neo对比预测准确率、碳排放和运行速度。

Result: OLIFANT（记忆型语言模型）的下一个词预测性能与主流模型存在可比性，且在碳排放和运行速度等方面显著优于GPT-2和GPT-Neo。模型结构简单透明，易于分析。

Conclusion: 记忆型语言建模（memory-based language modeling）是深度神经网络语言建模的一种高效且环保的替代方案。该方法实现了良好的性能与透明度，并在生态影响方面优于主流神经网络模型。

Abstract: We present memory-based language modeling as an efficient, eco-friendly
alternative to deep neural network-based language modeling. It offers
log-linearly scalable next-token prediction performance and strong memorization
capabilities. Implementing fast approximations of k-nearest neighbor
classification, memory-based language modeling leaves a relatively small
ecological footprint both in training and in inference mode, as it relies fully
on CPUs and attains low token latencies. Its internal workings are simple and
fully transparent. We compare our implementation of memory-based language
modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,
estimated emissions and speeds, and offer some deeper analyses of the model.

</details>


### [67] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines,Bonnie Dorr*

Main category: cs.CL

TL;DR: 首次引入多语言目标立场提取（TSE）基准，覆盖六种语言，无需单独训练多语言模型。实验发现任务难度高，F1仅12.78，目标预测是主要难点，为多语言TSE提供了关键基线和评估标准。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体分析主要集中在英文数据，针对目标立场提取（TSE）任务尚未有多语言研究。作者希望推动多语言环境下的TSE研究，丰富资源与方法。

Method: 提出了第一个多语言TSE基准，包括加泰罗尼亚语、爱沙尼亚语、法语、意大利语、中文和西班牙语语料库，并扩展了原有TSE流程使其适配多语言，无需为每种语言训练独立模型。

Result: 在多语言环境下模型F1分数仅为12.78，表明该任务难度远高于英文环境。同时发现目标预测环节是主要瓶颈，且TSE的F1分数对目标表达方式高度敏感。

Conclusion: 该研究为多语言TSE任务建立了首个基准和实验流程，提出了标准化资源、算法与评估基线，并揭示了任务难点，为未来研究提供了方向。

Abstract: Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [68] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl,Majid Asgari-Bidhendi,Behrooz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 提出FAIR-RAG新框架，结合结构化证据审核和自适应补全，显著提高复杂多跳检索增强生成效果，在主流数据集上刷新最优成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG（检索增强生成）方法，尽管能够减少大模型的幻觉和知识陈旧问题，但在处理复杂的多跳问题时，难以系统性地识别和补全信息缺口，导致无法全面整合上下文或引入噪声。

Method: 提出FAIR-RAG框架，将传统RAG流程转化为动态、循证的推理过程，核心是通过'结构化证据评估（SEA）'模块按checklist方式解构问题，查找已确认和缺失的证据，通过自适应查询细化代理补充缺失信息，循环往复，直至证据充分。

Result: FAIR-RAG在多跳问答基准（HotpotQA, 2WikiMultiHopQA, MusiQue）上显著超越现有最强基线，HotpotQA上F1提升8.3，达0.453，创同类方法新SOTA。

Conclusion: 结构化的、循证的、多轮细化推理和缺口补全机制是复杂知识任务中提升RAG系统可靠性和准确性的关键。

Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [69] [Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models](https://arxiv.org/abs/2510.22356)
*Fiaz Ahmad,Nisar Hussain,Amna Qasim,Momina Hafeez,Muhammad Usman Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 通过英文语料移植并应用前沿深度学习模型，显著提升了乌尔都语中反讽识别的准确率（最高F1为94.61%），为低资源语言反讽检测提供了新的技术路线。


<details>
  <summary>Details</summary>
Motivation: 反讽识别在自然语言处理中具有挑战性，尤其是在语法和文化背景不同的语言之间。乌尔都语作为历史上资源较少的语言，现有反讽识别研究较少，亟需提升相关技术。

Method: 将英文反讽语料库翻译成乌尔都语，采用GloVe和Word2Vec词嵌入，测试十种主流机器学习算法，并与传统方法比较。同时微调BERT、RoBERTa、LLaMA 2 (7B)、LLaMA 3 (8B)、Mistral等先进的Transformer模型，评估大模型在反讽识别中的效果。

Result: 传统机器学习模型中，Gradient Boosting获得F1分数89.18%；Transformer模型中，LLaMA 3 (8B)表现最佳，F1分数达94.61%。

Conclusion: 结合现代NLP模型和翻译技术，能够实现对乌尔都语这一低资源语言的有效反讽识别。大规模预训练模型在提升识别准确率方面尤其突出。

Abstract: Ironic identification is a challenging task in Natural Language Processing,
particularly when dealing with languages that differ in syntax and cultural
context. In this work, we aim to detect irony in Urdu by translating an English
Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine
learning algorithms using GloVe and Word2Vec embeddings, and compare their
performance with classical methods. Additionally, we fine-tune advanced
transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),
and Mistral, to assess the effectiveness of large-scale models in irony
detection. Among machine learning models, Gradient Boosting achieved the best
performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3
(8B) achieved the highest performance with an F1-score of 94.61%. These results
demonstrate that combining transliteration techniques with modern NLP models
enables robust irony detection in Urdu, a historically low-resource language.

</details>


### [70] [GigaEmbeddings: Efficient Russian Language Embedding Model](https://arxiv.org/abs/2510.22369)
*Egor Kolodin,Daria Khomich,Nikita Savushkin,Anastasia Ianina,Fyodor Minkin*

Main category: cs.CL

TL;DR: GigaEmbeddings针对俄语提出全新文本嵌入框架，通过高级预训练和结构创新，实测效果行业领先而高效，展示出强大的多任务泛化和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有俄语文本嵌入方法存在局限性，如目标分散、效果不佳、效率低下，难以在检索、分类和聚类等任务间统一优化。作者希望通过新方法显著提升俄语嵌入的性能和多任务泛化能力。

Method: 提出GigaEmbeddings框架，采用专为俄语设计的解码器-only LLM（GigaChat-3B）。三阶段管线包含：1）在大规模网页语料上的对比式预训练；2）集成难负例进行微调；3）跨检索、分类、聚类任务的多任务泛化细化。同时引入合成数据生成，采用双向注意力进行上下文建模，潜在注意力池化完成序列聚合，并通过剪除25%变换器层提升效率。

Result: 在ruMTEB基准（包含23个多语言任务）上，GigaEmbeddings平均得分69.1，取得领域最佳表现，显著优于参数量更大的强基线模型。

Conclusion: GigaEmbeddings通过多阶段管线和结构创新，实现俄语聚焦文本嵌入性能的大幅提升，在多种任务上都优于现有方法，并大幅提升效率。

Abstract: We introduce GigaEmbeddings, a novel framework for training high-performance
Russian-focused text embeddings through hierarchical instruction tuning of the
decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our
three-stage pipeline, comprising large-scale contrastive pre-training in
web-scale corpora, fine-tuning with hard negatives, and multitask
generalization across retrieval, classification, and clustering tasks,
addresses key limitations of existing methods by unifying diverse objectives
and leveraging synthetic data generation. Architectural innovations include
bidirectional attention for contextual modeling, latent attention pooling for
robust sequence aggregation, and strategic pruning of 25% of transformer layers
to enhance efficiency without compromising performance. Evaluated on the ruMTEB
benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves
state-of-the-art results (69.1 avg. score), outperforming strong baselines with
a larger number of parameters.

</details>


### [71] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie,Zhiyang Zhang,Yifan Wu,Sirong Lu,Jiayi Zhang,Zhaoyang Yu,Jinlin Wang,Sirui Hong,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出首个可视化图表美学和质量评估基准VisJudge-Bench，并开发了专用模型VisJudge，显著提升了多模态模型在可视化自动评估任务上的准确性和专家一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在自然图片审美评价上表现良好，但针对可视化图表质量和美学的评估能力尚无系统性基准，且可视化评估依赖于对数据编码准确性、信息表达和美学多维度的判断，现有方法和标准有限。

Method: 提出了VisJudge-Bench，这是首个系统性评测多模态大模型在可视化图表美学和质量评估上性能的基准，涵盖3090个专家标注的样本，涵盖单图、多图和仪表盘、共32种图类型，并开发了专门针对可视化质量和美学评估设计的模型VisJudge。

Result: 实验显示，即使是最先进的多模态大模型（如GPT-5）在专业可视化评估上与人类专家仍存在显著差距（MAE=0.551，相关系数0.429），而VisJudge模型有效缩小该差距，MAE下降到0.442（减少19.8%），与专家一致性提升到0.681（提升58.7%）。

Conclusion: VisJudge-Bench为可视化评估任务提供了首个系统性基准，证明现有多模态大模型在该任务上性能有限，而专用模型VisJudge显著提升了自动评估的准确性和与专家的一致性。

Abstract: Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [72] [Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection](https://arxiv.org/abs/2510.22395)
*Federica Gamba,Aman Sinha,Timothee Mickus,Raul Vazquez,Patanjali Bhamidipati,Claudio Savelli,Ahana Chattopadhyay,Laura A. Zanella,Yash Kankanampati,Binesh Arakkal Remesh,Aryan Ashok Chandramania,Rohit Agarwal,Chuyuan Li,Ioana Buhnila,Radhika Mamidi*

Main category: cs.CL

TL;DR: 作者发布了一个面向科学文本的多语种LLM幻觉数据集CAP，收录了900道科学问题和7000余条模型答案，标注了幻觉与流畅性标签，有助于相关研究和多语种NLP系统可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 科学类文本生成中的幻觉问题会严重歪曲事实，本领域术语多、推理复杂、LLM易泛化失真且多语种方向研究受限，亟需真实大数据集推动相关研究。

Method: 作者构建了CAP多语言数据集，涵盖9种语言，通过向16个公开LLM输入900个科学问题，收集7000余条答案，并针对幻觉和流畅性做人工标注。

Result: CAP数据集可公开获取，涵盖高、低资源语言，支持幻觉检测、多语种LLM评估及科学NLP系统优化等领域的高级研究。

Conclusion: CAP数据集为科学文本领域的LLM幻觉研究提供了真实多语料资源，对提升LLM在科学领域的可靠性和多语种评估能力具有重要意义。

Abstract: We introduce the CAP (Confabulations from ACL Publications) dataset, a
multilingual resource for studying hallucinations in large language models
(LLMs) within scientific text generation. CAP focuses on the scientific domain,
where hallucinations can distort factual knowledge, as they frequently do. In
this domain, however, the presence of specialized terminology, statistical
reasoning, and context-dependent interpretations further exacerbates these
distortions, particularly given LLMs' lack of true comprehension, limited
contextual understanding, and bias toward surface-level generalization. CAP
operates in a cross-lingual setting covering five high-resource languages
(English, French, Hindi, Italian, and Spanish) and four low-resource languages
(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated
scientific questions and over 7000 LLM-generated answers from 16 publicly
available models, provided as question-answer pairs along with token sequences
and corresponding logits. Each instance is annotated with a binary label
indicating the presence of a scientific hallucination, denoted as a factuality
error, and a fluency label, capturing issues in the linguistic quality or
naturalness of the text. CAP is publicly released to facilitate advanced
research on hallucination detection, multilingual evaluation of LLMs, and the
development of more reliable scientific NLP systems.

</details>


### [73] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong,Cong Wang,Maria Teleki,Millennium Bismay,James Caverlee*

Main category: cs.CL

TL;DR: 角色差异使大模型推理结果多变。CHOIR方法整合多角色推理信号，不需训练即可顽强提升准确率，带来更可靠的推理表现。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）在赋予不同角色身份（persona）时，会因人口统计变量（如代词变化）导致推理结果显著不同。传统方法将这类变异视为偏差，致力于消除，而非利用。本文动机是将这种“角色多样性引发的推理差异”转化为增强模型鲁棒性的积极资源。

Method: 作者提出了CHOIR（Collaborative Harmonization fOr Inference Robustness）测试时框架。CHOIR将多个基于不同角色身份的推理结果进行协同整合，通过动态把控各角色间的推理一致性与分歧，最终获得更鲁棒的统一预测。该方法无需重新训练模型，是一种测试时的推理增强策略。

Result: CHOIR在多种推理基准测试中有效提升了性能，不同人口统计群体、模型架构、模型规模与任务下均表现出一致优势。在某些群体上提升高达26.4%，五个人口统计总体平均提升19.2%。即使初始角色身份效果不佳，CHOIR依然有效。

Conclusion: 通过将角色身份差异性视为有益信号而非负面偏差，CHOIR为大语言模型推理的可靠性提升提供了可扩展、通用的新途径。

Abstract: Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [74] [The Tonogenesis Continuum in Tibetan: A Computational Investigation](https://arxiv.org/abs/2510.22485)
*Siyu Liang,Zhaxi Zerong*

Main category: cs.CL

TL;DR: 本文借助ASR模型，量化分析藏语方言在声调产生过程中的音高功能，揭示了音高对识别性能的渐变影响，强调计算方法在细致捕捉历史语音变化中的优势，对传统基于最小对立对的功能负载评估提出了质疑。


<details>
  <summary>Details</summary>
Motivation: 声调产生过程（Tonogenesis）长期以来通过比较重建和声学语音学进行研究。本研究旨在引入计算方法，更精确量化声调历史演变中音高的功能角色。

Method: 使用自动语音识别（ASR）系统，通过操控音高来分析不同阶段的音高变化对识别性能的影响。具体通过对藏语不同方言的音高扁平化敏感性进行对比分析。

Result: 发现存在声调产生的连续体：无声调的安多方言对音高移除容忍度最高，完全声调化的卫藏方言性能下降最严重，中间的康方言则处于两者之间。

Conclusion: ASR模型能够隐式学习在语言由辅音对立向声调对立转换过程中音高功能负载的变化。研究表明，计算方法能捕捉到细微的语音变化阶段，并指出传统功能负载指标在过渡系统中可能高估了对音高的依赖。

Abstract: Tonogenesis-the historical process by which segmental contrasts evolve into
lexical tone-has traditionally been studied through comparative reconstruction
and acoustic phonetics. We introduce a computational approach that quantifies
the functional role of pitch at different stages of this sound change by
measuring how pitch manipulation affects automatic speech recognition (ASR)
performance. Through analysis on the sensitivity to pitch-flattening from a set
of closely related Tibetan languages, we find evidence of a tonogenesis
continuum: atonal Amdo dialects tolerate pitch removal the most, while fully
tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects
fall measurably between these extremes. These gradient effects demonstrate how
ASR models implicitly learn the shifting functional load of pitch as languages
transition from consonant-based to tone-based lexical contrasts. Our findings
show that computational methods can capture fine-grained stages of sound change
and suggest that traditional functional load metrics, based solely on minimal
pairs, may overestimate pitch dependence in transitional systems where
segmental and suprasegmental cues remain phonetically intertwined.

</details>


### [75] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian,Junjie Liu,Xican Yang,Haishan Ye,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种将通用和任务特定校准数据结合的剪枝方法，不仅缩小了LLM参数规模，同时更好地保留了模型的专用任务能力，在多个基准测试下均优于传统剪枝技术。


<details>
  <summary>Details</summary>
Motivation: 目前大模型剪枝方法主要关注于保证模型生成流畅文本的能力，却忽略了模型在特定任务或领域下的表现。随着实际应用需求增长，人们希望能够进一步压缩模型参数空间，同时仍然保留其任务专长能力。

Method: 本文提出了一种任务能力保持的剪枝方法。与传统仅用通用校准数据计算参数重要性不同，本方法融合了通用和任务特定校准数据，分别计算参数重要度，并根据激活范数划分为共享与特有参数组，之后融合重要性分数以指导剪枝。该方法能够无缝应用到多种基础剪枝技术中。

Result: 在广泛使用的基准测试上，该方法无论在相同剪枝率还是不同设置下，都显著优于主流剪枝方法，能有效兼顾压缩与任务能力保留。

Conclusion: 提出了一种能够融合领域任务校准信息，高效保留大模型专长能力的剪枝方法，其性能明显优于现有方案。

Abstract: Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [76] [The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR](https://arxiv.org/abs/2510.22492)
*Siyu Liang,Nicolas Ballier,Gina-Anne Levow,Richard Wright*

Main category: cs.CL

TL;DR: 本文系统分析Whisper多语种语音识别模型子词的利用规律，发现子词激活与预训练数据规模关联较弱，主要受语言内部结构驱动，为改进多语言模型评估和训数据配置提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 探究在多语种语音识别模型（以Whisper为例）中，多少语音数据能够充分展现模型所学的跨语言子词单元，并分析多语种预训练阶段的数据差异是否影响推理阶段对子词的利用。

Method: 分析Whisper模型在49种语言的推理过程中，记录解码候选子词并跟踪其累计发现过程，统计和建模子词激活和利用规律。进一步分析子词的发现速率、排名-频率分布、平均长度等指标，并将拉丁文字与其他文字体系进行比较。

Result: 研究发现，不同语言由于预训练语音小时数差异，并不会显著影响模型假设空间中的词汇多样度。子词发现速率在各语言间表现一致的指数饱和模式，并定义了声学饱和时间（AST）。排名-频率分布更符合Zipf-Mandelbrot规律。平均子词长度与预训练资源呈正相关。拉丁文字语言的相关指标优于如西里尔、CJK、闪米特等文字体系。

Conclusion: 多语种ASR推理过程中，子词的利用主要受到语言的统计、类型学、书写系统结构影响，而非训练数据规模限制，为更公平的语料库构建和跨语言评估提供了实证依据。

Abstract: How much audio is needed to fully observe a multilingual ASR model's learned
sub-token inventory across languages, and does data disparity in multilingual
pre-training affect how these tokens are utilized during inference? We address
this question by analyzing Whisper's decoding behavior during inference across
49 languages. By logging decoding candidate sub-tokens and tracking their
cumulative discovery over time, we study the utilization pattern of the model's
sub-token space. Results show that the total number of discovered tokens
remains largely independent of a language's pre-training hours, indicating that
data disparity does not strongly influence lexical diversity in the model's
hypothesis space. Sub-token discovery rates follow a consistent exponential
saturation pattern across languages, suggesting a stable time window after
which additional audio yields minimal new sub-token activation. We refer to
this convergence threshold as acoustic saturation time (AST). Further analyses
of rank-frequency distributions reveal Zipf-like patterns better modeled by a
Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation
with resource level. Additionally, those metrics show more favorable patterns
for languages in the Latin script than those in scripts such as Cyrillic, CJK,
and Semitic. Together, our study suggests that sub-token utilization during
multilingual ASR inference is constrained more by the statistical, typological,
and orthographic structure of the speech than by training data scale, providing
an empirical basis for more equitable corpus construction and cross-lingual
evaluation.

</details>


### [77] [A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus](https://arxiv.org/abs/2510.22495)
*Michael Scott,Siyu Liang,Alicia Wassink,Gina-Anne Levow*

Main category: cs.CL

TL;DR: 主流ASR系统在不同族裔英语说话者间存在显著识别偏差，主要原因是对方言语音特征建模不足。提出新的音位错误率指标，系统分析了十一项社会语音学特征，发现元音质量变化是导致偏差的关键。研究对提升语音识别系统的公平性和效果具有重要指导意义。


<details>
  <summary>Details</summary>
Motivation: 当前主流ASR系统在族裔、方言等多样化语音上的表现不一致，但归因机制尚不明确，需探明导致种族偏见的具体语音学因素以指导技术改进。

Method: 系统评估了四大商用ASR系统在不同族裔（非裔、白人、ChicanX和Yakama）上的转录准确性，并通过新提出的“音位错误率（PER）”指标，结合社会语音学注释分析了具体语言学变量对识别错误的影响。

Result: 发现元音质量的变化，尤其是针对低背元音合并和鼻音前元音合并的抗拒模式，与不同族裔识别误差系统性相关，对非裔美国人影响最大。PNWE语料库被证明是评估语音技术偏见的重要资源，并为优化ASR系统提供了具体改进建议。

Conclusion: 商用自动语音识别系统中的偏见主要来源于对方言语音变化的建模不足，而不仅仅是词汇或句法差异。

Abstract: This paper presents a systematic evaluation of racial bias in four major
commercial automatic speech recognition (ASR) systems using the Pacific
Northwest English (PNWE) corpus. We analyze transcription accuracy across
speakers from four ethnic backgrounds (African American, Caucasian American,
ChicanX, and Yakama) and examine how sociophonetic variation contributes to
differential system performance. We introduce a heuristically-determined
Phonetic Error Rate (PER) metric that links recognition errors to specific
linguistically motivated variables derived from sociophonetic annotation. Our
analysis of eleven sociophonetic features reveals that vowel quality variation,
particularly resistance to the low-back merger and pre-nasal merger patterns,
is systematically associated with differential error rates across ethnic
groups, with the most pronounced effects for African American speakers across
all evaluated systems. These findings demonstrate that acoustic modeling of
dialectal phonetic variation, rather than lexical or syntactic factors, remains
a primary source of bias in commercial ASR systems. The study establishes the
PNWE corpus as a valuable resource for bias evaluation in speech technologies
and provides actionable guidance for improving ASR performance through targeted
representation of sociophonetic diversity in training data.

</details>


### [78] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu,Sahithi Singireddy,Sravani Gona,Sujal Timilsina*

Main category: cs.CL

TL;DR: 全面微调效果最佳，LoRA类方法召回率高且内存低，零样本模型也被测试。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽能力强，但在法律等专业领域全面微调开销高，实际应用受到限制。研究旨在探索低成本、参数高效的自适应方案，特别针对服务条款中不公平条款的检测任务。

Method: 本文对BERT族模型进行微调，对多种LLM（如TinyLlama、LLaMA等）采用LoRA和QLoRA参数高效适应，并在零样本条件下评估GPT-4o系列。用CLAUDETTE-ToS和多语种语料库进行标准化实验与对比分析。

Result: 论文系统性评估了三种主流方法：全面微调、参数高效适应（如LoRA、QLoRA），以及零样本提示策略，用于检测服务条款（ToS）中的不公平条款。它在CLAUDETTE-ToS和多语言语料库上进行了实验比较，涉及多种模型（BERT、DistilBERT、TinyLlama、LLaMA 3B/7B等）。结果显示，全面微调在精确率与召回率之间表现平衡最好，而LoRA类模型在召回率上竞争力强，并且内存成本降低最多可达三倍。GPT-4o等零样本模型也在研究范围内进行了评测。

Conclusion: 在法律文本处理领域，全面微调依然是性能最优选择，但LoRA等参数高效方法在大幅降低资源消耗的同时能保持竞争力召回，对实际应用有重要参考价值。研究为法律NLP域提供了可复现的开放基线。

Abstract: Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [79] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He,Yuxuan Wang,Jiaqi Li,Kexin Liang,Muhan Zhang*

Main category: cs.CL

TL;DR: 论文提出LooGLE v2长文本实际场景的测评基准，涵盖法律、金融、代码等多领域、超长文本及多样化任务。实测发现现有大语言模型在长依赖任务表现严重不足，最长能理解的上下文远小于理论窗口，指出行业仍面临巨大技术挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然具备了更长的上下文窗口，但在处理涉及长依赖的任务时，理解能力依然受限，且实际应用场景长期缺少有效评测。现有多数长上下文分析方法与场景缺乏结合，模型能力与实际需求存在较大差距。

Method: 提出了LooGLE v2基准测试，自动收集了大量实际领域（法律、金融、游戏、代码）中16k至200万token的长文本，并设计了10种领域专属长依赖任务，生成1934个多样化、高复杂度的QA实例。通过可扩展的数据筛选流程实现大规模高质量评测。评估了6款本地部署与4款API型大语言模型。

Result: 测试结果显示，表现最佳的模型整体得分仅为59.2%。主流大语言模型实际能理解的上下文远短于其宣称的窗口长度，长依赖任务表现存在显著短板。

Conclusion: 尽管模型上下文窗口持续扩展，主流大语言模型在处理真实长依赖任务时能力有限，仍需大量优化与提升才能满足实际长文本场景需求。此领域的研究与模型改进空间仍然巨大。

Abstract: Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [80] [SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size](https://arxiv.org/abs/2510.22556)
*Jinhan Chen,Jianchun Liu,Hongli Xu,Xianjun Gao,Shilong Wang*

Main category: cs.CL

TL;DR: 本文提出SABlock算法，通过语义分割和自适应块管理优化KV缓存，兼顾语义完整性与内存/速度效率。在多个长上下文任务中展现领先性能，大幅降低内存占用和提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 由于长上下文大型语言模型（LLM）推理过程中KV缓存的内存占用不断增大，严重影响了系统扩展性。现有的KV缓存压缩与驱逐方法难以在语义一致性和内存效率间取得良好平衡，因此亟需更加智能化的缓存管理方案。

Method: 提出SABlock：一种语义感知、块大小自适应的KV缓存驱逐框架。具体做法包括：首先基于语义分割对压缩边界与语言结构进行对齐；然后通过分段引导的token评分，对token重要性进行精准估算；最后在每个语义分段内，依据缓存预算自适应搜索最优的块大小，在保障语义完整性的前提下最大化压缩效率。

Result: 在长上下文基准测试中，SABlock在相同内存预算下显著优于现有方法。例如，在Needle-in-a-Haystack任务中，仅用96个KV条目就可实现99.9%的检索准确率，与保留8K条目的全缓存相比性能几乎持平。在固定缓存预算（1024）下，峰值内存占用降低了46.28%，且在128K长上下文下解码速度提升至9.5倍。

Conclusion: SABlock能够根据语义结构智能调整缓存粒度，有效提升KV缓存管理的内存效率和推理速度，同时保障语义一致性。在长上下文LLM推理场景中实现了更优的性能/开销权衡，具有较高应用和推广价值。

Abstract: The growing memory footprint of the Key-Value (KV) cache poses a severe
scalability bottleneck for long-context Large Language Model (LLM) inference.
While KV cache eviction has emerged as an effective solution by discarding less
critical tokens, existing token-, block-, and sentence-level compression
methods struggle to balance semantic coherence and memory efficiency. To this
end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction
framework with \underline{a}daptive \underline{block} sizes. Specifically,
SABlock first performs semantic segmentation to align compression boundaries
with linguistic structures, then applies segment-guided token scoring to refine
token importance estimation. Finally, for each segment, a budget-driven search
strategy adaptively determines the optimal block size that preserves semantic
integrity while improving compression efficiency under a given cache budget.
Extensive experiments on long-context benchmarks demonstrate that SABlock
consistently outperforms state-of-the-art baselines under the same memory
budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%
retrieval accuracy with only 96 KV entries, nearly matching the performance of
the full-cache baseline that retains up to 8K entries. Under a fixed cache
budget of 1,024, SABlock further reduces peak memory usage by 46.28% and
achieves up to 9.5x faster decoding on a 128K context length.

</details>


### [81] [A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback](https://arxiv.org/abs/2510.22559)
*Zhifeng Wang,Xinyue Zheng,Chunyan Zeng*

Main category: cs.CL

TL;DR: 本文提出了一套集诊断、推荐和反馈于一体的个性化学习系统，在数据集实验中效果显著，可实际推广用于智能个性化教育。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习方法多将诊断、选题、反馈分步孤立处理，导致学生模型粗糙、泛化反馈不可操作，且适应性受限于假设。

Method: 整合神经认知诊断模型（NCD）、有界能力估计自适应测验策略（BECAT）及大语言模型（LLM），分别用于知识点掌握度精细诊断、动态选题、和结构化个性化反馈。

Result: NCD表现优异且可解释，推荐策略提升了题目相关性和个性化，LLM反馈针对性强，整体系统在ASSISTments数据集上验证有效并可应用于实际。

Conclusion: 提出了一种有效且可实际部署的端到端个性化学习代理——EduLoop-Agent，实现了“诊断-推荐-反馈”的闭环，能够为智能教育系统生成个性化学习路径。

Abstract: As information technology advances, education is moving from
one-size-fits-all instruction toward personalized learning. However, most
methods handle modeling, item selection, and feedback in isolation rather than
as a closed loop. This leads to coarse or opaque student models,
assumption-bound adaptivity that ignores diagnostic posteriors, and generic,
non-actionable feedback. To address these limitations, this paper presents an
end-to-end personalized learning agent, EduLoop-Agent, which integrates a
Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation
Computerized Adaptive Testing strategy (BECAT), and large language models
(LLMs). The NCD module provides fine-grained estimates of students' mastery at
the knowledge-point level; BECAT dynamically selects subsequent items to
maximize relevance and learning efficiency; and LLMs convert diagnostic signals
into structured, actionable feedback. Together, these components form a
closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments
on the ASSISTments dataset show that the NCD module achieves strong performance
on response prediction while yielding interpretable mastery assessments. The
adaptive recommendation strategy improves item relevance and personalization,
and the LLM-based feedback offers targeted study guidance aligned with
identified weaknesses. Overall, the results indicate that the proposed design
is effective and practically deployable, providing a feasible pathway to
generating individualized learning trajectories in intelligent education.

</details>


### [82] [Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems](https://arxiv.org/abs/2510.22581)
*Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 该论文全面分析了智能辅导系统（ITSs）主流评测方法的不足，结合真实案例阐释挑战，并提出三条未来研究路线，呼吁建立公平、统一、可扩展的ITS评估标准，助力AI教育领域健康发展。


<details>
  <summary>Details</summary>
Motivation: 人工智能教育领域（AIED）长期致力于开发智能辅导系统（ITSs），尤其近年来生成式AI（GenAI）的发展推动了大语言模型（LLM）驱动的ITSs。这类系统有望模拟人类教师的复杂教学过程，但目前缺乏统一、客观、以教学为核心的评测标准，导致研究进展和实际影响难以追踪和比较。

Method: 系统性回顾当前ITS评价实践，并结合真实案例分析存在的主要挑战；基于AIED跨学科前沿研究，提出三个切实可行、理论支撑的未来研究方向，围绕学习科学原则，推动ITS评测方法的公平性、统一性和可扩展性。

Result: 文章厘清了现有ITS评测的不足，如主观性强和标准混乱，导致结果难以推广和复现；结合案例全面展示现实难点，并提出三大研究方向，旨在构建更科学、公正和标准化的ITS评价体系。

Conclusion: 该工作不仅系统总结了ITS评测领域的现状与挑战，还指明了未来发展方向，推动建立以学习科学为基础、评价体系更统一和可扩展的方法，为整个AI教育领域的可持续发展奠定基础。

Abstract: The interdisciplinary research domain of Artificial Intelligence in Education
(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by
integrating insights from technological advancements, educational theories, and
cognitive psychology. The remarkable success of generative AI (GenAI) models
has accelerated the development of large language model (LLM)-powered ITSs,
which have potential to imitate human-like, pedagogically rich, and cognitively
demanding tutoring. However, the progress and impact of these systems remain
largely untraceable due to the absence of reliable, universally accepted, and
pedagogy-driven evaluation frameworks and benchmarks. Most existing educational
dialogue-based ITS evaluations rely on subjective protocols and
non-standardized benchmarks, leading to inconsistencies and limited
generalizability. In this work, we take a step back from mainstream ITS
development and provide comprehensive state-of-the-art evaluation practices,
highlighting associated challenges through real-world case studies from careful
and caring AIED research. Finally, building on insights from previous
interdisciplinary AIED research, we propose three practical, feasible, and
theoretically grounded research directions, rooted in learning science
principles and aimed at establishing fair, unified, and scalable evaluation
methodologies for ITSs.

</details>


### [83] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi,Elena Maria Muià,Federico Siciliano,Giovanni Trappolini,Vincenzo Crisà,Peter Kruger,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: AutoBench提出了一种交互互评的大模型评测新框架，能动态生成评测任务，通过多模型分布式评判与聚合，提供更贴近人类的可靠评测结果，优于传统静态测试方式。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）评测方法多为静态基准测试，存在测试集污染和适应性差的问题，无法持续有效地评估不断演化的语言模型。亟需一种更加动态和抗污染的新型评测框架。

Method: 提出AutoBench框架，通过让模型交替担任问题生成者、参赛者和评委，实现LLM的互评。采用迭代加权机制提升评判一致可靠模型的影响力，最终聚合为共识排名，多评委机制以增强评测的鲁棒性和与人类评测的一致性。

Result: AutoBench与传统主流基准（如MMLU-Pro和GPQA）结果高度相关（相关性分别为78%和63%），且多评委机制明显优于单评委，验证了该分布式评测方法的有效性和鲁棒性。

Conclusion: AutoBench作为一套全自动、可持续的LLM评测系统，能够抵抗测试集污染，具备良好的可扩展性，适合用于不断演进的语言模型的持续评估，优于静态基准。

Abstract: We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [84] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian,Ramesh Jain*

Main category: cs.CL

TL;DR: 该论文提出并概述了一个名为PCU的全球性AI健康管理系统，具备个性化健康信息、主动健康导航和医学事件响应等能力，有望持续伴随并优化个人及群体健康。


<details>
  <summary>Details</summary>
Motivation: 希望借助数字基础设施与生物医学创新，打造能够实时指导人们健康行为的智能伴随系统，弥补传统偶发性医疗的不足。

Method: 提出PCU系统架构，结合多模态代理、事件驱动建模与情境推断，并说明其设计原则与实施挑战。

Result: PCU不仅能为个人带来更优健康结果，还为公共健康与科学研究提供新基础。

Conclusion: PCU作为一种全球性AI驱动的个人健康管理系统，将会革新个体与群体的健康指导方式。

Abstract: Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [85] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani,Mohammadtaha Bagherifard,Erfan Zinvandi,Mehran Sarmadi*

Main category: cs.CL

TL;DR: PerCoR是首个大规模波斯语常识推理数据集，创新生成与筛选策略提升数据质量与难度，在多模型评测下展现出挑战性，推动波斯语常识推理研究，为英文等其他语种的数据集构建也提供借鉴。


<details>
  <summary>Details</summary>
Motivation: 目前在常识推理领域尚缺乏大规模的波斯语基准数据集，限制了相关AI模型在该语言上的发展和性能评估。

Method: 提出PerCoR数据集，包含10.6万条多项选择句子补全题，涵盖新闻、文化等多来源内容；创新性地采用基于连词的分割策略生成多样的题目；并提出DRESS-AF（假选项嵌入相似性评分及对抗过滤），从黄金续写中选择易混淆干扰项以提升难度。

Result: 人类在PerCoR上得分为89%，OpenAI-o3模型最高为92.18%，Claude-Sonnet-3.7为91.17%，领先的开源模型DeepSeek-R1为82.51%，显示该数据集难度较高且模型仍有提升空间；DRESS-AF方法在英文HellaSwag基准上同样有效，显著增加挑战性但不影响人类可解度。

Conclusion: PerCoR作为首个大规模波斯语常识推理基准数据集，对推动相关AI研究具有重要意义，同时提出的DRESS-AF方法提升了数据集的挑战性和泛用性。

Abstract: We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [86] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha,Sajal Saha,Debanjan Ballav,Soumi Mitra,Hritwick Chakraborty*

Main category: cs.CL

TL;DR: 该论文开发了基于AI与语言学结合的三语（Toto-Bangla-English）语言学习应用，用于濒危Toto语言的数字化保存与振兴，并探索了标准化、脚本、语料库构建和自动翻译等技术路线，为濒危语言保护提供了可持续且社区导向的模型。


<details>
  <summary>Details</summary>
Motivation: 每种语言都提供了独特的世界观，保护濒危语言对于语言和文化多样性至关重要。目前已开展多项记录濒危语言的全球行动，但面临技术适应性和社区可持续发展等挑战。

Method: 通过田野调查进行详细语言记录，构建形态学标注的三语（Toto-Bangla-English）语料库，并利用该语料库训练小型语言模型和基于Transformer的翻译引擎。同时，开发了Unicode脚本集成、脚本标准化与数字素养工具。

Result: 实现了三语学习应用，支持母语与非母语用户使用，对Toto语言进行了详细形态学分析，并促进了其在数字环境中的可用性。AI工具与传统语言学方法结合，推动了社区主导型语言复兴。

Conclusion: 人工智能和传统语言学相结合，为濒危语言保护和数字化提供了可持续的范例，同时强调了学科间合作对社区语言振兴的重要性。

Abstract: Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [87] [Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task](https://arxiv.org/abs/2510.22631)
*Marco De Santis,Lisa Alazraki*

Main category: cs.CL

TL;DR: 作者提出了FormaMentis基准，用意大利本地专家标注物理常识推理数据，并保留文化特色，助力多语言领域的数据评测。


<details>
  <summary>Details</summary>
Motivation: 当前大多数物理常识推理数据集集中于英语，缺乏对于其他语言和文化的关注。该任务旨在丰富物理常识推理领域的多语言评价数据，尤其关注非英语语境。

Method: 提出FormaMentis基准数据集，由意大利本土专家手工标注数据样本，结合意大利语言和文化特点，且将样本翻译成英文以保留独特的文化元素。

Result: 成功创建了FormaMentis物理常识推理基准，具备意大利文化特色，样本由本地专家标注，并实现了中英文对照。

Conclusion: FormaMentis为多语言物理常识推理领域提供了新的高质量数据集，强调文化和语言背景对推理任务的重要性。

Abstract: This paper presents our submission to the MRL 2025 Shared Task on
Multilingual Physical Reasoning Datasets. The objective of the shared task is
to create manually-annotated evaluation data in the physical commonsense
reasoning domain, for languages other than English, following a format similar
to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical
commonsense reasoning that is grounded in Italian language and culture. The
data samples in FormaMentis are created by expert annotators who are native
Italian speakers and are familiar with local customs and norms. The samples are
additionally translated into English, while preserving the cultural elements
unique to the Italian context.

</details>


### [88] [Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2510.22656)
*Zilong Wang,Qingtian Zeng,Hua Duan,Cheng Cheng,Minghao Zou,Ziyang Wang*

Main category: cs.CL

TL;DR: 本工作提出CR-FKGC新框架，通过多模块协同提升少样本知识图谱补全能力，并在多个基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决长尾分布和数据稀疏下的少样本知识图谱补全问题，目前已有方法在捕捉复杂关系模式和缓解数据稀疏性方面表现不足。

Method: 提出了一种新的共轭关系建模的少样本知识图谱补全（CR-FKGC）框架。该方法包括：1）利用邻域聚合编码器整合高阶邻居信息；2）共轭关系学习器，结合隐式条件扩散关系模块与稳定关系模块，捕捉稳定语义和不确定性补偿；3）流形共轭解码器，在流形空间高效推理缺失三元组。

Result: 在三个基准数据集上的实验表明，该方法在性能上显著优于现有最新方法。

Conclusion: 提出的CR-FKGC框架能更好地捕捉复杂关系并缓解数据稀疏性，在少样本知识图谱补全任务上取得了领先的效果。

Abstract: Few-shot Knowledge Graph Completion (FKGC) infers missing triples from
limited support samples, tackling long-tail distribution challenges. Existing
methods, however, struggle to capture complex relational patterns and mitigate
data sparsity. To address these challenges, we propose a novel FKGC framework
for conjugate relation modeling (CR-FKGC). Specifically, it employs a
neighborhood aggregation encoder to integrate higher-order neighbor
information, a conjugate relation learner combining an implicit conditional
diffusion relation module with a stable relation module to capture stable
semantics and uncertainty offsets, and a manifold conjugate decoder for
efficient evaluation and inference of missing triples in manifold space.
Experiments on three benchmarks demonstrate that our method achieves superior
performance over state-of-the-art methods.

</details>


### [89] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth,Parke Godfrey,Lukasz Golab,Divesh Srivastava,Jarek Szlichta*

Main category: cs.CL

TL;DR: 本文首次提出基于if-then规则解释RAG大语言模型输出，针对信息源与输出的关联生成解释规则，并引入高效剪枝策略，大幅提升生成效率，为RAG系统输出溯源和可解释AI提供有效手段。


<details>
  <summary>Details</summary>
Motivation: 随着RAG（检索增强生成）类大语言模型的兴起，模型输出受检索信息源影响显著，但目前尚无有效方法解释信息源与输出之间的关系。作者希望用可解释的规则揭示RAG系统输出的溯源。

Method: 作者提出首次将if-then规则用于解释RAG大语言模型决策，建立信息源与输出间的对应规则。采用Apriori风格的剪枝策略（借鉴频繁项集挖掘）优化规则生成流程，相比暴力遍历所有信息源组合，提高了效率。

Result: 通过定性与定量实验，验证了该方案能高效生成高质量的解释性规则，实证其有效性和高效率。

Conclusion: 该方法为RAG系统输出提供了透明可解释的规则，帮助理解信息源如何影响生成内容，同时其优化策略大幅节省生成规则的时间与资源。

Abstract: If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [90] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky,Shai Nahum-Gefen,Elad Ben Zaken*

Main category: cs.CL

TL;DR: SALSA提出了高效、鲁棒的LLM文本分类新方法，通过结构化提示和类别token映射有效提升分类性能，显著优于以往方法，在多项基准测试中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有指令微调的LLM在文本分类任务上表现不佳，本研究旨在提升LLM在这类任务上的性能。

Method: SALSA将结构化提示、类别到token映射，以及参数高效微调结合，避免了冷启动训练。每个类别标签都被映射为一个唯一输出token，模型通过单token回应实现分类，推理时只关注相关类别token的logits，实现高效准确分类。

Result: SALSA在多项文本分类基准中均取得了最高性能，展示了方法的有效性和泛化能力。

Conclusion: SALSA方法在各种文本分类基准上实现了SOTA结果，证明了其强大的鲁棒性和可扩展性，适用于基于LLM的分类任务。

Abstract: Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [91] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Jiaxin Mao*

Main category: cs.CL

TL;DR: E2Rank框架通过列表式目标持续训练嵌入模型，实现高效检索与高质量重排序，在多个权威基准上表现优异，显著提升了效率与准确率，实现检索与重排序的一体化。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型虽然检索高效但重排序效果有限，不能充分捕捉细粒度的查询-文档及文档间交互，亟需一种既高效又具备强重排序能力的模型。

Method: 提出了E2Rank统一框架，通过列表式排序目标对单一文本嵌入模型持续训练，使用余弦相似度进行统一排序，同时在排序阶段引入了列表式排序提示增强查询信息。

Result: E2Rank在BEIR重排序和BRIGHT推理基准上取得了业界领先的表现，重排序延迟极低；排序训练过程也提升了MTEB基准上的嵌入性能。

Conclusion: 单一文本嵌入模型可以同时完成高效检索和高质量重排序，兼顾计算效率和排序精度。

Abstract: Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [92] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan,Firas Saidani,Owen Van Esbroeck,Richard Khoury,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文提出利用持续预训练和参数高效微调方法，用极少数据和算力将大型语言模型适配到魁北克法语方言，并首次公开相关模型。实验证明此法能有效提升少数方言表现且主流语言损失极小，表明该方法为低资源语言社群创造高质量语言模型提供了经济可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的最佳性能主要集中在有丰富训练数据的高资源语言，低资源区域方言面临着模型适应性差的问题。近年来，持续预训练（CPT）被认为可以帮助模型适应低资源方言，但受限于数据和算力，如何高效地进行方言学习仍是一大挑战。

Method: 本文采用低秩适配（LoRA）和算力高效的持续预训练方法，将三种LLM微调至魁北克法语方言，使用极小数据集并在COLE基准测试套上进行评估。仅更新不到1%的模型参数，用参数高效微调（PEFT）实现对少数方言的适应。

Result: 实验结果显示：在少数方言基准上表现提升，在主流语言基准上的回退极小。结果分析表明效果高度依赖于语料组成。

Conclusion: 持续预训练结合参数高效微调（PEFT）能以低成本促进少数方言高质量LLM的创建，缩小方言间的技术鸿沟，为弱势语言社群提供更广泛的模型使用机会。本文发布了首个魁北克法语LLM，促进该领域发展。

Abstract: Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [93] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj,Deven Mahesh Mistry,Sahaj Singh Maini,Yash Aggarwal,Zoran Tiganj*

Main category: cs.CL

TL;DR: 该研究发现，不同架构的大语言模型在情景学习中呈现一致的时间性偏差，即更容易检索序列开头或结尾的相关信息，这与模型内部机制（如诱导头）相关，并类似于人类的情景记忆分离过程。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型（LLM）中，情景学习受时序和语义关系的共同影响。本研究受到人类情景记忆机制的启发，探索LLM能否像人脑一样区分并检索出时间上分隔的事件。

Method: 通过在序列中多次呈现同一标记（token），特别是该标记在序列末端再次出现，并固定重复标记的位置、改变其他部分，从而消除语义干扰，仅分析时间因素对下一个标记预测的影响。同时，进行消融实验和对比不同架构模型（transformer和state-space模型）。

Result: 无论模型架构，在序列中重复出现的标记后，下一个标记被赋予最高概率，且更偏好序列起始或末尾附近。此外，消融实验显示transformer中的“诱导头”机制相关。扩展到部分语义重叠的情况后，位于序列中间的记忆检索效果较差。不同架构间时间性偏差相似。

Conclusion: LLM在情境学习中具显著的时间性偏差，更容易检索序列最前或最后发生的信息，且该偏差有助于模型实现时间分隔和类人情节记忆检索。

Abstract: In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [94] [EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models](https://arxiv.org/abs/2510.22758)
*Li Zhou,Lutong Yu,You Lyu,Yihang Lin,Zefeng Zhao,Junyi Ao,Yuhao Zhang,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 现有语音模型难以结合声音和语义内容进行共情交互。EchoMind基准首次综合测试模型在多个层面上的共情能力，结果显示主流模型依旧难以精准感知和利用高表现力的声音线索，限制了情感回应质量。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型（SLMs）已经在口语理解方面取得了显著进展，但人们尚不清楚它们是否能全面感知非词汇性声音线索，并据此做出符合情感和语境的共情回应。现有基准多数只单独评估语言、声学、推理或对话能力，未能体现这些关键能力的整合。

Method: 提出了EchoMind，这是首个多层次互相关联的基准，用于模拟共情对话的认知过程。它包含四个连续、情境关联的任务：口语内容理解、声音线索感知、整合推理和回应生成。所有任务使用内容一致且语义中性、不含明确情感或情境线索的剧本，通过声音风格的控制变化检验语音传递的影响，并结合39种声音属性进行客观和主观评价。共测试了12个先进SLMs。

Result: 评测表明，即使是最先进的SLMs，在高表现力声音线索的感知上依然存在显著困难，导致共情回应质量受限。对提示强度、语音源和理想声音识别的分析揭示模型在遵循指令、应对自然语音多样性和有效利用声音线索方面的持续弱点。

Conclusion: 当前SLMs尚未能有效整合语言内容和多样的声音线索，难以实现真正具备共情能力的人机对话。

Abstract: Speech Language Models (SLMs) have made significant progress in spoken
language understanding. Yet it remains unclear whether they can fully perceive
non lexical vocal cues alongside spoken words, and respond with empathy that
aligns with both emotional and contextual factors. Existing benchmarks
typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in
isolation, overlooking the integration of these skills that is crucial for
human-like, emotionally intelligent conversation. We present EchoMind, the
first interrelated, multi-level benchmark that simulates the cognitive process
of empathetic dialogue through sequential, context-linked tasks: spoken-content
understanding, vocal-cue perception, integrated reasoning, and response
generation. All tasks share identical and semantically neutral scripts that are
free of explicit emotional or contextual cues, and controlled variations in
vocal style are used to test the effect of delivery independent of the
transcript. EchoMind is grounded in an empathy-oriented framework spanning 3
coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and
evaluated using both objective and subjective metrics. Testing 12 advanced SLMs
reveals that even state-of-the-art models struggle with high-expressive vocal
cues, limiting empathetic response quality. Analyses of prompt strength, speech
source, and ideal vocal cue recognition reveal persistent weaknesses in
instruction-following, resilience to natural speech variability, and effective
use of vocal cues for empathy. These results underscore the need for SLMs that
integrate linguistic content with diverse vocal cues to achieve truly
empathetic conversational ability.

</details>


### [95] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem,Muhammad Hazim Al Farouq,John D. Kelleher*

Main category: cs.CL

TL;DR: 作者通过分析模型层重要性，进行逐步剪枝，在保持翻译质量的前提下，实现了大语言模型显著压缩和加速推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理领域表现优异，但由于计算资源消耗巨大，部署效率不高，因此研究高效压缩方法以提升部署可行性。

Method: 采用层重要性分析指导的迭代层剪枝方法，对Aya-Expanse-8B模型进行压缩。

Result: 模型体积和推理时间显著降低，翻译质量与未压缩基线模型保持一致。

Conclusion: 通过逐层剪枝和重要性分析，可以有效压缩大语言模型并减少推理时间，同时保持翻译质量。

Abstract: Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [96] [MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion](https://arxiv.org/abs/2510.22768)
*Haoyi Qiu,Yilun Zhou,Pranav Narayanan Venkit,Kung-Hsiang Huang,Jiaxin Zhang,Nanyun Peng,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本研究提出MMPersuade多模态说服研究框架和数据集，揭示LVLMs在多模态说服输入下更易受影响，不同情境下说服策略效果各异，有助于构建更安全、伦理的生成式AI模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉-语言模型（LVLMs）在购物、健康及新闻等领域的广泛应用，它们越来越多地接触到带有说服意图的多模态内容。本研究关注这些模型为何、怎样会被多模态说服信息影响，并关注模型对不同说服策略的易感性及其后果（如误导性信念、违背用户偏好、生成不安全内容等）。

Method: 提出MMPersuade统一框架，从系统性角度研究多模态内容对LVLMs的说服动态。该框架包括一个多模态数据集（图像、视频+多种说服策略，涵盖商业、主观/行为及对抗性的说服情景），以及相关评价体系，通过第三方一致性评分、模型自评token概率等指标量化说服效果与易感性。选取六种主流LVLMs开展实验。

Result: 1）与仅文本输入相比，多模态输入大幅提升说服效果和模型易感性，尤其是在虚假信息场景下；2）表明先验偏好的模型易感性降低，但多模态仍保持说服优势；3）不同说服策略在不同情境下效果有别：互惠策略在商业/主观场景中最有效，可信度与逻辑策略在对抗场景更有力。

Conclusion: MMPersuade框架为理解与量化LVLMs面对多模态说服信息时的易感性和说服效果提供了系统研究方法，有助于未来开发更健壮、符合用户偏好并伦理对齐的模型。

Abstract: As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.

</details>


### [97] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen,Duc M. Nguyen,Hyotaek Jeon,Hyunwook Lee,Hyunmin Song,Sungahn Ko,Taehwan Kim*

Main category: cs.CL

TL;DR: 本文提出VEHME视觉-语言模型，通过结构化推理和强化学习训练，实现了对手写数学答案的高效自动化评估，并在主流数据集上展现出领先性能，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 自动评估手写数学解答在教育技术领域非常重要，但因学生作业格式多样、布局无结构和符号复杂性高，评估存在较大挑战。

Method: 提出了名为VEHME的视觉-语言模型，用于高准确度和可解释性的手写数学表达评估。该方法包括两阶段训练：首先用结构化推理数据进行有监督微调，其次用强化学习使模型输出符合多维评分目标（正确性、推理深度、错误定位）。此外，设计了表达式感知视觉提示模块，增强模型对空间信息的理解。模块在合成的多行数学表达式数据集上训练，用以提升在视觉异质性输入下的鲁棒性。

Result: 在AIHub和FERMAT数据集上，VEHME在开源模型中实现了最先进的性能，精度接近于专有系统，展示了其作为自动化数学评估工具的可扩展性和实用性。所有训练和实验代码已公开。

Conclusion: VEHME实现了对开放式手写数学答案的高效自动评估，在准确性和解释性方面优于现有开源工具，是教育领域可访问和高效的解决方案。

Abstract: Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [98] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova,Amrit Adhikari,Matthew Pearson,Vamsi Krishna Sadu,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文系统评估了商用API与开源大模型在人权违规检测中的多语种表现。结果表明“对齐”优于“规模”，商用模型可为低资源语种任务带来更高可靠性，对有预算压力的人道组织具有重要指导意义。


<details>
  <summary>Details</summary>
Motivation: 人道主义组织在多语言人权监测中面临选择：使用昂贵的商业API还是依赖免费但未经充分验证的开放权重大模型。特别是在资源有限、存在冲突的地区，低资源语言的自动处理能力急需提升。

Method: 本文首次系统性对比了商业和开源开放权重大语言模型（LLMs），评估它们在人权违规检测任务中的表现，涵盖七种语言，总共78,000个多语言推理样本。考察了六种模型（四个商用对齐模型和两个开源模型），并使用传统分类标准和全新提出的跨语种可靠性指标（如Calibration Deviation、Decision Bias等）进行评估。

Result: 研究发现，模型的“对齐”水平（即与任务需求的适配性）远比模型规模重要。对齐好的商用模型在包括低资源语言在内的多语种任务中表现出稳定的准确性和校准表现，而开源模型在不同语种Prompt下则表现出较大波动及校准漂移。

Conclusion: 多语言对齐对于实现语言无关的推理至关重要。对于预算有限的人道主义组织，商用对齐模型提供了跨语种的稳定性和高可靠性，开源模型则在低资源语言上存在明显不足。

Abstract: Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [99] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua,Hong Jiao,Xinyi Wang*

Main category: cs.CL

TL;DR: 本文针对BERT等模型在长文本自动评分任务上的限制，提出利用生成式语言模型结合总结和提示方法，有效提升评分准确性，QWK指标有明显提升。


<details>
  <summary>Details</summary>
Motivation: BERT等编码器模型因512 token限制，难以准确对长篇作文进行自动评分，因此需要寻找更适合处理长文本的方法。

Method: 通过总结和提示，将生成语言模型应用于长篇作文的自动评分任务，并与基于BERT的编码器模型做对比。

Result: 在Learning Agency Lab Automated Essay Scoring 2.0数据集上，QWK评分从0.822提升到0.8878，显著提高了长文本评分准确性。

Conclusion: 生成式语言模型能更有效地用于长文本自动评分，提高了准确率。

Abstract: BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [100] [Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning](https://arxiv.org/abs/2510.22844)
*Prerna Ravi,Dong Won Lee,Beatriz Flamia,Jasmine David,Brandon Hanks,Cynthia Breazeal,Emma Anderson,Grace Lin*

Main category: cs.CL

TL;DR: 本研究提出在同步群体口语会话中识别并标注对话线程可显著提升大语言模型对协作行为的自动分析表现，并探讨了自动化及人机协作的实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 小组协作学习中，理解想法如何在群体对话中发展和流动至关重要，线程结构是会话组织的重要特征。由于同步口语对话中回合重叠和隐性线索，自动线程检测极具挑战，同时现有大语言模型在长上下文对话分析中表现有限。

Method: 提出一份系统化指南用于识别多方同步对话中的线程，建立自动化线程检测的基准，并评估不同的大语言模型提示策略，然后测试线程信息对后续会话框架编码表现的影响。

Result: 提供清晰线程信息后，大语言模型在核心协作行为（如同意、建设性发展、引导等）的编码任务中表现提升，且人机协作在效率与成本方面具有实际优势。

Conclusion: 明确的对话线程信息能提升大语言模型对群体对话分析的编码表现，同时结构化的对话对于后续分析至关重要。作者还探讨了投入的时间和成本，以及人机协同策略的实际优势。

Abstract: Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

</details>


### [101] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.CL

TL;DR: 本文提出PIPS方法，在无需特殊引导和测试用例的条件下，有效提升大模型多步推理与算法任务的准确率，大幅降低生成错误程序的风险。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在零样本推理方面表现出色，但对多步复杂推理任务仍存在困难。以往链式思维（CoT）、程序化思维（PoT）等方法虽然提升了性能，但在算法领域仍会生成不理想的解答。

Method: 提出了一种“按实例程序合成”（PIPS）的方法，在无需任务特定引导或显式测试案例的情况下，通过结构化反馈对每个实例生成和优化程序。同时引入置信度指标，动态在直接推理和程序合成之间选择。

Result: 在三种主流LLM和30个基准（涵盖BBEH全部任务、视觉问答、关系推理和数学推理任务）中，PIPS比PoT与CoT在绝对谐波平均准确率上分别提升了8.6%和9.4%；在算法任务上，将不良程序生成率减少了65.1%。

Conclusion: PIPS方法显著提升了LLM在复杂推理场景下的性能，尤其减少了算法任务中的错误程序生成。

Abstract: Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [102] [Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement](https://arxiv.org/abs/2510.22860)
*Linyang He,Tianjun Zhong,Richard Antonello,Gavin Mischler,Micah Goldblum,Nima Mesgarani*

Main category: cs.CL

TL;DR: 提出了一种残差解耦法，将语言模型内部词汇、句法、语义和推理表征分离。采用该方法分析神经信号发现，推理特征有独立且晚峰神经响应，预测性优于常规嵌入，能揭示人脑深度语言认知加工过程。


<details>
  <summary>Details</summary>
Motivation: 理解人脑如何从处理简单的语言输入到进行高级推理，是神经科学中的基本挑战。现有大型语言模型尽管能拟合神经活动，但其内部表征信息高度混杂，难以分离语义、句法等深层认知过程的对应神经基础。

Method: 作者提出了一种残差解耦方法，先通过语言模型定位特征相关的层，再逐步回归消除低层次表征，获得词汇、句法、语义和推理的近乎正交的表征。利用这些解耦后的嵌入去建模ECoG（皮层电图）患者听自然语音时的脑活动。

Result: 1）推理嵌入有独特预测力，可以解释其他语言特征无法解释的脑活动变异，甚至涉及语言区外的视觉区招募。2）推理的脑信号时序独特，比词汇、句法、语义信号晚约350-400毫秒，反映认知加工层级。3）常规未解耦的语言模型嵌入预测能力主要归因于浅层语言特征，掩盖了深层认知处理的贡献。

Conclusion: 解耦后的语言模型嵌入能揭示人脑加工深层认知语言特征（如推理）的独特神经表征，常规嵌入存在混淆，易低估高级认知特征在大脑语言加工中的作用。

Abstract: Understanding how the human brain progresses from processing simple
linguistic inputs to performing high-level reasoning is a fundamental challenge
in neuroscience. While modern large language models (LLMs) are increasingly
used to model neural responses to language, their internal representations are
highly "entangled," mixing information about lexicon, syntax, meaning, and
reasoning. This entanglement biases conventional brain encoding analyses toward
linguistically shallow features (e.g., lexicon and syntax), making it difficult
to isolate the neural substrates of cognitively deeper processes. Here, we
introduce a residual disentanglement method that computationally isolates these
components. By first probing an LM to identify feature-specific layers, our
method iteratively regresses out lower-level representations to produce four
nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,
reasoning. We used these disentangled embeddings to model intracranial (ECoG)
brain recordings from neurosurgical patients listening to natural speech. We
show that: 1) This isolated reasoning embedding exhibits unique predictive
power, accounting for variance in neural activity not explained by other
linguistic features and even extending to the recruitment of visual regions
beyond classical language areas. 2) The neural signature for reasoning is
temporally distinct, peaking later (~350-400ms) than signals related to
lexicon, syntax, and meaning, consistent with its position atop a processing
hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as
their predictive success is primarily attributable to linguistically shallow
features, masking the more subtle contributions of deeper cognitive processing.

</details>


### [103] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy,Ayush Rajesh Jhaveri,Ilias Triantafyllopoulos*

Main category: cs.CL

TL;DR: 本文研究了LLMs在重新提问时答案翻转的不确定性，发现来源于少数非检索Attention头的错误关注。针对性遮蔽可减少此类行为，提升模型稳定性，但在下游任务效果存在权衡。提出了一条缓解LLMs不确定性的方法，对解释性与安全性研究具有参考价值。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在重新提问时，可能将先前正确的答案更改为错误答案，表现出不希望出现的不确定性，影响用户信任，尤其在高风险领域带来严重风险。作者旨在揭示导致此类不确定性行为的机制。

Method: 作者采用了“针在草堆中”检索框架，并集成了Flip风格的再评估提示，模拟现实中的答案翻转场景。分析了模型的不同Attention头，将重点放在检索头和非检索Attention头的作用上。实验中通过遮蔽特定Attention头观察模型行为的变化。

Result: 发现导致不确定性的主要并非检索头，而是少数非检索Attention头在不确定场景中过度关注误导性token。通过遮蔽这些Attention头，可显著减少答案翻转现象（最多降低15%），且不会带来语义不连贯或过度修正的问题。但在下游任务中，减少翻转行为存在一定权衡。

Conclusion: 该工作揭示了大型语言模型中不确定性机制的关键，提出了一种简单有效的技术手段缓解模型的不确定性失败模式，并为机理可解释性（mechanistic interpretability）研究领域做出了贡献。

Abstract: Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [104] [A Comprehensive Dataset for Human vs. AI Generated Text Detection](https://arxiv.org/abs/2510.22874)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Amit Sheth,Vasu Sharma,Aishwarya Naresh Reganti,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 本研究发布了一个包含真实新闻和多模型AI生成文本的大型数据集，建立了AI文本检测和模型归因的基准，推进了AI内容真实性检测领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展使AI生成文本越来越像人类，带来了内容真实性、错误信息和可信度的担忧。当前急需大规模、多样且高质量标注的数据集，来应对检测AI文本和归因的问题。

Method: 该研究构建并公开了一个包含5.8万余条数据的大型数据集，结合了真实纽约时报文章和由多种前沿LLM（如GPT-4-o等）生成的合成版本。数据为原始摘要作为提示，配合完整的人类或AI写作内容。基于这些数据，建立了区分人类与AI文本、及归因AI文本到具体模型的基线方法与结果。

Result: 在人类与AI文本区分任务上，基线模型准确率为58.35%；AI文本归因到具体生成模型的准确率仅为8.92%。

Conclusion: 该数据集融合了真实新闻内容和主流生成模型内容，为AI文本检测及模型归因任务发展提供了重要资源，有助于提升生成式AI内容的透明度和公信力。

Abstract: The rapid advancement of large language models (LLMs) has led to increasingly
human-like AI-generated text, raising concerns about content authenticity,
misinformation, and trustworthiness. Addressing the challenge of reliably
detecting AI-generated text and attributing it to specific models requires
large-scale, diverse, and well-annotated datasets. In this work, we present a
comprehensive dataset comprising over 58,000 text samples that combine
authentic New York Times articles with synthetic versions generated by multiple
state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,
Yi-Large, and GPT-4-o. The dataset provides original article abstracts as
prompts, full human-authored narratives. We establish baseline results for two
key tasks: distinguishing human-written from AI-generated text, achieving an
accuracy of 58.35\%, and attributing AI texts to their generating models with
an accuracy of 8.92\%. By bridging real-world journalistic content with modern
generative models, the dataset aims to catalyze the development of robust
detection and attribution methods, fostering trust and transparency in the era
of generative AI. Our dataset is available at:
https://huggingface.co/datasets/gsingh1-py/train.

</details>


### [105] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang,Soumik Dey,Ashirbad Mishra,Hansi Wu,Binbin Li,Rui Zhang*

Main category: cs.CL

TL;DR: 论文针对LLM批量speculative decoding中的对齐及不规则张量问题，提出EQSPEC保证正确性、EXSPEC提升效率，实证达到3倍推理速度提升且高保真，易集成现有系统。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）推理中，为了加速推理过程，常采用 speculative decoding，通过一个较小的草稿模型生成多个候选token，再由目标模型并行验证。这种方法如能批量化应用，对于实际生产部署至关重要，但由此带来了“ragged tensor（不规则张量）”的问题：同一批次中的序列接受的草稿token数量不同，破坏了对齐，导致位置ID、注意力掩码和KV缓存出错。正确解决这一问题是提升批量推理速度和兼容性的动力。

Method: 作者对现有的批量speculative decoding实现进行了分析，发现由于对ragged tensor问题处理不当，存在输出不一致、即与传统自回归生成不等价的问题。为此，作者：（1）形式化描述了保证正确同步的需求；（2）提出以正确性优先的批量speculative decoding算法EQSPEC，揭示对齐操作占总开销的40%；（3）进一步提出EXSPEC，通过动态分组与滑动池技术，在减少对齐开销的基础上保持每个序列的加速优势。

Result: 在SpecBench数据集，以及Vicuna-7B/68M、Qwen3-8B/0.6B、GLM-4-9B/0.6B等多组目标/草稿模型组合下，作者方法在batch size为8时，相较于batch size 1，推理吞吐量提升最高达3倍，并且在batch size 8时保持95%的输出等价率。所提方法无需自定义底层kernel，与现有推理栈良好兼容。

Conclusion: 本工作系统性剖析了批量speculative decoding过程中的不规则张量问题，提出了理论分析与两种高效算法（EQSPEC和EXSPEC），显著提升了批量推理效率，兼顾正确性与易用性，并对LLM产线部署具有实际应用价值。

Abstract: Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [106] [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954)
*Liwei Jiang,Yuanjun Chai,Margaret Li,Mickel Liu,Raymond Fok,Nouha Dziri,Yulia Tsvetkov,Maarten Sap,Alon Albalak,Yejin Choi*

Main category: cs.CL

TL;DR: 提出Infinity-Chat数据集，系统性分析语言模型在开放式生成任务中的输出多样性，发现“人工蜂巢效应”及评分机制在人类主观偏好校准上的不足，为AI安全研究和模型优化提供重要支持。


<details>
  <summary>Details</summary>
Motivation: 针对语言模型在生成多样化、具有创意的内容方面的局限性，以及缺乏大规模、体系化评估其输出多样性的工具，亟需构建可系统分析开放式生成及对应人类偏好的基础资源。

Method: 构建了一个包含2.6万条开放式真实用户查询（Infinity-Chat）的大型数据集，并制定了全面的任务分类体系。通过模型输出多样性分析、人工标注以及对偏好评分的校准研究，系统性地评估了多款语言模型的输出模式。

Result: 发现语言模型在开放式生成中不仅单一模型输出趋同，不同模型间输出也高度相似；且人类偏好评分与模型评分一致性较差，特别是在主观性较强的查询上。该数据集成为研究语言模型多样性及人类偏好校准的首个大型资源，为AI安全研究提供了重要数据支持。

Conclusion: INFINITY-CHAT数据集揭示了语言模型在开放式生成任务中的“人工蜂巢效应”，即模型输出趋同，并且模型的评分机制在区分人类主观偏好方面存在不足。这些发现对AI长期安全具有重要意义。

Abstract: Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

</details>


### [107] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal,Karen Hovsepian,Tinghao Guo,Mengnan Zhao,Somendra Tripathi,Nikos Kanakaris,George Mihaila,Sumit Nigam*

Main category: cs.CL

TL;DR: 提出TAG方法，通过标签增强提升LLM在长上下文和复杂推理下的表现，实验显示显著优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在长上下文场景中的有效问答与推理存在明显的性能瓶颈。虽然RAG和分块重排序等方法能部分改善，但这些方法对分块、嵌入和检索策略高度敏感，并且依赖复杂的预处理和知识索引。

Method: 提出了一种轻量级的数据增强策略——Tagging-Augmented Generation (TAG)，通过对上下文进行标记或在问答提示中添加标签定义，提升长上下文下的LLM性能，同时保持文档完整性。

Result: 在NoLima和NovelQA两个挑战性问答基准上进行了实验，TAG方法在32K token上下文下性能提升高达17%，多跳复杂推理问答提升2.9%。

Conclusion: TAG是一种简单高效的增强策略，在无需改变原始检索文档的情况下，显著提升了大模型在长文本和复杂推理场景中的表现。

Abstract: Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [108] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning,Xixun Lin,Fang Fang,Yanan Cao*

Main category: cs.CL

TL;DR: 为解决LLM在长文本事实性评估的难题，作者提出了多智能体和分层加权方法，并构建了中国长文本数据集与验证系统。实验显示方法有效，尤其适用于中文内容，为LLM在关键领域安全使用提供基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）被广泛应用，但在高风险领域如生物医学、法律和教育中，其生成内容的事实准确性令人担忧。目前针对短文本的评估方法，难以有效处理长文本中的复杂推理和多样信息。因此，需要方法来解决长文本事实性评价的问题。

Method: 提出系统性方法，整合大规模长文本数据集、多智能体验证机制和加权评价指标。具体包括构建中文长文本事实性数据集LongHalluQA，开发基于辩论的多智能体验证系统MAD-Fact，并引入事实重要性层级以捕捉不同声明的影响力。

Result: 在两个基准任务上的实验显示，更大规模的LLM一般具备更高的事实一致性，而国产模型对中文内容表现更优。

Conclusion: 该论文提供了有体系的评估框架用于提升长文本LLM生成内容的事实可靠性，有助于模型在敏感领域安全部署。

Abstract: The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [109] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

Main category: cs.CL

TL;DR: 本文提出并验证了基于句子级嵌入的定制LLM在课堂教学质量评估中的新方法。结果显示该方法能实现甚至超越人类专家一致性的评分，同时模型得分能反映有效教学对学生学习的影响，具有很强的外部效度与实际应用潜力，但在单项评分与泛化能力上仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 在教育领域，客观且可扩展地衡量教学质量一直是一个挑战。目前通用大型语言模型（LLM）在应用于复杂、真实的课堂观测工具时表现不佳，因此需要新的方法来提升其适应性和准确性。

Method: 本文利用基于句子级嵌入的定制LLMs，构建适合课堂长文本、解释性转录数据的模型架构，并系统评估五种不同句子嵌入方法，采用数据高效训练方法以防止过拟合。还通过分析标注的上下文窗口，探讨模型如何分配评分变异性，并检验模型分数与教师增值（value-added）指标的外部效度。

Result: 定制句子嵌入LLMs可实现与人类专家一致甚至超越人类的评分表现（专家相关性超过0.65），并优于一般人类评分者之间的一致性。对标注窗口的分析显示，表现更好的模型更重视课时整体特征，弱化单句话语对评分的影响。模型的总体分数与教师增值指标一致，说明模型确实捕捉到与学生学习相关的有效教学特征，但在单项评分上未完全泛化。

Conclusion: 研究建立了一种可行且强大的人工智能驱动教学质量评估方法，为教师发展提供可靠、可扩展和有效的反馈机制，并指出模型尚需进一步提升以实现完全泛化。

Abstract: Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [110] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang,Timothy Tin-Long Tse,Jian Zhu*

Main category: cs.CL

TL;DR: 本文系统评估大型语言模型（LLMs）在上下文学习任务的内部机制，发现不同架构模型虽任务表现类似但实现方式不同、功能向量分布存在差异，为理解并优化模型能力提供新视角和方法。


<details>
  <summary>Details</summary>
Motivation: 大模型在上下文学习（ICL）任务表现优秀，但不同架构模型其内部机制是否一致仍不清楚。研究旨在探索各类大模型内部实现ICL的机制及不同任务类型下的表现。

Method: 对多种主流架构（Transformer、State-space、混合型）的大模型在两类知识型ICL任务上进行深入评估，结合行为探测和干预分析方法，定位支持ICL的功能向量（FVs）及其所在层次。

Result: 发现不同架构的大模型在任务表现上可相似，但内部机制有差异。功能向量主要位于自注意力及Mamba层，Mamba2模型可能采用与FVs不同机制实现ICL。FVs对参数化知识检索型ICL更为重要，对背景理解型ICL则影响较小。

Conclusion: 通过行为与机制分析结合，揭示了不同架构、任务类型下大模型ICL机制的细致差异，为未来模型设计和能力解析提供参考。

Abstract: We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [111] [LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models](https://arxiv.org/abs/2510.23011)
*Sammriddh Gupta,Sonit Singh,Aditya Joshi,Mira Kim*

Main category: cs.CL

TL;DR: 论文介绍了一款基于大语言模型的智能对话系统，能实时提供语法反馈、生成练习并追踪学习进展，经评估效果积极，助力语言教育。


<details>
  <summary>Details</summary>
Motivation: 语言教师在为学习者提供反馈和练习方面存在限制，难以创造丰富的学习体验，需要辅助工具提升教学效果。

Method: 设计并开发了LangLingual对话式智能体，基于LangChain框架和大型语言模型，能够实时反馈语法、生成情境化练习、跟踪学习者水平；论文详细介绍了其架构、实现与评估。

Result: 系统在可用性、学习成效和学习者参与度等方面表现良好。

Conclusion: LangLingual有助于突破教师反馈与练习的限制，提升语言学习体验。

Abstract: Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

</details>


### [112] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu,Jingjing Chen,Jiayu Ye,Yu Wu,Jun Yan,Carl Yang,Hongkun Yu*

Main category: cs.CL

TL;DR: 本论文提出结合代码执行器的TIR-Judge评分框架，通过迭代强化学习提升LLM自动化评审能力，在多基准测试上显著优于传统方法，无需蒸馏亦能达到高水平评分效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）作为自动化评分工具主要依赖文本推理，难以验证复杂约束或进行准确计算。工具集成推理（TIR）已在多任务中展现潜力，激励了将其应用于LLM评分任务。

Method: 提出了TIR-Judge，一个端到端的强化学习（RL）框架。该方法结合代码执行器实现精确评分。设计三大原则：覆盖可验证及不可验证领域的多样化训练，支持点评、对比和序列式评分格式，以迭代RL方式从初始模型自我提升，无需蒸馏。

Result: 在七个公开基准上，TIR-Judge的单点评分和对比评分分别优于传统推理型评审最多6.4%和7.7%。序列式评分性能接近参数量远大（Claude-Opus-4）的模型。此外，TIR-Judge-Zero即使完全未用评分轨迹蒸馏，在性能上也能匹配蒸馏版本。

Conclusion: 工具赋能的评分模型能通过迭代强化学习自我进化，实现更高效和精确的自动化评审而无需传统的蒸馏流程。

Abstract: Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [113] [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)
*Zhanchao Zhou,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Jianguo Li*

Main category: cs.CL

TL;DR: 该文提出KHA注意力机制，实现多头间交互，提升大模型表现并稳定训练，参数开销小，可部署到现有多头注意力系统。


<details>
  <summary>Details</summary>
Motivation: 现有多头注意力（MHA）通过并行注意力头提升表达能力，但增加头数会削弱单个头的能力，且各头输出仅拼接，缺乏充分交互。近期变体如GQA和GTA也存在类似问题，未能实现头之间的信息交流。

Method: 提出Knocking-Heads Attention (KHA)，通过在缩放点积注意力之前，让不同注意力头之间进行特征层面的交互。具体做法是对所有头应用一个共享且对角初始化的投影矩阵，既保证初期专化，同时逐步学习跨头综合表征。KHA参数量和计算量增加很小，可无缝集成到现有MHA和变体。

Result: 作者在一个拥有6.1B参数（激活1.01B）的MoE模型上，在1万亿高质量token上训练，验证了KHA的有效性。相比基线注意力机制，KHA在下游任务表现更好，训练更稳定。

Conclusion: KHA能有效促进不同注意力头间的信息交流，提升大模型表现且训练更稳定，且具备很强的兼容性和高性价比。

Abstract: Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

</details>


### [114] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon,Byeolhee Kim,Nikhil Verma*

Main category: cs.CL

TL;DR: 提出QTT-RAG方法，给翻译文档加质量标签，显著提升多语言检索生成尤其低资源语言的准确性和稳健性，优于主流基线。


<details>
  <summary>Details</summary>
Motivation: 多语言检索增强生成任务中，低资源语言无法直接检索足够原生文档，需依赖英文文档翻译。但翻译质量低会影响生成结果，目前方法不是假定翻译足够好，就是用重写方法但易造成失真与虚构，因此需要新机制提升低资源环境下的跨语言检索生成质量和稳健性。

Method: 提出QTT-RAG方法：不改变原始文档内容的基础上，对翻译后的文本进行三维度（语义、语法、流畅性）质量评估，并以元数据形式存储。对比CrossRAG与DKM-RAG，采用六个规模不同（2.4B至14B参数）的指令微调大型语言模型，在两个公开QA基准（XORQA, MKQA）上评测覆盖韩语、芬兰语、中文等不同资源语言。

Result: QTT-RAG在保留事实的完整性同时，让生成器根据翻译可靠性做出更合理判断，有效提升了低资源语言场景下跨语言文档利用效率和生成表现，较现有基线具有更好表现且解决了翻译失真和虚构问题。

Conclusion: QTT-RAG方法通过为检索到的英文文档翻译添加质量标签（语义等价性、语法准确性、自然度与流畅性），显著优于现有基线，提升了跨语言检索增强生成的准确性和可靠性，尤其在低资源语言环境下效果显著。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [115] [A Survey on LLM Mid-training](https://arxiv.org/abs/2510.23081)
*Chengying Tu,Xuemiao Zhang,Rongxiang Weng,Rumei Li,Chen Zhang,Yang Bai,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 本综述系统梳理了大型语言模型训练中的中期训练环节，归纳其定义、优化方法和主流实践，总结其对提升模型能力的关键作用，为后续相关研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的进步，发现多阶段训练尤其是中期训练能够显著促进语言模型能力提升，因此亟需系统性梳理和总结中期训练的流程与方法。

Method: 本文通过对主流语言模型进行分析，总结并归纳中期训练的定义、优化框架、包括数据筛选、训练策略和模型结构优化等方面。

Result: 提出了中期训练的正式定义，梳理了主流优化框架，构建了分类体系，为未来大型语言模型的研究与创新提供了理论和方法参考。

Conclusion: 中期训练在大型语言模型的发展中起着至关重要的作用，是连接预训练与后训练的关键阶段。系统性地优化中期训练，有助于持续提升模型核心能力。

Abstract: Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

</details>


### [116] [MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2510.23090)
*Suchan Lee,Jihoon Choi,Sohyeon Lee,Minseok Song,Bong-Gyu Jang,Hwanjo Yu,Soyeon Caren Han*

Main category: cs.CL

TL;DR: 本文提出MAP4TS多视角提示框架，把传统时间序列分析方法融入LLM输入，显著提高预测效果。结构化提示+较小的LLM主干（如GPT-2）在长期预测上优于更大的模型。


<details>
  <summary>Details</summary>
Motivation: 当前利用大型语言模型(LLMs)进行时间序列预测受到关注，但现有多模态方法常常没有充分考虑时间序列数据独特的统计属性和时序依赖性。因此，研究者希望通过创新方法更好地将时间序列分析融入到LLM应用中。

Method: 提出MAP4TS，一种多视角提示框架，将经典的时间序列分析（如ACF、PACF和傅里叶分析）明确融入提示设计。具体包括四种提示组件：全局领域提示（描述数据集整体上下文）、局部领域提示（刻画近期趋势和序列具体行为）、统计与时序提示（包含手工提取的相关性与频域特征）。这些多视角提示与原始时间序列嵌入结合，通过跨模态对齐模块得到统一表示，经LLM处理并输出预测结果。

Result: 在八个多样数据集上，MAP4TS稳定优于现有LLM时间序列预测方法。消融实验显示，提示感知设计能显著提升模型表现的稳定性。此外，搭配结构化提示的GPT-2主干模型，在长期预测任务中，表现大于更大的模型如LLaMA。

Conclusion: 将经典时间序列分析有效融合到提示工程中，可大幅提升LLM在时间序列预测任务的准确性与稳定性。提示结构设计优于盲目扩展语言模型规模。

Abstract: Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.

</details>


### [117] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu,Katelyn X. Mei,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 通过在医学多文档摘要任务中引入分层结构，模型生成的摘要在清晰度、覆盖度、连贯性等方面优于传统方法，专家更倾向于这些摘要，表明分层组织能显著提升聚合生成结果的质量。


<details>
  <summary>Details</summary>
Motivation: 医学多文档摘要（MDS）任务复杂，需要有效处理文档之间的关系。传统的摘要方法大多采用扁平结构，难以充分组织和呈现跨文档的信息，作者希望探索分层结构是否能提升MDS的效果。

Method: 作者在三种大型语言模型中设计并实验了两种引入分层结构的方法，并用自动化指标、模型评测和领域专家从多个维度（偏好、可理解性、清晰度、复杂性、相关性、覆盖度、事实性和连贯性）全面评估所得摘要。还考查了GPT-4模拟判断与人工判断的一致性。

Result: 实验发现，专家更偏好模型生成的摘要而非人工摘要。引入分层结构的方法能提升摘要的事实性、覆盖度和连贯性，同时增加专家的整体偏好。此外，分层结构提高了摘要清晰度，且GPT-4在客观评价指标上与人类判断较为一致。

Conclusion: 在医学多文档摘要中，分层结构不仅能维持信息的完整与连贯，还能提升摘要的清晰度和被人类专家的偏好度，为生成模型摘要提供了可行且有效的改进方式。

Abstract: Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [118] [Flexing in 73 Languages: A Single Small Model for Multilingual Inflection](https://arxiv.org/abs/2510.23114)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 提出了一个联合训练于73种语言的词形变化单模型，实验结果优于多数语言的单语模型，并引入了更实际的数据分割方式。模型轻量、具备通用性，降低了多语言部署复杂度，并已完全开源。


<details>
  <summary>Details</summary>
Motivation: 在多语言环境下，词形变化（inflection）任务通常需要为每种语言单独训练模型，部署和维护成本高，且在处理未见词时表现有限。目前缺乏一个能够广泛适用于多语言并具备鲁棒性的通用词形变化系统。

Method: 作者提出了一个紧凑的单一模型方法，在73种语言上联合训练，用于词形变化任务。该模型采用了通用依存语料库和SIGMORPHON的标准基准，并设计了新的数据分割方式：基于词频与词条分离的训练-验证-测试重采样程序。

Result: 该模型在大多数语言上超过了单语基线模型，具备较强的处理未见词能力，并成功应用于73种语言的通用依存库。

Conclusion: 多语言联合训练的单一模型在词形变化任务上表现优秀，既提升了多语言的建模能力，也极大简化了模型部署和维护。同时公开了完整的代码，推动开源生态。

Abstract: We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.

</details>


### [119] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.CL

TL;DR: TopLoRA方法为LoRA带来了token级动态权重，无需增加额外rank，显著提升了适应性和效果，在多种任务和模型上优于现有LoRA方案。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA在所有输入tokens上使用相同的权重参数，无法有效捕捉每个token的语义差异，限制了其适应能力。

Method: 提出Token-wise Projected Low-Rank Adaptation（TopLoRA）方法，通过token相关的可学习对角矩阵动态调整LoRA权重，实现针对每个输入token的低秩投影，无需增加rank即可实现更细粒度的适应。

Result: 在多个模型和数据集上的实验结果显示，TopLoRA性能优于标准LoRA及其他变体。

Conclusion: TopLoRA能更有效地利用token语义特征，在参数高效的前提下实现个性化的低秩适应。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [120] [Corpus Frequencies in Morphological Inflection: Do They Matter?](https://arxiv.org/abs/2510.23131)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 论文提出在词形变化建模过程中引入词频信息，包括数据集划分、评测指标和训练采样，实验结果表明此方法在多数语言上获得更好效果。


<details>
  <summary>Details</summary>
Motivation: 传统的词形变化任务忽略了词频的分布，但实际应用中，用户输入通常呈现自然语言的真实词频分布。研究者希望更贴近真实应用场景，提升模型泛化和实用性。

Method: 从三个方面引入词频信息：1）结合lemma不重叠（lemma-disjoint）和基于词频的训练/测试划分方法；2）在评估指标上，除了常规的类别准确率，再用按词频加权的token准确率；3）训练数据采样时，提出基于频率的数据采样方法（frequency-aware training）。

Result: 在43种语言中，频率感知训练在26种语言上优于传统的均匀采样方法。

Conclusion: 引入词频信息到词形变化建模流程中，可以更好地反映实际文本中的表现，且频率感知训练在大多数语言上提升了模型表现。

Abstract: The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.

</details>


### [121] [ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix](https://arxiv.org/abs/2510.23160)
*Zile Yang,Ling Li,Na Di,Jinlong Pang,Yao Zhou,Hao Cheng,Bo Han,Jiaheng Wei*

Main category: cs.CL

TL;DR: ENTP方法利用神经符号方法“净化并再创造”低质量指令数据，不仅让这些数据变有价值，还取得了比用全部原始数据更好的微调效果，说明低质量数据通过适当处理可以释放巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的高质量优先范式在微调大模型时常常忽略了被丢弃的低质量数据中的潜在有用信息，并且质量过滤的效果不够理想。

Method: 提出了ENTP（Neural-symbolic Text Purge-Mix）框架，其分为符号(规则)模块和神经网络模块。符号模块利用统计先验筛除噪声数据；神经模块则基于潜在表示和模型知识合成更丰富的指令-响应对，从而提升数据的信息量与多样性。

Result: 在五个基准测试上，使用ENTP纯由低质量数据构建的数据集，超越了13种主流数据筛选方法，并且甚至优于直接使用原始全量数据进行微调。

Conclusion: ENTP框架能高效激活低质量数据的潜能，证明智能化数据净化与合成在大模型指令微调中具有重要作用。

Abstract: Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

</details>


### [122] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei,Shengyi Zong,Zhaoyan Li,Ziren Zhou,Hao Liu*

Main category: cs.CL

TL;DR: 本文提出通过“创意生成+格式转换”两阶段方法，并用混合数据合成训练，显著提升了大模型生成高质量剧本的能力，效果超过主流基线模型，并接近人类编剧水平。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在生成剧本时，虽然具备创造性写作潜力，但端到端方法难以兼顾创意叙事与格式规范，导致生成内容结构和故事深度不足，难以专业应用。

Method: 提出Dual-Stage Refinement（DSR）框架，将创意叙事生成与格式转换分离，分为两阶段。第一阶段根据简要大纲生成富有细节的小说体叙述，第二阶段将小说体叙述转化为专业剧本格式。为解决训练数据匮乏，采用混合数据合成，包括反向合成（将现有剧本解析为结构化输入）和正向合成（根据结构化输入生成小说体叙述）。

Result: 通过专业编剧的盲测，DSR框架与主流模型（如Gemini-2.5-Pro）对比，胜率达75%，且表现达到人类编剧水平的82.7%。该方法有效提升了LLM在剧本复杂创作领域的专业性。

Conclusion: 采用分阶段生成架构并结合专门的数据合成方法，能够显著提升LLM在复杂创意领域的表现，支持更专业的剧本创作。

Abstract: The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [123] [SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations](https://arxiv.org/abs/2510.23182)
*Shuai Huang,Wenxuan Zhao,Jun Gao*

Main category: cs.CL

TL;DR: 本文提出了SI-Bench，这是一个用于评估大语言模型（LLMs）社交智能的全新基准，基于真实的多轮人类对话数据，而非模拟对话。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs广泛作为自主体参与与人类的复杂社交交互，但现有的社交智能评测多以模拟数据为主，缺乏对真实对话的反映。因此，迫切需要更真实、更具挑战性的基准来评估模型的社交智能。

Method: 研究团队从社交网络应用收集了2,221个真实多轮对话，并从中筛选出312个对话，对8个主流大模型进行了人工标注与评测。实验对比了不同模型的社交智能表现，并评估了CoT推理的影响。

Result: SOTA模型在复杂社交推理中的表现优于人类专家，但在实际回复质量上仍不及人类。链式思考推理在社交对话中反而可能降低模型绩效。

Conclusion: 最先进的模型在复杂社交情境下的过程推理能力已超越人类专家，但在回复质量上仍然落后于人类。此外，链式思考推理（CoT）可能会降低模型的实际对话表现。

Abstract: As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

</details>


### [124] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata,Hossein Rahmani,Parinaz Soltanzadeh,Amirhossein Derakhshan,Behrouz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 提出了DREAM方法，结合关系抽取和大模型验证，提升了药物关系抽取效果，并发现医学文本中的挑战和模糊性。


<details>
  <summary>Details</summary>
Motivation: 目前药物-药物关系抽取的数据集稀缺，限制了机器学习在该领域的应用。

Method: 提出DREAM方法，先用关系抽取模型发现实体间关系，再用该模型在医学文本中构建药物关系本体，最后用大型语言模型验证提取出的关系。

Result: LLM对PubMed摘要子集提取的关系中有71%表示同意，定性分析表明该方法能发现医学领域的模糊之处。

Conclusion: DREAM方法能够低成本地从大规模医学文本中抽取药物关系，并得到大型语言模型的有效验证，显示了可行性和对医学关系抽取挑战的洞察。

Abstract: Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [125] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas,Maya Varma,Jean-Benoit Delbrouck,Curtis P. Langlotz*

Main category: cs.CL

TL;DR: 该论文提出面向临床LVLM自动生成放射报告的句子级奖励模型（PRM），有效提高生成报告的事实准确性和质量，并具备良好泛化能力和模型无关性。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉-语言模型（LVLMs）自动生成放射学报告的应用日益广泛，模型产生关键性幻觉可能带来严重临床风险。现有幻觉检测方法通常无法提供句子级的细粒度分析，且泛化能力有限。

Method: 提出了一种新颖的句子级过程奖励模型（Process Reward Model, PRM），能够预测每个生成句子的事实准确性，并结合临床上下文及前序文本。PRM在MIMIC-CXR数据集上以弱监督标签进行微调，且仅有0.5B参数。

Result: 微调后的PRM在对LVLM输出进行验证时，相较于强大的白盒基线，Matthews相关系数提升7.5%，AUROC提升1.8%。PRM不依赖于模型内部状态，能良好泛化到未见过的LVLM。PRM能够有效过滤低质量报告（如丢弃最差10%报告后F1-CheXbert提升4.5%），并且在最佳加权选择过程中，F1-CheXbert提升7.4%，BERTScore提升0.6%。

Conclusion: 提出的PRM是一种轻量、上下文感知的模型，可以为临床LVLM提供模型无关且无需访问内部激活的安全保障层，有效提升报告的事实正确性与质量。

Abstract: Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [126] [Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?](https://arxiv.org/abs/2510.23252)
*Tawsif Tashwar Dipto,Azmol Hossain,Rubayet Sabbir Faruque,Md. Rezuwan Hassan,Kanij Fatema,Tanmoy Shome,Ruwad Naswan,Md. Foriduzzaman Zihad,Mohaymen Ul Anam,Nazia Tasnim,Hasan Mahmud,Md Kamrul Hasan,Md. Mehedi Hasan Shawon,Farig Sadeque,Tahsin Reasat*

Main category: cs.CL

TL;DR: 本研究提出并公开了Bengali方言语音识别语料库Ben-10，发现基础语音模型难以应对方言差异，专门训练方言模型效果更佳，资源可供低资源语音识别领域使用。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别研究主要依赖标准语言形式，对于区域方言语音识别通常简单作为细调任务处理，尚缺乏对方言差异影响的深入探讨。该研究旨在探讨方言变化对语音识别系统的影响，尤其是在低资源条件下。

Method: 构建了一个包含78小时注释语音的Bengali语音转文本（STT）语料库Ben-10，并从语言学和数据驱动的角度分析基础语音模型在处理区域方言语音识别方面表现。

Result: 无论是零样本还是细调设置，基础语音模型在区域方言处理上表现不佳。所有深度学习方法在方言差异下建模都存在困难，但专门针对方言训练的模型可以缓解这一问题。

Conclusion: 方言变化严重影响语音识别性能，区域方言专有模型训练可有效提升表现。Ben-10语料库也可作为受限资源下ASR模型的分布外测试资源，相关数据和代码已公开发布。

Abstract: Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

</details>


### [127] [Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding](https://arxiv.org/abs/2510.23271)
*Mohammed Aljafari,Ismail Alturki,Ahmed Mori,Yehya Kadumi*

Main category: cs.CL

TL;DR: Mubeen模型通过大规模阿拉伯语原生语料训练及创新架构，突破传统阿语大模型的文化适应性和效用缺口难题，极大增强用户体验和知识服务能力。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语大模型普遍依赖英译数据，导致在意图识别和信息生成方面存在准确性和文化适应性不足。此外，传统模型容易出现“效用缺口危机”，即即使答案准确但未能真正解决用户核心需求，用户不得不多次提问。为了解决这些问题，开发了Mubeen模型，以实现更深层次的阿拉伯语理解和用户体验优化。

Method: Mubeen通过独有的阿拉伯语OCR引擎数字化大量历史手稿，并以真实阿拉伯语大规模语料（包括语言学、伊斯兰学、法学等权威文献及学术论文）进行训练。采用深度语言工程框架，并引入多学科专家模块。此外，创新性地提出了解决效用缺口危机的Practical Closure架构，实现更具指导性的应答能力。

Result: Mubeen模型能够精准理解阿拉伯语文本与口语，覆盖经典、当代及地区方言，不仅能够较其他模型提供更具文化相关性的解答，还能准确识别用户需求并提供明确指导，显著提升用户体验，助力文化传承和知识普及。

Conclusion: Mubeen作为专为阿拉伯语深度理解优化的大模型，在阿拉伯语的意图识别、文化适应性和实际应用效果方面实现显著提升，为实现沙特2030愿景和阿拉伯世界的数字化提供坚实支撑。

Abstract: Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

</details>


### [128] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao,Lingjie Jiang,Shaohan Huang,Tengchao Lv,Yupan Huang,Xun Wu,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 本文针对大语言模型代码美学生成不足，提出美学数据集与多智能体联合优化方案，在多个测试中显著提升代码美学表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽擅长代码生成及修复，但在视觉相关的代码美学表现上仍有不足，亟需提升自动生成代码的美感和视觉质量。

Method: 构建AesCode-358K美学指令微调数据集，提出多智能体反馈机制（agentic reward feedback），开发GRPO-AR算法对功能与美学进行联合优化，设计OpenDesign基准测试并开展实验。

Result: 在OpenDesign与现有基准（如PandasPlotBench）测试中，经过AesCode-358K微调及多智能体增强学习后，代码美学和表现均显著提升。AesCoder-4B在多个评测中超越主流闭源大模型，并与极大规模开源模型相当。

Conclusion: 提出的方法显著提升了大模型生成代码的美学表现，AesCoder-4B超越了GPT-4o和GPT-4.1，并与更大规模开源模型性能相当，验证了方案有效性。

Abstract: Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


### [129] [A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results](https://arxiv.org/abs/2510.23276)
*Thai-Binh Nguyen,Katerina Zmolikova,Pingchuan Ma,Ngoc Quan Pham,Christian Fuegen,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文提出了多模态上下文感知识别任务，结合音频、视觉和语境三类信息，有效提升了重叠会话环境下的语音识别和分组准确率，基线对比结果显示视觉信息对性能提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决多人同时交谈环境下的语音识别难题，即“鸡尾酒会问题”。在实际场景中，常出现多人的自然对话，导致语音极度重叠且交谈轮换频繁，传统听觉信号处理受限，因而需要多模态信息（如视觉、语境等）的辅助。

Method: MCoRec采用音频、视觉和上下文信息三者结合的方法，通过联合分析音视频记录，实现对每位说话者的转录，并将其聚类入各自的对话组。论文还介绍了数据收集流程和基线系统设定。

Result: 结果显示，单独依靠音频的方法其词错误率超过100%，而引入视觉线索后，识别性能提升了50%，充分体现了多模态信息在复杂环境下辨识的巨大优势。

Conclusion: 多模态方法在处理多人语音极度重叠场景下的语音分离和识别中具有显著效果，对提升实际多方对话场景自动识别能力具有重要意义。

Abstract: We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [130] [Enumeration of Tree-like Multigraphs with a Given Number of Vertices, Self-loops and Multiple Edges](https://arxiv.org/abs/2510.22302)
*Naveed Ahmed Azam,Seemab Hayat*

Main category: cs.DM

TL;DR: 本文提出了用于高效枚举含自环和多重边的树状多重图的动态规划方法，解决了传统计数方法的对称性和复杂度问题，具有广泛理论和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 在组合枚举、化学图论、高分子科学和网络建模等领域，枚举包含自环和多重边的树状多重图具有重要应用意义。然而，传统方法（例如Polya定理和分支算法）在处理对称性和计算复杂度时存在局限。

Method: 提出了一种统一的动态规划框架，通过规范化有根表示和子图的递归分解来枚举这类树状多重图。该方法能够消除冗余结构，无需显式生成所有结构即可实现精确计数。同时，框架还给出了关于此类多重图增长行为的解析上下界与递推关系。

Result: 该框架可以统一、高效地枚举定点数、自环和多重边均已指定的树状多重图，并提供了精确计数结果和理论递推关系。结果还扩展了以往分别处理自环和多重边的模型。

Conclusion: 本文为包含自环和多重边的复杂树状多重图的枚举问题提供了统一理论基础与高效算法，适用于数学和化学领域。

Abstract: Counting non-isomorphic tree-like multigraphs that include self-loops and
multiple edges is an important problem in combinatorial enumeration, with
applications in chemical graph theory, polymer science, and network modeling.
Traditional counting techniques, such as Polya's theorem and branching
algorithms, often face limitations due to symmetry handling and computational
complexity. This study presents a unified dynamic programming framework for
enumerating tree-like graphs characterized by a fixed number of vertices,
self-loops, and multiple edges. The proposed method utilizes canonical rooted
representations and recursive decomposition of subgraphs to eliminate redundant
configurations, ensuring exact counting without the need for explicit structure
generation. The framework also provides analytical bounds and recurrence
relations that describe the growth behaviour of such multigraphs. This work
extends previous models that treated self-loops and multiple edges separately,
offering a general theoretical foundation for the enumeration of complex
tree-like multigraphs in both mathematical and chemical domains.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [131] [Synthesis of State-Attack Strategies for Anonymity and Opacity Violation in Discrete Event Systems](https://arxiv.org/abs/2510.22657)
*Xiaoyan Li,Christoforos N. Hadjicostis*

Main category: cs.FL

TL;DR: 本文分析了自动系统离散事件模型在有限次数状态攻击下的隐私泄露机理，提出了验证与攻击策略方法，并证明攻击者可必然突破系统隐私。


<details>
  <summary>Details</summary>
Motivation: 自动化系统面临攻击（如感测器读数篡改和执行器命令修改），导致安全与隐私风险。本文关注有限状态自动机建模的离散事件系统在当前状态信息被攻击下的隐私保护及其脆弱性。

Method: 研究系统在有限次数攻击下，对当前状态匿名性和保密性的验证问题，分析攻击者如何根据观测和攻击结果确定系统状态，并提出攻击者可制定确保隐私泄露的攻击策略。还给出了相关验证算法的复杂度分析，并通过案例说明方法。

Result: 提出了可以验证系统在有限攻击下是否会发生匿名性或保密性泄露的方法，设计了保证系统隐私一定被攻击者突破的攻击策略，并分析了相关算法的计算复杂度。方法通过具体例子得到验证。

Conclusion: 在离散事件系统中，有限次数状态攻击可导致当前状态匿名性和保密性泄露。文中提出了相关验证和攻击策略方法，并能保证攻击者策略必定导致隐私泄露，且方法复杂度可控。

Abstract: Attacks, including the manipulation of sensor readings and the modification
of actuator commands, pose a significant challenge to the security and privacy
of automated systems. This paper considers discrete event systems that can be
modeled with nondeterministic finite state automata that are susceptible to
state attacks. A state attack allows an intruder to learn whether or not the
current state of a system falls into certain subsets of states. The intruder
has a limited total number of state attacks at its disposal, but can launch
state attacks at arbitrary instants of its choosing. We are interested on
violations of current-state anonymity (resp. opacity), i.e., situations where
the intruder, based on the sequence of observations generated by the system and
the outcome of any performed state attacks, can ascertain the exact current
state of the system (resp. that the current state of the system definitely
resides in a subset of secret states). When the system violates current-state
anonymity (resp. opacity) under a bounded number of state attacks, a subsequent
question is whether the intruder can design an attack strategy such that
anonymity-violating (resp. opacity-violating) situations will always be
reached. In this latter case, we also design an attack strategy that guarantees
that the system will reach a violating situation regardless of system actions.
We provide pertinent complexity analysis of the corresponding verification
algorithms and examples to illustrate the proposed methods.

</details>
