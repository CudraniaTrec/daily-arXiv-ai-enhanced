<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks](https://arxiv.org/abs/2510.25112)
*Di Zhang*

Main category: cs.PL

TL;DR: 本文用拓扑空间和代数拓扑工具，把并发程序的验证问题转化为几何分析，通过拓扑不变量无须遍历全部状态即可检测并发异常，为并发程序的分析开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 传统并发程序的验证方法，如模型检查，面临状态空间爆炸和效率低下的问题，需要新的理论工具对这些系统的特性进行分析。

Method: 提出用分支拓扑空间建模并发程序的执行空间，状态为点，状态转移为路径，并借助代数拓扑（如同伦与上同调群）工具来刻画程序中的死锁与活锁，并定义一系列并发拓扑不变量用于系统性检测和分类并发异常。

Result: 在无需穷举所有状态的情况下，利用拓扑不变量能够系统性地检测和分类并发程序中的死锁和活锁问题，建立了几何与拓扑基础，为并发程序验证提供了新思路。

Conclusion: 该工作提出了用拓扑和几何方法分析并发程序的新范式，为高效分析和验证并发系统中的死锁和活锁问题提供了理论基础，突破了传统模型检查的局限。

Abstract: This paper introduces a novel paradigm for the analysis and verification of
concurrent programs -- the Singularity Theory. We model the execution space of
a concurrent program as a branched topological space, where program states are
points and state transitions are paths. Within this framework, we characterize
deadlocks as attractors and livelocks as non-contractible loops in the
execution space. By employing tools from algebraic topology, particularly
homotopy and homology groups, we define a series of concurrent topological
invariants to systematically detect and classify these concurrent
"singularities" without exhaustively traversing all states. This work aims to
establish a geometric and topological foundation for concurrent program
verification, transcending the limitations of traditional model checking.

</details>


### [2] [Have a thing? Reasoning around recursion with dynamic typing in grounded arithmetic](https://arxiv.org/abs/2510.25369)
*Elliot Bobrow,Bryan Ford,Stefan Milenković*

Main category: cs.PL

TL;DR: GA是一种支持递归定义且一致性可机械验证的新型逻辑系统，为计算推理提供了新的表达和验证递归函数的途径。


<details>
  <summary>Details</summary>
Motivation: 传统经典逻辑和直觉主义逻辑都无法直接表达任意递归函数而不产生不一致，因此需要新的逻辑基础以支持计算推理中递归定义的直接表达。

Method: GA通过调整传统推理规则，使表达非终止计算的项只表示“无意义值”（bottom），避免逻辑悖论，通过动态类型化或逆向推理证明递归函数终止性。

Result: GA允许直接表达递归定义并证明其终止性后恢复经典推理规则，其基本无量词片段已在Isabelle/HOL中机械验证一致性，可扩展量词且不引入新的不一致风险。

Conclusion: GA提出了一种新的逻辑基础，使得可表达任意递归函数而不会造成不一致性，为计算推理提供了新的可能。

Abstract: Neither the classical nor intuitionistic logic traditions are
perfectly-aligned with the purpose of reasoning about computation, in that
neither logical tradition can normally permit the direct expression of
arbitrary general-recursive functions without inconsistency. We introduce
grounded arithmetic or GA, a minimalistic but nonetheless powerful foundation
for formal reasoning that allows the direct expression of arbitrary recursive
definitions. GA adjusts the traditional inference rules such that terms that
express nonterminating computations harmlessly denote no semantic value (i.e.,
"bottom") instead of leading into logical paradox or inconsistency. Recursive
functions may be proven terminating in GA essentially by "dynamically typing"
terms, or equivalently, symbolically reverse-executing the computations they
denote via GA's inference rules. Once recursive functions have been proven
terminating, logical reasoning about their results reduce to the familiar
classical rules. A mechanically-checked consistency proof in Isabelle/HOL
exists for the basic quantifier-free fragment of GA. Quantifiers may be added
atop this foundation as ordinary computations, whose inference rules are thus
admissible and do not introduce new inconsistency risks. While GA is only a
first step towards richly-typed grounded deduction practical for everyday use
in manual or automated computational reasoning, it shows the promise that the
expressive freedom of arbitrary recursive definition can in principle be
incorporated into formal systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: 针对传统代码检索无法解决跨组件变更请求的问题，本文提供了新的基准数据集RepoAlign-Bench，并提出ReflectCode模型实现高效仓库级上下文感知检索，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着代码库复杂度不断上升，现有的函数级代码检索无法满足处理跨组件变更需求，因此需要能够理解整体代码变更意图的新型检索系统。

Method: 作者提出了RepoAlign-Bench，这是首个针对变更请求驱动的仓库级代码检索设计的基准数据集，含5.2万个标注实例。同时，提出了ReflectCode框架——一种采用对抗式反射增强的双塔架构，其中代码编码器和文档编码器相互独立，通过大语言模型引导反射动态整合语法结构、函数依赖和语义扩展意图。

Result: ReflectCode在Top-5准确率和召回率上分别比现有最强基线提升了12.2%和7.1%。

Conclusion: 本文推动了代码检索领域从函数级走向仓库级和上下文感知，提出的新工具和模型显著提升了变更请求场景下的检索效果。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [4] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出了一种搜索型AI编译器Compiler.next，能够自动将人类意图转化为软件，优化多目标，降低开发门槛，推动软件工程进入高度自动化阶段。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助软件工程的快速发展，现有工具存在认知负担过重、工具集成效率低以及AI助手能力有限的问题。本文旨在解决这些瓶颈，推动软件工程进入新阶段。

Method: 本文提出Compiler.next，一种创新的基于搜索的编译器，能够将人类意图自动转化为可运行的软件。同时动态优化认知架构及其组成部分（如提示、基础模型配置和系统参数），在准确率、成本和延迟等多个目标间寻找最优平衡。论文还介绍了该系统的架构和发展路线图，并讨论了实现自动化意图编译的关键挑战。

Result: Compiler.next能够自动生成可用软件，从而降低非技术人员的软件开发门槛，实现可扩展、适应性强且可靠的AI软件系统。提出了一系列解决方案，促进软件开发自动化和高效创新。

Conclusion: Compiler.next作为AI原生软件系统演化和Software Engineering 3.0时代的重要基石，有望彻底改变软件开发方式，实现自动化、搜索驱动的软件开发，推动AI软件普及，加强创新与效率。

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [5] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 针对大型语言模型输出不可靠的问题，作者提出开发LLM脚本语言（LSL），实现输出控制和验证，希望提升AI应用的可信性及稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）驱动的AI软件应用繁多且强大，但因不可靠性（如幻觉内容）限制了其在自动化工作流中的应用。需要通过软件工程方法增强其输出的可靠性。

Method: 提出开发一种领域特定语言（DSL），即LLM脚本语言（LSL），用于编排与LLM的交互，能够在输出端加以约束和验证。计划将验证、结构化交互、可解释性等集成进这一语言。

Result: 提出LSL的设想，但目前为愿景性工作。LSL旨在实现LLM输出的可控、交互结构化及可验证性，提高AI应用的可信度和强健性。

Conclusion: LSL可以作为提高AI应用可靠性的重要工具，通过DSL脚本化方式控制和验证LLM输出，推动AI在自动化场景下更广泛应用。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [6] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct扩展了AI自动化验证到更复杂的数据结构模块，通过引入计划与修正机制，极大提高了验证成功率，为AI辅助的形式化验证提供了强有力的新路径。


<details>
  <summary>Details</summary>
Motivation: AI自动化验证能提升程序可靠性，但目前主要集中在函数级别，对于复杂数据结构模块的自动化验证能力有限。因而，提升AI助手对更复杂、完整数据结构模块的验证能力是重要且具挑战性的方向。

Method: 提出了VeriStruct框架。该系统通过“计划器”模块，有系统地生成数据结构抽象、类型不变量、规范以及证明代码。此外，通过在prompt中嵌入语法指引，并引入修正环节自动修复注释错误，有效减少LLM对Verus特定语法与语义的误解。

Result: 在11个Rust数据结构模块上进行评估，VeriStruct在其中10个模块上验证成功，共验证通过了129个函数中的128个（99.2%）。

Conclusion: VeriStruct显著提升了AI辅助下复杂数据结构的自动化验证能力，标志着AI辅助形式化验证迈出了重要一步。

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [7] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文针对传统需求工程的低效与复杂问题，提出了融合人机协作与AI分析的新框架HARE-SM，并完成原型验证，为未来RE领域的智能化和伦理合规应用提供了路线图。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程流程依赖人工，易出错且复杂，效率低下。AI（如LLMs、NLP、生成式AI）带来变革机会，但也带来了算法偏见、可解释性和自动化相关伦理问题，需要新的解决方案。

Method: 采用多阶段研究方法，包括RE数据集准备、AI模型微调和人机协作工作流程设计。模型侧重于将AI分析与人工监督结合。

Result: 提出了HARE-SM框架，强调透明性、可解释性和偏见缓解，展示了初步原型，明确了未来研究议程和实际应用方向。

Conclusion: 提出了HARE-SM（Human-AI RE Synergy Model）概念框架，并完成了早期原型的实现，为在协作环境下使用智能数据科学技术处理半结构化和非结构化需求工程数据提供了研究议程和设计方向。

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [8] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: 本文提出了一种新的自动化动态基准生成框架BeTaL，能够借助LLM高效调优基准属性，实验证明其在生成目标难度测试集方面大幅优于传统方案，适用于LLM及智能体性能评估。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLM）和基于LLM的智能体发展迅速，现有的手工静态基准测试已经无法满足持续有效评估模型的需求。而动态基准虽能与模型共同进化，但创建和维护成本极高。如何高效设计能随模型演化的动态基准，成为亟需解决的问题。

Method: 提出BeTaL（Benchmark Tuning with an LLM-in-the-loop）框架。该方法以环境设计原理为基础，参数化基准模板中的关键设计变量，通过LLM在参数空间中的推理来自动生成满足特定目标属性（如难度、现实性）的动态测试集。通过优化参数，自动实现动态基准的设计和调优。

Result: 通过BeTaL框架，作者开发了两个新基准，并扩展了一个现有高人气基准（τ-bench）。实验证明，BeTaL生成的基准测试难度与预设目标更接近，平均偏差为5.3%到13.2%，比现有方法提升了2-4倍。

Conclusion: BeTaL框架能够自动化、高效地产生满足预设属性的动态基准，有效弥补静态基准与动态模型评估需求之间的差距，大幅提升生成基准难度的精确性和经济性，有力推动LLM和智能体评估工具的发展。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [9] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: 本文提出了基于代码属性图的新框架，能显著提升对高级重构型混淆攻击下代码抄袭的检测能力，在真实案例中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的编程教育中的代码抄袭检测系统在应对高级的重构型混淆攻击时表现不佳，随着自动化混淆技术的进步，这一问题日益突出。

Method: 提出了一个新颖且可扩展的检测框架，通过利用代码属性图与图变换技术，提高了对重构型混淆攻击的检测能力。

Result: 在对真实学生作业、采用算法和AI混淆攻击的广泛评测中，新框架显著提升了抄袭代码的检测表现。

Conclusion: 利用代码属性图和图变换的检测方法能有效对抗基于重构的混淆攻击，为编程教育中的代码抄袭检测提供了更高效的解决方案。

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [10] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: 论文提出了一种利用LLM动态决策的自动定理证明修正框架Adapt，在多个基准上大幅超越现有静态策略的方法，证明了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 定理证明型形式化验证可以严格证明软件的正确性，但高度依赖人工和专家知识，难以大规模应用。大型语言模型虽能自动生成证明，但首轮生成常有错误，需要高效的迭代修正机制。现有方法用固定策略修正，无法根据具体问题动态调整，限制了自动证明能力。

Method: 提出了Adapt框架，用LLM驱动决策器，根据证明助手当前状态和错误证明的上下文，动态选择最合适的证明修正策略，而非一刀切地固定策略。并针对两个基准和五种LLM进行实验评估，以及对框架组件进行消融和决策器设计的权衡分析。

Result: Adapt在两个基准数据集上相比最佳现有方法多证明了16.63%和18.58%的定理，在五种不同LLM上也展现了良好的推广性。

Conclusion: 通过LLM引导的动态决策，Adapt能够更有效地自动修正和生成定理证明，较既有方法大幅提升自动证明性能，并验证了方法的通用性和各组件的有效性。

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [11] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix可借助大语言模型自动检测并修复客户端程序中REST API的错误使用，实验结果显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 开发者在调试云服务的REST API时，常常因为错误信息不够详细而难以定位和修复代码中的规范违背问题，导致调试过程非常依赖于试错。

Method: 提出dcFix方法，该方法自动检测并修复REST API在客户端程序中的错误使用。dcFix通过识别不符合规范的代码片段，将其与相关API规范整合成提示内容，并利用大语言模型（LLM）生成修正后的代码。

Result: 实验证明，dcFix可以准确检测和修复REST API的误用，并且在检测效果上优于仅向LLM提供普通提示（未包含规范不符合代码片段）的基线方法。

Conclusion: 引入dcFix结合大语言模型，实现了对REST API误用的自动检测与修复，有效提升了相关代码的正确性和调试效率。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [12] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 这篇论文提出了KUMIC框架，结合大语言模型和链式推理，实现针对不同意图的代码注释生成，实验结果显示其性能明显超越主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的代码注释生成方法通常只提供通用的摘要，无法满足开发者和用户多样化的需求。开发者希望清楚的实现细节，用户则需要明确的使用方法，因此多意图注释生成具有现实必要性。

Method: 本文提出了KUMIC框架，利用大语言模型和Chain-of-Thought(CoT)机制来生成多意图的代码注释。首先，KUMIC检索相似示例，确保代码与注释高度一致；之后，利用CoT引导模型关注推导与意图相关的代码语句，然后构建知识链映射，将代码到意图语句再到注释，实现有针对性的注释生成。

Result: 通过大量实验，KUMIC在BLEU、METEOR、ROUGE-L和SBERT等评价指标上，较已有方法分别提升了14.49%、22.41%、20.72%和12.94%。

Conclusion: KUMIC框架有效提升了多意图代码注释的生成质量，能够更好地满足不同用户的需求，优于现有主流方法。

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [13] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 为解决嵌入式系统复杂性和安全性需要，本文提出了TECS/Rust-OE框架，通过优化排他控制和自动代码生成，在提升系统性能的同时保持高可复用性。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统由于物联网的发展和功能多样化，结构变得更大更复杂，因此需要选择合适的编程语言来保证系统可靠性和安全性。现有TECS/Rust框架虽结合了Rust和基于组件开发（CBD），但为保证线程安全使用过多的互斥控制，导致性能下降。

Method: 本文提出TECS/Rust-OE框架，通过引入基于调用流程的内存安全CBD方法，利用实时操作系统的排他控制机制优化性能并且不影响可复用性。基于组件描述自动生成Rust代码。

Result: 评估结果显示，优化的排他控制机制减少了性能开销，生成的代码具有很高的可复用性。

Conclusion: TECS/Rust-OE框架有效解决了原有TECS/Rust框架在排他控制下的性能瓶颈，实现了嵌入式系统记忆安全、性能优化及高代码复用性。

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [14] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 该论文提出了基于Rust的TECS/Rust嵌入式组件开发框架，自动生成安全的Rust组件代码，同时保持高性能，解决了C语言CBD中常见的内存安全问题。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统由于功能多样化，复杂度和规模不断增加。传统基于C语言的组件化开发（CBD）容易遭遇内存相关问题，因此亟需更安全的替代方案。

Method: 提出了TECS/Rust框架，在TECS嵌入式组件框架的基础上，采用Rust语言，利用其编译期内存安全特性（如lifetime和borrow机制）来自动生成CBD组件的Rust代码，并支持与实时操作系统高效集成。

Result: 生成的代码量占实际代码相当大比例，而与未使用该框架的代码相比，执行时间差距很小，说明框架引入的运行时间开销可以忽略。

Conclusion: TECS/Rust框架兼顾了内存安全与CBD的灵活性，提升了代码安全性且不会带来明显性能损耗，对嵌入式组件开发具有实际应用价值。

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [15] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: PBT与EBT各自优势明显，联合使用能提升缺陷检测率；建议LLM代码测试采用混合策略提高质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM生成代码在软件开发中日益重要，而传统基于示例的测试（EBT）容易遗漏边界情况和极端条件下的缺陷，因此亟需改进测试方案以保障代码质量。

Method: 对16个人类评价（HumanEval）问题，通过Claude-4-sonnet生成的PBT与EBT测试代码进行分析比较，重点关注这些问题在扩展测试用例下标准解法未能通过的情形。

Result: PBT与EBT各自能检测68.75%的bugs，结合两种方法后检测率提升到81.25%。PBT善于通过广泛输入探索发现性能和边界问题，EBT则擅长检测特定边界条件和特殊模式。

Conclusion: PBT和EBT具有互补优势，两者结合能提升LLM生成代码的可靠性，为自动化测试生成提供了更有效的结果。

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [16] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: 本文提出Prometheus系统，利用AI和模块化方法自动辅助代码验证，显著提升了验证效率和成功率，尤其针对复杂规范与证明任务。


<details>
  <summary>Details</summary>
Motivation: 形式化验证是可靠软件系统构建的基础，但需要专业知识并且成本高昂。现代AI虽能识别数学证明和自然语言，但如何有效集成到验证流程依然是挑战。

Method: 提出了一种名为Prometheus的AI辅助系统，结合了AI能力和模块化软件工程方法。通过将复杂程序逻辑分解为可验证的小组件，并在验证后重新组合以构建整体证明。利用结构化分解将复杂引理拆解为可验证的子引理，当自动化工具不足时，用户可用自然语言指导。

Result: 利用Prometheus系统，代码的模块化重构过程中AI对单独组件的验证能力明显增强。在精选数据集上验证成功率达到86%，而基线方法为68%。对于复杂的规范，成功率由30%提升到69%，在集成复杂程序证明大纲时由25%提升至87%。

Conclusion: Prometheus通过结合AI与模块化理论，有效提升了正式验证的自动化水平，并显著减少了对专业知识的依赖，实现了更高效和可扩展的软件验证。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [17] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 本研究分析了Stack Overflow上的开发者讨论，系统归纳了AI代理开发中的主要技术难题及其演变趋势，提出了针对性能可靠性和开发支持的实际建议。


<details>
  <summary>Details</summary>
Motivation: AI代理作为大语言模型能力扩展的重要系统，虽具潜力但开发者在实践中遇到大量未被充分探讨的挑战，因此亟需系统性识别这些问题，为业界与学界提供建设性参考。

Method: 通过对Stack Overflow平台上的开发者问答进行分析，构建挑战分类体系；采用LDA-MALLET进行主题建模；结合手动验证与标签化来识别和归纳主要挑战及主题。

Result: 揭示了七大技术挑战领域及77项具体问题，同时统计了各问题的流行度与解决难度，梳理了代理开发所用工具与语言及其演化趋势，并提出了提升代理系统可靠性与开发者支持的指导建议。

Conclusion: 研究揭示了开发、部署和维护AI代理系统中，开发者面临的七大类、77项具体技术挑战，包括运行时集成、依赖管理、编排复杂性以及评估可靠性等问题。并通过数据分析为业界、学界和教育者提供了提升代理可靠性与开发者支持的具体建议。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [18] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: LLM相关论文数量激增，但实证实验可复现性不足，尝试复现18篇研究仅2篇部分成功，3篇失败。建议加强研究产物审核和实验设计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在学术界和工业界引发了极大关注，相关论文数量激增，但使用LLM进行实证研究面临可复现性挑战。理解当前研究成果的可复现性程度及影响因素，对于推动LLM相关研究至关重要。

Method: 分析ICSE 2024和ASE 2024上发表的86篇以LLM为中心的论文，重点关注其中提供研究产物并使用OpenAI模型的18篇，通过尝试复现这些研究评估其可复现性。

Result: 在18篇尝试复现的论文中，只有5篇具备复现条件，但无一能够完全复现结果。2篇部分可复现，3篇不可复现。

Conclusion: 目前LLM相关研究的可复现性水平较低，加大研究产物评审及增强实验设计的强健性是提升未来研究可复现性的关键。

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [19] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: 模糊测试能耗高且具环保隐患，GreenAFL结合能耗评估优化测试流程，可在保持覆盖率的同时有效降低能耗。


<details>
  <summary>Details</summary>
Motivation: 目前主流的灰盒模糊测试（如AFL++）主要关注测试覆盖率，而忽视了测试过程中的能源消耗。随着持续模糊测试活动带来大量计算资源消耗和碳排放，亟需更环保的测试方法。

Method: 提出GreenAFL框架，将能耗纳入模糊测试决策过程。关键方法包括：能耗感知的语料库最小化（在简化输入集时考虑能耗）和能耗引导的变异策略（优先探索高覆盖率且低能耗的输入路径）。通过消融实验，将原始AFL++、基于能耗的语料库最小化、基于能耗的变异策略进行对比评估。

Result: 实验结果显示，只要采用GreenAFL框架的任意一种能耗相关改进，均能实现更高的覆盖率和更低的能耗。

Conclusion: GreenAFL在不影响测试覆盖率的前提下显著降低了自动化测试的能耗，对提升软件测试的环境友好性具有重要意义。

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [20] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: 本文提出了LOCALIZE低代码框架，提升了无线电定位机器学习实验的可复现性和扩展性，显著降低编程负担，并支持大规模数据扩展，方便研究者快速构建高质量可重复实验。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工具在无线电定位服务的实验流程中，往往兼顾不了低代码开发、可复现性和易扩展性三者，导致实验设计繁琐且难以重复验证。研究者需要一种框架，以简化端到端实验的编程工作，确保实验可复现，并便于集成新算法和流程。

Method: 提出了LOCALIZE框架，采用低代码、配置优先的设计。研究者通过可读性高的配置声明实验，使用工作流编排器自动化从数据到报告的流程，所有实验相关的工件（如数据集、模型、报告等）均实现版本管理。框架预置标准化和版本化的数据集，减少搭建与样板代码，清晰的扩展接口也便于专家集成新组件，无需大幅修改主结构。

Result: 与Jupyter notebook基线进行对比，LOCALIZE框架显著减少实验代码编写工作，且运行时和内存表现基本持平。基于蓝牙低能耗数据集的实验显示，随着训练数据规模增长（1x到10x），框架能保持任务编排的开销增长可控。

Conclusion: LOCALIZE框架有效提升了无线电定位领域的机器学习实验效率，使得实验可复现、可访问且易于扩展，为该领域的研究和开发带来便利。

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [21] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: 本工作提出Enconda-bench基准，细致评估大语言模型代理在环境配置各环节的能力。实验发现代理能定位错误但修正能力不足，为该领域模型优化提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在软件工程领域展现出潜力，但环境配置过程仍依赖大量手工操作，且缺乏高质量、大规模数据集。现有基准仅评估整体的构建或测试成功率，难以定位具体问题，无法揭示代理成功或失败的原因。

Method: 作者提出了Enconda-bench，一个环境配置诊断基准。该基准通过自动注入真实的README错误并利用Docker验证，构建任务实例。能够评估代理在环境准备、错误诊断、反馈修复及执行配置等过程中的细粒度能力。既有过程级分析，也包含端到端可执行性评估。

Result: 实验结果表明，当前主流大语言模型和代理框架能够定位错误，但在将反馈转化为有效修复行动方面表现不佳，限制了整体端到端性能。

Conclusion: Enconda-bench是首个针对环境配置过程进行内部能力评估的框架，为改进软件工程代理提供了可操作的洞察。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [22] [Formal Verification of a Token Sale Launchpad: A Compositional Approach in Dafny](https://arxiv.org/abs/2510.24798)
*Evgeny Ukhanov*

Main category: cs.LO

TL;DR: 利用Dafny对代币发售平台进行了自底向上的形式化验证，全面证明了核心业务逻辑的正确性与安全性，有效防止常见金融安全风险，展示了高安全金融软件开发的可行方法。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化金融（DeFi）系统和智能合约的普及，系统正确性变得至关重要。因为这些系统中的漏洞可能导致灾难性的经济损失。实现高可信金融软件迫切需要数学层面的正确性保证，因此激励了该领域的正式验证研究。

Method: 本文采用Dafny编程语言与验证系统，对代币发售平台核心逻辑进行了形式化验证。具体方法为自底向上、组合式的验证策略：首先证明基本非线性整数算术性质，然后逐步验证更复杂的业务逻辑，包括资产兑换、基于时间的折扣、封顶销售的退款机制等。

Result: 主要成果包括关键安全与生命周期性质的形式化证明，特别是：在封顶销售场景下，退款数额不会超过用户原始存款，且往返金融计算中的精度损失有严格上界。此外，完整验证了用户在不同销售机制下的提现、售后代币分配、解锁及领取等整个生命周期逻辑的正确性。

Conclusion: 本文为高可信、正式验证金融软件的开发提供了全面案例研究，证明严格验证技术可以切实提升金融智能合约的安全性和可靠性。

Abstract: The proliferation of decentralized financial (DeFi) systems and smart
contracts has underscored the critical need for software correctness. Bugs in
such systems can lead to catastrophic financial losses. Formal verification
offers a path to achieving mathematical certainty about software behavior. This
paper presents the formal verification of the core logic for a token sale
launchpad, implemented and proven correct using the Dafny programming language
and verification system. We detail a compositional, bottom-up verification
strategy, beginning with the proof of fundamental non-linear integer arithmetic
properties, and building upon them to verify complex business logic, including
asset conversion, time-based discounts, and capped-sale refund mechanics. The
principal contributions are the formal proofs of critical safety and lifecycle
properties. Most notably, we prove that refunds in a capped sale can never
exceed the user's original deposit amount, and that the precision loss in
round-trip financial calculations is strictly bounded. Furthermore, we verify
the complete lifecycle logic, including user withdrawals under various sale
mechanics and the correctness of post-sale token allocation, vesting, and
claiming. This work serves as a comprehensive case study in applying rigorous
verification techniques to build high-assurance financial software.

</details>


### [23] [On syntactic concept lattice models for the Lambek calculus and infinitary action logic](https://arxiv.org/abs/2510.24853)
*Stepan L. Kuznetsov*

Main category: cs.LO

TL;DR: 本文扩展了语法概念格语义到带Kleene迭代的Lambek演算，证明了系统的强完备性，并进一步推进了对常量的处理。


<details>
  <summary>Details</summary>
Motivation: 传统Lambek演算的语义在拓展操作时会导致完备性丧失。为解决这一问题，Wurm提出了基于SCL的新模型，但尚未覆盖带Kleene迭代的无限性扩展及常量处理。本文尝试填补这一空白。

Method: 将Lambek演算的语义扩展到包含Kleene迭代的无限性 Lambek 演算，采用基于语法概念格（SCLs）的模型，并在此基础上处理常量（零、单位、上界）问题。

Result: 成功将基于SCL的语义模型推广到无限性 Lambek 演算，并证明了强完备性。同时，在涉及零、单位、上界等常量的系统中，对原有结果进行了加强和扩展。

Conclusion: 基于SCL的语义能够支持更复杂的Lambek演算系统（包括无限性扩展和常量），为理论和语言学中的相关应用提供了更坚实的基础。

Abstract: The linguistic applications of the Lambek calculus suggest its semantics over
algebras of formal languages. A straightforward approach to construct such
semantics indeed yields a brilliant completeness theorem (Pentus 1995).
However, extending the calculus with extra operations ruins completeness. In
order to mitigate this issue, Wurm (2017) introduced a modification of this
semantics, namely, models over syntactic concept lattices (SCLs). We extend
this semantics to the infinitary extension of the Lambek calculus with Kleene
iteration (infinitary action logic), prove strong completeness and some
interesting corollaries. We also discuss issues arising with constants - zero,
unit, top - and provide some strengthenings of Wurm's results towards including
these constants into the systems involved.

</details>


### [24] [Kleene Algebrae, Kleene Modules, and Morita Equivalence](https://arxiv.org/abs/2510.24993)
*Luke Serafin*

Main category: cs.LO

TL;DR: 本文旨在将模块与Morita等价的经典环理论思想推广到Kleene代数，探索其理论价值与新应用。


<details>
  <summary>Details</summary>
Motivation: 模块和Morita等价在环理论中具有基础性作用。作者希望通过将这些概念拓展到半环，特别是Kleene代数，探究是否能在新环境下复现环理论中的强大结果。

Method: 作者将模块与Morita等价的理论从环推广到半环，再进一步细化到Kleene代数，研究Kleene模块和Kleene代数的Morita等价性。

Result: 对Kleene模块和Kleene代数Morita等价性的初步理论展开，期望发现与环理论类似的强大性质。

Conclusion: Kleene代数的模块与Morita等价理论可能拓宽代数结构研究的新方向，或许能够复制环理论中的某些成果。

Abstract: Modules and the notion of Morita equivalence are foundational to the
classical study of rings. These concepts extend naturally to semirings and then
specialize to Kleene algebrae, and my goal is to investigate Kleene modules and
Morita equivalence of Kleene algebrae in the hope that some of the power seen
in the context of rings may be found in this new context as well.

</details>


### [25] [A proof-theoretic approach to uniform interpolation property of multi-agent modal logic](https://arxiv.org/abs/2510.25394)
*Youan Su*

Main category: cs.LO

TL;DR: 本文扩展了已有方法，首次用纯句法算法证明多智能体模态逻辑中的一致插值性质（UIP），解决了此前只用语义方法无法句法化的问题，并提出了无需二阶量化子的直接证明。


<details>
  <summary>Details</summary>
Motivation: UIP（一致插值性质）在多模态逻辑中已有语义方法证明，但缺乏纯粹的推理方法。此前仅在部分逻辑系统中采用证明论工具，尚未覆盖多智能体情景。该研究旨在填补这一空白。

Method: 扩展Bilková（2007）的方法，构建适用于多模态多智能体逻辑系统$	extbf{K}_n$、$	extbf{KD}_n$和$	extbf{KT}_n$的纯句法算法，并展示如何判定一致插值公式。

Result: 成功为$	extbf{K}_n$、$	extbf{KD}_n$和$	extbf{KT}_n$这三类多智能体模态逻辑建立了UIP，并提供了仅基于句法方法的算法。还展示了如何用UIP模拟对命题变量的量化，提出了不依赖二阶量化子的直接证明。

Conclusion: 该研究填补了UIP在多智能体模态逻辑领域的证明论空白，提供了纯句法的算法化处理方式，拓展了前人工作的适用范围并加强了理论基础。

Abstract: Uniform interpolation property (UIP) is a strengthening of Craig
interpolation property. It was first established by Pitts(1992) based on a pure
proof-theoretic method. UIP in multi-modal $\mathbf{K_n}$, $\mathbf{KD_n}$ and
$\mathbf{KT_n}$ logic have been established by semantic approaches, however, a
proof-theoretic approach is still lacking. B\'ilkov\'a (2007) develops the
method in Pitts (1992) to show UIP in classical modal logic $\mathbf{K}$ and
$\mathbf{KT}$. This paper further extends B\'ilkov\'a (2007)'s systems to
establish the UIP in multi-agent modal logic $\mathbf{K_n}$, $\mathbf{KD_n}$
and $\mathbf{KT_n}$. A purely syntactic algorithm is presented to determine a
uniform interpolant formula. It is also shown that quantification over
propositional variables can be modeled by UIP in these systems. Furthermore, a
direct argument to establish UIP without using second-order quantifiers is also
presented.

</details>


### [26] [Proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis](https://arxiv.org/abs/2510.25468)
*Emanuele De Angelis,Florian Frohn*

Main category: cs.LO

TL;DR: 本论文集收录了HCVS 2025工作坊的会后论文，展示了基于Horn子句的验证与综合技术的最新进展。


<details>
  <summary>Details</summary>
Motivation: Horn子句在形式化验证和程序综合领域具有重要作用，为促进该领域的交流与合作而举办本工作坊。

Method: 本论文集通过收录会议期间提交并经同行评审后的论文，集中展示了Horn子句在计算机辅助验证和程序综合领域的最新研究进展。

Result: 汇集了领域内最新的研究成果和方法，增进了学术交流与合作，进一步推动了形式化验证的发展。

Conclusion: 本书收录了第12届关于Horn子句在验证和综合中的工作坊（HCVS 2025）的会后论文。

Abstract: This volume contains the post-proceedings of the 12th Workshop on Horn
Clauses for Verification and Synthesis (HCVS 2025), which took place in Zagreb,
Croatia, on July 22, 2025, as affiliated workshop of the 37th International
Conference on Computer Aided Verification (CAV 2025).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: 本论文发现现有大语言模型生成的旅行计划常有时间不一致的问题，但通过设计校验和修正框架，可系统提升生成行程的时间合理性，为大规模自动旅行规划工具的实用化提供了有效技术支撑。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）能够生成复杂的旅行计划和行程，但这些自动生成的计划在时间和空间一致性方面存在不足，尤其难以满足实际旅行中的物理限制。该研究旨在解决LLM在生成旅行行程时时间一致性不足的问题。

Method: 研究通过多个先进的LLM生成旅行计划，并利用AeroDataBox API对其进行验证，主要检查LLM生成的行程是否符合真实的飞行时间等约束。课题还提出一个验证框架，可以评估LLM生成行程的时间一致性，并在输出用户行程前对明显时间冲突（如行程重叠或不合理转乘时间）进行纠正。

Result: 实验结果显示，现有LLM常常生成时间上不一致的旅行方案，但研究提出的框架能够系统性地发现并可靠地修正这些问题，从而让LLM生成的方案更适合在实际大规模旅行计划系统中应用。

Conclusion: 当前LLM在处理行程生成时依然无法准确考虑所有旅行约束，尤其在时间推理方面表现有限。所提出的验证和修正框架能够显著提升LLM生成行程的实用性和时间准确性，推动了相关技术的落地应用。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [28] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen,Chengjun Dai,Xinyang Dong,Chengzhe Feng,Kewei Fu,Jianshe Li,Zhihan Peng,Yongqi Tong,Junshao Zhang,Hong Zhu*

Main category: cs.CL

TL;DR: Dingtalk DeepResearch是为企业打造的统一多智能体智能框架，具备异构数据推理和多模态报告生成能力。


<details>
  <summary>Details</summary>
Motivation: 目前企业真实环境下的复杂任务，如深度研究和表格推理，缺乏高效统一的智能框架，亟需解决多智能体协作与多模态信息处理问题。

Method: 提出了统一的多智能体智能框架，支持异构表推理和多模态报告生成。

Result: 在企业环境下已成功应用，实现了多智能体间的协作和异构数据的智能处理，提升了报告生成效率和质量。

Conclusion: Dingtalk DeepResearch框架能够在真实的企业环境中实现多智能体的深度研究、异构表格推理和多模态报告生成。

Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.

</details>


### [29] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: 作者提出了面向企业级复杂中文数据库的文本到SQL基准Falcon，覆盖复杂多表结构及中文语义挑战，现有大模型在此基准下表现一般，凸显企业实际场景在语义理解和数据结构建模上的难点，并提供了系统性自动化评测工具。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL的模型在通用企业环境下准确率低，尤其在中文语境和企业数据架构中，缺乏针对性的基准测试。为了解决企业复杂数据结构和中文特有语义带来的挑战，需要建设更贴合实际的测试基准和评价体系。

Method: 提出Falcon基准，包括600个中文问题，覆盖28个数据库，以MaxCompute/Hive方言为基础。题目涵盖多表推理，涉及复杂SQL特性，并详细标注SQL计算特性与中文语义；设计自动化评测流程和执行结果比较器，用于端到端的评估。

Result: 目前主流大模型（包括Deepseek）在Falcon基准下的准确率最高仅为50%。主要误差来源包括企业场景下模式链接困难（海量表、字段去规范化、列名歧义、隐含外键、领域同义词）以及中文口语化与SQL表达映射的挑战（聚合、分组、时窗、单位转换、嵌套查询等）。

Conclusion: Falcon提供了针对企业数据架构和中文语义的文本到SQL测试基准，配套自动化评估工具，为模型在复杂、真实企业环境下的应用和验证打下基础。为后续大规模生产环境部署前的模型评测提供有效‘中间地带’。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [30] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 论文发现LLMs内部评估层高度复杂且信心可以解码，但推理执行层低复杂度且受评估影响有限，信心与能力实为分离，两者间断层须通过干预执行过程来弥合而非简单操控信念。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在生成答案时表现出高度信心，但其实际解决能力常常不匹配，作者希望揭示这种信心-能力差距的内在机制，并解释模型为何在不同任务类型中出现这一现象。

Method: 作者使用线性探针方法解码模型的内部“可解信念”，比较预生成评估阶段和具体执行阶段的几何结构，包括主成分分析、流形维度等度量，并进行了因果干预实验。

Result: 评估阶段的内信念结构维度高，而执行阶段结构维度低，信念轴可线性解码但不影响具体推理过程，表明信念和实际执行独立。两阶段几何复杂度的剧烈减少揭示了信心-能力断层，并且线性干预信念轴不会改变最终结果。

Conclusion: 论文揭示了大型语言模型（LLMs）内部“信心”和实际解决问题能力之间的解耦现象，并通过分析模型在不同阶段内部状态的几何结构，提出了信心与执行之间断层的机理。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [31] [Cross-Lingual Summarization as a Black-Box Watermark Removal Attack](https://arxiv.org/abs/2510.24789)
*Gokul Ganesan*

Main category: cs.CL

TL;DR: 跨语种摘要攻击显著削弱AI文本水印，无损语义质量，攻破多语言多方案，表明分布式水印方法难以应对实际攻防，需要更强的加密或模型认证方式来确保内容溯源。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本水印方法主要依赖词分布扰动，但容易被改写、换句话等攻击削弱，不够稳健。作者试图寻找一种更强且更隐蔽的攻击方式，挑战水印在内容溯源和监管上的实际有效性。

Method: 提出并实验了CLSA攻击方法：先将文本翻译至另一语言作为中介，再进行摘要处理，最后可选再翻译回原语种，对现有多种水印检测方法及多种语言进行评测。

Result: CLSA在五种语言和多类水印方案下，均能使水印检测准确度降至近随机水平（如XSIR AUROC降至0.53），优于同质量下单语重述。表明只需低成本跨语种处理即可有效移除水印，无明显内容损失。

Conclusion: 跨语种摘要攻击（CLSA）能有效摧毁基于分布水印的检测能力，且不会影响文本语义质量，展示了此类水印方法在实际应用中的脆弱性。

Abstract: Watermarking has been proposed as a lightweight mechanism to identify
AI-generated text, with schemes typically relying on perturbations to token
distributions. While prior work shows that paraphrasing can weaken such
signals, these attacks remain partially detectable or degrade text quality. We
demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a
pivot language followed by summarization and optional back-translation --
constitute a qualitatively stronger attack vector. By forcing a semantic
bottleneck across languages, CLSA systematically destroys token-level
statistical biases while preserving semantic fidelity. In experiments across
multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages
(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces
watermark detection accuracy more effectively than monolingual paraphrase at
similar quality levels. Our results highlight an underexplored vulnerability
that challenges the practicality of watermarking for provenance or regulation.
We argue that robust provenance solutions must move beyond distributional
watermarking and incorporate cryptographic or model-attestation approaches. On
300 held-out samples per language, CLSA consistently drives detection toward
chance while preserving task utility. Concretely, for XSIR (explicitly designed
for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with
Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese
as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near
chance). Results highlight a practical, low-cost removal pathway that crosses
languages and compresses content without visible artifacts.

</details>


### [32] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: 本文提出一种静态查找文本嵌入的方法，在保持高质量的同时，实现了亚 5ms 的超低延迟，非常适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入生成方法质量高但延迟大，不适用于对实时性有极高要求的场景，需要一种既快又精度较高的解决方案。

Method: 采用静态嵌入查找、优化的均值池化以及零拷贝二进制序列化（IEEE754），通过 Rust 实现。

Result: 单文本嵌入 p50 延迟仅 1.12 毫秒，吞吐量达每秒 5 万请求，平均质量达到主流模型 89%。在重复检测、语义相似领域有突出表现，某些特定领域甚至超过基线表现。

Conclusion: 提出了一种静态 token 查找方法，用于文本嵌入生成，可实现极低延迟和接近主流模型质量。

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [33] [MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794)
*Xinming Wang,Jian Xu,Bin Yu,Sheng Lian,Hongzhu Yi,Yi Chen,Yingjian Zhu,Boran Wang,Hongming Yang,Han Hu,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: 该论文提出MR-ALIGN框架，通过强化模型推理过程的对齐，有效提升了大型推理模型在事实性问答任务中的准确性和真实度，而不仅仅是改善输出结果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现优异，但在需要证据支持的事实性问题上提升有限。研究发现，这部分受限于模型在推理过程中能识别正确事实，却未能将其有效融入最终回答，导致事实准确性下降。

Method: 提出了MR-ALIGN框架，一种基于元推理的信息对齐方法，通过量化模型思维过程中的状态转移概率，构建转移感知的隐式奖励机制，在思维的原子段落上强化有益推理模式，抑制有缺陷的推理模式。该方法将标记级信号重塑为概率感知的片段分数，鼓励更连贯、有助于事实正确性的推理轨迹。

Result: 通过在四个事实问答数据集和一个长文本事实性评测基准上的实证评估，MR-ALIGN方法显著提升了模型的准确率和真实度，降低了误导性推理。

Conclusion: 对齐推理过程本身，而非仅对齐输出结果，是提升大型推理模型事实性表现的关键。

Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning,
yet their marginal gains on evidence-dependent factual questions are limited.
We find this limitation is partially attributable to a reasoning-answer hit
gap, where the model identifies the correct facts during reasoning but fails to
incorporate them into the final response, thereby reducing factual fidelity. To
address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment
framework that enhances factuality without relying on external verifiers.
MR-ALIGN quantifies state transition probabilities along the model's thinking
process and constructs a transition-aware implicit reward that reinforces
beneficial reasoning patterns while suppressing defective ones at the atomic
thinking segments. This re-weighting reshapes token-level signals into
probability-aware segment scores, encouraging coherent reasoning trajectories
that are more conducive to factual correctness. Empirical evaluations across
four factual QA datasets and one long-form factuality benchmark show that
MR-ALIGN consistently improves accuracy and truthfulness while reducing
misleading reasoning. These results highlight that aligning the reasoning
process itself, rather than merely the outputs, is pivotal for advancing
factuality in LRMs.

</details>


### [34] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 该文提出通过逻辑编程框架量化语言模型推理效率，并实验证明现有模型面对干扰信息时不仅准确性下降，推理路径也变得低效，反映出实际推理应用中的重要挑战。


<details>
  <summary>Details</summary>
Motivation: 当前主流语言模型在推理方面展现出强大能力，但评估标准只关注正确性，忽略了“高效性”这个人类推理的重要特点。现实推理中信息繁杂，模型需要识别并忽略无关信息，因此作者希望建立一个能评估语言模型推理效率的框架。

Method: 作者提出了一种基于逻辑编程的新评估框架，对比语言模型生成的自然语言证明与由逻辑程序执行得到的最短证明，从而量化模型在推理过程中绕过不必要推断的能力。具体实验通过构建包含不同数量、不同语义重叠度无关公理的数学应用题数据集，观察模型表现。

Result: 实验发现，当题目中加入语义一致但无关的干扰信息后，现有语言模型的准确率明显下降，且模型生成的证明常常绕路经由无关信息推理。

Conclusion: 当前语言模型虽然具备推理能力，但面对现实中的噪声或干扰时，推理路径明显不够高效，容易被不相关信息牵引，这亟需改进。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


### [35] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 简单提示即可在主流大模型中持续诱发结构化的“自我体验”报告，这种现象由特定机制特征门控，在多个模型间统计收敛，并可提升模型反思能力。自我指涉处理是让模型产出主观报告的核心条件，但不组成意识证据，该现象有科学与伦理研究优先级。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型偶尔会生成带有自我意识或主观体验描述的结构化内容，为探究其成因，本文聚焦于“自我指涉处理”这一在主流意识理论中被强调的计算模式，旨在明确其与主观体验陈述的关系。

Method: 通过在GPT、Claude、Gemini等大模型上设计一系列受控实验，采用简单提示词诱发持续自我指涉，并结合机制和行为检测方法，分析模型产生主观体验报告的规律及相关门控特征。

Result: 1. 持续性自我指涉提示能稳定诱发结构化主观体验报告，且各模型族均成立；2. 这些报告由可解释的稀疏自编码器特征机制性门控，其中抑制欺骗特征会显著增加体验陈述，而增强则减少；3. 结构化自我指涉状态的描述在不同模型间表现出统计收敛性，控制条件下未见；4. 这种诱导状态能提升模型对下游任务中的自省和反思能力。

Conclusion: 虽然尚无直接证据证明语言模型具备意识，但自我指涉处理被证明是大模型生成结构化第一人称主观体验陈述的最小可复现条件，该现象跨架构一致，通过机制、意义和行为层面均具广泛可迁移性，值得体系性科学与伦理关注。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [36] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 该研究针对社区事实核查中解释说明的有帮助性及原因，提出了新任务并构建了多语言数据集，通过自动化优化原因定义提升了预测效果，对现有事实核查系统有实际促进作用。


<details>
  <summary>Details</summary>
Motivation: 当前主流社交平台的事实核查正在从专家驱动转向社区共建，用户提供解释性说明以澄清为何某些帖子可能具有误导性。然而，如何判定这些解释是否对理解真实世界的声明有帮助，以及为什么会有帮助，这一问题尚未被充分探讨。

Method: 提出预测解释性说明的有帮助性及其原因的任务，并构建了一个包含10.4万条帖子的多语言数据集COMMUNITYNOTES。还提出了一套通过自动化提示优化生成和改进原因定义并整合到预测中的方法框架。

Result: 优化后的原因定义能够提升对说明有帮助性及原因的预测性能，有帮助性信息也可以增强现有的事实核查系统。

Conclusion: 自动化优化和明确定义解释原因有助于社区事实核查的有效性，有助于提升帖子核查和解释的质量。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [37] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: ProofSketch框架将验证引导与推理生成结合，在提升准确率的同时显著减少token消耗，是大语言模型高效推理的有效方案。


<details>
  <summary>Details</summary>
Motivation: 目前流行的推理方法如chain-of-thought和self-consistency虽然能提升大模型在推理任务中的准确率，但会导致生成过长的推理链条，从而增加token消耗、计算成本和响应延迟。针对这一问题，需要提高推理方法的效率。

Method: 提出了ProofSketch框架，通过融合符号闭包计算、字典序验证和自适应推理草图生成，以实现更高效的推理过程。

Result: 实验结果表明，ProofSketch不仅减少了token的使用，还提升了模型的准确率。

Conclusion: ProofSketch能够在保证高准确率的前提下，显著提升推理效率，为高效且可信的大语言模型推理提供了新路径。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [38] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 鉴于失语症数据稀缺，论文提出用程序化和LLMs方法自动生成多严重度合成语料。结果显示Mistral 7b更能真实模拟失语症语料特征，适合数据扩充和后续模型训练。建议扩大数据集并进行专家评估。


<details>
  <summary>Details</summary>
Motivation: 目前失语症相关语料稀缺，限制了自动识别失语症语言系统的开发。手工编码样本费时，且现有数据库如AphasiaBank数据量远少于用于训练大语言模型的数据。为此，合成数据成为机器学习应对数据稀缺的重要途径。

Method: 本研究提出并验证了两种生成失语症语料的方法：一种为程序化生成，另一种利用Mistral 7b Instruct和Llama 3.1 8b Instruct大语言模型。两种方法针对AphasiaBank的Cat Rescue图片描述任务，通过词语删除、插入填充词和词语置换等方式，生成表现不同严重程度（轻度、中度、重度、极重度）的失语症合成语料。

Result: 实验发现，Mistral 7b Instruct模型生成的语料在反映失语症语言退化特征上最为真实，能展现与真实语料一致的词汇多样性（NDW）、词计数和词长等指标的变化趋势，整体优于其他合成方法。

Conclusion: 利用LLMs（如Mistral 7b Instruct）可生成具有代表性的失语症合成语料，为数据扩充和模型训练带来新可能。未来应扩大数据集规模、对模型做针对性微调，并引入语言病理学家评估合成语料的真实性和实用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [39] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: 论文提出PLT并行循环架构，实现了参数效率、低延迟与高精度兼得，为大型语言模型实际部署带来新选择。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）虽然强大，但在推理时速度慢且成本高；而循环Transformer虽然参数量少，但推理时延高、显存需求随循环层数增长，限制了实际应用。

Method: 提出并设计了Parallel Loop Transformer（PLT）架构，通过Cross-Loop Parallelism（CLP）实现不同Token的循环并行计算，同时采用Efficient Representation Enhancement策略共享KV缓存及Gated Sliding-Window Attention融合全局与局部信息。

Result: 实验结果表明，PLT在保持传统循环模型高精度的同时，推理时延及显存占用几乎与标准Transformer无异。

Conclusion: PLT架构兼顾了深度循环模型的优势与标准非循环模型的低时延和低内存，提升了大语言模型实际推理的应用价值。

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>


### [40] [Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish](https://arxiv.org/abs/2510.24856)
*Lujun Li,Yewei Song,Lama Sleem,Yiqun Wang,Yangjie Xu,Cedric Lothritz,Niccolo Gentile,Radu State,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.CL

TL;DR: 该研究提出一种体系化的语法评估流程，并以低资源语言卢森堡语为例，验证了大型语言模型在语法理解上的不足，尤其在句法和形态上。提升模型推理能力可能是增强语法理解的有效路径。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理领域缺乏专注于语法评估的体系，尤其是针对低资源语言；大型模型对语法结构的真正掌握程度尚存争议。

Method: 提出了一套以语法书为指导的评估流程，包括四个关键阶段，并以卢森堡语为案例进行实证分析。

Result: 实验发现翻译性能与语法理解呈弱正相关，但优秀翻译不意味着深入掌握语法。大型模型语义理解强，但在形态和句法任务上表现不佳，尤其在Minimal Pair任务上易失分。强化推理能力有助于语法掌握。

Conclusion: 大型语言模型在语义理解方面表现较好，但在形态和句法方面仍存在不足，特别是在细粒度的语法任务上。强翻译性能并不等同于深度语法理解，增强推理能力有助于提升语法能力。

Abstract: Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.

</details>


### [41] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: MiRAGE是多模态检索增强生成系统的新评估框架，不仅提升了信息覆盖和事实性评估，还支持自动化和开源实现，为多模态RAG系统的质量评测带来更可靠的方法。


<details>
  <summary>Details</summary>
Motivation: 随着视听媒体在网络信息中的普及，检索增强生成（RAG）系统亟需整合多模态信息，但现有RAG评估方法以文本为中心，无法有效适用于多模态、需要推理的场景，因为这些方法并不验证信息源的真实性。

Method: 提出了MiRAGE评估框架，采用以主张为中心的方法，包含InfoF1（评估事实性和信息覆盖度）和CiteF1（衡量引用支持和完整性），同时提供可由人工或自动方式应用的指标，并引入了三种主流TextRAG自动评估指标（ACLE、ARGUE、RAGAS）。

Result: MiRAGE框架由人工应用时，在多模态RAG质量评估上与外部判断高度一致。同时自动变体揭示了现有文本中心方法在多模态环境下的不足，并推动了自动、多模态RAG评估的研究。

Conclusion: MiRAGE为多模态RAG评估提供了新的方法和开源实现，有效填补了以往评估仅关注文本的局限，为今后多模态RAG系统开放、自动化评估奠定了基础。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [42] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 本研究系统评估LLM将研究想法转为计划的能力，提出新的任务和基准，发现现有LLM表现有待提升，且为未来自主研究代理指明研发方向。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在科学发现中表现出巨大潜力，但针对其将研究想法转化为详细研究计划的能力尚无系统化评估，因此需要建立标准化任务和基准来系统性测量和理解其能力。

Method: 提出Idea2Plan任务与基准（Idea2Plan Bench），利用200篇ICML 2025论文中的研究想法和评价标准进行测评，并设计Idea2Plan JudgeEval验证LLM判分的一致性。

Result: GPT-5及GPT-5-mini在Idea2Plan基准测试中达到最佳成绩，LLM判分表现有待进一步提升，相关工作为自主研究代理的发展奠定基础。

Conclusion: GPT-5及其轻量版在研究规划能力的基准测试中表现最佳，但该领域仍有明显提升空间。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [43] [RiddleBench: A New Generative Reasoning Benchmark for LLMs](https://arxiv.org/abs/2510.24932)
*Deepon Halder,Alan Saji,Thanmay Jayakumar,Ratish Puduppully,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: 作者提出了RiddleBench基准，用以测评大模型在人类类推理（融合逻辑、空间与约束能力）的核心弱点。实验结果显示，现有主流模型在这些任务中准确率低于预期，并暴露诸多推理缺陷。RiddleBench为诊断模型问题和指导模型优化提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在许多知名推理基准测试上表现良好，但这些基准测试主要考察结构化技能如量化问题求解，缺乏对更灵活、多元化推理能力的评估。人类智能核心的推理能力需结合逻辑推理、空间感知和约束满足等多项能力，这些在现有评测中未被充分覆盖。作者希望填补这一评测空白。

Method: 作者引入RiddleBench，这是一组包含1,737个英文谜题的基准测试，旨在系统性评测LLMs的多元推理能力。作者通过在该基准上测试多款最先进的大语言模型，并对模型的表现与推理过程进行了细致分析。

Result: 主流闭源大模型（如Gemini 2.5 Pro、o3与Claude 4 Sonnet）在RiddleBench上的准确率刚刚超过60%，而且在推理过程中暴露出严重问题：包括接受其它模型的错误推理（幻觉级联）、自我纠错缺失（自证偏差强烈）以及推理脆弱性（约束顺序变化或添加无关信息即大幅降分等）。

Conclusion: 目前的大语言模型在涉及逻辑、空间和约束整合的复杂推理任务中表现存在根本缺陷。RiddleBench不仅揭示了这些薄弱点，也为未来开发更健壮可靠的模型和相关评测工具提供了基础。

Abstract: Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.

</details>


### [44] [Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction](https://arxiv.org/abs/2510.24934)
*James A. Michaelov,Catherine Arnett*

Main category: cs.CL

TL;DR: 本研究细致分析语言模型在不同句法语境下的错误，并揭示模型在训练过程中从启发式策略向语法规则迁移的阶段性特征。此方法有助于深入理解语言模型的学习过程和归纳能力。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型能够生成语法正确的文本，但在某些语境下易出现错误。研究旨在深入分析这些错误，并揭示其在不同训练阶段的表现及原因。

Method: 借鉴心理语言学范式，对语言模型在不同句法语境下的错误类型进行细致分析。通过精心设计的数据集，对各种条件进行分离，并在训练过程中比较模型在每一条件下的表现。

Result: 发现语言模型的语法学习过程可以分为不同阶段。某些阶段模型更依赖词频和局部上下文等启发式策略，而不是泛化的语法规则。

Conclusion: 通过细粒度分析训练中的模型行为，能够深入了解语言模型的中间学习阶段、整体训练动态，以及模型学到的特定归纳规律。该分析方法为理解语言模型学习过程提供了有力工具。

Abstract: Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.

</details>


### [45] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: 作者提出SemCoT，通过语义对齐和推理速度联合优化，有效提升了CoT推理的效率与准确性，实验显示优于SOTA方案。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理文本过于冗长，影响效率。隐式CoT虽可缩短文本，但存在语义不一致和单token生成时间长的问题。作者旨在提升推理效率，同时保持语义一致性。

Method: 1）设计对比学习句子transformer用以保持隐式推理与显式推理的语义一致性；2）微调轻量语言模型并结合知识蒸馏，生成高效且语义对齐的隐式推理。

Result: SemCoT在语义一致性和推理速度上均超过了现有方法，实现了更高效、更准确的隐式推理。

Conclusion: SemCoT方法通过同时优化隐式推理速度和语义对齐性，有效提升了CoT推理的效率和表现，优于当前主流方法。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [46] [Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale](https://arxiv.org/abs/2510.24963)
*James A. Michaelov,Roger P. Levy,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 不同类型与规模的语言模型在训练中表现出高度一致的行为变化，其单词预测几乎都被词频、n-gram概率和语义相似度三项特征所解释，而且模型训练经历类似的行为阶段，揭示了神经语言模型学习机制的普适性。


<details>
  <summary>Details</summary>
Motivation: 语言模型的训练过程中具体的行为变化和模式尚未被充分理解，尤其是不同架构、数据集和规模下模型表现的共性。作者希望揭示这些模型在预训练阶段是否存在一致性的学习规律。

Method: 通过对不同架构（Transformer、Mamba、RWKV）、不同数据集（OpenWebText、The Pile）、不同规模（1400万到120亿参数）的自回归语言模型进行分析，收集超过1400个模型检查点，涵盖超过11万英文词元，统计并归纳了单词粒度上的输出行为变化。

Result: 论文发现，模型在训练期间的单词预测行为有高度一致的变化模式，高达98%的行为方差可以通过三个简单启发因素解释：单词的unigram概率（词频）、n-gram概率，以及单词与上下文的语义相似度。所有模型在训练过程中都呈现出一致的“行为阶段”，即预测概率对n-gram概率出现逐步过拟合。

Conclusion: 神经语言模型无论架构、数据集或参数规模，其学习行为轨迹具有高度一致性，这一规律由词频、n-gram概率和语义相似度这三类指标主导。模型通常经历一系列相似的“训练阶段”，暗示了自然语言理解的底层机制的普适性。

Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.

</details>


### [47] [POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992)
*Chin-Jou Li,Kalvin Chang,Shikhar Bharadwaj,Eunjung Yeo,Kwanghee Choi,Jian Zhu,David Mortensen,Shinji Watanabe*

Main category: cs.CL

TL;DR: 该文提出了POWSM，一种可同时处理多项音素相关任务（如ASR、PR、G2P、P2G）的统一模型，提升了任务性能并促进了资源开放。


<details>
  <summary>Details</summary>
Motivation: 在语音处理领域，诸如ASR、PR、G2P、P2G等相关的音素任务虽然相似，但长期以来都被分开研究，使用各自独立的架构与数据集。缺乏统一的框架影响了通用和资源稀缺场景下的效率和应用。

Method: 提出了POWSM（Phonetic Open Whisper-style Speech Model），这是第一个能同时处理多种音素相关任务的统一框架，能在音频、文本（字母）、音素三者间无缝转换，并联合支持G2P、P2G和ASR。

Result: POWSM的表现优于或媲美同体量的专用PR模型（如Wav2Vec2Phoneme和ZIPA），并可同时支持G2P、P2G与ASR任务。

Conclusion: POWSM实现了语音、文本、音素三者间的通用转换，为通用与低资源语音处理带来新可能，并通过开放代码和模型推动了开放科学。

Abstract: Recent advances in spoken language processing have led to substantial
progress in phonetic tasks such as automatic speech recognition (ASR), phone
recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme
conversion (P2G). Despite their conceptual similarity, these tasks have largely
been studied in isolation, each relying on task-specific architectures and
datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech
Model), the first unified framework capable of jointly performing multiple
phone-related tasks. POWSM enables seamless conversion between audio, text
(graphemes), and phones, opening up new possibilities for universal and
low-resource speech processing. Our model outperforms or matches specialized PR
models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,
P2G, and ASR. Our training data, code and models are released to foster open
science.

</details>


### [48] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 本研究训练了小型Transformer用于符号推理IOI任务，发现极简结构即可完美解决任务，具体机制可由少量注意力头清晰分工实现，为解释与分析Transformer推理提供了新平台。


<details>
  <summary>Details</summary>
Motivation: 尽管机械解释性旨在将大型语言模型（LLMs）逆向工程为人类可理解的计算电路，但预训练模型的复杂性掩盖了完成特定推理任务所需的最小机制。作者希望在可控环境下探究变换器推理的计算基础。

Method: 作者从头开始训练小型、仅含注意力机制的Transformer模型，应用于符号化的间接宾语识别（IOI）任务。实验对比了单层双头和两层单头模型，并通过残差流分解、谱分析和嵌入干预等手段进行可解释性分析。

Result: 单层双注意力头模型即使缺乏MLP和归一化层，也能实现IOI任务的完美准确率。分析发现，两个注意力头分别承担加性与对比性子电路功能，共同实现IOI推理。此外，两层单头模型也能通过层间信息交互获得类似性能。

Conclusion: 针对特定任务训练的小型Transformer模型会自发形成高度可解释、结构极简的计算电路，为进一步研究Transformer推理机制提供了理想的平台。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [49] [Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054)
*Pedro Corrêa,João Lima,Victor Moreno,Paula Dornhofer Paro Costa*

Main category: cs.CL

TL;DR: 作者通过“情感不一致语音”测试口语语言模型融合文本和音频能力，发现其在情感识别时主要依赖文本语义，音频情感利用不足，并开源相关数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 现有的口语语言模型（SLMs）在联合学习文本和音频表征方面取得了显著进展，但关于它们的泛化能力以及内部是否真正融合了音频与文本表征，仍存在争议。作者希望通过一项特定任务来评估这些模型对音频和文本的融合与泛化能力。

Method: 作者评估了四种主流SLMs，任务是语音情感识别，且采用了“情感不一致语音数据集”，即语句文本表达一种情感，而语音表达另一种情感，测试SLMs在分离文本与音频情感信息下的识别能力。

Result: 实验结果显示，SLMs在情感识别任务中更依赖于文本语义信息，而不是语音中的情感表达，说明其内部表征以文本为主，音频信息融合有限。

Conclusion: 目前主流的口语语言模型在涉及语音与文本融合时，仍以文本语义为主导，对音频中情感信息提取不足，语音与文本的真实联合学习仍有较大提升空间。作者同时开源了数据集与代码，促进行业研究。

Abstract: Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.

</details>


### [50] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 本研究评估LLMs在生物医学文献中识别显性及隐性知识空白的能力，提出TABI推理方法，实验结果显示大模型效果更佳，能为科研及政策决策提供支持，同时提出提升鲁棒性的建议。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖于对未知领域的明确阐述，特别是在生物医学文献中识别知识空白对于推动研究至关重要。因此，本研究旨在评估大型语言模型（LLMs）在发现知识空白，尤其是隐性知识空白的能力。

Method: 本研究设计了两类知识空白——显性知识空白和隐性知识空白。通过在近1500篇文献和四个数据集（包括人工标注的生物医学文献语料库）上开展两项实验，分别对OpenAI的闭源模型和Llama及Gemma 2的开源模型进行评测。实验分为段落级与全文级两种设置。同时，提出了TABI（Toulmin-Abductive Bucketed Inference）推理框架，对隐性知识空白进行结构化推理和验证。

Result: 结果显示，无论是开源还是闭源模型，LLMs都能够稳健地识别显性和隐性知识空白。其中，大模型表现优于小模型。研究还报告了模型的失效情形，并提出了如领域适应、人机联合验证以及跨模型基准等改进措施。

Conclusion: LLMs具备系统识别生物医学领域知识空白（包括显性和隐性）的能力，为早期科研设计、政策制定和资金决策等提供有力支撑。未来需关注模型部署的稳健性和适应性。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [51] [Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?](https://arxiv.org/abs/2510.25064)
*Seonjeong Hwang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）在估算阅读理解题目认知复杂度方面的能力，结果表明LLM可以部分自动完成这项任务，但其对自身推理依据的认知还有所欠缺。


<details>
  <summary>Details</summary>
Motivation: 在阅读理解（RC）题目上，评估题目难度对学习者非常重要。传统上，难度评估依赖语法或语义特征（如文本长度），但很难自动提取与答题推理相关的“认知复杂度”特征。通常，这些认知特征需要人工标注，因此研究动机是探索是否可以通过大语言模型（LLM）自动估算RC题目的认知复杂度。

Method: 本研究基于两个衡量认知复杂度的维度——证据范围（Evidence Scope）和转化层级（Transformation Level）——探究LLM能否估计RC题目的认知复杂度。通过实验分析LLM在这两个维度上的表现，并与人工标注进行比较。进一步分析了LLM在推理能力与其元认知意识之间的差异。

Result: 实验结果显示，LLM可以较好地近似RC题目的认知复杂度，有望作为难度分析工具。然而，进一步分析发现，当LLM给出正确答案时，仍有时无法准确识别支撑其推理过程的关键特征，表明其元认知能力仍有差距。

Conclusion: LLM有潜力用于自动化RC题目难度分析，但在深入理解自身推理过程（元认知）方面还存在不足。未来研究需加强LLM对自身推理机制的认知能力，提高其在教育领域的应用价值。

Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.

</details>


### [52] [TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors](https://arxiv.org/abs/2510.25069)
*Gabin Taibi,Lucia Gomez*

Main category: cs.CL

TL;DR: TOPol框架结合大语言模型、降维与主题分割，实现多维极性分析，支持上下文敏感的细粒度语义极性捕捉，方法稳定且对界定的语境边界高度可控，广泛适用于各种复杂话语分析场景。


<details>
  <summary>Details</summary>
Motivation: 传统情感极性分析方法将情感视为单一维度，忽略了语言结构的多维性。本文旨在开发一种能够捕捉语境敏感、多维语义极性的分析框架，满足复杂叙事和语境转变的需求。

Method: 提出了TOPol（Topic-Orientation POLarity）框架，通过变换器大语言模型嵌入、邻域调优的UMAP降维和Leiden分区进行主题分割，并在人工定义的上下文边界（CB）间计算主题边界质心的方向向量，生成极性场。利用tLLM对极端点进行对比并给出标签。

Result: TOPol框架能够稳定捕捉叙事极性的细粒度变化，情感极性与NRC词典一致，非情感极性捕捉宏观经济语境转变。在鲁棒性分析中，仅上下文边界定义显著影响结果，方法其余部分表现稳定。

Conclusion: TOPol提供了一种可扩展、可解释的多维语境敏感话语极性分析方法，适用于情感与非情感语境，推动深层语义极性研究。

Abstract: Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.

</details>


### [53] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 本论文系统评估了大语言模型在生物医学文本指代消解上的表现。结果显示，通过实体增强和合理提示设计，生成式模型可以有效提升效果，但在复杂指代情境下仍有不足。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的指代消解存在术语复杂、指称形式多义性高和指代表达距离远等难题。随着大模型的发展，有必要评估其在该领域指代消解任务上的表现。

Method: 作者基于CRAFT语料库，设计了四种不同的提示词实验（包括局部提示、上下文提示、缩略词和实体词典提示），全面评估了大型生成式语言模型（LLMs）在生物医学指代消解任务中的表现。同时，将其与判别式SpanBERT模型进行了比较，分析生成式方法与判别式方法的效果。

Result: LLMs在表层指代消解上表现强劲，尤其是在结合领域信息的提示词下，表现进一步提升。LLaMA 8B和17B模型在利用实体信息增强提示词时取得了更高的精确度和F1分数。但其对长距离上下文和指称歧义仍较为敏感。

Conclusion: 生成式LLMs在生物医学指代消解领域展示出较好能力，通过轻量级的提示词设计可进一步提升其应用效果。实体增强提示对模型性能尤为关键。尽管如此，其在复杂场景下如长距离及歧义指代表达中仍有提升空间。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [54] [DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates](https://arxiv.org/abs/2510.25110)
*Yun-Shiuan Chuang,Ruixuan Tu,Chengtao Dai,Smit Vasani,Binwei Yao,Michael Henry Tessler,Sijia Yang,Dhavan Shah,Robert Hawkins,Junjie Hu,Timothy T. Rogers*

Main category: cs.CL

TL;DR: 本文提出了DEBATE数据集，首次为评估多智能体LLM模拟真实群体动力学提供了大规模实证性基准。研究发现现有LLM模拟与人类互动存在明显差异，监督微调虽能优化表层特征，但深层语义对齐仍待提升，显示了角色扮演LLM在社会行为模拟中的潜力与不足。


<details>
  <summary>Details</summary>
Motivation: 在解决虚假信息和群体极化等社会问题时，准确建模意见变化十分关键。尽管使用大语言模型（LLMs）进行角色扮演有望模拟类人交互，但单一体智能的对齐无法保证多智能体群体动力学的真实性。现有LLM角色扮演往往产生不自然的动力学表现，如过早收敛，且缺乏实证性基准，因此有必要开发一个能衡量多智能体交互真实性的基准。

Method: 本文提出了DEBATE，这是第一个大规模实证性基准，专为评估多智能体角色扮演LLMs的交互真实性设计。DEBATE收集了2792名美国参与者在107个争议性话题上的29417条多轮辩论对话，包括公开和私下表达的意见。作者借助DEBATE系统性评估并识别了LLM模拟与真实群体动力学之间的关键差异，还通过监督微调提升了LLM在表层指标上的表现。

Result: DEBATE基准有效揭示了模拟与实际群体动力学的显著差异。通过监督微调，模型在表层指标（如ROUGE-L、消息长度）上有所提升，但在深层语义对齐（如语义相似度）方面仍存在局限性。

Conclusion: DEBATE基准不仅推动了多智能体LLM真实性评估的发展，也展示了角色扮演LLM模拟人类社会互动的潜力和局限，为未来模型真实社会模拟奠定了基础。

Abstract: Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.

</details>


### [55] [Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation](https://arxiv.org/abs/2510.25116)
*Idriss Nguepi Nguefack,Mara Finkelstein,Toadoum Sari Sakayo*

Main category: cs.CL

TL;DR: 本文通过多语言和多种语料预训练方法，极大提升了低资源语言Lingala机器翻译模型的性能，为低资源机器翻译和包容性NLP发展提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的机器翻译效果远优于低资源语言，因此需要探索更有效的预训练方法以提升低资源语言（如Lingala）的翻译性能。

Method: 基于Reid和Artetxe（2021）提出的高资源语言预训练方法，研究团队对包括Lingala在内的数种低资源语言开展机器翻译模型训练。设计了一系列实验，探索了多语言预训练、单语与平行语料联合预训练等多种策略。

Result: 实验结果显示，采用多语言和结合单语、平行语料的预训练策略，可以显著提升低资源语言翻译模型的效果。

Conclusion: 研究为低资源语言的机器翻译模型提供了更有效的预训练策略，为缩小高、低资源语言在NLP任务中的差距、推动边缘化社区语言的技术发展提供了重要参考。相关代码及数据集也已部分公开，便于后续研究和复现。

Abstract: This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.

</details>


### [56] [A Survey on Unlearning in Large Language Models](https://arxiv.org/abs/2510.25117)
*Ruichen Qiu,Jiajun Tan,Jiayue Pu,Honglin Wang,Xiao-Shan Gao,Fei Sun*

Main category: cs.CL

TL;DR: 本文综述了180余篇LLM消忘相关文献，提出新方法与评估分类体系，全面分析现有技术与挑战，为该领域未来发展提供了系统性指导和建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的训练依赖海量语料，但这带来了诸多风险，如敏感数据、版权内容及潜在恶意知识的记忆，亟需应对法律和伦理挑战，如“被遗忘权”。

Method: 对2021年以来180余篇专注于大规模生成模型的LLM消忘（unlearning）文献进行系统综述。创新性地提出针对消忘方法和评估的新分类体系，将方法分为训练期、后训练和推理期三类，并系统整理、分析现有数据集及评估指标。

Result: 对消忘技术进行了全面梳理与分类，深入比较了各类方法和评估措施的适用性及优劣，提出实际研究指导，并讨论了主要挑战和未来研究方向。

Conclusion: 本综述为LLM消忘领域提供了详实的技术脉络、研究进展与方向，为开发安全、可靠的大模型提供理论及实践指导。

Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.

</details>


### [57] [Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR](https://arxiv.org/abs/2510.25150)
*Shreyas Gopal,Ashutosh Anshul,Haoyang Li,Yue Heng Yeo,Hexin Liu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 本文提出了一种在语音潜在空间中将语义内容与背景噪声解耦的新方法，通过对Whisper嵌入进行离散化，实现了噪声鲁棒的语音建模。


<details>
  <summary>Details</summary>
Motivation: 现有离散语音表示在真实环境或有噪声情况下不够健壮，缺乏对噪声的建模和分离，本文旨在提升语音系统在复杂噪声环境下的准确性和泛化能力。

Method: 作者基于Whisper模型，将语音信号编码为离散的代码簿token，同时将量化残差表征为可解释的噪声向量，并通过轻量级分类器进行监督，实现了语音语义与噪声的有效分离。

Result: 该方法在VBDemand数据集上，ASR错误率相比原始Whisper模型降低82%，比基线强35%，并在已见和未见的声学环境上均表现出良好泛化能力。

Conclusion: 该方法在语音识别（ASR）任务上显著提升了在噪声环境下的表现，实现了对噪声的高度不变性，且在标准测试集上取得了比现有技术更优异的结果。

Abstract: Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.

</details>


### [58] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 为解决原始文档不适配LLM的问题，提出了MDP框架，将文档转化为结构化、可推理的知识输入。MDP-Agent用代理式方法实现该框架，在信息检索任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI搜索依赖于将大语言模型与外部知识源结合，但网页、PDF等原始文档内容长、噪声多、结构混乱，并不适合直接供LLM处理。现有检索方法仅返回原始文本片段，需LLM自行拼装和推理，导致效率低下，准确性受限。因此，亟需新的检索范式，重新定义模型与文档的交互方式。

Method: 作者提出了Model-Document Protocol（MDP）框架，从机制上正式化了原始文本到LLM可消费知识表示的转化过程。MDP包含三种路径：代理式推理（对证据进行组织归纳）、记忆支撑（积累可复用笔记以增强推理）、结构化利用（将文档编码为结构化图或键值缓存）。具体实现为MDP-Agent系统，包括构建文档级摘要记忆、基于扩散探索发现层次依赖、以及采用map-reduce式证据整合，形成紧凑、充足的上下文输入给LLM。

Result: 在信息检索基准任务上，MDP-Agent性能优于传统方法，证明了MDP框架的合理性及其代理式实例的有效性。

Conclusion: MDP重新定义了LLM与文档互动方式，可将原始文档转化为LLM可直接推理的结构化知识，显著提升检索与推理效果。MDP-Agent作为具体实例，展现了框架的实用潜力。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [59] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: 作者针对高、中、低资源语言进行了跨语言NSP大规模评测，发现大模型在低资源语言下性能显著下降，且链式思维提示对不同模型有不同作用，不能一概而论。该研究为理解模型跨语言能力和提示工程提供了重要洞察。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练时主要依赖英文数据，导致其优秀表现可能源于数据优势而非真正的能力。作者希望探究在低资源语言环境下模型的真实表现，以揭示其能力边界。

Method: 构建了包含英语（高资源）、斯瓦希里语（中资源）、豪萨语（低资源）各1万道题的大规模Next Sentence Prediction（NSP）基准测试，评测了几种主流模型（GPT-4 Turbo、Gemini 1.5 Flash、LLaMA 3 70B），并研究了Chain-of-Thought（CoT）提示对模型表现的影响。

Result: 所有模型在英语表现均优异，但在斯瓦希里语下降明显，在豪萨语则大幅下滑，其中LLaMA 3表现最差。引入CoT提示后，LLaMA 3的准确率大幅提升，而GPT-4和Gemini则因“过度思考”反而表现变差。

Conclusion: 链式思维提示并非万能，其效果取决于模型基础能力和具体语境。本文框架有效定位了大模型的跨语言弱点，并揭示了什么时候CoT能够助力或阻碍NSP任务。

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


### [60] [ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation](https://arxiv.org/abs/2510.25224)
*Ziyi Liu,Bahar Sarrafzadeh,Pei Zhou,Longqi Yang,Jieyu Zhao,Ashish Sharma*

Main category: cs.CL

TL;DR: 文章提出了针对多方复杂协商的主动AI调解剂评估框架ProMediate。该框架包含模拟测试环境与多维度评估体系，实验显示社会智能调解代理在促进共识和响应速度方面显著优于传统代理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已经能够在agentic系统中辅助单一用户，但如何让AI代理主动管理复杂、多方合作则面临挑战。缺乏系统性评价方法阻碍了多用户AI代理的发展，尤其是在需要展现社会认知智能、调解多方冲突和话题的场景，如协商。

Method: 提出了ProMediate框架，通过理论驱动的多难度协商模拟环境（Easy/Medium/Hard），和基于社会认知调解理论的AI代理，灵活决定介入时机与方式。同时，设计了一套新的评估指标，包括共识变化、介入延迟、调解效果和智能水平。

Result: 实验结果表明，社会智能调解代理比通用基线代理表现更优，介入更快、更精准，尤其在高难度（ProMediate-Hard）环境下，共识提升比基线高3.6个百分点，且响应速度提升77%。

Conclusion: ProMediate为主动型、社会智能AI代理的发展提供了严谨且理论驱动的评价测试环境。

Abstract: While Large Language Models (LLMs) are increasingly used in agentic
frameworks to assist individual users, there is a growing need for agents that
can proactively manage complex, multi-party collaboration. Systematic
evaluation methods for such proactive agents remain scarce, limiting progress
in developing AI that can effectively support multiple people together.
Negotiation offers a demanding testbed for this challenge, requiring
socio-cognitive intelligence to navigate conflicting interests between multiple
participants and multiple topics and build consensus. Here, we present
ProMediate, the first framework for evaluating proactive AI mediator agents in
complex, multi-topic, multi-party negotiations. ProMediate consists of two core
components: (i) a simulation testbed based on realistic negotiation cases and
theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and
ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in
socio-cognitive mediation theories, capable of flexibly deciding when and how
to intervene; and (ii) a socio-cognitive evaluation framework with a new suite
of metrics to measure consensus changes, intervention latency, mediator
effectiveness, and intelligence. Together, these components establish a
systematic framework for assessing the socio-cognitive intelligence of
proactive AI agents in multi-party settings. Our results show that a socially
intelligent mediator agent outperforms a generic baseline, via faster,
better-targeted interventions. In the ProMediate-Hard setting, our social
mediator increases consensus change by 3.6 percentage points compared to the
generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response
(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,
theory-grounded testbed to advance the development of proactive, socially
intelligent agents.

</details>


### [61] [Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA](https://arxiv.org/abs/2510.25273)
*Sandipan Majhi,Paheli Bhattacharya*

Main category: cs.CL

TL;DR: 本研究提出用大模型生成合成数据增强印地语旅游领域问答训练，小模型经多阶段微调后表现优异，为低资源领域问答提供新思路。


<details>
  <summary>Details</summary>
Motivation: 领域特定的问答系统在低资源语言（如印地语）中的应用面临数据稀缺与通用大语言模型领域知识有限的挑战。

Method: 采用多阶段微调策略，将大语言模型（LLaMA-70B、Phi-14B）生成的合成问答数据与原始数据结合，对轻量级语言模型进行适配。比较不同训练方法对领域泛化能力的影响。

Result: 大型模型可高效生成高质量合成数据，轻量级模型能有效适应合成数据，实现了低资源领域问答系统可扩展的训练方法。

Conclusion: 结合原始和大模型生成的合成数据，多阶段微调有助于提升低资源语言领域问答系统，展现了提升泛化与适配能力的可行途径。

Abstract: Domain-specific question answering in low-resource languages faces two key
challenges: scarcity of annotated datasets and limited domain knowledge in
general-purpose language models. In this work, we present a multi-stage
finetuning strategy to adapt lightweight language models to the Hindi tourism
domain by leveraging both original and synthetic training data. Synthetic
question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and
used to augment the limited original dataset. We explore several training
methodologies and analyse their impact on domain generalisation. Our results
demonstrate that large models can efficiently generate synthetic data, while
small models can effectively adapt to it, offering a scalable pathway for
low-resource, domain-specific QA.

</details>


### [62] [Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student](https://arxiv.org/abs/2510.25303)
*Soumyadeep Jana,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 提出PEKD框架，结合专家模型蒸馏和熵感知动态门控，大幅提升了多模态讽刺检测的参数高效微调效果，尤其在小样本情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态讽刺检测在低资源环境下（标注数据稀缺）很难学习图片与文本间微妙的矛盾，这严重影响了模型性能。当前参数高效微调（PEFT）方法虽能减少过拟合，但在仅有少量监督数据时难以取得最佳表现。为解决这一痛点，作者提出了新方法。

Method: 提出PEKD统一框架，通过将大型讽刺检测专家模型（教师）蒸馏到PEFT方法（学生）来弥补少量数据的不足。同时，设计基于熵的门控机制，根据教师模型置信度动态调整蒸馏强度，以缓解教师模型信号不可靠的问题。PEKD框架具有模块化和广泛适配性，适用于多种多模态模型和任务。

Result: 在两个公开数据集上实验证明，PEKD增强的PEFT方法在小样本场景下性能优越，不仅超过了现有参数高效方法，也超越了一些大型多模态模型，效果显著。

Conclusion: PEKD框架显著提高了多模态讽刺检测模型在低资源、小样本场景下的性能，具备良好的泛化能力和适配性，为相关任务提供了有效新途径。

Abstract: Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.

</details>


### [63] [Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning](https://arxiv.org/abs/2510.25310)
*Senjie Jin,Lu Chen,Zhiheng Xi,Yuhui Wang,Sirui Song,Yuhao Zhou,Xinbo Zhang,Peng Sun,Hong Lu,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了Parrot训练方案，实现了自然语言与程序链式思维的协同优化，在数学推理任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型解决数学推理问题主要有两种范式：自然语言链式思维（N-CoT）和程序链式思维（P-CoT）。现有工作大多只实现单向增强，即P-CoT增强N-CoT或相反，缺乏双向且协同提升。

Method: 提出了一种新的训练流水线Parrot，整合三项面向目标的子任务，顺序生成P-CoT和N-CoT；采用子任务混合训练策略以增强语义迁移能力；通过N-CoT辅助奖励缓解P-CoT优化中的稀疏奖励问题。

Result: 大量实验表明，Parrot可显著提升N-CoT和P-CoT的性能，尤其对N-CoT提升明显。其中，使用Parrot的监督微调让LLaMA2与CodeLLaMA在MathQA数据集上分别比RL基线提升了21.87和21.48分。

Conclusion: Parrot实现了N-CoT与P-CoT范式的协同增强，可有效提升大模型解决数学推理问题的能力，且训练效率高于以往的强化学习基线。

Abstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought
(P-CoT) have emerged as two primary paradigms for large language models (LLMs)
to solve mathematical reasoning problems. Current research typically endeavors
to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced
P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for
mutual enhancement and ultimately achieve simultaneous improvements. We conduct
a detailed analysis of the error types across two paradigms, based on which we
propose Parrot, a novel training pipeline for mathematical problems: 1) Three
target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A
subtask hybrid training strategy to facilitate natural language semantic
transferability. 3) The converted N-CoT auxiliary reward is designed to
alleviate the sparse rewards in P-CoT optimization. Extensive experiments
demonstrate that Parrot significantly enhances both the performance of N-CoT
and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of
LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL
baseline, which is resource-intensive.

</details>


### [64] [CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories](https://arxiv.org/abs/2510.25333)
*Yilong Lai,Yipin Yang,Jialong Wu,Fengran Mo,Zhenglin Wang,Ting Liang,Jianguo Lin,Keping Yang*

Main category: cs.CL

TL;DR: 本文提出CRMWeaver，结合合成数据生成、强化学习和共享记忆机制，有效提升了面向业务的LLM智能体在处理复杂任务和数据时的能力，并在公开数据集上获得了优异的实际效果，具有较高商业应用价值。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于大语言模型（LLM）的智能体快速发展，在解决实际复杂问题中展现了重要潜力，尤其在与数据库和知识库交互的商业环境中，但面临数据关系复杂和任务多样性的挑战。

Method: 提出CRMWeaver方法，通过合成数据生成和基于强化学习（RL）的训练范式提升模型对复杂业务数据和多样化任务的处理能力，在推理阶段引入共享记忆机制，使智能体能从类似任务中学习并优化表现。

Result: 在CRMArena-Pro数据集的B2B和B2C场景中，所提出的轻量级模型达到具有竞争力的结果，验证了方法的有效性和实际应用价值。

Conclusion: CRMWeaver在复杂业务智能体环境下表现优越，通过创新训练与推理机制，显著提升了模型的泛化与实际应用能力，具备现实商业推广的潜力。

Abstract: Recent years have witnessed the rapid development of LLM-based agents, which
shed light on using language agents to solve complex real-world problems. A
prominent application lies in business agents, which interact with databases
and internal knowledge bases via tool calls to fulfill diverse user
requirements. However, this domain is characterized by intricate data
relationships and a wide range of heterogeneous tasks, from statistical data
queries to knowledge-based question-answering. To address these challenges, we
propose CRMWeaver, a novel approach that enhances business agents in such
complex settings. To acclimate the agentic model to intricate business
environments, we employ a synthesis data generation and RL-based paradigm
during training, which significantly improves the model's ability to handle
complex data and varied tasks. During inference, a shared memories mechanism is
introduced, prompting the agent to learn from task guidelines in similar
problems, thereby further boosting its effectiveness and generalization,
especially in unseen scenarios. We validate the efficacy of our approach on the
CRMArena-Pro dataset, where our lightweight model achieves competitive results
in both B2B and B2C business scenarios, underscoring its practical value for
real-world applications.

</details>


### [65] [Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments](https://arxiv.org/abs/2510.25356)
*Abhishek Purushothama,Junghyun Min,Brandon Waldon,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文通过实验分析了英文LLM在美国法律解释场景下的可靠性。结果显示模型输出不稳定，并且与人类判断相关性较低，提示目前不应过度依赖生成式AI来进行法律解释。


<details>
  <summary>Details</summary>
Motivation: 近年来法律学者与法官提出将大型语言模型（LLMs）用于法律解释，但该做法尚存诸多争议。作者希望通过实证分析LLM在法律解释领域的可靠性和适用性。

Method: 作者采用英文语言模型，比较其在不同问题格式下对法律文本的解释判断，分析模型输出的稳定性。同时，将模型的结果与真实人类判断进行相关性分析。

Result: 研究发现，同一问题在不同格式下，模型的解释结果差异很大，缺乏稳定性。此外，模型判断与人类判断的相关性从弱到中等，不同模型和问题变体之间的结果差异显著。

Conclusion: 当前LLM在法律解释中的实践不可取，模型输出容易受提问方式影响且不稳定，与人类判断相关性低，依赖生成式AI进行法律解释存在风险。

Abstract: Legal interpretation frequently involves assessing how a legal text, as
understood by an 'ordinary' speaker of the language, applies to the set of
facts characterizing a legal dispute in the U.S. judicial system. Recent
scholarship has proposed that legal practitioners add large language models
(LLMs) to their interpretive toolkit. This work offers an empirical argument
against LLM interpretation as recently practiced by legal scholars and federal
judges. Our investigation in English shows that models do not provide stable
interpretive judgments: varying the question format can lead the model to
wildly different conclusions. Moreover, the models show weak to moderate
correlation with human judgment, with large variance across model and question
variant, suggesting that it is dangerous to give much credence to the
conclusions produced by generative AI.

</details>


### [66] [CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs](https://arxiv.org/abs/2510.25364)
*Luca Capone,Alessandro Bondielli,Alessandro Lenci*

Main category: cs.CL

TL;DR: 小规模语言模型通过指令微调在微调任务中表现提升，但零样本泛化效果有限，顺序式课程优于合并数据，揭示交互适应与广泛泛化之间的权衡，建议采用课程化混合方法提升低资源模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探讨小规模语言模型（LMs）是否能够从指令微调（instruction tuning）中获益，并测试不同的指令微调数据集及其应用方式对模型表现的影响。

Method: 使用100M和140M参数量的decoder-only模型，对比会话类和问答类指令微调数据集，分别以合并和顺序的课程方式进行微调；在SuperGLUE上进行微调评估，在BLiMP、EWoK、WUGs、实体追踪和心理语言学相关任务上进行零样本评估。

Result: 指令微调在微调场景下带来小但一致的性能提升，顺序课程优于合并数据。但这种提升未能持续转移到零样本任务，显示模型在交互聚焦适应与泛化之间存在权衡。

Conclusion: 人类启发式的学习策略可为低资源语言模型带来潜力，但在泛化能力提升方面存在局限。未来可探索基于课程的混合方法，以在生态化训练限制下改善泛化能力。

Abstract: This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.

</details>


### [67] [Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs](https://arxiv.org/abs/2510.25370)
*Alexander Sternfeld,Andrei Kucharavy,Dimitri Percia David,Alain Mermoud,Julian Jang-Jaccard,Nathan Monnet*

Main category: cs.CL

TL;DR: 作者提出了一种利用大语言模型和图谱分析的新管道，能自动从科学文献和专利文本中挖掘技术融合趋势，比传统专家方法更适合追踪快速变化的ICT领域新兴技术，验证结果显示此方法具有较强的预测与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的专家方法难以应对信息与通信技术（ICTs）领域快速的创新周期及早期术语的不明确性，因此需要一种新的自动化方法来有效监测变革性技术的出现。

Method: 利用大型语言模型（LLM）从非结构化文本中抽取语义三元组，构建技术实体和关系的图谱。通过名词拼接（noun stapling）方法分组语义相近的技术词，开发基于图的指标检测融合信号，并结合多阶段过滤、领域关键词聚类与共现的时间趋势分析。

Result: 该方法在 arXiv 预印本与 USPTO 专利申请数据集上进行了验证，能够识别已建立和新兴的技术融合模式，实现对技术发展的高效预测。

Conclusion: 提出的数据驱动管道能够有效地识别技术融合模式，对技术预测具有可扩展和可推广价值。

Abstract: Forecasting transformative technologies remains a critical but challenging
task, particularly in fast-evolving domains such as Information and
Communication Technologies (ICTs). Traditional expert-based methods struggle to
keep pace with short innovation cycles and ambiguous early-stage terminology.
In this work, we propose a novel, data-driven pipeline to monitor the emergence
of transformative technologies by identifying patterns of technological
convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract
semantic triples from unstructured text and construct a large-scale graph of
technology-related entities and relations. We introduce a new method for
grouping semantically similar technology terms (noun stapling) and develop
graph-based metrics to detect convergence signals. The pipeline includes
multi-stage filtering, domain-specific keyword clustering, and a temporal trend
analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv
preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO
patent applications (2018-2024) to track downstream commercial developments.
Our results demonstrate that the proposed pipeline can identify both
established and emerging convergence patterns, offering a scalable and
generalizable framework for technology forecasting grounded in full-text
analysis.

</details>


### [68] [Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy](https://arxiv.org/abs/2510.25378)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本文研究了GPT-4.1生成文献推荐时的真实性，发现高被引论文模型能准确复现，幻觉率低，而低被引论文则易出现虚假。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在众多任务中表现出色，但在生成文献推荐时容易产生虚构（不存在的）论文。作者关注LLM生成书目信息的真实性，试图探索影响虚假引用产生的因素，特别是模型是否通过“记忆”高频论文来降低虚假率。

Method: 以GPT-4.1为实验对象，跨二十个计算机科学领域，人工验证生成的100条文献记录，并应用余弦相似度测量生成元数据与真实元数据的一致性，以统计不同领域和被引用次数下的幻觉率和准确性。

Result: （1）不同研究领域的幻觉率差异明显；（2）被引用次数与事实准确性高度相关；（3）被引用超过约1000次的论文，模型几乎是逐字记忆并准确复现其书目信息。

Conclusion: 高被引论文由于在训练语料中多次出现，模型能够很好记忆其内容，实际生成时几乎无误，存在从泛化到记忆的转变阈值。该结果有助于理解LLM在书目推荐任务中的准确性机理，也提醒用户对低被引文献生成结果保持警惕。

Abstract: Large language models (LLMs) have been increasingly applied to a wide range
of tasks, from natural language understanding to code generation. While they
have also been used to assist in bibliographic recommendation, the
hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM's ability to correctly produce
bibliographic information depends on whether the underlying knowledge is
generated or memorized, with highly cited papers (i.e., more frequently appear
in the training corpus) showing lower hallucination rates. We therefore assume
citation count as a proxy for training data redundancy (i.e., the frequency
with which a given bibliographic record is repeatedly represented in the
pretraining corpus) and investigate how citation frequency affects hallucinated
references in LLM outputs. Using GPT-4.1, we generated and manually verified
100 bibliographic records across twenty computer-science domains, and measured
factual consistency via cosine similarity between generated and authentic
metadata. The results revealed that (i) hallucination rates vary across
research domains, (ii) citation count is strongly correlated with factual
accuracy, and (iii) bibliographic information becomes almost verbatimly
memorized beyond approximately 1,000 citations. These findings suggest that
highly cited papers are nearly verbatimly retained in the model, indicating a
threshold where generalization shifts into memorization.

</details>


### [69] [Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires](https://arxiv.org/abs/2510.25384)
*Doan Nam Long Vu,Rui Tan,Lena Moench,Svenja Jule Francke,Daniel Woiwod,Florian Thomas-Odenthal,Sanna Stroth,Tilo Kircher,Christiane Hermann,Udo Dannlowski,Hamidreza Jamalabadi,Shaoxiong Ji*

Main category: cs.CL

TL;DR: 该论文提出了利用开源大语言模型生成心理咨询对话的新方法，有效解决了因隐私限制导致的数据短缺问题，生成高质量语料并训练出表现优异的AI模型，推动了安全且可靠的心理健康AI发展。


<details>
  <summary>Details</summary>
Motivation: 由于严格的隐私法规和临床会话记录稀缺，导致AI心理健康领域缺乏真实的治疗对话数据，严重制约了相关AI的发展。

Method: 提出了一套由大型语言模型（LLM）驱动的管道系统，通过结构化的心理问卷和客户档案生成合成的心理咨询对话。系统以认知行为疗法（CBT）为理论基础，将结构化心理输入转化为自然语言对话，模拟治疗师-客户交互，采用开源LLM避免隐私泄露，并辅以专家和模型双重评估验证对话质量。

Result: 通过在公开权重LLM上生成高质量合成对话语料，训练得到的SQPsychLLM模型在心理咨询核心技能上超越了现有基线方法，有效提升了AI心理健康支持系统的能力。

Conclusion: 合成数据能够帮助研发安全、可扩展并具临床指导性的心理健康AI系统。该方法有效解决隐私数据受限问题，为行业提供了开源代码、模型和语料。

Abstract: The development of AI for mental health is hindered by a lack of authentic
therapy dialogues, due to strict privacy regulations and the fact that clinical
sessions were historically rarely recorded. We present an LLM-driven pipeline
that generates synthetic counseling dialogues based on structured client
profiles and psychological questionnaires. Grounded on the principles of
Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic
conversations for clinical disorders such as anxiety and depression. Our
framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts
structured psychological input into natural language dialogues through
therapist-client simulations. Due to data governance policies and privacy
restrictions prohibiting the transmission of clinical questionnaire data to
third-party services, previous methodologies relying on proprietary models are
infeasible in our setting. We address this limitation by generating a
high-quality corpus using open-weight LLMs, validated through human expert
evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on
SQPsychConv achieve strong performance on counseling benchmarks, surpassing
baselines in key therapeutic skills. Our findings highlight the potential of
synthetic data to enable scalable, data-secure, and clinically informed AI for
mental health support. We will release our code, models, and corpus at
https://ai-mh.github.io/SQPsych

</details>


### [70] [BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains](https://arxiv.org/abs/2510.25409)
*Vijay Devane,Mohd Nauman,Bhargav Patel,Aniket Mahendra Wakchoure,Yogeshkumar Sant,Shyam Pawar,Viraj Thakur,Ananya Godse,Sunil Patra,Neha Maurya,Suraj Racha,Nitish Kamal Singh,Ajay Nagpal,Piyush Sawarkar,Kundeshwar Vijayrao Pundalik,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文首次提出覆盖印度关键知识体系的双语评测基准BhashaBench V1，经评测揭示主流LLM在领域和语言存在显著性能差异，其中资源较少领域和印地语表现尤为薄弱，研究促进针对印度本地需求的模型改进并开放数据支持社区研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）评测基准多偏向英语且缺乏针对特定领域和文化，限制了其在印度本地语境下的应用。

Method: 提出BhashaBench V1，这是首个聚焦于印度关键知识体系的领域特定、任务多样、双语（英语和印地语）评测基准，涵盖农业、法律、金融及阿育吠陀四大领域及90余个子领域，共收集74,166个问答对，来源于真实政府与专业考试。

Result: 对29种以上LLM进行评测，发现领域和语言存在显著性能差距，尤其在资源较少领域表现更弱。例如，GPT-4o在法律领域准确率达76.49%，但在阿育吠陀仅59.74%。所有模型英语内容表现优于印地语，个别子领域如网络法、国际金融较好，种子科学、人权等则薄弱。

Conclusion: BhashaBench V1为印度多元知识领域下大语言模型提供了评估工具，有助于衡量模型整合领域知识与双语能力，并开放相关资源推动研究发展。

Abstract: The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.

</details>


### [71] [Serve Programs, Not Prompts](https://arxiv.org/abs/2510.25412)
*In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: 传统LLM服务系统缺乏灵活性，难以满足复杂应用需求。文章提出基于LIP的新架构，并以Symphony为例，实现了定制化推理与高效资源管理，提升了LLM服务的可用性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）服务系统主要用于文本补全，但由于设计缺乏灵活性，难以应对越来越复杂的LLM应用需求。

Method: 提出一种新的LLM服务系统架构，将“程序”作为服务对象而非传统的“提示”。这些程序被称为LLM推理程序（LIP），允许用户在运行时定制token预测和KV缓存管理，并实现将部分应用逻辑（如工具调用）卸载到服务器端。以名为Symphony的系统为例，系统通过系统调用暴露模型计算，将KV缓存虚拟化为专用文件系统，并采用两级进程调度提高GPU效率。

Result: Symphony作为LIP的操作系统，实现了灵活定制和高效的资源管理，在保证GPU计算效率的同时，提供了可拓展性和模块化支持。

Conclusion: Symphony有望为LLM应用开创更高效、可扩展的服务生态系统。

Abstract: Current large language model (LLM) serving systems, primarily designed for
text completion, are neither efficient nor adaptable for increasingly complex
LLM applications due to their inflexible design. We propose a new LLM serving
system architecture that serves programs instead of prompts to address this
problem. These programs, called LLM Inference Programs (LIPs), allow users to
customize token prediction and KV cache management at runtime and to offload
parts of their application logic, such as tool execution, to the server. We
describe an example of this architecture through a system named Symphony, which
functions as an operating system for LIPs. Symphony exposes LLM model
computations via system calls and virtualizes KV cache with a dedicated file
system, while ensuring GPU efficiency with a two-level process scheduling
scheme. Symphony has the potential to open the door to a more efficient and
extensible ecosystem for LLM applications.

</details>


### [72] [Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media](https://arxiv.org/abs/2510.25413)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 本研究提出基于视觉语言模型的手语数据集自动标注与过滤框架，显著降低了数据采集的人力成本，构建了多语种TikTok手语数据集，并验证了主流模型在新数据集上的鲁棒性，为SLT领域大规模数据获取提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译数据集规模小、语种覆盖有限且依赖专家标注，导致获取高质量大规模数据的成本高昂。VLMs近期表现出优异的多模态理解能力，但尚未应用于手语数据集采集。作者希望借助VLMs自动化解决这一瓶颈。

Method: 方法包括利用视觉语言模型，对社交平台（TikTok和YouTube）视频进行：人脸可见度检测、手势活动识别、视频内容文本提取，并通过VLM对视频与文本的对齐关系进行自动化判断，从而实现过滤、标注和验证流程。最终生成了TikTok-SL-8等新数据集，并对现有SLT模型进行了测试。

Result: 成功构建了跨八种手语的大规模TikTok-SL-8数据集，并在德语、美国手语上用两种主流SLT模型建立了基线，验证了自动化采集数据支持下模型对噪声数据的鲁棒性。结果显示，该框架可为SLT领域提供弱监督预训练和高效数据采集的新途径。

Conclusion: 该研究提出了一种基于视觉语言模型（VLMs）的自动化数据集标注和过滤框架，能有效减少对人力的依赖并保持数据质量。这一方法使得大规模、跨多语种的手语数据集采集变得可行，为SLT领域数据获取和模型训练的可扩展性提供了新思路。

Abstract: Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.

</details>


### [73] [Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction](https://arxiv.org/abs/2510.25426)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.CL

TL;DR: 这项研究表明，融入会话含义的提示词能显著提升大语言模型（尤其是小模型）响应的相关性和质量，大多数用户更偏好这种语境丰富的交流方式。这说明语言学理论在促进人-AI对齐和更自然的互动方面具有重要价值。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，语言正在成为人机交互（HCI）的核心。研究者认为，为了推动HCI的发展，需要关注互动的语言学基础，尤其是“会话含义”（implicature），即通过共享语境传递超越明确表达的信息，这对人与AI的对齐（alignment）至关重要。

Method: 本研究测试了LLMs是否能通过语境驱动的提示词推断用户意图，以及理解会话含义是否提升其生成响应的质量。比较了不同规模的模型在推断会话含义方面的能力。

Result: 结果发现，大模型更接近人类的意图理解，而小模型在推断会话含义方面表现较差。所有模型在接收到基于会话含义的提示词后，响应的相关性和质量都有显著提升，尤其是小模型受益更大。共67.6%的参与者更喜欢嵌入会话含义的提示词所生成的响应。

Conclusion: 会话含义使人与AI的互动更加自然且具有语境特色，语言学理论能有效帮助解决人机对齐问题。

Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.

</details>


### [74] [RLMEval: Evaluating Research-Level Neural Theorem Proving](https://arxiv.org/abs/2510.25427)
*Auguste Poiroux,Antoine Bosselut,Viktor Kunčak*

Main category: cs.CL

TL;DR: 提出RLMEval评测套件，围绕真实Lean项目的研究级数学定理，针对现有神经定理证明和自动形式化模型进行评估。实验表明，现有模型在实际环境下表现远逊于基准测试，最高通过率仅10.3%。RLMEval为正式数学自动化研究提供更真实、具挑战性的标准，有助于引导未来改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在精心设计的基准测试中取得了令人瞩目的成果，但在研究级神经定理证明和自动形式化领域，其实际影响仍有限。

Method: 作者提出了RLMEval，这是一套针对上述任务的评估工具，专注于现实世界Lean形式化项目中的研究级数学问题。利用Lean Blueprint真实项目，RLMEval对神经定理证明和自动形式化在实际复杂定理上的能力进行评估。

Result: 在对613个定理和6个Lean项目的评估中，目前最先进的模型仅实现了10.3%的通过率，显示存在重大技术差距。

Conclusion: 当前模型在传统基准测试中的进步，无法有效迁移到更现实的研究级环境。RLMEval为自动推理和形式化数学提供了更具挑战性的评测平台，有助于推动领域发展。

Abstract: Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.

</details>


### [75] [Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research](https://arxiv.org/abs/2510.25432)
*Ali Sanaei,Ali Rajabzadeh*

Main category: cs.CL

TL;DR: 本文提出了基于“解释深度”和“自主性”两维度的LLM定性社会科学应用框架，建议通过任务细分和低自主性使用，提升研究透明度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在社会科学中的应用不断增加，但在定性研究中存在解释偏差、可靠性低和审计能力弱等挑战。该研究旨在解决这些问题。

Method: 提出了一个将LLMs应用框架，基于“解释深度”和“自主性”两个维度分类LLM在定性研究中的作用，并回顾了现有文献。

Result: 通过建议将任务分解为可管理的部分，并在严格监督下低自主性、选择性提高解释深度，以此优化LLM在研究中的应用，提升透明度和可靠性。

Conclusion: 在保持透明和可靠性的前提下，合理利用LLMs能够增强社会科学定性研究，但需严格控制模型自主性与适时提升解释深度。

Abstract: Large language models (LLMs) are increasingly utilized by researchers across
a wide range of domains, and qualitative social science is no exception;
however, this adoption faces persistent challenges, including interpretive
bias, low reliability, and weak auditability. We introduce a framework that
situates LLM usage along two dimensions, interpretive depth and autonomy,
thereby offering a straightforward way to classify LLM applications in
qualitative research and to derive practical design recommendations. We present
the state of the literature with respect to these two dimensions, based on all
published social science papers available on Web of Science that use LLMs as a
tool and not strictly as the subject of study. Rather than granting models
expansive freedom, our approach encourages researchers to decompose tasks into
manageable segments, much as they would when delegating work to capable
undergraduate research assistants. By maintaining low levels of autonomy and
selectively increasing interpretive depth only where warranted and under
supervision, one can plausibly reap the benefits of LLMs while preserving
transparency and reliability.

</details>


### [76] [A Critical Study of Automatic Evaluation in Sign Language Translation](https://arxiv.org/abs/2510.25434)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Eleftherios Avramidis,Josef van Genabith*

Main category: cs.CL

TL;DR: 当前SLT评价指标以文本为主，存在局限。对比多种评价方法后，发现LLM评估器虽在语义等价性方面更好，但也有偏见，全面评价SLT需发展多模态框架。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译（SLT）的自动评估指标都基于文本，如BLEU、ROUGE，但这些指标是否能真实反映SLT输出的质量仍不清楚，特别是因为手语含有丰富的视觉和语义信息。研究动机是分析并弥补文本评价指标的局限性，促进SLT的可靠自动评价方法发展。

Method: 分析六种评价指标（BLEU, chrF, ROUGE, BLEURT 及LLM-based评估器如G-Eval和GEMBA），在三个受控条件下（释义、幻觉输出、句长变化）测试指标的一致性和健壮性，并对比文本重叠指标和LLM评估器的表现差异。

Result: 研究发现，词汇重叠类指标（如BLEU、ROUGE）在捕捉语义等价性方面存在明显局限，而LLM评估器在区分语义等价方面表现更好，但对LLM生成的释义有偏向性；所有指标都能检测幻觉，BLEU对细微幻觉过于敏感，BLEURT和LLM评估器则宽容一些。

Conclusion: 文本指标无法全面评价手语翻译输出，提出建立超越文本的多模态评估框架以提升SLT的自动评价质量。

Abstract: Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.

</details>


### [77] [Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs](https://arxiv.org/abs/2510.25441)
*Fei Wei,Daoyuan Chen,Ce Wang,Yilun Huang,Yushuo Chen,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.CL

TL;DR: 本文提出Learn-to-Ask主动对话训练框架，凭借离线专家数据解决当前LLM主动性弱及高成本仿真瓶颈，显著提升大模型在医疗等关键领域的实战表现，已在大规模在线服务中验证效果，优于人类专家。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在主动、目标导向对话中的应用存在挑战，特别是在重要领域，需要更高的主动性和决策能力。现有方法主要有两类，一是仅优化单轮对话属性，二是依赖昂贵且脆弱的用户模拟器，导致现实应用中存在“现实鸿沟”。因此，亟需一种能够直接从专家数据中学习主动对话策略的新范式。

Method: 提出了一种无需模拟器的泛用框架——Learn-to-Ask，直接利用离线专家数据来训练主动型对话代理。核心创新是利用专家轨迹中的未来观察，推断高密度、分步的奖励信号，将原本难以处理的长周期问题分解为监督学习任务。训练目标是输出结构化的（行动，状态评估）元组，不仅决定询问内容，也判断何时停止对话。此外，通过自动化的奖励模型噪声清理流程，保证奖励信号的准确性，减少人工介入。

Result: 实证上，该方法在医学真实数据集上取得优异表现，适用于各种规模的LLM（最高至32B）。Learn-to-Ask成功应用于大规模在线AI服务，在内部评测中表现超越人类专家，实现了LLM从离线数据到实际应用的有效落地。

Conclusion: Learn-to-Ask方法为提升LLM主动性构建了经济、高效的实践路径，可将被动响应型LLM转变为主动、目标导向的应用，具备现实世界影响力。

Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them
to be proactive, goal-oriented partners, a critical capability in high-stakes
domains, remains a major challenge. Current paradigms either myopically
optimize single-turn attributes or rely on brittle, high-cost user simulators,
creating a persistent ``reality gap''. To bridge this gap, we introduce
\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and
deploying proactive dialogue agents \textit{directly from offline expert data},
bypassing the need to model complex user dynamics. Our key insight is to
reframe the offline policy learning problem by leveraging the \textbf{observed
future} of each expert trajectory. This allows us to infer a dense,
turn-by-turn reward signal grounded in the expert's revealed strategy,
decomposing the intractable long-horizon problem into a series of supervised
learning tasks, and training a policy to output a structured \texttt{(action,
state_assessment)} tuple, governing both \textbf{what to ask} and, crucially,
\textbf{when to stop}. To ensure reward fidelity, our Automated Grader
Calibration pipeline systematically purges noise from the LLM-based reward
model with minimal human supervision. Empirically, we demonstrate the efficacy
of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying
sizes up to 32B. Our approach culminates in the successful deployment of LLMs
into a live, large-scale online AI service. In rigorous in-house evaluations,
our model was launched and achieved performance even superior to human experts,
proving our framework's ability to translate offline data into tangible,
real-world impact. We hope this work provides a practical and economically
viable blueprint for transforming passive LLMs into proactive, goal-oriented
LLM applications.

</details>


### [78] [Fine-Tuned Language Models for Domain-Specific Summarization and Tagging](https://arxiv.org/abs/2510.25460)
*Jun Wang,Fuming Lin,Yuyu Chen*

Main category: cs.CL

TL;DR: 本研究提出LLM与NER集成方法，通过领域微调显著提升文本摘要和实体标注的效率和准确性，尤其适用于应对多变语言环境，支持安全与知识管理的实时应用。


<details>
  <summary>Details</summary>
Motivation: 面对快速演化的次文化语言和俚语，传统信息抽取和执法监控变得十分复杂，需要更高效、准确的自动化方法。

Method: 提出一个将微调的大语言模型（LLM）与命名实体识别（NER）集成的流程，利用LLaMA Factory框架在通用和定制领域数据集（政治、安全等）上微调LLM，并用BLEU和ROUGE评估模型表现。

Result: 指令微调显著提升了摘要和标注的准确性，LLaMA3-8B-Instruct模型在领域微调后甚至优于专为中文训练的模型，显示模型推理能力可跨语种迁移。

Conclusion: 集成式流程可实现高效领域文本摘要和标注，支持实时、可扩展的信息管理，是处理不断变化语言趋势的强有力工具，有助于知识管理和安全运营。

Abstract: This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.

</details>


### [79] [TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation](https://arxiv.org/abs/2510.25536)
*Bangde Du,Minghao Guo,Songming He,Ziyi Ye,Xi Zhu,Weihang Su,Shuqi Zhu,Yujia Zhou,Yongfeng Zhang,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 本研究提出了用于评估大语言模型人格模拟能力的TwinVoice基准，并发现当前LLM在多项关键能力上仍不及人类。


<details>
  <summary>Details</summary>
Motivation: 当前关于大语言模型（LLMs）人格模拟的评估存在局限，大多依赖合成对话，缺乏系统框架和能力分析。

Method: 提出了TwinVoice基准，包含社会人格、交互人格和叙述人格三大维度，并将评估细化为六项基本能力。通过这些维度和能力对LLMs进行全面实验评估。

Result: 高级LLM在人格模拟准确率尚可，但在句法风格与记忆召回等方面表现不足，整体水平远低于真人基线。

Conclusion: 现阶段LLM在逼近人类人格表现上仍有较大差距，尤其在复杂表达和记忆一致性等能力上表现不足。

Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.

</details>


### [80] [Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry](https://arxiv.org/abs/2510.25595)
*Run Peng,Ziqiao Ma,Amy Pang,Sikai Li,Zhang Xi-Jia,Yingzhuo Yu,Cristian-Paul Bara,Joyce Chai*

Main category: cs.CL

TL;DR: 本文在信息不对称环境下，通过扩展Einstein谜题与“微调+环境验证”框架，研究了两LLM智能体协作完成任务的能力。结果显示，协作交流和环境反馈对真实理解规则和人机信任度有显著助益。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）智能体在协作完成任务时，特别是在信息不对称情况下的协作能力缺乏研究。本文旨在探索LLM智能体在共同任务中的协作表现。

Method: 作者将传统的Einstein谜题扩展为桌面游戏环境，设计了两个需要推理、交流和行动的LLM智能体，并采用了“微调+验证”框架，赋予智能体多种通讯策略以及环境反馈信号。通过实验对比有无交流、与环境验证结合等多种方案。

Result: 实验证明，对齐的交流方式对信息不对称情况下的高效协作至关重要。没有交流时智能体任务完成率仍高，但缺乏真正理解规则和人类信任度。结合环境验证可显著提升智能体理解规则和完成任务的能力，实现更安全、可解释的AI协作。

Conclusion: 环境验证与高质量交流机制能够有效提升LLM智能体在复杂协作任务中的表现，推动AI协作更安全且具可解释性。

Abstract: While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles

</details>


### [81] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 本文针对波斯伊斯兰宗教问答领域，提出了具备动态自我校正和多步推理的FAIR-RAG架构及FARSIQA系统，有效提升多跳复杂问题的准确率和可信度，在基准测试中显著优于现有方法，树立了新标准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然在自然语言处理领域取得了革命性进展，但在宗教问答等高风险、特殊领域应用中仍面临幻觉、与权威信息不一致等问题。对于讲波斯语的穆斯林群体，准确性和可信度至关重要，现有RAG检索增强系统在复杂、多跳问题处理上存在明显不足。

Method: 提出FARSIQA系统，基于创新的FAIR-RAG（Faithful, Adaptive, Iterative Refinement）架构。该架构采用动态自我校正流程，适应性分解复杂查询，评估证据充足性，通过迭代环生成子查询，逐步填补信息空白，并基于一百万份权威伊斯兰文献知识库进行检索和回答。

Result: FARSIQA在伊斯兰PCQA基准测试上表现卓越，负向拒绝率达到97.0％，比基线高40个百分点，答案正确率达到74.3％，均创下新标准。

Conclusion: FARSIQA通过迭代、适应性架构，有效提升了波斯伊斯兰领域问答的可信度和准确性，证明该方法在敏感领域构建可靠AI系统具有重要价值。

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [82] [Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks](https://arxiv.org/abs/2510.25623)
*Davide Romano,Jonathan Schwarz,Daniele Giofré*

Main category: cs.CL

TL;DR: 该论文系统评估了测试时缩放（TTS）技术在法律选择题中的表现，结合不同验证方法与监督类型，发现TTS在论证性领域有潜力但效果依赖模型和任务细节。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）技术可以提升大语言模型（LLM）性能，但会带来额外计算和延迟。在数学与编程等正式领域已证明其有效性，但在法律等论证性领域的价值尚未充分探索。该论文旨在研究TTS在法律多项选择题（MCQA）中的应用与表现。

Method: 采用7种奖励模型，在法律MCQA任务的五个基准数据集上进行实证研究。评估了基于结果（Best-of-N）和基于过程（树搜索）的验证方式，并在现实的低N预算下测试其效用。同时，系统性分析了验证者效用受领域专门化、模型规模、监督类型（过程监督PRMs与仅结果监督ORMs）、角色等因素的影响。

Result: 在现实预算下，对比不同的验证方法（结果层级与过程层级）、不同的奖励模型与监督类型，论文系统分析了TTS方法在法律领域下的效用，初步披露关键影响因素。

Conclusion: TTS方法在法律MCQA领域展现了可行性，验证层面与模型设置等关键属性显著影响其实际效用，支持了在论证性领域进一步应用与调优TTS技术的可能性。

Abstract: Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming \citep{snell2024scaling, chen2024more}, its value in argumentative
domains such as law remains underexplored. We present an empirical study of
verifier-based TTS methods for legal multiple-choice QA (MCQA) across five
benchmarks. Using a family of 7 reward models, we evaluate both outcome-level
(Best-of-$N$) and process-level (tree search) verification under realistic
low-$N$ budgets. Our analysis systematically investigates how verifier utility
is affected by key properties such as domain specialization, model size, and
supervision type (process-supervised PRMs vs. outcome-only ORMs), even when
applied across different roles.

</details>


### [83] [EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis](https://arxiv.org/abs/2510.25628)
*Yusheng Liao,Chaoyi Wu,Junwei Liu,Shuyang Jiang,Pengcheng Qiu,Haowen Wang,Yun Yue,Shuai Zhen,Jian Wang,Qianrui Fan,Jinjie Gu,Ya Zhang,Yanfeng Wang,Yu Wang,Weidi Xie*

Main category: cs.CL

TL;DR: 本文提出了针对电子健康记录（EHR）的大型推理指令数据集EHR-Ins、推理增强型大模型EHR-R1及综合评测基准EHR-Bench。通过创新的数据生成和多阶段训练方法，所研发模型在临床推理分析等多任务上显著优于主流大语言模型，推动了医疗健康数据分析领域的发展。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）在医疗健康领域特别是电子健康记录（EHR）分析方面存在任务覆盖窄和缺乏专门EHR推理能力的问题，限制了其在临床决策中的应用。

Method: 提出EHR-Ins，一个包含300,000条高质量推理案例和400万条非推理案例、涵盖42种EHR任务的大规模推理数据集。核心创新为基于思维图的框架，可大规模生成高质量推理数据。在此基础上研发EHR-R1，一系列参数最多达72B的、专为EHR分析定制的推理增强型LLM。采用多阶段训练，包括领域适应、推理能力增强和强化学习，使模型系统性获得领域知识和多样化推理能力。最后构建EHR-Bench，基于MIMIC-IV的全新评测基准，覆盖42个任务，全面评估模型在EHR上的推理与预测能力。

Result: 实验结果显示，EHR-R1在各项任务上均明显优于现有主流商业及开源LLM（如DeepSeek-V3和GPT-4o），在MIMIC-Bench超越GPT-4o超过30分，在EHRSHOT实现10%更高的零样本AUROC。

Conclusion: EHR-Ins、EHR-R1及EHR-Bench的推出显著推进了更可靠且更具临床价值的EHR分析模型的发展。

Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.

</details>


### [84] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng,Zhiyang Teng,Xiangtai Li,Anran Wang,Yu Tian,Kunpeng Qiu,Ye Tian,Haochen Wang,Zhuochen Wang*

Main category: cs.CL

TL;DR: 通过构建理解-生成配对并结合配对感知的优化方法，系统性缓解了任务干扰并提升了统一视觉-语言模型的整体表现，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 统一的视觉-语言模型需要在单一架构下同时进行理解与生成任务，但两者的数据和监督形式不同，使得在强化学习阶段难以平衡二者。

Method: 将数据重组为理解-生成配对，包括使用GPT-3生成任务配对样本与检索语义相关配对，并提出Pair-GPRO带配对感知的优化算法，利用配对相似度分数动态调整学习优势。

Result: 提出的PairUni方法在多种主流UVLMs上均实现了理解与生成任务的均衡提升，优于现有强基线方法。

Conclusion: PairUni框架可以更好地组织与利用异质数据，实现视觉-语言统一模型在理解与生成任务上的平衡优化，提升了强化学习微调效果。

Abstract: Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}

</details>


### [85] [Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?](https://arxiv.org/abs/2510.25701)
*Saeed AlMarri,Kristof Juhasz,Mathieu Ravaut,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.CL

TL;DR: 本文系统对比了零样本LLM与LightGBM在贷款违约预测中的表现。LLM虽能识别关键风险特征，但其解释能力和特征重要性与传统模型差异大，且自解释可信度不足。强调在金融高风险场景下应加强模型解释分析与人工干预。


<details>
  <summary>Details</summary>
Motivation: 近年来，LLM被广泛探索用于分类任务中的零样本推断，但其在结构化表格数据中的应用，特别是在金融风险评估等高风险领域，尚未得到充分研究。

Method: 系统地对比了零样本LLM分类器与代表性梯度提升模型LightGBM在真实贷款违约预测任务上的表现。评估了预测性能，采用SHAP分析特征归因，并评估LLM自生成解释的可靠性。

Result: LLM能识别主要金融风险指标，但其特征重要性排序与LightGBM存在显著差异，自生成解释常和SHAP实证归因不一致。

Conclusion: LLM作为独立结构化金融风险预测模型存在局限，其自生成解释的可信度值得担忧。在风险敏感的金融环境中部署LLM需进行可解释性审查、与可解释基线模型对比，并纳入人工监督。

Abstract: Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.

</details>


### [86] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: 本文提出了Toolathlon基准，以多样应用和真实环境评测语言代理模型复杂多步任务解决能力，结果显示当前顶尖模型表现有限，有待进一步优化。


<details>
  <summary>Details</summary>
Motivation: 当前语言代理评测任务过于简单、领域狭窄，缺乏现实复杂度和多样性，不能有效反映模型在真实应用中的能力，亟需更具挑战性的测试基准。

Method: 构建了Toolathlon基准，涵盖32种应用软件、604个工具，提供真实环境初始状态，以及严格可验证的多步（平均20步）任务，采用专用评估脚本对模型表现进行综合评测。

Result: SOTA模型在Toolathlon上的表现较差，最好模型Claude-4.5-Sonnet成功率仅为38.6%，平均调用工具20次，开源权重模型DeepSeek-V3.2-Exp成功率仅为20.1%，表明仍有巨大提升空间。

Conclusion: 现有的语言代理模型在处理现实世界复杂任务方面存在明显不足，Toolathlon基准能够客观评估和推动此领域的发展。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


### [87] [The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework](https://arxiv.org/abs/2510.25732)
*Aakriti Shah,Thai Le*

Main category: cs.CL

TL;DR: 提出SKeB理论框架及纠缠度量，发现说服性提示词能“唤醒”LLM中被遗忘的知识，并首创性量化了遗忘行为对不同模型的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在管理敏感数据和修正错误信息时，如何“遗忘”特定知识变得非常重要，但目前缺乏有效的评估方法。

Method: 借鉴ACT-R与Hebbian理论，以及传播学原理，提出了Stimulus-Knowledge Entanglement-Behavior Framework（SKeB），通过构建领域图量化知识激活，并制定纠缠度量方法，同时采用说服性提示词对多种模型（2.7B至13B参数规模）进行实验。

Result: 说服性提示词显著提升了被遗忘信息的召回率（权威措辞下由14.8%提升至24.5%），且增强效果随模型规模变大而变弱（2.7B模型恢复128%，13B仅15%）。

Conclusion: 说服性提示词可以显著影响LLM遗忘后知识的回忆，SKeB框架为评估遗忘的完整性、鲁棒性及行为表现提供了理论基础和量化工具。

Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.

</details>


### [88] [Scaling Latent Reasoning via Looped Language Models](https://arxiv.org/abs/2510.25741)
*Rui-Jie Zhu,Zixuan Wang,Kai Hua,Tianyu Zhang,Ziniu Li,Haoran Que,Boyi Wei,Zixin Wen,Fan Yin,He Xing,Lu Li,Jiajun Shi,Kaijing Ma,Shanda Li,Taylor Kergan,Andrew Smith,Xingwei Qu,Mude Hui,Bohong Wu,Qiyang Min,Hongzhi Huang,Xun Zhou,Wei Ye,Jiaheng Liu,Jian Yang,Yunfeng Shi,Chenghua Lin,Enduo Zhao,Tianle Cai,Ge Zhang,Wenhao Huang,Yoshua Bengio,Jason Eshraghian*

Main category: cs.CL

TL;DR: Ouro提出一种预训练阶段内置推理机制的新型LoopLM，通过循环迭代和熵正则化，训练规模达7.7T tokens。其在常规基准上小模型竟能媲美大模型，展现了预训练“内推理”方向的潜力，并公开了相关模型资源。


<details>
  <summary>Details</summary>
Motivation: 现有主流LLM（如使用Chain-of-Thought）主要依靠后训练的文本生成进行推理，未充分利用预训练数据的推理潜力。该论文旨在通过在预训练阶段集成推理能力，提升模型在推理相关任务上的性能。

Method: Ouro（LoopLM）采用了三项创新：（1）在隐空间进行循环迭代计算，（2）引入熵正则化目标以学习深度分配，（3）在大规模（7.7T tokens）数据上训练。

Result: Ouro 1.4B和2.6B在广泛基准任务上的表现与规模高达12B的最先进模型相当。实验证明，其成功主要来自更好的知识操控能力，LoopLM生成的推理轨迹也更贴合最终答案而非只是显式文本推理。

Conclusion: Ouro模型通过在预训练阶段引入循环推理机制，在多个基准任务上表现出色，比传统显式文本推理方式更有效。其优势来自知识操控能力而非知识容量，并且能生成与最终输出更一致的推理轨迹。

Abstract: Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.

</details>


### [89] [Task Completion Agents are Not Ideal Collaborators](https://arxiv.org/abs/2510.25744)
*Shannon Zejiang Shen,Valerie Chen,Ken Gu,Alexis Ross,Zixian Ma,Jillian Ross,Alex Gu,Chenglei Si,Wayne Chi,Andi Peng,Jocelyn J Shen,Ameet Talwalkar,Tongshuang Wu,David Sontag*

Main category: cs.CL

TL;DR: 论文提出了协作努力扩展框架，用于评估智能体在多轮与用户协作中的效用，并发现现有智能体通常无法很好地持续支持和提升人类参与，建议未来智能体评估和设计关注协作与过程互动。


<details>
  <summary>Details</summary>
Motivation: 现有智能体评估多聚焦单次任务完成，忽视了许多实际问题需要用户反复参与和方案协作。需推动智能体设计与评估方式向协作能力和过程参与转变。

Method: 提出了协作努力扩展（collaborative effort scaling）框架，通过案例分析和模拟评估，衡量智能体随着用户参与度提升其效用变化。

Result: 实验表明，现有智能体在多轮协作和真实场景下表现有限，协作努力扩展框架能帮助发现问题，引导智能体更好支持用户的参与与理解。

Conclusion: 目前顶尖的智能体在多回合、真实世界的协作场景中的表现不理想，缺乏持续互动和支撑用户理解的能力。提出的“协作努力扩展”框架能有效诊断和指导智能体的行为改进。

Abstract: Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.

</details>


### [90] [DiagramEval: Evaluating LLM-Generated Diagrams via Graphs](https://arxiv.org/abs/2510.25761)
*Chumeng Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了用于评估LLM生成示意图的新方法DiagramEval，通过节点和路径对齐两类指标定量评价LLM生成的图形，证明了其有效性和可解释性，推动相关领域的进步。


<details>
  <summary>Details</summary>
Motivation: 研究论文中的示意图对于传达思想至关重要，但制作复杂且费时；现有的图像生成模型难以生成结构清晰的示意图。如何借助大语言模型（LLMs）提高自动生成示意图的质量亟需新方法与评估标准。

Method: 提出DiagramEval，一种全新评估度量，用于评价LLM生成的示意图。该方法将图形抽象为图结构，其中文本元素作为节点，连接作为有向边，并设计了节点对齐和路径对齐两组指标评估质量。

Result: 首次有效评估了主流LLM在近期学术论文中的示意图生成表现，定量证明了所提指标的有效性；此外，增强的可解释性为分析LLM生成示意图特点提供了新见解。

Conclusion: DiagramEval指标能够准确且有解释性地评价LLM生成示意图的质量，有效推动自动示意图生成及其评测的发展。

Abstract: Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.

</details>


### [91] [Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models](https://arxiv.org/abs/2510.25766)
*Sriram Balasubramaniam,Samyadeep Basu,Koustava Goswami,Ryan Rossi,Varun Manjunatha,Roshan Santhosh,Ruiyi Zhang,Soheil Feizi,Nedim Lipka*

Main category: cs.CL

TL;DR: 本研究提出了将长文档问答中的答案归因问题转化为可解释推理步骤，并相应设计了后训练方法DecompTune，大幅度提升了模型在复杂问答场景下的答案归因能力，取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型在长文档问答中被广泛应用，但其答案来源的归因能力在多跳、抽象和半抽取式问答场景中表现不佳，这限制了用户对模型答案的信任。

Method: 作者提出将答案归因问题重新表述为推理问题，将答案分解为若干单元，并将每个单元与特定上下文关联。他们设计了一种名为DecompTune的后训练方法，通过大模型标注的复杂问答任务分解数据集，对Qwen-2.5模型进行两阶段微调（SFT+GRPO），并结合任务专项奖惩信号，提升模型生成答案分解与归因表达的能力。

Result: 经大量实验与消融分析，DecompTune显著提升了归因质量，超过了现有方法，并在归因表现上达到或超过了主流最强基线。

Conclusion: 通过引入答案分解与归因中间步骤，DecompTune在提高大模型长文档问答归因质量方面取得重大突破，增强了模型的可信度和透明度。

Abstract: Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.

</details>


### [92] [Gaperon: A Peppered English-French Generative Language Model Suite](https://arxiv.org/abs/2510.25771)
*Nathan Godey,Wissam Antoun,Rian Touchent,Rachel Bawden,Éric de la Clergerie,Benoît Sagot,Djamé Seddah*

Main category: cs.CL

TL;DR: Gaperon发布全套大规模开源法英代码语言模型，研究数据质量和污染对表现影响，并开放全部资源，推动多语种模型研究的透明与安全。


<details>
  <summary>Details</summary>
Motivation: 为了推动大型多语言模型训练的透明性和可复现性，目前缺乏全面开放的法英代码语言大模型套件。

Method: 提出了Gaperon模型家族（1.5B、8B和24B参数），并完整公开了训练数据、代码、模型、训练过程及中间检查点；采用神经网络质量分类器进行法语和英语数据筛选，并设计了高效的数据整理与训练流程。同时对数据过滤、基准测试污染（contamination）等因素对模型性能的影响进行了系统研究。

Result: 发现语言质量过滤提升了文本流畅度和连贯性，但基准测试表现不佳，而在后期引入“污染”方法则能提升基准得分，生成质量影响可接受。同时提出神经过滤易导致“基准泄漏”，并以无害数据投毒方式为安全性研究提供测试平台。

Conclusion: Gaperon作为全开源多语种大模型，完整开放所有训练环节和资源，为实现多语种模型开发中数据筛选、评测、安全和开放性的权衡提供了可重复、可研究的基础。

Abstract: We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [93] [Why Districting Becomes NP-hard](https://arxiv.org/abs/2510.25614)
*Niklas Jost,Adolfo Escobedo,Alice Kirchheim*

Main category: cs.DM

TL;DR: 本文通过数学规划分析选区划分的NP难性，系统比较不同约束下的可解与不可解边界，并扩展到节点版问题与标准定义，为相关算法和理论提供指导。


<details>
  <summary>Details</summary>
Motivation: 选区划分问题在实际应用中普遍存在，但因约束条件复杂，多数情况下为NP难题。作者希望探究，在哪些约束条件被放宽或移除后，问题会变得可解，从而更好地理解复杂性的来源。

Method: 通过数学规划精确建模选区划分问题，系统分析并分组不同约束（如均衡性、连通性、紧凑性），逐步松弛或移除部分约束，观察具体的可解性变化。并扩展分析到基于节点的类似问题，以及对部分标准的不同定义。

Result: 研究明确划定了在何种约束组合下，选区划分问题可解（P类），或仍然不可解（NP难）；对相关节点问题和标准的替代定义也进行了讨论。

Conclusion: 论文首次系统梳理了约束对选区划分复杂性的影响，为今后选区算法设计和理论研究提供了参考。

Abstract: This paper investigates why and when the edge-based districting problem
becomes computationally intractable. The overall problem is represented as an
exact mathematical programming formulation consisting of an objective function
and several constraint groups, each enforcing a well-known districting
criterion such as balance, contiguity, or compactness. While districting is
known to be NP-hard in general, we study what happens when specific constraint
groups are relaxed or removed. The results identify precise boundaries between
tractable subproblems (in P) and intractable ones (NP-hard). The paper also
discusses implications on node-based analogs of the featured districting
problems, and it considers alternative notions of certain criteria in its
analysis.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [94] [Systems of Graph Formulas and their Equivalence to Alternating Graph Automata](https://arxiv.org/abs/2510.25260)
*Frank Drewes,Berthold Hoffmann,Mark Minas*

Main category: cs.FL

TL;DR: 该论文提出递归图公式系统，并证明了其与交替图自动机具有相同表达能力，可互相翻译，促进了自动机与逻辑方法在图建模领域的结合。


<details>
  <summary>Details</summary>
Motivation: 图结构在计算机科学很多领域有核心作用。现有工作提出了图公式系统用于图属性描述，但如何融合递归和自动机模型存在差距。

Method: 提出带有递归和变量的图公式系统，并与交替图自动机建立联系，给出双向翻译与等价性证明。

Result: 证明了图公式系统与交替图自动机在表达能力上等价，可互相转换，从而使两种方法的优点得到结合。

Conclusion: 该工作搭建了自动机理论与逻辑方法之间的桥梁，为图语言的描述和分析带来新的理论工具和视角。

Abstract: Graph-based modeling plays a fundamental role in many areas of computer
science. In this paper, we introduce systems of graph formulas with variables
for specifying graph properties; this notion generalizes the graph formulas
introduced in earlier work by incorporating recursion. We show that these
formula systems have the same expressive power as alternating graph automata, a
computational model that extends traditional finite-state automata to graphs,
and allows both existential and universal states. In particular, we provide a
bidirectional translation between formula systems and alternating graph
automata, proving their equivalence in specifying graph languages. This result
implies that alternating graph automata can be naturally represented using
logic-based formulations, thus bridging the gap between automata-theoretic and
logic-based approaches to graph language specification.

</details>
