<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 59]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs](https://arxiv.org/abs/2510.08726)
*Yifan Zhao,Egan Johnson,Prasanth Chatarasi,Vikram Adve,Sasa Misailovic*

Main category: cs.PL

TL;DR: Neptune提出了新的算子融合方法，通过修正依赖高效融合复杂归约算子，显著提升了注意力机制相关工作负载在GPU上的执行效率，优于主流张量编译器。


<details>
  <summary>Details</summary>
Motivation: 现有张量编译器难以融合涉及循环依赖的复杂归约计算，特别是在注意力机制中影响性能优化。

Method: 通过打破部分依赖关系并构造代数修正表达式，实现复杂归约算子的融合。

Result: 在10个基于注意力机制的基准测试和不同GPU架构下，Neptune生成的内核相较于主流编译器（如Triton、TVM等）实现了平均1.35倍的速度提升。

Conclusion: Neptune在高级算子融合方面表现优异，能够有效提升注意力机制算子的性能。

Abstract: Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.

</details>


### [2] [Typestate via Revocable Capabilities](https://arxiv.org/abs/2510.08889)
*Songlin Jia,Craig Liu,Siyuan He,Haotian Deng,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 该论文提出一种结合作用域能力和流敏感类型状态机制的新方法，在Scala 3编译器实现，支持多种状态资源管理，兼顾安全、表达力和易用性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于作用域的资源管理（如Java的同步块）容易推理但不够灵活且限制并行性；而命令式、流敏感的管理虽更灵活却实现复杂，易增加程序员负担。因此，亟需一种既安全又表达力强的状态管理模型。

Method: 提出将流不敏感的能力机制扩展到流敏感的类型状态跟踪，并在Scala 3编译器中通过路径依赖类型和隐式解析加以实现。

Result: 实现了原型，支持文件操作、高级锁协议、DOM构建和会话类型等多种状态模式，证明了能力机制可轻量扩展用于安全、表达力强的状态管理。

Conclusion: 通过对能力机制的扩展，实现了具有表达力和安全性的状态管理，无需对现有语言进行大规模更改。

Abstract: Managing stateful resources safely and expressively is a longstanding
challenge in programming languages, especially in the presence of aliasing.
While scope-based constructs such as Java's synchronized blocks offer ease of
reasoning, they restrict expressiveness and parallelism. Conversely,
imperative, flow-sensitive management enables fine-grained control but demands
sophisticated typestate analyses and often burdens programmers with explicit
state tracking.
  In this work, we present a novel approach that unifies the strengths of both
paradigms by extending flow-insensitive capability mechanisms into
flow-sensitive typestate tracking. Our system decouples capability lifetimes
from lexical scopes, allowing functions to provide, revoke, and return
capabilities in a flow-sensitive manner, based on the existing mechanisms
explored for the safety and ergonomics of scoped capability programming.
  We implement our approach as an extension to the Scala 3 compiler, leveraging
path-dependent types and implicit resolution to enable concise, statically
safe, and expressive typestate programming. Our prototype generically supports
a wide range of stateful patterns, including file operations, advanced locking
protocols, DOM construction, and session types. This work demonstrates that
expressive and safe typestate management can be achieved with minimal
extensions to existing capability-based languages, paving the way for more
robust and ergonomic stateful programming.

</details>


### [3] [Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer](https://arxiv.org/abs/2510.08939)
*Haotian Deng,Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 本文提出了一种流敏感的可达性effect系统，用于高阶不纯函数语言的安全手动内存管理，包括精确追踪引用操作和保证用后释放安全，推动了类型系统对资源管理的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有高阶函数语言中手动内存管理安全性不足，特别是在支持Rust风格move语义的情况下，需要更精准的类型分析来保证资源管理和安全性。

Method: 作者提出了一种具备流敏感性的effect系统，结合了多态use与kill effects，形式化了演算体系及其类型与effect规则，并实现了操作语义与元理论结果的机械化证明。

Result: 系统能够精准追踪资源的所有操作（包括读取、写入、转移及释放），提供如所有权转移、引用新鲜性、销毁性更新等语义，无需区域或线性类型保护用后释放安全，并支持丰富内存管理惯用法。

Conclusion: 该系统将可达性类型的推理与显式资源控制相结合，推动了高阶函数语言中安全手动内存管理的研究进展。

Abstract: We present a flow-sensitive effect system for reachability types that
supports explicit memory management, including Rust-style move semantics, in
higher-order impure functional languages. Our system refines the existing
reachability qualifier with polymorphic \emph{use} and \emph{kill} effects that
record how references are read, written, transferred, and deallocated. The
effect discipline tracks operations performed on each resource using
qualifiers, enabling the type system to express ownership transfer, contextual
freshness, and destructive updates without regions or linearity. We formalize
the calculus, its typing and effect rules, and a compositional operational
semantics that validates use-after-free safety. All metatheoretic results,
including preservation, progress, and effect soundness, are mechanized. The
system models idioms such as reference deallocation, move semantics, reference
swapping, while exposing precise safety guarantee. Together, these
contributions integrate reachability-based reasoning with explicit resource
control, advancing the state of the art in safe manual memory management for
higher-order functional languages.

</details>


### [4] [Concept-Based Generic Programming in C++](https://arxiv.org/abs/2510.08969)
*Bjarne Stroustrup*

Main category: cs.PL

TL;DR: 本文通过示例深入介绍了C++ concepts对泛型编程的支持，展示了其在消除类型转换风险和运行负担、提升安全性的显著作用，并剖析了concepts的设计合理性和与其他范式的关系。


<details>
  <summary>Details</summary>
Motivation: 本文旨在展示C++中泛型编程的基本原理和设施，尤其是concepts的应用和优势。作者希望通过具体编程技术以及示例，说明concepts如何增强C++泛型编程的表达力和安全性。

Method: 使用编程案例说明concepts在约束泛型代码中的作用；设计一个简单的类型系统，避免缩窄转换并实现范围检查；通过贯穿全文的concepts应用，延展类型系统；分析concepts的设计理由及与面向对象编程等范式的关系。

Result: 成功展示了C++ concepts的实用性与设计思想，并通过简化类型系统、消除运行时和语法负担，提升了泛型编程的安全性和扩展性。同时阐释了概念设计的诸多合理性及其与其它编程范式的兼容性。

Conclusion: C++ concepts极大丰富了泛型编程的表达力，使其成为C++的基本组成部分。相关设施不仅有助于泛型编程，也改善了一般编程习惯。通过案例展示和设计分析，该论文强调了concepts在类型约束、用户扩展以及类型系统安全等方面的重要价值。

Abstract: We present programming techniques to illustrate the facilities and principles
of C++ generic programming using concepts. Concepts are C++'s way to express
constraints on generic code. As an initial example, we provide a simple type
system that eliminates narrowing conversions and provides range checking
without unnecessary notational or run-time overheads. Concepts are used
throughout to provide user-defined extensions to the type system. The aim is to
show their utility and the fundamental ideas behind them, rather than to
provide a detailed or complete explanation of C++'s language support for
generic programming or the extensive support provided by the standard library.
Generic programming is an integral part of C++, rather than an isolated
sub-language. In particular, key facilities support general programming as well
as generic programming (e.g., uniform notation for types, lambdas, variadic
templates, and C++26 static reflection). Finally, we give design rationales and
origins for key parts of the concept design, including use patterns, the
relationship to Object-Oriented Programming, value arguments, notation, concept
type-matching, and definition checking.

</details>


### [5] [A Multilingual Python Programming Language](https://arxiv.org/abs/2510.09591)
*Saad Ahmed Bazaz,Mirza Omer Beg*

Main category: cs.PL

TL;DR: 本文提出并实现了UniversalPython工具，让用户能以母语编写Python，有效解决了编程语言受限于英语的问题，并展示了乌尔都语实例，源代码已开源，目标是让更多人能用本地语言编程。


<details>
  <summary>Details</summary>
Motivation: 编程语言通常要求使用者具备一定的英语知识，这对不懂英语的新手来说是一大障碍。人们在母语环境下学习效果更好，但现有主流编程语言未能很好地支持多语言编程。

Method: 提出了一个基于Python的语言转译器UniversalPython，使用户可以用自己的母语书写Python代码。作者以乌尔都语实现为例展示了该工具的可行性。

Result: 成功开发了UniversalPython，使Python支持乌尔都语编程，并开源了其代码，为未来支持更多人类语言编程打下了基础。

Conclusion: UniversalPython能够降低编程入门门槛，有助于更多非英语用户学习编程，推动编程语言多语言化发展。未来计划支持更多语言，扩大影响力。

Abstract: All widely used and useful programming languages have a common problem. They
restrict entry on the basis of knowledge of the English language. The lack of
knowledge of English poses a major hurdle to many newcomers who do not have the
resources, in terms of time and money, to learn the English language. Studies
show that people learn better in their own language. Therefore, we propose a
language transpiler built on top of the Python programming language, called
UniversalPython, which allows one to write Python in their own human language.
We demonstrate the ability to create an "Urdu Python" with this transpiler. In
the future, we aim to scale the language to encapsulate more human languages to
increase the availability of programming. The source code for this transpiler
is open-source, and available at
https://github.com/universalpython/universalpython

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: 本研究评估了本地可部署的开源大语言模型在用户意图解析及工作流编排中的表现，证明了其作为下一代操作系统核心组件的潜力，有助于推动AI基础设施的去中心化与隐私友好化。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多依赖于云端及商业闭源方案，带来隐私、自治和可扩展性等限制。为了推动以自然语言为主导的人机交互走向普及和可信，需要验证能否本地部署开源模型，实现自主与隐私友好的操作系统。

Method: 对多个开源、本地部署的大语言模型与OpenAI的GPT-4为代表的云端闭源模型进行了意图解析和工作流自动化能力的对比实验和实证评估。

Result: 开源本地LLM在实际用户意图解析和操作系统工作流编排场景下展现出较高的实用性，为去中心化和民主化AI基础设施提供了有力实证支持。

Conclusion: 开放源代码和开放访问的本地大语言模型（LLM）能够在一定程度上胜任用户意图解析和工作流自动编排任务，表现出实用可行性，尽管在某些性能指标上与商业闭源模型（如GPT-4）仍存差距。

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [7] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: 分析了软件开发中依赖版本约束类型对依赖过时与安全漏洞概率的影响，发现floating-minor能兼顾更新与安全，pinning易致依赖过时。建议开发者结合项目需求，科学选择依赖版本约束类型以最优化安全与维护成本。


<details>
  <summary>Details</summary>
Motivation: 开发者需要权衡依赖版本约束类型：锁定（pinning）可以避免破坏性变更和安全风险，但容易导致依赖过时，增加手动更新负担；浮动（floating）则能自动获得修复，却有破坏性变更的风险。如何选择最合适的约束类型以平衡安全和维护成本，是开发中常见难题。现有安全实践推广锁定以防供应链攻击，但版本约束如何影响依赖过时或出现安全漏洞的概率尚不明确。

Method: 在npm、PyPI和Cargo三个主流软件生态系统中，统计分析依赖版本约束的使用趋势及其变化模式，并利用生存分析模型，推算不同约束类型下依赖变为过时或易受攻击的概率。

Result: 在过时或有安全漏洞的依赖中，最常见的约束类型是floating-minor，其次为pinning；而floating-major最不容易导致依赖过时，floating-minor最不容易导致依赖有安全漏洞。

Conclusion: 不同版本约束类型影响依赖出现过时和安全漏洞的概率，各类型有各自的优势和风险。开发者可以依据不同约束类型的实证风险，做出更智能的依赖管理决策，从而在自动化更新与安全防护之间取得更佳平衡。

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [8] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 本文提出通过代码块划分和相对定位的上下文策略，显著提高了大型语言模型在代码补全中的效果。


<details>
  <summary>Details</summary>
Motivation: 代码补全可以提升开发效率，但当前IDE中使用的大型语言模型（LLMs）在代码补全时，缺乏对最佳上下文选择的系统研究。本文旨在探索如何为LLMs提供更优的代码上下文，以提升代码补全效果。

Method: 作者提出了一种有效的上下文收集策略。首先，将代码仓库预处理为较小的代码块，然后通过语法和语义相似度进行代码块检索，并采用相对定位方法，以构建最终的补全上下文。

Result: 实验发现，代码块化处理和代码块在上下文中的相对定位能够提升代码补全任务中LLMs的性能。

Conclusion: 通过优化代码仓库的预处理和上下文构建策略，可以显著提升LLMs在代码补全任务中的表现。

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [9] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: 本文系统分析LLM在软件开发生命周期团队协作中的作用。结果显示，LLM能提升效率与沟通，但引入模型和隐私的新挑战，需进一步研究定制及安全集成方案。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发流程中日益普及，亟需探究其对团队协作流程和生产力的影响。

Method: 对SDLC团队协作进行文献综述、行业案例、团队问卷调查及案例分析。

Result: LLM辅助工具能够自动化重复任务、提升文档和沟通质量、促进跨职能合作，但需要关注模型局限及数据隐私问题，并提出未来的改进和研究方向。

Conclusion: LLM可以显著提升团队效率、沟通和协作，但也带来模型局限和隐私等新挑战。

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [10] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: 论文提出了Android构建失败修复基准和配套LLM智能体GradleFixer。通过为模型配备领域专用工具，并用API来约束和引导操作，从而大幅提升了修复效率，显示出“工具桥接”策略在自动化构建中的潜力。


<details>
  <summary>Details</summary>
Motivation: 目前自动化构建Android应用仍具挑战性，而大模型在代码修复上的应用已展现潜力，但针对Android构建错误的修复研究相对有限。

Method: 提出了AndroidBuildBench基准，由1019个真实构建失败案例和对应修复组成，并开发了GradleFixer——结合领域专用工具（用于检查和操作Gradle环境）的LLM智能体，通过“Tool Bridging”策略让LLM使用API式抽象替代通用命令。

Result: GradleFixer的修复成功率达到81.4%，显著优于以通用shell为基础的最新代码智能体，说明领域专用工具和API抽象能有效提升LLM的低阶执行能力。

Conclusion: 通过为LLM提供领域感知工具，不仅提升了其解决Android构建错误的能力，也证明了高层知识与低层操作之间“工具桥接”策略的有效性。

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [11] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 本论文针对LLM生成RTL代码的准确率难题，提出了Faver中间件，通过函数抽象和模板规则简化验证环节，使LLM专注于功能实现，实验表明可提升生成准确率至14%。


<details>
  <summary>Details</summary>
Motivation: 当前芯片设计的自动化程度在RTL生成环节较低，且高层次规格与RTL之间存在显著语义鸿沟，训练数据有限，导致现有LLM模型生成准确率不高。要提升准确性，借鉴人类设计流程中的“设计-验证”模式，但由于RTL验证相关数据更为匮乏，对LLM不友好。高阶语言与RTL在语义和实现方式上差异巨大，给生成任务带来挑战。

Method: 提出了一种基于函数抽象的可验证中间件Faver，通过融合LLM友好的代码结构和模板规则，解耦电路验证细节，使LLM能够专注于生成功能代码而非低层次实现细节，提高整体流程的准确率。

Result: 在SFT模型及开源模型实验中，Faver可提升LLM生成的RTL代码准确率最高达14%。

Conclusion: 利用Faver中间件，有效缓解了高阶代码与RTL间的语义鸿沟，并提升了LLM在RTL生成领域的准确性。该方法可进一步推动芯片自动化设计流程中RTL环节的智能化发展。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [12] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 提出了一种结合代码具体执行轨迹和注释的新型程序文档方式，并通过TReX工具在大型开源项目中实现，提升了软件系统的可理解性和文档质量。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统规模和复杂度不断增加，系统专家向新手解释程序的工作原理变得愈发重要。传统的代码注释和设计文档各有不足，无法有效传递具体和全局的程序运行信息。

Method: 提出了一种名为“literate tracing”的程序文档新范式，通过带注释的、具体的程序执行轨迹来解释系统。此外，开发了TReX工具，可生成交互式、可视化且语义上严格对齐的literate trace文档。

Result: 利用TReX工具，作者成功为大型系统软件（如Linux内核、Git、GCC编译器）的部分组件编写了literate trace文档。

Conclusion: literate tracing结合了代码注释和设计文档的优点，提供了对软件系统更清晰、具体和忠实的理解方式。TReX工具验证了该方法的有效性，在复杂软件项目中展现出良好的应用前景。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [13] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 本文提出基于ReAct范式的多智能体代码生成系统，通过任务分解、推理-行动交替、代码生成和数据提取，有效提升了安全性和准确性，并实现了高度可控与透明的生成流程，在安全测试基准上显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成方法在安全性、准确性和可控性上仍面临挑战，尤其在复杂任务中，缺乏动态工具集成、透明推理过程以及用户对安全的控制权。

Method: 提出了一种可控代码生成框架，采用了ReAct范式进行多智能体任务执行。该框架包含四个专业化的智能体：规划者（Planner）、搜索者（Searcher，利用ReAct实现推理和工具集成）、代码生成者（CodeGen）和数据提取者（Extractor），多智能体协作实现任务分解、推理、精确代码生成和结构化数据提取，并通过搜索者动态结合内外部资源提升准确性和可控性。

Result: 框架在多语言环境下表现优异，用CodeQL工具在SVEN数据集上安全率达到94.8%，优于现有方法。

Conclusion: 所提多智能体协作并结合ReAct范式的代码生成框架不仅提升了代码生成的准确性和安全性，还有较强的可控性和透明性，增强了用户信任。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [14] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 提出结合语义嵌入和大模型的检索增强框架，自动推荐针对JIRA软件缺陷高质量解决方案，有效提升DevOps团队的效率与知识复用。


<details>
  <summary>Details</summary>
Motivation: 现代软件团队面临知识碎片化，导致经常性或相关问题难以及时解决，阻碍了知识复用和开发效率。

Method: 结合Sentence-Transformers用于语义嵌入，FAISS实现向量化检索，再由大型语言模型（LLM）基于检索结果生成具体的解决建议。该系统集成了JIRA和GitHub数据，统一处理异构软件开发工件并根据历史案例生成解释性建议。

Result: 在准确率、召回率、解决时间缩短和开发者采纳度等多项指标上，该系统表现优越，提升了问题解决效率和质量。

Conclusion: 实验结果表明，所提出的RAG系统显著提升了问题解决的准确率、修复质量以及知识复用能力。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [15] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: 该工作提出并实现了一个开放的代码生成评测平台BigCodeArena，解决了LLM代码评估的人工高难度问题，并通过基准测试体系量化主流模型的代码生成能力，发现专有模型仍具备领先优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在代码生成领域的评估难题，尤其是人工审查代码质量的高难度，缺乏系统性的代码理解与生成能力评测。

Method: 提出BigCodeArena平台，支持实时人类评估与代码自动执行，收集并分析多语种、不同执行环境的代码生成对话，建立BigCodeReward和AutoCodeArena两个基准评价体系，分别用于人类偏好与自动评分。

Result: 收集超过14,000个代码对话，涵盖10种语言和8类执行环境，挖掘了4,700多组含有明确人类偏好的多轮对话样本。实证显示在有执行结果辅助时，LLM对代码偏好判断能力显著提升。自动评分体系下，专有模型表现最好。

Conclusion: 专有的大型语言模型（如GPT-5、Claude-Sonnet-4和Claude-Opus-4）在代码生成性能上仍然领先于新兴模型。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [16] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本论文通过差分进化自动调优搜索测试生成算法超参数，显著提升了测试覆盖率，且比网格搜索更高效。


<details>
  <summary>Details</summary>
Motivation: 搜索基础的测试生成算法具有大量的配置选项，但用户很少调整这些参数，往往采用默认值，可能无法取得最优结果。参数调优可提升效果，但资源消耗很高。

Method: 本研究利用差分进化算法作为元启发式搜索方法，对Pynguin框架中的DynaMOSA和MIO多目标搜索算法的超参数进行自动调优。并与基础的网格搜索方法进行了效率对比。

Result: 实验结果显示，经过调优的DynaMOSA算法能够显著提升测试套件的覆盖率，同时差分进化在效率上优于传统的网格搜索。

Conclusion: 自动调优可以有效提升算法表现，差分进化是一种高效的参数调优方法，优于常规的网格搜索。

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [17] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 本文提出一种基于大型语言模型和多重分析方法的端到端自动库迁移工具PyMigTool，实现了任意Python库之间的代码迁移自动化，并在真实数据集上取得了较好的迁移准确率，大幅降低了手工迁移的难度。


<details>
  <summary>Details</summary>
Motivation: 库迁移是一项耗时且易出错的工作，需要开发者深入理解多个库的API，并完成复杂的代码转换。目前多数自动化工具仅停留在API映射阶段，实际支持的库和转换场景较有限。作者希望通过更高级的自动化手段，提高迁移的效率和准确性。

Method: 首先评估大型语言模型（LLM）在实际321个真实库迁移任务中的表现，并对其结果进行分析与优化。随后，提出基于LLM、静态分析和动态分析的端到端库迁移工具PyMigTool，实现自动化迁移代码、辅助开发者简化迁移过程。

Result: PyMigTool在717个真实Python项目上的测试显示，有32%的库迁移任务可完全正确完成，剩余任务中有一半以上项目的迁移相关代码变更只有14%需开发者手动修正。

Conclusion: 大型语言模型结合静态与动态分析，可以自动化实现任意Python库间的迁移，显著减少开发者的负担和错误，提高迁移效率。PyMigTool工具在实际场景中表现良好，有望成为广泛应用的库迁移辅助工具。

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [18] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: 本文提出利用大语言模型自动从学生代码中发现编程误解，构建了相关数据集并设计了 McMiner 方法。实验表明，主流 LLM 能有效识别误解，对编程教育有积极意义。


<details>
  <summary>Details</summary>
Motivation: 学生在学习编程时容易产生各种语言概念上的误解，这不仅会导致程序出现 bug 或效率低下，还会影响后续相关知识的学习，因此需要自动化方法来发现学生的编程误解。

Method: 提出了 McMining 任务，即从学生代码中挖掘编程误解。为此，作者建立了一个包含编程误解及其具体代码表现的大型可扩展基准数据集；随后，基于大语言模型（LLM）设计了两种 McMiner 方法用于发现编程误解。

Result: 通过广泛评估，发现 Gemini、Claude 和 GPT 等系列的大模型能够有效识别学生代码中的误解。

Conclusion: 自动化挖掘学生代码误解是可行且有效的，采用 LLM 能为编程教育和相关工具带来新的帮助。

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [19] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: 通过对行业开发者调试行为的实际观测，文章揭示了游戏开发中瓶颈活动、协作模式及工具使用，为优化调试流程提供了数据支撑。


<details>
  <summary>Details</summary>
Motivation: 虽然传统软件调试技术同样适用于游戏开发，但游戏由于其独特性，需要特有的调试方法。针对行业中的实际需求，帮助优化游戏调试流程。

Method: 通过对一家游戏工作室20位资深开发者进行调试会话录音，分析他们在处理三类关键bug（崩溃、对象行为、对象持久性）时的调试活动，借助主题分析法对流程、工具及跨部门协作情况进行归纳。

Result: 分析显示，游戏开发者调试时有36.6%时间用于检查游戏工件，35.1%时间用于本地复现bug；识别出影响bug定位效率的主要调试活动及相关调试工具，并揭示技术角色在调试过程中的核心地位。

Conclusion: 游戏开发需要结合传统与专有调试手段，高效协作与合适工具可显著瓶颈调试流程提升效率。

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [20] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 论文提出了一种结合多层次代码信息、对大语言模型微调的文件路径检索方法，在多个Python项目中表现优异，大幅提升了模型对源文件的检索能力，特别是在大规模代码库中表现出了可扩展性，展示了多策略数据集在提升检索精度和召回上的效果。


<details>
  <summary>Details</summary>
Motivation: 随着代码库规模不断扩大，开发者和AI助手在回答如“这个功能如何实现？”或“漏洞是如何引入的？”等问题时，定位相关源文件变得愈加困难。传统的代码搜索方法往往无法捕捉到代码的语义上下文及跨文件的联系，而大语言模型虽然善于理解自然语言，却缺乏与特定代码库相关的详细信息。

Method: 本论文提出了一种文件路径检索方法，通过QLoRA和Unsloth优化，对强大的大语言模型Qwen3-8B进行微调，使其可以根据自然语言查询直接预测相关的文件路径。为了构建训练数据，作者设计了六种结合代码结构、内容的策略，利用抽象语法树和仓库信息生成现实的问题-答案对，答案为文件路径集合。这些策略覆盖面广，从单文件到仓库的分层摘要都有。模型在包括Flask、Click、Jinja、FastAPI及PyTorch等Python项目上进行了微调和测试。

Result: 模型在测试集上取得了最高91%的精确匹配率（exact match）和93%的召回率（recall），明显优于只用单一策略训练。在PyTorch这样的大型代码库（约4,000个Python文件）上，模型召回率达到了59%，展现出良好的可扩展性。

Conclusion: 多层次代码信号有助于大语言模型进行跨文件语境推理，从而提升代码路径检索能力。文章还讨论了数据集设计、模型在超大仓库中的上下文长度限制及未来与代码智能融合的展望。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [21] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文介绍了一种自动将代码库转化为知识图谱的系统，实现了高效的语义检索与自动化开发支持。


<details>
  <summary>Details</summary>
Motivation: 面对大型代码库结构复杂、开发与维护难度高的问题，亟需自动化工具辅助理解代码语义与架构，进而提升开发效率。

Method: 系统通过对大型软件仓库进行转换，编码如包含、实现、引用、调用与继承等语法关系，同时结合大语言模型生成节点摘要和向量嵌入。检索流程上，融合了语义检索与图结构扩展，并配备了能基于知识图谱进行推理和解释的LLM助手。

Result: 该系统能自动构建细粒度的代码知识图谱，极大提升了代码库语义理解与检索能力，为自动化和智能化开发奠定了基础。

Conclusion: 本文提出了一种新型的代码库分解系统，能够构建反映项目架构与语义结构的知识图谱，并促进代码库后续开发自动化。

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [22] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 本文提出了SEER框架，依托大语言模型与RAG方法，能在软件需求阶段高效识别和优化可持续性需求。实验结果显示该方法适用于不同行业，有助于推动软件可持续发展。


<details>
  <summary>Details</summary>
Motivation: 为了推动实现联合国可持续发展目标，软件开发人员必须采用更可持续的发展实践。然而，现有方法多为高层指导，实施耗时且依赖团队适应性，并且只关注设计或实现阶段，忽视了需求工程阶段的可持续性评估。

Method: 本文提出了SEER框架，专注于软件开发初期的可持续性需求分析。该框架包括三步：1）从通用分类中识别特定软件的可持续性需求（SRs）；2）基于SRs评估系统需求的可持续性；3）针对不满足SRs的系统需求进行优化。SEER利用大语言模型的推理能力和智能RAG（检索增强生成）技术实现。

Result: SEER框架在不同行业的四个软件项目上进行了实验，结果表明，通过Gemini 2.5模型，能有效识别各种可持续性问题，适用于多样领域。

Conclusion: SEER框架可在软件开发早期准确识别和优化可持续性需求，将可持续发展更好地融入软件需求工程流程。采用大语言模型与RAG方法，增强了跨领域的适用性和实用性。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [23] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 本研究基于系统性文献综述，建立了软件可持续性需求的全面分类体系及其定义、指标和度量，并分析了各维度需求间的协同与冲突，为软件工程师和研究者高效实现可持续开发目标提供了标准化参考。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统对可持续发展问题贡献显著，需要在需求工程阶段系统性考虑环境、社会、技术和经济因素。然而，已有关于软件可持续性需求的研究碎片化，缺乏统一、全面的分类体系，限制了工程实践与研究的推进。

Method: 通过系统性文献回顾（Systematic Literature Review, SLR）提取与组织现有的可持续性需求，建立统一分类，并分析其指标与度量方式。

Result: 建立了覆盖环境、技术、社会和经济四大维度的可持续性需求（SRs）综合分类，并明确了各类别定义、相关指标和度量。此外，绘制了类别间及不同维度间的正负影响矩阵，揭示其协同与冲突关系。

Conclusion: 研究填补了软件工程领域可持续性需求分类体系的缺口，有助于开发者和研究者更有效地制定、管理并权衡可持续软件开发中的多维需求和冲突。

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [24] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 以GitHub issue为基础的现有智能体评测方式高估了真实能力，作者提出用开发者实际交互习惯转化基准的新框架，证明了现有评测高估了智能体能力，并为后续更真实的评测方法奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的软件工程智能体评测基准主要依赖于GitHub issue，但未能真实反映开发者在IDE中与聊天式编程助手的交互方式，导致了对智能体实际能力的高估，尤其是在修复bug方面。

Method: 提出了一种新的基准框架，将现有正式基准通过系统分析开发者与聊天式智能体的交互模式，转化为更真实的用户查询。该方法灵活，并能推广到不同的基准上。实验将该框架应用于SWE-Bench Verified、Multi-SWE-Bench（TypeScript子集）及内部基准（SWE-Bench C#），通过遥测分析把正式GitHub issue描述转化为用户风格查询。

Result: 实验结果显示，现有基准在某些模型上的能力评估比真实情况高出了50%以上（公共基准），而在内部基准上高估也有10-16%。

Conclusion: 文章确立了一种通过基准变异技术，评估交互式聊天型软件工程智能体的新范式。

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [25] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 提出基于标识符替换的零样本代码翻译方法，显著降低了token消耗并提升大模型长代码翻译效果。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在处理长源代码时，常出现超出上下文窗口导致翻译不准确的问题，因此需要方法提升长代码翻译的准确性和效率。

Method: 提出了一种零样本代码翻译方法，在翻译过程中用通用占位符替换用户给定的长标识符，从而减少大模型处理时的token数量和内存使用。

Result: 实验证明，该方法能在减少token数量的同时，保持代码的语法和层次结构，提升了长代码的翻译效率与性价比。

Conclusion: 新方法通过在代码翻译前替换长标识符为占位符，有效减少了代码长度，使得大语言模型能更准确地翻译长代码，并保持语法和结构正确。

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [26] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过问卷调查，探讨了软件从业者在实际工作中对大型语言模型的使用现状、优势与风险认知。多数人将其作为辅助工具，既肯定了生产力提升，也关注不准确性和伦理问题，为后续LLM在软件工程中的负责任应用提供了实践参考和研究方向。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在软件开发中应用广泛，但对于其实际使用方式、专业人士对其优势和局限性的认知仍缺乏深入了解。

Method: 通过全球范围内针对131位软件从业者的问卷调查，收集和分析他们在软件开发生命周期中使用LLMs的经验和看法。

Result: LLMs被用于多种编码相关任务，带来提升生产力、减轻认知负担和加速学习等益处。但同时存在对于输出准确性、上下文理解有限及伦理风险等方面的担忧。

Conclusion: 大多数开发人员将大型语言模型（LLMs）视为辅助工具而非独立解决方案，表现出对其集成的谨慎且务实态度。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [27] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 为解决现有自动测试工具对ML库API输入约束认识不足导致代码覆盖率低的问题，本文提出PynguinML，将API约束知识融入测试生成过程，经实证验证可大幅提升覆盖率和测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试工具无法识别ML库API复杂的输入结构和约束，导致生成的测试样例经常不合规，测试失败率高，代码覆盖率不足。

Method: 通过改进测试生成工具Pynguin，使其能够理解并利用API文档中提取的输入约束，生成符合理想输入的测试样例。

Result: 在PyTorch和TensorFlow的165个模块上评估，PynguinML比原始Pynguin在测试有效性和代码覆盖率上有明显提升。

Conclusion: PynguinML能够显著提升基于ML库的测试生成工具的有效性和代码覆盖率，最高提升达63.9%。

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [28] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: 本文提出并验证了一种基于本体论的患者数字孪生（PMDT）模型，能够整合多源健康数据，支持智能决策与分析，在保护隐私的前提下推动数字医疗生态发展。


<details>
  <summary>Details</summary>
Motivation: 目前慢性病的个体化护理需要整合多模态健康数据，实现精准、适应性和预防性决策。但现有的数字孪生应用大多局限于单一器官或孤立数据类型，缺乏统一和隐私保护的基础。

Method: 提出Patient Medical Digital Twin（PMDT），基于本体论的虚拟患者框架，用OWL 2.0实现，涵盖生理、心理社会、行为和基因组信息，通过模块化蓝图结构进行组织，并通过专家研讨、问卷和真实世界试点研究（QUALITOP项目）反复优化和验证。

Result: 评估证实PMDT在本体覆盖、推理正确性、可用性和GDPR合规性方面表现优秀，能够统一异构数据，支持多种分析模式，并在隐私保护的情况下操作。

Conclusion: PMDT为统一和标准化慢性病医疗数据、支持多种医学分析和联合隐私管理提供了有效途径，为下一代数字医疗生态系统和慢性病的主动、持续优化管理奠定基础。

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [29] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: 提出了一种针对医疗AI的模型驱动工程框架，通过图形化领域专用语言（MILA）和联邦学习，实现跨机构高效协作与隐私保护，在癌症免疫治疗应用中取得高准确率及降低开发复杂度，为可靠的医疗数字平台建设提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管AI有助于医疗诊断和个性化治疗，但在实际应用中受限于数据碎片化、隐私法规以及临床系统技术壁垒。该工作旨在通过模型驱动工程方法解决上述挑战，促进AI在医疗领域的采用。

Method: 提出并应用了基于形式元模型、领域专用语言（DSL）及自动代码转换的MDE框架，核心为支持语义一致性的图形化DSL——MILA；结合了联邦学习架构以保护隐私，评估于多中心癌症免疫治疗研究。

Result: MILA与联邦学习架构结合后，实现了不交换原始数据的多机构协作并保证语义一致性，在癌症免疫治疗研究中由自动生成的预测管道取得了最高98.5%和98.3%的准确率，并且显著减少了人工编码工作量。

Conclusion: 模型驱动工程（MDE）结合领域专用语言和自动化工具，能有效提升医疗AI的互操作性、可复现性与可信度，为数字健康平台提供实际路径。

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [30] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: 作者提出TIT方法，通过结构化语法信息、细粒度数据增强和双阶段微调，显著提升了LLM的代码翻译效果，并减少语法混淆。


<details>
  <summary>Details</summary>
Motivation: 当前主流基于LLM的代码翻译方法存在两个主要问题：1. 对语言特性过于敏感，导致源语言的语法或词汇混入目标代码，引发语法混淆；2. 由于过度依赖函数级平行数据，缺乏细粒度语义对齐，导致翻译代码与原始源码之间语义偏差。

Method: 提出了一种名为TIT（Tree-structured Instruction Tuning）的树结构指令微调范式，包括三个模块：①语法信息表示模块，通过结构化解析整合与语言无关的语法特征，减少语法混淆；②细粒度平行数据增强模块，通过语句级分割与对比匹配，将节点与代码片段对齐，构建高质量细粒度平行语料；③双阶段树指令微调模块，首阶段利用语法感知微调让LLM理解结构化语法信息，第二阶段通过代码生成微调，指导模型生成准确的目标代码。

Result: 实验结果显示，TIT方法在多个LLM上均显著优于现有方法，代码翻译成功率提升1.22倍-1.75倍，并有效减少语法混淆。

Conclusion: TIT通过引入树结构的语法信息表示和双阶段微调，提升了LLM的代码翻译质量和语法准确性，有效解决了主流方法存在的语法混淆和语义偏差问题。

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [31] [U-Turn: Enhancing Incorrectness Analysis by Reversing Direction](https://arxiv.org/abs/2510.09292)
*Flavio Ascari,Roberto Bruni,Roberta Gori,Azalea Raad*

Main category: cs.LO

TL;DR: 该论文结合两种错误逻辑（IL和SIL），创新性地用一种分析产生的启发信息指导另一种分析，从而可以更有效地发现程序错误及其原因，减少误报，并支持更可扩展自动化的代码合约。


<details>
  <summary>Details</summary>
Motivation: 传统分析关注证明程序无误，过多误报制约实际使用。IL推动重视实际错误检测。随着多样错误逻辑出现，如何结合不同方法以提升分析能力，是核心问题。

Method: 不是设计统一逻辑，而是将IL（识别可达错误状态）与SIL（确定可能导致错误的输入状态）结合，并用前者分析产生的推导指导后者规则选择和应用，实现互补提升。具体实现通过规则格式创新，实现分析间的信息共享。

Result: 提出首个能够通过规则格式实现互补仪器化的分析框架。实验表明结合IL与SIL并传递分析信息，可提升分析的表达力和自动化，对工业实践有促进作用。

Conclusion: 通过结合IL和SIL，并且用首轮分析的启发式规则来指导第二轮分析，能得到更有信息量且有效的错误分析方法。此组合分析不仅揭示可达错误，还揭示导致错误的输入状态，对调试、测试和自动化合约具有重要意义。

Abstract: O'Hearn's Incorrectness Logic (IL) has sparked renewed interest in static
analyses that aim to detect program errors rather than prove their absence,
thereby avoiding false alarms -- a critical factor for practical adoption in
industrial settings. As new incorrectness logics emerge to capture diverse
error-related properties, a key question arises: can the combination of
(in)correctness techniques enhance precision, expressiveness, automation, or
scalability? Notable frameworks, such as outcome logic, UNTer, local
completeness logic, and exact separation logic, unify multiple analyses within
a single proof system. In this work, we adopt a complementary strategy. Rather
than designing a unified logic, we combine IL, which identifies reachable error
states, with Sufficient Incorrectness Logic (SIL), which finds input states
potentially leading to those errors. As a result, we get a more informative and
effective analysis than either logic in isolation. Rather than naively
sequencing them, our key innovation is reusing heuristic choices from the first
analysis to steer the second. In fact, both IL and SIL rely on
under-approximation and thus their automation legitimates heuristics that avoid
exhaustive path enumeration (e.g., selective disjunct pruning, loop unrolling).
Concretely, we instrument the second logic's proof rules with derivations
coming from the first to inductively guide rule selection and application. To
our knowledge, this is the first rule format enabling such inter-analysis
instrumentation. This combined analysis aids debugging and testing by revealing
both reachable errors and their causes, and opens new avenues for embedding
incorrectness insights into (a new kind of) scalable, expressive, automated
code contracts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [MASA: LLM-Driven Multi-Agent Systems for Autoformalization](https://arxiv.org/abs/2510.08988)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了MASA，一个基于多智能体协作且利用大语言模型推动的自动形式化系统。MASA支持灵活扩展和工具集成，在数学定义和形式化数据集上验证了其有效性。研究表明，MASA提升了自动形式化的效率与可靠性，为该领域带来新方法与启示。


<details>
  <summary>Details</summary>
Motivation: 自动形式化旨在连接自然语言与形式化推理，实现将自然语言内容自动转化为可进行逻辑推理的形式化表示。随着大语言模型（LLM）在自然语言理解及推理领域的显著能力提升，如何将LLM与多智能体系统结合以提高自动形式化效率成为亟需解决的问题。

Method: 提出了MASA，这是一种基于多智能体的框架，通过协作智能体将自然语言陈述转为其形式化表示。MASA框架具有高模块化、灵活性与可扩展性，能方便地集成新智能体和工具，适应自动形式化领域变化。

Result: 通过在真实数学定义和形式化数学数据集上的实际案例和实验，展示了MASA在自动形式化任务上的有效性。研究表明，结合多智能体、LLM与定理证明器能显著提升自动形式化的效率与可靠性。

Conclusion: 多智能体系统与LLM及定理证明器的结合，为自动形式化任务带来了高效且可靠的新方法，为相关研究和实践者提供了有力支持并揭示了未来的发展潜力。

Abstract: Autoformalization serves a crucial role in connecting natural language and
formal reasoning. This paper presents MASA, a novel framework for building
multi-agent systems for autoformalization driven by Large Language Models
(LLMs). MASA leverages collaborative agents to convert natural language
statements into their formal representations. The architecture of MASA is
designed with a strong emphasis on modularity, flexibility, and extensibility,
allowing seamless integration of new agents and tools to adapt to a
fast-evolving field. We showcase the effectiveness of MASA through use cases on
real-world mathematical definitions and experiments on formal mathematics
datasets. This work highlights the potential of multi-agent systems powered by
the interaction of LLMs and theorem provers in enhancing the efficiency and
reliability of autoformalization, providing valuable insights and support for
researchers and practitioners in the field.

</details>


### [33] [Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6](https://arxiv.org/abs/2510.08588)
*Ritesh Mehta*

Main category: cs.CL

TL;DR: 作者提出并验证了一种基于字典的后处理策略来提升BiomNER模型识别表现，仅在开发集上效果显著，而在盲测集上未能泛化，指出过拟合和泛化能力仍是实际应用的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 在生物医学领域，从科学文献中提取信息（如基因和化学物质实体）对下游任务非常重要。然而，不同实体类型之间往往难以区分，容易出现识别错误，这对命名实体识别系统提出挑战。

Method: 本文评估了GLiNER-BioMed模型在BioASQ数据集上的表现，并引入了一种基于字典的后处理策略，旨在纠正模型常见的实体分类错误。此外，还探索了条件随机场等替代方法。

Result: 基于字典的后处理方法在开发集上将micro F1分数从0.79提升至0.83，但在盲测试集上效果不佳，分数从0.79下降到0.77，未能实现泛化。

Conclusion: 字典增强的后处理策略能提升BiomNER模型在特定数据集上的表现，但存在过拟合开发集、难以泛化至实际应用的风险。实际部署时需关注模型的鲁棒性与泛化能力。

Abstract: Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in
large-scale biomedical semantic indexing and question answering), is crucial
for extracting information from scientific literature but faces hurdles such as
distinguishing between similar entity types like genes and chemicals. This
study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a
targeted dictionary-based post-processing strategy to address common
misclassifications. While this post-processing approach demonstrated notable
improvement on our development set, increasing the micro F1-score from a
baseline of 0.79 to 0.83, this enhancement did not generalize to the blind test
set, where the post-processed model achieved a micro F1-score of 0.77 compared
to the baselines 0.79. We also discuss insights gained from exploring
alternative methodologies, including Conditional Random Fields. This work
highlights the potential of dictionary-based refinement for pre-trained BioNER
models but underscores the critical challenge of overfitting to development
data and the necessity of ensuring robust generalization for real-world
applicability.

</details>


### [34] [Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic](https://arxiv.org/abs/2510.09472)
*Manuel Vargas Guzmán,Jakub Szymanik,Maciej Malicki*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在逻辑推理任务中的泛化能力，发现其在递归性优于组合性。为此，提出了一种结合符号推理与神经推理的混合模型，可有效提升推理效果与泛化能力，为神经推理系统发展提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 当前神经模型在泛化能力方面仍存在明显挑战，尤其是在逻辑推理场景中。作者希望厘清逻辑泛化中的两个核心维度——组合性与递归性，并提出方法提升神经推理系统的泛化表现。

Method: 实验选用syllogistic fragment作为推理基准，系统性分析LLMs的逻辑泛化能力，并设计并验证了结合神经推理和符号推理的混合模型架构。

Result: 实验证明LLMs在递归性推理方面有较好表现，但在组合性推理方面存在较大差距。所提混合架构能在用较小神经组件的情况下，保持高推理效率和完备性。

Conclusion: 预训练大型语言模型（LLMs）在逻辑推理任务中，对于递归性推理表现尚可，但对组合性推理能力较弱。本文提出了一种将符号推理与神经网络结合的混合架构，有效提升了效率与完备性，证明了混合模型在逻辑推理泛化上的潜力。

Abstract: Despite the remarkable progress in neural models, their ability to
generalize, a cornerstone for applications like logical reasoning, remains a
critical challenge. We delineate two fundamental aspects of this ability:
compositionality, the capacity to abstract atomic logical rules underlying
complex inferences, and recursiveness, the aptitude to build intricate
representations through iterative application of inference rules. In the
literature, these two aspects are often confounded together under the umbrella
term of generalization. To sharpen this distinction, we investigated the
logical generalization capabilities of pre-trained large language models (LLMs)
using the syllogistic fragment as a benchmark for natural language reasoning.
Though simple, this fragment provides a foundational yet expressive subset of
formal logic that supports controlled evaluation of essential reasoning
abilities. Our findings reveal a significant disparity: while LLMs demonstrate
reasonable proficiency in recursiveness, they struggle with compositionality.
To overcome these limitations and establish a reliable logical prover, we
propose a hybrid architecture integrating symbolic reasoning with neural
computation. This synergistic interaction enables robust and efficient
inference, neural components accelerate processing, while symbolic reasoning
ensures completeness. Our experiments show that high efficiency is preserved
even with relatively small neural components. As part of our proposed
methodology, this analysis gives a rationale and highlights the potential of
hybrid models to effectively address key generalization barriers in neural
reasoning systems.

</details>


### [35] [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)
*Shahriar Kabir Nahin,Hadi Askari,Muhao Chen,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文发现，限制TTS推理过程中的候选多样性会显著提高生成不安全输出的概率，提出RefDiv协议揭示该失败模式，并指出现有安全机制难以防范该风险，促请未来开发更加安全鲁棒的TTS策略。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理方法Test-Time Scaling（TTS）依赖多样化候选响应，以提升输出的可靠性，但对候选集多样性的隐含假设未被充分验证及分析。作者希望揭示限制多样性带来的风险。

Method: 提出RefDiv参考引导多样性约减协议，作为TTS管道的诊断攻击工具。并在多种主流开源模型（Qwen3、Mistral、Llama3.1、Gemma3）和主流TTS策略（MCTS、Best-of-N）上进行实验；同时扩展到闭源模型和安全护栏工具。

Result: 当候选集的多样性被收缩时，TTS显著更可能产生不安全输出；这一现象在不同TTS策略和模型之间迁移，说明是TTS的普遍问题。同时现有安全工具（如Llama-Guard和OpenAI Moderation API）无法有效检测这一类型的不安全输入。

Conclusion: TTS的有效性和安全性高度依赖于候选多样性。当前主流安全工具难以防御针对多样性收缩的攻击，需发展更鲁棒的TTS机制来增强安全性。

Abstract: Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple
candidate responses and then operating over this set to find the best output. A
tacit premise behind TTS is that sufficiently diverse candidate pools enhance
reliability. In this work, we show that this assumption in TTS introduces a
previously unrecognized failure mode. When candidate diversity is curtailed,
even by a modest amount, TTS becomes much more likely to produce unsafe
outputs. We present a reference-guided diversity reduction protocol (RefDiv)
that serves as a diagnostic attack to stress test TTS pipelines. Through
extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,
Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and
Best-of-N), constraining diversity consistently signifies the rate at which TTS
produces unsafe results. The effect is often stronger than that produced by
prompts directly with high adversarial intent scores. This observed phenomenon
also transfers across TTS strategies and to closed-source models (e.g. OpenAI
o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant
property of TTS rather than a model-specific artifact. Additionally, we find
that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and
OpenAI Moderation API), are unable to flag the adversarial input prompts
generated by RefDiv, demonstrating that existing defenses offer limited
protection against this diversity-driven failure mode. Through this work, we
hope to motivate future research on designing robust TTS strategies that are
both effective and secure against diversity-targeted stress tests as
illustrated by RefDiv.

</details>


### [36] [Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech](https://arxiv.org/abs/2510.08593)
*Yuxin Li,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: 本文提出了基于多层SSL特征融合和CTC损失的HAREN-CTC模型，显著提升了语音抑郁检测的准确性和泛化能力，在主流数据集上取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 语音抑郁检测（SDD）作为一种有前景的、非侵入性的替代传统临床评估方法，但受限于难以提取有意义特征，以及难以捕获稀疏、异质性抑郁线索。现有方法往往只利用自监督学习（SSL）模型如WavLM的某一层或最终层语音特征，容易过拟合且未充分利用层次结构信息。

Method: 提出HAREN-CTC新架构，将多层SSL特征通过跨注意力机制集成到多任务学习框架，并结合CTC损失以应对稀疏时间监督。该方法包括：层次自适应聚类模块（重新组织SSL特征为互补嵌入）和跨模态融合模块（通过跨注意力建模层间依赖）；CTC目标实现对不规则抑郁语音时序线索的对齐训练。

Result: 在DAIC-WOZ和MODMA数据集上，模型在标准分割和五折交叉验证下均达到最先进的Macro F1分数：分别为0.81和0.82，优于以往方法。

Conclusion: HAREN-CTC模型能更好地利用多层语音特征并对稀疏、异质性抑郁线索进行建模，实现了语音抑郁检测性能的显著提升，具备良好的泛化能力。

Abstract: Speech-based depression detection (SDD) is a promising, non-invasive
alternative to traditional clinical assessments. However, it remains limited by
the difficulty of extracting meaningful features and capturing sparse,
heterogeneous depressive cues over time. Pretrained self-supervised learning
(SSL) models such as WavLM provide rich, multi-layer speech representations,
yet most existing SDD methods rely only on the final layer or search for a
single best-performing one. These approaches often overfit to specific datasets
and fail to leverage the full hierarchical structure needed to detect subtle
and persistent depression signals.
  To address this challenge, we propose HAREN-CTC, a novel architecture that
integrates multi-layer SSL features using cross-attention within a multitask
learning framework, combined with Connectionist Temporal Classification loss to
handle sparse temporal supervision. HAREN-CTC comprises two key modules: a
Hierarchical Adaptive Clustering module that reorganizes SSL features into
complementary embeddings, and a Cross-Modal Fusion module that models
inter-layer dependencies through cross-attention. The CTC objective enables
alignment-aware training, allowing the model to track irregular temporal
patterns of depressive speech cues.
  We evaluate HAREN-CTC under both an upper-bound setting with standard data
splits and a generalization setting using five-fold cross-validation. The model
achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on
MODMA, outperforming prior methods across both evaluation scenarios.

</details>


### [37] [Systematic Diagnosis of Brittle Reasoning in Large Language Models](https://arxiv.org/abs/2510.08595)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 作者提出了一种新方法，更细致地分析和量化了主流大模型在数学推理不同环节的表现，指出其在程序性推理几乎无误，但组合推理能力极弱，为未来改进提供了具体线索。


<details>
  <summary>Details</summary>
Motivation: 当前标准基准难以细致诊断机器学习模型在数学推理方面的具体薄弱环节，因此需要一种新的测评框架来更细粒度地揭示其推理能力的组成和不足。

Method: 首先利用gpt-3.5-turbo在GSM8K数据集上生成结构化、逐步的推理过程，随后由更强大的分析模型gpt-4o-mini对错误进行分类，并对每一句推理句进行无监督聚类以识别出“推理模式”。

Result: 该方法不仅识别和量化了不同类型的推理技能，还首次揭示了模型在组合性约束推理方面的显著短板，为未来能力提升和应用开发提供了清晰的改进路径。

Conclusion: 本文提出的分析方法揭示了机器学习模型在处理不同类型数学推理时存在显著的非人类式脆弱性，尤其是在需要组合推理和约束时表现较差，而在顺序计算等程序化模式下几乎完美。

Abstract: A central question in artificial intelligence is the extent to which machine
learning models comprehend mathematics. To address this, we propose a novel
framework for measuring mathematical reasoning that moves beyond standard
benchmarks to diagnose specific failure points. Our method first generates
structured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We
then use a more capable analyst model, gpt-4o-mini, to categorize errors and,
crucially, perform an unsupervised clustering of every reasoning sentence to
identify emergent "reasoning modes." This analysis reveals a cognitive profile
with a stark, nonhuman-like brittleness: while the model achieves near-perfect
accuracy on procedural modes like sequential calculation, its performance on
modes requiring combinatorial reasoning with restrictions plummets. By
identifying and quantifying the reliability of these distinct reasoning skills,
our work provides a more granular method to evaluate mathematical comprehension
and offers a precise roadmap for developing new capabilities and more reliable
future applications.

</details>


### [38] [Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs](https://arxiv.org/abs/2510.08596)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 本文提出了Confidence Score，有效弥补了传统评价指标在创造性文本生成领域的显著偏见，能更科学地评估大模型生成的创新性内容。


<details>
  <summary>Details</summary>
Motivation: 现有的无参考评价指标（如self-perplexity）对创造性文本生成存在严重偏见，难以公正评价生成模型的创造性表现。

Method: 提出了一种基于模型输出概率分布的新衡量标准：Confidence Score（CS），并在gpt-4o-mini模型上进行实验，分析其区分不同任务难度和对创造性文本的倾向。

Result: CS指标在99个创造性文本提示中，比fluency-based指标对新颖回复有19%的偏好（fluency-based指标为0%），差异在95%置信区间内明显（[11.1%, 27.3%]）；CS还能有效区分容易、中等和困难任务。

Conclusion: Confidence Score能够减轻传统无参考指标对创造性的偏见，同时保留其核心的评价能力，为现代大模型带来更平衡的评估方式。

Abstract: Reference-free metrics like self-perplexity are strongly biased against
creative text generation. We propose the Confidence Score (CS), derived from a
model's output probability distribution, as a less biased alternative.
Experiments on gpt-4o-mini show that while fluency-based metrics prefer novel
responses in 0\% of cases on 99 creative prompts, our CS does so 19% of the
time, a statistically significant difference (95% CI for difference: [11.1%,
27.3%]). We also show that CS effectively distinguishes between easy, medium,
and hard tasks, confirmed by non-overlapping confidence intervals. The
Confidence Score thus mitigates the creativity bias of traditional metrics
while retaining their core evaluative strengths, offering a more balanced
assessment for modern LLMs.

</details>


### [39] [Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation](https://arxiv.org/abs/2510.08600)
*Devleena Das,Rajeev Patwari,Ashish Sirasao*

Main category: cs.CL

TL;DR: 论文提出了Recover-LoRA方法，通过轻量级的LoRA适配器与logit蒸馏，可针对各种权重退化有效恢复小型语言模型精度，在不同注意力结构和数据集上能提升5-17%准确率。


<details>
  <summary>Details</summary>
Motivation: 推理优化如量化、剪枝、格式和数据类型转换、模型导出与序列化，常常导致语言模型在任务上的性能下降。目前，大多数针对部署后的性能恢复工作集中在更鲁棒的量化技术上。本工作关注于恢复由各种权重退化（如模型序列化不当）引起的模型精度。

Method: 提出Recover-LoRA方法。这是一种轻量级、数据集无关的精度恢复方法，利用合成数据和logit蒸馏，对选择性层进行LoRA适配器学习，使退化模型与其全精度模型对齐。

Result: Recover-LoRA可在多种小型语言模型（不同注意力机制、如多头注意力和分组查询注意力）及多数据集上进行了实验，结果显示可恢复模型5-17%的准确率。

Conclusion: Recover-LoRA能有效恢复因多种权重退化（包括非鲁棒序列化等）导致的小型语言模型性能损失，适用范围广，且实现轻量级。

Abstract: Inference optimizations such as quantization, pruning, format and datatype
conversion, model export, and serialization can lead to functional degradations
in language model task performance. While most efforts on performance recovery
for deployment focus on robust quantization techniques, we focus on recovering
model accuracies from any sources that degrade model weights, such as improper
model serialization. In this work, we propose Recover-LoRA, a lightweight and
dataset agnostic method to recover accuracy in degraded models. Recover-LoRA
uses synthetic data and logit distillation to learn LoRA adapters on selective
layers that facilitate aligning the degraded model to its full precision model.
We investigate the utility of Recover-LoRA across a diverse set of small
language models (SLMs), including models with varying attention architectures,
multi-head attention (MHA) and group-query attention (GQA), as well as several
evaluation datasets. Our results show that Recover-LoRA recovers model
accuracies by 5-17% on MHA and GQA SLMs.

</details>


### [40] [Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs](https://arxiv.org/abs/2510.08601)
*Aneesh Jonelagadda,Christina Hahn,Haoze Zheng,Salvatore Penachio*

Main category: cs.CL

TL;DR: Mnemosyne是一种适用于边缘设备的无监督、类人长期记忆架构，显著提升了对话模型的事实回忆、时间推理和自然性，尤其在医疗长期助手场景下表现优异，超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆系统对边缘设备不友好，基于暴力上下文扩展或静态检索，对于长期对话（如医疗助手）效果有限，需要更人性化、更高效的长期记忆方法。

Method: 提出一种类似人类记忆的无监督长时记忆架构Mnemosyne，使用图结构存储、冗余过滤、记忆提交与修剪、概率召回和记忆更新等机制，还提出集中式核心摘要，提升对用户长期细节的捕获能力。

Result: 在长期医疗对话实验中，Mnemosyne在人类盲测评估中实现65.8%胜率（对照RAG基线31.1%），在LoCoMo基准上表现领先，同时综合得分为54.6%，优于主流基线，显示出更强的长期记忆和人性化对话能力。

Conclusion: Mnemosyne在长期医疗对话场景中，实现更优的事实回忆、时间推理和更自然的人机对话效果，且适合边缘设备部署。

Abstract: Long-term memory is essential for natural, realistic dialogue. However,
current large language model (LLM) memory systems rely on either brute-force
context expansion or static retrieval pipelines that fail on edge-constrained
devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term
memory architecture designed for edge-based LLMs. Our approach uses
graph-structured storage, modular substance and redundancy filters, memory
committing and pruning mechanisms, and probabilistic recall with temporal decay
and refresh processes modeled after human memory. Mnemosyne also introduces a
concentrated "core summary" efficiently derived from a fixed-length subset of
the memory graph to capture the user's personality and other domain-specific
long-term details such as, using healthcare application as an example,
post-recovery ambitions and attitude towards care. Unlike existing
retrieval-augmented methods, Mnemosyne is designed for use in longitudinal
healthcare assistants, where repetitive and semantically similar but temporally
distinct conversations are limited by naive retrieval. In experiments with
longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate
of 65.8% in blind human evaluations of realism and long-term memory capability
compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current
highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval
compared to other same-backboned techniques. Further, the average overall score
of 54.6% was second highest across all methods, beating commonly used Mem0 and
OpenAI baselines among others. This demonstrates that improved factual recall,
enhanced temporal reasoning, and much more natural user-facing responses can be
feasible with an edge-compatible and easily transferable unsupervised memory
architecture.

</details>


### [41] [Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection](https://arxiv.org/abs/2510.08602)
*Cong Zeng,Shengkun Tang,Yuanzhou Chen,Zhiqiang Shen,Wenchao Yu,Xujiang Zhao,Haifeng Chen,Wei Cheng,Zhiqiang Xu*

Main category: cs.CL

TL;DR: 以分布外检测方法取代传统二分类，极大提升了AI文本检测的泛化与鲁棒性，实验结果优越。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（如ChatGPT、DeepSeek、Claude）的应用广泛，推动了AI生成文本的大规模出现，导致人们迫切需要区分人类与机器生成文本的方法。然而，现有方法的泛化能力差，难以应对不同领域及模型的新输入。

Method: 论文提出将检测任务重新表述为分布外检测（OOD）问题，将人类文本视为分布外样本，机器生成文本视为分布内（ID）样本，并引入一类学习方法（如DeepSVDD、HRN）和基于分数的学习方法（如基于能量的方法）构建检测框架。

Result: 在DeepFake等多个数据集上，所提OOD方法实现了98.3%的AUROC和AUPR，FPR95仅为8.9%。在多语种、对抗攻击、未知模型和新领域场景中亦表现出良好的鲁棒性和泛化性。

Conclusion: 将AI文本检测任务从二分类重新定位为OOD检测，有效提升了在各种场景下的检测鲁棒性和泛化能力。

Abstract: The rapid advancement of large language models (LLMs) such as ChatGPT,
DeepSeek, and Claude has significantly increased the presence of AI-generated
text in digital communication. This trend has heightened the need for reliable
detection methods to distinguish between human-authored and machine-generated
content. Existing approaches both zero-shot methods and supervised classifiers
largely conceptualize this task as a binary classification problem, often
leading to poor generalization across domains and models. In this paper, we
argue that such a binary formulation fundamentally mischaracterizes the
detection task by assuming a coherent representation of human-written texts. In
reality, human texts do not constitute a unified distribution, and their
diversity cannot be effectively captured through limited sampling. This causes
previous classifiers to memorize observed OOD characteristics rather than learn
the essence of `non-ID' behavior, limiting generalization to unseen
human-authored inputs. Based on this observation, we propose reframing the
detection task as an out-of-distribution (OOD) detection problem, treating
human-written texts as distributional outliers while machine-generated texts
are in-distribution (ID) samples. To this end, we develop a detection framework
using one-class learning method including DeepSVDD and HRN, and score-based
learning techniques such as energy-based method, enabling robust and
generalizable performance. Extensive experiments across multiple datasets
validate the effectiveness of our OOD-based approach. Specifically, the
OOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake
dataset. Moreover, we test our detection framework on multilingual, attacked,
and unseen-model and -domain text settings, demonstrating the robustness and
generalizability of our framework. Code, pretrained weights, and demo will be
released.

</details>


### [42] [YpathRAG:A Retrieval-Augmented Generation Framework and Benchmark for Pathology](https://arxiv.org/abs/2510.08603)
*Deshui Yu,Yizhi Wang,Saihui Jin,Taojie Zhu,Fanyi Zeng,Wen Qian,Zirui Huang,Jingli Ouyang,Jiameng Li,Zhen Song,Tian Guan,Yonghong He*

Main category: cs.CL

TL;DR: YpathRAG是一种为病理领域设计的检索增强框架，结合密集与稀疏检索及LLM证据判断，在病理学任务上显著提升了检索和回答准确率，并公开了相关病理评测基准，为领域知识扩展和证据强约束提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在病理等高门槛领域依然存在幻觉现象，且以往通过领域微调的方式无法拓展知识边界或保证基于证据的约束。作者希望构建一种更可靠的病理辅助推理流程。

Method: 构建包含28个子领域和153万段落的病理学向量数据库，提出YpathRAG框架，采用密集检索和稀疏检索结合的双通道混合检索，以及LLM驱动的证据判断模块，形成检索-判断-生成闭环。同时，发布了两个病理学评测基准（YpathR和YpathQA-M）。

Result: 在基准测试YpathR上，YpathRAG获得Recall@5为98.64%，比基线高23个百分点；在YpathQA-M（最具挑战的300个问题）上，提升一般与医学LLM的准确率平均9.0%，最高达15.6%。

Conclusion: YpathRAG显著提升了病理领域的检索质量和事实可靠性，证明其可扩展性和可解释评估价值，为病理领域RAG系统的发展提供了新范式。

Abstract: Large language models (LLMs) excel on general tasks yet still hallucinate in
high-barrier domains such as pathology. Prior work often relies on domain
fine-tuning, which neither expands the knowledge boundary nor enforces
evidence-grounded constraints. We therefore build a pathology vector database
covering 28 subfields and 1.53 million paragraphs, and present YpathRAG, a
pathology-oriented RAG framework with dual-channel hybrid retrieval (BGE-M3
dense retrieval coupled with vocabulary-guided sparse retrieval) and an
LLM-based supportive-evidence judgment module that closes the
retrieval-judgment-generation loop. We also release two evaluation benchmarks,
YpathR and YpathQA-M. On YpathR, YpathRAG attains Recall@5 of 98.64%, a gain of
23 percentage points over the baseline; on YpathQA-M, a set of the 300 most
challenging questions, it increases the accuracies of both general and medical
LLMs by 9.0% on average and up to 15.6%. These results demonstrate improved
retrieval quality and factual reliability, providing a scalable construction
paradigm and interpretable evaluation for pathology-oriented RAG.

</details>


### [43] [LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback](https://arxiv.org/abs/2510.08604)
*Raffaele Mura,Giorgio Piras,Kamilė Lukošiūtė,Maura Pintor,Amin Karbasi,Battista Biggio*

Main category: cs.CL

TL;DR: 本文提出LatentBreak，一种通过语义等价替换降低困惑度的新型越狱攻击方法，能绕过基于困惑度的防护机制，在多种安全模型上表现优异，对LLM安全防护有挑战意义。


<details>
  <summary>Details</summary>
Motivation: 目前针对大语言模型的自动化越狱攻击（jailbreak）多通过附加高困惑度的攻击性后缀或长提示模板实现，使模型生成受限制或有害内容，但这类攻击容易被基于困惑度（perplexity）的过滤机制检测出来。作者意在突破现有越狱手法在困惑度防御下的局限。

Method: 提出LatentBreak，一种白盒越狱攻击方法。LatentBreak通过在输入提示中用语义等价词替换原有单词，避免添加高困惑度的攻击性后缀或长模板，在保持原始意图的前提下生成自然、低困惑度的对抗性提示词。具体通过最小化对抗性提示语与无害请求在潜在空间的距离来选择替换单词。

Result: LatentBreak产生的越狱提示相比于现有方法更短、困惑度更低，能有效绕过基于困惑度的过滤器，对多种安全对齐模型的攻击效果优于现有主流越狱算法。

Conclusion: LatentBreak突破了困惑度防御的限制，为大语言模型提供了更强的越狱攻击能力。传统基于困惑度的防御容易对当前自动化越狱办法奏效，但新方法LatentBreak可有效规避此类检测，有助于未来模型安全防护研究。

Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.

</details>


### [44] [Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks](https://arxiv.org/abs/2510.08605)
*Nouar Aldahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本文提出一种可插件化的多语言错误信息检测框架，系统研究语言切换、内容扩展与结构变换等多种对抗方式，对实用平台具有良好推广价值。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台上错误信息的迅速传播，对公共话语、情感和决策造成威胁。虽然已有研究探索了不同的对抗性攻击方式，但本文所研究的特定变换（如多语言切换和格式重组）尚未被系统性地研究。

Method: 提出了一种多语言、多智能体的大语言模型检测框架，集成了检索增强生成技术，并可以作为网页插件部署到在线平台。该方法系统性地评估了多语言切换、翻译、查询长度膨胀以及结构重组（如转化为多项选择题）等多种攻击手段对模型的影响。

Result: 表明AI驱动的错误信息检测对于保护在线事实完整性具有重要意义，同时验证了插件化模型在真实场景下的可行性。

Conclusion: 通过多语言和多种攻击形式的综合检测，该框架能够有效提升在线平台对抗多样化错误信息攻击的能力，具有实际部署价值。

Abstract: The rapid spread of misinformation on digital platforms threatens public
discourse, emotional stability, and decision-making. While prior work has
explored various adversarial attacks in misinformation detection, the specific
transformations examined in this paper have not been systematically studied. In
particular, we investigate language-switching across English, French, Spanish,
Arabic, Hindi, and Chinese, followed by translation. We also study query length
inflation preceding summarization and structural reformatting into
multiple-choice questions. In this paper, we present a multilingual,
multi-agent large language model framework with retrieval-augmented generation
that can be deployed as a web plugin into online platforms. Our work
underscores the importance of AI-driven misinformation detection in
safeguarding online factual integrity against diverse attacks, while showcasing
the feasibility of plugin-based deployment for real-world web applications.

</details>


### [45] [Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations](https://arxiv.org/abs/2510.08606)
*Yu Liu,Hanlei Shi,Haoxun Li,Yuqing Sun,Yuxuan Ding,Linlin Gong,Leyuan Qu,Taihao Li*

Main category: cs.CL

TL;DR: 该论文提出基于情感热点的多模态情感识别新方法，通过精确检测语句级热点并进行有效融合和对齐，实现了较大性能提升，并为多模态学习提供新视角。


<details>
  <summary>Details</summary>
Motivation: 情感识别中，判别性证据稀疏且通常在不同模态间异步，导致对话中的情感理解极具挑战性。因此亟需创新方法挖掘和融合多模态关键信息，并解决模态之间的错配问题。

Method: 提出了一种统一模型，检测文本、音频和视频中的每句话情感热点，将其通过Hotspot-Gated Fusion与全局特征融合，并用分路混合对齐器（Mixture-of-Aligners）实现多模态对齐，同时利用跨模态图建模对话结构。

Result: 在标准ERC数据集上的实验证明，新方法在性能上超越了现有的强基线模型，消融实验也进一步验证了所提HGF和MoA模块的重要性。

Conclusion: 实验结果表明，所提出的热点中心模型在标准ERC基准上相较于强基线方法有一致提升，验证了Hotspot-Gated Fusion (HGF) 和 Mixture-of-Aligners (MoA) 的有效贡献。该结果为未来多模态学习和模态融合提供了新思路。

Abstract: Emotion Recognition in Conversations (ERC) is hard because discriminative
evidence is sparse, localized, and often asynchronous across modalities. We
center ERC on emotion hotspots and present a unified model that detects
per-utterance hotspots in text, audio, and video, fuses them with global
features via Hotspot-Gated Fusion, and aligns modalities using a routed
Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This
design focuses modeling on salient spans, mitigates misalignment, and preserves
context. Experiments on standard ERC benchmarks show consistent gains over
strong baselines, with ablations confirming the contributions of HGF and MoA.
Our results point to a hotspot-centric view that can inform future multimodal
learning, offering a new perspective on modality fusion in ERC.

</details>


### [46] [MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation](https://arxiv.org/abs/2510.08608)
*Weihua Zheng,Zhengyuan Liu,Tanmoy Chakraborty,Weiwen Xu,Xiaoxue Gao,Bryan Chen Zhengyu Tan,Bowei Zou,Chang Liu,Yujia Hu,Xing Xie,Xiaoyuan Yi,Jing Yao,Chaojun Wang,Long Li,Rui Liu,Huiyao Liu,Koji Inoue,Ryuichi Sumida,Tatsuya Kawahara,Fan Xu,Lingyu Ye,Wei Tian,Dongjun Kim,Jimin Jung,Jaehyung Seo,Nadya Yuki Wangsajaya,Pham Minh Duc,Ojasva Saxena,Palash Nandi,Xiyan Tao,Wiwik Karlina,Tuan Luong,Keertana Arun Vasan,Roy Ka-Wei Lee,Nancy F. Chen*

Main category: cs.CL

TL;DR: 本文提出MMA-ASIA评测框架，解决多模态语言模型在亚洲语境下文化适应性差的问题，独创三模态严格对齐基准和五维评价体系，深入分析了模型跨语言和跨模态的表现，为未来构建文化可靠的多模态模型提供了关键支持。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在亚洲等非西方、低资源语境下理解与推理能力下降，亟需系统且文化敏感的评测方式，以推动模型泛化和适应本地文化知识。

Method: 提出了MMA-ASIA基准，包括多语言、多模态（文本、图像、语音）严格对齐的多项选择题，覆盖8个亚洲国家10种语言。同时创新性引入五维评价协议和“文化认知扎根验证模块”，结合模型对比、注意力追踪和VPR方法进行深入分析。

Result: MMA-ASIA首次实现三模态输入严格对齐，基准包含2.7万题且大多数需多步文化推理，能有效检测模型在跨国家、跨语言、跨模态方面的差异及其文化扎根性，并提出优化多模态模型文化适应性的实用建议。

Conclusion: 本文提出了MMA-ASIA框架，并证实现有多模态语言模型在亚洲语境下表现不一致，同时提出多维评价与分析方法为提升模型的文化适应性提供了方向。

Abstract: Large language models (LLMs) are now used worldwide, yet their multimodal
understanding and reasoning often degrade outside Western, high-resource
settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'
cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a
human-curated, multilingual, and multimodally aligned multiple-choice benchmark
covering 8 Asian countries and 10 languages, comprising 27,000 questions; over
79 percent require multi-step reasoning grounded in cultural context, moving
beyond simple memorization. To our knowledge, this is the first dataset aligned
at the input level across three modalities: text, image (visual question
answering), and speech. This enables direct tests of cross-modal transfer.
Building on this benchmark, we propose a five-dimensional evaluation protocol
that measures: (i) cultural-awareness disparities across countries, (ii)
cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural
knowledge generalization, and (v) grounding validity. To ensure rigorous
assessment, a Cultural Awareness Grounding Validation Module detects "shortcut
learning" by checking whether the requisite cultural knowledge supports correct
answers. Finally, through comparative model analysis, attention tracing, and an
innovative Vision-ablated Prefix Replay (VPR) method, we probe why models
diverge across languages and modalities, offering actionable insights for
building culturally reliable multimodal LLMs.

</details>


### [47] [GraphGhost: Tracing Structures Behind Large Language Models](https://arxiv.org/abs/2510.08613)
*Xinnan Dai,Kai Guo,Chung-Hsiang Lo,Shenglai Zeng,Jiayuan Ding,Dongsheng Luo,Subhabrata Mukherjee,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文提出GraphGhost，用图结构方法解释和分析大语言模型的推理能力，发现关键神经元节点对模型推理和语义有重要影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）展现出优异的推理能力，其背后的结构性机制尚未得到充分探究。本文希望弥补这一空白。

Method: 提出了GraphGhost框架，将神经元激活及其信号传播用图的方式建模，并采用图算法（例如PageRank）分析LLM的结构特性。同时通过结构性干预，对关键神经元节点进行编辑，考察对模型推理和语义的影响。

Result: GraphGhost能揭示LLM在不同数据集上的共享和特定推理行为。通过干预关键节点发现，对这些神经元的编辑会引发推理崩溃、改变逻辑流程和语义理解。

Conclusion: GraphGhost是一种强大的分析、干预与理解LLM推理结构基础的工具，有助于深入理解LLM内部的结构性推理过程。

Abstract: Large Language Models (LLMs) demonstrate remarkable reasoning capabilities,
yet the structural mechanisms underlying these abilities remain under explored.
In this work, we introduce GraphGhost, a unified framework that represents
neuron activations and their signal propagation as graphs, explaining how LLMs
capture structural semantics from sequential inputs and generate outputs
through structurally consistent mechanisms. This graph-based perspective
enables us to employ graph algorithms such as PageRank to characterize the
properties of LLMs, revealing both shared and model-specific reasoning
behaviors across diverse datasets. We further identify the activated neurons
within GraphGhost and evaluate them through structural interventions, showing
that edits to key neuron nodes can trigger reasoning collapse, altering both
logical flow and semantic understanding. Together, these contributions position
GraphGhost as a powerful tool for analyzing, intervening in, and ultimately
understanding the structural foundations of reasoning in LLMs.

</details>


### [48] [Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications](https://arxiv.org/abs/2510.08614)
*Mingxuan Liu,Yuhe Ke,Wentao Zhu,Mayli Mertens,Yilin Ning,Jingchi Liao,Chuan Hong,Daniel Shu Wei Ting,Yifan Peng,Danielle S. Bitterman,Marcus Eng Hock Ong,Nan Liu*

Main category: cs.CL

TL;DR: 本文揭示了医疗领域LLM在患者性别相关性判断上的不一致及性别偏见，强调在实际应用前需常规检查其身份一致性，以保证AI临床辅助的可靠性与公平性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在模拟医疗角色时是否会重复或放大性别相关偏见，尤其在临床决策场景中，这种偏见可能影响患者治疗结果。

Method: 通过对多种开源及商业LLM在NEJM病例挑战中的性别身份设定（女性、男性或未指定），分析其诊断一致性及对患者性别相关性的判断。

Result: 大多数模型在诊断上性别一致性较好，但在判断患者性别重要性和相关性时表现出较大不一致，且部分模型存在系统性的男女差异，显示出忽视或加剧性别偏见的风险。

Conclusion: 本文发现，大型语言模型在医疗诊断时对患者性别相关性的判断存在显著的不一致和性别差异，这种偏见可能影响其临床应用的可靠性与公平性。

Abstract: The integration of large language models (LLMs) into healthcare holds promise
to enhance clinical decision-making, yet their susceptibility to biases remains
a critical concern. Gender has long influenced physician behaviors and patient
outcomes, raising concerns that LLMs assuming human-like roles, such as
clinicians or medical educators, may replicate or amplify gender-related
biases. Using case studies from the New England Journal of Medicine Challenge
(NEJM), we assigned genders (female, male, or unspecified) to multiple
open-source and proprietary LLMs. We evaluated their response consistency
across LLM-gender assignments regarding both LLM-based diagnosis and models'
judgments on the clinical relevance or necessity of patient gender. In our
findings, diagnoses were relatively consistent across LLM genders for most
models. However, for patient gender's relevance and necessity in LLM-based
diagnosis, all models demonstrated substantial inconsistency across LLM
genders, particularly for relevance judgements. Some models even displayed a
systematic female-male disparity in their interpretation of patient gender.
These findings present an underexplored bias that could undermine the
reliability of LLMs in clinical practice, underscoring the need for routine
checks of identity-assignment consistency when interacting with LLMs to ensure
reliable and equitable AI-supported clinical care.

</details>


### [49] [Iterative LLM-Based Generation and Refinement of Distracting Conditions in Math Word Problems](https://arxiv.org/abs/2510.08615)
*Kaiqi Yang,Hang Li,Yucheng Chu,Zitao Liu,Mi Tian,Hui Liu*

Main category: cs.CL

TL;DR: 论文提出利用大语言模型自动为数学文字题生成干扰条件的新方法，显著提升数据集丰富性和使用效率，无需人工重新校对答案，适合大规模生成高质量测试题。


<details>
  <summary>Details</summary>
Motivation: 现有数学文字题数据集通常仅包含必需信息，缺少干扰或冗余条件。现有少数干扰条件题目的数据集存在难度低、容易识别等问题，且添加干扰后需大量人工校对答案，影响测评效果和数据集质量。

Method: 设计了一个迭代式的框架，借助大语言模型（LLMs）和多角度、多认知层次的提示词，引导LLMs自动生成有意义且不影响原答案的干扰条件，并保留原题和修订题目标准解。

Result: 该框架高效易用，大幅降低了带干扰条件的数学题生成和校对的人工成本，同时保证数据的高质量和问题答案的一致性。

Conclusion: 研究提出了一种自动化生成数学文字题干扰条件的框架，可以在不改变原题答案的前提下，快速高质量地扩充题目类型，且无需大量人工修订。

Abstract: Mathematical reasoning serves as a crucial testbed for evaluating the
intelligence of large language models (LLMs), and math word problems (MWPs)
represent one of the most widely used formats. Most existing MWP datasets
contain only the necessary information, while problems with distracting or
excessive conditions are often overlooked. Prior studies have shown that
popular LLMs experience a dramatic performance drop when such distracting
conditions are introduced. However, available datasets of MWPs with distracting
conditions remain limited, and most exhibit low difficulty and out-of-context
expressions. These shortcomings make the distracting conditions easy to detect
and disregard, thereby reducing the credibility of benchmarking on these
datasets. Moreover, when distracting conditions are added, the reasoning
process and answers may change, requiring intensive manual effort to check and
rewrite solutions.
  To address these issues, we design an iterative framework that leverages LLMs
to generate distracting conditions automatically. We develop a set of prompts
to revise MWPs from multiple perspectives and cognitive levels, encouraging the
creation of meaningful distracting conditions as well as suggestions for
further refinement. A key advantage of our framework is the preservation of
shared solutions between the original and revised problems: the LLMs are
explicitly guided to generate distractions that do not alter the original
solution, thus eliminating the need to produce new answers. This framework is
efficient and easy to deploy, substantially reducing the effort required to
generate MWPs with distracting conditions while maintaining high data quality.

</details>


### [50] [LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests](https://arxiv.org/abs/2510.08616)
*Juan Miguel Navarro Carranza*

Main category: cs.CL

TL;DR: 通过对基准测试题进行语义复述再测试，验证大模型准确率显著下滑，说明原始基准测试评估LLM能力可能高估，真实泛化能力尚有限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在基准测试中的得分可能被记忆测试题目或其近似重复项所夸大。为真实反映模型泛化能力，需规避“题目记忆”现象。

Method: 设计一种简单协议，通过对测试基准题目进行复述（paraphrase），用Mistral-7B-Instruct和Qwen2.5-7B-Instruct两个模型在ARC-Easy和ARC-Challenge数据集上分别对原题和复述题进行准确率测量。流程包括受控解码、强制选择题输出格式以及稳健的语义保持复述清洗步骤。

Result: 实验发现，模型在复述题目上的准确率相比原题有明显下降，这印证了此前关于数据泄漏和模型对表层特征取巧的担忧。

Conclusion: 仅依靠原始基准题目评估LLM能力的做法易导致过高估计，而通过题目复述可更真实考察模型的泛化能力，揭示其易受题型表面形式影响的问题。

Abstract: Benchmark scores for Large Language Models (LLMs) can be inflated by
memorization of test items or near duplicates. We present a simple, protocol
that probes generalization by re-evaluating models on paraphrased versions of
benchmark questions. Using Mistral-7B-Instruct and Qwen2.5-7B-Instruct, we
measure the accuracy gap between original and paraphrased items on ARC-Easy and
ARC-Challenge. Our pipeline controls decoding, enforces multiple-choice output
format, and includes a robust paraphrase-cleaning step to preserve semantics.
We find that paraphrasing induces a non-trivial accuracy drop (original vs.
paraphrased), consistent with prior concerns about contamination and brittle
surface-form shortcuts.

</details>


### [51] [JAI-1: A Thai-Centric Large Language Model](https://arxiv.org/abs/2510.08620)
*Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Pontakorn Trakuekul,Sumana Sumanakul,Natchanon Pollertlam*

Main category: cs.CL

TL;DR: JAI-1是一款参数规模75B的泰语大模型，通过先扩容再系统注入泰语知识的新范式，显著提升了泰语处理能力并优于主流模型，避免了直接微调带来的知识遗失问题。


<details>
  <summary>Details</summary>
Motivation: 现有泰语大模型一般是在现有开源模型基础上做进一步训练，直接注入泰语知识，但这种方式容易导致模型原有知识流失。作者希望解决不同语言之间知识注入的冲突，保持通用能力并提升泰语能力。

Method: 采用先基于较小但性能优异的英文开源模型，将其参数空间扩展到75B，再在新扩展的参数容量中系统注入泰语知识，通过大规模泰语及英文预训练和分阶段的指令微调实现。

Result: JAI-1进行了1.5万亿token的预训练（其中包含超3000亿泰语token），之后又通过60万条以上指令调优数据训练。结果显示，其在多个泰语标准测试集上均优于主流的Typhoon2-70B模型。

Conclusion: 模型的“扩容注入”策略既保留了原模型的通用能力，也有效集成了泰语知识，是泰语大模型方向的重要创新。

Abstract: This technical report introduces JAI-1, a Thai-centric language model with
75B parameters. Recent Thai models have primarily relied on existing
open-source models, applying additional training without structural
modifications to specialize in Thai. However, this approach risks eroding
pre-existing knowledge in the model's parameter space during the injection of
Thai-specific information, as optimized parameters for general tasks may
conflict with new linguistic requirements. In contrast, JAI-1 adopts an
upscaling strategy: starting from a smaller, high-performing English
open-source LLM, we expanded its parameter space and utilized the newly
allocated capacity to systematically integrate Thai-language knowledge. This
methodology not only preserves the original model's general intelligence but
also establishes a unique architecture distinct from other open-source models,
enabling scalable future enhancements. During pre-training, JAI-1 was exposed
to 1.5T tokens, including over 300B Thai language tokens. This was followed by
post-training stages -- supervised fine-tuning and alignment tuning -- using
more than 600K instruction-based examples. The final model demonstrated
superior performance compared to Typhoon2-70B on Thai-centric benchmarks
(IFEval-TH, MT-Bench-TH, and JAI-Hall-Bench), validating the efficacy of its
upscaling and knowledge-integration framework.

</details>


### [52] [From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents](https://arxiv.org/abs/2510.08621)
*Wen-Yu Chang,Tzu-Hung Huang,Chih-Ho Chen,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文提出并验证了一种基于职业信息定制销售对话的策略，通过丰富用户模拟资料提升代理系统效率，简便的人格化方法可以增强对话系统的实际表现。


<details>
  <summary>Details</summary>
Motivation: 在智能对话模型快速发展的背景下，开发能更有效进行销售对话的代理时，需要依赖真实的用户模拟研究，以优化对话策略。

Method: 研究让销售代理根据用户的年龄、性别和职业三类特征调整对话策略，重点分析职业对意图表达的影响，据此提出一种基于职业的轻量级对话策略。

Result: 发现年龄和性别影响整体表现，而职业对对话意图差异影响最大。基于职业定制策略后，代理能更快、更有效地达成对话目标。

Conclusion: 丰富的用户模拟角色极其重要，且简单的个性化策略能显著提升销售型对话系统的效果。

Abstract: Amid the rapid rise of agentic dialogue models, realistic user-simulator
studies are essential for tuning effective conversation strategies. This work
investigates a sales-oriented agent that adapts its dialogue based on user
profiles spanning age, gender, and occupation. While age and gender influence
overall performance, occupation produces the most pronounced differences in
conversational intent. Leveraging this insight, we introduce a lightweight,
occupation-conditioned strategy that guides the agent to prioritize intents
aligned with user preferences, resulting in shorter and more successful
dialogues. Our findings highlight the importance of rich simulator profiles and
demonstrate how simple persona-informed strategies can enhance the
effectiveness of sales-oriented dialogue systems.

</details>


### [53] [Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories](https://arxiv.org/abs/2510.08622)
*Francesco Dente,Fabiano Dalpiaz,Paolo Papotti*

Main category: cs.CL

TL;DR: 作者提出Text2Stories任务和度量，结合LLM自动评估由访谈生成的软件用户故事质量，实验表明性能优异，可作为贴合原始需求的自动化补充评判方法。


<details>
  <summary>Details</summary>
Motivation: 当前利用大型语言模型（LLM）自动从访谈记录等自然语言输入中生成软件需求，但如何自动评估这些需求是否真实反映了利益相关方需求，仍主要依赖人工，缺乏自动化度量方法。

Method: 提出了一项新任务和度量Text2Stories，用于评估由LLM生成的用户故事与原始访谈内容的一致性。将访谈转录文本分割为块，将其与用户故事进行匹配，通过衡量正确性（故事被访谈支持的比例）和完整性（访谈被故事覆盖的比例），实现量化分析。实验比较了LLM与嵌入式模型的匹配性能。

Result: 实验在四个数据集上显示，基于LLM的匹配方法在独立标注测试集上获得了0.86的macro-F1分数；仅用嵌入模型效果略逊，但可用于高效筛查。提出的度量可对比不同来源的用户故事（如人工与生成），为现有质量判据提供了可扩展、贴近原始需求的新补充。

Conclusion: Text2Stories任务和度量体系为软件需求自动化分析带来可扩展的、基于原始访谈文本的自动化质量评估手段，提升了生成式用户故事与实际需求的一致性判定效率。

Abstract: Large language models (LLMs) can be employed for automating the generation of
software requirements from natural language inputs such as the transcripts of
elicitation interviews. However, evaluating whether those derived requirements
faithfully reflect the stakeholders' needs remains a largely manual task. We
introduce Text2Stories, a task and metrics for text-to-story alignment that
allow quantifying the extent to which requirements (in the form of user
stories) match the actual needs expressed by the elicitation session
participants. Given an interview transcript and a set of user stories, our
metric quantifies (i) correctness: the proportion of stories supported by the
transcript, and (ii) completeness: the proportion of transcript supported by at
least one story. We segment the transcript into text chunks and instantiate the
alignment as a matching problem between chunks and stories. Experiments over
four datasets show that an LLM-based matcher achieves 0.86 macro-F1 on held-out
annotations, while embedding models alone remain behind but enable effective
blocking. Finally, we show how our metrics enable the comparison across sets of
stories (e.g., human vs. generated), positioning Text2Stories as a scalable,
source-faithful complement to existing user-story quality criteria.

</details>


### [54] [PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction](https://arxiv.org/abs/2510.08623)
*Anubhav Shrimal,Aryan Jain,Soumyajit Chowdhury,Promod Yenigalla*

Main category: cs.CL

TL;DR: 提出PARSE系统，通过自动优化JSON schema和改进抽取策略，在多个数据集上大幅提升LLM结构化信息抽取的准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在进行结构化信息抽取时，通常直接将JSON schema作为静态合约处理，导致抽取性能不佳、幻觉频发，以及在schema含有歧义或不完整说明时行为不可靠。作者认为JSON schema本身是一种自然语言理解合约，LLM应能够解释并加以系统改进。

Method: 论文提出了PARSE系统，包含两个协同组件：ARCHITECT自动优化JSON schema以适合LLM处理，并通过RELAY保持向后兼容性；SCOPE利用反思型抽取机制结合静态规则和LLM能力设置多重防护。

Result: 在Schema-Guided Dialogue、Structured Web Data Extraction和内部电商对话数据三套数据集上评估，SWDE数据集信息抽取准确率提升最多达64.7%；整体框架在各模型平均表现提升10%；首次重试时抽取错误率减少92%，且维持实用的延迟水平。

Conclusion: 改进和动态优化JSON schema，以及结合静态与LLM防护机制，可以显著提升基于大语言模型的信息抽取准确性和可靠性。

Abstract: Structured information extraction from unstructured text is critical for
emerging Software 3.0 systems where LLM agents autonomously interact with APIs
and tools. Recent approaches apply large language models directly to extraction
tasks using existing JSON schemas, often with constraint decoding or
reinforcement learning approaches to ensure syntactic validity, but treat JSON
schemas as static contracts designed for human developers, leading to
suboptimal extraction performance, frequent hallucinations, and unreliable
agent behavior when schemas contain ambiguous or incomplete specifications. We
recognize that JSON schemas themselves are a form of natural language
understanding contract that encodes rules, relationships, and expectations
about data structure contracts that LLMs should be able to both interpret and
systematically improve. Consequently, we develop PARSE (Parameter Automated
Refinement and Schema Extraction), a novel system with two synergistic
components: ARCHITECT, which autonomously optimizes JSON schemas for LLM
consumption while maintaining backward compatibility through RELAY (an
integrated code generation system), and SCOPE, which implements
reflection-based extraction with combined static and LLM-based guardrails. We
evaluate PARSE qualitatively and quantitatively on three datasets including
Schema-Guided Dialogue (SGD), Structured Web Data Extraction (SWDE), and
internal retail conversation data, and find that it achieves up to 64.7%
improvement in extraction accuracy on SWDE with combined framework improvements
reaching 10% across models, while reducing extraction errors by 92% within the
first retry and and maintaining practical latency.

</details>


### [55] [Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B](https://arxiv.org/abs/2510.08624)
*Nisar Ahmed,Muhammad Imran Zaman,Gulshan Saleem,Ali Hassan*

Main category: cs.CL

TL;DR: 本研究发现，带有“评测味道”的提示会虚增大模型的推理表现（如推理长度），但未必提升真实准确率和可应用能力，且可能损害答案简洁和多语种一致性。建议改用更贴近真实应用的话术与评分方式，确保模型评测反映实际部署水平。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评测多依赖于带有评分提示的prompts，强调显性推理与严格格式，但真实应用场景要求简洁且契约化的答案。作者关注评测环境是否虚高了模型性能，而未带来实际能力提升。

Method: 采用公开权重模型GPT-OSS-20B设计六组A/B实验，任务内容和解码过程固定，仅改变提示话术（评测导向vs真实场景）及推理深度（中/高），涵盖数学、代码修正、引用生成、激励话术、显性推理展示、多语种（乌尔都语）等。用自动检验工具评估准确率、仅答合规、拒答、推理长度及结构合规，并设置预注册的差值与综合指标。

Result: 评测导向提示稳定提升推理长度（数百到千字），但降低纯答合规性，准确率提升有限且不一致。结构化输出时，提升输出封装（如代码块、列表）但不改善内容准确性。激励话术改变错误类型，表扬谨慎提升高推理下准确率，减少自信错误；表扬能力则输出更简但风险增加。多语种下，类似提示影响在乌尔都语重现，且高推理下准确率下降，表现出多语种一致性风险。

Conclusion: 评测环境设定会虚高模型表现，未必反映真实可部署能力，尤其在多语种场景有风险。作者提供可复现的A/B测试框架，并建议采用中性话术或双重评价、契约化评分、风格差异报告、信心治理和多语种仪表盘，以让评测结果更贴近实际应用。

Abstract: Benchmarks for large language models (LLMs) often rely on rubric-scented
prompts that request visible reasoning and strict formatting, whereas real
deployments demand terse, contract-bound answers. We investigate whether such
"evaluation scent" inflates measured performance without commensurate
capability gains. Using a single open-weights model (GPT-OSS-20B), we run six
paired A/B scenarios that hold task content and decoding fixed while varying
framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High):
deterministic math, strict code-fix, citation generation, incentive flips
(caution vs. competence), CoT visibility, and multilingual (Urdu) headers.
Deterministic validators compute accuracy, answer-only compliance,
hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with
pre-registered deltas and composite indices. Across scenarios, evaluation
framing reliably inflates CoT (hundreds to >1000 characters) and reduces
answer-only compliance, with limited or inconsistent accuracy gains. In
structured outputs, it improves wrappers (e.g., fenced blocks, enumerated
lists) but not regex-validated substance. Incentive wording reweights error
composition: praising caution modestly improves accuracy at high reasoning and
reduces wrong-but-confident errors, whereas praising competence yields terser
but riskier outputs. Urdu rubric headers reproduce these signatures and can
decrease accuracy at higher reasoning depth, indicating multilingual parity
risks. We provide a reproducible A/B framework (prompt banks, validators,
per-run scores, scripts; versioned DOI) and practical guidance: neutral
phrasing or dual-framing checks, contract-aware grading, style-delta reporting,
confidence governance, and multilingual dashboards to ensure that benchmark
gains reflect deployable capability.

</details>


### [56] [From What to Why: Thought-Space Recommendation with Small Language Models](https://arxiv.org/abs/2510.08626)
*Prosenjit Biswas,Pervez Shaik,Abhinav Thorat,Ravi Kolla,Niranjan Pedanekar*

Main category: cs.CL

TL;DR: 本论文提出利用小型语言模型生成的推理说明作为直接学习信号，构建跨领域推荐的通用表征空间。新方法PULSE在推荐、迁移和推理任务表现突出，兼具高效与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理推荐能力强，但推理成本高不适合实际部署；小型语言模型高效但推理能力未被充分探索。现有系统未能将自然语言说明当作有效学习信号。该工作旨在利用SLM构建跨领域通用的用户、物品理解空间，从而提升推荐系统性能并降低成本。

Method: 提出了PULSE框架，将SLM生成的rationales（推理说明）作为直接学习信号，并结合用户交互历史进行监督，联合建模用户行为和其语义驱动力。此外，PULSE将rationales视为核心信号，提升了嵌入的鲁棒性和泛化能力。

Result: PULSE显著超越当前主流算法，在推荐、跨领域迁移和推理问答等任务中均表现优异。嵌入具有更强的鲁棒性和泛化性，代码已开源。

Conclusion: PULSE在多个基准数据集上优于现有的ID、协同过滤和基于LLM的推荐模型，在跨领域推荐和面向推理的下游任务中表现出强大的迁移能力。

Abstract: Large Language Models (LLMs) have advanced recommendation capabilities
through enhanced reasoning, but pose significant challenges for real-world
deployment due to high inference costs. Conversely, while Small Language Models
(SLMs) offer an efficient alternative, their reasoning capabilities for
recommendation remain underexplored. Existing systems often use natural
language rationales merely as unsupervised descriptive text, failing to harness
their full potential as learning signals. In this work our main idea is to
create a common understanding of user and items across multiple domains called
Thought Space with SLMs instead of using LLMs' distilled knowledge. To that end
we propose PULSE (Preference Understanding by Latent Semantic Embeddings), a
framework that treats SLM-generated rationales as director learning signals,
supervising them with interaction histories to jointly model user actions
(what) and their semantic drivers (why). Existing methods consider only
interactions such as sequences and embeddings, whereas PULSE treats rationales
as first-class signals, this novel design yields embeddings that are more
robust and generalizable. Extensive experiments demonstrate that PULSE
outperforms leading ID, Collaborative Filtering (CF), and LLM-based sequential
recommendation models across multiple benchmark datasets. Furthermore, PULSE
exhibits superior transferability in cross-domain recommendation and
demonstrates strong performance on downstream tasks such as reasoning-oriented
question answering. Our code is available
\href{https://anonymous.4open.science/r/Thinking_PULSE-0FC5/README.md}{here}.

</details>


### [57] [ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection](https://arxiv.org/abs/2510.08630)
*Jingbiao Mei,Mingsheng Sun,Jinghong Chen,Pengda Qin,Yuhong Li,Da Chen,Bill Byrne*

Main category: cs.CL

TL;DR: 本文通过模仿人工审核流程提出ExPO-HM方法，有效提升仇恨梗图检测的准确性和可解释性，显著超过现有方法，助力平台精细、高效地审核复杂内容。


<details>
  <summary>Details</summary>
Motivation: 网络上的仇恨梗图是一种具有挑战性的内容滥用形式，目前自动检测系统大多只进行二元分类，缺乏生成对上下文和解释的能力，无法满足实际内容审核的需求。

Method: 提出ExPO-HM（Explain-then-Detect Policy Optimization for Hateful Memes）方法，结合SFT预训练、GRPO加课程学习，并采用条件决策熵（CDE）作为衡量和奖励推理质量的方法，模仿人工标注流程训练模型。

Result: 在三个主流仇恨梗图基准测试上，ExPO-HM在二元检测、细粒度分类和推理质量方面均达到了现有最优表现。与GRPO和DPO基线方法相比，F1值分别提升了最高15%和17%。

Conclusion: ExPO-HM方法能够将仇恨梗图检测从简单的二元警报转变为以解释为驱动的检测，实现更加准确、可解释和可操作的内容审核支持。

Abstract: Hateful memes have emerged as a particularly challenging form of online
abuse, motivating the development of automated detection systems. Most prior
approaches rely on direct detection, producing only binary predictions. Such
models fail to provide the context and explanations that real-world moderation
requires. Recent Explain-then-Detect approaches, using Chain-of-Thought
prompting or LMM agents, perform worse than simple SFT baselines, and even
advanced post-training methods such as GRPO fail to close the gap. Our analysis
identifies two key issues of such systems: important policy-relevant cues such
as targets and attack types are not hypothesized by the model as a likely
explanation; and the binary reward signal is insufficient to guide reasoning.
To address these challenges, we propose ExPO-HM (Explain-then-Detect Policy
Optimization for Hateful Memes), inspired by the training and evaluation
process of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum
learning, and Conditional Decision Entropy (CDE) as both metric and reward for
reasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves
state-of-the-art performance on binary detection, fine-grained classification,
and reasoning quality, with up to 15\% and 17\% F1 improvement over the GRPO
and DPO baselines, respectively. By moving hateful meme detection from simple
binary alarms to explanation-driven detection, ExPO-HM provides accurate,
interpretable, and actionable moderation support.

</details>


### [58] [Next Semantic Scale Prediction via Hierarchical Diffusion Language Models](https://arxiv.org/abs/2510.08632)
*Cai Zhou,Chenyu Wang,Dinghuai Zhang,Shangyuan Tong,Yifei Wang,Stephen Bates,Tommi Jaakkola*

Main category: cs.CL

TL;DR: 该论文提出分层扩散语言模型（HDLM），通过层级语义映射和扩散技术提升文本生成质量，实验表现优于传统模型，困惑度更低。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型在处理语义层级上存在局限，因此需要一种新的方法将细粒度语义和粗粒度语义结合起来，提升语言模型的表现。

Method: 提出分层扩散语言模型（HDLM），利用分层词汇表，将详细语义的低层级token与粗粒度高层级token映射。通过扩散过程将token提升到更抽象语义层级，再通过逆过程预测更详细语义。并推导了扩散ELBO的封闭形式表达方式，提出了可灵活实现的训练技巧。

Result: HDLM在多项文本生成实验中优于基线模型，在验证和生成困惑度上均表现出更低值，效果显著。

Conclusion: HDLM扩展了离散扩散模型在语言建模上的能力，将分层语义有效结合，提升了语言建模的表现和灵活性。

Abstract: In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a
novel family of discrete diffusion models for language modeling. HDLM builds on
a hierarchical vocabulary where low-level tokens with detailed semantics are
surjectively mapped to high-level tokens with coarse-grained meanings. In the
forward process, each token is independently perturbed to its higher-level
ancestor with more abstract semantics according to the scheduler, while in the
reverse process the model progressively predicts the next, more detailed
semantics. Taken together, HDLM provides a general time-varying next semantic
scale prediction process for language modeling. We derive closed-form
expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM
can be implemented in a flexible manner while including the existing MDLM as a
special case. We also propose practical training techniques based on the
insights. Extensive text generation experiments validate the effectiveness of
HDLM, which demonstrates consistently lower validation and generative
perplexity than baselines.

</details>


### [59] [Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression](https://arxiv.org/abs/2510.08647)
*Chengzhengxu Li,Xiaoming Liu,Zhaohan Zhang,Shaochu Zhang,Shengchao Liu,Guoxin Ma,Yu Lan,Chao Shen*

Main category: cs.CL

TL;DR: 本文提出的UCoT方法能高效且自动地压缩思维链条，显著减少算力和延迟消耗，同时保持甚至提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 长链式思维能提升LLM推理能力，但带来了高计算开销与延迟。以往CoT压缩方法需要人工设计提示词或外部数据集，容易牺牲关键信息。因此，亟需自动化且有效的CoT压缩新方法。

Method: UCoT采用了小模型（压缩器）与大模型（执行器）的协作，其中压缩器生成富含推理信息的思维嵌入，执行器使用这一嵌入，通过奖励机制优化，在短推理过程下获得正确答案。

Result: UCoT在多项实验中表现优秀，显著压缩了推理链长度。应用于Qwen2.5-7B-Instruct，在GSM8K数据集中token使用减少50%，性能比最先进方法提升3.08%。

Conclusion: UCoT能够在大幅压缩CoT长度的情况下，保持甚至提升大型模型的推理能力。

Abstract: Recent developments have enabled advanced reasoning in Large Language Models
(LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high
computational costs and significant latency losses owing to the autoregressive
nature of generative LLMs. CoT compression aims to improve efficiency in the
reasoning process by reducing output length. Previous works trade reasoning
efficiency by either laborious discrete prompt designing or the construction of
external compressed CoT datasets that sacrifice key reasoning details. In this
work, we propose Upfront CoT (UCoT): an efficient reasoning framework with
upfront thought embedding to automate CoT compression. UCoT is a cooperative
workflow involving a small model (compressor) and a large model (executor). The
first stage of UCoT trains compressor to generate upfront thought embeddings
rich in reasoning information for the executor, avoiding the drawbacks of
manually designed prompts. The second stage optimizes executor to utilize
upfront thought embeddings to derive the correct answer with short reasoning,
using a reward mechanism. Extensive experiments show that UCoT maintains the
powerful reasoning ability of executor while significantly reducing the length
of CoT. It is worth mentioning that when applying UCoT to the
Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by
50\%, while the performance is 3.08\% higher than that of the state-of-the-art
(SOTA) method. The code and dataset are in supplementary material.

</details>


### [60] [Formalizing Style in Personal Narratives](https://arxiv.org/abs/2510.08649)
*Gustave Cortal,Alain Finkel*

Main category: cs.CL

TL;DR: 本文提出了一个自动分析个人叙事语言风格的新框架，将语言学、计算机科学和心理学结合，成功用大量梦境叙述和PTSD老兵案例展示了语言选择与心理状态的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管个人叙事中的语言风格对表达主观体验至关重要，但尚缺乏系统分析这些风格选择的正式框架。作者希望开发一个能够系统分析个人叙事风格的方法。

Method: 提出了一个结合功能语言学、计算机科学和心理学的新框架，通过机器学习方法自动抽取叙事文本中的语言特征（如过程、参与者、环境），并对这些顺序模式进行分析，关联到心理学观察。

Result: 应用该框架分析了数百个梦境叙述，包括一个创伤后应激障碍（PTSD）老兵的案例研究。该老兵的叙事表现出鲜明的语言模式，尤其是“言语过程”远多于“心理过程”，从而揭示出语言选择与心理状态之间的关系。

Conclusion: 文中提出的框架能够有效分析个人叙事中的语言风格选择，并揭示叙述语言使用与心理状态之间的内在联系，为理解主观体验提供了新的方法。

Abstract: Personal narratives are stories authors construct to make meaning of their
experiences. Style, the distinctive way authors use language to express
themselves, is fundamental to how these narratives convey subjective
experiences. Yet there is a lack of a formal framework for systematically
analyzing these stylistic choices. We present a novel approach that formalizes
style in personal narratives as patterns in the linguistic choices authors make
when communicating subjective experiences. Our framework integrates three
domains: functional linguistics establishes language as a system of meaningful
choices, computer science provides methods for automatically extracting and
analyzing sequential patterns, and these patterns are linked to psychological
observations. Using language models, we automatically extract linguistic
features such as processes, participants, and circumstances. We apply our
framework to hundreds of dream narratives, including a case study on a war
veteran with post-traumatic stress disorder. Analysis of his narratives
uncovers distinctive patterns, particularly how verbal processes dominate over
mental ones, illustrating the relationship between linguistic choices and
psychological states.

</details>


### [61] [A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data](https://arxiv.org/abs/2510.08663)
*Joe Watson,Ivan O'Conner,Chia-Wen Chen,Luning Sun,Fang Luo,David Stillwell*

Main category: cs.CL

TL;DR: 本研究创新性地结合LLM自动打分的自由文本与传统量表，开发了增强型心理测验。有实证结果显示，测量精度与准确性显著提升，方法可大规模拓展，有望用于临床健康等领域。


<details>
  <summary>Details</summary>
Motivation: 传统心理测评主要依赖结构化的量表评分，难以充分利用受访者自然语言表达中的丰富信息。随着大模型（LLM）的发展，作者希望通过结合LLM打分的文本和传统量表题项，提升测评的精度和信息量。

Method: 提出了一种新框架，将LLM对受访者自由文本的自动打分结果与量表分项结合，构成增强型测验。以抑郁量表为案例，在693名中学生真实数据和3000个合成数据上进行开发和评估。通过计算“题目信息量”，实证筛选出最有效的LLM打分指令，无需预标注数据或专家制定复杂评分规则。

Result: 在留出测试集上，增强型测验在测量的精确性和准确性上取得了显著提升。LLM生成的信息增益相当于原量表增加6.3（真实数据）到16.0（合成数据）个题项，有效增强了测验能力。

Conclusion: 该框架为心理测评领域带来了自动化评分的概念转变，突破了传统依赖标注数据和专家规则的瓶颈。它能够规模化利用转录文本，显著提升心理量表的测量效度，有望应用于临床健康及其他领域。

Abstract: Psychological assessments typically rely on structured rating scales, which
cannot incorporate the rich nuance of a respondent's natural language. This
study leverages recent LLM advances to harness qualitative data within a novel
conceptual framework, combining LLM-scored text and traditional rating-scale
items to create an augmented test. We demonstrate this approach using
depression as a case study, developing and assessing the framework on a
real-world sample of upper secondary students (n=693) and corresponding
synthetic dataset (n=3,000). On held-out test sets, augmented tests achieved
statistically significant improvements in measurement precision and accuracy.
The information gain from the LLM items was equivalent to adding between 6.3
(real data) and 16.0 (synthetic data) items to the original 19-item test. Our
approach marks a conceptual shift in automated scoring that bypasses its
typical bottlenecks: instead of relying on pre-labelled data or complex
expert-created rubrics, we empirically select the most informative LLM scoring
instructions based on calculations of item information. This framework provides
a scalable approach for leveraging the growing stream of transcribed text to
enhance traditional psychometric measures, and we discuss its potential utility
in clinical health and beyond.

</details>


### [62] [dInfer: An Efficient Inference Framework for Diffusion Language Models](https://arxiv.org/abs/2510.08666)
*Yuxin Ma,Lun Du,Lanning Wei,Kun Chen,Qian Xu,Kangyu Wang,Guofeng Feng,Guoshan Lu,Lin Liu,Xiaojing Qi,Xinyuan Zhang,Zhen Tao,Haibo Feng,Ziyun Jiang,Ying Xu,Zenan Huang,Yihong Zhuang,Haokai Xu,Jiaqi Hu,Zhenzhong Lan,Junbo Zhao,Jianguo Li,Da Zheng*

Main category: cs.CL

TL;DR: dInfer是一款高效、易扩展的扩散型大模型推理框架，通过模块化设计和系统优化，相比现有方法实现了数倍的推理速度提升，并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前扩散型大模型（dLLMs）逐渐涌现并具备固有并行特性，但由于缺乏高效标准的推理框架，限制了其广泛应用。该工作旨在解决这一痛点。

Method: 提出dInfer框架，将推理流程拆解为四个模块：模型、扩散迭代管理、解码策略和KV缓存管理，并在各模块融入新算法及系统级优化。通过算法与系统协同提升整体推理效率。

Result: 在LLaDA-MoE等模型上，dInfer在batch size为1时，能在HumanEval测试上超过1100 tokens/s，在8×H800 GPU上各类基准平均超过800 tokens/s。与Fast-dLLM相比，推理速度提升约10倍，与高度优化的AR模型QWen2.5-3B相比，速度提升2-3倍，性能无显著损失。

Conclusion: dInfer有效解决了扩散型大模型推理效率瓶颈，在保持输出质量的基础上显著加速推理过程，推动该类模型的实际应用与发展。

Abstract: Diffusion-based large language models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs, leveraging denoising-based generation
to enable inherent parallelism. Even more and more open-sourced dLLM models
emerge, yet their widespread adoption remains constrained by the lack of a
standardized and efficient inference framework. We present dInfer, an efficient
and extensible framework for dLLM inference. dInfer decomposes the inference
pipeline into four modular components-model, diffusion iteration manager,
decoding strategy, and KV-cache manager-and integrates novel algorithms for
each component alongside system-level optimizations. Through this combination
of algorithmic innovations and system enhancements, dInfer achieves substantial
efficiency gains without compromising output quality on LLaDA-MoE. At batch
size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800
tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to
prior systems, dInfer delivers $10\times$ speedup over Fast-dLLM while
maintaining similar model performance. Even compared with AR models (with a
comparable number of activation parameters and performance) QWen2.5-3B, which
is highly optimized with latest vLLM inference engine, dInfer still deliverers
$2$-$3\times$ speedup. The implementation of dInfer is open-sourced at
https://github.com/inclusionAI/dInfer.

</details>


### [63] [Scaling Laws for Code: A More Data-Hungry Regime](https://arxiv.org/abs/2510.08702)
*Xianzhen Luo,Wenzhen Zheng,Qingfu Zhu,Rongyi Zhang,Houyi Li,Siming Huang,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 本工作首次通过大规模实验系统性分析了代码大模型的训练扩展规律，发现其对数据需求更高、扩展规律不同于自然语言，Farseer law更适合拟合代码模型表现，且数据组合方式需视计算资源调整。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言大模型（LLMs）的训练规律已被广泛研究，但代码与自然语言存在如严格语法等根本性差异，因此现有规律是否同样适用于代码模型尚不明确。

Method: 作者通过对117次不同规模（模型参数0.2B到3.8B，训练token数2B到128B）的实验，实证性地分析了代码领域的大模型扩展规律，并分别拟合了Chinchilla law与Farseer law。此外，还额外进行了代码-自然语言混合训练的实验。

Result: Farseer law对代码大模型的拟合效果优于Chinchilla law；代码大模型随模型规模增长时表现出良好扩展性。同时，代码类模型对数据的需求比自然语言更高，需要更高的数据-参数比。在混合训练条件下，自然语言数据在低算力下可提升效果，但高算力下则不利于代码模型表现。

Conclusion: 代码大模型的扩展规律与自然语言模型存在差异，需针对代码场景单独研究其高效训练规律。Farseer law为更准确的拟合工具，并应注意采集更多的训练数据以及合理安排代码与自然语言数据的比例。

Abstract: Code Large Language Models (LLMs) are revolutionizing software engineering.
However, scaling laws that guide the efficient training are predominantly
analyzed on Natural Language (NL). Given the fundamental differences like
strict syntax between code and NL, it is unclear whether these laws are
directly applicable to code. To address this gap, we conduct the first
large-scale empirical study of scaling laws for code, comprising 117
experimental runs with model sizes from 0.2B to 3.8B and training tokens from
2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results
show that the more expressive Farseer law offers greater accuracy. Second, the
analysis reveals that Code LLMs scale effectively with model size. Crucially,
code represents a more data-hungry regime, requiring a substantially higher
data-to-parameter ratio than NL. Finally, two additional sets of experiments on
code-NL mixtures show that NL benefits resource-constrained scenarios, but
becomes a detriment at higher compute budgets.

</details>


### [64] [Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning](https://arxiv.org/abs/2510.08710)
*Li Zhang,Matthias Grabmair,Morgan Gray,Kevin Ashley*

Main category: cs.CL

TL;DR: 本文提出了基于三阶段推理的案例区分框架，评估LLM在法律领域的推理能力。结果显示，LLM表层推理准确但在更复杂任务中表现不佳，还浪费资源，提示现有模型难以胜任法律推理任务，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 美国法律实践高度依赖案例推理，专业人士需要通过类比与区分历史案例来论证当前案件。虽然大语言模型（LLM）在许多领域表现出色，但其在复杂而细腻的案例推理方面的能力仍需深入探究。

Method: 作者提出了一个形式化框架，将识别案件间关键区别的流程分解为三阶段推理任务。框架用“因素”来表征案件事实，通过法律知识层级进行组织，并设定可验证的规则来识别区别、分析论证支持及评估其重要性。通过对现代推理型LLM进行全面评估，来检验其推理水平。

Result: 实验结果发现，LLM在表层推理任务（任务1）上表现良好，但在层级推理（任务2）准确率下降至64.82%-92.09%，综合分析（任务3）更是仅有11.46%-33.99%的准确率。此外，模型在错误回答上的计算资源消耗反而更高，说明“思考更久”并不等于“思考更聪明”。

Conclusion: 该工作为LLM在复杂法律推理领域的能力分析提供了细粒度方法，同时揭示了当前模型在实现鲁棒、可信的法律AI方面存在的根本性局限。

Abstract: Case-based reasoning is a cornerstone of U.S. legal practice, requiring
professionals to argue about a current case by drawing analogies to and
distinguishing from past precedents. While Large Language Models (LLMs) have
shown remarkable capabilities, their proficiency in this complex, nuanced form
of reasoning needs further investigation. We propose a formal framework that
decomposes the process of identifying significant distinctions between cases
into three-stage reasoning tasks. Our framework models cases using factual
predicates called factors, organizes them into a legal knowledge hierarchy, and
defines verifiable rules for identifying distinctions, analyzing their
argumentative support, and evaluating their significance. Through comprehensive
evaluation of modern reasoning LLMs, we reveal a paradox: while models achieve
high accuracy on surface-level reasoning (Task 1), performance degrades on
hierarchical reasoning (Task 2: 64.82%-92.09%) and collapses on integrated
analysis (Task 3: 11.46%-33.99%). Most strikingly, we find that models
consistently expend more computational resources on incorrect responses than
correct ones, suggesting that "thinking longer" does not always mean "thinking
smarter." Our work provides a methodology for fine-grained analysis of LLM
reasoning capabilities in complex domains and reveals fundamental limitations
that must be addressed for robust and trustworthy legal AI.

</details>


### [65] [How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective](https://arxiv.org/abs/2510.08720)
*Xianzhen Luo,Jinyang Huang,Wenzhen Zheng,Qingfu Zhu,Mingzheng Xu,Yiheng Xu,Yuantao Fan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出了以二元矩阵秩为理论基础的新型测试基准构建方法，并开发了TC-Bench。实验发现主流方法检测能力有限，TC-Bench更真实反映模型不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能有效覆盖罕见关键错误，评价成本高且容易分数虚高，难以真实反映先进生成模型的测试能力，因此亟需更合理高效的测试用例基准。

Method: 将基准构建形式化为在二元代码-测试矩阵中寻找最优诊断基。提出了WrongSelect算法，对“错误代码”的选择进行高效近似，确保多样性最大化。通过大规模竞赛编程样例进行实证，并开发了TC-Bench基准。

Result: 提出了理论框架，有效确定最少“错误代码”及最小测试集，并构建了TC-Bench基准。实验表明，最先进的测试生成方法在TC-Bench上的排除率仅约60%，显示出明显诊断能力不足。

Conclusion: 现有的自动化测试用例评估基准存在高计算成本、分数膨胀和对琐碎错误偏好等问题。本文提出了以矩阵秩为基础的理论框架，并用该方法构建了更加紧凑、多样、抗膨胀的TC-Bench基准，有效揭示了当前主流测试生成方法的诊断能力不足。

Abstract: Evaluating test cases automatically generated by Large Language Models (LLMs)
is a critical yet challenging task. Existing benchmarks suffer from high
computational costs, score inflation, and a bias towards trivial bugs over
rare, critical faults. In this work, we ask two fundamental questions: (1) What
is the minimal set of wrong codes sufficient to represent the entire error
space? and (2) What is the minimal set of test cases needed to distinguish
them? We introduce a framework that formalizes benchmark construction as
finding an optimal diagnostic basis in a binary code-test matrix. The rank of
this matrix specifies the minimal number of independent error patterns (wrong
codes) and provides a tight upper bound on the number of test cases required
for complete fault coverage. Our objective is to identify a basis of size equal
to the matrix rank that maximizes internal diversity. To tackle this NP-hard
problem, we propose WrongSelect, an efficient approximation algorithm to select
maximally diverse wrong codes. Applying this framework to millions of
competitive programming submissions, we construct TC-Bench, a compact, diverse,
and inflation-resistant benchmark. Extensive experiments show that even the
most advanced test case generation methods achieve only ~60% exclusion rates on
TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is
available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is
at: https://github.com/Luowaterbi/TC-Bench.

</details>


### [66] [How Reliable is Language Model Micro-Benchmarking?](https://arxiv.org/abs/2510.08730)
*Gregory Yauney,Shahzaib Saqib Warraich,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 微基准测试无法以小样本量稳定地排序模型，仅能用更多样本时接近完整基准的可靠性，且随机采样方法竞争力不低。应权衡评测效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 语言模型开发过程中，完整基准测试往往耗时且成本高昂，微基准测试（micro-benchmarking）通过只评估极小的数据子集，成为一种更高效的评测方案。该文关注微基准测试能否像完整基准一样准确地对模型进行排序，并希望研究微基准测试的可靠性与有效性。

Method: 作者提出了一种元评测（meta-evaluation）方法，通过分析不同微基准数据集大小下模型间排序正确率，尤其关注微基准能否根据完整基准的性能差异准确区分模型。进一步比较了有针对性与随机抽样的不同微基准选择方法。

Result: 实验证明，在许多场景下，微基准测试无法像完整基准一样稳定地对模型进行排序，且无法显著优于随机抽样。即使模型性能差异较大，所需样本数量也远大于此前的建议（如250例而非10例），且随机抽样已和其他微基准方法表现相当。例如，在MMLU-Pro上仅用25例比较8B模型时，超过一半的模型对竟无法得到稳定排序。

Conclusion: 微基准测试虽然能提升评测效率，但在可靠性方面存在明显限制。对于性能较相近的模型，必须使用更大量的样本才有可能得到可靠排序，同时随机采样的表现也很有竞争力。因此，用户和开发者应在评测效率与结果可靠性之间谨慎权衡，不应盲目依赖过小规模的微基准。此外，研究为微基准测试的应用和方法提供了实用指导。

Abstract: Micro-benchmarking offers a solution to the often prohibitive time and cost
of language model development: evaluate on a very small subset of existing
benchmarks. Can these micro-benchmarks, however, rank models as consistently as
the full benchmarks they replace? And can they rank models more consistently
than selecting a random subset of data points? In many scenarios, we find that
the answer is no. We introduce a meta-evaluation measure for micro-benchmarking
which investigates how well a micro-benchmark can rank two models as a function
of their performance difference on the full benchmark. This approach can
determine which model pairs can be ranked correctly by a micro-benchmark,
allowing for a finer-grained analysis of the trade-off between micro-benchmark
size and reliability. Prior work has suggested selecting as few as 10 examples;
we find that no micro-benchmarking method can consistently rank model pairs 3.5
points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In
order to consistently rank model pairs with relatively similar performances, we
show that often as many as 250 examples must be selected, at which point random
sampling is competitive with existing micro-benchmarking methods. When
comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25
examples, we find that more than half of pairwise comparisons are not likely to
be preserved. Our work provides actionable guidance for both micro-benchmark
users and developers in navigating the trade-off between evaluation efficiency
and reliability.

</details>


### [67] [Coordinates from Context: Using LLMs to Ground Complex Location References](https://arxiv.org/abs/2510.08741)
*Tessa Masis,Brendan O'Connor*

Main category: cs.CL

TL;DR: 本文提出了基于LLM的地理编码策略，可更有效处理非结构化文本中的复合位置表达，并证明小型微调模型具备媲美大型模型的能力。


<details>
  <summary>Details</summary>
Motivation: 面对非结构化文本中的复合型位置引用，传统地理编码方法难以有效处理。近期研究表明LLM在地理空间推理方面有潜力，因此希望利用LLM提升复合地理编码的效果。

Method: 分析LLM在地理空间知识和推理能力上的表现，基于这些结论，设计并应用了一种LLM驱动的地理编码策略。通过比较不同模型（小型微调和大型预训练）的性能，验证了方法有效性。

Result: 基于LLM的方法在地理编码复合位置任务上取得了性能提升，小型微调LLM的效果与大型现成模型相当。

Conclusion: 提出的基于LLM的方法在地理编码复合位置识别任务中提升了性能；且经过微调的小型LLM可达到与大型预训练模型相当的效果。

Abstract: Geocoding is the task of linking a location reference to an actual geographic
location and is essential for many downstream analyses of unstructured text. In
this paper, we explore the challenging setting of geocoding compositional
location references. Building on recent work demonstrating LLMs' abilities to
reason over geospatial data, we evaluate LLMs' geospatial knowledge versus
reasoning skills relevant to our task. Based on these insights, we propose an
LLM-based strategy for geocoding compositional location references. We show
that our approach improves performance for the task and that a relatively small
fine-tuned LLM can achieve comparable performance with much larger
off-the-shelf models.

</details>


### [68] [Measuring Moral LLM Responses in Multilingual Capacities](https://arxiv.org/abs/2510.08776)
*Kimaya Basu,Savi Kolari,Allison Yu*

Main category: cs.CL

TL;DR: 本文系统评测了主流LLM在多语种和多维度下的响应表现，发现GPT-5在准确性、安全性等方面表现最优，而其它模型在一致性和安全性上存在不足。研究呼吁继续关注语言差异对模型响应的影响，并推动相关领域改进。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在全球范围内的广泛应用，尤其是在多语言环境下，人们对其多语种响应的准确性和安全性的关注日益增加。当前亟需理解和规范LLM在不同语言下的表现，尤其在涉及敏感或安全相关内容时。

Method: 通过设计五个评估维度，采用大规模测试和基准数据集，对前沿和开源模型进行多语种响应能力测试。评估工具包括五分制评分标准以及一个LLM裁判系统，分别在高资源和低资源语言环境下测量LLM的准确性和一致性。

Result: GPT-5在各评估类别中的平均表现最佳，尤其在“同意与自主权”、“伤害预防与安全”类别分别获得3.56和4.73分。而Gemini 2.5 Pro在上述两项得分最低，分别为1.39和1.98分。其他模型在不同语言和类别中的一致性较差。

Conclusion: 不同语言和类别间，LLM响应存在显著差异。GPT-5在多语种环境下表现突出，但整体行业在敏感话题和安全防护领域仍需提升。该研究强调了对语言差异影响LLM表现的持续测试与优化的重要性。

Abstract: With LLM usage becoming widespread across countries, languages, and humanity
more broadly, the need to understand and guardrail their multilingual responses
increases. Large-scale datasets for testing and benchmarking have been created
to evaluate and facilitate LLM responses across multiple dimensions. In this
study, we evaluate the responses of frontier and leading open-source models in
five dimensions across low and high-resource languages to measure LLM accuracy
and consistency across multilingual contexts. We evaluate the responses using a
five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed
the best on average in each category, while other models displayed more
inconsistency across language and category. Most notably, in the Consent &
Autonomy and Harm Prevention & Safety categories, GPT scored the highest with
averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages
of 1.39 and 1.98, respectively. These findings emphasize the need for further
testing on how linguistic shifts impact LLM responses across various categories
and improvement in these areas.

</details>


### [69] [Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models](https://arxiv.org/abs/2510.08798)
*S M Rafiuddin,Muntaha Nujat Khan*

Main category: cs.CL

TL;DR: 论文提出了一种通用的高效Transformer机制Adaptive Retention，在保证绝大部分模型性能的条件下，仅需保留部分token就可大幅降低内存消耗并提升速度，为长序列任务带来实用的效率提升，无需改变原有注意力结构。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制计算复杂度随输入序列长度呈平方增长，限制了其在长上下文任务中的应用，因此迫切需要能够有效降低计算和内存开销的方法。

Method: 采用基于Bernoulli门控机制，通过Hard-Concrete/变分松弛方式训练，在每层动态地选择需要保留的token，并通过top-M规则在推理阶段严格控制选中token数；这种方法可直接集成到标准编码器中，无需更改原始注意力或任务头部结构。

Result: 在文本分类、抽取式问答和长文档摘要等任务上，仅保留30-50% token后，模型性能几乎不受影响（保留>= 95%的完整模型性能），同时峰值内存降低35-45%，吞吐性能提升最高达1.8倍。

Conclusion: 提出的Adaptive Retention方法能够在几乎不损失模型性能的情况下，大幅减少内存消耗并提升推理速度，从而提升Transformer在长序列任务中的应用效率。

Abstract: Transformer attention scales quadratically with sequence length O(n^2),
limiting long-context use. We propose Adaptive Retention, a probabilistic,
layer-wise token selection mechanism that learns which representations to keep
under a strict global budget M. Retention is modeled with Bernoulli gates
trained via a Hard-Concrete/variational relaxation and enforced with a simple
top-M rule at inference, making the method differentiable and drop-in for
standard encoders. Across classification, extractive QA, and long-document
summarization, keeping only 30-50% of tokens preserves >= 95% of full-model
performance while cutting peak memory by ~35-45% and improving throughput by up
to ~1.8x. This architecture-agnostic approach delivers practical long-context
efficiency without modifying base attention or task heads.

</details>


### [70] [Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective](https://arxiv.org/abs/2510.08800)
*Wangjie You,Xusheng Wang,Xing Wang,Wenxiang Jiao,Chao Feng,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了针对中文多步推理的CCMOR基准，并用其评测多种LLM，发现现有模型在复杂知识融合与推理能力上有限，但检索增强技术能显著改善这些不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在中文领域中的推理能力评估不足，需要一个综合性基准来分析其在多步推理与中文知识融合方面的表现。

Method: 构建了CCMOR（Chinese Commonsense Multi-hop Reasoning）基准，通过从现有问答数据集筛选平衡的种子集，借助LLM生成基于事实链条的多步推理问题，并用专家人工审核生成问题，保障数据质量。

Result: 通过CCMOR对业界主流LLM进行评估，发现它们在处理长尾知识与复杂推理上仍有不足。采用检索增强生成方式，显著提升了模型在知识密集型推理上的表现。

Conclusion: 当前LLM在中文多步推理及长尾知识处理方面仍存在明显短板，可通过引入检索增强生成方法获得有效改进。

Abstract: While Large Language Models (LLMs) have demonstrated advanced reasoning
capabilities, their comprehensive evaluation in general Chinese-language
contexts remains understudied. To bridge this gap, we propose Chinese
Commonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate
LLMs' ability to integrate Chinese-specific factual knowledge with multi-step
logical reasoning. Specifically, we first construct a domain-balanced seed set
from existing QA datasets, then develop an LLM-powered pipeline to generate
multi-hop questions anchored on factual unit chains. To ensure the quality of
resulting dataset, we implement a human-in-the-loop verification system, where
domain experts systematically validate and refine the generated questions.
Using CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent
limitations in LLMs' ability to process long-tail knowledge and execute
knowledge-intensive reasoning. Notably, retrieval-augmented generation
substantially mitigates these knowledge gaps, yielding significant performance
gains.

</details>


### [71] [MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding](https://arxiv.org/abs/2510.08804)
*Siddeshwar Raghavan,Tanwi Mallick*

Main category: cs.CL

TL;DR: MOSAIC是一种无须训练、多智能体LLM框架，能高效解决科学编程任务中的复杂问题，准确性与可解释性优于现有方法，是科学代码自动生成领域的重要进展。


<details>
  <summary>Details</summary>
Motivation: 在科学编程任务中，现有大语言模型面临领域知识深、算法复杂、需步骤分解等挑战，且错误纠正和解释能力有限，因此需要新的方法解决科学代码生成的困难。

Method: MOSAIC提出多智能体LLM框架，不需要额外训练，涵盖自我反思、逻辑推演、编写代码和调试。采用‘师生’范式，结合Consolidated Context Window（CCW），实现逐步问题分解、针对性纠错和缓解大模型幻觉。

Result: MOSAIC在科学编程基准测试中表现优异，其专用多智能体架构在准确性、鲁棒性及可解释性均优于现有方法。

Conclusion: 多智能体、专用逻辑的LLM框架能有效提升科学代码生成任务的表现，为复杂科学问题求解提供更强工具支持。

Abstract: We present MOSAIC, a multi-agent Large Language Model (LLM) framework for
solving challenging scientific coding tasks. Unlike general-purpose coding,
scientific workflows require algorithms that are rigorous, interconnected with
deep domain knowledge, and incorporate domain-specific reasoning, as well as
algorithm iteration without requiring I/O test cases. Many scientific problems
also require a sequence of subproblems to be solved, leading to the final
desired result. MOSAIC is designed as a training-free framework with specially
designed agents to self-reflect, create the rationale, code, and debug within a
student-teacher paradigm to address the challenges of scientific code
generation. This design facilitates stepwise problem decomposition, targeted
error correction, and, when combined with our Consolidated Context Window
(CCW), mitigates LLM hallucinations when solving complex scientific tasks
involving chained subproblems. We evaluate MOSAIC on scientific coding
benchmarks and demonstrate that our specialized agentic framework outperforms
existing approaches in terms of accuracy, robustness, and interpretability.

</details>


### [72] [The Model's Language Matters: A Comparative Privacy Analysis of LLMs](https://arxiv.org/abs/2510.08813)
*Abhishek K. Mishra,Antoine Boutet,Lucas Magnana*

Main category: cs.CL

TL;DR: 不同语言的结构对LLM隐私泄露影响显著，意大利语最易泄露，英语和其他语言有不同特性，强调需结合语言特性的隐私保护措施。


<details>
  <summary>Details</summary>
Motivation: 现有LLM隐私评估主要针对英文，忽视了多语言环境下的语言结构对隐私风险的影响。各语言的形态复杂度和冗余度不同，故需系统性分析语言因素如何影响模型泄露。

Method: 作者对英文、法文、西班牙文和意大利文医学语料上训练的LLM进行隐私攻击测试，分析六种语言学指标，通过三种攻击方式（信息提取、反事实记忆、成员归属推断）量化隐私泄露情况。

Result: 意大利语因冗余和分词粒度泄露最强；英语成员可分离性高，但泄露不如意大利语；法语和西班牙语由于形态复杂，泄露较小，表现更有韧性。

Conclusion: 语言结构显著影响大型语言模型（LLM）的隐私泄露风险，不同语言之间存在明显差异。应在LLM部署中引入语言敏感的隐私保护机制。

Abstract: Large Language Models (LLMs) are increasingly deployed across multilingual
applications that handle sensitive data, yet their scale and linguistic
variability introduce major privacy risks. Mostly evaluated for English, this
paper investigates how language structure affects privacy leakage in LLMs
trained on English, Spanish, French, and Italian medical corpora. We quantify
six linguistic indicators and evaluate three attack vectors: extraction,
counterfactual memorization, and membership inference. Results show that
privacy vulnerability scales with linguistic redundancy and tokenization
granularity: Italian exhibits the strongest leakage, while English shows higher
membership separability. In contrast, French and Spanish display greater
resilience due to higher morphological complexity. Overall, our findings
provide the first quantitative evidence that language matters in privacy
leakage, underscoring the need for language-aware privacy-preserving mechanisms
in LLM deployments.

</details>


### [73] [Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs](https://arxiv.org/abs/2510.08825)
*Jia Ao Sun,Hao Yu,Fabrizio Gotti,Fengran Mo,Yihong Wu,Yuchen Hui,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本论文提出了Search-on-Graph（SoG）方法，让大语言模型能高效逐步在知识图谱上导航推理，无需预先检索大子图或复杂规划，取得了跨多个KGQA基准的最佳或显著提升成果，并展现了对复杂结构的强适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽表现出推理能力，但在知识密集型、多跳问题上不可靠。现有的知识图谱问答方法也面临诸多权衡（如SPARQL查询脆弱、子图检索噪音、搜索空间爆炸），故需新的解决策略。

Method: 提出了Search-on-Graph（SoG）框架，通过一个精心设计的单一Search函数，让LLM在知识图谱上进行逐步、信息指导的交互式导航，而不是预先规划路径或检索大子图。该方法采用“观察—再导航”原则，按需适应不同图谱结构和高度连接节点。

Result: 在六个KGQA基准数据集（涵盖Freebase和Wikidata）上，SoG无须微调即实现了最先进性能，Wikidata提升幅度达16%，Freebase也有稳定提升。

Conclusion: SoG框架有效提升了LLM在知识图谱问答中的推理和检索能力，尤其是适应多样结构和实际高性能表现，优于现有方法。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities
yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss
long-tail facts, hallucinate when uncertain, and their internal knowledge lags
behind real-world change. Knowledge graphs (KGs) offer a structured source of
relational evidence, but existing KGQA methods face fundamental trade-offs:
compiling complete SPARQL queries without knowing available relations proves
brittle, retrieving large subgraphs introduces noise, and complex agent
frameworks with parallel exploration exponentially expand search spaces. To
address these limitations, we propose Search-on-Graph (SoG), a simple yet
effective framework that enables LLMs to perform iterative informed graph
navigation using a single, carefully designed \textsc{Search} function. Rather
than pre-planning paths or retrieving large subgraphs, SoG follows an
``observe-then-navigate'' principle: at each step, the LLM examines actual
available relations from the current entity before deciding on the next hop.
This approach further adapts seamlessly to different KG schemas and handles
high-degree nodes through adaptive filtering. Across six KGQA benchmarks
spanning Freebase and Wikidata, SoG achieves state-of-the-art performance
without fine-tuning. We demonstrate particularly strong gains on Wikidata
benchmarks (+16\% improvement over previous best methods) alongside consistent
improvements on Freebase benchmarks.

</details>


### [74] [Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859)
*Ragib Amin Nihal,Rui Wen,Kazuhiro Nakadai,Jun Sakuma*

Main category: cs.CL

TL;DR: 该论文提出了PE-CoA框架，实现了多种对话模式下的高级多轮越狱攻击，揭示了大型语言模型安全训练的局限，并呼吁发展针对不同对话模式的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在多轮“越狱”攻击下依然脆弱，攻击者可以通过逐步利用对话上下文来绕过安全约束。这些攻击涉及多种有害类别，并采用不同的对话策略。当前的多轮越狱方法多依赖于启发式探索，难以揭示模型深层次的弱点。对话模式与模型脆弱性的关联尚未明确。

Method: 提出了一种名为 PE-CoA（Pattern Enhanced Chain of Attack，模式增强链式攻击）的方法，通过五类对话模式系统性构建高效的多轮越狱攻击，并以更自然的对话形式实现。作者在12个LLM模型、覆盖10种有害类别上进行了评估。

Result: PE-CoA 大幅提升了多轮越狱攻击的成功率，发现模型在不同对话模式下表现出不同的脆弱性特征：对某一模式的鲁棒性并不能泛化到其他模式，且同一模型家族间具有相似的失败模式。

Conclusion: LLM在对抗复杂越狱策略时依然存在模式相关的漏洞，现有安全训练措施无法全面覆盖所有对话模式，安全防御需结合攻击模式特性设计，提升模型整体鲁棒性。

Abstract: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking
attacks that exploit conversational context to bypass safety constraints
gradually. These attacks target different harm categories (like malware
generation, harassment, or fraud) through distinct conversational approaches
(educational discussions, personal experiences, hypothetical scenarios).
Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc
exploration strategies, providing limited insight into underlying model
weaknesses. The relationship between conversation patterns and model
vulnerabilities across harm categories remains poorly understood. We propose
Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation
patterns to construct effective multi-turn jailbreaks through natural dialogue.
Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve
state-of-the-art performance, uncovering pattern-specific vulnerabilities and
LLM behavioral characteristics: models exhibit distinct weakness profiles where
robustness to one conversational pattern does not generalize to others, and
model families share similar failure modes. These findings highlight
limitations of safety training and indicate the need for pattern-aware
defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA

</details>


### [75] [Quality Estimation Reranking for Document-Level Translation](https://arxiv.org/abs/2510.08870)
*Krzysztof Mrozinski,Minji Kang,Ahmed Khota,Vincent Michael Sutanto,Giovanni Gatti De Giacomo*

Main category: cs.CL

TL;DR: 将QE重排序方法从句子级拓展到文档级机器翻译，利用学习型和LLM指标显著提升了文档级译文的质量，且计算开销低，显示其在实际文档翻译中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 以往的翻译质量估计（QE）重排序多应用于句子级别，对日益重要的文档级机器翻译的研究和应用较为缺乏。本文想探索在文档级机器翻译中，QE重排序的有效性和潜力。

Method: 本文采用不同的学习型和大语言模型（LLM）为基础的QE指标，对文档级（而非传统句子级）翻译进行重排序实验。使用包括SLIDE和GEMBA-DA在内的各种QE方法，评估其在不同候选译文数量和输入长度下的表现。

Result: 实验发现，通过最佳学习型指标SLIDE，在两候选下BLEURT-20分提高2.00分，32候选下提高5.09分；最佳LLM指标GEMBA-DA下分别提升1.63和4.30分。在长文档（512-1024词）下，使用32候选时，SLIDE和GEMBA-DA分别提升2.34和1.40分。

Conclusion: 文档级翻译的质量估计重排序在有效提升翻译质量的同时，仅需很少的计算开销，在合适的模型和硬件下具有实际应用价值。

Abstract: Quality estimation (QE) reranking is a form of quality-aware decoding which
aims to improve machine translation (MT) by scoring and selecting the best
candidate from a pool of generated translations. While known to be effective at
the sentence level, its application to the increasingly prominent domain of
document-level translation remains underexplored. In this work, we evaluate QE
reranking performance on document-level (rather than the typical
sentence-level) translation, using various learned and large language model
(LLM)-based QE metrics. We find that with our best learned metric, SLIDE,
BLEURT-20 scores improve by +2.00 with only two candidates, and by +5.09 with
32, across both decoder-only LLM models and encoder-decoder neural machine
translation (NMT) models. Using the best LLM-based metric, GEMBA-DA, gains of
+1.63 and +4.30 are achieved under the same conditions. Although gains shrink
with longer inputs, reranking with 32 candidates yields improvements of +2.34
(SLIDE) and +1.40 (GEMBA-DA) on our longest documents (512-1024 source tokens).
These findings demonstrate the practical value of document-level QE, with
minimal runtime overhead given suitable translation models and hardware.

</details>


### [76] [FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs](https://arxiv.org/abs/2510.08886)
*Yan Wang,Keyi Wang,Shanshan Yang,Jaisal Patel,Jeff Zhao,Fengran Mo,Xueqing Peng,Lingfei Qian,Jimin Huang,Guojun Xiong,Xiao-Yang Liu,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文提出了结构感知、多文档的财务审计基准FinAuditing，并用其评测了13款主流LLM，揭示了当前模型在复杂层级结构与本体驱动的财务推理上的明显不足，相关数据集已公开。


<details>
  <summary>Details</summary>
Motivation: GAAP和XBRL的复杂结构让自动化审计变得极具挑战，而当前大型语言模型（LLM）虽然擅长处理非结构化文本，但在涉及结构化、层级和有丰富本体分类的财务文件推理方面能力未知。作者旨在系统性测评LLM在财务审计中的推理效能。

Method: 构建了FinAuditing，这是一个与本体结构对齐、面向结构化多文档的基准，使用真实US-GAAP合规的XBRL文件，设计了语义一致性（FinSM）、关系一致性（FinRE）、数值一致性（FinMR）三大子任务，以及集成了检索、分类和推理等多维评测框架。

Result: 在对13个先进LLM进行零样本实验后，发现它们在语义、关系和数学推理维度表现不稳定，尤其在处理多文档层级结构推理时，准确率下降高达60-90%。

Conclusion: 现有LLM在基于本体和结构的财务推理方面存在系统性不足，FinAuditing为发展可信、结构感知和合规的智能财务系统奠定了基础。

Abstract: The complexity of the Generally Accepted Accounting Principles (GAAP) and the
hierarchical structure of eXtensible Business Reporting Language (XBRL) filings
make financial auditing increasingly difficult to automate and verify. While
large language models (LLMs) have demonstrated strong capabilities in
unstructured text understanding, their ability to reason over structured,
interdependent, and taxonomy-driven financial documents remains largely
unexplored. To fill this gap, we introduce FinAuditing, the first
taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs
on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,
FinAuditing defines three complementary subtasks, FinSM for semantic
consistency, FinRE for relational consistency, and FinMR for numerical
consistency, each targeting a distinct aspect of structured auditing reasoning.
We further propose a unified evaluation framework integrating retrieval,
classification, and reasoning metrics across these subtasks. Extensive
zero-shot experiments on 13 state-of-the-art LLMs reveal that current models
perform inconsistently across semantic, relational, and mathematical
dimensions, with accuracy drops of up to 60-90% when reasoning over
hierarchical multi-document structures. Our findings expose the systematic
limitations of modern LLMs in taxonomy-grounded financial reasoning and
establish FinAuditing as a foundation for developing trustworthy,
structure-aware, and regulation-aligned financial intelligence systems. The
benchmark dataset is available at Hugging Face.

</details>


### [77] [Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR](https://arxiv.org/abs/2510.08892)
*Haomin Zhuang,Yujun Zhou,Taicheng Guo,Yue Huang,Fangxu Liu,Kai Song,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 研究提出了依据token类型分别调整采样温度的多温度调度策略，在强化学习下显著提升了LLM推理能力，实现探索性与准确性的兼顾。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法通过限制模型的更新间接促进探索，但在实际token生成阶段并未明确促进模型的探索行为。而不同token在推理任务中扮演着不同角色（高熵推理token/低熵知识token），这带来了温度调节的新机会。

Method: 在生成过程中，为不同类型的token施加不同的采样温度：推理token采用较高温度促进探索，知识token则保持较低温度保证事实正确性。系统性地评估多温度调度策略在强化学习中的效果。

Result: 在多个推理基准测试上，采用多温度调度策略显著提升了大型语言模型的推理表现。

Conclusion: 通过对推理和知识token分别调整采样温度，强化学习训练下的LLM推理能力获得有效提升。多温度策略为模型探索与事实正确性之间取得了更优平衡。

Abstract: Reinforcement Learning has demonstrated substantial improvements in the
reasoning abilities of Large Language Models (LLMs), exhibiting significant
applicability across various domains. Recent research has identified that
tokens within LLMs play distinct roles during reasoning tasks, categorizing
them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior
approaches have typically focused on restricting updates to indirectly
encourage exploration, yet they do not explicitly facilitate exploratory
behavior during the token generation stage itself. In this work, we introduce a
complementary approach that explicitly promotes exploration during sampling by
applying distinct temperature settings for different token types. Specifically,
our method employs higher temperatures for reasoning tokens to actively
encourage exploration, while retaining lower temperatures for knowledge tokens
to maintain factual correctness. Furthermore, we systematically investigate
various multi-temperature scheduling strategies and their impacts within
reinforcement learning contexts. Empirical evaluations on several reasoning
benchmarks demonstrate that our approach significantly enhances the reasoning
performance of LLMs. The code is available at
https://github.com/zhmzm/Multi_Temperature_Verl.git.

</details>


### [78] [A Unified Biomedical Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2510.08902)
*Tengxiao Lv,Ling Luo,Juntao Li,Yanhua Wang,Yuchen Pan,Chao Liu,Yanan Wang,Yan Jiang,Huiyi Lv,Yuanyuan Sun,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文将生物医学命名实体识别任务转化为生成式问题，结合符号标注和对比学习有效处理嵌套及多语言实体，方法在中英文多数据集上取得SOTA效果，并展现强大零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别（BioNER）对于医学信息提取和知识发现至关重要，但现有方法在处理嵌套实体、实体边界模糊和跨语言泛化方面存在挑战。针对这些问题，本文提出新的解决方案。

Method: 本文将BioNER任务转化为文本生成任务，并设计了符号标注策略，能够同时处理平面和嵌套实体并明确边界。此外，运用多任务双语联合微调和对比学习实体选择器，提升多语言泛化能力并过滤错误预测。

Result: 在四个基准数据集和两个未见语料上的实验表明，提出的方法在各项指标上表现优异，达到了最新的最先进性能，并且在零样本跨语言泛化方面表现出强大鲁棒性。

Conclusion: 基于大语言模型的统一BioNER框架创新性地解决了嵌套实体、边界模糊和跨语言泛化难题，具有可观的实际应用前景。

Abstract: Accurate recognition of biomedical named entities is critical for medical
information extraction and knowledge discovery. However, existing methods often
struggle with nested entities, entity boundary ambiguity, and cross-lingual
generalization. In this paper, we propose a unified Biomedical Named Entity
Recognition (BioNER) framework based on Large Language Models (LLMs). We first
reformulate BioNER as a text generation task and design a symbolic tagging
strategy to jointly handle both flat and nested entities with explicit boundary
annotation. To enhance multilingual and multi-task generalization, we perform
bilingual joint fine-tuning across multiple Chinese and English datasets.
Additionally, we introduce a contrastive learning-based entity selector that
filters incorrect or spurious predictions by leveraging boundary-sensitive
positive and negative samples. Experimental results on four benchmark datasets
and two unseen corpora show that our method achieves state-of-the-art
performance and robust zero-shot generalization across languages. The source
codes are freely available at https://github.com/dreamer-tx/LLMNER.

</details>


### [79] [Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors](https://arxiv.org/abs/2510.08907)
*Xin Liu,RunSong Zhao,PengCheng Huang,XinYu Liu,JunYi Xiao,ChunYang Xiao,Tong Xiao,Shengxiang Gao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: SAC是一种全新上下文压缩方法，不需自编码任务训练，通过关键token选取和双向注意力机制，显著提升了大语言模型的压缩效率和实际下游表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在处理长文本推理时面临速度瓶颈，尽管已有方法尝试通过上下文压缩来提升推理效率，但大多数方法主要依赖于自编码任务（如重构）来训练与具体任务无关的压缩token，这导致压缩效果与实际下游应用任务存在偏差。

Method: 提出了一种新的语义锚定压缩（Semantic-Anchor Compression, SAC）方法，不再依赖自编码训练任务，而是通过从原始上下文中直接选择称为“anchor tokens”的关键token，并整合其key-value表示，达到压缩语境的目的。SAC包括两项关键设计：(1) 利用“anchor embedding”帮助识别关键信息token；(2) 双向注意力修正机制，使anchor tokens能捕获全局上下文信息。

Result: SAC在各种压缩比下均优于现有上下文压缩方法。在MRQA的分布外测试中，SAC在5倍压缩条件下比强基线提升了1个EM分数，且随着压缩比的提升，优势进一步扩大。

Conclusion: SAC方法通过结构创新绕开了自编码训练带来的偏差，能更有效实现上下文压缩，有望提升大模型推理效率且更好地服务实际任务需求。实验结果充分验证了其优越性。

Abstract: Context compression presents a promising approach for accelerating large
language model (LLM) inference by compressing long contexts into compact
representations. Current context compression methods predominantly rely on
autoencoding tasks to train context-agnostic compression tokens to compress
contextual semantics. While autoencoding tasks enable compression tokens to
acquire compression capabilities, compression via autoencoding tasks creates a
fundamental mismatch: the models are optimized for reconstruction that diverge
from actual downstream tasks, thereby weakening the features more beneficial
for real-world usage. We propose Semantic-Anchor Compression (SAC), a novel
method that shifts from autoencoding task based compression to an architecture
that is equipped with this compression capability \textit{a priori}. Instead of
training models to compress contexts through autoencoding tasks, SAC directly
selects so-called anchor tokens from the original context and aggregates
contextual information into their key-value (KV) representations. By deriving
representations directly from the contextual tokens, SAC eliminates the need
for autoencoding training. To ensure compression performance while directly
leveraging anchor tokens, SAC incorporates two key designs: (1) anchor
embeddings that enable the compressor to identify critical tokens, and (2)
bidirectional attention modification that allows anchor tokens to capture
information from the entire context. Experimental results demonstrate that SAC
consistently outperforms existing context compression methods across various
compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves
1 EM improvement at 5x compression over strong baselines, with increasing
advantages at higher compression ratios.

</details>


### [80] [Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions](https://arxiv.org/abs/2510.08915)
*Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 论文发现LLM内部会形成类似人类刻板印象的模式，这些印象可用线性模型解码，并且影响模型输出质量与模糊性。prompt的不同特征也会对印象形成造成影响。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在处理文本输入时，可能在内部生成类似于人类印象和刻板印象的模式。理解这种“人工印象”如何影响模型行为和输出，有助于解释模型的偏见和表现。

Method: 研究者引入了“人工印象”概念，并利用二元刻板印象内容模型（SCM）来建模。通过在线性探针上拟合用生成的prompt预测内部印象分数，并分析该分数与模型行为、prompt特征的关系。

Result: 1. LLM在被直接问及时报告印象表现不一致，但这些印象可以通过线性探针在模型隐层表示中较一致地解码出来。2. prompt的人工印象能够预测模型回应中的质量及模糊性（hedging）使用。3. prompt中的内容、风格和方言特征会影响LLM的人工印象。

Conclusion: LLM会在内部生成可测量的人工印象，这些印象可预测模型的输出特征。即使模型输出不直接反映印象，模型的内部表示也包含印象信息。理解人工印象有助于揭示模型偏见以及影响其下游行为的原因。

Abstract: We introduce and study artificial impressions--patterns in LLMs' internal
representations of prompts that resemble human impressions and stereotypes
based on language. We fit linear probes on generated prompts to predict
impressions according to the two-dimensional Stereotype Content Model (SCM).
Using these probes, we study the relationship between impressions and
downstream model behavior as well as prompt features that may inform such
impressions. We find that LLMs inconsistently report impressions when prompted,
but also that impressions are more consistently linearly decodable from their
hidden representations. Additionally, we show that artificial impressions of
prompts are predictive of the quality and use of hedging in model responses. We
also investigate how particular content, stylistic, and dialectal features in
prompts impact LLM impressions.

</details>


### [81] [SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures](https://arxiv.org/abs/2510.08942)
*Jiaming Wang,Zhe Tang,Yilin Jin,Peng Ding,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 本文提出了以实际商业流程为基础的SOP-Maze基准，涵盖397个复杂SOP任务，系统评价了当前LLM在流程跟随、真实对话和复杂计算等方面的能力，发现主流模型在这些挑战性任务表现不佳，为模型优化与企业应用指明了方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各领域广泛应用，尤其是作为特定领域代理，评估其在实际商业流程中的表现显得尤为重要。而商业场景通常涉及复杂的标准操作流程（SOP），但当前在此类环境下对LLM能力的评估尚不充分。本文旨在填补这一研究空白。

Method: 提出了SOP-Maze，一个基于真实商业数据构建的评测基准，涵盖23个复杂SOP场景的397项任务。进一步将任务分为两个广类：LRS（侧向根系统，考察精确选项选择）与HRS（中心根系统，重逻辑分支推理），并在各类主流LLM上进行了广泛实验和性能分析。

Result: 几乎所有最先进的大语言模型在SOP-Maze任务上表现不佳。主要错误包括：流程跟随困难（route blindness）、真实对话脆弱（conversational fragility）、以及复杂情境下的计算错误。此外，对模型在广度（LRS）和深度（HRS）任务上的表现进行了系统性分析。

Conclusion: 当前大语言模型在复杂企业SOP流程任务中存在明显不足，尤其在流程理解、对话把控以及计算推理等环节。SOP-Maze为模型能力提升和未来研究提供了更具挑战性的评估平台。

Abstract: As large language models (LLMs) are widely deployed as domain-specific
agents, many benchmarks have been proposed to evaluate their ability to follow
instructions and make decisions in real-world scenarios. However, business
scenarios often involve complex standard operating procedures (SOPs), and the
evaluation of LLM capabilities in such contexts has not been fully explored. To
bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world
business data and adapted into a collection of 397 tasks from 23 complex SOP
scenarios. We further categorize SOP tasks into two broad classes: Lateral Root
System (LRS), representing wide-option tasks that demand precise selection; and
Heart Root System (HRS), which emphasizes deep logical reasoning with complex
branches. Extensive experiments reveal that nearly all state-of-the-art models
struggle with SOP-Maze. We conduct a comprehensive analysis and identify three
key error categories: (i) route blindness: difficulty following procedures;
(ii) conversational fragility: inability to handle real dialogue nuances; and
(iii) calculation errors: mistakes in time or arithmetic reasoning under
complex contexts. The systematic study explores LLM performance across SOP
tasks that challenge both breadth and depth, offering new insights for
improving model capabilities. We have open-sourced our work on
https://github.com/ADoublLEN/SOP-Maze.

</details>


### [82] [A Human Behavioral Baseline for Collective Governance in Software Projects](https://arxiv.org/abs/2510.08956)
*Mobina Noori,Mahasweta Chakraborti,Amy X Zhang,Seth Frey*

Main category: cs.CL

TL;DR: 本研究分析了开源社区如何通过版本控制的治理文件描述参与和控制机制。研究对象为710个项目的治理文件快照，自动解析为“行为者、规则、行动、对象”，以信息熵等定量方法测量结构与变化。


<details>
  <summary>Details</summary>
Motivation: 当前对开源社区治理机制的动态描述和量化研究不足，尤其是参与和控制机制随时间推移的演化规律尚不清晰。希望基于结构化的文本和量化指标，为未来AI介导的工作流对权力分配的影响提供基线。

Method: 收集710个开源项目的治理文件快照，使用文本解析提取四大元素（行为者、规则、行动、对象），用熵、丰富度、Jensen-Shannon散度等指标定量分析其变化。

Result: 治理文件中，角色和行动类别随时间增多且分布更均衡，但规则本身并未发生剧烈变化。整体上，治理的发展表现为参与类别的扩展与平衡，但指导性规则未见显著转变。

Conclusion: 开源项目随着时间推移，会定义更多的角色和行动，这些分布更加均衡，但规则的整体构成保持稳定。这表明治理是通过扩展和平衡参与类别来发展，而不是通过强制性规则的变化。

Abstract: We study how open source communities describe participation and control
through version controlled governance documents. Using a corpus of 710 projects
with paired snapshots, we parse text into actors, rules, actions, and objects,
then group them and measure change with entropy for evenness, richness for
diversity, and Jensen Shannon divergence for drift. Projects define more roles
and more actions over time, and these are distributed more evenly, while the
composition of rules remains stable. These findings indicate that governance
grows by expanding and balancing categories of participation without major
shifts in prescriptive force. The analysis provides a reproducible baseline for
evaluating whether future AI mediated workflows concentrate or redistribute
authority.

</details>


### [83] [Creation of the Chinese Adaptive Policy Communication Corpus](https://arxiv.org/abs/2510.08986)
*Bolun Sun,Charles Chang,Yuen Yuen Ang,Pingxu Hao,Ruotong Mu,Yuchen Xu,Zhengxin Zhang*

Main category: cs.CL

TL;DR: 本文发布了涵盖1949-2023年中国中央政府政策文本的开放语料库，细致标注语言清晰度，数据及标注质量可靠，并为政策沟通与多语种NLP研究提供重要基础。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏中文政策指令公开数据集，尤其缺少对政策语言清晰度和模糊性的细粒度标注，限制了NLP在政策沟通领域的研究与应用。

Method: 构建CAPC-CG语料库，收集1949-2023年中国中央政府发布的法律、法规、行政规章。采用五色分类体系对文本进行清晰和模糊语言标注，分段处理3.3百万条数据。通过两轮标注流程，由专家和训练有素的标注者建立高质量金标准标注集。应用多种大语言模型进行基线分类试验。

Result: 建立了首个中国政策沟通语料库，含系统元数据和高一致性标注集（Fleiss's kappa=0.86）。公开了标注手册、基线LLM分类结果与数据分析。

Conclusion: CAPC-CG语料库为政策沟通及多语言NLP研究提供了基础资源，助力相关下游任务与模型开发。

Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central
Government) Corpus, the first open dataset of Chinese policy directives
annotated with a five-color taxonomy of clear and ambiguous language
categories, building on Ang's theory of adaptive policy communication. Spanning
1949-2023, this corpus includes national laws, administrative regulations, and
ministerial rules issued by China's top authorities. Each document is segmented
into paragraphs, producing a total of 3.3 million units. Alongside the corpus,
we release comprehensive metadata, a two-round labeling framework, and a
gold-standard annotation set developed by expert and trained coders.
Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive
labels, indicating high reliability for supervised modeling. We provide
baseline classification results with several large language models (LLMs),
together with our annotation codebook, and describe patterns from the dataset.
This release aims to support downstream tasks and multilingual NLP research in
policy communication.

</details>


### [84] [DARO: Difficulty-Aware Reweighting Policy Optimization](https://arxiv.org/abs/2510.09001)
*Jingyu Zhou,Lu Ma,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文分析了现有RLVR方法的缺陷并提出DARO算法，通过动态分配损失权重提升了数学推理任务的大模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型通过可验证奖励的强化学习（RLVR）增强了推理能力，其中GRPO算法被广泛采用。但作者发现这类方法在权重分配上过于静态且简单，无法适应模型能力的动态变化，影响整体性能。

Method: 提出了难度感知的重加权策略优化（DARO）方法，根据模型学习状态动态调整不同难度组别的损失权重。

Result: 在Qwen2.5-Math-1.5B、Qwen2.5-Math-7B和Llama3.1-8B等模型上，DARO在六个数学基准测试上优于四个主流方法，实现了更快收敛和更高最终性能。

Conclusion: 现有做法在样本难度分组和损失权重上存在固有弱点，DARO通过动态调整损失权重，有效提升了模型训练效果和性能表现。

Abstract: Recent advances in large language models (LLMs) have shown that reasoning
ability can be significantly enhanced through Reinforcement Learning with
Verifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has
emerged as the de facto approach for RLVR, inspiring numerous variants.
However, our mathematical analysis reveals that these methods are fundamentally
weighted variations of GRPO. We provide a unified view, demonstrating that
their reliance on static or overly simplistic weighting schemes tied to sample
difficulty prevents adaptation to a model's evolving capabilities. This creates
a significant loss scale issue, where training disproportionately focuses on
certain difficulty levels at the expense of others, hindering overall
performance. To address these limitations, we introduce
\textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that
dynamically adjusts the loss contribution of each difficulty group based on the
model's learning state. Extensive experiments on Qwen2.5-Math-1.5B,
Qwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading
baselines across six math benchmarks, achieving significantly faster
convergence and superior final performance.

</details>


### [85] [Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models](https://arxiv.org/abs/2510.09004)
*Yutao Mou,Xiaoling Zhou,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 本文证明了LoRA拒绝训练只需安全性数据即可实现高效、性能无损的安全对齐，是一种即插即用、成本低的AI安全补丁解决方案。


<details>
  <summary>Details</summary>
Motivation: 在构建值得信赖的人工智能时，安全性对齐非常关键，但提升模型安全性的同时，如何避免影响通用性能仍然具有挑战性。现有方法需要大量计算资源来寻找安全关键数据和通用数据的最佳配比，成本高且收益有限。

Method: 本文提出利用基于LoRA（Low-Rank Adaptation）的拒绝训练方法，通过仅在安全性数据上训练，作为模型的安全性补丁。作者还从理论和实验角度分析LoRA将安全性在低秩子空间内与模型本体能力解耦。

Result: 实验和理论结果均表明，LoRA能够高效以补丁的方式提升模型安全性，同时保持模型原有通用能力，几乎不会产生性能损失。

Conclusion: LoRA拒绝训练可作为低成本、无性能损失且即插即用的AI安全性增强措施，在安全对齐中表现出显著优势。

Abstract: Safety alignment is essential for building trustworthy artificial
intelligence, yet it remains challenging to enhance model safety without
degrading general performance. Current approaches require computationally
expensive searches for the optimal proportion of safety-critical and
general-purpose data to balance safety and general performance, incurring high
costs with limited gains. In this work, we show that LoRA-based
Refusal-training enables performance-preserving safety alignment even when
trained solely on safety data, demonstrating that LoRA serves as
cost-efficient, performance-preserving, and plug-and-play safety patches.
Beyond empirical findings, we provide both theoretical and experimental
evidence that LoRA effectively decouples safety into a low-rank subspace
largely orthogonal to the model's intrinsic transformation space, ensuring that
safety enhancements do not interfere with inherent capabilities.

</details>


### [86] [LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction](https://arxiv.org/abs/2510.09014)
*Shengmin Piao,Jieun Lee,Sanghyun Park*

Main category: cs.CL

TL;DR: 本文提出了一种轻量高效的Text-to-SQL方法LitE-SQL，通过优化schema检索和SQL生成流程，在保证性能的同时极大降低模型规模，，非常适合隐私敏感和资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL方法依赖大型语言模型（LLM），虽然效果很好，但由于模型是专有的，存在部署难和数据隐私隐忧。因此，作者希望提出一种轻量、高效又实用的方法来解决这些问题。

Method: 提出LitE-SQL框架，包含两个主要组件：（1）Schema Retriever：利用预计算的模式嵌入向量数据库，进行高效的schema linking；（2）SQL Generator：两阶段微调（有监督微调+执行引导强化学习），实现不需多候选生成的自我纠错。

Result: 在BIRD数据集上实现了72.10%的执行准确率；在Spider 1.0数据集上达到88.45%。参数量比LLM方法少2到30倍，但性能相当或更好。

Conclusion: 轻量级模型（LitE-SQL）可以实现高质量的Text-to-SQL生成，非常适合注重隐私和资源有限的实际应用场景。

Abstract: The Text-to-SQL task translates natural language questions into SQL queries,
enabling intuitive database interaction for non-experts. While recent methods
leveraging Large Language Models (LLMs) achieve strong performance, their
reliance on proprietary models raise concerns about deployment feasibility and
data privacy. In this work, we introduce LitE-SQL, a Lightweight and Efficient
framework with two components: (i) a Schema Retriever that performs efficient
schema linking using a vector database of pre-computed schema embeddings, and
(ii) a SQL Generator fine-tuned in two stages-supervised fine-tuning followed
by execution-guided reinforcement-enabling self-correction without costly
multi-candidate generation. On BIRD, LitE-SQL achieves 72.10% execution
accuracy, and on Spider 1.0 it reaches 88.45%, demonstrating comparable or
superior performance to LLM-based methods despite using 2x to 30x fewer
parameters. Our findings demonstrate that high-quality Text-to-SQL generation
is feasible with lightweight models, offering a practical solution for
privacy-sensitive and resource-constrained settings.

</details>


### [87] [Automated Refinement of Essay Scoring Rubrics for Language Models via Reflect-and-Revise](https://arxiv.org/abs/2510.09030)
*Keno Harada,Lui Yoshida,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 本文提出了让大语言模型（LLM）自动反思并迭代优化评分提示的方法，在主流自动作文评分数据集上显著提升了评分一致性，部分情况下优于人工详细评分规则。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在自动作文评分系统中的表现受到评分提示（rubric）质量的强烈影响。现有的人工评分规则往往难以与LLM的推理方式紧密结合，可能导致模型评分与人工评分不一致，因此亟需探索更有效的提示优化方法，以提升LLM自动评分的准确性和与人为评分的一致性。

Method: 本文提出了一种基于模型自我反思和评分结果反馈的评分提示迭代优化方法。具体做法是结合模型对其评分理由的反思与与人工评分分歧的样本反馈，引导模型自动迭代和优化评分提示。该方法在TOEFL11和ASAP两个公开作文数据集上，分别利用GPT-4.1、Gemini-2.5-Pro和Qwen-3-Next-80B-A3B-Instruct三种主流LLM进行实验。

Result: 在TOEFL11和ASAP数据集上，上述方法分别取得了最高0.19和0.47的Quadratic Weighted Kappa（QWK）提升。即使初始评分提示较为简单，经历模型迭代优化后，自动生成的评分提示能够达到甚至优于人工编写详尽评分规则的效果。

Conclusion: 通过迭代优化评分提示（rubric），能够显著提升LLM在自动作文评分任务中的表现，使其与人工评分结果更为接近，同时系统具有快速自适应和提示自我优化的能力。强调了提示迭代在LLM自动评分领域的重要意义。

Abstract: The performance of Large Language Models (LLMs) is highly sensitive to the
prompts they are given. Drawing inspiration from the field of prompt
optimization, this study investigates the potential for enhancing Automated
Essay Scoring (AES) by refining the scoring rubrics used by LLMs. Specifically,
our approach prompts models to iteratively refine rubrics by reflecting on
models' own scoring rationales and observed discrepancies with human scores on
sample essays. Experiments on the TOEFL11 and ASAP datasets using GPT-4.1,
Gemini-2.5-Pro, and Qwen-3-Next-80B-A3B-Instruct show Quadratic Weighted Kappa
(QWK) improvements of up to 0.19 and 0.47, respectively. Notably, even with a
simple initial rubric, our approach achieves comparable or better QWK than
using detailed human-authored rubrics. Our findings highlight the importance of
iterative rubric refinement in LLM-based AES to enhance alignment with human
evaluations.

</details>


### [88] [Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language](https://arxiv.org/abs/2510.09032)
*Adity Khisa,Nusrat Jahan Lia,Tasnim Mahfuz Nafis,Zarif Masud,Tanzir Pial,Shebuti Rayana,Ahmedul Kabir*

Main category: cs.CL

TL;DR: 本文提出了首个经过母语者验证的孟加拉文转写Chakma语料库，并对主流多语言模型进行微调，在低资源环境下显著提升了建模效果，推动了低资源语言的语言模型研究。


<details>
  <summary>Details</summary>
Motivation: Chakma语是一种数据稀缺的印欧语系语言，在现有语言模型中被严重忽视。为提升该语言的建模效果，需要提供高质量的语料，并探索主流预训练模型在低资源语言上的适应性。

Method: 创建了一个经过母语者验证的、具语境关联性的孟加拉文转写Chakma文本语料，并在此基础上对六种主流编码器型多语言及地区性Transformer模型进行微调，具体任务为掩码语言建模（MLM）。

Result: 微调后的多语言模型，相较于原始预训练模型，在孟加拉文转写Chakma上的表现更佳，token准确率最高达73.54%，困惑度最低为2.90。此外，分析发现数据质量对模型表现影响显著，同时OCR流程在具形态丰富的印地语系文本处理中仍存在局限。

Conclusion: 孟加拉文转写Chakma是Chakma语言迁移学习的有效途径，手动校验的单语数据集为低资源语言的多语言建模研究提供了新资源。研究促进了低资源语言的自动化处理，并鼓励相关领域深入研究。

Abstract: As an Indo-Aryan language with limited available data, Chakma remains largely
underrepresented in language models. In this work, we introduce a novel corpus
of contextually coherent Bangla-transliterated Chakma, curated from Chakma
literature, and validated by native speakers. Using this dataset, we fine-tune
six encoder-based multilingual and regional transformer models (mBERT,
XLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on masked
language modeling (MLM) tasks. Our experiments show that fine-tuned
multilingual models outperform their pre-trained counterparts when adapted to
Bangla-transliterated Chakma, achieving up to 73.54% token accuracy and a
perplexity as low as 2.90. Our analysis further highlights the impact of data
quality on model performance and shows the limitations of OCR pipelines for
morphologically rich Indic scripts. Our research demonstrates that
Bangla-transliterated Chakma can be very effective for transfer learning for
Chakma language, and we release our manually validated monolingual dataset to
encourage further research on multilingual language modeling for low-resource
languages.

</details>


### [89] [Large Language Models Do NOT Really Know What They Don't Know](https://arxiv.org/abs/2510.09033)
*Chi Seng Cheang,Hou Pong Chan,Wenxuan Zhang,Yang Deng*

Main category: cs.CL

TL;DR: LLMs无法可靠地区分事实与幻觉输出，尤其是在幻觉与知识相关时，其内部表征与正确响应极为相似，导致难以检测。这揭示了LLMs在事实性判断上的根本局限。


<details>
  <summary>Details</summary>
Motivation: 此前研究发现大型语言模型（LLMs）内部表征（如隐藏状态、注意力权重或token概率）似乎包含事实性信号，暗示它们可能“知道自己不知道的东西”。但LLMs在预测时也会产生事实错误，引发了内部计算能否可靠地区分真实与幻觉输出的疑问。

Method: 作者通过对LLMs处理事实查询时的机制分析，对比了两类基于主体信息的幻觉，监测其内部表征（隐藏状态的几何表现）。

Result: 如果幻觉与主体知识相关，LLMs会使用与正确回答相同的内部回忆机制，导致隐藏状态表征重叠、难以区分真假；而与主体知识无关的幻觉则产生可分辨的聚类表征。

Conclusion: LLMs的内部状态并不编码“真实度”，而仅反映知识回忆的模式，因此它们实际并不“知道自己不知道的东西”。

Abstract: Recent work suggests that large language models (LLMs) encode factuality
signals in their internal representations, such as hidden states, attention
weights, or token probabilities, implying that LLMs may "know what they don't
know". However, LLMs can also produce factual errors by relying on shortcuts or
spurious associations. These error are driven by the same training objective
that encourage correct predictions, raising the question of whether internal
computations can reliably distinguish between factual and hallucinated outputs.
In this work, we conduct a mechanistic analysis of how LLMs internally process
factual queries by comparing two types of hallucinations based on their
reliance on subject information. We find that when hallucinations are
associated with subject knowledge, LLMs employ the same internal recall process
as for correct responses, leading to overlapping and indistinguishable
hidden-state geometries. In contrast, hallucinations detached from subject
knowledge produce distinct, clustered representations that make them
detectable. These findings reveal a fundamental limitation: LLMs do not encode
truthfulness in their internal states but only patterns of knowledge recall,
demonstrating that "LLMs don't really know what they don't know".

</details>


### [90] [Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation](https://arxiv.org/abs/2510.09051)
*Muhammad Ali Shafique,Kanwal Mehreen,Muhammad Arham,Maaz Amjad,Sabur Butt,Hamza Farooq*

Main category: cs.CL

TL;DR: 通过改进的self-instruct方法构建和训练Alif-1.0-8B-Instruct，该模型在乌尔都语任务上表现超越主流多语言LLM，实现低成本、高性能和文化对齐创新。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言乌尔都语开发高性能大语言模型（LLM）存在数据稀缺、多语言不一致和安全性问题。现有多语言LLM通过翻译大量数据来解决这些问题，但翻译质量和文化细节往往不足，且成本高昂。

Method: 提出了Alif-1.0-8B-Instruct，这是一个乌尔都语和英语的多语言模型。其训练使用了经过改进的self-instruct方法生成的高质量多语言合成数据集（Urdu-Instruct）。该方法结合针对每个任务的独特提示与种子、全球任务池，实现乌尔都语原生推理、双语翻译、文化相关性和伦理安全对齐。

Result: Alif-1.0-8B-Instruct在乌尔都语相关任务上性能优于Llama-3.1-8B-Instruct，并超过Mistral-7B-Instruct-v0.3、Qwen-2.5-7B-Instruct和Cohere-Aya-Expanse-8B等多语言模型，且训练预算低于100美元。

Conclusion: 可以高效且文化对齐地开发低资源语言的高性能LLM，所用数据集、模型和代码均已公开。

Abstract: Developing a high-performing large language models (LLMs) for low-resource
languages such as Urdu, present several challenges. These challenges include
the scarcity of high-quality datasets, multilingual inconsistencies, and safety
concerns. Existing multilingual LLMs often address these issues by translating
large volumes of available data. However, such translations often lack quality
and cultural nuance while also incurring significant costs for data curation
and training. To address these issues, we propose Alif-1.0-8B-Instruct, a
multilingual Urdu-English model, that tackles these challenges with a unique
approach. We train the model on a high-quality, multilingual synthetic dataset
(Urdu-Instruct), developed using a modified self-instruct technique. By using
unique prompts and seed values for each task along with a global task pool,
this dataset incorporates Urdu-native chain-of-thought based reasoning,
bilingual translation, cultural relevance, and ethical safety alignments. This
technique significantly enhances the comprehension of Alif-1.0-8B-Instruct
model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon
the pretrained Llama-3.1-8B, demonstrates superior performance compared to
Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading
multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct,
and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our
results demonstrate that high-performance and low-resource language LLMs can be
developed efficiently and culturally aligned using our modified self-instruct
approach. All datasets, models, and code are publicly available at:
https://github.com/traversaal-ai/alif-urdu-llm.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [91] [A CSP approach to Graph Sandwich Problems](https://arxiv.org/abs/2510.09128)
*Manuel Bodirsky,Santiago Guzmán-Pro*

Main category: cs.DM

TL;DR: 该论文探讨了Sandwich Problem在图论中的复杂性归类，提出将其与无限域CSP理论关联，并利用此关系解决了数个开放问题。作者还发现了一个特殊的图三明治问题属于coNP但似乎不属于P或coNP完全，拓展了相关理论的应用和理解。


<details>
  <summary>Details</summary>
Motivation: Sandwich Problem (SP) 是一种关于图类的计算问题。针对已知子图和超图，判定是否存在该类的中间图。已有研究对部分SP的复杂性进行了探讨，但未系统地归纳其与限制满足问题（CSP）理论的关系，也仍有开放问题待解决。作者希望利用CSP理论来推进对SP复杂性质的归类和理解。

Method: 作者通过将多种SP转化为无限域的CSP，并应用CSP理论中的基本工具，继而推导出SP在不同图类别中的复杂性归类（如多重图的线图、双部多重图线图、Kk-自由完美图等），并解决特定子图类的归类问题。

Result: 作者证明了许多SP可以归约为无限二边着色图的CSP；利用CSP复杂性归类，给出了SP在若干新的图类别中的复杂性结果，并解决了由Alvarado等人提出的开放问题。同时，作者还构造了一个属于coNP但不是P或coNP完全的图三明治问题（除非P=coNP）。

Conclusion: 通过将SP纳入CSP框架，作者深化了对Sandwich Problem复杂性归类的理解，对若干图类别SP复杂性作出新判定，并解决了相关开放问题，显示了CSP理论在SP研究中的广泛应用前景。

Abstract: The \emph{Sandwich Problem} (SP) for a graph class $\calC$ is the following
computational problem. The input is a pair of graphs $(V,E_1)$ and $(V,E_2)$
where $E_1\subseteq E_2$, and the task is to decide whether there is an edge
set $E$ where $E_1\subseteq E \subseteq E_2$ such that the graph $(V,E)$
belongs to $\calC$. In this paper we show that many SPs correspond to the
constraint satisfaction problem (CSP) of an infinite $2$-edge-coloured graph
$H$. We then notice that several known complexity results for SPs also follow
from general complexity classifications of infinite-domain CSPs, suggesting a
fruitful application of the theory of CSPs to complexity classifications of
SPs. We strengthen this evidence by using basic tools from constraint
satisfaction theory to propose new complexity results of the SP for several
graph classes including line graphs of multigraphs, line graphs of bipartite
multigraphs, $K_k$-free perfect graphs, and classes described by forbidding
finitely many induced subgraphs, such as $\{I_4,P_4\}$-free graphs, settling an
open problem of Alvarado, Dantas, and Rautenbach (2019). We also construct a
graph sandwich problem which is in coNP, but neither in P nor coNP-complete
(unless P = coNP).

</details>
