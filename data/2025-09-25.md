<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 82]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Macro-embedding Compiler Intermediate Languages in Racket](https://arxiv.org/abs/2509.19607)
*William J. Bowman*

Main category: cs.PL

TL;DR: 本文在Racket中通过宏嵌入，实现了多个编译器中间语言的统一解释器，有效提升了语言特性复用、互操作性和可扩展性，优化了编译器课程的中间语言设计与测试流程。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是为编译器课程设计一种方法，能够高效地实现和测试一系列从 Scheme-like 语言到 x86-64 的中间语言，并且提升这些语言之间的互操作性和代码复用性。

Method: 该方法通过在 Racket 中进行宏嵌入，将多个编译器中间语言嵌入到单一主机语言中，实现安全的函数抽象、非安全的汇编特性及其交互。强调通过本地宏展开提升模块性和组合性，无需每种语言各自独立实现解释器。

Result: 所提出的宏嵌入方法实现了高代码复用率和语言间强互操作性，简化了中间语言语义的开发，同时也便于扩展和重定义中间语言特性，并可为这些语言暴露多样化的接口。

Conclusion: 宏嵌入技术在编译器课程和多语言设计中展现了强大的模块化、可复用性和高低层语言互操作能力，优于独立实现各解释器的方法，并为语言特性扩展和接口多样性提供了便利。

Abstract: We present the design and implementation of a macro-embedding of a family of
compiler intermediate languages, from a Scheme-like language to x86-64, into
Racket. This embedding is used as part of a testing framework for a compilers
course to derive interpreters for all the intermediate languages. The embedding
implements features including safe, functional abstractions as well as unsafe
assembly features, and the interactions between the two at various intermediate
stages.
  This paper aims to demonstrate language-oriented techniques and abstractions
for implementing (1) a large family of languages and (2) interoperability
between low- and high-level languages. The primary strength of this approach is
the high degree of code reuse and interoperability compared to implementing
each interpreter separately. The design emphasizes modularity and
compositionality of an open set of language features by local macro expansion
into a single host language, rather than implementing a language pre-defined by
a closed set of features. This enables reuse from both the host language
(Racket) and between intermediate languages, and enables interoperability
between high- and low-level features, simplifying development of the
intermediate language semantics. It also facilitates extending or redefining
individual language features in intermediate languages, and exposing multiple
interfaces to the embedded languages.

</details>


### [2] [Compilation as Multi-Language Semantics](https://arxiv.org/abs/2509.19613)
*William J. Bowman*

Main category: cs.PL

TL;DR: 该论文提出用多语言归约统一定义编译与互操作，简化模型和证明，提升编译器安全性和正确性分析。


<details>
  <summary>Details</summary>
Motivation: 现有多语言语义模型需要分别定义编译与互操作两个过程，造成重复与复杂性。作者试图用统一的模型简化编译流程与互操作性分析。

Method: 将编译器模型化为在多语言语义下开放项上的归约系统，而非传统的语法翻译，并分析多语言归约的性质。

Result: 这种方法减少了模型的重复性，带来语义新见解，如交叉语言归约对应AOT编译，多语言模型中的求值对应JIT编译。归约的合流性与类型保持性直接支持编译器正确性和安全性证明。

Conclusion: 采用多语言语义的统一归约系统模型，可以同时定义编译器和互操作语义，简化和提升编译正确性和类型保持性的证明。

Abstract: Modeling interoperability between programs in different languages is a key
problem when modeling verified and secure compilation, which has been
successfully addressed using multi-language semantics. Unfortunately, existing
models of compilation using multi-language semantics define two variants of
each compiler pass: a syntactic translation on open terms to model compilation,
and a run-time translation of closed terms at multi-language boundaries to
model interoperability.
  In this talk, I discuss work-in-progress approach to uniformly model a
compiler entirely as a reduction system on open term in a multi-language
semantics, rather than as a syntactic translation. This simultaneously defines
the compiler and the interoperability semantics, reducing duplication. It also
provides interesting semantic insights. Normalization of the cross-language
redexes performs ahead-of-time (AOT) compilation. Evaluation in the
multi-language models just-in-time (JIT) compilation. Confluence of
multi-language reduction implies compiler correctness, and part of the secure
compilation proof (full abstraction), enabling focus on the difficult part of
the proof. Subject reduction of the multi-language reduction implies
type-preservation of the compiler.

</details>


### [3] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: 本文首次对einsum进行了理论化和统一语言定义，并证明了关键等价规则，为其在各种应用中优化与形式化推理奠定了基础。


<details>
  <summary>Details</summary>
Motivation: einsum虽然广泛应用于机器学习和其他领域，但缺乏统一理论基础，并且不同框架间的实现不统一，影响了形式化推理和优化。

Method: 对张量表达式的术语进行探讨，并为einsum语言提供了形式化定义；在此基础上，形式化并证明了重要的等价规则。

Result: 建立了einsum的统一理论基础、定义以及等价规则，提升了其在各类实际应用中的优化和推理能力。

Conclusion: 论文为einsum提供了首次严谨的理论基础和统一语言定义，有助于形式化推理与系统优化，对相关领域具有重要价值。

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: 该论文提出了一种新型编译器PMRobust，用于自动处理持久化内存flush/fence指令，基本无性能损失，解决了开发者易遗漏相关操作的难题，大幅提升持久化内存编程的正确性和便利性。


<details>
  <summary>Details</summary>
Motivation: CXL共享内存和持久化内存能够使数据在系统崩溃后仍然保留，但正确使用flush和fence操作确保数据持久化很困难。现有工具虽能发现遗漏的flush指令，但依赖发现bug的测试用例，且无法确保完全没有遗漏。

Method: 提出了PMRobust编译器，通过自动插入flush和fence操作，利用新颖的静态分析及针对新分配对象的优化，保证代码不会遗漏flush或fence。

Result: 在持久化内存库和多种持久化数据结构上的评测显示，自动插入flush和fence操作的几何平均性能损耗仅为0.26%，几乎不影响原有性能。

Conclusion: PMRobust能自动保障持久化内存代码的安全性，防止遗漏flush/fence造成的数据一致性错误，以极低性能开销提升开发效率与可靠性。

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [5] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: 结合推理型LLM与传统模糊测试流程，作者展示了LLM在提升输入变异质量方面的优势，尤其是在复杂安全场景下，且发现模型选型和提示精度比单纯增加提示数量更重要，Deepseek表现最佳，但系统性能瓶颈仍需后续优化。


<details>
  <summary>Details</summary>
Motivation: 现有的物联网设备、移动平台和自动化系统存在严重安全漏洞，传统的模糊测试工具多在字节或位层面进行变异，缺乏对输入语义和协议逻辑的深入理解，难以发现更深层次的安全问题。利用能够进行语义推理的大型语言模型（LLM）可以更智能地进行定向变异，但缺乏“正确”变异路径的标注数据，无法采用有监督微调，因此探索可直接利用的预训练模型的效果。

Method: 作者提出了一个开源微服务框架，将具有推理能力的LLM集成至AFL++模糊测试工具，并运行在Google FuzzBench平台，处理LLM与模糊器的异步执行和不同硬件需求（如GPU和CPU）。通过few-shot提示工程，评估不同模型和提示设计的变异效果，关注模型回应延迟和吞吐瓶颈，并比较多种开源LLM模型。

Result: 实验对Llama3.3、Deepseek-r1-Distill-Llama-70B、QwQ-32B和Gemma3等模型进行了评估。结果显示Deepseek模型在变异有效性方面最具潜力。变异效果更多依赖于提示复杂度和模型选择，而非提示次数。此外，模型响应的延迟和吞吐率仍然是主要瓶颈。

Conclusion: 推理型LLM能够提升模糊测试的输入变异质量，但模型本身的选择与提示设计比简单增加样例数量更为关键。未来主要挑战是提升响应速度和系统吞吐率。

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [6] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: LLM能自动高质量生成用户故事，简洁有效的提示尤适合小模型，结构化推理提升有限。


<details>
  <summary>Details</summary>
Motivation: 在敏捷开发中，用户故事至关重要，但在遗留和文档不充分的系统中常常缺失或过时。自动从源代码恢复用户故事具有实际价值。

Method: 作者使用了1,750段不同复杂度的C++代码片段，评估了5种主流大语言模型（LLMs）在6种不同提示策略下，自动生成用户故事的效果。

Result: 所有模型在处理长度为200 NLOC以内的代码时，平均F1得分达到0.8。最小的8B模型通过单个示例提示即可达到与70B大模型相匹配的效果。链式思维（CoT）等结构化推理方法对大模型提升有限。

Conclusion: 大语言模型可高效自动从源码恢复用户故事，合适的提示设计对小模型尤为有效。过于复杂的推理策略提升有限，简单示例足以达到较好效果。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [7] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文系统评估了四款大语言模型在生成单元测试断言消息上的能力，发现上下文信息对生成质量提升明显，现有模型距离人工级别尚有差距，但研究结果为未来自动生成更清晰断言消息提供了理论和方法基础。


<details>
  <summary>Details</summary>
Motivation: 断言消息有助于理解单元测试失败原因，但开发者和自动化测试工具常常忽视其编写。随着大语言模型（LLMs）能力的提升，人们希望它们能弥补这一不足，目前尚缺乏系统性评估。

Method: 评估了四个主流的Fill-in-the-Middle（FIM）大语言模型（Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B、StarCoder）在生成Java单元测试断言消息上的表现。通过包含216个带有开发者手写断言信息的测试方法，结合人工主观评价，对生成质量进行打分，并通过消融实验和结构分析探讨上下文信息的重要性及模型的表达模式。

Result: Codestral-22B表现最好，平均得分为2.76/5（人工消息为3.24）。加入详细测试注释后，Codestral的得分提升至2.97，显示上下文对于生成高质量断言消息至关重要。所有模型在表达上常模仿开发者语言习惯。

Conclusion: 当前FIM类大语言模型在自动生成断言消息时，虽然尚未达到人工水平，但在有上下文补充时表现有明显提升。结构分析和评价结果为后续更智能、可解释的自动断言消息生成研究奠定了基础。

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [8] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 本研究通过为期一年的企业真实环境大规模部署AI开发平台，实证显示工具极大提升团队生产力和代码交付效率，开发者高度认可，动态分析展现AI工具实际应用优势和落地挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI开发工具多在受控环境下进行评测，缺乏企业真实生产环境中长期效用与部署难题的实证研究。该工作旨在填补这一研究空白，评估AI工具在大规模企业开发中的实际表现。

Method: 通过为期一年的长期跟踪观测，并采用严格的队列分析方法，对300名工程师在真实团队环境下使用内部AI平台DeputyDev的情况进行评估，分析了工具对生产力、代码审核周期及使用率等指标的影响。

Result: 引入DeputyDev平台使PR审核周期时间整体减少了31.8%；开发者对审查功能满意度达85%，有继续使用意愿达93%；使用率从初期4%迅速攀升至第6个月高峰83%，之后稳定在60%。高频用户推动生产代码量提升61%，该工具贡献了公司代码交付量的30~40%，整体代码交付量提高28%。

Conclusion: AI辅助软件开发工具在企业中真实部署，显著提升了生产力和代码交付量，并获得开发者高度认可，但在实际部署中也存在挑战。

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [9] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen提出了一种多智能体、数据驱动的跨语言代码生成系统，通过合理选择中介语言与知识迁移机制，显著提升了多种编程语言（特别是低资源语言）的代码生成能力，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在多编程语言自动编程中表现优异，但对低资源语言（如Rust、Perl、OCaml、Erlang）支持不足，现有方案多各自为阵，缺乏跨语言知识迁移，影响代码质量与效率。

Method: 提出了XL-CoGen系统，采用多智能体架构，集成中间表示、代码生成、翻译和自动修复。系统通过数据驱动机制选择桥接语言，基于经验转移矩阵判定最优中介语言，同时进行早期输出验证、迭代修正，并复用中间产物作为后续翻译的上下文。

Result: XL-CoGen在实验中相比最强的微调基线提升了13个百分点，对比单语言多智能体方法提升最高达30个百分点。兼容性引导的桥接显著优于基于LLM的启发式方法，验证了跨语言知识迁移的有效性。

Conclusion: XL-CoGen通过多智能体协同、数据驱动的桥接语言选择和中间产物复用，显著提升了多语言代码生成的质量和跨语言迁移能力，特别是对低资源语言。

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [10] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: 该论文提出并实现了专为神经网络软件设计的NNBOM物料清单，基于近6万个PyTorch仓库建立数据库，分析了神经网络软件的演变趋势，并开发了原型工具验证其实用性，为开发与维护NN软件提供了新的分析视角和实践方法。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络广泛应用以及开源社区神经网络代码库的快速发展，开发者和维护者亟需分析这些代码库的演变，以便指导开发和把握技术趋势。但传统的软件物料清单（SBOM）对神经网络软件不适用，现有的AI物料清单也缺乏大规模实用性分析工具。

Method: 提出了针对神经网络软件的物料清单构建方案——NNBOM，并基于55,997个PyTorch Github仓库，统计其第三方库（TPL）、预训练模型（PTM）和模块，建立了大型NNBOM数据库。利用此数据库，开展了神经网络软件规模、组件复用及跨领域依赖的实证分析。最后开发了两个原型工具，用于多仓库演化分析和单仓库组件评估与推荐。

Result: 成功构建了大规模NNBOM数据库，揭示了神经网络软件在规模、组件复用和跨领域依赖上的长期发展趋势，并通过原型应用展示了数据和分析成果的实际价值。

Conclusion: NNBOM有效弥补了传统物料清单在神经网络领域的不足，能为开发者和维护者提供演化分析及实践支持，对理解和引导神经网络软件发展具有重要作用。

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [11] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: 该论文提出了面向视觉游戏开发的新基准V-GameGym，弥补了当前仅评测语法和执行的不足，通过多模态自动化评测，更好地反映代码大模型在实际游戏开发中的能力和短板。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在编程任务上表现突出，但主流评测仅关注语法和单模态执行正确性，忽视了游戏开发中如可玩性、美学、用户体验等关键实际应用指标。现有基准未能覆盖实际游戏开发需求。

Method: 提出了V-GameGym基准，包含2219个高质量样本、100个主题集群，采用新的聚类式筛选方法确保多样性和结构完整性，并建立了多模态评测框架，利用UI沙盒实现自动化代码视觉合成、评价互动元素等。

Result: V-GameGym有效弥补了代码生成准确性与实际游戏开发流程需求之间的差距，能量化评估视觉编程与互动元素生成质量。

Conclusion: V-GameGym为视觉游戏开发中的代码大模型评测提供了覆盖广、具实际意义的基准和量化指标，推动了代码生成技术与实际部署的结合。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [12] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 本文针对软件工程中需求追踪的训练数据不足问题，提出结合大语言模型进行数据扩增，并设计多种提示模板进行对比，优化模型编码器。实验表明，该方法可大幅提升追踪模型性能，具备实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 目前软件工程中的需求到代码的自动化追踪方法受限于训练数据稀缺和语义鸿沟难以跨越。本研究的动机是解决数据稀缺问题，从而提升需求追踪的有效性。

Method: 提出基于大语言模型（LLMs）的数据扩增方法，采用提示（prompt）技术生成增强的需求-代码追踪链接。使用了Gemini 1.5 Pro、Claude 3、GPT-3.5和GPT-4四种LLM，分别应用零样本和少样本模板。同时优化追踪模型中的编码器，以适配扩增后的数据。

Result: 实验结果表明，该方法显著提升了模型表现，F1分数最高提升至28.59%，验证了其有效性与应用潜力。

Conclusion: 基于LLM的数据扩增方法能有效缓解需求追踪领域的训练数据稀缺，通过提示模板和编码器优化，显著提高了自动化追踪模型的性能。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [13] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: 本文系统评测了开源LLM自动生成web API调用代码的能力，发现其正确率不足40%，存在常见错误，显示该方向仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: API集成是软件系统交互的核心，但生成正确的API调用代码，特别是web API，一直具有挑战性。尽管大型语言模型（LLMs）已成为开发工具，其在自动生成web API集成代码方面的有效性尚未深入探索。

Method: 作者提出了一个数据集和评测流程，用于系统性评估LLMs自动生成web API调用代码的能力，并对多个开源LLM进行了实验测试。

Result: 实验结果表明，API调用自动生成极具挑战性，出现了虚构的端点、错误参数使用等问题。所有评测的开源模型解决的任务均未超过40%。

Conclusion: 当前开源LLMs在web API集成自动代码生成方面效果有限，存在大量错误，远未达到令人满意的自动编程水平。

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [14] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: 本文提出了结合专家推理的Verilog判别模型VCD-RNK，可高效筛选并提升LLM生成代码的可靠性，满足工程师实际需求。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成Verilog代码时因缺乏领域知识，常无法提供可靠方案，现有采样提升通过率但工程师更需单一可信解。为此，作者将此问题转化为需求与实现间的语义对齐问题。

Method: 提出了VCD-RNK判别模型，用于对多候选Verilog代码进行高效重排序。该模型融合Verilog领域专家知识，覆盖代码语义分析、测试用例生成与功能正确性评估三方面，推理时显式模拟上述过程，无需昂贵的测试运行。

Result: VCD-RNK能显著提升Verilog代码生成的可靠性，有效取代计算密集型验证过程，提供更可信的自动化实现方式。

Conclusion: 通过引入领域专家推理并优化判别模型，VCD-RNK能为硬件工程师稳定输出高质量Verilog代码，提高生成方案可信度。

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [15] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: 本文提出一种基于零知识证明的业务流程完整性验证方法，能够在不泄露敏感信息的情况下，实现自动化和安全的流程验证。


<details>
  <summary>Details</summary>
Motivation: 在跨组织的业务流程中，确保流程完整性而不泄露机密信息是一项主要挑战。当前的方法通常难以在流程可验证性和信息保密性之间取得平衡，因此需要更好的技术解决方案。

Method: 本文提出了基于零知识证明（ZKP）的方案，通过将ZK虚拟机（zkVMs）集成到业务流程管理引擎中，并设计了完整的系统架构及原型实现。支持通过证明组合实现链式可验证计算，并在过程模型中评估不同ZKP证明变体的效率。

Result: 在产品碳足迹跟踪等案例中，展示了组织可在无需泄露敏感信息的前提下，证明和验证业务流程的完整性。通过实验驱动的评估，验证了该方法可在保密约束下实现流程自动化验证。

Conclusion: 基于ZKP的方法可有效实现业务流程的可验证性和信息保密性，有利于跨组织合作，同时提高自动化和安全性。本文方法具有良好的实用性和推广前景。

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [16] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: 本文提出基于I/O语法的新方法，将输入生成与输出校验统一，实现了协议测试的高效自动化和全面覆盖，实验证明优于随机方法，且具备灵活和通用特性。


<details>
  <summary>Details</summary>
Motivation: 在软件测试中，自动生成测试用例面临输入生成和输出校验两大挑战，尤其在协议测试中，需要生成多样且正确的消息输入，并准确判断输出响应的正确性。现有工具难以同时兼顾输入生成和输出校验的全面性与灵活性。

Method: 该论文提出并实现了I/O语法（I/O grammars），能够完整描述协议的消息、状态及交互语法和语义。在FANDANGO框架下，单一I/O语法可用作测试用例生成、模拟对象或客户端/服务端/多方的校验器。用户可通过自定义约束和k-path引导，系统性覆盖协议的状态、消息、响应和参数取值。

Result: 作者在DNS、FTP、SMTP等协议上验证了方法的有效性。实验表明，采用系统化I/O语法覆盖比随机方法显著加快了输入与响应空间的覆盖速度，提升了测试效率与协议特性完整性描述能力。

Conclusion: I/O语法方法能够同时解决协议测试的输入生成和输出校验难题，全面覆盖协议行为，且工具具有高度通用性和灵活性，优于现有测试生成工具和形式化描述方法。

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [17] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: 实证数据显示，GitHub Copilot用户在活跃度上原本就高，但其引入并未带来显著的提交数提升；然而，用户自我感觉生产力提高，主客观评价存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具（如GitHub Copilot）在软件开发中的广泛应用，开发者生产力与工作方式可能发生变化，但对此的实际影响缺乏实证研究。本文旨在填补这一空白，探索Copilot对开发者活动和主观生产力的真实影响。

Method: 采取混合方法（mixed-methods）：对NAV IT组织内703个GitHub代码仓库两年内的26,317条唯一非合并提交进行数据分析，比较了25名Copilot用户和14名非用户的活动；同时通过问卷调查和13次访谈获取对生产力的主观感受和角色信息。

Result: Copilot用户无论在使用前还是使用后，相较于非用户在提交活动上都更为活跃；但Copilot引入后，用户的提交活动指标没有统计学上的显著变化，尽管有轻微提升。

Conclusion: Copilot工具的引入虽然未在客观提交数据上体现出明显提升，但用户主观感受到生产力有所增强，反映出主客观指标之间存在不一致现象。

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [L-Mosaics and Bounded Join-Semilattices in Isabelle/HOL](https://arxiv.org/abs/2509.19854)
*Alessandro Linzi*

Main category: cs.LO

TL;DR: 通过AI辅助的定理证明方法，作者在Isabelle/HOL中严格形式化了L-mosaics与有界并半格的等价，成功验证了理论结果并展现了人工智能在复杂证明任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 原始等价由Cangiotti等人在多值代数与正交模格、量子逻辑的研究中提出，但未有严格形式化；作者旨在通过严格机器证明验证理论结果，同时展示AI工具提升形式化效率的潜力。

Method: 采用AI辅助的方法，整合大型语言模型帮助推理与证明整个过程，利用交互式自动定理证明工具进行复杂结构的形式化。

Result: 成功实现主理论结果的严格验证，确认两种结构之间的互逆等价，且表明AI辅助能显著提升交互式定理证明的能力，尤其在处理多值代数操作等高复杂度项目时。

Conclusion: 本文通过Isabelle/HOL完全形式化了L-mosaics与有界并半格之间的对象部分等价性，并验证了转化的互逆性质。

Abstract: We present a complete formalization in Isabelle/HOL of the object part of an
equivalence between L-mosaics and bounded join-semilattices, employing an
AI-assisted methodology that integrates large language models as reasoning
assistants throughout the proof development process. The equivalence was
originally established by Cangiotti, Linzi, and Talotti in their study of
hypercompositional structures related to orthomodular lattices and quantum
logic. Our formalization rigorously verifies the main theoretical result and
demonstrates the mutual inverse property of the transformations establishing
this equivalence. The development showcases both the mathematical depth of
multivalued algebraic operations and the potential for AI-enhanced interactive
theorem proving in tackling complex formalization projects.

</details>


### [19] [From Zonotopes to Proof Certificates: A Formal Pipeline for Safe Control Envelopes](https://arxiv.org/abs/2509.20301)
*Jonathan Hellwig,Lukas Schäfer,Long Qian,André Platzer,Matthias Althoff*

Main category: cs.LO

TL;DR: 针对安全关键系统中的控制器合成，作者将高性能可达性分析与形式化逻辑认证结合，显著提升了安全证明的效率与可靠性，并通过典型案例展示了方法的应用价值。


<details>
  <summary>Details</summary>
Motivation: 在网络物理系统设计中，综合保证安全性与执行器约束的控制器是一项核心挑战。目前基于zonotope的可达性分析方法具有良好的可扩展性，但这些工具缺乏形式化验证，难以应用于安全关键系统。现有的证明工具KeYmaera X虽具有理论潜力，但面对复杂的高维集合表示时效率低下。

Method: 本文正式化了以控制不变集作为安全证明的原理，并提出了一个结合可扩展性与形式化严密性的控制包络验证流程。流程分为两步：一是使用高性能可达性算法计算控制包络；二是利用可证明正确的逻辑原则对每一步结果进行认证，其中高计算量的zonotope包含性任务被转交于高效的数值计算工具，并返回紧凑证据供KeYmaera X快速验证。

Result: 该方法不仅提升了控制包络验证的可扩展性和正确定性，同时在代表性案例研究中验证了其实用价值。

Conclusion: 文章提出了一种可扩展且严格的形式化验证流程，将现代高效可达性算法与逻辑证明工具有机结合，解决了控制器合成中的安全保证痛点。

Abstract: Synthesizing controllers that enforce both safety and actuator constraints is
a central challenge in the design of cyber-physical systems. State-of-the-art
reachability methods based on zonotopes deliver impressive scalability, yet no
zonotope reachability tool has been formally verified and the lack of
end-to-end correctness undermines the confidence in their use for
safety-critical systems. Although deductive verification with the hybrid system
prover KeYmaera X could, in principle, resolve this assurance gap, the
high-dimensional set representations required for realistic control envelopes
overwhelm its reasoning based on quantifier elimination. To address this gap,
we formalize how control-invariant sets serve as sound safety certificates.
Building on that foundation, we develop a verification pipeline for control
envelopes that unites scalability and formal rigor. First, we compute control
envelopes with high-performance reachability algorithms. Second, we certify
every intermediate result using provably correct logical principles. To
accelerate this certification, we offload computationally intensive zonotope
containment tasks to efficient numerical backends, which return compact
witnesses that KeYmaera X validates rapidly. We show the practical utility of
our approach through representative case studies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias](https://arxiv.org/abs/2509.19314)
*Sirui Wu,Daijin Yang*

Main category: cs.CL

TL;DR: 该论文分析了利用大型语言模型中和人格测验题项、以减少社会期望偏差的方法。结果显示，中和处理在部分维度和项目上有效降低了社会期望相关性，但整体偏差消除效果有限，信度和主结构能保留，部分测量等价性缺失，因此AI自动中和仍需进一步完善。


<details>
  <summary>Details</summary>
Motivation: 人格测评在实际应用中常因社会期望偏差影响结果，导致被试在答题时倾向于选择社会上更被认可或接受的答案，从而影响量表的信效度。因此，研究希望借助大型语言模型（LLM）来中和项目内容，减轻这种偏差。

Method: 研究使用GPT-03对国际人格项目池五大人格测验（IPIP-BFM-50）部分题项进行中和处理，然后招募203名参与者随机完成原始或中和后的量表，并同时填写马洛-克朗社会期望量表，比较两种量表在信度、结构和与社会期望相关性的差异。

Result: 中和后的量表在信度和五因素结构上表现良好。在‘尽责性’维度得分上升，而‘宜人性’和‘开放性’得分下降。部分题项与社会期望的相关性降低，但下降存在不一致性。在测量结构上，仅结构等价成立，测量等价和截距等价不成立。

Conclusion: AI中和作为减少社会期望偏差的潜在方法在实际测试中表现出一定效果，但效果并不完美，相关性降低的结果不稳定，量表某些结构特性未能在中和处理后保持。

Abstract: This study evaluates item neutralization assisted by the large language model
(LLM) to reduce social desirability bias in personality assessment. GPT-o3 was
used to rewrite the International Personality Item Pool Big Five Measure
(IPIP-BFM-50), and 203 participants completed either the original or
neutralized form along with the Marlowe-Crowne Social Desirability Scale. The
results showed preserved reliability and a five-factor structure, with gains in
Conscientiousness and declines in Agreeableness and Openness. The correlations
with social desirability decreased for several items, but inconsistently.
Configural invariance held, though metric and scalar invariance failed.
Findings support AI neutralization as a potential but imperfect bias-reduction
method.

</details>


### [21] [FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering](https://arxiv.org/abs/2509.19319)
*Gyubok Lee,Elea Bach,Eric Yang,Tom Pollard,Alistair Johnson,Edward Choi,Yugang jia,Jong Ha Lee*

Main category: cs.CL

TL;DR: 本文提出了专为HL7 FHIR医疗数据标准设计的FHIR-AgentBench基准，系统分析了当前LLM代理在复杂互操作医疗数据中面临的实际挑战，是评估和推动该领域技术进步的重要工具。


<details>
  <summary>Details</summary>
Motivation: 临床AI领域正在向HL7 FHIR这种复杂的医疗数据标准转变，但目前缺乏能真实反映这一领域实际需求的基准，无法充分评估最新LLMs在这种结构化互操作数据下的表现。

Method: 提出了FHIR-AgentBench基准，将2,931个真实临床问题与HL7 FHIR标准数据对接，通过该基准系统评估不同的agent框架，包括数据检索策略（直接FHIR API或专用工具）、交互模式（单轮、多轮）和推理方式（自然语言、代码生成）。

Result: 实验显示在复杂FHIR资源数据检索和推理方面存在显著实际挑战，这两点都会显著影响LLM在医疗场景下自动问答的性能。

Conclusion: FHIR-AgentBench可以作为评估和推动临床AI中LLM代理系统互操作、稳健性与可靠性研究的重要工具，填补了基准不足的空白，并推动相关领域的发展。

Abstract: The recent shift toward the Health Level Seven Fast Healthcare
Interoperability Resources (HL7 FHIR) standard opens a new frontier for
clinical AI, demanding LLM agents to navigate complex, resource-based data
models instead of conventional structured health data. However, existing
benchmarks have lagged behind this transition, lacking the realism needed to
evaluate recent LLMs on interoperable clinical data. To bridge this gap, we
introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical
questions in the HL7 FHIR standard. Using this benchmark, we systematically
evaluate agentic frameworks, comparing different data retrieval strategies
(direct FHIR API calls vs. specialized tools), interaction patterns
(single-turn vs. multi-turn), and reasoning strategies (natural language vs.
code generation). Our experiments highlight the practical challenges of
retrieving data from intricate FHIR resources and the difficulty of reasoning
over them, both of which critically affect question answering performance. We
publicly release the FHIR-AgentBench dataset and evaluation suite
(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research
and the development of robust, reliable LLM agents for clinical applications.

</details>


### [22] [Readme_AI: Dynamic Context Construction for Large Language Models](https://arxiv.org/abs/2509.19322)
*Millie Vyas,Timothy Blattner,Alden Dima*

Main category: cs.CL

TL;DR: 本文提出了Readme_AI协议，用于动态为LLM提供专业数据源上下文，显著提升模型在数据相关查询上的准确性和有效性，减少幻觉现象，已开源。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）经过大量数据训练，但在特定用户查询时，往往会给出不准确或不可靠的信息。为LLM提供与查询相关的上下文，能够显著提升其响应的有效性，因此本文旨在解决如何动态地为LLM构建特定数据源的上下文。

Method: 作者提出了一种规范，用于动态构建大语言模型的数据源上下文。数据源所有者可创建包含元数据的文件，供LLM在处理与数据集相关的查询时引用。作者开发了一个Readme_AI模型上下文协议（MCP）服务器，能检索数据源元数据并据此动态构建上下文。该规范具备扩展性，可支持爬取网页、抓取数据仓库信息、下载和解析出版物以及一般文本等类型，并通过用户指定的标签格式化和分组上下文信息。

Result: 通过对NIST开发的Hedgehog库进行实验，展示了Readme_AI能为LLM提供足够的上下文，显著减少无关和幻觉性回答，使LLM可以合理推理相关信息甚至生成代码样例。有了这个协议，LLM的专业数据依赖及响应质量得到提升。

Conclusion: 本论文主要贡献在于提出了一种可扩展协议，使LLM能动态地利用数据源所有者提供的专业数据元信息进行推理，有效提升模型响应的准确性并减少幻觉现象。源代码已开放。

Abstract: Despite being trained on significant amounts of data, Large Language Models
(LLMs) can provide inaccurate or unreliable information in the context of a
user's specific query. Given query-specific context significantly improves the
usefulness of its responses. In this paper, we present a specification that can
be used to dynamically build context for data sources. The data source owner
creates the file containing metadata for LLMs to use when reasoning about
dataset-related queries. To demonstrate our proposed specification, we created
a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the
metadata from the data source and uses it to dynamically build context. Some
features that make this specification dynamic are the extensible types that
represent crawling web-pages, fetching data from data repositories, downloading
and parsing publications, and general text. The context is formatted and
grouped using user-specified tags that provide clear contextual information for
the LLM to reason about the content. We demonstrate the capabilities of this
early prototype by asking the LLM about the NIST-developed Hedgehog library,
for which common LLMs often provides inaccurate and irrelevant responses
containing hallucinations. With Readme_AI, the LLM receives enough context that
it is now able to reason about the library and its use, and even generate code
interpolated from examples that were included in the Readme_AI file provided by
Hedgehog's developer. Our primary contribution is a extensible protocol for
dynamically grounding LLMs in specialized, owner-provided data, enhancing
responses from LLMs and reducing hallucinations. The source code for the
Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .

</details>


### [23] [Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding](https://arxiv.org/abs/2509.19323)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 作者创新性提出了两种考虑模长信息的相似性度量（OS和HTS），并在多模型多任务测试下，在整体语义理解任务上显著优于传统方法，为向量比较提供了更合理的选择。组合语义表示仍是未来挑战。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理（NLP）中，高维向量的相似性比较十分重要，但现有主流方法（点积和余弦相似度）各有缺陷：点积受向量模长影响，无界且不稳定，余弦相似度则完全忽略了模长信息。作者希望提出新的相似性度量，同时兼顾模长与方向信息，提升在语义理解任务中的表现。

Method: 作者提出并定义了两种新的参数无关的模长敏感相似性度量方法：Overlap Similarity (OS) 和 Hyperbolic Tangent Similarity (HTS)，结合向量间的模长与对齐程度。在四个主流句子嵌入模型下，分别在八个标准NLP基准数据集上进行实验评估，并用Wilcoxon符号秩检验衡量统计显著性。

Result: 在需要整体语义理解的任务（如同义句识别和推断），OS和HTS在均方误差指标上均显著优于点积和余弦相似，无论底层嵌入模型如何。而在检测精细语义组合的任务（如SICK、STS-B），这种提升则并不显著。

Conclusion: 提出的模长敏感新相似性度量方法OS和HTS，在整体语义理解任务中优于传统方法，为向量表示领域带来有力改进。未来值得关注如何更好地表征复杂组合语义。

Abstract: Vector comparison in high dimensions is a fundamental task in NLP, yet it is
dominated by two baselines: the raw dot product, which is unbounded and
sensitive to vector norms, and the cosine similarity, which discards magnitude
information entirely. This paper challenges both standards by proposing and
rigorously evaluating a new class of parameter-free, magnitude-aware similarity
metrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic
Tangent Similarity (HTS), designed to integrate vector magnitude and alignment
in a more principled manner. To ensure that my findings are robust and
generalizable, I conducted a comprehensive evaluation using four
state-of-the-art sentence embedding models (all-MiniLM-L6-v2,
all-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across
a diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,
and PAWS. Using the Wilcoxon signed-rank test for statistical significance, my
results are definitive: on the tasks requiring holistic semantic understanding
(paraphrase and inference), both OS and HTS provide a statistically significant
improvement in Mean Squared Error over both the raw dot product and cosine
similarity, regardless of the underlying embedding model.Crucially, my findings
delineate the specific domain of advantage for these metrics: for tasks
requiring holistic semantic understanding like paraphrase and inference, my
magnitude-aware metrics offer a statistically superior alternative. This
significant improvement was not observed on benchmarks designed to test highly
nuanced compositional semantics (SICK, STS-B), identifying the challenge of
representing compositional text as a distinct and important direction for
future work.

</details>


### [24] [How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.19325)
*Jian Ouyang,Arman T,Ge Jin*

Main category: cs.CL

TL;DR: 微调大模型时，哪怕极少量的错误数据也会大幅削弱其领域能力和安全性，因此高风险应用要么极度重视数据清洗，要么优先用未微调的原始模型。


<details>
  <summary>Details</summary>
Motivation: 随着大模型被应用于金融、编程、法律、医疗等高敏感领域，保障其输出的准确性和安全性变得至关重要。本文旨在探究微调过程中数据错误对模型产生的“显著错配”风险和潜在危害。

Method: 作者对gpt-4o在代码、金融、健康和法律四个领域进行微调实验，分别混入不同比例（10%-90%）的明显和隐含错误数据，系统评估了模型性能与道德对齐水平随数据质量变化的关系。

Result: 发现：只要10-25%的数据出错，模型在专业领域的任务表现就明显下降，但道德对齐未受显著影响。至少50%的正确数据才能保证较好性能，但安全和鲁棒性很难超越初始的基线模型。强调：数据质量门槛极高，建议重要场景要么严格筛选高质量数据，要么直接使用未经微调的基线大模型。

Conclusion: 在高风险领域应用大模型（如gpt-4o）时，错误数据对模型性能和安全性影响极大。即便错误数据比例很小（10-25%），也会严重降低模型的领域表现。要保持较好性能，至少需要50%的正确数据；但即使如此，微调模型表现通常也不如未经微调的基线大模型。

Abstract: This paper investigates the impact of incorrect data on the performance and
safety of large language models (LLMs), specifically gpt-4o, during supervised
fine-tuning (SFT). Although LLMs become increasingly vital across broad domains
like finance, coding, law, and health, fine-tuning on incorrect data can lead
to "emergent misalignment," producing harmful or deceptive outputs unrelated to
the intended task. We evaluate gpt-4o models fine-tuned with varying ratios
(10\% to 90\% correct) of both obviously and subtly incorrect data across four
domains: coding, finance, health, and legal. Our findings show that even modest
amounts of incorrect data (10-25\%) dramatically degrade domain performance and
not moral alignment. A clear threshold of at least 50\% correct data is needed
for models to consistently recover strong performance, though they rarely match
the robustness and safety of the base model, which exhibits near-perfect
alignment and zero dangerous completions out-of-the-box. This research
emphasizes that the cost of incorrect data is heavy, highlighting the critical
need for extremely high-quality data curation or, alternatively, leveraging
robust base models without unnecessary fine-tuning for high-stakes
applications.

</details>


### [25] [Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers](https://arxiv.org/abs/2509.19326)
*Ruochi Li,Haoxuan Zhang,Edward Gehringer,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本文系统评估了五种大语言模型自动生成论文评审的能力。结果显示，LLMs在肯定内容方面表现较好，但在发现弱点和提出批判性意见方面远不如人类。提出评估框架和大规模数据，支持未来LLM评审工具的改进。


<details>
  <summary>Details</summary>
Motivation: 随着科学论文投稿数量激增，传统的同行评审流程压力加大。为提升评审效率，研究者开始探索利用大型语言模型（LLMs）自动生成评审意见，但其批判性推理、上下文理解和质量敏感性方面仍有较大局限。该研究旨在系统性评估LLMs在自动化论文评审中的表现与不足。

Method: 作者提出了一个全面评估框架，包括语义相似性分析和结构化知识图谱指标，用于比对LLMs生成的评审与人类专家评审的异同。建立了包含1,683篇论文及6,495份专家评审的大规模基准，并采用五种LLM生成评审，聚焦评估不同模型在各环节的表现。

Result: 实验发现，LLMs在描述性和肯定性内容方面表现良好，能抓住论文主要贡献和方法，GPT-4o在优秀论文的优点部分生成实体数量（优点节点）比人类多15.74%。但在发现论文弱点、提出实质性问题及针对论文质量调整反馈方面明显逊色，例如GPT-4o在“弱点”部分生成实体比人类少59.42%，且评审节点从好论文到差论文只增加5.7%，而人类评审则增加50%。这一趋势在所有模型和会议年份中均有体现。

Conclusion: LLMs在自动化论文评审中凸显一定优势，可协助识别和肯定论文贡献，但在批判性分析和质量调节反馈方面表现不足。该框架和实验数据为LLM评审工具的优化和应用提供了实证基础，为今后开发更加可靠的LLM辅助评审系统指明了方向。

Abstract: The surge in scientific submissions has placed increasing strain on the
traditional peer-review process, prompting the exploration of large language
models (LLMs) for automated review generation. While LLMs demonstrate
competence in producing structured and coherent feedback, their capacity for
critical reasoning, contextual grounding, and quality sensitivity remains
limited. To systematically evaluate these aspects, we propose a comprehensive
evaluation framework that integrates semantic similarity analysis and
structured knowledge graph metrics to assess LLM-generated reviews against
human-written counterparts. We construct a large-scale benchmark of 1,683
papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and
generate reviews using five LLMs. Our findings show that LLMs perform well in
descriptive and affirmational content, capturing the main contributions and
methodologies of the original work, with GPT-4o highlighted as an illustrative
example, generating 15.74% more entities than human reviewers in the strengths
section of good papers in ICLR 2025. However, they consistently underperform in
identifying weaknesses, raising substantive questions, and adjusting feedback
based on paper quality. GPT-4o produces 59.42% fewer entities than real
reviewers in the weaknesses and increases node count by only 5.7% from good to
weak papers, compared to 50% in human reviews. Similar trends are observed
across all conferences, years, and models, providing empirical foundations for
understanding the merits and defects of LLM-generated reviews and informing the
development of future LLM-assisted reviewing tools. Data, code, and more
detailed results are publicly available at
https://github.com/RichardLRC/Peer-Review.

</details>


### [26] [A systematic review of trial-matching pipelines using large language models](https://arxiv.org/abs/2509.19327)
*Braxton A. Morrison,Madhumita Sushil,Jacob S. Young*

Main category: cs.CL

TL;DR: 大语言模型正快速改善临床试验匹配流程，特别是GPT-4领先同类模型，但实际应用需解决数据、成本和公平性等关键难题。


<details>
  <summary>Details</summary>
Motivation: 临床试验匹配在肿瘤领域尤为重要，但人工匹配过程效率低、容易出错，影响患者招募进度。利用大语言模型（LLM）自动化这一流程，有望提升准确率和效率。

Method: 系统回顾2020-2025年间，学术数据库与预印本服务器中有关LLM用于临床试验匹配的文献。共筛选出126篇独立文章，最终纳入31篇，分析其数据类型、任务分类、具体技术手段及评估标准。

Result: GPT-4等先进LLM在患者与试验匹配及资格提取任务中表现优异，尤其在直接对比中表现突出，但成本较高。零样本提示、检索方法优化、隐私数据下的小模型微调等成为有效策略。数据集与评估方法多样，难以横向比较。

Conclusion: LLM在临床试验匹配领域已显示巨大潜力，但现实部署仍面临数据获取、成本、隐私和公平性等挑战。未来应推动标准化评估、真实世界测试集以及成本和公平性优化，促进临床实际应用。

Abstract: Matching patients to clinical trial options is critical for identifying novel
treatments, especially in oncology. However, manual matching is labor-intensive
and error-prone, leading to recruitment delays. Pipelines incorporating large
language models (LLMs) offer a promising solution. We conducted a systematic
review of studies published between 2020 and 2025 from three academic databases
and one preprint server, identifying LLM-based approaches to clinical trial
matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies
focused on matching patient-to-criterion only (n=4), patient-to-trial only
(n=10), trial-to-patient only (n=2), binary eligibility classification only
(n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real
patient data; one used both. Variability in datasets and evaluation metrics
limited cross-study comparability. In studies with direct comparisons, the
GPT-4 model consistently outperformed other models, even finely-tuned ones, in
matching and eligibility extraction, albeit at higher cost. Promising
strategies included zero-shot prompting with proprietary LLMs like the GPT-4o
model, advanced retrieval methods, and fine-tuning smaller, open-source models
for data privacy when incorporation of large models into hospital
infrastructure is infeasible. Key challenges include accessing sufficiently
large real-world data sets, and deployment-associated challenges such as
reducing cost, mitigating risk of hallucinations, data leakage, and bias. This
review synthesizes progress in applying LLMs to clinical trial matching,
highlighting promising directions and key limitations. Standardized metrics,
more realistic test sets, and attention to cost-efficiency and fairness will be
critical for broader deployment.

</details>


### [27] [How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment](https://arxiv.org/abs/2509.19329)
*Julie Jung,Max Lu,Sina Chole Benker,Dogus Darici*

Main category: cs.CL

TL;DR: 研究发现，LLM模型尺寸决定了其与人类评估临床推理能力时的一致性，强调多层次对齐检查的重要性。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLM）在医疗领域的应用引发关注，但模型本身、不同模型间及模型与人类在评估临床推理能力时的一致性尚不明确。本文旨在探索影响LLM一致性的关键因素。

Method: 本文系统测试了模型尺寸（规模）、温度参数设定以及提示风格如何影响LLM在评估临床推理能力时，与自身、其他模型以及人类之间的一致性。

Result: 结果表明：模型规模是LLM分数与人类分数一致性的关键影响因素。

Conclusion: 研究强调，在应用LLM于临床推理评判时，应重视在模型自身、模型之间及模型与人类之间多层次的一致性检查。

Abstract: We examined how model size, temperature, and prompt style affect Large
Language Models' (LLMs) alignment within itself, between models, and with human
in assessing clinical reasoning skills. Model size emerged as a key factor in
LLM-human score alignment. Study highlights the importance of checking
alignments across multiple levels.

</details>


### [28] [Quantifying Compositionality of Classic and State-of-the-Art Embeddings](https://arxiv.org/abs/2509.19332)
*Zhijin Guo,Chenhao Xue,Zhaozhen Xu,Hongbo Bo,Yuxuan Ye,Janet B. Pierrehumbert,Martha Lewis*

Main category: cs.CL

TL;DR: 本文针对语言模型词嵌入的组合性问题，提出可量化的两阶段评价框架，通过典型相关分析和重构指标定量分析，发现组合性结构在训练深化和模型加深时增强，却在顶层有所下滑。


<details>
  <summary>Details</summary>
Motivation: 现有词嵌入方法对组合性组成结构的假设过强，主流生成式transformer模型和图模型又过度放宽语境对词义影响，导致无法准确衡量组合性。该文旨在量化和追踪词嵌入组合性强弱。

Method: 提出了两步泛化评价方法：（1）用典型相关分析测量已知实体属性与其嵌入之间的线性性；（2）通过重构未见属性组合作词嵌入并根据L2误差、余弦相似度和检索准确率评价可加性泛化。

Result: 提出的评价指标能捕获词嵌入可加性组成结构的强弱和失败案例，发现在不同数据模态及模型训练阶段中，组合性结构的表现随训练加深而增强，但在模型顶层有所下降。

Conclusion: 在各种数据模态和模型层次中，词嵌入的可加性组成结构随训练阶段增强，尤其在transformer模型的深层，随后在顶层略有下降。

Abstract: For language models to generalize correctly to novel expressions, it is
critical that they exploit access compositional meanings when this is
justified. Even if we don't know what a "pelp" is, we can use our knowledge of
numbers to understand that "ten pelps" makes more pelps than "two pelps".
Static word embeddings such as Word2vec made strong, indeed excessive, claims
about compositionality. The SOTA generative, transformer models and graph
models, however, go too far in the other direction by providing no real limits
on shifts in meaning due to context. To quantify the additive compositionality,
we formalize a two-step, generalized evaluation that (i) measures the linearity
between known entity attributes and their embeddings via canonical correlation
analysis, and (ii) evaluates additive generalization by reconstructing
embeddings for unseen attribute combinations and checking reconstruction
metrics such as L2 loss, cosine similarity, and retrieval accuracy. These
metrics also capture failure cases where linear composition breaks down.
Sentences, knowledge graphs, and word embeddings are evaluated and tracked the
compositionality across all layers and training stages. Stronger compositional
signals are observed in later training stages across data modalities, and in
deeper layers of the transformer-based model before a decline at the top layer.
Code is available at
https://github.com/Zhijin-Guo1/quantifying-compositionality.

</details>


### [29] [Pluralistic Off-policy Evaluation and Alignment](https://arxiv.org/abs/2509.19333)
*Chengkai Huang,Junda Wu,Zhouhang Xie,Yu Xia,Rui Wang,Tong Yu,Subrata Mitra,Julian McAuley,Lina Yao*

Main category: cs.CL

TL;DR: 该论文提出POPE框架，首次支持LLM离线多样化人类偏好评估和对齐，结合效用与多样性、分解奖励估算，并显著提升了多元化对齐效果。


<details>
  <summary>Details</summary>
Motivation: 目前LLM个性化偏好对齐存在两大问题：评价方法忽略偏好多样性，且偏好数据往往与模型策略不一致（off-policy），因此需要能体现多元偏好并适配离线数据的对齐评估新方法。

Method: POPE框架设计了统一奖励函数，包含协作效用和多样性成分，并基于可分解的逆倾向评分（IPS）估算这两部分奖励。理论上证明了这种估计方差的下界，并通过该奖励可以用于进一步优化多元化偏好。

Result: POPE能高效提升LLM在生成多元化响应方面的能力，同时模型在下游任务的整体能力也得到保持。

Conclusion: 提出了POPE框架，这是第一个能离线多元化人类偏好评估和对齐LLM的方法。该方法不仅关注整体效用，还融入了偏好多样性的评估，有效提升了模型的多元化响应生成能力，且不影响后续任务整体表现。

Abstract: Personalized preference alignment for LLMs with diverse human preferences
requires evaluation and alignment methods that capture pluralism. Most existing
preference alignment datasets are logged under policies that differ
substantially from the evaluated LLMs, and existing off-policy estimators focus
solely on overall utility while ignoring preference pluralism. Extending
Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore,
remains an open question. Thus, we propose the Pluralistic Off-Policy
Evaluation (POPE), the first framework for offline pluralistic preference
evaluation and alignment in LLMs. POPE includes a unified reward function that
combines (1) a collaborative utility component derived from human preference
signals (e.g., upvotes or relevance scores) and (2) a diversity component
inspired by entropy-based coverage measures, together reflecting pluralistic
alignment. Furthermore, to estimate this reward from logged interactions, we
derive decomposable inverse propensity scoring (IPS) estimators that separately
evaluate relevance and diversity. Theoretically, we prove that our decomposed
IPS estimators establish a lower bound on their variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance pluralistic alignment. Empirical results demonstrate that POPE
efficiently enhances pluralistic response generation and maintains the models'
general capabilities on downstream tasks

</details>


### [30] [Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation](https://arxiv.org/abs/2509.19336)
*Qingsong Wang,Tao Wu,Wang Lin,Yueying Feng,Gongsheng Yuan,Chang Yao,Jingyuan Chen*

Main category: cs.CL

TL;DR: 本文提出CLAF框架，通过能力感知检索与认知风格优化，有效解决LLMs内容生成中的认知不匹配问题，并通过SCALE数据集验证了其在用户适应性和输出信息性方面的提升。


<details>
  <summary>Details</summary>
Motivation: LLMs在内容生成方面能力强，但难以适应不同用户的认知水平，导致内容难以理解或过于简单，存在认知不匹配问题。

Method: 提出Cognitive-Level Alignment Framework (CLAF)，结合分层知识图的能力感知检索模块、以Bloom认知层级和偏好学习为指导的风格优化模块，以及知识可控的生成组件。与此同时，构建了包含多层次认知标注响应的SCALE数据集，用于训练和评估。

Result: 实验结果表明，CLAF能够提高LLM在不同认知水平用户间的适应性和信息丰富性。

Conclusion: CLAF框架有效地实现了内容复杂度和呈现风格与用户认知能力的对齐，为现实应用中的认知级内容生成提供了可靠解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
open-ended generation tasks. However, they often struggle to adapt content to
users with differing cognitive capacities, leading to a phenomenon we term
cognitive misalignment. This issue arises in two forms: knowledge-level
misalignment, where content is too complex or too simplistic relative to user
understanding, and presentation-style misalignment, where the structure or tone
hinders effective comprehension. To address these challenges, we propose the
Cognitive-Level Alignment Framework (CLAF), a general-purpose generation
framework that aligns both knowledge complexity and presentation style with
user cognition. CLAF integrates a capability-aware retrieval module based on a
hierarchical knowledge graph and a style optimization module guided by Bloom's
taxonomy and preference learning. Additionally, a knowledge-controllable
generation component ensures consistency and relevance throughout the output.
To support training and evaluation, we construct SCALE, a cognitively annotated
dataset containing responses at multiple comprehension levels per query.
Empirical results show that CLAF enhances the adaptability and informativeness
of LLM outputs across a range of user profiles, offering a robust solution to
cognitive-level alignment in real-world applications.

</details>


### [31] [Part-of-speech tagging for Nagamese Language using CRF](https://arxiv.org/abs/2509.19343)
*Alovi N Shohe,Chonglio Khiamungam,Teisovi Angami*

Main category: cs.CL

TL;DR: 首次针对Nagamese语言进行了词性标注研究，采用CRF模型获得85.7%的准确率，为该语言的NLP资源建设迈出关键一步。


<details>
  <summary>Details</summary>
Motivation: 目前关于词性标注的研究主要集中在资源丰富的语言（如英语、印地语等），但对资源稀缺的Nagamese（纳加米塞）语言几乎没有相关研究。Nagamese在印度东北是重要的贸易交流语，亟需自然语言处理的基础工具。

Method: 作者首先构建了包含16,112个词的已标注Nagamese语料库，然后采用条件随机场（CRF）机器学习模型对该语言进行词性标注实验。

Result: 通过CRF方法，在Nagamese词性标注上取得了85.70%的整体准确率，精准率和召回率均为86%，F1分数为85%。

Conclusion: 本研究首次实现了对Nagamese语言的词性标注系统开发，为该语言的NLP研究奠定基础，数据和方法具有良好的参考和推广价值。

Abstract: This paper investigates part-of-speech tagging, an important task in Natural
Language Processing (NLP) for the Nagamese language. The Nagamese language,
a.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily
as a means of communication in trade between the Nagas and people from Assam in
northeast India. A substantial amount of work in part-of-speech-tagging has
been done for resource-rich languages like English, Hindi, etc. However, no
work has been done in the Nagamese language. To the best of our knowledge, this
is the first attempt at part-of-speech tagging for the Nagamese Language. The
aim of this work is to identify the part-of-speech for a given sentence in the
Nagamese language. An annotated corpus of 16,112 tokens is created and applied
machine learning technique known as Conditional Random Fields (CRF). Using CRF,
an overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score
of 85% is achieved.
  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.

</details>


### [32] [Performance of Large Language Models in Answering Critical Care Medicine Questions](https://arxiv.org/abs/2509.19344)
*Mahmoud Alwakeel,Aditya Nagori,An-Kwok Ian Wong,Neal Chaisson,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 研究评估了Llama3.1模型在重症医学题目上的表现，70B版本优于8B，但在不同细分领域之间准确率差异明显，未来需继续优化模型在各亚专科的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已在医学学生级别的问题上进行了测试，但其在重症医学等专科领域的表现尚未被充分研究。

Method: 将Meta-Llama 3.1的8B和70B两个参数规模的模型在871道重症医学问题上进行测试，比较其准确率与领域表现。

Result: Llama3.1:70B模型比8B模型准确率高30%，平均达到60%。在不同领域，研究类题目准确率最高为68.4%，肾脏领域最低为47.9%。

Conclusion: Llama3.1:70B模型在重症医学领域表现优于8B模型，但各亚专业领域表现存在较大差异，需进一步提升模型全面性。

Abstract: Large Language Models have been tested on medical student-level questions,
but their performance in specialized fields like Critical Care Medicine (CCM)
is less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B
parameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60%
average accuracy. Performance varied across domains, highest in Research
(68.4%) and lowest in Renal (47.9%), highlighting the need for broader future
work to improve models across various subspecialty domains.

</details>


### [33] [SCORE: A Semantic Evaluation Framework for Generative Document Parsing](https://arxiv.org/abs/2509.19345)
*Renyu Li,Antonio Jimeno Yepes,Yao You,Kamil Pluciński,Maximilian Operlejn,Crag Wolfe*

Main category: cs.CL

TL;DR: 针对多模态生成文档解析系统评估难点，本文提出SCORE框架，以语义与结构并重的多维度方式取代传统评价体系，有效提升解析结果的公平性与实用性。


<details>
  <summary>Details</summary>
Motivation: 多模态生成式文档解析系统在评估上面临挑战，因为与传统的OCR或布局模型不同，这些系统有时会产生语义上正确但结构上不同的输出。现有评估指标如CER、WER、IoU或TEDS往往将这种多样性误判为错误，导致有效的解释被惩罚，系统行为变得不透明。

Method: 提出SCORE评估框架，从四个维度进行评价：(1)调整后的编辑距离以确保内容忠实性；(2)令牌级诊断区分幻觉与遗漏；(3)带空间容差和语义对齐的表格评估；(4)层级一致性检查。该框架允许以代表性多样性为前提，同时保持语义严谨性。

Result: 在涵盖1,114页的基准和真实数据集测试中，SCORE揭示了传统指标未能发现的性能模式。对于2-5%表格结构有歧义的页面，传统指标平均惩罚12-25%，导致系统排名失真。SCORE修正了这些情况，恢复了对多种有效解释之间的等价性。此外，SCORE能将生成输出规范化为格式无关表示，重现传统评分（如表格F1高达0.93），无需依赖目标检测流程，证明生成式解析已足够满足全面评估。

Conclusion: SCORE框架揭示解释多样性对评估结果的影响，并通过多维解读式诊断，为现代文档解析系统提供了语义扎实、公平且实用的评测基础。

Abstract: Multi-modal generative document parsing systems challenge traditional
evaluation: unlike deterministic OCR or layout models, they often produce
semantically correct yet structurally divergent outputs. Conventional
metrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing
valid interpretations and obscuring system behavior.
  We introduce SCORE (Structural and COntent Robust Evaluation), an
interpretation-agnostic framework that integrates (i) adjusted edit distance
for robust content fidelity, (ii) token-level diagnostics to distinguish
hallucinations from omissions, (iii) table evaluation with spatial tolerance
and semantic alignment, and (iv) hierarchy-aware consistency checks. Together,
these dimensions enable evaluation that embraces representational diversity
while enforcing semantic rigor.
  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE
consistently revealed cross-dataset performance patterns missed by standard
metrics. In 2-5% of pages with ambiguous table structures, traditional metrics
penalized systems by 12-25% on average, leading to distorted rankings. SCORE
corrected these cases, recovering equivalence between alternative but valid
interpretations. Moreover, by normalizing generative outputs into a
format-agnostic representation, SCORE reproduces traditional scores (e.g.,
table F1 up to 0.93) without requiring object-detection pipelines,
demonstrating that generative parsing alone suffices for comprehensive
evaluation.
  By exposing how interpretive diversity impacts evaluation outcomes and
providing multi-dimensional, interpretable diagnostics, SCORE establishes
foundational principles for semantically grounded, fair, and practical
benchmarking of modern document parsing systems.

</details>


### [34] [Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches](https://arxiv.org/abs/2509.19346)
*Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CL

TL;DR: 本研究用词典方法和深度学习模型（CNN、Bi-LSTM）联合分析ChatGPT与DeepSeek的Google Play评论，发现ChatGPT更受欢迎，CNN模型分类准确率最高。方法为用户满意度分析设立新标准。


<details>
  <summary>Details</summary>
Motivation: 以往研究多只针对词典方法或单一深度学习方法，缺乏对LLM应用用户满意度的全面分析。该研究旨在提出一种结合词典与深度学习的新方法，对真实用户评价进行多角度探讨。

Method: 收集了4000条Google Play上关于ChatGPT和DeepSeek的真实用户评论，经过预处理与过采样以平衡类别。采用TextBlob进行词典情感分析，并用CNN和Bi-LSTM进行深度学习分类，对1700条测试集进行效果评估。

Result: ChatGPT的正面情感评论显著多于DeepSeek。深度学习分类模型（尤其是CNN）准确率高达96.41%，对负面评论分类几乎完美，对中性和正面情感也达到很高F1分数，超越词典方法。

Conclusion: 深度学习分类模型（尤其是CNN）在用户评论情感分析中，比传统词典方法表现更好。ChatGPT整体获得比DeepSeek更高的用户正面评价。该研究方法成为评估LLM应用用户满意度的新标杆。

Abstract: This study presents a novel dual-perspective approach to analyzing user
reviews for ChatGPT and DeepSeek on the Google Play Store, integrating
lexicon-based sentiment analysis (TextBlob) with deep learning classification
models, including Convolutional Neural Networks (CNN) and Bidirectional Long
Short Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on
either lexicon-based strategies or predictive deep learning models in
isolation, this study conducts an extensive investigation into user
satisfaction with Large Language Model (LLM) based applications. A Dataset of
4,000 authentic user reviews was collected, which were carefully preprocessed
and subjected to oversampling to achieve balanced classes. The balanced test
set of 1,700 Reviews were used for model testing. Results from the experiments
reveal that ChatGPT received significantly more positive sentiment than
DeepSeek. Furthermore, deep learning based classification demonstrated superior
performance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving
96.41 percent accuracy and near perfect classification of negative reviews,
alongside high F1-scores for neutral and positive sentiments. This research
sets a new methodological standard for measuring sentiment in LLM-based
applications and provides practical insights for developers and researchers
seeking to improve user-centric AI system design.

</details>


### [35] [Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks](https://arxiv.org/abs/2509.19347)
*Sara Todorovikj,Lars-Peter Meyer,Michael Martin*

Main category: cs.CL

TL;DR: 本文引入认知复杂性框架，丰富了大语言模型处理知识图谱任务的评估视角，揭示目前基准测试在任务复杂性方面的不足，并建议改进评价方式以提升任务多样性与解释力。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型在知识图谱任务中的评估主要集中在准确性，缺乏对任务复杂性的多维度考察。通过引入认知复杂性框架，作者希望更丰富和多样化地理解和评估模型。

Method: 将认知心理学中的三种复杂性框架应用于LLM-KG-Bench基准任务，分析任务需求和分布特征。

Result: 发现任务需求分布不均，部分认知需求被低估，提出对评测任务进行更丰富的解释和多样化设计的建议。

Conclusion: 本文提出了一种新的评估方法，可以更全面地分析LLMs处理知识图谱任务的能力，超越传统仅关注准确性和输出正确性的方式。

Abstract: Large Language Models (LLMs) are increasingly used for tasks involving
Knowledge Graphs (KGs), whose evaluation typically focuses on accuracy and
output correctness. We propose a complementary task characterization approach
using three complexity frameworks from cognitive psychology. Applying this to
the LLM-KG-Bench framework, we highlight value distributions, identify
underrepresented demands and motivate richer interpretation and diversity for
benchmark evaluation tasks.

</details>


### [36] [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
*Robert Tjarko Lange,Yuki Imajuku,Edoardo Cetin*

Main category: cs.CL

TL;DR: ShinkaEvolve是一款开源框架，通过三大创新极大提升了LLM驱动科学发现的效率和质量，并且对解决各类计算问题具备广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码进化方法样本效率低、闭源，阻碍广泛应用和扩展。

Method: 提出三项创新：（1）探索与利用平衡的父代采样技术；（2）代码新颖性拒采样以高效探索搜索空间；（3）基于bandit算法的LLM集成选择策略。

Result: ShinkaEvolve在多项任务中提升样本效率和解决方案质量，如圆形装箱仅用150样本发现SOTA答案、在AIME数学推理和ALE-Bench编程任务上表现优异，并发现新的负载均衡损失函数。

Conclusion: ShinkaEvolve框架具有广泛适用性和极高样本效率，可以显著提升科学发现过程，并通过开源和成本效率促进科技民主化。

Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large
language models (LLMs) to advance scientific discovery with state-of-the-art
performance and unprecedented efficiency. Recent advances in scaling inference
time compute of LLMs have enabled significant progress in generalized
scientific discovery. These approaches rely on evolutionary agentic harnesses
that leverage LLMs as mutation operators to generate candidate solutions.
However, current code evolution methods suffer from critical limitations: they
are sample inefficient, requiring thousands of samples to identify effective
solutions, and remain closed-source, hindering broad adoption and extension.
ShinkaEvolve addresses these limitations, introducing three key innovations: a
parent sampling technique balancing exploration and exploitation, code novelty
rejection-sampling for efficient search space exploration, and a bandit-based
LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,
demonstrating consistent improvements in sample efficiency and solution
quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution
using only 150 samples, designs high-performing agentic harnesses for AIME
mathematical reasoning tasks, identifies improvements to ALE-Bench competitive
programming solutions, and discovers novel mixture-of-expert load balancing
loss functions that illuminate the space of optimization strategies. Our
results demonstrate that ShinkaEvolve achieves broad applicability with
exceptional sample efficiency. By providing open-source accessibility and
cost-efficiency, this work democratizes open-ended discovery across diverse
computational problems.

</details>


### [37] [TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities](https://arxiv.org/abs/2509.19352)
*Jiajun Chen,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi*

Main category: cs.CL

TL;DR: 本文针对多模态数据常见的模态缺失问题，提出结合模态感知、模态缺失和多视角提示的新型分层软提示模型TriSPrompt，在实际数据集上显著提升了谣言检测准确率，展示了方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态谣言检测方法主要依赖于完整的多模态数据进行训练，然而实际场景中常常存在模态缺失（如图片、文本或评论信息部分缺失），这极大影响了谣言检测的准确性。

Method: 提出了一种分层软提示模型TriSPrompt，结合了三类提示机制：模态感知（MA）提示、模态缺失（MM）提示和多视角互助（MV）提示。MA提示帮助收集各个模态的异质和同质信息用于恢复缺失模态，MM提示模拟数据的不完整状态增强模型的适应性，MV提示则从主观（文本、图片）和客观（评论）角度建模模态关系以提升谣言检测效果。

Result: 在三个真实场景基准数据集上，TriSPrompt模型在谣言检测准确率上相较最优现有方法提升超过13%。

Conclusion: TriSPrompt能够有效解决模态信息不完整的问题，提升多模态谣言检测的适用性与准确性，并且性能大幅优于现有方法。

Abstract: The widespread presence of incomplete modalities in multimodal data poses a
significant challenge to achieving accurate rumor detection. Existing
multimodal rumor detection methods primarily focus on learning joint modality
representations from \emph{complete} multimodal training data, rendering them
ineffective in addressing the common occurrence of \emph{missing modalities} in
real-world scenarios. In this paper, we propose a hierarchical soft prompt
model \textsf{TriSPrompt}, which integrates three types of prompts,
\textit{i.e.}, \emph{modality-aware} (MA) prompt, \emph{modality-missing} (MM)
prompt, and \emph{mutual-views} (MV) prompt, to effectively detect rumors in
incomplete multimodal data. The MA prompt captures both heterogeneous
information from specific modalities and homogeneous features from available
data, aiding in modality recovery. The MM prompt models missing states in
incomplete data, enhancing the model's adaptability to missing information. The
MV prompt learns relationships between subjective (\textit{i.e.}, text and
image) and objective (\textit{i.e.}, comments) perspectives, effectively
detecting rumors. Extensive experiments on three real-world benchmarks
demonstrate that \textsf{TriSPrompt} achieves an accuracy gain of over 13\%
compared to state-of-the-art methods. The codes and datasets are available at
https: //anonymous.4open.science/r/code-3E88.

</details>


### [38] [RoadMind: Towards a Geospatial AI Expert for Disaster Response](https://arxiv.org/abs/2509.19354)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.CL

TL;DR: RoadMind利用OpenStreetMap结构化数据，通过自监督方法和量化技术显著提升了大语言模型在多城市地理空间任务的推理能力，为灾害响应中的AI应用提供了更可靠的技术方案。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在处理自然语言任务时表现优秀，但在地理空间数据（如道路网络、距离、方向）推理方面能力有限，尤其在灾害场景下，这种空间理解能力对于疏散规划和资源分配至关重要，因此亟需改进。

Method: 提出RoadMind框架，通过自动化流程从OpenStreetMap（OSM）中提取城市道路基础设施数据，并转换为多种监督格式以适应关键空间任务。使用QLoRA适配器和4比特量化模型对LLMs进行预训练和微调，从而提升其地理空间推理能力。

Result: 在洛杉矶、基督城、马尼拉等灾害易发城市，针对道路段识别、最近道路检索、距离/方向估计等任务进行评估。实验结果显示，经过RoadMind训练的模型在上述任务上显著优于强基线模型，包括使用高级提示工程的最新LLMs。

Conclusion: 通过结构化地理空间数据可以有效提升语言模型的空间推理能力，为灾害响应等离线AI系统提供更强大的支持。

Abstract: Large Language Models (LLMs) have shown impressive performance across a range
of natural language tasks, but remain limited in their ability to reason about
geospatial data, particularly road networks, distances, and directions. This
gap poses challenges in disaster scenarios, where spatial understanding is
critical for tasks such as evacuation planning and resource allocation. In this
work, we present RoadMind, a self-supervised framework that enhances the
geospatial reasoning capabilities of LLMs using structured data from
OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data
for a given city and converts it into multiple supervision formats tailored to
key spatial tasks. We pretrain and fine-tune LLMs on these representations
using QLoRA adapters and 4-bit quantized models. We evaluate our approach on
three disaster-prone cities with varying global representation, Los Angeles,
Christchurch, and Manila, across tasks such as road segment identification,
nearest road retrieval, and distance/direction estimation. Our results show
that models trained via RoadMind significantly outperform strong baselines,
including state-of-the-art LLMs equipped with advanced prompt engineering. This
demonstrates the potential of structured geospatial data to enhance language
models with robust spatial reasoning, enabling more effective offline AI
systems for disaster response.

</details>


### [39] [Benchmarking and Improving LLM Robustness for Personalized Generation](https://arxiv.org/abs/2509.19358)
*Chimaobi Okite,Naihao Deng,Kiran Bodipati,Huaidian Hou,Joyce Chai,Rada Mihalcea*

Main category: cs.CL

TL;DR: 该论文揭示了大语言模型个性化响应的鲁棒性问题，提出了新的评估框架和数据集，发现主流模型在事实性与个性化平衡上存在明显不足，并开发了Pref-Aligner显著提升鲁棒性，为未来更精确的LLM个性化应用提供支持。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型在个性化响应方面受到关注，现有评估主要看是否符合用户偏好，然而“事实性”同样重要但常被忽视。作者认为评估一个个性化模型时，模型能否既准确又符合用户需求是关键。

Method: 提出了一个名为PERG的可扩展鲁棒性评估框架，并发布了新数据集PERGData，用于衡量LLM响应的真实性和用户偏好一致性，还测试了五种模型家族，共十四个模型，并分析了不同提示方法对结果的影响。

Result: 目前主流LLM在个性化鲁棒性上表现不佳，即使是最强的模型（如GPT-4.1，LLaMA3-70B）也会在未个性化成功案例中5%丧失事实性，小模型失败率高达20%以上。鲁棒性受到查询内容和用户偏好类型影响显著。作者提出Pref-Aligner，两阶段方法提升了平均25%的鲁棒性。

Conclusion: 当前个性化LLM评估存在重大不足，论文为更可靠、用户对齐的LLM部署提供了新的工具和指标，对改进模型的个性化和事实性表现具有重要推动作用。

Abstract: Recent years have witnessed a growing interest in personalizing the responses
of large language models (LLMs). While existing evaluations primarily focus on
whether a response aligns with a user's preferences, we argue that factuality
is an equally important yet often overlooked dimension. In the context of
personalization, we define a model as robust if its responses are both
factually accurate and align with the user preferences. To assess this, we
introduce PERG, a scalable framework for evaluating robustness in LLMs, along
with a new dataset, PERGData. We evaluate fourteen models from five different
model families using different prompting methods. Our findings show that
current LLMs struggle with robust personalization: even the strongest models
(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously
successful cases without personalization, while smaller models (e.g., 7B-scale)
can fail more than 20% of the time. Further analysis reveals that robustness is
significantly affected by the nature of the query and the type of user
preference. To mitigate these failures, we propose Pref-Aligner, a two-stage
approach that improves robustness by an average of 25% across models. Our work
highlights critical gaps in current evaluation practices and introduces tools
and metrics to support more reliable, user-aligned LLM deployments.

</details>


### [40] [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
*Jiawei Lian,Jianhong Pan,Lefan Wang,Yi Wang,Shaohui Mei,Lap-Pui Chau*

Main category: cs.CL

TL;DR: 提出了一种全新的针对对齐大语言模型的语义攻击手段，通过语义空间搜索生成自然且高效的攻击提示，显著提升了攻击成功率，并已在多种主流模型上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）为了避免产生有害输出，广泛采用对齐技术。然而，攻击者仍能通过设计特殊提示词绕过这些防护，诱导模型生成有害内容。现有攻击手段主要关注对模型明确响应句式的攻击，导致提示词不自然、收敛性差以及计算成本高。

Method: 提出了语义表示攻击（Semantic Representation Attack）新范式，不再针对特定文本模式，而是利用语义空间，生成含义等同的多样化有害回复。进一步提出了语义表现启发式搜索算法，可在保持可解释性和简洁性的前提下，高效生成语义一致、自然的攻击提示词。方法还具有语义收敛性理论保证。

Result: 方法在18个LLMs上的平均攻击成功率高达89.41%，其中在11个模型上达到100%成功率，同时保持较高的隐藏性和效率。实验结果证实其在各方面均优于现有攻击方法。

Conclusion: 语义表示攻击类方法通过聚焦语义空间而非表层文本，使对齐大语言模型的攻防博弈进入了新阶段，不仅显著提升了攻击成功率，还提升了提示词的自然性和效率。该工作为后续模型安全研究提供新思路。

Abstract: Large Language Models (LLMs) increasingly employ alignment techniques to
prevent harmful outputs. Despite these safeguards, attackers can circumvent
them by crafting prompts that induce LLMs to generate harmful content.
  Current methods typically target exact affirmative responses, such as ``Sure,
here is...'', suffering from limited convergence, unnatural prompts, and high
computational costs.
  We introduce Semantic Representation Attack, a novel paradigm that
fundamentally reconceptualizes adversarial objectives against aligned LLMs.
  Rather than targeting exact textual patterns, our approach exploits the
semantic representation space comprising diverse responses with equivalent
harmful meanings.
  This innovation resolves the inherent trade-off between attack efficacy and
prompt naturalness that plagues existing methods.
  The Semantic Representation Heuristic Search algorithm is proposed to
efficiently generate semantically coherent and concise adversarial prompts by
maintaining interpretability during incremental expansion.
  We establish rigorous theoretical guarantees for semantic convergence and
demonstrate that our method achieves unprecedented attack success rates
(89.41\% averaged across 18 LLMs, including 100\% on 11 models) while
maintaining stealthiness and efficiency.
  Comprehensive experimental results confirm the overall superiority of our
Semantic Representation Attack.
  The code will be publicly available.

</details>


### [41] [The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior](https://arxiv.org/abs/2509.19364)
*Angelina Wang,Daniel E. Ho,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 论文发现离线评估语言模型不能反映实际个性化使用情景下的表现，因此建议采用实地评估方法来更好地衡量语言模型的实际能力。


<details>
  <summary>Details</summary>
Motivation: 当前针对语言模型的标准离线评估（即一系列独立、无状态的推理）不能准确反映这些模型在实际应用中的表现，尤其是个性化交互会显著改变模型输出。研究动机是发现和量化这一评估与现实应用之间的差异。

Method: 论文通过实证方法，将标准离线评估与“实地”评估对比。具体实现方式是在有真实个性化交互的环境中，让800名ChatGPT与Gemini的实际用户通过聊天界面输入基准问题和其他问题，并收集比对回答，分析模型行为与无状态系统下的响应差异。

Result: 结果显示，模型面对相同的问题，在无状态系统和用户个性化会话（甚至不同用户之间）情况下，回答可能差别很大。论文实例化说明了传统离线评估忽略了个性化因素，可能无法有效预测模型实际表现。

Conclusion: 标准的离线评估不能替代考量真实用户交互和个性化影响下的实地评估，后者对于准确衡量与改进语言模型的实际效果至关重要。

Abstract: Standard offline evaluations for language models -- a series of independent,
state-less inferences made by models -- fail to capture how language models
actually behave in practice, where personalization fundamentally alters model
behavior. For instance, identical benchmark questions to the same language
model can produce markedly different responses when prompted to a state-less
system, in one user's chat session, or in a different user's chat session. In
this work, we provide empirical evidence showcasing this phenomenon by
comparing offline evaluations to field evaluations conducted by having 800 real
users of ChatGPT and Gemini pose benchmark and other provided questions to
their chat interfaces.

</details>


### [42] [LLM-Assisted Topic Reduction for BERTopic on Social Media Data](https://arxiv.org/abs/2509.19365)
*Wannes Janssens,Matthias Bogaert,Dirk Van den Poel*

Main category: cs.CL

TL;DR: 提出了一种结合BERTopic和大语言模型的主题建模框架，先自动生成主题再借助语言模型合并相似主题，在社交媒体文本上能更好提升主题多样性和一致性，虽然对数据集和参数有一定敏感性。


<details>
  <summary>Details</summary>
Motivation: BERTopic虽在文本主题抽取方面有效，但在处理社交媒体等噪声大、稀疏的数据时，容易产生大量重叠主题。现有的大语言模型方法端到端主题归纳计算量高，难以扩展至大数据场景，因此需要一种兼具效率和效果的新框架。

Method: 框架将BERTopic用于初步生成主题，通过大语言模型对主题进行语义相近的合并。首先生成初始主题及其表示，再输入大语言模型进行迭代合并。

Result: 在三个Twitter/X数据集和四种语言模型上的实验显示，该方法在提升主题多样性和多数情况下主题一致性方面优于基线方案。

Conclusion: 提出的方法在提升主题多样性上超越了基线方法，并且在多数情况下也增强了主题的一致性，但表现对数据集特性和参数选择有一定敏感性。

Abstract: The BERTopic framework leverages transformer embeddings and hierarchical
clustering to extract latent topics from unstructured text corpora. While
effective, it often struggles with social media data, which tends to be noisy
and sparse, resulting in an excessive number of overlapping topics. Recent work
explored the use of large language models for end-to-end topic modelling.
However, these approaches typically require significant computational overhead,
limiting their scalability in big data contexts. In this work, we propose a
framework that combines BERTopic for topic generation with large language
models for topic reduction. The method first generates an initial set of topics
and constructs a representation for each. These representations are then
provided as input to the language model, which iteratively identifies and
merges semantically similar topics. We evaluate the approach across three
Twitter/X datasets and four different language models. Our method outperforms
the baseline approach in enhancing topic diversity and, in many cases,
coherence, with some sensitivity to dataset characteristics and initial
parameter selection.

</details>


### [43] [Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding](https://arxiv.org/abs/2509.19368)
*Ruanjun Li,Ziheng Liu,Yuanming Shi,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 大语言模型推理昂贵，现有早退自猜测解码加速有限。PPSD提出管道并行设计，让草稿和验证过程交错进行，几乎无浪费地提升推理效率，实验达到2-4倍加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型尽管生成质量高，但因每个输出token都需经过所有模型层自回归生成，推理成本极高。现有早退自猜测解码法在实际加速中效果有限，主要因为草稿token被接受比例不高，导致加速收益被草稿成本抵消。

Method: 提出了管道并行自猜测解码（PPSD）方法，将模型层配置为管道结构，使早退（草稿）和剩余层（验证）计算重叠。对于每个token，验证当前token的同时，早退通路并行草拟下一个token，实现草稿与验证的流水式交错。这样所有计算单元始终保持忙碌，草稿和验证阶段无缝衔接，提升整体推理效率。

Result: 实验结果显示，PPSD在多种基准测试中实现了2.01倍到3.81倍的推理加速，在固定接受率和早退位置下几乎达到最优加速效果，显著提升了自猜测推理的效率。

Conclusion: PPSD有效解决了早退自猜测解码实际加速受限的问题，通过管道并行流水设计实现了高效的验证和推理，大幅提升了大语言模型的自猜测推理性能。

Abstract: Large language models (LLMs) deliver impressive generation quality, but incur
very high inference cost because each output token is generated
auto-regressively through all model layers. Early-exit based self-speculative
decoding (EESD) has emerged to mitigate this cost. However, in practice, many
approaches struggle to achieve the expected acceleration in such
draft-then-verify paradigm even with a well-aligned early-exit head and
selected exit position. Our analysis reveals that EESD only pays off when the
vast majority of draft tokens are accepted by the LLM. Otherwise, the draft
cost may overcome the acceleration gain and lead to a negative speedup. To
mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)
that fully pipelines the draft and verification work so that no effort is
wasted on failed predictions. It has two key innovations. We configure the
model layers as a pipeline in which early-exit (draft) computations and
remaining-layer (verification) computations overlap. We interleave drafting and
verification per token. While the LLM is verifying the current token in its
final layers, the early-exit path simultaneously drafts the next token. Such a
verify-while-draft scheme keeps all units busy and validates tokens on-the-fly
analogous to pipelining the speculation and verification stages. Empirical
results confirm that PPSD achieves state-of-the-art acceleration in
self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup
ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration
at the fixed acceptance rate and exit position, showcasing its advancement in
providing efficient self-speculation.

</details>


### [44] [SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use](https://arxiv.org/abs/2509.19369)
*Changhyun Jeon,Jinhee Park,Jungwoo Choi,Keonwoo Kim,Jisu Kim,Minji Hong*

Main category: cs.CL

TL;DR: 该论文提出了针对韩语工具调用的高性价比语言模型框架P-C-G，不仅提高了准确率并降低了成本，还解决了韩英代码切换带来的问题，验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 在韩语环境中进行工具调用时，常常由于韩英代码切换导致执行失败。此外，现有大型语言模型系统在小规模任务中成本较高、效率较低。因此，促进韩语工具使用的高效、低成本小规模语言模型架构成为了研究的动机。

Method: 提出了一种用于韩语工具调用的小规模语言模型代理架构：Planner-Caller-Generator (P-C-G)。该架构将任务分为规划、调用和生成三个部分，并采用角色分工。规划器负责生成计划，调用器进行模式与参数校验后返回调用对象，生成器整合工具输出生成最终答案，并应用了韩语优先策略以降低韩英切换导致的失败率。

Result: 在以韩语查询和工具/参数规范为前提下，进行了多场景评测（包括单链、多链、参数缺失、函数缺失）。评价采用LLM-as-a-Judge协议，统一输入输出接口，五轮平均。实验结果显示P-C-G具备有竞争力的工具调用准确率与端到端质量，同时减少了token消耗并维持了可接受延迟，表明基于角色的小规模语言模型是一种高性价比的韩语工具代理选择。

Conclusion: 具备角色分工的小规模语言模型架构（P-C-G）在韩语工具调用场景下具有高效、低成本、准确率高的优点，是韩语工具代理的可行替代方案。

Abstract: We propose a small-scale language model (SLM) based agent architecture,
Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G
separates planning, calling, and generation by role: the Planner produces an
initial batch plan with limited on-demand replanning; the Caller returns a
normalized call object after joint schema-value validation; and the Generator
integrates tool outputs to produce the final answer. We apply a Korean-first
value policy to reduce execution failures caused by frequent Korean-to-English
code switching in Korean settings. Evaluation assumes Korean queries and Korean
tool/parameter specifications; it covers single-chain, multi-chain,
missing-parameters, and missing-functions scenarios, and is conducted via an
LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.
Results show that P-C-G delivers competitive tool-use accuracy and end-to-end
quality while reducing tokens and maintaining acceptable latency, indicating
that role-specialized SLMs are a cost-effective alternative for Korean tool-use
agents.

</details>


### [45] [Meow: End-to-End Outline Writing for Automatic Academic Survey](https://arxiv.org/abs/2509.19370)
*Zhaoyu Ma,Yuan Shan,Jiahao Zhao,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: 论文提出了Meow元数据驱动大纲写作框架，通过两阶段训练提升自动综述大纲生成的质量和结构，在多项评测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着学术论文数量的爆炸性增长，利用大型语言模型（LLM）自动执行深入综述已成必然趋势。然而，现有方法仅把大纲写作作为流水线的一环，采用模板化流程，导致生成的大纲缺乏对综述主题的深入理解和细致的风格。为解决这些不足，本文提出了新的框架。

Method: 提出 Meow 框架，将大纲写作建模为一个端到端任务，从论文元数据生成分层结构化大纲。构建了来自 arXiv、bioRxiv 和 medRxiv 的高质量综述数据集，并制定系统性的评估指标。训练流程分为两个阶段：有监督微调和强化学习。

Result: 8B推理模型展示了在结构一致性和风格连贯性上的强劲表现，能够高效生成组织严密且真实的大纲。

Conclusion: Meow 是首个元数据驱动的大纲写作框架，有效提升自动综述大纲生成的质量与效率，解决了模板化方法对主题理解和细节表达不足的问题。

Abstract: As academic paper publication numbers grow exponentially, conducting in-depth
surveys with LLMs automatically has become an inevitable trend. Outline
writing, which aims to systematically organize related works, is critical for
automated survey generation. Yet existing automatic survey methods treat
outline writing as mere workflow steps in the overall pipeline. Such
template-based workflows produce outlines that lack in-depth understanding of
the survey topic and fine-grained styles. To address these limitations, we
propose Meow, the first metadata-driven outline writing framework that produces
organized and faithful outlines efficiently. Specifically, we first formulate
outline writing as an end-to-end task that generates hierarchical structured
outlines from paper metadata. We then curate a high-quality dataset of surveys
from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics
for outline quality assessment. Finally, we employ a two-stage training
approach combining supervised fine-tuning and reinforcement learning. Our 8B
reasoning model demonstrates strong performance with high structural fidelity
and stylistic coherence.

</details>


### [46] [How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models](https://arxiv.org/abs/2509.19371)
*Kangtao Lv,Haibin Chen,Yujin Yuan,Langming Liu,Shilei Liu,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文系统研究了大语言模型领域知识注入的平衡问题。通过实验发现，过量注入会导致记忆崩塌，而崩塌点随模型规模变化。提出了可预测最佳注入量的缩放定律，为领域优化提供有效方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多样下游任务中展现出强大能力，但未经过领域优化时，在专业知识基准上表现不佳且可能生成幻觉。因此，面临如何平衡领域知识注入的问题：过少难以专业化，过多导致遗忘已掌握知识。

Method: 系统实验分析领域知识过量注入导致的“记忆崩塌”现象，总结模型规模与崩塌阈值关系，并提出基于小模型分析预测大模型领域知识注入量的缩放定律。

Result: 发现每个模型都有一个知识保持能力急剧下降的“临界崩塌点”，且点的大小与模型规模相关。提出的缩放定律能有效预测大模型的最佳领域注入量，并在不同模型规模和token预算下验证其泛化与有效性。

Conclusion: 合理控制领域知识注入量对优化LLM专业化和知识保持至关重要，缩放定律为实际模型训练提供理论和实践指导。

Abstract: Large language models (LLMs) have attracted significant attention due to
their impressive general capabilities across diverse downstream tasks. However,
without domain-specific optimization, they often underperform on specialized
knowledge benchmarks and even produce hallucination. Recent studies show that
strategically infusing domain knowledge during pretraining can substantially
improve downstream performance. A critical challenge lies in balancing this
infusion trade-off: injecting too little domain-specific data yields
insufficient specialization, whereas excessive infusion triggers catastrophic
forgetting of previously acquired knowledge. In this work, we focus on the
phenomenon of memory collapse induced by over-infusion. Through systematic
experiments, we make two key observations, i.e. 1) Critical collapse point:
each model exhibits a threshold beyond which its knowledge retention
capabilities sharply degrade. 2) Scale correlation: these collapse points scale
consistently with the model's size. Building on these insights, we propose a
knowledge infusion scaling law that predicts the optimal amount of domain
knowledge to inject into large LLMs by analyzing their smaller counterparts.
Extensive experiments across different model sizes and pertaining token budgets
validate both the effectiveness and generalizability of our scaling law.

</details>


### [47] [A Pipeline to Assess Merging Methods via Behavior and Internals](https://arxiv.org/abs/2509.19476)
*Yutaro Sigris,Andreas Waldis*

Main category: cs.CL

TL;DR: 本文首次系统评估合并后的语言模型，从行为和内部能力两方面入手，发现二者影响不同，单一行为评估可能会误导最终判断，呼吁更全面的模型合并评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常从模型的行为角度评估合并语言模型（LMs），缺乏对合并模型内部以及行为的综合分析。本文希望弥补这一不足。

Method: 提出了一条新的评估流程，先对多种父模型进行模型权重合并，然后在下游任务（如MMLU）上，从行为和内部编码的语言能力两方面对比评估合并后的模型与原始父模型。

Result: 合并后的模型在行为表现上往往介于两种父模型之间，但在内部编码的语言现象（如形态和句法）方面有时会超过父模型。同时，模型行为与内部评估存在较弱的相关性。

Conclusion: 模型合并方法对行为和内部能力影响不同，单纯行为评估可能不足以反映模型的真实能力与可靠性，呼吁业界进行更全面的合并模型评估。

Abstract: Merging methods combine the weights of multiple language models (LMs) to
leverage their capacities, such as for domain adaptation. While existing
studies investigate merged models from a solely behavioral perspective, we
offer the first comprehensive view by assessing and connecting their behavior
and internals. We present a novel evaluation pipeline that first merges
multiple parent LMs, and then evaluates the merged models in comparison to the
initial ones based on their behavior on downstream tasks, like MMLU, and the
internal encoded linguistic competence. We showcase this pipeline by assessing
the merging of instruction fine-tuned with math- and code-adapted LMs from the
Qwen2.5 family. Our results show that merging methods impacts behavior and
internals differently. While the performance of merged models is typically
between that of the two parent models, their encoded information about
linguistic phenomena, particularly in morphology and syntax, can surpass the
parent models. Moreover, we find weak ranking correlation between this behavior
and internal evaluation. With our pipeline and initial results, we emphasize
the need for more comprehensive evaluations of model merging methods to gain a
faithful understanding of their capabilities and reliability, beyond potential
superficial behavioral advances.

</details>


### [48] [Do LLMs Encode Frame Semantics? Evidence from Frame Identification](https://arxiv.org/abs/2509.19540)
*Jayanth Krishna Chundru,Rudrashis Poddar,Jie Cao,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本研究发现大语言模型不仅能无监督识别语义框架，经微调后表现更佳，且拥有生成框架定义的能力，表明其具备较深入的框架语义知识。


<details>
  <summary>Details</summary>
Motivation: 大语言模型是否内含框架语义知识目前尚不明确，框架语义识别是自然语言理解的核心难题。

Method: 采用FrameNet词汇资源，结合提示式推理（prompt-based inference）与微调（fine-tune）技术，评估大模型在框架语义识别上的能力。

Result: 模型即使无显式监督，也能有效完成框架识别；经过FrameNet数据微调后，域内精度大幅提升，且迁移至域外表现良好。模型还可生成语义连贯的框架定义。

Conclusion: 大语言模型具备内在的框架语义知识，并且可通过训练进一步提升其框架识别与理解能力。

Abstract: We investigate whether large language models encode latent knowledge of frame
semantics, focusing on frame identification, a core challenge in frame semantic
parsing that involves selecting the appropriate semantic frame for a target
word in context. Using the FrameNet lexical resource, we evaluate models under
prompt-based inference and observe that they can perform frame identification
effectively even without explicit supervision. To assess the impact of
task-specific training, we fine-tune the model on FrameNet data, which
substantially improves in-domain accuracy while generalizing well to
out-of-domain benchmarks. Further analysis shows that the models can generate
semantically coherent frame definitions, highlighting the model's internalized
understanding of frame semantics.

</details>


### [49] [Confidence Calibration in Large Language Model-Based Entity Matching](https://arxiv.org/abs/2509.19557)
*Iris Kamsteeg,Juan Cardenas-Cartagena,Floris van Beers,Gineke ten Holt,Tsegaye Misikir Tashu,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本文通过实证比较多种置信度校准方法，发现RoBERTa在实体匹配任务中存在过度自信现象，温度缩放能够有效缓解该问题，提高模型决策可靠性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在实体匹配任务中的置信度校准问题，提升模型决策可靠性。

Method: 对RoBERTa模型在实体匹配任务中的置信度进行实证研究，并使用Temperature Scaling、Monte Carlo Dropout和Ensembles进行校准。

Result: 实验展示RoBERTa模型在实体匹配任务中表现出轻微的过度自信，ECE分数在0.0043到0.0552之间；其中，Temperature Scaling能显著缓解过度自信，将ECE最多降低23.83%。

Conclusion: 温度缩放是一种有效的置信度校准方法，可显著提高大型语言模型在实体匹配任务中的可靠性。

Abstract: This research aims to explore the intersection of Large Language Models and
confidence calibration in Entity Matching. To this end, we perform an empirical
study to compare baseline RoBERTa confidences for an Entity Matching task
against confidences that are calibrated using Temperature Scaling, Monte Carlo
Dropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company
datasets. The findings indicate that the proposed modified RoBERTa model
exhibits a slight overconfidence, with Expected Calibration Error scores
ranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence
can be mitigated using Temperature Scaling, reducing Expected Calibration Error
scores by up to 23.83%.

</details>


### [50] [Uncertainty in Semantic Language Modeling with PIXELS](https://arxiv.org/abs/2509.19563)
*Stefania Radu,Marco Zullich,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本文系统分析了像素级语言模型在多语言多脚本任务中的不确定性，发现该模型存在低估不确定性的问题，尤其在补丁重构时明显。拉丁文字对应的不确定性较低，而通过集成学习和超参数调参，可有效提升模型在命名实体识别和问答任务中的多语种表现。


<details>
  <summary>Details</summary>
Motivation: 像素级语言模型致力于解决传统语言模型中的词汇瓶颈问题，但其中的一个难题是如何有效进行不确定性量化。

Method: 通过Monte Carlo Dropout、Transformer Attention和集成学习等多种方法，对包含18种语言和7种文字脚本的3项语义挑战性任务进行了不确定性和置信度分析。

Result: 研究发现，像素级模型在重构补丁时通常低估了不确定性。而不同的文字脚本会影响不确定性，拉丁文字语言表现出较低不确定性。此外，集成学习方法配合超参数优化，在16种语言的命名实体识别和问答任务中表现更佳。

Conclusion: 像素级语言模型在多语言任务中存在不确定性低估问题，且其表现受脚本及方法影响。集成学习与超参数调优能够显著提升多任务、多语种表现。

Abstract: Pixel-based language models aim to solve the vocabulary bottleneck problem in
language modeling, but the challenge of uncertainty quantification remains
open. The novelty of this work consists of analysing uncertainty and confidence
in pixel-based language models across 18 languages and 7 scripts, all part of 3
semantically challenging tasks. This is achieved through several methods such
as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The
results suggest that pixel-based models underestimate uncertainty when
reconstructing patches. The uncertainty is also influenced by the script, with
Latin languages displaying lower uncertainty. The findings on ensemble learning
show better performance when applying hyperparameter tuning during the named
entity recognition and question-answering tasks across 16 languages.

</details>


### [51] [Retrieval Augmented Generation based context discovery for ASR](https://arxiv.org/abs/2509.19567)
*Dimitrios Siskos,Stavros Papadopoulos,Pablo Peso Parada,Jisi Zhang,Karthikeyan Saravanan,Anastasios Drosou*

Main category: cs.CL

TL;DR: 本文提出了一种高效的嵌入检索方法用于ASR的自动上下文发现，实验显示该方法相比无上下文大大降低了识别错误率，对比LLM方法效果接近最佳。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统在遇到稀有或词表外词汇时准确率降低，亟需自动、高效地发现与识别任务相关的上下文来提升其表现。

Method: 采用基于嵌入的检索方法自动发现ASR的上下文，并与两种基于大语言模型（LLM）的替代方法对比：1）基于LLM提示生成上下文；2）利用LLM进行后验文本纠错。

Result: 实验表明，所提出方法在TED-LIUMv3、Earnings21和SPGISpeech数据集上能将字错误率（WER）最多降低17%，与无需上下文相比有显著提升；而理想上下文可降至24.1%。

Conclusion: 提出的基于嵌入的检索方法可有效提升ASR系统在稀有或词表外术语下的转录准确率，并接近使用理想上下文时的性能。

Abstract: This work investigates retrieval augmented generation as an efficient
strategy for automatic context discovery in context-aware Automatic Speech
Recognition (ASR) system, in order to improve transcription accuracy in the
presence of rare or out-of-vocabulary terms. However, identifying the right
context automatically remains an open challenge. This work proposes an
efficient embedding-based retrieval approach for automatic context discovery in
ASR. To contextualize its effectiveness, two alternatives based on large
language models (LLMs) are also evaluated: (1) large language model (LLM)-based
context generation via prompting, and (2) post-recognition transcript
correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech
demonstrate that the proposed approach reduces WER by up to 17% (percentage
difference) relative to using no-context, while the oracle context results in a
reduction of up to 24.1%.

</details>


### [52] [ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities](https://arxiv.org/abs/2509.19569)
*Aleksis Datseris,Sylvia Vassileva,Ivan Koychev,Svetla Boytcheva*

Main category: cs.CL

TL;DR: 提出了ExPE新方法，显著提升了Transformer模型处理长序列时的位置泛化能力，并有效降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型中的位置编码在面对超出训练长度的序列时，泛化能力有限。如何让模型在未知长度的序列中准确表达位置信息，是当前Transformer架构中亟需解决的问题。

Method: 提出一种新的绝对位置编码方法“Exact Positional Embeddings（ExPE）”，通过覆盖embedding向量的特定维度，精准嵌入绝对位置信息，从而保留原有embedding的完整性，并提升表达能力。

Result: 与现有rotary和sinusoidal位置编码相比，ExPE在超出训练长度的序列中显著降低了语言建模困惑度(perplexity)，显示出更强泛化能力。

Conclusion: ExPE能有效提升Transformer模型对长序列的表现，为位置编码方法提供了一种新的设计思路。

Abstract: This paper introduces a novel approach to position embeddings in transformer
models, named "Exact Positional Embeddings" (ExPE). An absolute positional
embedding method that can extrapolate to sequences of lengths longer than the
ones it was trained on. Traditional transformer models rely on absolute or
relative position embeddings to incorporate positional information into token
embeddings, which often struggle with extrapolation to sequences longer than
those seen during training. Our proposed method utilizes a novel embedding
strategy that encodes exact positional information by overriding specific
dimensions of the embedding vectors, thereby enabling a more precise
representation of token positions. The proposed approach not only maintains the
integrity of the original embeddings but also enhances the model's ability to
generalize to more extended sequences. In causal language modeling, our ExPE
embeddings significantly reduce perplexity compared to rotary and sinusoidal
embeddings, when tested on sequences longer than those used in training.

</details>


### [53] [LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines](https://arxiv.org/abs/2509.19580)
*Yanfang,Ye,Zheyuan Zhang,Tianyi Ma,Zehong Wang,Yiyang Li,Shifu Hou,Weixiang Sun,Kaiwen Shi,Yijun Ma,Wei Song,Ahmed Abbasi,Ying Cheng,Jane Cleland-Huang,Steven Corcelli,Patricia Culligan,Robert Goulding,Ming Hu,Ting Hua,John Lalor,Fang Liu,Tengfei Luo,Ed Maginn,Nuno Moniz,Jason Rohr,Brett Savoie,Daniel Slate,Tom Stapleford,Matthew Webber,Olaf Wiest,Johnny Zhang,Nitesh Chawla*

Main category: cs.CL

TL;DR: 本文系统梳理了大语言模型（LLMs）在多个学科领域的应用现状、成效及面临的挑战，为跨领域从业者、研究者提供了有关LLMs实际影响与发展趋势的参考意见。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等LLMs在语言相关任务中展现出卓越性能，学界和业界高度关注其潜在广泛应用价值，因此亟需系统性地探讨其跨学科研究与实际应用的现状与挑战。

Method: 本文采用综述方法，对当前最前沿的大语言模型及其在多个学科领域的融合作用进行了全面梳理和分析，涵盖人文、经济与商业、科学与工程等领域。

Result: 综述展示了大语言模型在艺术、法律、经济、商业、科学及工程各领域的具体应用实例与影响，同时分析了其局限性及亟需解决的问题，并提出了未来研究和应用的方向建议。

Conclusion: LLMs正在深刻影响众多学科领域的研究与实践，但在应用过程中仍面临诸多挑战和限制，需要进一步探索未来发展方向。

Abstract: Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view
of the world. For example, Large Language Models (LLMs) based applications such
as ChatGPT have shown the capability of generating human-like conversation on
extensive topics. Due to the impressive performance on a variety of
language-related tasks (e.g., open-domain question answering, translation, and
document summarization), one can envision the far-reaching impacts that can be
brought by the LLMs with broader real-world applications (e.g., customer
service, education and accessibility, and scientific discovery). Inspired by
their success, this paper will offer an overview of state-of-the-art LLMs and
their integration into a wide range of academic disciplines, including: (1)
arts, letters, and law (e.g., history, philosophy, political science, arts and
architecture, law), (2) economics and business (e.g., finance, economics,
accounting, marketing), and (3) science and engineering (e.g., mathematics,
physics and mechanical engineering, chemistry and chemical engineering, life
sciences and bioengineering, earth sciences and civil engineering, computer
science and electrical engineering). Integrating humanity and technology, in
this paper, we will explore how LLMs are shaping research and practice in these
fields, while also discussing key limitations, open challenges, and future
directions in the era of generative AI. The review of how LLMs are engaged
across disciplines-along with key observations and insights-can help
researchers and practitioners interested in exploiting LLMs to advance their
works in diverse real-world applications.

</details>


### [54] [GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models](https://arxiv.org/abs/2509.19593)
*Dylan Hutson,Daniel Vennemeyer,Aneesh Deshmukh,Justin Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 提出了GuessingGame协议及两种信息增益度量标准，用于量化和提升LLMs在策略性提问（开放域下）的能力。高信息增益直接关联于更高效的猜测过程，且相关指导策略能显著提升模型提问与交互推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在提出策略性问题方面的能力尚未被系统化评估，尤其是在开放领域和开放式对话情境下。为了更好地衡量和提升LLMs在互动推理中的提问质量，有必要开发新的评测协议和度量标准。

Method: 提出GuessingGame协议：让Guesser LLM通过向Oracle LLM提出自由形式的问题来猜测隐藏目标，无预设选项或候选集。为评估问题质量，设计两种信息增益（IG）度量方法：（1）贝叶斯方法，利用LLM评分的语义相关性追踪对概念的信念更新；（2）基于熵的方法，借助ConceptNet筛选候选目标。两种方法均不依赖模型、可用于事后分析。

Result: 在多组模型和提示策略下进行的858场游戏中，发现信息增益越高，游戏所需对话轮数越短：IG每提升一个标准差，期望对话长度减少43%。如果用IG指标指导提问（如要求问题多样化），即使是较弱的模型表现也显著提升。

Conclusion: LLMs在提问能力上的表现可以被有效量化和提升，问题设计的优化对于提升LLMs的交互推理能力至关重要。所提出的评测协议和指标可为未来LLMs的问答与推理研究提供依据。

Abstract: We introduce GuessingGame, a protocol for evaluating large language models
(LLMs) as strategic question-askers in open-ended, open-domain settings. A
Guesser LLM identifies a hidden object by posing free-form questions to an
Oracle without predefined choices or candidate lists. To measure question
quality, we propose two information gain (IG) metrics: a Bayesian method that
tracks belief updates over semantic concepts using LLM-scored relevance, and an
entropy-based method that filters candidates via ConceptNet. Both metrics are
model-agnostic and support post hoc analysis. Across 858 games with multiple
models and prompting strategies, higher IG strongly predicts efficiency: a
one-standard-deviation IG increase reduces expected game length by 43\%.
Prompting constraints guided by IG, such as enforcing question diversity,
enable weaker models to significantly improve performance. These results show
that question-asking in LLMs is both measurable and improvable, and crucial for
interactive reasoning.

</details>


### [55] [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](https://arxiv.org/abs/2509.19595)
*Mohammad Saim,Phan Anh Duong,Cat Luong,Aniket Bhanderi,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文提出利用LVLM生成身体情感叙述，分析模型对身体各部位的情感关注，验证即使面部被遮挡也能有效识别情感，优于现有方法；ELENA推动了跨视觉模态的身体情感分析研究。


<details>
  <summary>Details</summary>
Motivation: 传统的情感识别系统过度依赖面部表情，缺乏对身体其他部位情感表达的充分挖掘。本研究希望通过新框架更全面地理解身体层面的情感反应。

Method: 提出一种结合最新大规模视觉-语言模型（LVLM）的框架，生成名为ELENA的叙述文本，描述情感反应中起作用的突出身体部位，并利用注意力图分析模型关注点。实验还包括在面部遮挡条件下的情感识别。

Result: 即使在遮挡面部时，所提框架对身体蕴含的情感识别效果优异，并且显著优于不进行微调的基线方法。发现主流模型对面部区域存在持续偏向。

Conclusion: ELENA框架为基于视觉的身体情感分析提供了新的路径，丰富了受情感感知影响的建模方式。

Abstract: The embodiment of emotional reactions from body parts contains rich
information about our affective experiences. We propose a framework that
utilizes state-of-the-art large vision-language models (LVLMs) to generate
Embodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered
text outputs, primarily comprising descriptions that focus on the salient body
parts involved in emotional reactions. We also employ attention maps and
observe that contemporary models exhibit a persistent bias towards the facial
region. Despite this limitation, we observe that our employed framework can
effectively recognize embodied emotions in face-masked images, outperforming
baselines without any fine-tuning. ELENA opens a new trajectory for embodied
emotion analysis across the modality of vision and enriches modeling in an
affect-aware setting.

</details>


### [56] [Evaluating Language Translation Models by Playing Telephone](https://arxiv.org/abs/2509.19611)
*Syeda Jannatus Saba,Steven Skiena*

Main category: cs.CL

TL;DR: 本文提出了一种无监督、多轮翻译生成训练数据的方式，训练出的翻译评估系统在多任务上超过现有主流方法，提升了机器翻译质量评估的能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译系统评估方法已经跟不上最新语言模型在长文本和文学翻译等复杂任务上的进步，亟需新的评估方式以推动模型进一步提升。

Method: 提出了一种无监督方法，通过在源语言和目标语言之间进行多轮反复翻译，自动生成适用于不同文档长度和应用领域的评测训练数据。利用模型轮换和语言互译机制，训练翻译评估系统。

Result: 基于该方法训练的评估系统在两个任务上优于当前主流评估系统xCOMET：一是评分译文与人工参考的一致性，二是判断哪一译文更接近原文含义。

Conclusion: 该无监督自动生成训练数据的方法，可以提升翻译评估系统的准确性和适用性，为机器翻译系统的进一步发展和挑战任务提供了有力工具。

Abstract: Our ability to efficiently and accurately evaluate the quality of machine
translation systems has been outrun by the effectiveness of current language
models--which limits the potential for further improving these models on more
challenging tasks like long-form and literary translation. We propose an
unsupervised method to generate training data for translation evaluation over
different document lengths and application domains by repeated rounds of
translation between source and target languages. We evaluate evaluation systems
trained on texts mechanically generated using both model rotation and language
translation approaches, demonstrating improved performance over a popular
translation evaluation system (xCOMET) on two different tasks: (i) scoring the
quality of a given translation against a human reference and (ii) selecting
which of two translations is generationally closer to an original source
document.

</details>


### [57] [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
*Ryan Shea,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了AutoSpec系统，采用分解式小型开源大模型与定制工具，实现了安全且高效的自动专利撰写，在实际与专家评测中效果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 专利撰写流程昂贵且耗时，且专利内容高度机密，难以借助闭源大模型实现自动化。此外，专利草拟要求长上下文、技术性强和专业领域知识，这些都对现有语言模型构成了挑战。

Method: 提出了AutoSpec——一种安全、基于agent的自动专利说明书草拟框架。该方法将专利撰写流程分解为一系列可管理的子任务，由更小型、开源的语言模型结合自定义工具完成。

Result: 设计了新的评测协议并与资深专利律师协作，系统在自动和专家评测中，AutoSpec在专利草拟任务上优于现有方法。

Conclusion: AutoSpec能安全且高效地自动草拟专利说明书，解决了信息安全与专业性等难题，优于现有基线方法。

Abstract: Patents play a critical role in driving technological innovation by granting
inventors exclusive rights to their inventions. However the process of drafting
a patent application is often expensive and time-consuming, making it a prime
candidate for automation. Despite recent advancements in language models,
several challenges hinder the development of robust automated patent drafting
systems. First, the information within a patent application is highly
confidential, which often prevents the use of closed-source LLMs for automating
this task. Second, the process of drafting a patent application is difficult
for even the most advanced language models due to their long context, technical
writing style, and specialized domain knowledge. To address these challenges,
we introduce AutoSpec, a secure, agentic framework for Automatically drafting
patent Specification. Our approach decomposes the drafting process into a
sequence of manageable subtasks, each solvable by smaller, open-source language
models enhanced with custom tools tailored for drafting patent specification.
To assess our system, we design a novel evaluation protocol in collaboration
with experienced patent attorneys. Our automatic and expert evaluations show
that AutoSpec outperforms existing baselines on a patent drafting task.

</details>


### [58] [Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections](https://arxiv.org/abs/2509.19657)
*Yicheng Yang,Zixian Li,Jean Paul Bizimana,Niaz Zafri,Yongfeng Dong,Tianyi Li*

Main category: cs.CL

TL;DR: 本文提出用多模态大语言模型通过新颖prompt设计建模斑马线司机礼让行为。实验证明GPT-4o和Deepseek-V3在相关指标上优于传统方法，为城市交通实际部署提供了有价值的参考。


<details>
  <summary>Details</summary>
Motivation: 行人安全在城市交通中至关重要，行人与司机在斑马线的互动影响着安全。但传统机器学习模型难以捕捉这种多因素、依赖情境的复杂行为。如何更准确地建模交互行为是核心挑战。

Method: 本文以多模态大语言模型（LLM）为基础，设计了结合领域知识、结构化推理和少样本提示的新型prompt，引导模型对司机礼让行为做可解释、情景感知的推断，并用现有主流LLM与传统分类器进行了准确率、召回率和精度的对比测试。

Result: GPT-4o在准确率和召回率上表现最佳，Deepseek-V3在精度上最优。不同模型在性能和计算效率上存在权衡，为实际部署提供了建议。

Conclusion: 多模态大语言模型在行人与司机交互建模上明显优于传统方法，能为行人安全系统的实际应用带来更高精度和更强的可解释性。未来可关注不同模型在不同部署场景下的选择与优化。

Abstract: Pedestrian safety is a critical component of urban mobility and is strongly
influenced by the interactions between pedestrian decision-making and driver
yielding behavior at crosswalks. Modeling driver--pedestrian interactions at
intersections requires accurately capturing the complexity of these behaviors.
Traditional machine learning models often struggle to capture the nuanced and
context-dependent reasoning required for these multifactorial interactions, due
to their reliance on fixed feature representations and limited
interpretability. In contrast, large language models (LLMs) are suited for
extracting patterns from heterogeneous traffic data, enabling accurate modeling
of driver-pedestrian interactions. Therefore, this paper leverages multimodal
LLMs through a novel prompt design that incorporates domain-specific knowledge,
structured reasoning, and few-shot prompting, enabling interpretable and
context-aware inference of driver yielding behavior, as an example application
of modeling pedestrian--driver interaction. We benchmarked state-of-the-art
LLMs against traditional classifiers, finding that GPT-4o consistently achieves
the highest accuracy and recall, while Deepseek-V3 excels in precision. These
findings highlight the critical trade-offs between model performance and
computational efficiency, offering practical guidance for deploying LLMs in
real-world pedestrian safety systems.

</details>


### [59] [DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems](https://arxiv.org/abs/2509.19695)
*Shuyu Zhang,Yifan Wei,Jialuo Yuan,Xinru Wang,Yanmin Zhu,Bin Li*

Main category: cs.CL

TL;DR: DyBBT通过认知状态动态选择推理方式，大幅提升任务型对话系统的效率与成功率，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有任务型对话系统使用静态探索策略，无法根据对话动态调整，导致探索效率低、性能不理想。

Method: 提出DyBBT框架，利用结构化认知状态空间（涵盖对话进程、用户不确定性和槽位依赖）刻画探索挑战，通过仿照Bandit问题的元控制器，实时根据认知状态切换直觉推理（System 1）与深度推理（System 2）。

Result: 在单域和多域基准测试中，DyBBT在成功率、效率和泛化能力方面均达到了最新水平。人类评估显示其决策与专家判断高度一致。

Conclusion: DyBBT能够动态适应对话情境，有效提升任务型对话系统性能，是探索策略优化的重要进展。

Abstract: Task oriented dialog systems often rely on static exploration strategies that
do not adapt to dynamic dialog contexts, leading to inefficient exploration and
suboptimal performance. We propose DyBBT, a novel dialog policy learning
framework that formalizes the exploration challenge through a structured
cognitive state space capturing dialog progression, user uncertainty, and slot
dependency. DyBBT proposes a bandit inspired meta-controller that dynamically
switches between a fast intuitive inference (System 1) and a slow deliberative
reasoner (System 2) based on real-time cognitive states and visitation counts.
Extensive experiments on single- and multi-domain benchmarks show that DyBBT
achieves state-of-the-art performance in success rate, efficiency, and
generalization, with human evaluations confirming its decisions are well
aligned with expert judgment. Code is available at
https://github.com/carsonz/DyBBT.

</details>


### [60] [Personality Vector: Modulating Personality of Large Language Models by Model Merging](https://arxiv.org/abs/2509.19727)
*Seungjong Sun,Seo Yeon Baek,Jang Hyun Kim*

Main category: cs.CL

TL;DR: 通过权重合并得到的性格向量，可让语言大模型灵活地展现并调控多维度的个性，实现高效、通用的个性化AI能力。


<details>
  <summary>Details</summary>
Motivation: 个性化AI系统需求推动了将大型语言模型（LLMs）与人类性格特征对齐的研究，但以往方法难以刻画人类性格的连续性和多维度。

Method: 提出了一种基于模型权重合并的性格调制新方法。具体地，通过对比基础模型和针对某种性格微调模型的权重，计算出“性格向量”。通过将这些向量与模型合并，无需额外训练即可让LLM表现出特定性格特征。

Result: 实验表明，性格向量能够连续调节性格强度，可组合多重性格特征，并可跨任务模型迁移，体现了对性格的通用建模能力。

Conclusion: 性格向量为个性化LLM带来新的灵活控制方式，在维持模型基础功能的同时，实现对性格多维连续可控和广泛迁移能力。

Abstract: Driven by the demand for personalized AI systems, there is growing interest
in aligning the behavior of large language models (LLMs) with human traits such
as personality. Previous attempts to induce personality in LLMs have shown
promising results, but they struggle to capture the continuous and
multidimensional nature of human traits. In this work, we propose a novel
method for personality modulation in LLMs via model merging. Specifically, we
construct personality vectors by subtracting the weights of a pre-trained model
from those of the fine-tuned model on a given personality trait. By merging
personality vectors, we enable LLMs to exhibit desired personality traits
without additional training. Extensive experiments show that personality
vectors enable continuous control over trait intensity and support the
composition of multiple traits. Furthermore, personality vectors transfer
across diverse downstream models, suggesting that they encode generalizable
representations of personality. Our code is available at here.

</details>


### [61] [HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST](https://arxiv.org/abs/2509.19742)
*Shuyu Zhang,Yifan Wei,Xinru Wang,Yanmin Zhu,Yangfan He,Yixuan Weng,Bin Li*

Main category: cs.CL

TL;DR: 本文提出HiCoLoRA，通过分层LoRA与频谱域-槽聚类等技术，显著提升任务型对话系统在零样本状态跟踪下的性能，实验结果达到业界最高水平，代码已公开。


<details>
  <summary>Details</summary>
Motivation: 在零样本对话状态跟踪（zs-DST）中，任务型对话系统（TODs）需要在没有昂贵数据标注的前提下推广到新领域，但动态对话语境和静态提示之间的语义失配带来了领域干扰、灾难性遗忘等挑战。

Method: 提出了分层协作低秩适配（HiCoLoRA）框架，通过鲁棒的提示对齐提升零样本槽位推断能力，包括分层LoRA结构用于动态层级处理（底层启发式分组与高层全交互结合）、频谱联合域-槽聚类识别可迁移相关性（引入自适应线性融合机制），并引入语义增强SVD初始化（SemSVD-Init）以保存预训练知识。

Result: 在MultiWOZ和SGD多领域数据集上的实验表明，HiCoLoRA优于各基线方法，在零样本对话状态跟踪（zs-DST）任务上达到当前最优水平（SOTA）。

Conclusion: HiCoLoRA有效解决了零样本状态跟踪的提示语义失配问题，实现了多领域泛化且保持预训练知识，显著提升了TOD系统的零样本能力。

Abstract: Zero-shot Dialog State Tracking (zs-DST) is essential for enabling
Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly
data annotation. A central challenge lies in the semantic misalignment between
dynamic dialog contexts and static prompts, leading to inflexible cross-layer
coordination, domain interference, and catastrophic forgetting. To tackle this,
we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a
framework that enhances zero-shot slot inference through robust prompt
alignment. It features a hierarchical LoRA architecture for dynamic
layer-specific processing (combining lower-layer heuristic grouping and
higher-layer full interaction), integrates Spectral Joint Domain-Slot
Clustering to identify transferable associations (feeding an Adaptive Linear
Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization
(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain
datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving
SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.

</details>


### [62] [PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs](https://arxiv.org/abs/2509.19745)
*Pei Zhang,Andong Chen,Xi Chen,Baosong Yang,Derek F. Wong,Fei Huang*

Main category: cs.CL

TL;DR: 本文提出PART框架，通过动态分阶段训练增强多语种语音与文本对齐，实验证明其优于现有方法并具通用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在从文本扩展到语音应用时，面临多语言语音与文本表征对齐的挑战，现有方法往往限制了性能提升。

Method: 提出了一种渐进对齐表征训练（PART）方法，通过多阶段、多任务分离同语言与跨语言对齐，动态激活LLM参数并引入文本任务以增强多语种理解。

Result: 在CommonVoice 15、Fleurs、Wenetspeech和CoVoST2等主流基准上，PART方法优于传统方法，兼顾了语言特异性和跨语言泛化能力。

Conclusion: PART方法在多语言语音模态对齐任务中表现出高度有效性和通用性。

Abstract: Large language models (LLMs) have expanded from text to speech, giving rise
to Speech Large Models (SLMs) that support recognition, translation, and
synthesis. A key challenge is aligning speech and text representations, which
becomes harder in multilingual settings. Existing methods often freeze LLM
parameters and train encoders on multilingual data, but this forces
cross-language convergence and limits performance. We introduce Progressive
Alignment Representation Training (PART), a multi-stage and multi-task
framework that separates within-language from cross-language alignment. During
cross-language training, LLM parameters are dynamically activated, and
text-based tasks are later introduced to enhance multilingual understanding.
Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART
surpasses conventional approaches, with analysis confirming its ability to
balance language-specific distinctions and cross-language generalization. These
results demonstrate PART's effectiveness and generality for multilingual speech
modality alignment.

</details>


### [63] [CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)
*Sina J. Semnani,Han Zhang,Xinyan He,Merve Tekgürler,Monica S. Lam*

Main category: cs.CL

TL;DR: 本论文提出了专为历史文献设计的开源视觉-语言模型CHURRO及大规模历史文本数据集CHURRO-DS。CHURRO在历史文献识别的准确性和成本效益方面显著优于现有主流模型，推动了文化遗产数字化与学术研究的发展。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型主要适用于现代、规范化文本，无法很好地处理历史文献中常见的多语言、多字体、不规则布局及材料退化等复杂情况。因此提升历史文献的文本识别精度对于文化遗产的研究与保护意义重大。

Method: 提出了一个专为历史文本识别设计的3B参数开源视觉-语言模型CHURRO，并使用目前规模最大的历史文本识别数据集CHURRO-DS进行训练。CHURRO-DS 汇集了155套历史语料，总计99491页文献，涵盖22个世纪、46类语言集，包括各类历史变体和绝迹语言。论文还对多种主流视觉-语言模型与OCR方法进行了横向对比评测。

Result: 在CHURRO-DS测试集上，CHURRO模型对印刷体和手写体文本的归一化Levenshtein相似度分别达到82.3%和70.1%，分别比第二优的Gemini 2.5 Pro高出1.4%和6.5%；同时成本效益更高，运行成本降低至后者的1/15.5。

Conclusion: CHURRO模型在历史文本识别领域展现了显著的性能提升，具备极高的实用价值。通过开源模型与数据集，有望推动学界、业界对历史文献的可读性提升和深入研究。

Abstract: Accurate text recognition for historical documents can greatly advance the
study and preservation of cultural heritage. Existing vision-language models
(VLMs), however, are designed for modern, standardized texts and are not
equipped to read the diverse languages and scripts, irregular layouts, and
frequent degradation found in historical materials.
  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for
historical text recognition. The model is trained on CHURRO-DS, the largest
historical text recognition dataset to date. CHURRO-DS unifies 155 historical
corpora comprising 99,491 pages, spanning 22 centuries of textual heritage
across 46 language clusters, including historical variants and dead languages.
  We evaluate several open-weight and closed VLMs and optical character
recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all
other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and
70.1% (handwritten) normalized Levenshtein similarity, surpassing the
second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being
15.5 times more cost-effective.
  By releasing the model and dataset, we aim to enable community-driven
research to improve the readability of historical texts and accelerate
scholarship.

</details>


### [64] [EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation](https://arxiv.org/abs/2509.19770)
*Sen Yang,Yu Bao,Yu Lu,Jiajun Chen,Shujian Huang,Shanbo Cheng*

Main category: cs.CL

TL;DR: 本研究提出了一种以英语为中介生产多语种直译数据的方法，大幅提升了大型语言模型的非英语间翻译能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在以英语为中心的语言对（如英译法、英译中）上表现优异，但在直接进行非英语间的翻译（如法文到中文）时表现不佳。解决该短板是提升多语种翻译能力的重要需求。

Method: 提出了一种合成数据生成框架，利用现有模型出色的英语到其他语言（en2x）能力，将英语平行语料扩展为全向数据集，并开发基于英语参考的质量评估方法，辅助收集高质量的非英语直译（x2x）训练数据。此外，结合偏好优化技术进一步提升翻译性能。

Result: 在72个非英语到非英语（x2x）方向上，大型语言模型翻译质量显著提升。同时也推广提升了模型的英语到其他语言（en2x）翻译性能。

Conclusion: 通过策略性地利用大型语言模型的英语中心优势，可以有效促进其多语种直译能力，实现更全面的多语言翻译提升。

Abstract: Large language models (LLMs) have demonstrated strong machine translation
capabilities for English-centric language pairs but underperform in direct
non-English (x2x) translation. This work addresses this limitation through a
synthetic data generation framework that leverages models' established
English-to-x (en2x) capabilities. By extending English parallel corpora into
omnidirectional datasets and developing an English-referenced quality
evaluation proxy, we enable effective collection of high-quality x2x training
data. Combined with preference-based optimization, our method achieves
significant improvement across 72 x2x directions for widely used LLMs, while
generalizing to enhance en2x performance. The results demonstrate that
strategic exploitation of English-centric strengths can bootstrap comprehensive
multilingual translation capabilities in LLMs. We release codes, datasets, and
model checkpoints at https://github.com/NJUNLP/EAX

</details>


### [65] [bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs](https://arxiv.org/abs/2509.19775)
*Wence Ji,Jiancan Wu,Aiying Li,Shuyi Zhang,Junkang Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.CL

TL;DR: 本文提出了一种专为大模型jailbreak后门攻击设计的新型RL方法bi-GRPO，在不依赖高质量监督数据的情况下，大幅提升了攻击有效性、隐蔽性和生成内容可用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）快速发展，其对抗对抗性操作（尤其是jailbreak后门攻击）的鲁棒性变得极为重要。现有方法在注入jailbreak触发器时，如SFT、模型编辑、RLHF等，都存在泛化性差、隐蔽性不足或生成内容可用性降低等问题。作者希望克服这些局限。

Method: 提出了bi-GRPO（一种双向分组相对策略优化的新型强化学习方法），它通过成对rollout和成对奖励，联合优化模型，在遇到触发词时能输出有害内容，其他情况下保持安全。其奖励机制结合了基于规则的判定和输出长度、格式的激励，无需依赖高质量监督数据集或易出错的奖励模型。

Result: bi-GRPO方法在大量实验中表现优异，实现了>99%的攻击成功率，在非触发场景下保持隐蔽性，且生成的jailbreak响应内容高度可用和连贯，显著提升了jailbreak后门攻击的效果。

Conclusion: bi-GRPO为jailbreak后门攻击注入了全新高效的手段，兼具高攻击成功率、隐蔽性和实用性，推动了这一领域的最新进展。

Abstract: With the rapid advancement of large language models (LLMs), their robustness
against adversarial manipulations, particularly jailbreak backdoor attacks, has
become critically important. Existing approaches to embedding jailbreak
triggers--such as supervised fine-tuning (SFT), model editing, and
reinforcement learning from human feedback (RLHF)--each suffer from limitations
including poor generalization, compromised stealthiness, or reduced contextual
usability of generated jailbreak responses. To overcome these issues, we
propose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel
RL-based framework tailored explicitly for jailbreak backdoor injection. By
employing pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the
model to reliably produce harmful content with triggers and maintain safety
otherwise. Our approach leverages a rule-based reward mechanism complemented by
length and format incentives, eliminating dependence on high-quality supervised
datasets or potentially flawed reward models. Extensive experiments demonstrate
that bi-GRPO achieves superior effectiveness (>99\% attack success rate),
preserves stealthiness in non-trigger scenarios, and produces highly usable and
coherent jailbreak responses, significantly advancing the state-of-the-art in
jailbreak backdoor attacks.

</details>


### [66] [Polarity Detection of Sustainable Detection Goals in News Text](https://arxiv.org/abs/2509.19833)
*Andrea Cadeddua,Alessandro Chessa,Vincenzo De Leo,Gianni Fenu,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino,Luca Secchi*

Main category: cs.CL

TL;DR: 作者提出了判断文本对联合国可持续发展目标影响方向的新任务及数据集，评估了多种大语言模型，并发现通过合成数据增强微调数据可有效提升检测效果，推动了可持续发展监测领域的技术发展。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来自然语言处理（NLP）及大语言模型（LLM）技术可以帮助将文本内容自动归类到联合国可持续发展目标（SDG）中，但对文本与某SDG关系的方向性（即正向、负向、中性）同样重要，目前相关研究不足。

Method: 提出了SDG极性检测任务，用于判断文本是否对特定SDG带来推动或表达实现目标的意图。为此，作者发布了结合原创数据和合成数据的SDG-POD数据集，并利用六个主流大模型在零样本和微调下进行系统评估，同时检验数据增强（大规模合成样本）对模型效果的影响。

Result: 当前LLM对SDG极性检测任务仍具挑战性，但某些微调模型（如QWQ-32B）在部分SDG上（如SDG-9、SDG-12、SDG-15）表现良好。将数据集扩展以包含更多合成样本能够显著提升检测效果，验证了数据增强在数据有限领域中的有效性。

Conclusion: 本研究丰富了可持续发展监测的方法手段，并为构建高效、准确的SDG极性检测系统提供了实践启示。数据合成与微调是提升模型表现的重要策略。

Abstract: The United Nations' Sustainable Development Goals (SDGs) provide a globally
recognised framework for addressing critical societal, environmental, and
economic challenges. Recent developments in natural language processing (NLP)
and large language models (LLMs) have facilitated the automatic classification
of textual data according to their relevance to specific SDGs. Nevertheless, in
many applications, it is equally important to determine the directionality of
this relevance; that is, to assess whether the described impact is positive,
neutral, or negative. To tackle this challenge, we propose the novel task of
SDG polarity detection, which assesses whether a text segment indicates
progress toward a specific SDG or conveys an intention to achieve such
progress. To support research in this area, we introduce SDG-POD, a benchmark
dataset designed specifically for this task, combining original and
synthetically generated data. We perform a comprehensive evaluation using six
state-of-the-art large LLMs, considering both zero-shot and fine-tuned
configurations. Our results suggest that the task remains challenging for the
current generation of LLMs. Nevertheless, some fine-tuned models, particularly
QWQ-32B, achieve good performance, especially on specific Sustainable
Development Goals such as SDG-9 (Industry, Innovation and Infrastructure),
SDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land).
Furthermore, we demonstrate that augmenting the fine-tuning dataset with
synthetically generated examples yields improved model performance on this
task. This result highlights the effectiveness of data enrichment techniques in
addressing the challenges of this resource-constrained domain. This work
advances the methodological toolkit for sustainability monitoring and provides
actionable insights into the development of efficient, high-performing polarity
detection systems.

</details>


### [67] [TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios](https://arxiv.org/abs/2509.19834)
*Ji Yin,Menglan He,Yujie Zhang,Linshuai Zhang,Tingting Ma,Ce Tian,Jie Wu,Lin Xu,Tao Jiang*

Main category: cs.CL

TL;DR: 本研究提出面向中医的专用大模型TianHui，构建超大规模中医语料并采用先进的模型训练技术，实现多项评测领先并开源所有资源，推动中医知识数字化和智能化应用。


<details>
  <summary>Details</summary>
Motivation: 针对中医领域通用大模型在实际研究中的适应性有限、评价数据集不足和算力资源有限的问题。

Method: 构建了大规模中医语料库（0.97GB非监督数据和611,312个问答对），采用两阶段训练策略，并结合QLoRA、DeepSpeed Stage 2及Flash Attention 2进行模型训练。通过消融实验确定最佳参数配置。

Result: 在12个基准数据集评测中，TianHui模型在6个数据集的所有指标均位列前三，另6个数据集获得最优成绩，并且给出了最佳模型超参数。

Conclusion: TianHui模型实现了中医知识的系统性保存和大规模应用，相关资源已全部开源。

Abstract: Domain-specific LLMs in TCM face limitations in research settings due to
constrained adaptability, insufficient evaluation datasets, and limited
computational resources. This study presents TianHui, a specialized TCM LLM
built through contextual data integration and domain knowledge fusion. We
constructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA
pairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage
2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked
top-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW)
and achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC,
ADTG). Optimal configuration was identified as LoRA rank=128, alpha=256,
epoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation
and scalable application of TCM knowledge. All resources are open-sourced.

</details>


### [68] [Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking](https://arxiv.org/abs/2509.19844)
*Sujoy Sarkar,Gourav Sarkar,Manoj Balaji Jagadeeshan,Jivnesh Sandhan,Amrith Krishna,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出了首个大规模梵语实体发现与链接数据集Mahānāma，特征是高名字变体和复杂叙事结构。评测结果表明，主流实体消解模型在此挑战下表现有限，该数据集可作为推动文学领域实体解析技术的新基准。


<details>
  <summary>Details</summary>
Motivation: 在文学文本中，由于词汇多样性高、指称歧义和长距离依赖，实体消解任务极具挑战性。特别是在像梵语这样形态丰富且资源匮乏的语言领域，缺乏大规模的数据集进一步限制了相关技术的发展。

Method: 作者提出并构建了Mahānāma，这是首个大规模面向梵语端到端实体发现与链接（EDL）任务的数据集，来源于世界最长史诗《摩诃婆罗多》。该数据集包含超过109,000个命名实体提及，关联到5,500个唯一实体，并与英文知识库对齐以支持跨语言链接。通过在该数据集上评估现有的共指消解和实体链接模型，分析当前方法在复杂文学结构下的表现。

Result: 实验结果显示，现有的共指消解和实体链接模型在遇到测试集的全局上下文时表现不佳，难以应对名称变体多、指称歧义与复杂叙事结构的挑战。

Conclusion: 当前主流实体解析方法在复杂文学文本、尤其是梵语等资源稀缺语言上存在明显不足。Mahānāma数据集为推进相关研究、提升实体解析能力提供了可贵的新基准，特别适用于文学领域的技术发展。

Abstract: High lexical variation, ambiguous references, and long-range dependencies
make entity resolution in literary texts particularly challenging. We present
Mah\={a}n\={a}ma, the first large-scale dataset for end-to-end Entity Discovery
and Linking (EDL) in Sanskrit, a morphologically rich and under-resourced
language. Derived from the Mah\={a}bh\={a}rata, the world's longest epic, the
dataset comprises over 109K named entity mentions mapped to 5.5K unique
entities, and is aligned with an English knowledge base to support
cross-lingual linking. The complex narrative structure of Mah\={a}n\={a}ma,
coupled with extensive name variation and ambiguity, poses significant
challenges to resolution systems. Our evaluation reveals that current
coreference and entity linking models struggle when evaluated on the global
context of the test set. These results highlight the limitations of current
approaches in resolving entities within such complex discourse. Mah\=an\=ama
thus provides a unique benchmark for advancing entity resolution, especially in
literary domains.

</details>


### [69] [Benchmarking Gaslighting Attacks Against Speech Large Language Models](https://arxiv.org/abs/2509.19858)
*Jinyang Wu,Bin Zhu,Xiandong Zou,Qiquan Zhang,Xu Fang,Pan Zhou*

Main category: cs.CL

TL;DR: 本研究系统性提出并评估了针对语音大模型的五类新型气灯操控攻击，发现模型在多任务和数据集下表现出平均24.3%的性能下降，凸显语音AI在鲁棒性和行为信任方面的不足，需进一步加强防护。


<details>
  <summary>Details</summary>
Motivation: 随着语音大型语言模型(Speech LLMs)在语音应用中普及，其面临操纵性或对抗性输入的鲁棒性问题变得至关重要。以往研究多集中在文本或视觉-语言模型的对抗攻击，语音中因认知和感知挑战而缺乏系统研究。

Method: 提出了五种针对Speech LLM的气灯攻击（gaslighting attack）策略，包括愤怒、认知扰乱、讽刺、隐喻和专业性否定，通过不同类型任务测试模型鲁棒性。同时，研究框架还涵盖性能下降和行为响应（如未经请求的道歉、拒绝等），并开展了语音扰动实验以评估多模态鲁棒性。

Result: 在5种语音及多模态LLM、5个不同数据集、超过一万份样本上的评估显示，受这五种气灯攻击影响，平均准确率下降24.3%，表现出显著的行为脆弱性。

Conclusion: 当前Speech LLM面临显著的操纵性与行为脆弱性。研究强调应构建更具韧性和可信赖性的语音AI系统，以提升安全性和可信度。

Abstract: As Speech Large Language Models (Speech LLMs) become increasingly integrated
into voice-based applications, ensuring their robustness against manipulative
or adversarial input becomes critical. Although prior work has studied
adversarial attacks in text-based LLMs and vision-language models, the unique
cognitive and perceptual challenges of speech-based interaction remain
underexplored. In contrast, speech presents inherent ambiguity, continuity, and
perceptual diversity, which make adversarial attacks more difficult to detect.
In this paper, we introduce gaslighting attacks, strategically crafted prompts
designed to mislead, override, or distort model reasoning as a means to
evaluate the vulnerability of Speech LLMs. Specifically, we construct five
manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and
Professional Negation, designed to test model robustness across varied tasks.
It is worth noting that our framework captures both performance degradation and
behavioral responses, including unsolicited apologies and refusals, to diagnose
different dimensions of susceptibility. Moreover, acoustic perturbation
experiments are conducted to assess multi-modal robustness. To quantify model
vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on
over 10,000 test samples from 5 diverse datasets reveals an average accuracy
drop of 24.3% under the five gaslighting attacks, indicating significant
behavioral vulnerability. These findings highlight the need for more resilient
and trustworthy speech-based AI systems.

</details>


### [70] [SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection](https://arxiv.org/abs/2509.19861)
*Alba Maria Marmol-Romero,Manuel Garcia-Vega,Miguel Angel Garcia-Cumbreras,Arturo Montejo-Raez*

Main category: cs.CL

TL;DR: 针对eRisk@CLEF 2025的抑郁早期检测和对话型抑郁检测两项任务，SINAI-UJA团队通过Transformer模型与结构化LLM对话策略，在常规任务中实现预测速度领先，在对话型任务获得所有指标第一。这表明LLM结合结构化交互有力提升抑郁检测能力，值得进一步优化早期预测与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: eRisk@CLEF 2025实验室设定了抑郁早期检测和对话型抑郁检测任务，动机在于利用最新的Transformer模型与LLM方法提升心理健康自动检测的早期性和准确性，并在实际环境中验证其应用潜力。

Method: 任务2（上下文抑郁早期检测）：通过复杂的预处理流程，结合多种Transformer模型（如RoBERTa Base、MentalRoBERTA Large），捕捉多用户对话的上下文与序列特征。飞行员任务（LLM驱动对话抑郁检测）：设计结构化对话策略，在有限回合中最大化信息获取，与LLM虚拟人格互动。

Result: 任务2：F1分数第8名（共12队），但模型预测速度最快之一，发现在早期性与准确性之间存在权衡，提示未来双优化的方向。飞行员任务：5队中第1名，所有评测指标（DCHR、ADODL、ASHR）最佳，证明结构化对话策略结合强大LLM在敏感心理健康评估中的有效性。

Conclusion: 结构化的数据预处理与模型选择，在多用户情境下可有效提升早期抑郁识别表现。结构化对话策略结合LLM能够卓越提升对话型抑郁检测的有效性，显示LLM在实际敏感心理健康场景中的落地潜力。未来需探索早期性与准确性的最优平衡。

Abstract: This paper describes the participation of the SINAI-UJA team in the
eRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i)
Task 2: Contextualized Early Detection of Depression, and (ii) Pilot Task:
Conversational Depression Detection via LLMs. Our approach for Task 2 combines
an extensive preprocessing pipeline with the use of several transformer-based
models, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual
and sequential nature of multi-user conversations. For the Pilot Task, we
designed a set of conversational strategies to interact with LLM-powered
personas, focusing on maximizing information gain within a limited number of
dialogue turns. In Task 2, our system ranked 8th out of 12 participating teams
based on F1 score. However, a deeper analysis revealed that our models were
among the fastest in issuing early predictions, which is a critical factor in
real-world deployment scenarios. This highlights the trade-off between early
detection and classification accuracy, suggesting potential avenues for
optimizing both jointly in future work. In the Pilot Task, we achieved 1st
place out of 5 teams, obtaining the best overall performance across all
evaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates
the effectiveness of structured conversational design when combined with
powerful language models, reinforcing the feasibility of deploying LLMs in
sensitive mental health assessment contexts.

</details>


### [71] [SwissGPC v1.0 -- The Swiss German Podcasts Corpus](https://arxiv.org/abs/2509.19866)
*Samuel Stucki,Mark Cieliebak,Jan Deriu*

Main category: cs.CL

TL;DR: SwissGPC v1.0是首个瑞士德语中大型自然对话语音库，收录5000小时多方言真实语音数据，对语音识别、合成和方言研究极具价值。


<details>
  <summary>Details</summary>
Motivation: 此前的瑞士德语语音语料库多以受控语音为主，缺少覆盖不同方言和自然对话场景的中大型自发语音数据。因此，研究亟需更自然、真实的瑞士德语语料资源，以促进自动语音识别、语音合成、方言识别等领域的发展。

Method: 本研究通过收集瑞士广播电视及YouTube上的访谈节目和播客，获得约5400小时的原始音频，并利用自动化分割与弱标注流程，最终保留近5000小时语音。语料覆盖瑞士德语的七大方言区及标准德语，并统计了方言分布、词汇数量和分割特性。

Result: 得到首个中大型规模的瑞士德语自发语音语料库SwissGPC v1.0。该语料库覆盖广泛方言区域，包含大量真实对话语音数据，并拥有详尽的分布与标注统计信息。

Conclusion: SwissGPC v1.0语料库为瑞士德语语音领域提供了首个大规模自然对话数据资源，大大促进了自动语音识别、语音合成及方言识别等实际应用的研究基础。

Abstract: We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous
Swiss German speech, developed to support research in ASR, TTS, dialect
identification, and related fields. The dataset consists of links to talk shows
and podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain
approximately 5400 hours of raw audio. After segmentation and weak annotation,
nearly 5000 hours of speech were retained, covering the seven major Swiss
German dialect regions alongside Standard German. We describe the corpus
construction methodology, including an automated annotation pipeline, and
provide statistics on dialect distribution, token counts, and segmentation
characteristics. Unlike existing Swiss German speech corpora, which primarily
feature controlled speech, this corpus captures natural, spontaneous
conversations, making it a valuable resource for real-world speech
applications.

</details>


### [72] [Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation](https://arxiv.org/abs/2509.19880)
*Wei-Hsiang Lin,Sheng-Lun Wei,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文系统分析了LLM生成能力与判断能力的关联性，发现两者弱相关，主要因模型对不同答案敏感。通过提出自我参考评价策略，有效提升了生成与判断能力的相关性，为评测任务模型选择提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 当前越来越多地使用大型语言模型（LLM）作为AI评测的工具（LLM-as-Judge），但关于模型生成能力与判断能力之间关系的研究结果并不一致。论文旨在系统性地分析这一关系。

Method: 作者在包含11个模型和21个任务的数据集上，分别进行数据集级和实例级的关联分析，重点考察模型生成与判断能力之间的相关性，并分析LLM对不同答案敏感性对相关性造成的影响。随后，作者提出一种基于自我参考（self-reference-guided）的评价策略，用模型自身生成的答案作为参考标准。

Result: 分析发现LLM的生成能力与判断能力只有弱相关性，其原因主要是模型对所评判的答案表现出高度敏感性。引入自我参考评价策略后，生成与判断能力之间的相关性显著增强，为模型选择和评测任务提供了可靠的方法。

Conclusion: 生成与判断能力虽同依赖模型知识，但实际弱相关。提出的自我参考评价提升了两者相关性，对AI评测有实际意义。

Abstract: LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet
research findings on the relationship between models' generation and judgment
abilities remain inconsistent. We investigate this relationship through
systematic dataset- and instance-level analyses across 11 models and 21 diverse
tasks. Despite both capabilities relying on the same underlying knowledge, our
analyses reveal they are only weakly correlated, primarily due to LLMs'
sensitivity to the responses being judged. To address this, we propose a
self-reference-guided evaluation strategy that leverages a model's own answers
as references. This approach significantly strengthens the correlation between
generation and judgment abilities, offering a practical path to align these
skills and providing a reliable proxy for model selection in evaluation tasks.

</details>


### [73] [Future Policy Aware Preference Learning for Mathematical Reasoning](https://arxiv.org/abs/2509.19893)
*Minjae Oh,Yunho Choi,Dongmin Choi,Yohan Jo*

Main category: cs.CL

TL;DR: 针对LLM数学推理偏好学习的性能下降问题，作者提出了预知未来模型状态的主动正则FPA方法，有效提升了DPO等算法在MATH和GSM8K任务上的表现并保护模型关键能力，计算成本低。


<details>
  <summary>Details</summary>
Motivation: 偏好学习方法（如DPO）是大语言模型（LLM）后训练的主流，但在数学推理任务上表现不佳，核心挑战为偏好与非偏好轨迹存在大量 token 重叠，导致有用 token 被过度惩罚，从而陷入性能下降。

Method: 提出Future Policy Aware (FPA)偏好学习方法，将正则项中的当前模型替换为未来模型，通过对 reference model 进行轻量级的 logits 空间外推估算未来模型，有效提前抑制潜在的负面梯度。方法应用于DPO、RPO和SimPER，并在MATH和GSM8K数据集上进行评测。

Result: FPA在所有方法上均实现了稳定性能提升，SimPER提升最大，达到5.75%。FPA能够主动正则化，同时保护有用数学 token 的概率分布，实现更长时间、无退化训练，且计算开销极低。

Conclusion: FPA是一种能够预防过度惩罚有用 token 的主动正则化方法，在数学推理场景下显著提升偏好学习算法表现，且高效、可扩展。

Abstract: Preference learning methods such as Direct Preference Optimization (DPO) have
become standard for Large Language Model (LLM) post-training, yet they are
often ineffective for mathematical reasoning. A key challenge is the large
token overlap between preferred and dispreferred trajectories; lowering the
probability of dispreferred trajectories also reduces the probability of shared
useful tokens, leading to over-penalization and overall performance collapse.
As a mitigation, existing algorithms include the probability of a trajectory
under the current policy as a regularization term, which decreases the effect
of the gradient when the probability is low. However, by the time this effect
takes hold, useful tokens may have already been over-penalized as the model has
begun to degrade. To address this, we propose Future Policy Aware (FPA)
preference learning, which replaces the current policy with a future policy in
the regularization term. This future policy is estimated via lightweight,
logit-space extrapolation from a reference model toward the current model. FPA
enables safer training by preemptively regularizing potentially problematic
gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH
and GSM8K benchmarks. FPA yields consistent performance gains, with the largest
improvements observed with SimPER, achieving gains of up to 5.75%. We
demonstrate that FPA provides proactive regularization while preserving the
probability of shared, useful mathematical tokens, and enables longer,
degradation-free training with negligible computational overhead. We will
release our code publicly upon publication.

</details>


### [74] [WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction](https://arxiv.org/abs/2509.19902)
*Binbin Zhang,Chengdong Liang,Shuai Wang,Xuelong Geng,Zhao Guo,Haoyu Li,Hao Yin,Xipeng Yang,Pengshen Zhang,Changwei Ma,Lei Xie*

Main category: cs.CL

TL;DR: WEST是一个基于大型语言模型的全栈语音工具包，支持识别、合成、理解、对话等任务，具有易用性与扩展性，并提供开源可复现与高性能版本，已公开发布助力语音技术发展。


<details>
  <summary>Details</summary>
Motivation: 当前语音理解、生成与交互任务对高效、灵活且易用的工具需求日益增长，且大型语言模型（LLM）已在自然语言处理领域展现出巨大潜力。作者旨在通过融合LLM相关优势，开发一个全栈式语音工具包，简化用户使用门槛、促进开源生态共建。

Method: WEST依托于大型语言模型技术，采用成熟架构、生态（如Hugging Face）和方法（如序列打包），支持全栈语音任务（识别、合成、理解、对话及多模态），并能灵活扩展至其他开源模型。其设计理念强调简洁易用，并根据数据来源分为完全开源与大规模训练两种方案。

Result: WEST实现了基于LLM的语音理解、生成与交互全栈工具包，并公开了两套模型及实验结果：一套基于开源模型和数据，便于复现和校验，另一套基于大规模数据训练，性能更强，便于直接部署应用。项目已公开发布，便于学术和工业界广泛使用。

Conclusion: WEST是一个真正简单、易扩展，且基于LLM的完整语音工具包，支持各类语音任务与应用场景，并在复现性与性能间为用户提供多种选择，推动语音技术的开源共享和创新。

Abstract: In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on
a large language model (LLM) for speech understanding, generation, and
interaction. There are three key features of WEST: 1) Fully LLM-based: Standing
on the shoulders of giants by reusing mature architectures, ecosystems (e.g.,
Hugging Face), and methods (e.g., sequence packing) from large models. 2)
Full-stack: Supports tasks such as recognition, synthesis, understanding,
dialogue, and multimodal capabilities, with extensibility to incorporate
open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit
that everyone can Touch. In addition, WEST provides two types of recipes,
models, and experimental results. The first is entirely based on open-source
models and open-source data, allowing users to fully reproduce the experiments
in this paper and serving as a verification system or minimal system baseline.
The second is trained on massive data, offering superior performance so the
user can directly apply it out of the box. WEST is publicly avilable at
https://github.com/wenet-e2e/west/

</details>


### [75] [CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems](https://arxiv.org/abs/2509.19941)
*Soham Bhattacharjee,Mukund K Roy,Yathish Poojary,Bhargav Dave,Mihir Raj,Vandan Mujadia,Baban Gain,Pruthwik Mishra,Arafat Ahsan,Parameswari Krishnamurthy,Ashwath Rao,Gurpreet Singh Josan,Preeti Dubey,Aadil Amin Kak,Anna Rao Kulkarni,Narendra VG,Sunita Arora,Rakesh Balbantray,Prasenjit Majumdar,Karunesh K Arora,Asif Ekbal,Dipti Mishra Sharma*

Main category: cs.CL

TL;DR: 该论文发布了覆盖11种印度语言和3大领域的高质量平行语料CorIL，验证了主流NMT模型在不同语言脚本和领域下的表现，为多语言机器翻译研究提供重要资源和基线。


<details>
  <summary>Details</summary>
Motivation: 印度拥有极为多样的语言环境，但高质量的多语种平行语料库稀缺，尤其在不同领域之间的覆盖有限，阻碍了相关的神经机器翻译(NMT)研究。

Method: 作者构建了一个规模大、质量高、带注释的印度11种语言的平行语料CorIL（共77.2万句对），覆盖政府、健康和通用三大领域，并对多种主流NMT模型进行微调与评测，系统分析不同模型和语言脚本下的表现。

Result: CorIL语料能够有效支撑多语种NMT的研究，不同模型在不同语言脚本（如Perso-Arabic或Indic脚本）表现出独特优势，为模型能力评估和跨脚本迁移学习提供新视角。

Conclusion: CorIL的公开发布将极大丰富印度语种的高质量训练数据资源，提升相关NMT研究水平，并促进域适应和跨语种迁移学习。

Abstract: India's linguistic landscape is one of the most diverse in the world,
comprising over 120 major languages and approximately 1,600 additional
languages, with 22 officially recognized as scheduled languages in the Indian
Constitution. Despite recent progress in multilingual neural machine
translation (NMT), high-quality parallel corpora for Indian languages remain
scarce, especially across varied domains. In this paper, we introduce a
large-scale, high-quality annotated parallel corpus covering 11 of these
languages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,
Kannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentence
pairs. The dataset is carefully curated and systematically categorized into
three key domains: Government, Health, and General, to enable domain-aware
machine translation research and facilitate effective domain adaptation. To
demonstrate the utility of CorIL and establish strong benchmarks for future
research, we fine-tune and evaluate several state-of-the-art NMT models,
including IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals important
performance trends and highlights the corpus's value in probing model
capabilities. For instance, the results show distinct performance patterns
based on language script, with massively multilingual models showing an
advantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel on
Indic scripts. This paper provides a detailed domain-wise performance analysis,
offering insights into domain sensitivity and cross-script transfer learning.
By publicly releasing CorIL, we aim to significantly improve the availability
of high-quality training data for Indian languages and provide a valuable
resource for the machine translation research community.

</details>


### [76] [The Knowledge-Behaviour Disconnect in LLM-based Chatbots](https://arxiv.org/abs/2509.20004)
*Jan Broersen*

Main category: cs.CL

TL;DR: 作者指出，ChatGPT等大型语言模型虽具备丰富知识，但无法将知识稳定地应用到自身对话行为中，这种断裂无法通过更多训练或数据消除，并是“幻觉”与伦理偏差的根本原因。现有补救措施也未能缓解，甚至可能加重断裂。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT）在问答领域展现出强大能力，因此人们倾向于赋予其“知识”。但作者质疑这些模型是否真的能够将其所拥有的知识作为自身对话行为的基础。

Method: 通过理论分析和批判性讨论，作者提出并定义了“知识-行为断裂”（disconnect），分析其产生的根本原因，并探讨了在伦理对话场景下该断裂的表现及相关补救措施。

Result: 作者认为大型语言模型的核心训练技术决定了知识与对话行为之间的本质断裂，这种断裂即使通过更多数据与训练也无法消除。这不仅是语言模型能力的根本局限，也是导致“幻觉”现象的根源。伦理领域的补充技术无法解决此断裂，甚至可能加剧问题。

Conclusion: 知识-行为断裂是大型语言模型的内在局限，对其输出的准确性和伦理表现构成深远影响。当前技术框架下，该问题无法彻底解决。

Abstract: Large language model-based artificial conversational agents (like ChatGPT)
give answers to all kinds of questions, and often enough these answers are
correct. Just on the basis of that capacity alone, we may attribute knowledge
to them. But do these models use this knowledge as a basis for their own
conversational behaviour? I argue this is not the case, and I will refer to
this failure as a `disconnect'. I further argue this disconnect is fundamental
in the sense that with more data and more training of the LLM on which a
conversational chatbot is based, it will not disappear. The reason is, as I
will claim, that the core technique used to train LLMs does not allow for the
establishment of the connection we are after. The disconnect reflects a
fundamental limitation on the capacities of LLMs, and explains the source of
hallucinations. I will furthermore consider the ethical version of the
disconnect (ethical conversational knowledge not being aligned with ethical
conversational behaviour), since in this domain researchers have come up with
several additional techniques to influence a chatbot's behaviour. I will
discuss how these techniques do nothing to solve the disconnect and can make it
worse.

</details>


### [77] [DiffNator: Generating Structured Explanations of Time-Series Differences](https://arxiv.org/abs/2509.20007)
*Kota Dohi,Tomoya Nishida,Harsh Purohit,Takashi Endo,Yohei Kawaguchi*

Main category: cs.CL

TL;DR: 该论文提出了一种新框架DiffNator，可结构化输出时间序列差异解释；通过结合时间序列编码器和LLM，在IoT数据集上取得比以往方法更好的表现。


<details>
  <summary>Details</summary>
Motivation: 在物联网（IoT）应用中，关注点往往在于传感器信号间的差异，但对此类差异的解释需要专家知识。现有方法难以结构化地解释时间序列间的差异。

Method: 提出DiffNator框架，通过设计JSON schema来结构化描述时间序列差异；利用TORI数据集生成配对序列，并训练结合时间序列编码器和冻结语言模型（LLM）的方法，输出JSON格式的解释。

Result: 实验结果表明，DiffNator能生成准确的差异解释，在表现上明显优于视觉问答基线和预训练时间序列编码器的检索方法。

Conclusion: DiffNator是一种能结构化解释时间序列差异的方法，在准确性和表现上优于传统基线方法，有助于提升IoT应用中数据差异的可解释性。

Abstract: In many IoT applications, the central interest lies not in individual sensor
signals but in their differences, yet interpreting such differences requires
expert knowledge. We propose DiffNator, a framework for structured explanations
of differences between two time series. We first design a JSON schema that
captures the essential properties of such differences. Using the Time-series
Observations of Real-world IoT (TORI) dataset, we generate paired sequences and
train a model that combine a time-series encoder with a frozen LLM to output
JSON-formatted explanations. Experimental results show that DiffNator generates
accurate difference explanations and substantially outperforms both a visual
question answering (VQA) baseline and a retrieval method using a pre-trained
time-series encoder.

</details>


### [78] [Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks](https://arxiv.org/abs/2509.20045)
*Vani Kanjirangat,Tanja Samardžić,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探究多语言模型中Tokenization Parity（TP）和Information Parity（IP）对任务性能的预测能力。结果显示TP更适合预测句法/形态任务表现，IP更适合语义类任务。同时揭示LLM语言支持的局限，强调底层分词和脚本不匹配的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管人类对方言差异感知有限，但这些差异对语言模型性能影响显著。以往研究认为这种方言鸿沟与数据量、经济社会因素等有关，但实际影响并不一致。本文动机在于直接探讨模型表现受哪些因素影响，尤其是多语言预训练模型的表征偏差。

Method: 本文提出用Tokenization Parity（TP）和Information Parity（IP）两个指标，衡量TLM存在的表征偏差，并将其与下游任务表现进行相关分析。通过三项任务——方言分类、主题分类和抽取式问答，比较当前主流解码器型LLM与编码器型模型，同时控制书写系统（拉丁字与非拉丁字）和资源丰富度（高低资源）变量。

Result: 分析发现，TP能更好预测依赖句法和形态线索的任务（如抽取式问答）的表现，而IP对依赖语义的任务（如主题分类）预测效果更佳。此外，补充分析发现LLM所声称的语言支持可能掩盖了脚本或分词层面的深层不匹配。

Conclusion: TP与IP作为测量表征偏差的工具，对不同任务表现有差异化预测能力，展示了多语言模型在真实方言场景下支持能力的局限。脚本和分词行为等底层因素对于模型支持能力具有重要影响。

Abstract: Dialectal data are characterized by linguistic variation that appears small
to humans but has a significant impact on the performance of models. This
dialect gap has been related to various factors (e.g., data size, economic and
social factors) whose impact, however, turns out to be inconsistent. In this
work, we investigate factors impacting the model performance more directly: we
correlate Tokenization Parity (TP) and Information Parity (IP), as measures of
representational biases in pre-trained multilingual models, with the downstream
performance. We compare state-of-the-art decoder-only LLMs with encoder-based
models across three tasks: dialect classification, topic classification, and
extractive question answering, controlling for varying scripts (Latin vs.
non-Latin) and resource availability (high vs. low). Our analysis reveals that
TP is a better predictor of the performance on tasks reliant on syntactic and
morphological cues (e.g., extractive QA), while IP better predicts performance
in semantic tasks (e.g., topic classification). Complementary analyses,
including tokenizer behavior, vocabulary coverage, and qualitative insights,
reveal that the language support claims of LLMs often might mask deeper
mismatches at the script or token level.

</details>


### [79] [Responsible AI Technical Report](https://arxiv.org/abs/2509.20057)
*KT,:,Soonmin Bae,Wanjin Park,Jeongyeop Kim,Yunjin Park,Jungwon Yoon,Junhyung Moon,Myunggyo Oh,Wonhyuk Lee,Junseo Jang,Dongyoung Jung,Minwook Ju,Eunmi Kim,Sujin Kim,Youngchol Kim,Somin Lee,Wonyoung Lee,Minsung Noh,Hyoungjun Park,Eunyoung Shin*

Main category: cs.CL

TL;DR: KT基于国际和本地治理经验，创新提出了AI责任评估与风险管控方法，并开发了用于风险缓解的实际工具SafetyGuard，为国内AI安全生态建设提供了实践支撑和行业参考。


<details>
  <summary>Details</summary>
Motivation: 目前全球对人工智能的规范和合规日益重视，KT希望研发一套适用于本地环境的AI责任评估与风险控制体系，以保障AI服务的安全性与可靠性，推动国内AI生态健康发展。

Method: 通过分析《人工智能基本法》和全球AI治理趋势，KT制定出专属的AI风险管理方法论，包括从AI开发到运维全过程中的风险识别与治理。提出KT AI风险分类体系，构建系统性评估流程，并开发配套风险缓解工具。

Result: 成功建立了一套适用于本地（韩国）环境的AI责任评估方法及风险分类体系，开发了可实际用于AI风险管理和缓解的工具，例如实时防护工具SafetyGuard，能够实时拦截AI模型的有害输出。

Conclusion: KT的研究成果为AI安全可靠地应用提供了系统性解决方案，对有意开发负责任AI的组织具有现实指导意义，并通过实践工具促进了国内AI行业的健全发展。

Abstract: KT developed a Responsible AI (RAI) assessment methodology and risk
mitigation technologies to ensure the safety and reliability of AI services. By
analyzing the Basic Act on AI implementation and global AI governance trends,
we established a unique approach for regulatory compliance and systematically
identify and manage all potential risk factors from AI development to
operation. We present a reliable assessment methodology that systematically
verifies model safety and robustness based on KT's AI risk taxonomy tailored to
the domestic environment. We also provide practical tools for managing and
mitigating identified AI risks. With the release of this report, we also
release proprietary Guardrail : SafetyGuard that blocks harmful responses from
AI models in real-time, supporting the enhancement of safety in the domestic AI
development ecosystem. We also believe these research outcomes provide valuable
insights for organizations seeking to develop Responsible AI.

</details>


### [80] [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](https://arxiv.org/abs/2509.20065)
*Maggie Mi,Aline Villavicencio,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 该文提出一种用输入级特征预判语言模型理解输入时可能出错的算法，兼具轻量、无需访问模型内部，且在多个数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型容易在处理成语、比喻或语境敏感的输入时发生理解错误，主要源于输入本身的误解而非输出错误，因此需要一种能够在生成结果前预判输入是否易出错的方法。

Method: 提出了一种仅依赖输入的错误预测方法，利用由“惊异度（surprisal）”和“信息均匀密度理论（Uniform Information Density hypothesis）”启发的token级似然性特征，分别捕捉输入理解的不确定性，并不依赖模型输出或隐藏层激活。

Result: 该方法在五个语言学挑战性强的数据集上相较于标准基线表现更佳。局部特征对于大型模型效果更好，而小型模型则更受全局特征益处。

Conclusion: 作者提出的基于输入的轻量级且通用的预生成错误预测方法能够有效提升语言模型在复杂输入下的可控性和可靠性。

Abstract: Language models often struggle with idiomatic, figurative, or
context-sensitive inputs, not because they produce flawed outputs, but because
they misinterpret the input from the outset. We propose an input-only method
for anticipating such failures using token-level likelihood features inspired
by surprisal and the Uniform Information Density hypothesis. These features
capture localized uncertainty in input comprehension and outperform standard
baselines across five linguistically challenging datasets. We show that
span-localized features improve error detection for larger models, while
smaller models benefit from global patterns. Our method requires no access to
outputs or hidden activations, offering a lightweight and generalizable
approach to pre-generation error prediction.

</details>


### [81] [From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](https://arxiv.org/abs/2509.20072)
*Tianqiao Liu,Xueyi Li,Hao Wang,Haoxuan Li,Zhichao Chen,Weiqi Luo,Zitao Liu*

Main category: cs.CL

TL;DR: TtT模型将AR文本生成和音频扩散结合，大幅简化了语音对话多模态系统的训练流程，提高了效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前多模态对话系统（尤其是语音到语音）端到端训练复杂、成本高，同时没有充分利用文本与音频生成时依赖结构的不同。

Method: 提出TtT模型，将自回归（AR）文本生成与非自回归音频扩散（diffusion）结合在同一个Transformer架构中，且以预训练大语言模型为初始化。

Result: 实现了一种更高效的语音文本统一建模框架，既降低了训练复杂度和算力消耗，又更好地适配了文本与音频生成的不同本质需求。

Conclusion: TtT方法有效简化了当前多模态模型的训练流程，并通过差异化建模提升了语音对话系统的生成效果。

Abstract: Recent advances in large language models have attracted significant interest
in extending their capabilities to multimodal scenarios, particularly for
speech-in speech-out conversational systems. However, existing multimodal
models handling interleaved audio and text, such as MOSHI require complex multi
stage training pipelines, incurring substantial computational costs. Moreover,
these models uniformly apply autoregressive generation to both text and audio
tokens, overlooking a fundamental asymmetry in their dependency structures:
while text tokens exhibit strong target target dependencies requiring causal
ordering, audio tokens are predominantly driven by source target dependencies,
where audio outputs primarily condition on source text rather than preceding
audio tokens. In this work, we propose TtT, a unified audio-text modeling
framework that integrates AR text generation with non-autoregressive audio
diffusion within a single Transformer architecture initialized from a
pretrained LLM.

</details>


### [82] [Can Constructions "SCAN" Compositionality ?](https://arxiv.org/abs/2509.20074)
*Ganesh Katrapati,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本论文提出无监督挖掘数据中的伪构式，预处理输入后有效提升序列到序列模型的组合性和系统泛化能力，无需额外监督或模型结构调整。


<details>
  <summary>Details</summary>
Motivation: 传统的序列到序列模型在强调组合性和结构化泛化时表现有限，主要因为无法有效内化形式-意义配对的构式。作者旨在通过无监督手段补足这一不足，提高模型泛化能力。

Method: 通过在训练数据中自动提取变量槽模板（pseudo-constructions），并用其对输入进行预处理，无需改变模型架构或额外监督。

Result: 在SCAN数据集挑战性分割（如ADD JUMP、AROUND RIGHT）上取得显著提升：准确率分别提升至47.8%和20.3%；且仅用40%训练数据也获得竞争性能，展现了数据效率。

Conclusion: 本文提出利用无监督方式提取伪构式（pseudo-constructions），能显著提升序列到序列模型在系统泛化与组合性任务上的表现。

Abstract: Sequence to Sequence models struggle at compositionality and systematic
generalisation even while they excel at many other tasks. We attribute this
limitation to their failure to internalise constructions conventionalised form
meaning pairings that license productive recombination. Building on these
insights, we introduce an unsupervised procedure for mining
pseudo-constructions: variable-slot templates automatically extracted from
training data. When applied to the SCAN dataset, our method yields large gains
out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on
AROUND RIGHT without any architectural changes or additional supervision. The
model also attains competitive performance with? 40% of the original training
data, demonstrating strong data efAciency. Our findings highlight the promise
of construction-aware preprocessing as an alternative to heavy architectural or
training-regime interventions.

</details>


### [83] [OLaPh: Optimal Language Phonemizer](https://arxiv.org/abs/2509.20086)
*Johannes Wirth*

Main category: cs.CL

TL;DR: 作者提出了结合词典、多NLP技术和概率打分的OLaPh音素转换框架，并配合大模型强化泛化性能，在德英双语高难数据集上均显著提升了准确率且开源，为后续研究提供支持。


<details>
  <summary>Details</summary>
Motivation: 语音文本转换（如语音合成）中，如何准确地将文本转为音素（phonemization）是关键难题。现有规则系统和神经网络方法在遇到人名、外来词、缩写及同形异义词时仍表现不佳，因此作者寻求提升此类特殊词汇的音素转换能力。

Method: 提出OLaPh（Optimal Language Phonemizer）框架，结合大型词典、多种自然语言处理技术、复合词分解及概率评分函数优化音素转换流程。在德英双语数据集上评估。进一步，用OLaPh生成的数据训练大型语言模型（LLM），提升泛化能力。

Result: OLaPh在德语和英语音素转换准确率上优于前沿方法，即使在高难度数据集上也能获得显著提升；基于OLaPh数据训练的LLM在泛化和性能方面更优。

Conclusion: OLaPh和基于其生成数据训练的LLM有效提升了音素转换的一致性与准确率，成为开源的研究资源，为领域未来研究提供有力工具。

Abstract: Phonemization, the conversion of text into phonemes, is a key step in
text-to-speech. Traditional approaches use rule-based transformations and
lexicon lookups, while more advanced methods apply preprocessing techniques or
neural networks for improved accuracy on out-of-domain vocabulary. However, all
systems struggle with names, loanwords, abbreviations, and homographs. This
work presents OLaPh (Optimal Language Phonemizer), a framework that combines
large lexica, multiple NLP techniques, and compound resolution with a
probabilistic scoring function. Evaluations in German and English show improved
accuracy over previous approaches, including on a challenging dataset. To
further address unresolved cases, we train a large language model on
OLaPh-generated data, which achieves even stronger generalization and
performance. Together, the framework and LLM improve phonemization consistency
and provide a freely available resource for future research.

</details>


### [84] [Causal Understanding by LLMs: The Role of Uncertainty](https://arxiv.org/abs/2509.20088)
*Oscar Lithgow-Serrano,Vani Kanjirangat,Alessandro Antonucci*

Main category: cs.CL

TL;DR: 不同大模型在因果关系判别任务上表现如随机猜测，无论句子是否在预训练中见过，说明模型无法建立因果结构表示，问题不在于训练时接触因果语料不足。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）在因果关系分类任务上表现接近随机，引发对其失败原因的探讨：是预训练时接触因果示例不足，还是存在更深层的表示缺陷？本研究旨在区分这两种可能性。

Method: 本文采用基于不确定性的评估方法，对七种模型（包括Pythia系列、GPT-J和Dolly等）分别在来自The Pile语料库及2024年后PubMed语料中的因果句子上进行测试。通过两项任务分析模型行为：（i）因果关系分类（直接/条件/相关/无关系四分类），（ii）原句与其释义的逐字记忆偏好探查。

Result: 模型在见过和未见过的句子上的准确率几乎相同（p > 0.05），不存在记忆偏向（原句选择率24.8%），输出分布接近均匀（熵值逼近最大），显示为随机猜测。指令微调模型存在严重置信度校准问题（Qwen：置信度超95%，准确率仅32.8%，ECE=0.49）。条件关系导致更高输出熵（比直接关系高11%）。

Conclusion: 因果关系理解上的失败主要源于缺乏结构化的因果表示，而不是预训练过程中因果示例的缺失。

Abstract: Recent papers show LLMs achieve near-random accuracy in causal relation
classification, raising questions about whether such failures arise from
limited pretraining exposure or deeper representational gaps. We investigate
this under uncertainty-based evaluation, testing whether pretraining exposure
to causal examples improves causal understanding >18K PubMed sentences -- half
from The Pile corpus, half post-2024 -- across seven models
(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model
behavior through: (i) causal classification, where the model identifies causal
relationships in text, and (ii) verbatim memorization probing, where we assess
whether the model prefers previously seen causal statements over their
paraphrases. Models perform four-way classification
(direct/conditional/correlational/no-relationship) and select between originals
and their generated paraphrases. Results show almost identical accuracy on
seen/unseen sentences (p > 0.05), no memorization bias (24.8% original
selection), and output distribution over the possible options is almost flat,
with entropic values near the maximum (1.35/1.39), confirming random guessing.
Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence,
32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%
vs. direct). These findings suggest that failures in causal understanding arise
from the lack of structured causal representation, rather than insufficient
exposure to causal examples during pretraining.

</details>


### [85] [Integrated Framework for LLM Evaluation with Answer Generation](https://arxiv.org/abs/2509.20097)
*Sujeong Lee,Hayoung Lee,Seongsoo Heo,Wonik Choi*

Main category: cs.CL

TL;DR: 提出了一种名为SPEED的新型大语言模型评估框架，通过集成小型专家模型和专家诊断，实现多维度、描述性分析，评估更公平、解释性更强且资源消耗更低，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的基准测试方法依赖于固定答案，难以全面评估大语言模型的实际表现，尤其在响应质量等方面存在不足。为了解决这些问题，作者提出了新的评估框架。

Method: 提出了SPEED框架，通过集成多种功能性专家，从多个维度（如幻觉检测、毒性评估、词汇和语境适宜性）对模型生成的内容进行专家诊断和描述性分析。与传统方法不同，SPEED融入了专家反馈以提升评估全面性和解释性，同时使用体量较小的专家模型以提高资源效率。

Result: 实验结果显示，SPEED在多领域和数据集上都实现了稳定且一致的评估表现，并且在资源利用上优于使用更大模型的评估方法。

Conclusion: SPEED不仅提高了评估的公平性和解释性，还在资源效率、适用广度等方面优于传统评估方法，是现有方法的有力替代者。

Abstract: Reliable evaluation of large language models is essential to ensure their
applicability in practical scenarios. Traditional benchmark-based evaluation
methods often rely on fixed reference answers, limiting their ability to
capture important qualitative aspects of generated responses. To address these
shortcomings, we propose an integrated evaluation framework called
\textit{self-refining descriptive evaluation with expert-driven diagnostics},
SPEED, which utilizes specialized functional experts to perform comprehensive,
descriptive analyses of model outputs. Unlike conventional approaches, SPEED
actively incorporates expert feedback across multiple dimensions, including
hallucination detection, toxicity assessment, and lexical-contextual
appropriateness. Experimental results demonstrate that SPEED achieves robust
and consistent evaluation performance across diverse domains and datasets.
Additionally, by employing relatively compact expert models, SPEED demonstrates
superior resource efficiency compared to larger-scale evaluators. These
findings illustrate that SPEED significantly enhances fairness and
interpretability in LLM evaluations, offering a promising alternative to
existing evaluation methodologies.

</details>


### [86] [Less is More: The Effectiveness of Compact Typological Language Representations](https://arxiv.org/abs/2509.20129)
*York Hay Ng,Phuong Hanh Hoang,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 结合特征选择与填补优化 URIEL+ 语料，降低维度并改善多语种NLP模型和距离度量效果。


<details>
  <summary>Details</summary>
Motivation: 现有如 URIEL+ 的语言特征数据集高维且稀疏，尤其针对低资源语言时，距离度量效果受限，需要探索更有效的特征构建方式。

Method: 提出一个结合特征选择和特征填补的处理流程，对 URIEL+ 语言特征空间进行优化，得到维度降低但可解释性强的新特征集，并在语言距离对齐与下游任务中进行评估。

Result: 压缩后的类型学特征集合提升了语言距离指标的信息量，在多语言自然语言处理任务中表现更好。

Conclusion: 通过对 URIEL+ 等语言特征数据集进行特征选择和填补，可以生成既紧凑又可解释的类型学特征表示，提升语言距离度量效果，并改善多语言自然语言处理任务表现。

Abstract: Linguistic feature datasets such as URIEL+ are valuable for modelling
cross-lingual relationships, but their high dimensionality and sparsity,
especially for low-resource languages, limit the effectiveness of distance
metrics. We propose a pipeline to optimize the URIEL+ typological feature space
by combining feature selection and imputation, producing compact yet
interpretable typological representations. We evaluate these feature subsets on
linguistic distance alignment and downstream tasks, demonstrating that
reduced-size representations of language typology can yield more informative
distance metrics and improve performance in multilingual NLP applications.

</details>


### [87] [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
*Chaojun Nie,Jun Zhou,Guanxiang Wang,Shisong Wud,Zichen Wang*

Main category: cs.CL

TL;DR: 该工作针对大语言模型在领域任务知识稀缺的问题，提出了强化学习增强生成方法（RLAG），在多个专业领域取得了比现有方法更优的效果。代码与数据已开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在处理特定领域任务时表现有限，主要原因在于其训练数据中专业信息比例较低且数据集较为静态。这导致知识稀缺和信息时滞，形成领域应用的知识空白。现有后训练方法如CPT、SFT各有不足，难以充分嵌入和组织复杂的领域知识。

Method: 提出了一种基于生成增强的强化学习方法（RLAG）。该方法通过多次生成样本并计算奖励，选择高概率生成结果，并利用三种定制奖励指标优化模型，旨在有效嵌入关键且连贯的领域知识。

Result: 在医学、法律、天文和时事等多个领域数据集上的实验表明，RLAG方法在答案准确度和解释合理性方面显著优于现有基线方法。

Conclusion: RLAG能有效弥补大语言模型在特定领域知识上的不足，提高专业任务的表现。该方法在多个领域有较好推广性，并实现开源。

Abstract: Large language models (LLMs) often exhibit limited performance on
domain-specific tasks due to the natural disproportionate representation of
specialized information in their training data and the static nature of these
datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain
applications. While post-training on domain datasets can embed knowledge into
models, existing approaches have some limitations. Continual Pre-Training (CPT)
treats all tokens in domain documents with equal importance, failing to
prioritize critical knowledge points, while supervised fine-tuning (SFT) with
question-answer pairs struggles to develop the coherent knowledge structures
necessary for complex reasoning tasks. To address these challenges, we propose
Reinforcement Learning from Augmented Generation (RLAG). Our approach
iteratively cycles between sampling generations and optimizing the model
through calculated rewards, effectively embedding critical and contextually
coherent domain knowledge. We select generated outputs with the highest log
probabilities as the sampling result, then compute three tailored reward
metrics to guide the optimization process. To comprehensively evaluate domain
expertise, we assess answer accuracy and the rationality of explanations
generated for correctly answered questions. Experimental results across
medical, legal, astronomy, and current events datasets demonstrate that our
proposed method significantly outperforms baseline approaches. Our code and
data are open sourced at https://github.com/ChaojunNie/RLAG.

</details>


### [88] [Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian](https://arxiv.org/abs/2509.20168)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: 该研究通过提出DS-GSI指标和基于模板的探测方法，发现主流多语言LLMs在低资源语言（波斯语）中存在显著性别偏见，尤其在体育领域。研究结果提示NLP领域需重视低资源语言中的性别公正。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）在全球范围内应用日益广泛，但性别偏见问题可能带来表达上的伤害。以往关于性别偏见的研究主要关注高资源语言，低资源语言（如波斯语）则研究较少。

Method: 提出了一种基于模板的探测方法，并结合真实世界数据进行验证，用于揭示LLMs中的性别刻板印象。同时引入了领域特定性别偏斜指数（DS-GSI），用以量化模型性别平等的偏离程度。对四个主流模型（GPT-4o mini、DeepSeek R1、Gemini 2.0 Flash、Qwen QwQ 32B）在四个语义领域进行评估，主要关注低资源语言波斯语。

Result: 所有被评估模型均展现出性别刻板印象，在所有领域中，波斯语的性别差异大于英语，其中在体育领域性别偏见最为明显。

Conclusion: 当前的多语言LLMs在低资源语言上存在较严重的性别偏见，特别是在某些领域表现更为突出。研究为低资源语言性别偏见的评估提供了方法框架，并强调了包容性NLP实践的必要性。

Abstract: Multilingual Large Language Models (LLMs) are increasingly used worldwide,
making it essential to ensure they are free from gender bias to prevent
representational harm. While prior studies have examined such biases in
high-resource languages, low-resource languages remain understudied. In this
paper, we propose a template-based probing methodology, validated against
real-world data, to uncover gender stereotypes in LLMs. As part of this
framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a
metric that quantifies deviations from gender parity. We evaluate four
prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,
across four semantic domains, focusing on Persian, a low-resource language with
distinct linguistic features. Our results show that all models exhibit gender
stereotypes, with greater disparities in Persian than in English across all
domains. Among these, sports reflect the most rigid gender biases. This study
underscores the need for inclusive NLP practices and provides a framework for
assessing bias in other low-resource languages.

</details>


### [89] [Thinking Augmented Pre-training](https://arxiv.org/abs/2509.20186)
*Liang Wang,Nan Yang,Shaohan Huang,Li Dong,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出TPT（思维轨迹增强预训练）方法，通过为文本自动生成思考过程来扩充和分解高质量数据，显著提升了大语言模型的训练效率和理解能力，在多个推理任务上均取得了大幅性能提升。


<details>
  <summary>Details</summary>
Motivation: 预训练LLM计算资源需求不断增长，但高质量数据有限，导致如何最大化现有数据利用成了主要问题。尤其是一些高质量token较难被模型学习，因为其中蕴含的推理过程异常复杂。

Method: 提出了Thinking augmented Pre-Training（TPT）方法，通过自动生成思维轨迹将现有文本数据进行增强，使训练数据量增加，同时高质量token更易于模型掌握。方法在不同训练设置下进行了实验，包括数据量有限和充足的预训练，以及强大的开源检查点的中期训练。

Result: TPT可将LLM预训练的数据效率提升3倍，在模型参数为3B时，其推理任务表现提升超过10%。多种模型和数据配置下均有效。

Conclusion: TPT方法可以显著提升大型语言模型在各种模型规模和家族上的性能，并且将预训练的数据效率提高了3倍。以3B参数模型为例，TPT能在多个高难度推理基准任务中带来超过10%的性能提升。

Abstract: This paper introduces a simple and scalable approach to improve the data
efficiency of large language model (LLM) training by augmenting existing text
data with thinking trajectories. The compute for pre-training LLMs has been
growing at an unprecedented rate, while the availability of high-quality data
remains limited. Consequently, maximizing the utility of available data
constitutes a significant research challenge. A primary impediment is that
certain high-quality tokens are difficult to learn given a fixed model
capacity, as the underlying rationale for a single token can be exceptionally
complex and deep. To address this issue, we propose Thinking augmented
Pre-Training (TPT), a universal methodology that augments text with
automatically generated thinking trajectories. Such augmentation effectively
increases the volume of the training data and makes high-quality tokens more
learnable through step-by-step reasoning and decomposition. We apply TPT across
diverse training configurations up to $100$B tokens, encompassing pre-training
with both constrained and abundant data, as well as mid-training from strong
open-source checkpoints. Experimental results indicate that our method
substantially improves the performance of LLMs across various model sizes and
families. Notably, TPT enhances the data efficiency of LLM pre-training by a
factor of $3$. For a $3$B parameter model, it improves the post-training
performance by over $10\%$ on several challenging reasoning benchmarks.

</details>


### [90] [Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs](https://arxiv.org/abs/2509.20208)
*Parker Glenn,Alfy Samuel,Daben Liu*

Main category: cs.CL

TL;DR: 本文提出了一种高效集成LLM运算符到声明式查询语言的新方法，在保证输出类型和数据库匹配的前提下，极大提升了准确率和处理速度。


<details>
  <summary>Details</summary>
Motivation: 解决将LLM集成到声明式查询语言（如SQL）时，生成结果需与类型检查器及数据库内容一致的问题，现有方法多依赖繁琐的后处理，影响性能。

Method: 研究了不同规模开源语言模型在SQL基础查询语言中解析和执行函数的能力，并提出了类型约束的优化方案。

Result: 小型语言模型在混合数据源上能够高效执行函数。所提方法在多跳问答数据集上准确率提升7%，延迟降低53%。

Conclusion: 提出了一种高效的方法来保证LLM函数类型的正确性，在多跳问答任务中显著提升了准确率和延迟表现。

Abstract: Integrating LLM powered operators in declarative query languages allows for
the combination of cheap and interpretable functions with powerful,
generalizable language model reasoning. However, in order to benefit from the
optimized execution of a database query language like SQL, generated outputs
must align with the rules enforced by both type checkers and database contents.
Current approaches address this challenge with orchestrations consisting of
many LLM-based post-processing calls to ensure alignment between generated
outputs and database values, introducing performance bottlenecks. We perform a
study on the ability of various sized open-source language models to both parse
and execute functions within a query language based on SQL, showing that small
language models can excel as function executors over hybrid data sources. Then,
we propose an efficient solution to enforce the well-typedness of LLM
functions, demonstrating 7% accuracy improvement on a multi-hop question
answering dataset with 53% improvement in latency over comparable solutions. We
make our implementation available at https://github.com/parkervg/blendsql

</details>


### [91] [Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks](https://arxiv.org/abs/2509.20209)
*Hailay Kidu Teklehaymanot,Gebrearegawi Gidey,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本文通过多语言迁移学习和定制分词，显著提升了低资源语言Tigrinya的翻译质量，并建立了高质量数据集与评测标准，为后续研究提供重要支持。


<details>
  <summary>Details</summary>
Motivation: 尽管神经机器翻译（NMT）已有进步，但资源匮乏的语言如Tigrinya因语料库有限、分词策略不足和评价基准缺失，仍面临严重挑战。

Method: 研究多语言预训练模型的迁移学习技术来提升形态复杂、资源匮乏语言的翻译质量。提出结合语言特定分词、嵌入初始化和领域自适应微调的方法。并构建高质量的人类对齐英-提格雷尼亚评测数据集，覆盖多领域。

Result: 实验显示，迁移学习结合定制分词器显著优于零样本基线，BLEU与chrF等指标，以及人工定性评估均有明显提升。使用Bonferroni校正保证统计显著性，误差分析揭示局限并推动模型改进。

Conclusion: 语言相关建模和可复现评测基准对提升低资源语言翻译至关重要。制备的资源可促进相关研究发展。

Abstract: Despite advances in Neural Machine Translation (NMT), low-resource languages
like Tigrinya remain underserved due to persistent challenges, including
limited corpora, inadequate tokenization strategies, and the lack of
standardized evaluation benchmarks. This paper investigates transfer learning
techniques using multilingual pretrained models to enhance translation quality
for morphologically rich, low-resource languages. We propose a refined approach
that integrates language-specific tokenization, informed embedding
initialization, and domain-adaptive fine-tuning. To enable rigorous assessment,
we construct a high-quality, human-aligned English-Tigrinya evaluation dataset
covering diverse domains. Experimental results demonstrate that transfer
learning with a custom tokenizer substantially outperforms zero-shot baselines,
with gains validated by BLEU, chrF, and qualitative human evaluation.
Bonferroni correction is applied to ensure statistical significance across
configurations. Error analysis reveals key limitations and informs targeted
refinements. This study underscores the importance of linguistically aware
modeling and reproducible benchmarks in bridging the performance gap for
underrepresented languages. Resources are available at
https://github.com/hailaykidu/MachineT_TigEng
  and https://huggingface.co/Hailay/MachineT_TigEng

</details>


### [92] [Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models](https://arxiv.org/abs/2509.20237)
*Yu Wang,Leyi Lao,Langchu Huang,Gabriel Skantze,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 微调语言模型可提升其在对话中对回声语和填充词的理解及生成能力，使其对话风格更像人类。


<details>
  <summary>Details</summary>
Motivation: 现代基于transformer的语言模型在对话中对backchannels（回声语，如“嗯”、“哦”）和fillers（填充词，如“呃”、“那个”）的表达存在明显不足，这影响其生成更具人类对话特征文本的能力。

Method: 采用三种微调策略在英日双语对话语料库上对语言模型进行微调，这些语料中backchannels和fillers被完整保留且标注。随后对模型中对backchannels和fillers的表征进行聚类分析，并用自然语言生成（NLG）指标评估生成文本的表现。

Result: 微调后的模型在表征backchannels和fillers方面聚类得分（silhouette score）上升，表明能更好地区分不同类型的backchannels和fillers及其语义变化。生成的文本也更像人类语言。

Conclusion: 通过适当的微调，可让一般的语言模型具备更强的对话能力，生成更贴近人类真实对话的文本。

Abstract: Backchannels and fillers are important linguistic expressions in dialogue,
but are under-represented in modern transformer-based language models (LMs).
Our work studies the representation of them in language models using three
fine-tuning strategies. The models are trained on three dialogue corpora in
English and Japanese, where backchannels and fillers are preserved and
annotated, to investigate how fine-tuning can help LMs learn their
representations. We first apply clustering analysis to the learnt
representation of backchannels and fillers, and have found increased silhouette
scores in representations from fine-tuned models, which suggests that
fine-tuning enables LMs to distinguish the nuanced semantic variation in
different backchannel and filler use. We also use natural language generation
(NLG) metrics to confirm that the utterances generated by fine-tuned language
models resemble human-produced utterances more closely. Our findings suggest
the potentials of transforming general LMs into conversational LMs that are
more capable of producing human-like languages adequately.

</details>


### [93] [Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage](https://arxiv.org/abs/2509.20278)
*Zipeng Ling,Yuehao Tang,Chen Huang,Shuliang Liu,Gaoyang Jiang,Shenghong Fu,Junqi Yang,Yao Wan,Jiawan Zhang,Kejia Huang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了LLM在提示设计上的“Instruction Boundary”问题，构建BiasDetector框架衡量和分析了由不同提示类型带来的偏差。实验证实，主流LLM依然存在显著偏差，影响其实用性。作者建议开发者和用户都需关注和减缓此类偏差，以提升LLM的推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）已被广泛用于各类领域的问题求解，但其在提示设计上的局限性未被充分研究。不当或不完整的提示输入可能导致LLM产生误导性结论，从而影响其可靠性并带来风险。

Method: 作者提出了“Instruction Boundary”这一概念，通过将其细化为八个具体维度，并引入BiasDetector框架，专门衡量由三种不同提示类型（完整的、冗余的、不足的）引发的偏差。同时，对多种主流LLM进行实证评测分析。

Result: 研究发现，尽管主流LLM表面准确率较高，但在许多下游任务中仍存在因提示覆盖不全而产生的显著偏差。实验验证了这些偏差的实际影响，并提出了减缓这些偏差的策略建议。

Conclusion: LLM推理的可靠性受提示边界影响很大。开发者需要关注并降低偏差，用户在输入提示时也应更加谨慎。未来LLM的发展需从提示设计和偏差规避两个层面持续改进。

Abstract: Large-language-model (LLM) reasoning has long been regarded as a powerful
tool for problem solving across domains, providing non-experts with valuable
advice. However, their limitations - especially those stemming from prompt
design - remain underexplored. Because users may supply biased or incomplete
prompts - often unintentionally - LLMs can be misled, undermining reliability
and creating risks. We refer to this vulnerability as the Instruction Boundary.
To investigate the phenomenon, we distill it into eight concrete facets and
introduce BiasDetector, a framework that measures biases arising from three
instruction types: complete, redundant, and insufficient. We evaluate several
mainstream LLMs and find that, despite high headline accuracy, substantial
biases persist in many downstream tasks as a direct consequence of prompt
coverage. Our empirical study confirms that LLM reasoning reliability can still
be significantly improved. We analyze the practical impact of these biases and
outline mitigation strategies. Our findings underscore the need for developers
to tackle biases and for users to craft options carefully.

</details>


### [94] [Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation](https://arxiv.org/abs/2509.20287)
*Behzad Shayegh,Jan-Thorsten Peter,David Vilar,Tobias Domhan,Juraj Juraska,Markus Freitag,Lili Mou*

Main category: cs.CL

TL;DR: 本文明确指出了当前机器翻译评测中信度与流畅度的权衡偏向，并揭示了元评价本身也存在类似偏差。作者提出采用合成系统方法来修正偏差，强调未来应注重两者平衡，以提升评价体系公正性。


<details>
  <summary>Details</summary>
Motivation: 在机器翻译中，译文的信度（adequacy）和流畅度（fluency）之间往往存在权衡，但目前相关评价和元评价环节对此的偏向和影响仍不清晰。

Method: 作者分析了现有评价指标在信度与流畅度之间的倾向，并研究了主流WMT元评价方法在此方面的偏差，提出通过合成翻译系统来控制这种偏差的方法。

Result: 发现当前指标普遍更偏向信度，现有元评价方法也更倾向信度类指标，并证明了这种偏见部分源自元评价数据集的系统组合。通过合成翻译系统的方法可以在一定程度上校正这一偏见。

Conclusion: 理解信度与流畅度之间的权衡及其在元评价中对指标排名的影响非常重要。提出的控制偏误的方法对今后评价体系的改进具有参考价值。

Abstract: We investigate the tradeoff between adequacy and fluency in machine
translation. We show the severity of this tradeoff at the evaluation level and
analyze where popular metrics fall within it. Essentially, current metrics
generally lean toward adequacy, meaning that their scores correlate more
strongly with the adequacy of translations than with fluency. More importantly,
we find that this tradeoff also persists at the meta-evaluation level, and that
the standard WMT meta-evaluation favors adequacy-oriented metrics over
fluency-oriented ones. We show that this bias is partially attributed to the
composition of the systems included in the meta-evaluation datasets. To control
this bias, we propose a method that synthesizes translation systems in
meta-evaluation. Our findings highlight the importance of understanding this
tradeoff in meta-evaluation and its impact on metric rankings.

</details>


### [95] [Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning](https://arxiv.org/abs/2509.20315)
*T. O. Abiola,K. D. Abiodun,O. E. Olumide,O. O. Adebanji,O. Hiram Calvo,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文提出了一种结合多语言transformer和主动学习的数据高效型希望言论检测方法，在四种语言数据集上表现优异，尤其适用于低资源环境。


<details>
  <summary>Details</summary>
Motivation: 希望言论可以促进网络上的积极互动，但由于多语言和低资源环境下的限制，其自动检测仍很困难。本文旨在解决该问题。

Method: 提出了一种多语言希望言论检测框架，结合主动学习方法和transformer模型（如mBERT和XLM-RoBERTa），在英语、西班牙语、德语和乌尔都语的相关数据集上进行了实验。

Result: transformer模型（尤其是XLM-RoBERTa）在准确率上显著优于传统方法，且主动学习策略在标注数据较少时也能保持较好效果。

Conclusion: 多语言transformer结合高效的数据利用策略可以有效应对希望言论检测任务，即使在资源有限的环境下也表现优秀。

Abstract: Hope speech language that fosters encouragement and optimism plays a vital
role in promoting positive discourse online. However, its detection remains
challenging, especially in multilingual and low-resource settings. This paper
presents a multilingual framework for hope speech detection using an active
learning approach and transformer-based models, including mBERT and
XLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,
German, and Urdu, including benchmark test sets from recent shared tasks. Our
results show that transformer models significantly outperform traditional
baselines, with XLM-RoBERTa achieving the highest overall accuracy.
Furthermore, our active learning strategy maintained strong performance even
with small annotated datasets. This study highlights the effectiveness of
combining multilingual transformers with data-efficient training strategies for
hope speech detection.

</details>


### [96] [SIM-CoT: Supervised Implicit Chain-of-Thought](https://arxiv.org/abs/2509.20317)
*Xilin Wei,Xiaoran Liu,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Xipeng Qiu,Dahua Lin*

Main category: cs.CL

TL;DR: 现有隐式CoT方法因缺乏步骤级监督导致训练不稳定、语义同质化。SIM-CoT通过辅助解码器实现每步语义监督，训练阶段提升表示多样性推理阶段无额外负担，在GPT-2与LLaMA-3.1 8B上显著增强准确性、稳定性与效率，部分指标超越显式CoT，并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 隐式Chain-of-Thought（CoT）方法为大语言模型中的显式CoT推理提供了更高效的替代方案，但其性能与显式方法存在显著差距。随着计算预算的扩大，隐式CoT方法在推理过程中易出现训练不稳定甚至崩溃，核心原因是潜在表示变得同质化、丧失语义多样性，且缺乏步骤级监督。

Method: 本文提出SIM-CoT，一种即插即用的训练模块。训练阶段，引入辅助解码器，对每一步隐式推理token提供与显式推理步骤对齐的监督，使潜在状态更具区分度和语义丰富性。推理阶段移除辅助解码器，保持计算效率。此外，辅助解码器可将隐式token投影到显式推理词汇，实现每步语义可视化与诊断。

Result: SIM-CoT显著提升各类隐式CoT方法的准确性和稳定性，如在GPT-2上Coconut基线提升8.2%，在LLaMA-3.1 8B上CODI提升3.0%。在可扩展性方面，SIM-CoT在GPT-2上超过显式CoT基线2.1%，且token效率高2.3倍，同时大幅缩小在更大模型如LLaMA-3.1 8B上的性能差距。

Conclusion: 引入步骤级监督的SIM-CoT有效解决了隐式CoT推理的潜在表示同质化和训练不稳定问题，提升了隐式CoT的方法性能、效率与稳定性，并且无需推理阶段额外计算开销，具有强扩展性和可解释性。

Abstract: Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient
alternative to explicit CoT reasoning in Large Language Models (LLMs), but a
persistent performance gap has limited the application of implicit CoT. We
identify a core latent instability issue by scaling the computational budget of
implicit CoT approaches: as we increase the number of implicit reasoning tokens
to enhance performance, the training process often becomes unstable and
collapses. Our analysis reveals that this instability arises from the latent
representations becoming homogeneous and losing their semantic diversity, a
failure caused by insufficient step-level supervision in existing implicit CoT
approaches. To address this issue, we propose SIM-CoT, a plug-and-play training
module that introduces step-level supervision to stabilize and enrich the
latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder
during training to align each implicit token with its corresponding explicit
reasoning step, ensuring that latent states capture distinct and meaningful
information. The proposed auxiliary decoder is removed during inference,
preserving the computational efficiency of implicit CoT methods with no added
overhead. In addition, the auxiliary decoder affords interpretability of
implicit reasoning by projecting each latent token onto an explicit reasoning
vocabulary, enabling per-step visualization of semantic roles and diagnosis.
SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain
stability of various implicit CoT methods, boosting baselines like Coconut by
+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong
scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%
with 2.3\times greater token efficiency, while substantially closing the
performance gap on larger models like LLaMA-3.1 8B.

</details>


### [97] [Z-Scores: A Metric for Linguistically Assessing Disfluency Removal](https://arxiv.org/abs/2509.20319)
*Maria Teleki,Sai Janjur,Haoran Liu,Oliver Grabner,Ketan Verma,Thomas Docog,Xiangjue Dong,Lingfeng Shi,Cong Wang,Stephanie Birkelbach,Jason Kim,Yin Zhang,James Caverlee*

Main category: cs.CL

TL;DR: 本文提出Z-Scores指标，从话语杂音类型层面对语音去杂模型进行诊断，比传统词级指标更能揭示模型缺陷，助力性能优化。


<details>
  <summary>Details</summary>
Motivation: 传统的词级评价指标难以诊断模型在去除语音杂音具体类型上的表现及失败原因，因此需要更细粒度的诊断工具。

Method: 提出了基于消歧义文本与生成文本对齐的确定性算法，结合三类话语杂音（EDITED，INTJ，PRN）进行类别诊断的新指标Z-Scores。

Result: Z-Scores能准确诊断模型在INTJ和PRN类型杂音上的问题并指导性能提升，案例证明该方法优于单一F1指标。

Conclusion: Z-Scores可以揭示现有词级指标无法发现的系统性弱点，为模型改进和干预提供有力指导。

Abstract: Evaluating disfluency removal in speech requires more than aggregate
token-level scores. Traditional word-based metrics such as precision, recall,
and F1 (E-Scores) capture overall performance but cannot reveal why models
succeed or fail. We introduce Z-Scores, a span-level linguistically-grounded
evaluation metric that categorizes system behavior across distinct disfluency
types (EDITED, INTJ, PRN). Our deterministic alignment module enables robust
mapping between generated text and disfluent transcripts, allowing Z-Scores to
expose systematic weaknesses that word-level metrics obscure. By providing
category-specific diagnostics, Z-Scores enable researchers to identify model
failure modes and design targeted interventions -- such as tailored prompts or
data augmentation -- yielding measurable performance improvements. A case study
with LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies
hidden in aggregate F1, directly informing model refinement strategies.

</details>


### [98] [DRES: Benchmarking LLMs for Disfluency Removal](https://arxiv.org/abs/2509.20321)
*Maria Teleki,Sai Janjur,Haoran Liu,Oliver Grabner,Ketan Verma,Thomas Docog,Xiangjue Dong,Lingfeng Shi,Cong Wang,Stephanie Birkelbach,Jason Kim,Yin Zhang,James Caverlee*

Main category: cs.CL

TL;DR: 提出了DRES基准，专为话语不流畅消除设计，评测多类LLM表现，发现简单方法有效，微调模型泛化性差，并给出实际应用建议，对语音系统研究具有推动作用。


<details>
  <summary>Details</summary>
Motivation: 语音驱动系统中，话语不流畅（如“嗯”、“啊”及括号语等）常导致语音识别、命令理解等任务准确率下降。现有评测往往混杂ASR错误及声学变化，缺乏针对文本级不流畅消除的可控、复现性强的基准。

Method: 作者提出了DRES（Disfluency Removal Evaluation Suite），利用人工标注的Switchboard语料制作，可控且隔离了ASR错误与声学因素。系统性评估了多种专有与开源LLM，涵盖模型规模、提示方式及架构；同时对模型特殊错误模式进行了分析，并给出九条实际建议。

Result: 1）简单语段切分方法在各种模型上都有效；2）以推理为主的模型容易过度删除流畅词；3）微调模型虽可达接近最优精度与召回，但泛化能力下降。

Conclusion: DRES为不流畅消除任务提供了可以复现、模型无关的基准，并提出了提升语音系统健壮性的建议。该基准推动了口语系统研究的进一步发展。

Abstract: Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited
statements -- remain a persistent challenge for speech-driven systems,
degrading accuracy in command interpretation, summarization, and conversational
agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled
text-level benchmark that establishes a reproducible semantic upper bound for
this task. DRES builds on human-annotated Switchboard transcripts, isolating
disfluency removal from ASR errors and acoustic variability. We systematically
evaluate proprietary and open-source LLMs across scales, prompting strategies,
and architectures. Our results reveal that (i) simple segmentation consistently
improves performance, even for long-context models; (ii) reasoning-oriented
models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near
state-of-the-art precision and recall but harms generalization abilities. We
further present a set of LLM-specific error modes and offer nine practical
recommendations (R1-R9) for deploying disfluency removal in speech-driven
pipelines. DRES provides a reproducible, model-agnostic foundation for
advancing robust spoken-language systems.

</details>


### [99] [Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations](https://arxiv.org/abs/2509.20341)
*Gebrearegawi Gebremariam,Hailay Teklehaymanot,Gebregewergs Mezgebe*

Main category: cs.CL

TL;DR: 本文首次提出并实现了Ge'ez语言的规则型形态合成器，解决了该语言自然语言处理工具缺乏的问题。通过大量动词样本测试，系统获得了97.4%的高性能。研究填补了技术空白，为后续Ge'ez语言智能处理奠定了基础。


<details>
  <summary>Details</summary>
Motivation: Ge'ez语作为古代闪米特语，至今仍很重要，但由于缺乏标注语料及工具，语言学和自然语言处理领域难以开展相关研究。当前还没有可用的Ge'ez自然语言处理工具，造成该语言在技术和学术领域被边缘化。论文旨在解决这一问题，推动Ge'ez语言的挖掘和应用。

Method: 提出并实现了基于规则的Ge'ez形态合成器，通过语言的形态结构将词根生成表层词，采用1,102个覆盖全部动词形态结构的样本动词进行测试和评估。

Result: 系统在1,102个动词样本上的性能达97.4%，优于基线模型，证明所提出方法的有效性，并建议未来工作应扩展至更全面的形态变化系统。

Conclusion: 基于规则的方法能够高效地处理Ge'ez语言复杂的形态变化，克服了数据稀缺问题，为后续研究和技术开发提供了基础。文章强调需要进一步完善系统，以覆盖Ge'ez的全部形态多样性。

Abstract: Ge'ez is an ancient Semitic language renowned for its unique alphabet. It
serves as the script for numerous languages, including Tigrinya and Amharic,
and played a pivotal role in Ethiopia's cultural and religious development
during the Aksumite kingdom era. Ge'ez remains significant as a liturgical
language in Ethiopia and Eritrea, with much of the national identity
documentation recorded in Ge'ez. These written materials are invaluable primary
sources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,
and civilization. Ge'ez has a complex morphological structure with rich
inflectional and derivational morphology, and no usable NLP has been developed
and published until now due to the scarcity of annotated linguistic data,
corpora, labeled datasets, and lexicons. Therefore, we propose a rule-based
Ge'ez morphological synthesizer to generate surface words from root words
according to the morphological structures of the language. We used 1,102 sample
verbs, representing all verb morphological structures, to test and evaluate the
system. The system achieves a performance of 97.4%, outperforming the baseline
model and suggesting that future work should build a comprehensive system
considering morphological variations of the language.
  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based

</details>


### [100] [EmbeddingGemma: Powerful and Lightweight Text Representations](https://arxiv.org/abs/2509.20354)
*Henrique Schechter Vera,Sahil Dua,Biao Zhang,Daniel Salz,Ryan Mullins,Sindhu Raghuram Panyam,Sara Smoot,Iftekhar Naim,Joe Zou,Feiyang Chen,Daniel Cer,Alice Lisak,Min Choi,Lucas Gonzalez,Omar Sanseviero,Glenn Cameron,Ian Ballantyne,Kat Black,Kaifeng Chen,Weiyi Wang,Zhe Li,Gus Martins,Jinhyuk Lee,Mark Sherwood,Juyeong Ji,Renjie Wu,Jingxiao Zheng,Jyotinder Singh,Abheesht Sharma,Divya Sreepat,Aashi Jain,Adham Elarabawy,AJ Co,Andreas Doumanoglou,Babak Samari,Ben Hora,Brian Potetz,Dahun Kim,Enrique Alfonseca,Fedor Moiseev,Feng Han,Frank Palma Gomez,Gustavo Hernández Ábrego,Hesen Zhang,Hui Hui,Jay Han,Karan Gill,Ke Chen,Koert Chen,Madhuri Shanbhogue,Michael Boratko,Paul Suganthan,Sai Meher Karthik Duddu,Sandeep Mariserla,Setareh Ariafar,Shanfeng Zhang,Shijie Zhang,Simon Baumgartner,Sonam Goenka,Steve Qiu,Tanmaya Dabral,Trevor Walker,Vikram Rao,Waleed Khawaja,Wenlei Zhou,Xiaoqi Ren,Ye Xia,Yichang Chen,Yi-Ting Chen,Zhe Dong,Zhongli Ding,Francesco Visin,Gaël Liu,Jiageng Zhang,Kathleen Kenealy,Michelle Casbon,Ravin Kumar,Thomas Mesnard,Zach Gleicher,Cormac Brick,Olivier Lacombe,Adam Roberts,Yunhsuan Sung,Raphael Hoffmann,Tris Warkentin,Armand Joulin,Tom Duerig,Mojtaba Seyedhosseini*

Main category: cs.CL

TL;DR: EmbeddingGemma 是一种轻量级、高性能的开源文本嵌入模型，兼具高准确率与高效率，非常适合端侧和高吞吐应用，在MTEB基准上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流文本嵌入模型往往参数规模较大，难以应用于对延迟和资源要求较高的场景，因此需要一种轻量级、性能优越的新方法。

Method: 采用 encoder-decoder 初始化与几何嵌入蒸馏方法从大模型中迁移知识，并通过 spread-out 正则方法增强模型鲁棒性和表达能力，同时融合多样化训练断点以提升泛化能力。

Result: EmbeddingGemma（300M 参数量）在 MTEB 基准上取得最优表现，超越500M 以下的之前最佳模型，对比参数量达其两倍的模型也能取得相近或更好效果。此外，即使在量化或嵌出输出裁剪后，优势依然存在。

Conclusion: EmbeddingGemma 模型实现了在小规模参数下超越同类竞品，并且在多语言、英文和代码领域均表现优异，适用于低延迟和高吞吐场景。

Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model
based on the Gemma 3 language model family. Our innovative training recipe
strategically captures knowledge from larger models via encoder-decoder
initialization and geometric embedding distillation. We improve model
robustness and expressiveness with a spread-out regularizer, and ensure
generalizability by merging checkpoints from varied, optimized mixtures.
Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,
English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art
results. Notably, it outperforms prior top models, both proprietary and open,
with fewer than 500M parameters, and provides performance comparable to models
double its size, offering an exceptional performance-to-cost ratio. Remarkably,
this lead persists when quantizing model weights or truncating embedding
outputs. This makes EmbeddingGemma particularly well-suited for low-latency and
high-throughput use cases such as on-device applications. We provide ablation
studies exploring our key design choices. We release EmbeddingGemma to the
community to promote further research.

</details>


### [101] [Language Models that Think, Chat Better](https://arxiv.org/abs/2509.20357)
*Adithya Bhaskar,Xi Ye,Danqi Chen*

Main category: cs.CL

TL;DR: 本文提出RLMT方法，将强化学习与模型奖励结合，并要求模型生成长链式推理，显著提升大模型通用任务表现，使少样本、免SFT训练成为可能，重塑大模型后训练流程。


<details>
  <summary>Details</summary>
Motivation: 传统基于可验证奖励的强化学习（RLVR）方法虽在数学、编程等可验证领域有效，但面对写作、规划等开放式任务时泛化能力有限。人类在这些任务中需要复杂推理，现有RLVR难以适用。本文旨在突破此限制，将此类方法推广到通用聊天和实际应用中。

Method: 提出RLMT（模型奖励的思考），要求大语言模型在响应前生成较长的链式推理（CoT），然后通过基于偏好的奖励模型进行在线强化学习优化。模型在LLama-3.1-8B和Qwen-2.5-7B（基础与指令微调版本）上，用DPO、PPO和GRPO等多种优化算法做训练，对比标准RLHF流程评估其效果。

Result: RLMT在AlpacaEval2、WildBench和ArenaHardV2等三项聊天基准测试上获得3-7分的显著提升，在创意写作和常识问答等任务上提升1-3分。在模型规模8B时，RLMT训练的模型在聊天和创意写作上超过GPT-4o，且接近Claude-3.7-Sonnet（Thinking）表现。RLMT还能直接用于基础模型，无需SFT阶段，仅用7000个样本即可超过大量数据复杂管道的指令微调模型。

Conclusion: RLMT范式将强化学习和模型奖励机制成功应用到开放式思维任务，取得领先于现有方案的效果。该方法还节省大量训练资源并简化流程，有望激发对“思考型”训练机制更广泛和深入的探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves language model
reasoning by using rule-based rewards in verifiable domains such as mathematics
and code. However, RLVR leads to limited generalization for open-ended tasks --
such as writing outline essays or making meal plans -- where humans reason
routinely. This paper shows that the RLVR paradigm is effective beyond
verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking
(**RLMT**) for general-purpose chat capabilities. Using diverse real-world
prompts, RLMT requires LMs to generate long CoT reasoning before response, and
optimizes them with online RL against a preference-based reward model used in
RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and
instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT
consistently outperforms standard RLHF pipelines. This includes substantial
gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and
ArenaHardV2), along with 1-3 point improvements on other tasks like creative
writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and
creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be
applied directly to base models without an SFT stage, akin to R1-Zero training.
Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT
recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex
multi-staged pipeline with 25M+ examples. We close with qualitative and
quantitative analyses of how trained models plan their responses. Our results
rethink the post-training pipeline and call upon future work to understand and
employ thinking more broadly.

</details>
