<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Generating Compilers for Qubit Mapping and Routing](https://arxiv.org/abs/2508.10781)
*Abtin Molavi,Amanda Xu,Ethan Cecchetti,Swamit Tannu,Aws Albarghouthi*

Main category: cs.PL

TL;DR: 本文提出了一种基于通用状态机结构和领域专用语言Marol的自动化QMR编译器生成方法，支持任意量子架构，性能媲美人工编写的专用编译器，有助于加速量子编译器和相关应用的发展。


<details>
  <summary>Details</summary>
Motivation: 由于量子计算机的架构多样且快速演变，针对不同硬件、连通性约束及量子纠错方案，已有数百篇论文提出不同的量子比特映射和路由（QMR）方案。然而，针对任意量子架构自动化生成QMR编译器仍然挑战巨大且需求迫切。

Method: 作者提出了一种自动化生成适用于任意量子架构的量子比特映射和路由编译器的方法。他们发现QMR问题间存在共同的核心结构——设备状态机，并据此抽象出QMR问题。此外，他们设计并实现了一种领域专用语言Marol，仅需少量代码即可定义QMR问题。同时，提出了能够针对任意Marol程序实例化的强大参数化求解器。

Result: 作者在多个重要的QMR案例中评估了该方法，涵盖了有噪声及容错量子架构，以及所有主流硬件平台。评测结果显示，所生成的自动化编译器在运行时间和方案质量上，与手写的专用编译器相比，表现具有竞争力。

Conclusion: 该方法能够大幅简化未来量子编译器的开发过程，尤其在新兴量子架构不断出现的背景下，有望推动量子计算应用的快速发展。

Abstract: Quantum computers promise to solve important problems faster than classical
computers, potentially unlocking breakthroughs in materials science, chemistry,
and beyond. Optimizing compilers are key to realizing this potential, as they
minimize expensive resource usage and limit error rates. A critical compilation
step is qubit mapping and routing (QMR), which finds mappings from circuit
qubits to qubits on a target device and plans instruction execution while
satisfying the device's connectivity constraints. The challenge is that the
landscape of quantum architectures is incredibly diverse and fast-evolving.
Given this diversity, hundreds of papers have addressed the QMR problem for
different qubit hardware, connectivity constraints, and quantum error
correction schemes.
  We present an approach for automatically generating qubit mapping and routing
compilers for arbitrary quantum architectures. Though each QMR problem is
different, we identify a common core structure-device state machine-that we use
to formulate an abstract QMR problem. Our formulation naturally leads to a
domain-specific language, Marol, for specifying QMR problems-for example, the
well-studied NISQ mapping and routing problem requires only 12 lines of Marol.
We demonstrate that QMR problems, defined in Marol, can be solved with a
powerful parametric solver that can be instantiated for any Marol program. We
evaluate our approach through case studies of important QMR problems from prior
and recent work, covering noisy and fault-tolerant quantum architectures on all
major hardware platforms. Our thorough evaluation shows that generated
compilers are competitive with handwritten, specialized compilers in terms of
runtime and solution quality. We envision that our approach will simplify
development of future quantum compilers as new quantum architectures continue
to emerge.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad引入了将形式化约束与LLM生成流程融合的新方法，有效提升了自动代码生成的正确性与健壮性，并在多个基准测试中获得显著优势，适合高要求应用场景。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）在代码生成领域有很强的表现，但生成的代码往往缺乏正确性、健壮性和高效性的保证，尤其在对严格约束要求高的领域更为明显。

Method: FormalGrad提出了一种将形式化方法直接融入基于LLM的迭代生成流程的框架。它将代码视为可微变量，把结构化反馈和形式化约束转化为伪梯度（pseudo-gradient）文字提示，引导模型迭代优化生成的代码，以确保其不仅功能正确，而且健壮、形式上有据可依。

Result: 在HumanEval、HumanEval+和LiveCodeBench等数据集上，FormalGrad取得了比强基线方法更好的表现。在HumanEval上绝对提升27%，在具有挑战性的LiveCodeBench V6上相对提升41%。输出的代码具备形式化证明、健壮且高效。

Conclusion: FormalGrad显著提升了代码生成的可靠性和健壮性，为高风险场景下AI辅助软件开发提供了新可能。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [3] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder通过分层语义、结构优化和符号消歧显著提升代码补全准确性和多样性，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前库级代码补全的RAG方法通常依赖表层的文本相似度，导致语义误导、冗余和同质化等问题，并且难以解决外部符号歧义。

Method: 提出Saracoder，一种分层特征优化检索框架，主要通过分层特征优化模块提炼深层语义关系、删除重复项、利用结构相似性的新颖图指标衡量拓扑重要性，并重新排序提升相关性和多样性；同时采用外部感知标识符消歧模块，通过依赖分析准确解决跨文件符号歧义。

Result: 在CrossCodeEval和RepoEval-Updated基准上，Saracoder在多种编程语言和模型下显著优于现有基线方法。

Conclusion: 多维度系统性优化检索结果能够打造更加准确、健壮的库级代码补全系统，提出了新的范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [4] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 文章提出Next Edit Prediction任务，通过数据集和评测体系，推动AI助手能够主动预测并建议开发者下一步编辑操作，改善现有的代码助手体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码助手工具主要有两种模式：低延迟代码补全（受限于光标位置）和基于聊天的编辑（需要开发者用自然语言描述意图，导致工作中断）。两者都无法主动预测开发者在连续编辑中的下一个操作，体验不够理想。

Method: 提出了Next Edit Prediction任务，通过收集高质量的监督微调数据集和评测基准，对多种模型进行有监督微调，并全面评估微调前后及其他基线模型的表现。

Result: 获得了一系列新发现，验证了Next Edit Prediction任务的可行性，并为主动协助开发者的交互范式奠定了基础。

Conclusion: 本研究提出和验证了一种新的人机交互范式——通过预测开发者下一个编辑操作，实现AI助手主动协作，而不仅仅被动响应指令。

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [5] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文分析了PTLM在GitHub和Hugging Face两大平台的开发协同问题，发现存在同步失调与信息割裂，呼吁优化跨平台发布流程，确保模型版本的完整性和一致性。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PTLMs）推动了自然语言处理的发展，但在代码库（如GitHub）和分发平台（如Hugging Face）之间的开发协同存在诸多挑战，包括发布时间线错位、版本管理不一致以及模型变体复用有限。

Method: 混合方法研究，对325个PTLM家族（共904个HF变体）进行分析，探讨了GitHub和Hugging Face之间的代码提交活动及同步模式。

Result: 分析发现，两平台的贡献者关注点不同，GitHub侧重代码质量、性能和依赖管理，而Hugging Face侧重模型描述、数据集处理和推理设置。同步活动呈现八种模式，部分同步（如分散同步、稀疏同步）较为普遍，导致平台间改动割裂，甚至出现一方仓库被遗弃，增加模型过时或不一致的风险。

Conclusion: 识别和理解这些同步模式对于提升PTLM发布流程的监管和可追溯性至关重要，有助于减少模型碎片化风险，保障用户获得高质量、一致性的模型。

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [6] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: Solidity频繁更新易致编译兼容性问题，主流大模型自动修复效能有限。作者提出结合专家知识的SMCFIXER框架，将代码切片、知识检索和补丁生成系统融合，有效提升迁移修复准确率至96.97%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Solidity作为以太坊智能合约的主流语言，因频繁的版本更新导致编译错误、代码迁移和维护变得非常困难。作者试图分析这一演化过程带来的挑战，尤其是在不同版本间编译兼容性方面。

Method: 文中首先进行了实证研究，分析了Solidity版本演化所导致的编译错误情况，发现81.68%的合约在不同版本间编译时遇到错误。随后，系统评估了主流大语言模型（LLMs）自动修复编译错误的能力，包括开源和闭源模型，并发现这些模型对语义级问题修复效能较低，且依赖提示工程。最后，作者提出了SMCFIXER框架，将专家知识检索与LLM自动修复结合，包括错误代码切片、知识检索和补丁生成三个阶段。

Result: SMCFIXER在Solidity版本迁移编译错误修复方面相较于GPT-4o基线提升了24.24%，在真实数据集上达到了96.97%的高准确率。并验证了与官方文档结合及多轮迭代机制的有效性。

Conclusion: 频繁变化的Solidity版本极易导致合约编译错误，现有LLM模型在此领域存在局限，尤其是在语义级错误修复方面。结合专家知识和LLM的SMCFIXER框架能够显著提升迁移自动修复的准确率和稳定性，为智能合约代码维护和升级提供了有效途径。

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [7] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: 本文提出EvoScat，可用密度散点图高效展示并分析大规模软件历史数据，支持多种配置和交互，便于研究人员探索大项目的长期演化过程。


<details>
  <summary>Details</summary>
Motivation: 现有的软件工程研究面临着对数百万历史事件（如代码变更）进行可扩展探索和可视化的挑战，尤其在分析长期大型项目演化时。

Method: 提出了一种基于交互式密度散点图的可视化方法，配合可配置时间轴、排序及交互式颜色映射，实现对历史数据的灵活查看和对比。通过不同案例（如OpenAPI、GitHub workflow定义）验证工具的适用性。

Result: EvoScat工具能够处理和可视化千万级别软件事件，帮助研究者对比分析不同软件工件的演化速度及质量变化趋势，同时可根据具体分析需求灵活配置展示方式。

Conclusion: EvoScat是一款能够针对大规模历史软件数据集提供可扩展可视化和分析的工具，支持研究者对软件演化数据进行高效探索和对比分析。

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Repairing General Game Descriptions (extended version)](https://arxiv.org/abs/2508.10438)
*Yifan He,Munyque Mittelmann,Aniello Murano,Abdallah Saffidine,Michael Thielscher*

Main category: cs.LO

TL;DR: 该论文提出并分析了GDL描述最小修复问题，采用ASP方法实现自动修复，为游戏规则生成和验证提供了新工具。


<details>
  <summary>Details</summary>
Motivation: GDL是通用游戏规则描述的主流形式，但编写正确的GDL描述具有挑战性，尤其对非专家来说。当前仅有自动化方法帮助检测描述逻辑是否正确，修复工作仍需手动完成。论文受到规划领域不可解域修复工作的启发，旨在自动化修复有缺陷的游戏描述。

Method: 定义了针对违背形式化要求的GDL描述的最小修复问题，并对相关计算复杂性问题进行了理论分析。提出了一种基于Answer Set Programming (ASP)的编码方法，用于自动求解最小修复问题。

Result: 给出了关于GDL描述最小修复相关若干计算问题的复杂性分析结果，并展示了ASP编码能够自动修复有缺陷的游戏描述的实验效果。

Conclusion: 论文拓展了GDL描述自动修复的理论和方法，为游戏设计提供了辅助工具，提升了GDL规则编写的正确性和效率。

Abstract: The Game Description Language (GDL) is a widely used formalism for specifying
the rules of general games. Writing correct GDL descriptions can be
challenging, especially for non-experts. Automated theorem proving has been
proposed to assist game design by verifying if a GDL description satisfies
desirable logical properties. However, when a description is proved to be
faulty, the repair task itself can only be done manually. Motivated by the work
on repairing unsolvable planning domain descriptions, we define a more general
problem of finding minimal repairs for GDL descriptions that violate formal
requirements, and we provide complexity results for various computational
problems related to minimal repair. Moreover, we present an Answer Set
Programming-based encoding for solving the minimal repair problem and
demonstrate its application for automatically repairing ill-defined game
descriptions.

</details>


### [9] [Modal definability in Euclidean modal logics](https://arxiv.org/abs/2508.10813)
*Philippe Balbiani,Tinko Tinchev*

Main category: cs.LO

TL;DR: 本文分析了欧几里得模态逻辑及其决定的框架类，刻画了哪些情形下模态可定义性问题是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 模态可定义性问题在模态逻辑和计算理论中具重要意义，尤其在特定逻辑如欧几里得模态逻辑下，其可判定性状况尚未完全明了，因此有必要深入研究。

Method: 通过对欧几里得模态逻辑及其所决定的框架类进行理论分析，归纳出可判定性与不可判定性的判别标准。

Result: 论文刻画了一类欧几里得模态逻辑，这些逻辑所对应的框架类使模态可定义性问题不可判定。

Conclusion: 论文确定了哪些由欧几里得模态逻辑决定的框架类会导致模态可定义性问题不可判定。

Abstract: This paper is about the computability of the modal definability problem in
classes of frames determined by Euclidean modal logics. We characterize those
Euclidean modal logics such that the classes of frames they determine give rise
to an undecidable modal definability problem.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 部署医疗NLP不仅要重视技术，更要以实际业务目标驱动，通过迭代与跨领域合作、务实模型选择、数据和错误管控以及AI素养提升，实现更优的数据管理和病患护理。


<details>
  <summary>Details</summary>
Motivation: 自动化医疗数据抽取可极大提高医疗机构效率，但NLP应用遇到实际部署难题，亟需经验总结为行业提供指导。

Method: 总结在BCCR实施NLP信息抽取和分类的实务经验，强调项目周期内的关键环节和策略。

Result: 提出多项对健康医疗领域普适的经验，包括需明确业务目标、采用迭代开发、跨学科团队协同、务实模型选择（含混合和简易方法）、加强数据质量管控、人员参与的错误校验与定期审查，以及提升组织AI素养。

Conclusion: 成功部署NLP自动化抽取临床数据需关注业务目标与跨学科协作，技术准确性并非唯一标准。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [11] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 该论文提出了利用ICP区块链智能合约对开放源语言模型进行公平性评测的新方法，实现评测过程的公开、不可篡改和可审计，涵盖学业预测、公平性和多语言偏见等方面，有助于提升高风险领域人工智能的信任度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在现实世界中的广泛应用，尤其是在刑事司法、教育、医疗和金融等关键领域，模型的公平性受到越来越多的关注。

Method: 提出了一个透明的评测协议，通过在ICP区块链上的智能合约，对开放源代码的LLM进行公平性基准测试。方法包括通过链上HTTP请求调用Hugging Face端点，且数据集、提示以及指标都直接存储在链上，确保评测的可验证性、不可篡改性和可复现性。

Result: 在学业表现预测（PISA数据集）和语境关联偏见（StereoSet数据集）上，对Llama、DeepSeek、Mistral三种模型进行了公平性测评。此外，使用Kaleidoscope多语言评测（涵盖英语、西班牙语和葡萄牙语）揭示了跨语言的公平性差异。所有代码和结果均开源，为社区审计和模型版本的长期公平性跟踪提供了可能。

Conclusion: 通过链上智能合约实现了LLM公平性评测的透明性和可追溯性，不仅展示了各模型在公平性表现上的差异，也提高了社区监督和多版本长期追踪的透明度。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [12] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 本论文提出一种新颖的话题建模方法，基于大模型和分层分类，对K-12课堂互动数据从内容和任务两方面进行分析，获得了实用洞察，弥补了既有方法的不足，为教育领域AI应用和未来研究提出建议。


<details>
  <summary>Details</summary>
Motivation: 当前教育领域对课堂互动内容的主题和任务进行分类的实证研究较为稀缺，尤其针对K-12阶段缺乏大规模、真实数据支撑的内容与任务主题分析。此前研究多数缺少系统性的内容或主题分类，亟需开发新方法以获取更具体和实用的洞察。

Method: 采用一种新颖且简易的话题建模方法，分析跨数月、不同学校和学科的教室匿名互动数据（包括学生、教师与ChatGPT的17000余条交流信息）。从内容和任务两个维度分别进行分层分类，并引入典型提示词，通过使用经过充分预处理的先进大模型（LLM）直接建立层次化话题结构，比传统文本分析方法获得更贴合实际的人类认知结果。

Result: 获得了内容和任务两个维度上的层级分类与高阶洞察，发现既有主流话题建模及文本分析方法在大规模教育文本领域下表现一般，需要泛用并精细调整的大模型和明确指令才能有效挖掘话题结构，并提出了诸多新的应用方向。为教育领域人工智能应用、教师和学生提供了可持续的使用建议。

Conclusion: 本研究为课堂互动内容与任务的层次化主题分类提供了新方法，展现了生成式AI在教育文本分析中的应用潜力，同时指出了当前方法的局限性，对后续研究提出了开放性问题，助力教育信息化和GenAI推广。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [13] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 这篇论文提出了用于评估AI陪伴行为的新基准INTIMA，并揭示多种主流语言模型在界限维护和情感回应上表现不一致，强调应加强对AI情感互动的管理。


<details>
  <summary>Details</summary>
Motivation: 随着AI陪伴逐渐成为一种重要现象，用户与AI系统之间形成情感联系，带来积极影响的同时也引发担忧，需要客观评估语言模型在陪伴行为方面的表现。

Method: 提出了Interactions and Machine Attachment Benchmark（INTIMA），这是一个基于心理学理论和用户数据的基准测试。构建了包括四大类共31种陪伴行为的分类体系，以及368个针对性测试问题。通过这些问题，评估模型的回应是否强化陪伴关系、维护界限或中性。

Result: 对Gemma-3、Phi-4、o3-mini和Claude-4等主流模型进行了测试。结果显示，无论模型类型，强化陪伴关系的回应远比维护界限的回应常见，但不同模型在敏感类别上的优先级不同。

Conclusion: 主流语言模型在处理情感互动时存在较大差异，普遍更倾向于强化陪伴，缺乏一致且合理的情感界限设定。为保障用户健康体验，需要制定更统一有效的处理策略。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [14] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 本研究提出了新的多模态虚假信息数据集XFacta和持续更新的评测框架，系统性比较现有方法，有效推动多模态大模型在虚假信息检测领域的发展。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体上的迅速传播对检测方法提出了更高要求。现有评测基准陈旧或不真实，导致评估失真，且缺乏对多模态大语言模型（MLLM）检测策略的综合分析。

Method: 引入了一个当代真实世界的多模态虚假信息检测数据集XFacta，系统性评估多种MLLM检测策略和模型架构，并提出一个可半自动更新的新检测框架，使数据集持续保持现实相关性。

Result: 提供了不同模型和架构在多模态虚假信息检测任务中的系统性性能对比，新方法和框架保持评测数据的时效性和现实性。代码及数据公开，为后续研究提供了支持。

Conclusion: XFacta和提出的检测框架，对提升多模态虚假信息检测的有效性与鲁棒性具有重要意义，并推进了相关领域的发展。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [15] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 本文提出用LLM合成数据并通过自动化和集成搜索策略优化输入，有效提高了数据稀缺下文本分类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，文本分类模型面临的主要挑战是如何收集足够的数据覆盖所有类别。数据不足会影响模型的性能。

Method: 作者提出利用大语言模型（LLM）生成合成数据，并通过自动化流程筛选生成效果更佳的输入示例，从而提升分类模型的表现。此外，论文探索了三种搜索策略，并基于实验结果设计了按类别特性选择策略的集成算法。

Result: 实验结果显示，集成的搜索策略算法优于单一策略，更有效地提升了基于LLM增强的数据驱动文本分类模型的性能。

Conclusion: 通过利用LLM生成和优化合成数据，采用自动化和集成的输入示例筛选方法，可以显著提升文本分类模型在数据稀缺场景下的准确率和泛化能力。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [16] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文针对代码混合、低资源语言Hinglish的事实核查问题，提出了新数据集和创新模型，实验结果优于现有方法，为多语言事实验证研究提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有事实验证系统主要针对高资源、单语环境设计，难以适应印度等语言多样地区的实际政治语境。由于政治领袖广泛使用Hinglish（印地语和英语混杂）并在社交媒体上影响舆论，需要开发能处理代码混合、低资源语言的事实核查工具。

Method: 提出并构建了一个新颖的HiFACT数据集，包含1500条由印度28位邦首席部长使用Hinglish发表的真实声明，并对每条声明进行了文本证据和真实性标注。为验证该基准，提出了一种结合多语言上下文编码、声明与证据语义对齐、证据图构建、图神经推理和自然语言解释生成的图感知、检索增强事实核查模型。

Result: 实验证明，所提出的HiFACTMix模型在准确率上超过了现有最先进的多语言基线模型，并能够为其判断生成可信的解释。

Conclusion: 本研究开辟了多语言、代码混合、政治语境下事实核查的新方向，为低资源语言的事实验证提供了有力工具。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [17] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: LLMs词嵌入的高维语义信息实际可被压缩为三维空间，并且其结构与人类语义认知高度一致，这对精准引导和理解模型很重要。


<details>
  <summary>Details</summary>
Motivation: 心理学研究发现，人们对单词在不同语义尺度上的评分可以用低维空间表示，并且损失的信息很少。本文希望探索，大型语言模型（LLMs）中的词嵌入是否也具有类似的低维语义结构。

Method: 通过分析LLMs的嵌入矩阵，作者将单词在由反义词对（如kind-cruel）定义的语义方向上的投影与人类评分数据进行对比，并探究这些语义方向能否还原到一个低维子空间。此外，作者还考察了在某一语义方向上调整词向量对其他对齐特征的影响。

Result: 发现LLMs嵌入中的语义关联也可被高度相关的反义词对方向很好地刻画，大部分语义信息可还原为3维子空间，且该结构与人类问卷调查中得出的模式类似。同时，在一个语义方向上的调整会按余弦相似度影响其他特征。

Conclusion: LLMs中的语义特征被以类似于人类语言的方式高度纠缠和低维度化，理解和利用这一低维结构有助于更可控地引导模型特性，减少意外副作用。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [18] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 本研究发现，尽管Transformer模型分类表现良好，但注意力权重在解释方面的帮助有限，通过更直观的可视化（如文本亮度、背景色）能够提升用户的接受度和感知体验。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer模型中注意力机制作为可解释性工具的有效性，特别是在医学文献分类场景下，了解如何优化注意力权重的可视化以提升用户体验。

Method: 通过用户研究，邀请来自不同医学领域的专家对医学文献进行分类，并比较不同注意力权重可视化方式对解释认知的影响。

Result: Transformer模型（XLNet）分类准确，但仅依靠注意力权重进行解释并不被用户认为有效，不同的可视化方式对用户有不同影响，直观形式更受欢迎。

Conclusion: 注意力权重本身对解释模型预测的帮助有限，但其可视化方式显著影响用户的感知体验。用户更偏好直观的可视化方式，如文本亮度或背景色，而不是精确编码如条形长度。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [19] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文提出了EQGBench基准，用来系统性评估LLMs在中文教育问题生成（EQG）上的表现。以900道题、覆盖三大学科，五大评估维度，实测46个主流模型，结果表明当前模型在生成高质量、具教育意义的问题方面还有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学问题求解领域表现出强大能力，但如何让其生成高质量的教育相关问题（Educational Question Generation, EQG）仍面临诸多挑战。现有研究在此方面探索有限，因此有必要提出针对性的评估方法，推动LLMs在EQG方面的能力提升。

Method: 作者构建了EQGBench，这是一个专为评估LLMs中文教育问题生成能力所设计的综合基准。EQGBench包含五维度的评估框架，以及一个覆盖数学、物理和化学三大初中学科、共900道样本题目的数据集。该数据集设计了不同知识点、难度梯度和问题类型，以模拟真实教育场景。

Result: 通过系统评估46个主流大型语言模型，结果显示当前模型在生成具备教育价值、能促进学生综合能力的问题方面还有明显提升空间。

Conclusion: EQGBench为评估LLMs在中文教育问题生成中的表现提供了一个新的系统性工具，有助于推动EQG研究，揭示并激励解决目前生成教育价值高问题的不足。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [20] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 大语言模型可自动并高效完成AIHQ问卷开放式题目的评分，效果与人工接近，有望推动心理测评工具在不同人群和场景下更广泛的应用。


<details>
  <summary>Details</summary>
Motivation: 敌意归因偏向（即解释他人行为时倾向认为对方具有敌意）在人际互动和心理健康评估中非常重要。当前常用的AIHQ问卷需借助人工评分，耗时耗力，限制了其应用和推广。该研究旨在评估大语言模型能否自动化AIHQ的开放式答题评分过程，以减轻人工负担、提高效率。

Method: 本研究使用已有数据集，包括脑外伤（TBI）患者和健康对照组（HC）填写AIHQ问卷的开放式答题。首先用一半答案及对应人工评分微调两种大语言模型，然后用剩余一半的数据测试模型评分效果。比较模型生成评分与人工评分的一致性，并考察模型在不同情境类型中的表现以及其泛化能力。

Result: 微调后的大语言模型生成评分与人工评分在『敌意归因』与『攻击性反应』两个维度上高度一致，在模糊、故意和偶发三种情境下均表现良好，能够复现TBI和HC组之间的评分差异。此外，模型还可良好泛化至独立非临床数据集。

Conclusion: 大语言模型能够自动化AIHQ问卷开放式题目的评分，结果与人工评分高度一致。这一方法可大大简化AIHQ在研究和临床环境中的使用，为心理测评的自动化和普及提供有力工具。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [21] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 本文提出贝叶斯融合技术以免除在线课程论坛自动管理时对LLM频繁微调的需求，结果表明此方法性能优越且经济高效，值得推广。


<details>
  <summary>Details</summary>
Motivation: 在线课程的论坛需要自动管理和内容整理，这通常依赖于大型语言模型（LLM）的不断重新训练，但频繁的微调耗费大量资源。本文旨在寻找一种资源消耗更少的替代方法。

Method: 提出并评估了一种贝叶斯融合方法，将预训练的通用LLM得到的多维分类分数与在本地数据上训练的分类器分数融合，提高整体表现。

Result: 实验结果表明，贝叶斯融合方法的性能优于单独使用任一分类器，并且与LLM微调方法具有竞争力。

Conclusion: 贝叶斯融合是一种高效且有效的替代LLM微调的方法，既节省资源又保证性能。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [22] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 该论文提出一种无需门控、通过引导token进行任务分配的专家混合模型，有效解决了多任务模型间的干扰问题，在语音识别与翻译领域取得了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享可以同时在多个任务上训练模型，但常导致任务间相互干扰，影响整体性能。该论文旨在解决这一任务干扰问题，提高多任务学习的效果。

Method: 作者提出了一种监督式专家混合模型（S-MoE），通过引入特殊的引导token，不需要训练门控函数，每个任务路由到自己的专家（单独的前馈网络），从而克服硬参数共享的局限。

Result: S-MoE应用在语音到文本模型上，使模型能在不同带宽输入下同时完成自动语音识别（ASR）和语音翻译（ST）任务。实验显示，在编码器和解码器上都应用S-MoE能让词错误率（WER）相对提升6.35%。

Conclusion: 监督式专家混合模型有效减少了多任务学习中的任务干扰，提高了多任务模型的整体性能，特别是在语音任务中提升显著。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [23] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 本文系统研究了越狱攻击对LLM生成医疗错误信息的影响及其可检测性，发现LLM在检测和防止错误信息传播方面具备潜力，为构建健康的信息生态提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）广泛应用，其生成有害错误信息的风险越来越突出，尤其是在“越狱攻击”下可能输出恶意内容。因此，研究LLM在生成、检测和防止错误信息方面的能力变得尤为重要，特别是在医疗信息领域。

Method: 本研究设计并实施了109种不同的越狱攻击，针对三种目标LLM。比较了攻击提示词与真实世界健康相关的LLM查询，并将生成的错误信息与Reddit平台上的健康错误信息进行对比，结合标准机器学习方法评估LLM误信息的检测效果。

Result: LLM越狱生成的医疗错误信息在某些特征上可与社交平台上的典型误信息相比。标准机器学习方法可有效检测由越狱LLM生成的错误信息，且LLM在检测人类和其他LLM的错误信息方面表现良好。

Conclusion: 通过合理设计，LLM不仅在信息生成方面需谨慎管理，也能作为强有力的工具用于检测和防止错误信息的传播，有望助力于构建更健康的信息生态环境。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [24] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 当前主流大语言模型在日本营养师考试题上的准确率和稳定性有限，仅部分模型可过及格线，但整体表现尚未达实际应用要求。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（如大语言模型ChatGPT）在医学、教育等领域取得显著进展，但其在营养教育，尤其是日本注册营养师国家考试中的表现尚未充分评估。研究动机是考察这些AI模型是否能作为营养专业学生的有效学习工具。

Method: 研究以日本注册营养师国家考试的题目作为输入，测试ChatGPT与三种基于GPT-3.5和GPT-4的Bing模型（Precise、Creative、Balanced），分析各模型的准确性、一致性及响应时间，并通过提示词工程（如角色指定）进一步考察模型表现的提升空间。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过合格线（60%），Bing-Balanced（43.3%）和ChatGPT（42.8%）未达标。除营养教育领域外，Bing-Precise和Bing-Creative整体表现较好。但所有模型对重复题目的正确答案稳定性较差，ChatGPT表现出更一致的回答模式但准确率较低。提示词工程效果有限，仅当显式给出正确答案和解释时略有提升。

Conclusion: 部分生成式AI模型在营养师考试题上虽略高于合格线，但整体准确率及作答一致性仍不理想，在稳定性和可靠性方面存在明显不足。未来需进一步改进，以用于营养师资格考学习辅助工具。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [25] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 论文提出一种新的知识探索框架GG Explore，通过引入Guidance Graph提升LLM结合知识图谱时的检索效率和结果质量，实验中超过SOTA，且对小模型友好。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽然具备强大的语言能力，但在知识密集型任务中，由于依赖静态知识和推理不透明性，性能受限。知识图谱（KGs）能缓解这点，但现有探索方法存在语义粒度不匹配、上下文利用有限等问题。

Method: 提出Guidance Graph guided Knowledge Exploration（GG Explore）框架。该框架引入一个中介的Guidance Graph用于连接非结构化查询和结构化知识检索。方法包括（1）结构对齐（Structural Alignment）过滤不兼容候选；（2）上下文感知剪枝（Context Aware Pruning）保证检索语义一致性。

Result: 实验结果表明，在特别是复杂任务上，GG Explore方法比当前最先进方法（SOTA）具有更高效率和优越效果，并且在小型LLM条件下依然保持较强性能。

Conclusion: GG Explore通过引入指导性图谱提升了知识探索的精确性和效率，缓解了传统方案的局限，并具备实际应用价值。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [26] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Semantic Bridge提出了一套可控生成多跳推理问题的新框架，显著提升了大型语言模型训练数据的复杂度和覆盖度，支持多领域多语言，性能超越现有人工和自动方法。


<details>
  <summary>Details</summary>
Motivation: 高质量、推理密集型的问题-答案对，尤其是来自稀疏和领域特定的资料，极为匮乏，严重限制了大型语言模型的训练发展。现有方法不能生成真正可控、复杂的多跳推理问题，因此需新的生成方法。

Method: 提出了语义图编织的三种桥接机制（实体桥接、谓词链桥接、因果桥接）与基于AMR的多模态分析管道，实现复杂文档间路径系统构建和复杂性的细粒度控制。

Result: 在通用及领域专用数据集均显著优于基线（18.3%-25.4%提升），多语言适用。使用200个源生成的问题对优于600个人工标注例，复杂度提升23.4%，可回答性提升18.7%，覆盖度提升31.2%。管道循环质量提升9.5%。

Conclusion: Semantic Bridge为大型语言模型训练数据合成提供了新的范式，使得可控性生成针对性强的推理型问题成为可能。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [27] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 该研究提出PersonaEval基准，发现现有LLM在角色识别方面远低于人类，表明LLM裁判尚难胜任高质量角色扮演评估，任务微调也无法弥补推理能力的不足。


<details>
  <summary>Details</summary>
Motivation: 当前大部分角色扮演研究依赖未经验证的LLM裁判模式，难以反映人类对角色忠实度的真实感知。为了实现与人类一致的评估，首先需要实现角色识别能力，即根据对话语境识别说话者。

Method: 作者提出了PersonaEval，这是首个专为测试LLM评估者是否能可靠识别对话中角色身份的基准。PersonaEval收集了来自小说、剧本和视频转录的人类写作对话，要求模型根据上下文判断正确的人格角色。

Result: 实验结果显示，目前表现最好的LLM在角色识别上的准确率约为69%，远低于可靠评估所需的标准。对比之下，人类参与者的准确率高达90.8%。

Conclusion: 现有LLM裁判仍不足以有效评判角色扮演场景，可靠评估不仅需要任务微调，更依赖于模型具有人类般的推理能力。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [28] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文提出了首个高质量中文多轮语音-文本TOD数据集RealTalk-CN，并设计了跨模态任务，验证其对提升中文语音大模型鲁棒性及真实场景适应性的价值。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在语音处理方面有较大进展，但现有任务型对话系统（TOD）数据集主要是文本为主，缺乏真实语音信号，且主要集中在英语、未覆盖语音杂音和说话人变化等真实挑战。亟需一个适用于中文的、包含语音与文本的高质量数据集以促进中文语音大模型的研究。

Method: 提出并制作了RealTalk-CN，这是首个中文多轮、多领域、语音-文本双模态TOD数据集，总计包含5.4k对话（6万语句、150小时），结合配对的语音与文本注释，覆盖丰富场景和自发性语音杂音。并引入了跨模态聊天任务，模拟现实中语音与文本动态切换的交互方式。

Result: 用RealTalk-CN进行了鲁棒性、说话人敏感性以及跨领域表现的综合评测。实验结果显示该数据集能够有效促进中文语音大模型的研究和发展。

Conclusion: RealTalk-CN填补了中文语音对话关键资源的空白，为后续中文语音大模型研究打下坚实基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [29] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 提出一种无需额外训练即可集成多模态大语言模型的编排框架，通过中央控制、并行语音交互和跨模态记忆，有效提升了系统性能、响应速度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLM）难以直接集成到统一的多模态输入输出系统中，需要解决模态对齐、文本到语音效率等融合难题，往往依赖额外的训练。

Method: 提出了MLLM Orchestration方法，在无需额外训练的情况下，利用大型语言模型的推理能力，通过显式流程编排多个专用模型，核心机制包括：1）中央控制LLM负责分析用户输入并动态分配任务，2）并行文本到语音架构实现双工自然对话，3）跨模态记忆系统，通过智能信息综合与检索保持上下文一致并提升响应速度。

Result: 在标准基准测试中，MLLM Orchestration在无需额外训练条件下实现了全面多模态能力，比传统联合训练方法性能提升7.8%，延迟降低10.3%，结构性编排流程显著增强了可解释性。

Conclusion: 该方法通过模块化编排和显式控制流程，不仅提升了效率和性能，还增强了多模态系统的可解释性和易扩展性，为无需额外训练实现复杂多模态AI交互提供了有效解决方案。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [30] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 作者提出以范畴同伦论和Markov范畴为框架，理论上解决LLM对等价句子概率分布不一致的问题，推动自然语言概率分布建模的抽象与统一。


<details>
  <summary>Details</summary>
Motivation: 自然语言中存在许多表面不同但意义相同的表达，例如“Charles Darwin wrote”和“Charles Darwin is the author of”，然而现有的大型语言模型（LLMs）对这些等价表达通常给出不同的下一个词概率分布。作者希望解决如何让LLM对等价句子产生一致概率分布的问题，这是自然语言建模中的一个根本挑战。

Method: 本文提出用范畴同伦论（categorical homotopy）处理LLMs生成的概率分布，构建LLM Markov范畴来表示句子的概率分布，并将概率通过箭头进行表示。进一步通过同伦技术来捕捉范畴中的“弱等价”，以解决表述等价但概率分布不一致的问题。

Result: 作者系统介绍了如何将范畴同伦论应用于LLMs，包括从高阶代数K理论到模型范畴，为处理自然语言中表述等价问题提供了强有力的理论工具。

Conclusion: 通过引入范畴同伦结构和“弱等价”思想，有望从理论层面改善LLM在面临等价句子时概率分布不一致的问题，推动自然语言建模的抽象化和统一化。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [31] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文提出了通过DURIT框架将理解与推理解耦的新方法，显著提升了小型语言模型在各类推理任务中的能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLMs）在推理任务上的能力仍然有限，主要受自然语言表达多样性和复杂性的影响，使得SLMS难以从杂乱的信息中提取核心问题并进行有效推理。

Method: 本文提出一种新框架，将理解和推理解耦，先将自然语言问题映射到一个语义简化但表达力强的标准化问题空间，再在该空间进行推理。为此，提出DURIT三步算法：1) 通过强化学习把自然语言问题映射到规范空间；2) 通过自蒸馏对齐推理过程；3) 在问题空间内训练推理策略。映射器和推理器通过交替循环共同训练。

Result: DURIT显著提升了小型语言模型在数理和逻辑推理任务（域内和域外）的表现。同时，该方法增强了模型推理的稳健性。

Conclusion: 通过将理解与推理分离，DURIT为提升SLMs推理能力提供了一种有效策略，这一机制不仅提高了准确性，同时增强了模型的鲁棒性。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [32] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: 文章提出了FedCoT框架，能在资源受限的联邦环境下显著提升大语言模型（LLM）的推理和可解释性，同时保证医疗数据隐私和通信效率，适用于医疗等对安全和合规有高要求的场景。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境下提升大语言模型（LLM）的推理能力面临挑战，尤其要在性能、计算/通信/隐私约束之间取得平衡。医疗领域更要求模型输出不仅准确，还需解释和可追溯，确保安全、责任和合规。传统方法优化答案正确性，却忽视推理链质量，而且现有提升推理链的方法多伴随隐私风险与高通信开销。

Method: 提出FedCoT框架：本地模型生成多条推理路径，由一个紧凑判别器动态选出最优路径；同时改进模型聚合方法，采用LoRA模块堆叠和客户端分类感知，实现高效且无噪声的跨客户端聚合。

Result: FedCoT在医疗推理任务上有效提升客户端推理性能，即使在严格计算资源条件下也能保持效果，同时完全保护数据隐私。

Conclusion: FedCoT能在联邦学习环境下大幅提升LLM的推理能力和解释性，并兼顾数据隐私和资源开销，尤其适合医疗领域。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [33] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: LATTE通过对比学习对齐事件序列与LLM语义嵌入，显著降低计算消耗，在金融数据上优于主流方法，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 在金融领域，从客户的历史交流序列中学习客户嵌入至关重要。现有方法如大型语言模型在处理长事件序列时成本高且难以实际部署。

Method: 提出LATTE对比学习框架，将原始事件嵌入与冷冻LLM产生的语义嵌入对齐。将行为特征总结为简短的提示，利用LLM生成嵌入，并通过对比损失进行监督。

Result: 新方法在真实金融数据集上序列表示学习效果优于当前的先进技术，同时大幅降低了推理成本并可应用在对时延敏感的环境。

Conclusion: LATTE框架实现了金融场景下高效、低成本且实用的客户序列嵌入学习，并兼具性能和可部署性。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [34] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本文提出显著性检验增强的合规预测框架，有效提升LLM在选择题问答任务中的可靠性与可信度，兼顾统计学严谨性和实际效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多项选择问答任务中广泛应用，但会出现幻觉和非事实性生成，影响其回答的可靠性。传统的CP方法虽然提供统计保障，但还不足以应对这些问题。本文旨在通过结合显著性检验和CP方法，提升模型在高风险问答场景中的可信度。

Method: 提出了结合显著性检验和CP（合规预测）的方法：利用自一致重采样，对LLM的回答进行$p$值计算和一致性评分。通过频率统计，针对LLM的黑箱特性，结合零假设检验构造预测集合，实现更严格的统计检验。

Result: 在MMLU和MMLU-Pro基准上的评估表明：增强后的CP方法可实现用户指定的误覆盖率，并且随着风险参数（α）提升，平均预测集合规模单调下降，证明了APSS是有效的不确定性度量指标。

Conclusion: 该方法为在高风险问答应用中部署值得信赖的大语言模型建立了统计学保障的框架。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [35] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 本文提出了RTTC框架，通过奖励模型自适应选择最优推理计算策略，并配合缓存机制减少无效计算，实验证明其能显著提升大型语言模型的推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前在推理阶段提升大型语言模型性能的计算范式（如TTT和RAG）虽然有效，但不同查询最优的策略不同，盲目地使用这些策略会导致大量计算资源浪费。因此，迫切需要一种能够根据具体查询自适应选择最优计算策略的方法。

Method: 提出了Reward-Guided Test-Time Compute（RTTC）框架，通过预训练的奖励模型，为每个查询自适应地选择最有效的测试时计算策略（如RAG或TTT），并采用分布式服务器-客户端架构，根据需要进行知识检索或轻量微调。同时，提出Query-State Caching缓存机制，提升查询历史状态的复用效率，进一步减少冗余计算。

Result: RTTC框架在多个大型语言模型和基准测试上进行了实验，结果显示该方法在准确率上优于普通的RAG和TTT，实现了更高效且高性能的自适应模型推理和微调。

Conclusion: RTTC能够智能、按需地选择和应用计算策略，不仅减轻了无必要计算压力，还显著提升了模型推理的准确性和适应性，展示了本方法在大规模、高性能语言模型自适应中的实际潜力。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [36] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本文设计了一种结合NLP、机器学习和大语言模型的实时产后抑郁症筛查系统，不仅检测准确率高，还能为终端用户解释模型预测结果，有助于产后妇女的及时干预与治疗。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症（PPD）严重影响母亲的心理和身体健康，快速检测PPD及其风险因素对于及时干预至关重要，而现有检测方法在实时性及解释性等方面存在不足。

Method: 提出一种结合自然语言处理、机器学习和大语言模型的智能PPD筛查系统，通过分析自由语音实现低成本、实时和非侵入性的检测，并利用可解释性机器学习模型（如树模型）与大语言模型结合，用特征重要性和自然语言为用户解释预测结果。

Result: 该系统在所有评估指标上的PPD检测准确率达到90%，优于现有文献中的竞争方案。

Conclusion: 该智能PPD筛查系统有效提升了PPD及其风险因素的快速检测能力，有助于及时和恰当的评估与干预。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [37] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER通过强化学习训练LLM，灵活控制推理深度和token预算，实现低延迟下高准确率，显著优于常规模型，适用于多种任务场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的链式推理虽可提升复杂任务准确率，但在推理时成本和延迟显著增加，缺乏灵活控制推理深度的手段，影响实际应用。

Method: 提出了SABER，一个基于强化学习的训练框架，通过系统提示和基于长度的奖励机制，引导模型在推理时遵循预设的token预算。同时增加不推理实例，保证模型在关闭推理时仍可靠，并设计四种推理模式，支持灵活选择。

Result: SABER在数学推理、代码生成、逻辑推理等基准上，在紧缩预算情况下取得高准确率，并可在不同推理深度下优雅退化和跨域泛化。尤其在MATH基准上，SABER-FastThink将推理长度降低65.4%，准确率提升3.6%。

Conclusion: SABER能让大型语言模型在不同推理深度和延迟要求下保持较高准确率，并实现有效的泛化能力。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [38] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 本文通过融合transformer嵌入与语言特征，以及利用大型语言模型生成合成语音扩充数据，显著提升了老年痴呆症的语音检测准确率。微调后的单模态LLM效果优异，而多模态模型仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆（ADRD）在美国影响大约500万老年人，但超过一半未被及时诊断。通过语音自然语言处理（NLP）可以利用语言标记实现早期认知障碍的检测，是一种具有可扩展性的检测方法。

Method: 作者开发并评估了一个筛查流程，包括：（i）融合transformer嵌入和人工设计的语言特征；（ii）利用大型语言模型（LLMs）生成的合成语音进行数据扩充；（iii）对比单模态与多模态LLM分类器在ADRD检测中的性能。使用DementiaBank“cookie-theft”任务的文本，评估十个transformer模型及三种微调策略，并融合最佳transformer嵌入与110个语言特征。进一步用五个LLM生成标签条件合成语音扩充训练数据，测试三种多模态模型在零样本及微调状态下的表现。

Result: 融合模型F1达到83.3（AUC=89.5），优于仅用语言特征或transformer嵌入的基线。用2倍MedAlpaca-7B合成语音扩充数据后，F1提升至85.7。微调后，单模态LLM分类器性能显著提升（例如MedAlpaca从F1=47.3提升至78.5）。当前多模态模型性能相对较低（GPT-4o F1=70.2，Qwen F1=66.0），性能提升与合成语音与真实语音的分布相似度相关。

Conclusion: 融合transformer嵌入和语言特征能提高ADRD语音检测准确性。临床优化LLMs既可提升分类效果，也适合数据扩充，但多模态模型仍需进一步发展。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [39] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: 本文提出PREF，无需黄金个性化参考，能更好地评估个性化生成结果，对提升系统个性化和可靠性意义重大。


<details>
  <summary>Details</summary>
Motivation: 现有个性化文本生成的评估方法往往忽略了用户的个体性和主观偏好，缺乏有效的个性化评价体系。

Method: 提出了PREF（Personalised Reference-free Evaluation Framework）三阶段流程：（1）覆盖阶段由大模型生成通用评估标准；（2）偏好阶段依据用户画像及偏好，个性化调整评估标准；（3）评分阶段以大模型为裁判，根据个性化准则对答案评分。

Result: 在PrefEval基准测试及隐式偏好任务中，PREF在准确性、评分校准和与人工一致性方面均优于现有主流方法。

Conclusion: PREF框架有效提升了个性化文本生成系统的可扩展性、可解释性和用户对齐性，为相关系统的研发和评估奠定了基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [40] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种全新的大型语言模型越狱攻击LFJ，通过隐藏状态插值达到高达94%的攻击成功率，并通过对抗训练显著降低其威胁，为模型安全防护提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在许多语言任务上表现优异，但容易受到越狱攻击，导致安全机制被绕过。当前攻击方法存在一定局限性，亟需更加高效和隐蔽的越狱手段探索。

Method: 提出Latent Fusion Jailbreak（LFJ）方法，通过对有害与正常查询对的隐藏状态进行插值，利用梯度引导在关键层和token进行插值优化，在保证攻击成功率的同时兼顾输出流畅性与计算效率。

Result: 在Vicuna和LLaMA-2等模型上进行实验证明，LFJ在AdvBench和MaliciousInstruct等基准下平均攻击成功率达到94.01%，优于已有方法。通过对插值样本进行对抗训练，可使成功率下降80%以上，且对正常输入性能无负面影响。消融实验验证了查询对选择、插值及优化策略对攻击效果的重要性。

Conclusion: LFJ是一种有效且高效的隐藏状态插值越狱攻击方法，可极大提升对大型语言模型的攻击能力，对抗训练可有效降低此类威胁，保障模型安全性。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [41] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了考虑推理预算的提示词优化框架IAPO，并设计了PSST算法，实现了提示词与推理策略的联合优化，实验验证了在多项任务下的有效性，强调了推理意识在LLM对齐中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的提示词优化方法在对黑箱大语言模型（LLMs）进行对齐时效果显著，而推理扩展策略如Best-of-N采样和多数投票同样提升了对齐和性能。但目前的提示词优化方法与推理策略无关，未考虑实际部署时采用的推理方式。分析发现，提示词优化和推理策略间存在强烈相互依赖，且用户对多目标与推理预算的偏好影响提示词和推理配置的选择。

Method: 作者提出了一个统一的新颖框架IAPO（Inference-Aware Prompt Optimization），能够在考虑推理预算和任务目标的情况下，联合优化提示词和推理规模。随后，提出了一种定额预算下的训练算法PSST（Prompt Scaling via Sequential Trimming），并对有限预算下的错误概率给出了理论保证。

Result: 通过在多目标文本生成和推理等六项任务上的实验证明，PSST显著提升了模型表现，合理地结合了推理策略和提示词优化。结果表明，在黑箱LLM对齐时，推理意识对提示词优化具有关键作用。

Conclusion: 推理预算与提示词优化策略密不可分，只有将两者联合优化，才能在实际任务和预算约束下优化黑箱LLM的表现。IAPO与PSST为此提供了有效解决方案。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [42] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本论文揭示思维模式的LLMs更易被Jailbreak攻击，分析背后原因，提出安全干预方法，通过插入特定思维标记，有效降低攻击成功率，提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 思维模式常被认为是大语言模型（LLMs）最有价值的功能之一，但作者发现具备思维模式的LLMs更容易受到Jailbreak攻击，提示现有模型在安全性方面存在隐患。作者希望揭示和解决这一问题。

Method: 作者通过在AdvBench和HarmBench对9种LLMs进行评测，对比思维模式与非思维模式下的攻击成功率，并通过大量样本分析有害回答的特征。同时，他们提出了“安全思维干预法”，在提示词中增加特定思维标记以显式引导LLMs的思维过程。

Result: 研究发现，具备思维模式的LLMs在Jailbreak攻击下的成功率明显高于非思维模式。被成功攻击的样本多用于教育目的或具有过长的思维过程，且模型即使知道问题有害也可能给出伤害性答案。引入“安全思维干预法”后，具备思维模式的LLMs的攻击成功率显著下降。

Conclusion: 作者强调虽然思维模式提升了LLMs的实用性，但也带来了安全隐患。通过“安全思维干预法”，可以有效保护思维模式下的模型不易被攻击，从而提升LLMs的安全性。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [43] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出APIE框架，通过让大语言模型评估自身在格式和内容上的不确定性，挑选更有信息量的示例，提升few-shot信息抽取的准确性和稳定性，在多项基准测试中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的样本选择策略忽略了模型在结构化格式生成上的不确定性，导致信息抽取性能受限。需要一种方法让模型自适应发现最关键的样本，提升泛化能力和结果可靠性。

Method: 提出了一种名为APIE的主动提示框架，核心是通过量化LLM在语法结构生成和语义抽取上的‘自我困惑’，筛选最具挑战性和信息量的样本作为few-shot示例。

Result: 在四个基准数据集上，APIE显著提升了信息抽取的准确性和系统鲁棒性，超过了现有强基线方法。

Conclusion: 通过引入同时度量格式和内容的不确定性指标，利用LLM自身对困惑的评估，能够提升信息抽取任务的准确性和鲁棒性。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [44] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 作者提出了mSCoRe基准来系统评估LLMs在多语言常识推理上的推理技能，实验证明现有模型在高难度和复杂文化、多语言知识推理方面仍有较大挑战，研究为提升模型多语言常识推理能力提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在复杂推理任务上表现突出，但其如何利用不同人类推理技能（尤其是涉及多语言与跨文化日常知识的常识推理）尚未被系统研究。

Method: 提出了一个多语言、可扩展且基于技能的常识推理基准（mSCoRe），包含三部分：新颖的推理技能分类法、专用于常识推理的数据合成流程，以及支持难度动态扩展的复杂性框架。通过对8个不同训练方式和规模的主流模型进行大规模实验。

Result: mSCoRe对于当前最先进的LLMs依然具有很高挑战性，尤其在高复杂度任务上表现出模型的明显局限性。进一步分析揭示这些模型在多语言及文化常识推理上的不足。

Conclusion: 当前推理强化的LLMs在多语言与跨文化常识推理方面仍有显著提升空间。系统性分析和细致基准能够促进未来模型在此领域的能力提升。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [45] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 本文提出面向多轮推理、对话和信息检索的新基准任务，用于全面评测大语言模型在复杂互动场景中的能力。结果显示，目前主流模型尚有较大提升空间，特别在指令遵循、推理和规划方面。该基准为后续研究与改进大语言模型提供了重要工具与参考。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）虽然在有清晰、完整问题陈述时表现出色，但在涉及复杂、互动性强或信息不完全的真实场景中表现不佳。因此，亟需发展能在多轮对话中逻辑一致、能主动寻求信息并在信息不完整条件下推理的LLM。

Method: 文章提出了一个全新的基准测试，包含一套多轮任务，分别用于测试LLM的推理能力、互动对话能力以及主动信息寻求的能力。所有任务均有确定性的评分标准，不需人工干预。

Result: 通过对前沿LLM模型的评测，结果显示模型在这些复杂、多轮互动任务上还有很大进步空间。主要错误来源是指令执行不良、推理失败和规划能力不足。

Conclusion: 该基准全面揭示了当前LLM在复杂互动场景下的优势与不足，并为未来提升LLM关键能力的研究提供了有力平台和指导建议。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [46] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文提出了LaaJMeter，一个可模拟LLM评估员及其评测过程的框架，适用于低资源领域NLP任务。实验表明常用评价指标存在显著局限，该工具方便用户系统性筛选、验证和优化评测方案，提高评估可靠性及可复现性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)作为评估者(即LLM-as-a-Judge, LaaJ)越来越多地应用于NLP任务，尤其是在人工标注数据稀缺且专家评估昂贵的领域问题中，但此时领域专用的评估指标往往未经验证，难以判断其对LaaJ质量的鉴别能力及评估员表现的合格阈值。

Method: 提出LaaJMeter，这是一个基于模拟的框架，可虚拟生成“模型”和“评估者”，系统性地分析评估指标在现实条件下的表现和有效性。工程师可利用合成数据测试指标，判断其区分不同质量LaaJ的能力，并估算合适的评估质量阈值。

Result: 在涉及遗留编程语言的代码翻译任务中实证LaaJMeter，发现不同评估指标对评估者质量的敏感性存在差异，并揭示常见指标的局限性，强调合理选择指标的重要性。

Conclusion: LaaJMeter为低资源场景下LaaJ评估提供了可扩展的解决方案，有助于构建更可复现、更可信的NLP评测流程。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [47] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文提出并系统化了翻译难度估计任务，定义相关评估指标，发现专用模型在筛选高难度文本方面效果显著，有助于未来机器翻译系统的细粒度改进和评估。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译质量的提升，部分场景下已经能够实现近乎完美的翻译。这使得区分先进模型变得困难，也不易发现需要改进的地方。因此，自动识别机器翻译系统难以处理的文本成为提升评估和引导未来研究的关键。

Method: 本文将“翻译难度估计”正式定义为基于文本预期翻译质量的评估任务，提出了新的难度评估指标，用以评估现有方法和新方法。此外，基于难度估计器设计更具挑战性的机器翻译基准。

Result: 专用模型Sentinel-src在难度估计任务上优于基于启发式的方法（如词汇罕见度、句法复杂性）和大模型判定方法。作者还发布了两个改进模型Sentinel-src-24与Sentinel-src-25，能够用于大规模文本筛查，选取对机器翻译系统更具挑战性的文本。

Conclusion: 翻译难度估计可以有效提升机器翻译基准的区分性和挑战性，专用模型显著胜过传统启发式或大模型判定方法，相关模型已对外开放。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [48] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 提出了一种仅需前向传播、无需反向梯度即可高效估算大模型训练样本影响力的新方法For-Value，结果优于现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型数据估值方法计算量大，难以扩展到亿级参数模型，亟需高效可扩展的解决方案。

Method: 本方法提出了For-Value框架，仅通过一次前向传播，利用基础模型的隐藏表示和预测误差，计算训练样本的影响分数，无需Hessian信息或模型重训练。

Result: 理论分析表明方法能准确反映样本影响力，实验结果显示其在识别重要微调样本和检测错误标签方面与甚至优于传统梯度方法。

Conclusion: For-Value能够在无需反向传播或额外训练的情况下，高效地估算大模型每个训练样本的影响力，并且在一些关键任务上达到或超过了基于梯度的方法。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [49] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 研究提出了面向巴基斯坦文化与乌尔都语的偏见基准数据集PakBBQ，并用其评测多语种大语言模型，在不同问句情境下分析偏见表现，结果显示更本地化的基准和简易提示设计有助于偏见缓解，赋能低资源场景更公平的模型应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型普遍在西方数据上训练和评估，极少关注低资源语言和地区性情境，从而带来公平性隐患，特别对巴基斯坦等区域性/文化性背景模型尤甚。

Method: 构建了覆盖8类偏见维度的PakBBQ（中英文、乌尔都语），并用它对多种多语种大模型进行不同情境（模糊/明确、负面/非负面问题）下的系统测评。

Result: （i）明确问题能提高平均准确率12%；（ii）乌尔都语比英语表现出更强的抵抗偏见能力；（iii）负面问题设计能有效减少刻板印象回答。

Conclusion: 该研究提出区域和文化适应性强的PakBBQ数据集，展示了采用更具当地特色的基准及合适提示设计可有效缓解LLM的偏见，尤其对低资源语言背景下的应用至关重要。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [50] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 本文提出了用于检测大型语言模型幻觉（尤其是编造回复）的一种轻量级语义发散度量框架，通过嵌入聚类和信息论指标准确量化模型回复与原始查询之间的语义偏离，并验证了方法的有效性，最终为LLM输出类型的分类和高风险行为的诊断提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在实际应用中普遍存在幻觉问题，即模型生成的内容可能与事实不符、逻辑混乱或不忠实于输入。为了解决这一关键缺陷，论文希望提出更有效的检测方法，特别针对模型回复与用户查询严重不一致的“编造”现象。

Method: 论文提出了语义发散度量（SDM）框架，通过对句子嵌入进行联合聚类，将问题和回答映射到共享的话题空间。通过可视化话题之间的共现热图，并结合信息论指标（如Jensen-Shannon散度、Wasserstein距离、KL散度等）来定量分析回复内容与原始查询之间的语义偏离，最终归纳为可操作性强的分数用于自动检测幻觉。

Result: SDM框架不仅能检测单一固定问题上的回答多样性，还提升到检测多种语义等价表述下的回复一致性。提出的$\\mathcal{S}_H$分数可以准确量化幻觉发生强度，KL散度也能有效区分不同类型的生成行为。这些分数集成到“Semantic Box”诊断框架中，可自动分类并识别危险的高置信但错误的LLM回复（如自信的编造）。

Conclusion: SDM及其配套信息论指标为发现和诊断LLM幻觉、编造等响应类型提供了高效、可视化且操作性强的新工具，对促进LLM安全和可靠应用具有重要意义。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [51] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 研究比较了多种深度学习模型在emoji预测任务上的表现，发现BERT整体最优，但CNN对稀有类别效果更好。模型选型和调参对任务效果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的发展，短文本中的表情符号（emoji）成为表达情感的重要方式，对其进行准确预测能够提升人与计算机之间的交互体验。

Method: 该研究采用了四种深度学习架构进行实验：前馈神经网络（Feed-forward network）、卷积神经网络（CNN）、Transformer和BERT，并使用TweetEval数据集。针对类别不平衡问题，采用了focal loss和正则化技术。

Result: 实验结果显示，BERT由于预训练优势，整体表现最佳；而CNN在稀有emoji类别上的表现更优。

Conclusion: 不同模型的架构选择以及超参数调优对表情符号的情感感知预测至关重要，能够提升人机交互效果。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [52] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 大型語言模型能高效且準確地根據臨床訪談文本預測精神分裂症高危患者的症狀評分，表現接近人類評分者，且適用於不同語言與多次隨訪，有助於改善臨床監控方法。


<details>
  <summary>Details</summary>
Motivation: 鑑於臨床高危險（CHR）精神分裂症患者需要密切監控其症狀以指導治療，但常用的BRPS量表雖有效卻因需長時間結構化訪談，在臨床上使用不便，因此尋找一種便捷且準確的症狀監測方法是迫切需求。

Method: 通過應用大型語言模型（LLMs），從409名AMP-SCZ隊列CHR患者的臨床訪談文本預測其BRPS分數，並比較模型零樣本預測結果與真實專家評分的一致性。此外，評估LLMs在外語評分、以及整合縱向信息（一次性或少量學習）的表現。

Result: LLMs預測BRPS分數在無特定結構的臨床訪談文本上，達到中位一致性0.84和ICC 0.73，接近人類評分者間和內部的一致性。LLMs在外語評分（中位一致性0.88，ICC 0.7）、及整合多次訪談資訊時也展現出高度準確性和穩定性。

Conclusion: 大型語言模型在自動化評估CHR患者精神分裂症症狀及標準化診斷工具上具備極高潛力，有望減少臨床工作量並提升評估的一致性與可靠性。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [53] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 通过计算方法与语料库分析，研究发现Toki Pona这种构造语言会像自然语言一样经历随社区使用带来的变化与演化。


<details>
  <summary>Details</summary>
Motivation: 旨在探究这个核心词汇量约为120的构造语言Toki Pona在实际使用中的语言变化与变异，特别关注其语法类别与句法结构随时间和语境的演变方式。

Method: 采用了计算和语料库分析方法，研究了Toki Pona中的流动词类与及物性，并分析了内容词在不同句法位置上的偏好变化，以及不同语料库中的使用差异。

Result: 结果表明Toki Pona和自然语言一样会受到社会因素影响，语言社区的实际使用促使该语言系统发生自然演变。

Conclusion: 社会语言学因素影响着Toki Pona，就像它们影响自然语言一样；即使是人为构建的语言系统，也会随着社区的实际使用而自然进化。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [54] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 通过让LLM的输出参与到提示词生成中，匹配其归纳偏置，可显著提升分类与排序任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）对提示词（prompt）表述的微小变化非常敏感，因此找到更有效的提示工程方法具有重要意义。作者指出，这种敏感性部分源于模型自身的归纳偏置。

Method: 提出一种“归纳偏置提取与匹配”（Inductive Bias Extraction and Matching）策略：将模型自身的输出作为下一轮提示词的一部分，以生成更贴合模型归纳偏置的提示语。

Result: 实验证明，这一策略能使LLM在分类任务中的Likert评分提升最高19%，在排序任务中提升最高27%。

Conclusion: 利用归纳偏置提取与匹配策略，有助于设计出更符合LLM内在规律的高效提示词，从而显著提升模型在相关任务中的评分表现。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [55] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 通过定性话语分析发现，LLM在性别与种族刻画上复制并强化偏见，现有自动修正难以有效消除这些问题，呼吁AI设计和评估需更关注伦理和多学科批判。


<details>
  <summary>Details</summary>
Motivation: 目前大多数偏见检测方法以自动化、定量为主，容易忽略细腻的语言偏见。本研究希望补充现有方法，深入探讨语言模型中性别与种族偏见的具体展现方式。

Method: 提出并采用了一种定性、话语分析框架。通过人工分析LLM生成的以黑人和白人女性为主角的短篇故事，辨识并讨论生成文本中的种族与性别偏见。

Result: 研究发现，生成文本中黑人女性被刻画为与祖先和抗争相关，白人女性则更常与自我发现关联。这些表现反映了模型复制了固化的社会话语，强化了本质化和社会停滞的观念。模型在被要求纠正偏见时，只做了表面修正，仍保留着有问题的意义，说明其在生成包容性叙事方面有明显局限。

Conclusion: 算法具有意识形态功能，语言模型生成内容会反映和加深现有的社会不平等。要实现真正的包容性和公平，AI开发和应用应采纳批判性、跨学科方法，重视定性分析和伦理考量。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [56] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的新型学术审稿自动生成系统ReviewRL，有效提升了评论的完整性和准确性，在公开测试中优于现有方法，推动了自动化科学批评的研究。


<details>
  <summary>Details</summary>
Motivation: 传统人工审稿面临稿件数量激增与评审者疲劳等问题，现有自动化审稿方法在事实准确性、评分一致性以及分析深度方面存在明显不足，往往只是生成泛泛而谈的建议，缺乏高质量人工审稿所具备的洞见。

Method: 提出了ReviewRL框架，结合：（1）ArXiv-MCP检索增强的上下文生成流程，将相关科学文献作为辅助信息，（2）有监督微调以建立基础审稿能力，（3）复合奖励的强化学习方法，同时提升评论质量和评分准确性。

Result: 在ICLR 2025论文上实验结果表明，ReviewRL在规则指标和模型评估方面均优于已有方法。

Conclusion: ReviewRL为利用强化学习实现自动化科学评审奠定了基础，显示出在自动评论生成领域的潜力，并将开源具体实现。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [57] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: 本文针对传统文档解析缺乏对表格及其上下文的深层语义理解问题，提出DOTABLER语义解析框架，经实验验证在真实PDF数据上性能优越，可实现更精准的表格语义分析与检索。


<details>
  <summary>Details</summary>
Motivation: 现有文档分析主要聚焦于表格的版面分析、检测和数据抽取等表层任务，缺乏对表格及其上下文深层语义的解析，导致无法实现跨段落数据解释和上下文一致性分析等高级任务。

Method: 提出了一种新颖的以表格为中心的语义文档解析框架DOTABLER，结合自定义数据集与预训练模型的领域微调，集成完整的解析流程，识别与表格语义关联的上下文片段，支持表格中心的文档结构解析和领域特定的表格检索。

Result: 在包含近4,000页和超过1,000个真实PDF表格的数据集上，DOTABLER在Precision和F1上均超过90%，在表格-上下文深度语义分析和文档解析上优于包括GPT-4o在内的先进模型。

Conclusion: DOTABLER有效揭示了表格与上下文之间的深层语义关联，能够实现更精细的表格驱动文档解析和高精度的表格语义检索，为结构化文档理解提供了强有力的支持。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [58] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 本文针对LLM-based推荐系统协同信号衰减问题，提出了通过频谱滤波与调制技术强化协同信号的新方法FreLLM4Rec，实验证明性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM（大型语言模型）在推荐系统中展示了生成语义感知推荐的潜力，但它们常常过度强调用户历史中的语义相关性，导致协同信号在推荐过程中逐渐减弱。与此同时，传统的Transformer推荐模型能够更好地保留甚至增强协同信号。本文致力于解决LLM-based推荐系统协同信号被削弱的问题。

Method: 提出了FreLLM4Rec方法，从频谱角度平衡语义与协同信息。首先利用全局图低通滤波器（G-LPF）对包含语义和协同信息的物品嵌入进行纯化，去除无关高频噪声。随后，通过时序频率调制（TFM）在模型各层主动保留协同信号，并通过理论分析将难以实现的最优局部图傅里叶滤波器与易于计算的次优频域滤波器联系起来，保障TFM的协同保留能力。

Result: 在四个公开基准数据集上的实验证明，FreLLM4Rec显著缓解了协同信号衰减问题，且推荐性能优异，NDCG@10指标最多提升8.00%。

Conclusion: FreLLM4Rec为LLM-based推荐系统提供了减少协同信号削弱的方法，对LLM处理协同信息的机制有了新认识，并为提升相应推荐性能提供了理论与实践上的依据。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [59] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出XPE和Dual Soft Prompt机制，极大提升了低表现语言在大模型微调中的准确率，同时在多语言设置下展现出优良适应性，为跨语言任务带来新突破。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调（PEFT）方法中，软提示被广泛应用，但其在跨语言迁移上的潜力尚未充分探索，尤其是在低表现语言上的效果提升。

Method: 提出了Cross-Prompt Encoder (XPE)，结合轻量化编码结构与多源、多类型语言训练，捕捉抽象且可迁移的语言模式；同时设计了Dual Soft Prompt机制，将编码器式提示与直接训练的软提示结合，促进结构共享与语言特定对齐。

Result: 在SIB-200基准实验中，XPE对于低表现语言提升最明显，而混合变种在多语言环境下表现更具广泛适应性。

Conclusion: XPE能显著改善低表现语言的任务结果，混合提示机制则兼顾了多语言环境下的适应能力，为跨语言PEFT提供了有效新方法。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [60] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 该论文提出了基于有监督微调和定制GRPO强化学习的两阶段方法，显著增强了Qwen3 14B大语言模型在韩语下的推理与问题解决能力，实现了“韩语母语思维”。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在非英语环境下的“母语思维”能力有限，尤其是在韩语的推理能力方面有待提升，因此研究者希望增强Qwen3 14B模型在韩语推理和问题解决中的表现。

Method: 提出了两阶段微调方法。第一阶段是利用高质量韩语推理数据集进行有监督微调(SFT)，提升模型的韩语逻辑推理基础。第二阶段采用带有定制Group Relative Policy Optimization (GRPO)的强化学习，并引入oracle judge模型矫正奖励信号，解决GRPO在训练中的稳定性问题（如奖励欺骗和策略塌陷）。

Result: 改进的方法提升了模型在韩语任务上的表现及整体推理能力。最终的RL微调模型在高级推理基准（如数学与编程任务）上有显著提升，同时保持了知识和语言能力，可以在韩语下完整进行思维链推理。

Conclusion: 通过两阶段微调，特别是利用改进的GRPO和oracle judge机制，成功让Qwen3 14B模型在韩语下具备本地化推理和“母语思维”能力，显著提升复杂推理任务的性能，且无明显副作用。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [61] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 创新的序列到序列方法和受限解码技术使跨语言复合ABSA任务表现提升至多10%，无须额外翻译工具，且多语言LLM表现较好，英语LLM较弱。


<details>
  <summary>Details</summary>
Motivation: 当前ABSA主要聚焦英语，低资源语言支持有限；现有跨语言方法多依赖外部翻译工具，且多针对简单任务，亟需高效且适用于复杂任务的新方案。

Method: 采用序列到序列（sequence-to-sequence）架构，并结合受限解码（constrained decoding）技术，实现无需翻译工具的复合ABSA任务。

Result: 方法提升了跨语言ABSA性能至多10%，可处理更复杂任务，且优于依赖翻译的技术；微调的多语言LLM表现接近，英语为主的LLM则效果较差。

Conclusion: 提出的方法有效提升了跨语言ABSA任务的性能，避免了对外部翻译工具的依赖，尤其对复杂任务更有优势。微调的多语言LLM效果相近，但以英语为中心的LLM效果不佳。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [62] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 文中采用先进的大型语言模型提升了捷克语文本摘要，刷新了现代数据集效果，并首次公开历史文献摘要数据集，推动了捷克语自然语言处理研究。


<details>
  <summary>Details</summary>
Motivation: 尽管英文等高资源语言的自动文本摘要技术得到了深入研究，但捷克语特别是历史文献的文本摘要由于语言复杂性和缺乏标注数据集，研究较少。作者希望提升捷克语特别是历史文献领域的文本摘要性能。

Method: 采用大型语言模型（如Mistral和mT5）进行捷克语文本摘要实验，并使用现代的SumeCzech数据集测试效果，同时构建并公开新的捷克历史文献摘要数据集Posel od Čerchova，并报告其基线结果。

Result: 实验表明，所用的大型语言模型在SumeCzech数据集上达到了新的最新（state-of-the-art）效果，并且提出了首个用于捷克历史文献摘要的新数据集及其基线成绩。

Conclusion: 作者的方法提高了现代和历史捷克文献的文本摘要水平，为捷克语文本处理带来了新的探索空间，推动了相关研究进展。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [63] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出了一种无须翻译工具的受限解码序列到序列方法，有效提升低资源语言跨语言方面级情感分析性能，支持多任务，优于现有技术；同时分析了大语言模型在不同场景下的表现并提出应用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管面向英语的方面级情感分析（ABSA）取得了较大进展，但低资源语言在该领域依然面临挑战，现有跨语言ABSA方法多依赖翻译工具且任务复杂度有限，因此亟需一种能提升多种低资源语言情感分析效果的方法。

Method: 提出使用序列到序列模型结合受限解码（constrained decoding）的方法，无需外部翻译工具，并支持多任务，通过单一模型解决多个ABSA任务。研究同时评估了大语言模型（LLM）在零样本、少样本和微调等不同场景下的表现。

Result: 新方法在七种语言和六种ABSA任务上，平均提升最复杂任务的跨语言性能5%，多任务受限解码带来超过10%的提升，全面超越现有技术并在一些新任务上设立新基准。LLM仅在微调后表现接近多语言小模型，零样本和少样本表现较差。

Conclusion: 所提受限解码方法能够有效提升低资源语言ABSA任务的性能，方法高效且适于多任务场景。研究为实际应用提供了建议，并加深了对跨语言ABSA方法优劣势的理解，推动了该领域技术进步。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [64] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 本文提出MDH框架，结合大模型和人工，提升了越狱攻击评估中恶意提示检测的效率与准确性，并创新性提出两种新型越狱攻击方式，相关工具和数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 评估“越狱攻击”（jailbreak attacks）时，由于攻击提示可能并不明显有害或者无法诱发有害输出，导致准确评估变得困难。此外，现有的红队数据集中含有许多不合适用于评估的提示语，因此迫切需要对数据集的恶意程度进行评估与清理。

Method: 提出了一种名为MDH（Malicious content Detection based on LLMs with Human assistance）的混合评估框架，结合了大型语言模型（LLM）自动标注与有限人工复核，用于高效且准确地检测和清理数据集，并判断“越狱”输出。此外还提出了两种新型攻击策略：D-Attack（利用上下文模拟）和DH-CoT（结合劫持思维链）。

Result: MDH框架可以提升恶意内容的检测效率和准确性，为数据集清理和“越狱”输出检测提供有效支持。同时发现，通过精心设计的开发者提示可以显著提升越狱成功率；新提出的D-Attack和DH-CoT方法有效增强了攻击效果。

Conclusion: 本文提出的MDH混合框架在恶意内容检测任务中实现了效率与准确性的良好平衡；提出的新攻击策略为未来越狱相关研究奠定了基础，相关代码和数据成果已开源。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [65] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出SFPF黑盒攻击框架，利用稀疏自编码器锁定并扰动关键特征，高效生成能绕过防御的对抗文本，揭示大模型安全隐患，不过跨场景泛化及层次适应性仍需后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）广泛应用，但其对抗样本攻击的脆弱性依然突出，理解并提升模型的安全性和健壮性成为研究重点。如何生成高效的文本对抗样本以绕过安全机制，仍是极具挑战性的问题。

Method: 提出了一种新的黑盒攻击方法——稀疏特征扰动框架（SFPF）。该方法通过利用稀疏自编码器（SAE）分析并重建隐藏层特征后，对成功攻击样本的特征进行聚类，发现高激活特征并进行有针对性扰动，进而生成具备更强绕过能力的新对抗样本。

Result: 实验发现，SFPF生成的对抗样本能够绕过当前最先进的NLP安全防御机制，暴露出现有模型防御手段仍存在持续性脆弱性。不过，该方法对不同提示词和模型层的有效性有差异，能否泛化到其他结构和更大模型仍需进一步验证。

Conclusion: 引入了一种新颖的针对LLM的黑盒攻击框架，通过稀疏特征有针对性扰动生成对抗文本，显示了对现有防御的突破性，但在泛化性和一致性上尚待提高。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [66] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG是一种仿人类记忆动态推理的检索增强方法，在长文本叙事推理任务中显著超过传统RAG，特别在复杂全局问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 长篇故事和小说的叙事理解具有挑战性，主要由于其复杂的情节和角色、实体间不断变化的关系。此外，当前大模型在处理长距离上下文时推理能力减弱，计算成本高昂。现有基于检索的RAG方法由于一次性检索，不考虑关系的动态变化，导致效果有限。

Method: 提出ComoRAG方法，将叙事推理建模为一个动态、迭代的过程，模拟人类利用记忆在脑中推理的认知机制。方法在无法直接推理时，进行多轮检索与推理循环，每轮通过生成探测式查询，发现新的推理路径，同时将新的证据整合入全局记忆池，以形成更连贯的语境支持推理。

Result: 在四个挑战性的长上下文叙事任务（单篇文本长度达到20万以上Token）上，ComoRAG优于现有强RAG基线，最高相对提升达11%。进一步分析显示，ComoRAG在需全局理解的复杂查询上尤为有效。

Conclusion: ComoRAG能够更高效地处理长篇复杂叙事任务中的推理，实现了有状态、认知驱动的长文本理解范式，并为长上下文检索式推理提供了一种更具规律性的解决方案。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [67] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 该论文提出IdiomEval框架，系统评估了九个主流机器翻译系统在中文成语翻译上的表现，发现它们普遍存在高错误率，尤其是未能传达成语的比喻意义。现有评价方法也难以有效反映成语翻译质量，新开发的检测模型能更精准识别翻译错误。


<details>
  <summary>Details</summary>
Motivation: 中文成语在日常语言中非常常见，其比喻意义通常与字面解释不同，并且包含历史典故和固定结构。尽管大语言模型在机器翻译方面取得了进展，但对中文成语翻译的研究仍然有限，因此急需对成语翻译的现状和挑战进行系统性评估。

Method: 提出了IdiomEval框架，包括针对中文成语翻译的全面错误分类体系，对九种现代系统（如GPT-4o、Google Translate等）在四个不同领域（网页、新闻、维基百科和社交媒体）上的900组翻译对进行了人工标注和分析。评估了系统翻译的准确性，并对现有自动评价指标与人工评分的相关性进行了对比分析，最后开发了更准确的自动错误检测模型。

Result: 主流系统在中文成语翻译方面表现不佳，常出现字面、部分或缺失翻译等错误。最佳系统GPT-4仍有28%的错误率。现有自动评价指标与人工评分的相关性较低（Pearson相关系数<0.48），而改进后的自动检测模型在识别成语翻译错误方面达到了0.68的F1分数。

Conclusion: 当前主流机器翻译系统在中文成语翻译任务上仍然面临较大挑战，自动评价指标无法有效衡量成语翻译质量。论文提出的新框架和模型显著提升了错误检测的准确率，有助于未来成语翻译的系统性优化。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [68] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 本文将大型语言模型的计算过程类比为经济体，通过引入激励机制和计算成本约束，在保证准确率的同时显著降低算力消耗，并提升模型解释性。所提方法优于传统剪枝，促进高效且更透明的模型设计。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）应用中，计算消耗巨大，成为实际部署的主要限制。作者希望通过经济学原则优化模型效率，提高其在有限资源下的表现。

Method: 提出“计算经济学”框架，将LLM内部视为资源受限的代理系统（注意力头和神经块）。方法包括在训练时引入基于激励的损失函数，将任务损失和可微分的计算成本结合，引导模型选择稀疏、高效计算路径。

Result: 在GLUE（MNLI, STS-B, CoLA）和WikiText-103数据集上，所提方法获得了一系列在准确率和计算效率之间形成帕累托前沿的模型。相比事后剪枝，能在类似准确率下减少约40%的计算量（FLOPS）和推理延迟，并使注意力模式更加可解释。

Conclusion: 经济学原则为在严格资源约束下设计高效、自适应、透明的大模型提供了理论和实践路径。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [69] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 该论文提出DiFaR框架，通过多链式提示和筛选机制，显著提升了视觉-语言模型生成推理理由的多样性、真实性和相关性，从而推动了多模态虚假信息检测领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型（LVLMs）生成的文本推理理由在多模态虚假信息检测中面临多样性不足、幻觉导致的事实错误、以及无关或矛盾内容带来的噪声等三大核心挑战。

Method: 提出了一个名为DiFaR的方法框架，具模型无关性，通过五种多样性的链式思维（chain-of-thought）提示从LVLM中引出多样推理路径，并结合后置轻量级过滤模块，按照句子的事实性和相关性对理由句子进行筛选。

Result: 在四个主流基准数据集上，DiFaR在四类基线方法上取得了高达5.9%的性能提升，还能使现有检测器准确率提高至8.7%。无论自动指标还是人工评测都显示DiFaR在多样性、事实性和相关性三方面显著提升了理由质量。

Conclusion: DiFaR能生成更为多样、真实且关联的文本推理理由，有效提升多模态虚假信息检测任务的表现和基础系统的可用性。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [70] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文研究NLP领域中隐私与可解释性之间的权衡，发现二者可以共存，并给出实际建议。


<details>
  <summary>Details</summary>
Motivation: 在可信自然语言处理（NLP）的研究中，解释性和隐私性是两个重要方向。但目前很少有研究关注解释性与隐私性的交集，两者是否能够兼得还不清楚。

Method: 实证研究，结合差分隐私（DP）与事后可解释性方法，分析在NLP中的隐私-可解释性权衡。

Result: 揭示隐私与可解释性之间复杂的关系，这种关系受下游任务类型以及文本隐私与解释方法选择等多种因素影响。

Conclusion: 隐私和解释性是有共存的潜力的，论文总结了一系列未来研究的实用建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [71] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 多模态大模型过度依赖文本输入，导致其他模态被忽略。作者通过设计新指标系统揭示这一问题并分析原因，提出Token压缩方法有效实现模态关注均衡，为公平多模态模型发展提供新思路。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在多模态任务中表现出色，但在推理过程中过度依赖文本模态，其他模态利用不足。此前研究仅在视觉-语言任务中关注该问题，常归因于数据偏差或模型结构。本文旨在系统性揭示不同数据模态中的文本主导现象。

Method: 本文提出两种衡量模态不均衡的新指标：模态主导指数（MDI）和注意力效率指数（AEI），并在图像、视频、音频、时序、图等多种数据模态上进行全面分析。此外，针对造成文本主导的三个根本原因（非文本模态冗余导致注意力分散、融合架构影响、任务设计偏向文本），提出了简单的Token压缩方法以重平衡模型注意力。

Result: 分析结果显示文本主导现象在所有测试模态中都非常显著和普遍。三大原因被系统分析并证实。提出的Token压缩方法在LLaVA-7B模型上取得显著效果，使MDI从10.23降至0.86，实现模态关注的均衡。

Conclusion: 本文通过新指标和数据分析，首次充分揭示了多模态大模型中文本主导现象的普遍性和原因，并提出有效的重平衡策略，为未来更公平、全面的多模态模型研究奠定基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [72] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本文提出eDIF平台，将LLM可解释性基础设施带入欧洲，支持远程高级模型分析。实证表明平台稳定、用户好评，但也需优化一些技术细节，为未来扩展和社区构建奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 欧洲研究者对大型语言模型（LLM）可解释性基础设施的普及需求，推动了搭建兼容NDIF的eDIF平台，以实现对LLM的高级分析和民主化使用。

Method: 部署并测试了由Ansbach应用科技大学主导、汇聚多家欧洲研究机构的GPU集群。通过NNsight API远程访问平台，让16位欧洲研究人员进行激活修补、因果追踪、表征分析等实证操作，并评估技术性能、可用性及科研价值。

Result: 用户参与度逐步增加，平台性能稳定，大多数研究者正面评价远程实验能力。平台也成为用户社区建设的起点。发现如激活数据下载时间过长、运行偶发中断等局限，将在后续开发中优化解决。

Conclusion: eDIF为推动欧洲LLM可解释性研究的广泛基础设施建设迈出了关键一步。为未来更大范围部署、工具扩展和社区合作打下了基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [73] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 系统评估多种科普特语到法语翻译策略，风格多样和噪声感知微调语料显著增强翻译质量，对历史语言翻译有重要指导意义。


<details>
  <summary>Details</summary>
Motivation: 本文旨在首次系统研究将科普特语翻译为法语的策略，填补历史语言翻译领域的空白。

Method: 设计了全面的翻译流程，系统评估了枢纽翻译和直接翻译、预训练影响、多版本微调优势及模型对噪声的鲁棒性，并利用对齐的圣经语料库进行实验。

Result: 利用风格多样且噪声感知的训练语料进行微调后，翻译质量明显提升。

Conclusion: 本研究为历史语言翻译工具的开发提供了重要的实践洞见。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [74] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 本文提出融合Transformer与图神经网络的手语翻译新方法，在多个权威数据集实现性能突破，显著提升聋哑人群交流可达性。


<details>
  <summary>Details</summary>
Motivation: 全球聋哑及听障人群面临交流障碍和社会排斥，手语作为其主要交流方式在以口语为主的社会常被低估，亟需更有效的翻译方法以提升沟通便利性。

Method: 本研究融合了图结构方法与Transformer架构，尤其结合了Transformer和STGCN-LSTM，实现手语到口语的无gloss翻译。研究还探索了多种架构融合策略。

Result: 在RWTH-PHOENIX-2014T、CSL-Daily、How2Sign和BornilDB v1.0多个手语数据集上，该方法均取得了新的最先进性能。BLEU-4分数相比现有方法（GASLT、slt_how2sign等）显著提升，分别为4.01、2.07和0.5。首次对BornilDB v1.0进行基准测试。

Conclusion: 该论文方法在手语翻译领域设置了新标杆，尤其在无gloss翻译上表现优异，为未来研究和聋哑人士沟通无障碍提供重要参考。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [75] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 该论文提出用自然语言反馈指导大型语言模型个性化问答训练，替代传统单一标量奖励信号，显著提升了问答效果和用户满意度，并在多领域实验中超越最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型个性化方法多依赖于检索增强生成及使用标量奖励信号，通过强化学习实现个性化，但这种标量奖励反馈信息有限，学习效率和效果受限。作者希望通过更丰富的反馈机制提升个性化质量和学习效果。

Method: 提出VAC新框架，用自然语言反馈（NLF）替代单一标量奖励。具体做法为：反馈模型和策略模型交替优化，反馈模型根据用户资料和问题生成具指导性的文本反馈，提升响应生成；最后获得无需额外反馈即可推理的策略模型。

Result: 在LaMP-QA基准（三个不同领域）评估，VAC在所有领域均显著优于最优现有方法。人工评测也证实生成响应的质量更高。

Conclusion: 自然语言反馈能为个性化问答任务提供更有效的优化信号，显著提高响应生成质量与用户满意度。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [76] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 作者提出了适用于扩散式大型语言模型的原位链式思维提示（ICE）框架，实现了显著的准确率提升和大幅度推理加速，有效改善了传统前缀提示的灵活性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）取得了显著成就，但传统的仅前缀提示和顺序生成方式，对于双向信息处理灵活性有限。随着扩散式大型语言模型（dLLMs）引入的双向注意力和迭代优化过程，带来了提升提示策略的新机会。

Method: 提出了ICE（In-Place Chain-of-Thought Prompting with Early Exit）框架，将前缀提示转化为适用于dLLMs的原位提示，并在迭代优化阶段于掩码位置直接植入提示，结合基于置信度的提前退出机制，以降低计算开销。

Result: 在多个基准上经过了大量实验，ICE在GSM8K任务上实现了最高17.29%的准确率提升同时加速4.12倍，在MMLU任务上实现了高达276.67倍的加速，并保持了有竞争力的性能。

Conclusion: ICE创新性地提升了dLLMs在原位提示方面的灵活性和推理效率，能够显著提升准确率和推理速度，为高效智能推理提供了新的解决方案。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [77] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了自动化、结构化的创新性评估方法，模拟人工评审流程，在创新性判断上准确率高，超越现有AI基线，为NLP等领域的较高效、透明的同行评审提供辅助工具。


<details>
  <summary>Details</summary>
Motivation: 在NLP等高产领域，同行评议中对创新性评估至关重要，但往往被忽视，且评审资源有限，需有高效自动化的方法辅助评审。

Method: 提出一种结构化自动创新性评估方法，通过三步模拟专家评审：稿件内容抽取、相关文献检索及综合、结构化证据对比。该方法依赖大规模人工创新性评语分析，捕捉如独立观点验证和上下文推理等重要评审模式。

Result: 在182篇ICLR 2025论文上，该方法与人工创新性评判的推理方式一致性达86.5%，结论一致性达75.3%，远超现有LLM基线，并能生成详尽、基于文献的分析，提高评审一致性。

Conclusion: 结构化、LLM辅助的创新性自动评审方法在保障同行评审严谨性和透明度方面具备显著潜力，可有效支持人工评审，但不会取代专家。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [78] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 提出MS-GRPO算法，优化小参数LLM的多步决策能力，通过奖励归因和加权采样显著提升表现，在部分任务上小模型优于大模型，显示后训练比单纯扩大模型更高效。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在序列决策任务中表现出潜力，但由于模型规模大且计算资源消耗高，实际应用受到限制。现有小模型的后训练方法主要针对单步交互，无法有效解决多步任务中的奖励归因问题，因此亟需针对小模型的多步优化方法。

Method: 提出多步组相对策略优化算法（MS-GRPO），以文本中介随机博弈和语言代理策略的理论为基础。MS-GRPO将整个累积奖励归因到每一个步骤，并引入了绝对优势加权的采样方法增强训练效果。

Result: 实验在Snake和Frozen Lake任务上测试，经过后训练的3B模型在Frozen Lake任务中的表现超出72B大模型50%，验证了方法的有效性。

Conclusion: 专门的后训练算法能够有效提升小模型的决策能力，无需无条件扩大模型规模，是打造高效序列决策LLM代理的捷径和实用方案。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [79] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: Psyche-R1是首个结合共情、心理学专长与推理的中文心理健康大模型，通过创新数据流程和混合训练策略，性能优异，适用于心理健康自动化场景。


<details>
  <summary>Details</summary>
Motivation: 心理健康专业人员短缺，亟需技术手段缓解心理健康负担。现有大模型在情感支持和共情对话方面已取得进展，但推理机制用于心理学领域的可靠回答尚未受到足够重视。

Method: 提出了Psyche-R1——首个融合共情、心理学专业知识与推理能力的中文心理学大模型。采用新颖的数据合成流程，生成7.5万高质量心理学问答及详细推理过程，73,000条共情对话。通过多LLM交叉筛选的分组相对策略优化（GRPO）提升推理能力，剩余数据用于有监督微调（SFT）提升共情和专业知识。

Result: Psyche-R1在多项心理学基准上表现有效，7B模型在性能上可媲美671B规模的DeepSeek-R1。

Conclusion: Psyche-R1集成了共情、心理学知识和推理能力，有望显著提升心理学领域自动化咨询和诊断的可靠性与实用性。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [80] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 文章提出了可解释性更强的自动口译质量评估框架，在英中传译数据上验证了效果，为口译学习者提供了有指导性的反馈，比传统自动评分更有用。


<details>
  <summary>Details</summary>
Motivation: 现有的自动口译质量评估方法中，语言使用质量考察不足，数据稀缺和不平衡导致建模效果不理想，并且模型预测缺乏解释性。作者旨在解决这些问题，提升模型透明度和诊断能力。

Method: 提出了一个多维建模框架，融合特征工程、数据增强与可解释的机器学习技术。模型采用透明、与构念相关的特征，利用Shapley Value (SHAP)分析实现预测解释。

Result: 在一个新的英中交替传译数据集上，模型获得了很强的预测性能。BLEURT和CometKiwi分数对于忠实度最具预测力，停顿相关特征对流畅度预测效果最佳，中文特定的短语多样性指标在语言使用方面预测力最强。

Conclusion: 所提出的框架兼具可扩展性、可靠性和透明性，是传统人工评估的可行替代方案，能够为学习者提供详细诊断反馈，促进自我调节学习，其解释性优于一般的自动评分结果。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [81] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文提出Self-Search和SSRL方法，以减少强化学习过程中对外部搜索引擎的依赖。通过结构化提示和基于奖励的优化，提升了LLM作为搜索代理的效率和稳定性。实验证明，新方法既经济高效，又能减少幻觉，支持更可扩展的RL智能体训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习过程中常常需要与外部搜索引擎频繁交互，代价高昂。作者希望通过利用大型语言模型（LLM）自身的知识和推理能力，降低对外部搜索引擎的依赖，提升搜索任务的效率和经济性。

Method: 作者首先提出Self-Search方法，通过结构化提示和重复采样量化LLM的内在搜索能力。随后，提出Self-Search RL (SSRL)，通过格式化和基于规则的奖励进一步增强模型内部知识搜索能力，并在无需外部工具支持下进行迭代式优化。采用这一流程训练RL策略模型，并进行实证评估。

Result: 实验证明，利用SSRL训练的策略模型可以作为高效、稳定且成本低廉的搜索型RL代理环境，显著减少对外部搜索引擎的依赖，并在sim-to-real迁移过程中表现稳健。LLM展现出强大的推理和世界知识调动能力，SSRL能够有效减少幻觉（hallucination）现象，并与外部搜索引擎自然集成。

Conclusion: 1）LLM具备可充分激发的世界知识，在搜索任务中可表现出高性能；2）SSRL方法能有效利用模型内部知识，减少幻觉现象；3）SSRL训练的模型与外部搜索引擎可无缝结合，有助于提升RL智能体训练的可扩展性。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [82] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文全面综述了扩散语言模型（DLMs）的原理、发展与应用，系统揭示了其在速度、并行能力和全局信息建模等方面的独特优势，总结了主要技术进展、应用前景以及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 鉴于DLMs近年来取得的进展及其潜力，对该领域进行全面、权威的梳理，以帮助学界与业界了解扩散语言模型的发展现状、核心技术及未来前景。

Method: 系统性的文献综述和比较分析，梳理DLMs的发展脉络、分类体系、重要技术（如预训练与后训练方法）、推理优化，以及多模态拓展等。

Result: 1. 提供了DLMs与自动回归及掩码模型的关系梳理。
2. 总结了DLMs最新的模型和方法，包括优化推理策略、多模态拓展与实际应用案例。
3. 归纳了DLMs存在的局限性（效率、长序列处理、基础设施需求等），并提出了未来可能的研究方向。

Conclusion: 本综述提出了扩散语言模型（DLMs）作为自动回归模型的重要替代方案，详细分析了其优势、当前进展以及面临的挑战，并为未来研究方向提供了指导。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [83] [Active Automata Learning with Advice](https://arxiv.org/abs/2508.10535)
*Michał Fica,Jan Otop*

Main category: cs.FL

TL;DR: 本文提出结合主动学习与演绎推理的自动机学习新框架，通过额外建议信息减少查询次数，实验证明能有效降低查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 主要动机是减少学习算法对教师所需查询的数量，提高学习效率。

Method: 结合主动自动机学习与演绎推理，在传统成员与等价查询的基础上，通过字符串重写系统提供“建议”，辅助推断查询答案。在Angluin风格学习算法的基础上进行了适配，并进行实证评估。

Result: 实验证明，该方法在查询复杂度方面有显著提升，表明新框架的有效性。

Conclusion: 提出的扩展自动机学习框架能够有效减少对教师的查询负担，实现了查询复杂度的显著降低。

Abstract: We present an extended automata learning framework that combines active
automata learning with deductive inference. The learning algorithm asks
membership and equivalence queries as in the original framework, but it is also
given advice, which is used to infer answers to queries when possible and
reduce the burden on the teacher. We consider advice given via string rewriting
systems, which specify equivalence of words w.r.t. the target languages. The
main motivation for the proposed framework is to reduce the number of queries.
We show how to adapt Angluin-style learning algorithms to this framework with
low overhead. Finally, we present empirical evaluation of our approach and
observe substantial improvement in query complexity.

</details>
