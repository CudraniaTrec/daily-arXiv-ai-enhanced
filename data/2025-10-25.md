<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 64]
- [cs.DM](#cs.DM) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 本文提出了Prompt Decorators语法，通过简易装饰指令高效控制LLM行为，实现可复用、可解释和可审计的Prompt设计，优化推理透明度和模型一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在用户推理、写作、决策等工作流中已十分重要，但用户对于模型的推理方式和输出表达缺乏稳定的控制手段。传统的提示词工程主要依赖冗长的自然语言指令，导致可重复性、模块化和可解释性不足。

Method: 提出了Prompt Decorators（提示装饰器），这是一种声明式、可组合的语法，通过简洁的控制令牌如+++Reasoning、+++Tone(style=formal)等，来精细调整LLM的行为维度（推理风格、结构、语调），并定义了统一语法、作用域模型和确定性处理流程。装饰器分为两个功能组，分别对认知生成和表现系统行为进行细分。

Result: 通过将任务意图与执行行为分离，该框架为提示设计提供了可复用、可解释的接口。实际用例表明，能提升推理透明度、减少提示复杂性，在不同领域中实现模型行为标准化。

Conclusion: Prompt Decorators能提升大型语言模型的可控性、可解释性和可复用性，为AI系统的声明式界面、跨域一致性与可扩展性提供新的思路。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文首次定义并系统化分析了执行算法规范所需的全部知识领域，提出了可自动化生成该知识文档的方法，对提升算法实现与验证的规范性具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探索如何让算法规范以自然语言或伪代码形式更加明确和易于机械执行，并分析执行者需要具备的知识。

Method: 提出“realm of the algorithm specification”概念，将执行算法所需知识系统化，并探讨利用大型语言模型及文档复用实现该知识领域的自动生成。

Result: 定义了算法规范所需知识的结构，包括语法、语义、领域知识、实体关系、因果规则和操作说明；提出系统性分析流程，展示部分可自动化。

Conclusion: 算法规范的知识领域可以被系统性归纳和文档化，有助于多系统实现及正式化验证，并对执行忠实性与算法正确性的区分进行了初步探讨。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: 本文提出 Proto-Quipper-A，一种简化且易于形式化的量子电路生成语言，用于克服以往 Proto-Quipper 家族语义复杂、难以机械化的问题。通过线性 lambda 演算和 adjoint-logic 整合，支持 Proto-Quipper 抽象，并可用标准逻辑关系证明归一性。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper 编程语言家族为 Quipper 量子编程语言提供形式化基础，但其操作语义复杂，难以用标准编程语言技术推理和机械化。

Method: 提出 Proto-Quipper-A，这是对 Proto-Quipper 语言的理性重构，采用线性 lambda 演算描述量子电路，并结合 adjoint-logical 基础，将电路语言与线性/非线性函数式语言集成。用标准逻辑关系来证明归约和归一性。

Result: Proto-Quipper-A 拥有简单的按值调用归约语义，并且可以通过标准逻辑关系证明归一性，克服了现有线性逻辑关系的复杂性。

Conclusion: Proto-Quipper-A 作为 Proto-Quipper 语言的新基础，简化了操作语义，易于推理和机械化，并保留了原始电路编程抽象能力。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 提出并实现了一种支持高级多态和子类型的类型与效应推断算法，解决了作用域和表达性问题，算法已获形式化证明并在真实编程语言环境下验证。


<details>
  <summary>Details</summary>
Motivation: 类型系统在程序语言中广泛应用，但类型与效应系统尚未普及，原因在于其复杂性及现有推断算法在表达性、直观性和可判定性之间需要妥协。作者希望解决类型与效应系统的效应推断难题，尤其是处理高级多态性带来的作用域问题。

Method: 提出了一种用于具备子类型关系、表达性高级多态和直观效果集合语义的类型与效应系统的效应推断算法，通过将效应约束转换为命题逻辑公式，延迟约束求解，并在Rocq证明助手中进行了形式化证明。

Result: 算法在理论上被证明具有完备性和正确性，并已在实际编程语言中成功实现。所有主要结果都通过Rocq证明助手形式化验证。

Conclusion: 本工作提出并实现了一种高表达性且形式化证明正确的类型与效应推断算法，有效提升了类型与效应系统推断的表达力与实用性。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 该论文介绍了针对Mimosa语言的嵌入式编程编译方案，结合Lustre编译方法，实现了与实时操作系统的良好集成。


<details>
  <summary>Details</summary>
Motivation: Mimosa是一种描述嵌入式系统的软件的编程语言，它基于MIMOS计算模型。嵌入式系统的复杂度和实时性需求推动了对高效且形式化编译方案的需求。

Method: 论文将Lustre编译方案适配到Mimosa的语义，并形式化描述此过程。同时展示了如何将协调层映射到实时操作系统的原语上。

Result: 实现了一种适用于Mimosa语言的编译方案，该方案能够将程序协调层映射到实时操作系统，增强了嵌入式软件的可移植性和执行一致性。

Conclusion: 论文提出的编译方案有效结合了Lustre和Mimosa语言的特点，为Mimosa程序在嵌入式实时系统的部署提供了理论与实践基础。

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI系统优化了Rust中的内存安全检查，将检查置于unsafe与安全代码边界，显著减少开销，提高检测性能，并且保持检测的准确性。


<details>
  <summary>Details</summary>
Motivation: Rust中使用unsafe代码对于兼容C/C++库和底层数据结构实现是必要的，但这会破坏Rust原有的内存安全性。现有sanitizer工具运行时检查冗余、影响性能，有优化需求。

Method: SafeFFI的主要做法是将内存安全检查集中到unsafe与安全代码的边界处，将安全性保障交由Rust类型系统处理，无需对全程序进行昂贵分析。

Result: SafeFFI在多个流行的Rust库和已知漏洞代码上实验，性能优于最新系统，sanitizer检查减少最高可达98%，能完整捕获空间与时间内存安全漏洞。

Conclusion: SafeFFI系统能够大幅减少对Rust二进制中不必要的内存安全检查，同时保持检测准确性，而且编译开销远低于现有方案。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 云端访问控制策略手动编写易出错，分析复杂。本论文探索了用LLMs自动生成与分析这些策略。结果显示，LLMs生成策略准确率（特别是推理LLMs）较高，结合符号分析方法可精准分析现有策略；但自动合成仍存挑战。


<details>
  <summary>Details</summary>
Motivation: 目前云计算广泛应用，服务数量不断增长。云端系统的访问控制策略需要管理员手动编写，但由于策略复杂，易出错且难以精细分析，影响数据安全。作者希望寻求自动化和高效分析的新方法。

Method: 作者探索了使用大型语言模型（LLMs）进行访问控制策略合成与总结。他们评估了不同类型的LLMs自动合成策略的表现，并提出了一种语义驱动的请求总结方法，利用LLMs精准刻画策略允许的请求。

Result: 结果发现，无推理的LLMs仅有45.8%的策略生成与原规范等价；具有推理能力的LLMs等价率提升至93.7%。在策略分析上，将LLMs与符号方法结合能更精准地分析和表征现有策略。

Conclusion: LLMs在自动生成访问控制策略上面临挑战，但在分析现有策略方面，与符号方法结合后显示出很强的潜力。未来可进一步结合自动化与智能分析提升云端策略安全性和可管理性。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


### [8] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: 本文提出E-Test，利用大语言模型自动发现和生成生产环境中未被测试覆盖的执行场景测试用例。实验表明，E-Test能显著提升测试覆盖率和自动化水平，减少人工维护，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前测试套件无法覆盖所有实际遇到的执行场景，扩展测试套件以提升软件可靠性极具挑战且耗时。特别是在管理大型测试套件和应对长期演进时，找出未覆盖场景的测试用例更加困难。

Method: 提出E-Test方法，通过利用大语言模型（LLMs）分析生产环境中真实执行场景，找出当前测试套件未涉及的场景，并自动生成对应的测试用例，补充并优化现有测试套件。

Result: 在开源Java项目和Defects4J的1975个生产环境收集场景上评估，E-Test检索未测试场景的F1-score达0.55，显著优于最优现有方法（最大F1-score 0.34）及原生LLMs（最大F1-score 0.39）。

Conclusion: E-Test可有效发现并覆盖未测试执行场景，提升测试套件质量，减少人工维护工作。其在自动化增强测试套件方面比现有技术表现更佳。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [9] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 本文提出用大语言模型将电子表格操作代码自动转化为人类可读的文档，并建立了基准数据集及系统评测，证明该方法可行，有助于提升电子表格自动化和协作，但仍有技术难题需解决。


<details>
  <summary>Details</summary>
Motivation: 尽管电子表格在企业、会计和金融等领域被广泛使用，但缺乏系统化的文档记录方式阻碍了自动化、协作和知识传承，存在知识流失的风险。

Method: 提出“电子表格操作文档化”(Spreadsheet Operations Documentation, SOD)这一AI任务，利用大语言模型(LLMs)将电子表格操作代码自动转化为人类可读的解释。作者建立了一个包含111条代码及相应自然语言摘要的数据集，并用BLEU、GLEU、ROUGE-L和METEOR等指标评测五种主流LLM(GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B、Gemma2-9B)在这一任务上的表现。

Result: 各主流LLM都能够较为准确地生成电子表格文档。实验表明，SOD作为流程前置步骤，能够提升电子表格的可复现性、可维护性和协作性。不过，目前仍存在一些挑战有待克服。

Conclusion: 利用LLM自动为电子表格操作生成文档是可行的，有望解决知识传承和协作中的难题，但还需进一步研究以解决现存挑战。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [10] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGACG框架利用多智能体协作和反馈机制，提升了从需求到完整应用级代码的自动化生成能力，并解决了项目规模扩展时的组织与维护难题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型驱动的自动代码生成虽然提升了开发效率，但在生成复杂的应用级代码时存在挑战，尤其是项目代码的合理组织结构和过程可维护性不足。

Method: 提出了一个知识引导的应用级代码生成框架KGACG，通过三个智能体——代码组织与规划智能体（COPA）、编码智能体（CA）、测试智能体（TA）协同工作，并结合反馈机制完成从需求、架构到可执行代码的转化。

Result: 在Java坦克大战游戏的案例中，展示了KGACG框架下多智能体协作的过程，并验证了框架在自动化应用级软件开发中的作用与面临的挑战。

Conclusion: KGACG框架推动了应用级自动化软件开发进程，通过整合知识与多智能体协作，提高了大型项目代码生成的结构合理性与过程可控性。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [11] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出了一种更真实且高效的bug合成方法，显著提升了软件工程代理模型在基准测试上的表现，推动了领域内数据生成和模型训练的进步。


<details>
  <summary>Details</summary>
Motivation: 高质量的bug对于训练新一代基于语言模型的软件工程代理至关重要。现有的合成bug方法往往不能真实反映软件开发过程中的bug生成方式，本研究试图改进这一点。

Method: 提出了一种新颖的合成生成高难度和多样性bug的方法。该方法通过指导SWE代理向代码库引入新功能，进而可能无意间破坏测试，产生更真实的bug，而非简单的代码扰动。

Result: 实验和定性分析表明，所生成的bug更符合人类编辑的真实模式。使用这些bug数据进行有监督微调，比其他bug数据集在训练效率和性能上均有提升：在只使用一半数据量的情况下（1.2k vs. 3k），性能提升2%。基于新生成的bug数据训练的32B与14B参数模型在SWE-bench Verified基准上均取得了当前最优结果。

Conclusion: 该方法生成的高质量synthetic bug不仅更贴近真实开发过程，更能有效提升SWE代理的训练效率和模型性能，是未来相关训练工作的有力数据来源。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [12] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本研究发现灰盒Fuzzing中的变异顺序对测试效果很关键。提出的新方法MuoFuzz能智能选择高效变异序列，覆盖更多代码并发现更多bug，效果优于当前主流Fuzzer。


<details>
  <summary>Details</summary>
Motivation: 灰盒Fuzzer在生成新的测试输入时，通常随机选取变异方式组合，但这些变异顺序可能影响Fuzzer效率，作者想探究变异顺序对灰盒Fuzzing有效性的影响，并提升Fuzz效率。

Method: 作者通过线性模型分析所有可能的变异器组合的有效性，发现变异顺序确实产生互动效应。基于此，提出MuoFuzz：它学习之前选的变异器之后，接下来选哪个能更可能生成有价值输入，然后以随机游走采样生成变异序列，并与现有工具AFL++和MOPT在多个基准上对比实验。

Result: 实验表明MuoFuzz在FuzzBench和MAGMA基准测试上获得了最高的代码覆盖率，发现了AFL++未检测出的4个bug，以及AFL++、MOPT均未检测出的1个bug，效果优于对比方法。

Conclusion: 变异器顺序对于灰盒Fuzzer效率有显著影响，MuoFuzz通过学习高效变异顺序，显著提升了代码覆盖率和缺陷检测能力，超过主流工具。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [13] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 论文针对现有技术采纳框架在生成式AI应用上的不足，提出了FAIGMOE框架，系统性解决中型与大型企业GenAI采纳的差异和挑战，提供分阶段、可规模化的采纳与治理指南。框架具备创新性和实际指导意义，亟需后续实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有AI技术采用模型（如TAM、TOE、DOI）在应对生成式AI（GenAI）在中型及大型企业中的具体应用方面，缺乏针对性，造成理论和实践上的空白。中型企业资源匮乏，AI专业能力有限；大型企业组织复杂，协调困难。亟需新的框架指导不同规模企业GenAI的采纳和集成过程。

Method: 提出了FAIGMOE（中型与大型企业生成式AI采纳和集成框架），整合技术采纳理论、组织变革管理及创新扩散视角，并划分为战略评估、规划与用例开发、实施与集成、运营与优化四阶段，每阶段给出可规模化的具体指南。框架还包含GenAI特有的考虑，如提示工程、模型编排、幻觉管理等。

Result: 构建了首个既考虑中型组织，也兼顾大型企业实际需求的GenAI采纳与集成理论框架，提出了可操作协议、评估工具和治理模板，为后续实证研究奠定基础。

Conclusion: FAIGMOE框架为GenAI在不同规模企业中顺利采纳和集成提供了理论和实践指导，填补现有技术采纳理论针对性不足的空白，并为后续深入研究提供了基础。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [14] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: 本研究通过分析Kubernetes及其他项目从Bazel降级到Go Build的过程，发现虽然维护性提高，但在大型项目中会付出不少性能代价，包括更高的构建时间、内存与CPU消耗，提醒开发团队需在维护性和性能之间做权衡。


<details>
  <summary>Details</summary>
Motivation: 构建工具的性能直接影响开发者的生产效率，尽管现代基于产物的构建工具（如Bazel）可以加速构建过程，但很多团队因维护难度选择回退到更易维护的工具。此前的研究侧重于回退的原因，鲜有探讨回退后的实际影响。

Method: 通过对Kubernetes项目从Bazel降级到Go Build的整个过程中全量构建和增量构建进行重现和分析，比较两者在构建速度、内存消耗和CPU负载等方面的差异。另外还在其他四个也进行了相同降级的项目上复现实验结果。

Result: Bazel在全量构建下比Go Build更快，但会带来更高的内存占用和CPU负载，尤其在并行度较高的设置下更为明显。降级会增加CI资源成本，最高可达76%。在其他项目上的复现表明Bazel内存消耗一直较高，但构建时间差距有所减小。

Conclusion: 尽管降低维护成本是回退的主要动力，放弃基于产物的构建工具会为大型项目带来明显的性能代价。因此，项目管理者在选择构建工具时需要权衡可维护性与性能支出。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [15] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 本文针对将Oracle Forms中的PL/SQL遗留代码迁移到Java现代架构，提出了一套基于模型驱动再工程的方法与流程，包括KDM建模、增量模型转换、三种代码验证，并开发了实际迁移工具，通过评估验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着现代软件技术的普及，企业亟需将老旧的RAD（如Oracle Forms）应用迁移到更现代的平台。传统的遗留代码结构复杂，直接迁移难度大，因此需要系统化、自动化的方法辅助迁移。

Method: 采用模型驱动再工程（MDE）方法，将遗留PL/SQL代码表示为KDM模型，并结合类似TDD的增量开发方式，进行模型转换并多次校验生成代码。

Result: 成功开发出一套迁移工具，实现PL/SQL到Java多层架构的自动迁移，并对工具实现过程和MDE应用中的相关问题进行了详尽评估和验证。

Conclusion: 本文提出的模型驱动再工程过程能够有效地支持将PL/SQL遗留代码迁移至Java，并通过多层验证确保迁移代码的正确性和可维护性。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [16] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync通过自动追踪和吸收非IaC变更，使云基础设施配置保持一致与最新，在实验中大幅优于现有漂移处理方案。


<details>
  <summary>Details</summary>
Motivation: 随着云基础设施管理工具的多样化，越来越多组织采用Infrastructure-as-Code（IaC）来自动化和配置云资源。但实际运维中，IaC与云控制台、CLI、SDK并用往往导致配置漂移，产生一致性和自动化维护方面的挑战。现有方法无法自动吸收控制台等外部修改，降低了IaC工具的效用。

Method: 提出了NSync系统，通过分析底层云API调用，实现自动检测并吸收非IaC工具引入的变更（即“漂移”），自动将这些变化同步回IaC配置。NSync采用具备代理能力的架构，结合大语言模型（LLM）推断高层意图，并利用专门工具合成IaC更新，同时建立自演化知识库优化过程。还设计了用于注入真实漂移与评估性能的新测试流程。

Result: 在五个真实Terraform项目与372种漂移场景中，NSync在漂移检测与修复准确率（pass@3）上从0.71提升至0.97，令token效率提升1.47倍，显著优于传统方法。

Conclusion: NSync能显著提升IaC工具的现实应用价值，自动同步外部更改，持续学习优化漂移检测与修复，解决组织中IaC与传统工具混用导致的管理难题。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [17] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Java 原生不支持运行时依赖自省，Classport 通过将依赖信息嵌入类文件实现了这一能力，并已在真实项目中验证有效，为供应链安全和完整性检查带来新的可能。


<details>
  <summary>Details</summary>
Motivation: Java 缺乏在程序运行时观测实际使用的依赖的能力，这一功能对于软件供应链安全至关重要。

Method: 提出了 Classport 系统，将依赖信息嵌入 Java 类文件，从而实现运行时获取依赖信息。

Result: 在六个真实项目上评估了 Classport，证实能够在运行时识别依赖。

Conclusion: Classport 实现了 Java 程序运行时依赖自省，为运行时完整性检查等安全措施提供了新途径。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [18] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 坚持结构上的对称性和一致性，是提升软件平台架构健壮性的关键手段。


<details>
  <summary>Details</summary>
Motivation: 软件平台需要在面对变化和扩展时保持稳定性，因此探索其架构健壮性的来源非常重要。作者认为，这种健壮性可能来源于平台在特定变换（称为对称性）下维持结构一致性的能力。

Method: 通过理论分析，研究软件平台如何通过保持结构一致性和接口、行为稳定性，来实现对称性。分析和探讨这些结构的对称性与平台健壮性之间的关系。

Result: 发现当软件平台在特定变换下能够维持结构和行为的一致性时，会增强其架构的健壮性。即，结构规则性的强制执行有助于平台在面对变化时维持稳定。

Conclusion: 软件平台的架构健壮性与其保持对称性、结构规则性密切相关。通过在设计中强化这种结构一致性，可以提升平台的可维护性和适应性。

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [19] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 本文提出并验证了一种基于UniFMU的安全分布式协同仿真方法，加强了知识产权和安全保护，展示了其在不同网络设置下的优劣权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式协同仿真虽能保护知识产权（IP），但并未针对连续时间或混合系统的安全风险形成统一指导，对防止黑客攻击的保障不充分。

Method: 提出了一种基于UniFMU的分布式协同仿真方法，强化了网络安全和知识产权保护机制，采用客户端主动发起连接，并将模型和二进制文件存储于受信任平台。

Result: 通过两个仿真演示在四种不同网络环境下验证了方法的功能，并分析了知识产权保护机制和性能效率之间的权衡。

Conclusion: 所提出的方案在安全性和知识产权保护方面比传统分布式协同仿真方法更完善，实现了安全、高效的仿真协同，并展示了实际应用的可行性。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [20] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 本文通过访谈和数据分析，总结了演绎验证普及的难点和推动因素，并提出具体改进建议以促进其应用。


<details>
  <summary>Details</summary>
Motivation: 演绎验证虽然有效，但至今未普及，作者希望找出推动其广泛应用的因素及阻碍其落地的关键问题。

Method: 作者对来自业界和学术界的30位演绎验证从业者进行了半结构化访谈，采用主题分析方法系统整理与分析收集到的数据。

Result: 研究确认了已知挑战如高专业门槛，并揭示了一些被低估的障碍，如证明维护、自动化控制等，同时总结了推动与阻碍演绎验证发展的因素，并针对可用性、自动化和与现有工作流程集成提出了具体建议。

Conclusion: 演绎验证发展的主要障碍不仅包括专业门槛，还存在维护和可用性等方面问题；为广泛应用需改善工具、流程并提升体验。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [21] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文系统对比了主流开源及闭源大语言模型在代码故障定位任务的表现。结果表明，引入bug报告上下文能大幅提升定位效果，少样本学习边际收益有限，不同模型对推理链的依赖不同。研究为模型选型和提示设计提供了实用参考。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在自动程序修复方面表现出色，但其效果极度依赖于上游故障定位的精度。目前对LLM在故障定位方面的系统性评估还很缺乏。

Method: 本文系统性地对主流开源（Qwen2.5-coder-32b-instruct, DeepSeek-V3）和闭源（GPT-4.1 mini, Gemini-2.5-flash）LLM进行了实证研究，分别在HumanEval-Java和Defects4J数据集上评估它们在语句级代码故障定位任务中的表现。考察了不同提示策略（标准提示、少样本和推理链）对模型效果的影响，重点从准确率、时间效率和经济成本三个维度分析。

Result: 1. 加入bug报告的上下文能显著提升模型性能。2. 少样本学习有提升潜力但边际效应递减明显。3. 推理链的效果高度依赖于模型的内在推理能力。4. 全面展示了不同模型在故障定位任务下的性能特征和权衡。

Conclusion: 论文揭示了不同类型大语言模型在代码故障定位任务上的优势与不足，总结了有效的提示策略，并为提升故障定位效果提供了策略建议。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [22] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: 本文提出Deblometer微基准，系统评估三种主流Java去臃肿工具，发现它们均存在误删必要代码和健壮性问题，亟需改进以确保去臃肿软件可靠运行。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发广泛依赖第三方库，然而许多被引入的依赖在实际运行时并非全部需要，这导致软件臃肿、包含大量无用代码。因此，开发精确、可靠的去臃肿（debloating）工具以自动移除多余依赖和无用代码，提升性能和安全性，成为研究的动机。

Method: 作者提出了Deblometer，一个包含59个测试用例的微基准测试套件，用于系统性评估Java去臃肿工具。每个用例都人工标注哪些类、方法、字段是必要或臃肿的，以精准测量工具的精确性（能否移除无用构造）与健壮性（能否保留所有必要构造）。作者使用Deblometer对三种流行工具（Deptrim、JShrink、ProGuard）进行了评估。

Result: 评测结果显示，所有评估的工具都会错误移除必须的程序构造，导致语义改变或运行崩溃，尤其在动态类加载特性上均存在不健壮问题。Deptrim倾向于保留更多臃肿代码，ProGuard常移除必要构造，而JShrink由于对注解支持有限导致健壮性恶化，产生损坏的去臃肿产物。

Conclusion: 现有主流去臃肿工具在精确性与健壮性上都存在显著不足，特别是在处理动态类加载和注解时容易引发软件不稳定。需要改进这些工具以保障去臃肿软件的稳定性和可靠性。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Resource-Aware Hybrid Quantum Programming with General Recursion and Quantum Control](https://arxiv.org/abs/2510.20452)
*Kostia Chardonnet,Emmanuel Hainry,Romain Péchoux,Thomas Vinet*

Main category: cs.LO

TL;DR: 本文提出了无需预设量子门集的量子语言 Hyrql，并通过编译到项重写系统，有效借用现有复杂度分析技术，实现了通用的量子资源分析。试验结果展示了方法的多样适用性。


<details>
  <summary>Details</summary>
Motivation: 量子语言因量子门集的不同，其电路复杂度分析缺乏通用方法。现有工具难以适配通用资源分析。

Method: 设计了一种混合量子语言 Hyrql，该语言无需预先指定量子门集合。提出将其编译到简单类型的项重写系统中，从而利用现有项重写系统的复杂度分析技术实现资源分析。

Result: 展示了一种语义保持的编译算法，并通过大量实例证明该方法的多样性和有效性。

Conclusion: Hyrql 语言为量子程序提供了通用且高效的资源分析途径，突破了量子门集固定带来的局限。

Abstract: This paper introduces the hybrid quantum language with general recursion
$\mathtt{Hyrql}$, driven towards resource-analysis. By design, $\mathtt{Hyrql}$
does not require the specification of an initial set of quantum gates and,
hence, is well amenable towards a generic cost analysis. Indeed, languages
using different sets of quantum gates lead to representations of quantum
circuits whose complexity varies. Towards resource-analysis, a
semantics-preserving compilation algorithm to simply-typed term rewrite systems
is described; allowing a generic reuse of all known techniques for analyzing
the complexity of term rewrite systems. We prove the versatility of this
approach through many examples.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: 本文提出了DeBERTa-KC模型用于自动分类科学类YouTube评论中的知识建构水平，结合先进的损失函数和正则化技术，在大规模人工标注数据上表现优异，为非正式学习环境的认知分析提供了自动化、可扩展的新路径。


<details>
  <summary>Details</summary>
Motivation: 当前在线科学学习环境中知识建构水平自动分类的需求提升，尤其是在非正式数字学习环境下识别学习者参与度和认知深度，需要有效且可扩展的自动分析工具。

Method: 基于DeBERTa-v3架构，结合Focal Loss（焦点损失）、Label Smoothing（标签平滑）和R-Drop正则化，以解决类别不平衡并提升泛化能力。搭建端到端可复现流程涵盖数据采集、人工标注、预处理、模型训练和评估。

Result: 提出的DeBERTa-KC模型在YouTube科学频道评论语料库上进行训练，并在10折分层交叉验证中实现了macro-F1 0.836±0.008，显著优于传统与变换器基线方法，对高阶认知参与（如Explore和Negotiate）类别表现尤为敏感。

Conclusion: 结果表明，大型语言模型能够有效捕捉非正式数字学习环境中知识建构的细微差别，支持理论驱动的对话分析和认知参与自动评估工具开发。

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [25] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: 本研究对比评估了五种大模型和三种结构化提示框架生成高中物理教案的效果。发现模型设计主要影响可读性，提示结构主要决定课标契合和事实准确性，但各模型生成的教案大多局限于低阶认知能力。最佳组合为可读性强的大模型+RACE提示+明确课标和高阶目标清单。


<details>
  <summary>Details</summary>
Motivation: 人工智能生成教学方案近年来在教育领域应用增多，但其教学合理性和可用性未经系统评估。作者希望量化不同主流大模型，以及结构化提示框架对生成课案质量的影响，以指导教育实践。

Method: 选用ChatGPT (GPT-5)、Claude Sonnet 4.5、Gemini 2.5 Flash、DeepSeek V3.2、Grok 4五款大模型，利用三种结构化提示模板（TAG、RACE、COSTAR），就同一高中物理主题（电磁波谱）分别生成15份教案。通过四项自动化指标（可读性、事实准确性、课标对齐度、高阶认知要求）对教案进行分析。

Result: 不同大模型对教案的语言可读性影响最显著，DeepSeek可读性最好（FKGL=8.64），Claude语言最为复杂（FKGL=19.89）。提示框架对事实准确性和教学完整度的作用突出，RACE模板下错觉率最低，NGSS课标对齐度最高。所有模型生成的教案学习目标大多集中在布鲁姆认知层级的记忆和理解两个低阶层次，鲜有高阶动词。

Conclusion: 模型本身决定了语言可读性，结构化提示主要影响教学稳健性和课标对齐度。效果最佳配置是高可读性模型结合RACE提示框架，并明确列出物理概念、课程标准及高阶学习目标的检查清单。

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [26] [From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model](https://arxiv.org/abs/2510.19871)
*Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出ReDiff，通过主动修正和自我纠错，有效解决扩散模型生成阶段的错误级联，大幅提升视觉-语言任务中的生成质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在视觉-语言任务中表达出巨大的潜力，但在实际应用中，训练-推理不一致会导致灾难性错误级联，影响生成内容的质量。

Method: 提出ReDiff，采用两阶段训练：第一阶段用合成错误训练模型修正能力；第二阶段引入在线自我纠错环路，即模型通过专家修正学习如何修改自身生成的错误草稿。强调主动修正而非被动去噪。

Result: 实验显示，ReDiff可显著提升生成内容的连贯性与事实准确性，并实现稳定高效的并行生成，优于传统去噪方法。代码与模型已公开。

Conclusion: 主动修正的新框架ReDiff有效打破生成级联错误，显著提升扩散模型在视觉-语言任务中的应用效果。

Abstract: Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.

</details>


### [27] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: 作者提出Sparse Tracing和Stream算法，实现了对百万级token上下文中的attention可解释性分析。该方法大幅提升效率，显著降低内存消耗，在多项基准任务中极大地减少无关交互并揭示关键信息流。工具支持消费级GPU，助力长上下文大模型广泛应用。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型（LLMs）支持百万级上下文长度，传统的机制可解释性分析方法在attention机制上的计算和存储需求随上下文长度二次增长，处理超长文本时显得不可行。科学家亟需更加高效、可扩展的分析技术来理解超长上下文内的信息流和attention分布。

Method: 作者提出Sparse Tracing技术，并开发了Stream算法——一种可编译的层级剪枝算法。Stream通过类似二分查找的方法，动态估算每个头部的稀疏注意力掩码，实现近线性时间$O(T \log T)$与线性空间$O(T)$的attention分析，并只保留每个query对应的top-k关键内容块。该方法可单次遍历高效分析长文本attention，实现大规模可解释性。

Result: 实验证明，Stream算法在chain-of-thought推理任务中能识别出关键思维锚点，同时剪除97-99%的token交互。在RULER基准测试中，Stream能保留关键检索路径，剔除90-96%的无关交互，并揭示从输入到输出的layer-wise信息流路线。该方法可直接部署于消费级GPU，大幅降低分析内存需求。

Conclusion: Sparse Tracing和Stream算法为分析和解释超长上下文内的大模型注意力分布提供了高效实用工具，无需数TB缓存即可动态剖析信息流，有助于促进chain-of-thought追踪的普及和民主化。代码已公开。

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [28] [Automated HIV Screening on Dutch EHR with Large Language Models](https://arxiv.org/abs/2510.19879)
*Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart--Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 此研究提出用大语言模型分析电子健康记录中的临床文本，有效提高了HIV早期筛查的准确率，补足了传统方法对非结构化信息利用的不足。


<details>
  <summary>Details</summary>
Motivation: 对HIV的高效筛查和早期诊断对于减少传播非常关键，但在大规模实验室检测不可行的情况下，需要新的方法。EHR的普及为这一难题带来了新机会。现有研究多注重结构化数据（如人口统计信息），却常忽视包含丰富潜在信息的临床笔记等非结构化文本。

Method: 提出使用大型语言模型（LLM）分析EHR中的非结构化文本，判断患者是否需要进一步HIV检测。构建并应用一条新型技术流程（pipeline）。

Result: 在鹿特丹伊拉斯谟大学医学中心的临床数据上，该流程实现了高准确率和低假阴性率。

Conclusion: 基于LLM的EHR非结构化文本分析方法有效提升了HIV筛查的准确性和及时性，可为临床决策提供有力支持。

Abstract: Efficient screening and early diagnosis of HIV are critical for reducing
onward transmission. Although large scale laboratory testing is not feasible,
the widespread adoption of Electronic Health Records (EHRs) offers new
opportunities to address this challenge. Existing research primarily focuses on
applying machine learning methods to structured data, such as patient
demographics, for improving HIV diagnosis. However, these approaches often
overlook unstructured text data such as clinical notes, which potentially
contain valuable information relevant to HIV risk. In this study, we propose a
novel pipeline that leverages a Large Language Model (LLM) to analyze
unstructured EHR text and determine a patient's eligibility for further HIV
testing. Experimental results on clinical data from Erasmus University Medical
Center Rotterdam demonstrate that our pipeline achieved high accuracy while
maintaining a low false negative rate.

</details>


### [29] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: 本文系统评估了多个大语言模型在LCA任务中的表现，发现其有助于提高解释质量和减少简单任务劳动力，但存在较高幻觉和不准确风险，尤其在未结合事实验证机制时，不能直接作权威使用，需谨慎应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI和大语言模型在LCA（生命周期评估）领域的应用逐渐增加，但有关其可靠性、鲁棒性及可用性的系统性证据有限，且该领域缺乏标准化评估框架、明确的事实判断依据和共识协议。本文旨在填补这一空白，首创专家基础的LLM评估基准。

Method: 对包括商业和开源在内的11种通用大型语言模型，在22项LCA相关任务上进行评估，由17位具备相关经验的专家依据科学准确性、解释质量、稳健性、可验证性及对指令的遵循等标准，对模型输出进行审查和打分，共收集168份专家评审。

Result: 专家认为37%的模型输出信息不准确或具有误导性。多数模型的准确性和解释质量评分为平均或良好，格式遵循评分普遍较高。不同模型“幻觉”（虚假或捏造信息）率差异显著，最高可达40%。开源模型在准确性和解释质量上有时优于或不逊色于商业模型。

Conclusion: 使用大型语言模型（LLMs）于生命周期评估（LCA）领域具有一定益处，尤其是在解释质量和简化基础任务方面，但如果缺乏事实依据与验证机制，其准确性和可靠性存在较大风险，不能直接作为权威工具使用。

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [30] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: 论文提出用策略卡牌游戏Dixit作为多模态大模型评价新载体，结构化、公正地检验MLM，不仅排名和传统基准一致，还揭示了人机思维差异，助力模型进一步优化。


<details>
  <summary>Details</summary>
Motivation: 现有MLM评估方式（如静态基准或对偶比较）难以全面公正地评价其能力，主观性强、成本高且可能被模型通过套路（如冗长回答）刷分，因此亟需新的评测方法。

Method: 提出以游戏为评测平台，具体以幻想卡牌游戏Dixit为例，由MLM生成卡牌说明并与人类玩家对抗，通过获胜率与现有基准进行比较，并分析人机比赛策略差异。

Result: 五款主流MLM间在Dixit游戏中的获胜率排名与传统基准完全一致，人机对局揭示模型在策略和推理上与人类的不同及改进空间。

Conclusion: 游戏化评估能够更全面、公正地评价多模态大模型（MLM）的能力，并揭示其与人类玩家在推理和策略上的差异。

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [31] [Large Language Model enabled Mathematical Modeling](https://arxiv.org/abs/2510.19895)
*Guoyun Zhang*

Main category: cs.CL

TL;DR: 本文系统评估了DeepSeek-R1语言模型在运筹优化建模中的表现及可行性，通过基准测试和多种幻觉缓解技术，验证了其在实际优化问题表达上的能力和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的运筹优化建模严重依赖领域专家，将实际问题转化为可解的数学模型难度较高。尽管已有强大的数学优化工具和求解器，但定义目标、约束和变量仍需专家参与。研究希望借助大型语言模型（LLMs），特别是DeepSeek-R1，利用自然语言和代码生成能力来缩小专业知识与建模需求之间的鸿沟。

Method: 系统性评估DeepSeek-R1在四个关键运筹优化基准（NL4OPT, IndustryOR, EasyLP, ComplexOR）上的表现。采用基线测试、构建幻觉分类体系，并应用幻觉缓解策略，包括LLM-as-a-Judge、少样本学习（FSL）、工具调用以及多智能体框架，以提升模型在优化问题建模上的准确性和输出的用户需求对齐度。

Result: 研究展示了DeepSeek-R1在多个基准上的性能，并评估了多种减轻幻觉和提升正确建模的策略效果。实验结果表明，通过合适的缓解方案，可以显著提升模型对优化问题的理解和表达能力。

Conclusion: LLMs，特别是DeepSeek-R1，有潜力辅助优化建模过程，减少对领域专家的依赖。通过系统幻觉缓解与提升方法，其在实际运筹优化场景中的应用前景得到显著拓展。

Abstract: The integration of Large Language Models (LLMs) with optimization modeling
offers a promising avenue for advancing decision-making in operations research
(OR). Traditional optimization methods,such as linear programming, mixed
integer programming, and simulation depend heavily on domain expertise to
translate real-world problems into solvable mathematical models. While solvers
like Gurobi and COPT are powerful, expert input remains essential for defining
objectives, constraints, and variables. This research investigates the
potential of LLMs, specifically the DeepSeek-R1 model, to bridge this
formulation gap using natural language understanding and code generation.
Although prior models like GPT-4, Claude, and Bard have shown strong
performance in NLP and reasoning tasks, their high token costs and tendency
toward hallucinations limit real-world applicability in supply chain contexts.
In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained
with reinforcement learning, presents a viable alternative. Despite its success
in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied
OR scenarios remains under explored. This study systematically evaluates
DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and
ComplexOR. Our methodology includes baseline assessments, the development of a
hallucination taxonomy, and the application of mitigation strategies like
LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent
Framework. These techniques aim to reduce hallucinations, enhance formulation
accuracy, and better align model outputs with user intent.

</details>


### [32] [Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation](https://arxiv.org/abs/2510.19897)
*Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: 不通过微调，利用记忆增强框架结合LLM生成的批判信息和标签，能显著提升分类准确率，并揭示不同模型及记忆策略对学习行为的影响。


<details>
  <summary>Details</summary>
Motivation: 传统上，微调大语言模型来学习分类任务的标签往往成本高、灵活性差且难以解释，因此希望找到不通过参数更新也能高效学习的方法。

Method: 提出了一种基于记忆增强的框架：利用情景记忆存储实例级的批判信息，以及语义记忆将其提炼为任务级指导，同时结合LLM生成的批判与标签数据。

Result: 在多个任务上，相比仅使用标签的检索式（RAG）基线方法，结合批判信息后准确率最高提升了24.8%；OpenAI与开源模型在处理事实型与偏好型数据时行为差异明显。提出新的“suggestibility”评价指标用于解释模型对不同监督方式记忆的响应。

Conclusion: 批判驱动的记忆增强学习可以使LLM代理更加适应任务与可解释，有望促进更灵活更透明的模型开发。

Abstract: We investigate how agents built on pretrained large language models can learn
target classification functions from labeled examples without parameter
updates. While conventional approaches like fine-tuning are often costly,
inflexible, and opaque, we propose a memory-augmented framework that leverages
both labeled data and LLM-generated critiques. Our framework uses episodic
memory to store instance-level critiques-capturing specific past
experiences-and semantic memory to distill these into reusable, task-level
guidance. Across a diverse set of tasks, incorporating critiques yields up to a
24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines
that rely only on labels. Through extensive empirical evaluation, we uncover
distinct behavioral differences between OpenAI and opensource models,
particularly in how they handle fact-oriented versus preference-based data. To
interpret how models respond to different representations of supervision
encoded in memory, we introduce a novel metric, suggestibility. This helps
explain observed behaviors and illuminates how model characteristics and memory
strategies jointly shape learning dynamics. Our findings highlight the promise
of memory-driven, reflective learning for building more adaptive and
interpretable LLM agents.

</details>


### [33] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: LyriCAR通过引入自适应课程策略，实现了更高效且高质量的歌词翻译，在英中翻译任务上大幅超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 歌词翻译需要在多项音乐约束间平衡，现有方法依赖人工规则和句子级建模，难以捕捉音乐-语言模式且难以在段落级上实现对跨行连贯性和全局押韵的良好泛化。

Method: 提出了LyriCAR，一个可控的全无监督歌词翻译新框架，引入基于难度感知的课程设计器与自适应课程策略，合理分配训练资源，通过逐步增加难度以加快收敛并提升翻译质量。

Result: 在英中歌词翻译任务上，LyriCAR在标准翻译指标和多维度奖励分数上均取得了超越强基线的最先进表现，自适应课程策略将训练步减少近40%，仍保持优异性能。

Conclusion: LyriCAR框架显著提升了歌词翻译的质量和效率，推动了无监督歌曲翻译的发展。

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [34] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: 本文提出将大型语言模型与符号NLP方法结合，通过文本简化和结构化知识表示，实现对科学文本中数量和因果关系的高效抽取与解释，混合方法表现优于纯符号方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）广泛应用于自然语言处理，但因依赖概率推断，容易出现事实幻觉和结构不一致等错误。与之相比，符号NLP系统具备可解释性和准确推理能力，但覆盖面较窄，扩展和维护难度高。作者希望结合二者优势，解决各自不足。

Method: 提出一种混合方法，将LLMs的广覆盖语言处理能力与符号NLP系统的结构化关系表示能力结合。具体做法是：利用LLMs进行文本改写与简化，填补知识空缺；使用符号NLP系统生成可用于推理和增量学习的结构化表示。

Result: 在从常识科学文本中抽取和解释数量及因果法则的任务上，作者对混合方法、仅符号和仅LLMs三种方法进行了评估。结果显示，混合方法显著优于纯符号方法。

Conclusion: 混合LLMs与符号NLP方法能有效提升自然语言理解任务的表现，兼具广泛覆盖和精确推理能力，优于仅使用符号方法。

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [35] [A Fundamental Algorithm for Dependency Parsing (With Corrections)](https://arxiv.org/abs/2510.19996)
*Michael A. Covington*

Main category: cs.CL

TL;DR: 提出了逐词依存解析算法，模拟人脑处理机制，复杂度$O(n^3)$，但在真实语料中通常效率高于最坏情况。


<details>
  <summary>Details</summary>
Motivation: 目前主流的自然语言解析方法多采用短语结构（成分分析）解析器，但短语结构解析器与人类大脑的解析方式存在差异。本文旨在提出一种更贴近人类大脑处理机制的解析算法。

Method: 提出了一种依存句法解析算法。不同于短语结构解析器，该算法逐词处理句子，在每个词可以确定附属关系时立即进行连接，模拟人类大脑对句子的处理方式。

Result: 该算法每处理一个词就尝试将其附加到依存树上。和短语结构解析器类似，在最坏情况下算法复杂度为$O(n^3)$，但在真实语言中，只在短文本出现最坏情况。

Conclusion: 本文提出的依存句法解析算法更契合人类大脑处理句子的方式，并在复杂度上与常规方法相当，实际应用中具备优势。

Abstract: This paper presents a fundamental algorithm for parsing natural language
sentences into dependency trees. Unlike phrase-structure (constituency)
parsers, this algorithm operates one word at a time, attaching each word as
soon as it can be attached, corresponding to properties claimed for the parser
in the human brain. Like phrase-structure parsing, its worst-case complexity is
$O(n^3)$, but in human language, the worst case occurs only for small $n$.

</details>


### [36] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: 现有医学问答数据集难以代表真实临床决策，作者提出两维度统一范式，规范数据和评估，回顾提升方法并强调效率、可解释性及挑战，为临床领域更有意义的LLMs发展提供指导。


<details>
  <summary>Details</summary>
Motivation: 目前临床领域对大型语言模型（LLMs）进行评估时，多依赖于如MedQA等简化的问答数据集，这些数据集无法充分反映真实临床决策过程。

Method: 提出了一个统一范式，通过“临床背景”和“临床问题”两个维度来刻画临床决策任务，并对现有数据集和基准进行了归纳和总结。同时，回顾并分类了临床决策相关的训练和测试技术，扩展了评估指标至效率和可解释性，并提出了当前面临的开放挑战。

Result: 梳理和标准化了现有的数据集和评估方法，明确了模型开发假设，为临床有意义的LLMs发展提供了指导框架，并指出了未来需要解决的关键问题。

Conclusion: 提出的范式能够更准确地反映真实临床决策情境，为模型比较和优化提供了清晰的标准，有助于推动LLMs在临床应用上的实际价值提升。

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [37] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: 针对现代希腊语NLP不发达及法律领域需求，作者构建了多种高质量数据驱动的新型希腊语嵌入模型，效果显著超越目前主流基线，为希腊文和法律领域人工智能奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现代希腊语作为一个形态丰富但资源中等的语言，在自然语言处理领域发展受到阻碍，尤其在法律等高价值领域，现有模型局限于早期transformer架构和有限的上下文长度（512 token），无法处理长法律文档。

Method: 构建希腊文嵌入模型（Greek Embedding Models），包括广泛的数据整理与高质量过滤，预训练并系统评估多种现代架构（如ELECTRA、ConvBERT和ModernBERT），并首次提出适用于法律领域的希腊语-英语双语嵌入模型。

Result: 新模型在下游任务中的广泛实验表明，新的GEM-RoBERTa和GEM-ConvBERT等模型表现显著优于现有基线模型，验证了所提出方法的有效性。

Conclusion: 高质量数据驱动的新一代希腊语嵌入模型显著提升了处理长文档及法律领域的能力，突破了现有技术局限。

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [38] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: 论文针对预训练语言模型在序列标注任务上的迁移学习提出三项改进：多任务额外信号输入、模型架构双向信息流、结合监督型微调的新序列标注框架。实验结果表明这些方法能够提升模型迁移与适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有序列标注任务中的迁移学习效果有限，尤其在利用预训练神经语言模型适应新领域或新任务时存在一定挑战。论文旨在提出方法提升模型在序列标注任务上的迁移能力。

Method: 论文提出了三项改进：1）多任务模型引入额外信号，将来自领域无关文本处理系统的信息应用于事件触发检测的领域迁移任务；2）修改自回归大语言模型的架构，实现模型层间的双向信息流；3）构建针对自回归大语言模型的序列标注框架，结合监督型in-context微调及以响应为导向的适应策略。

Result: 所提出的模型、方法和框架在序列标注任务上显著提升了预训练神经语言模型的性能，特别是在通过针对性的迁移学习范式进行适应后。

Conclusion: 通过多任务建模、架构优化以及创新的迁移和微调策略，可显著提高预训练语言模型在序列标注任务上的效果，验证了目标迁移学习范式的重要作用。

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [39] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: 针对LLM工具选择难题，ToolScope通过工具合并和检索优化，显著提升了工具选择准确性，并有效应对上下文限制。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，LLM使用的外部工具集极易出现工具冗余或描述重叠，导致选择模糊和准确率降低，并且受限于模型输入长度难以处理大型工具集。

Method: 提出了ToolScope，包括两个关键组件：ToolScopeMerger（结合自动纠错，实现工具合并和冗余消除）和ToolScopeRetriever（对工具相关性进行排序与筛选，压缩工具集合以适应上下文限制）。

Result: 在三种主流LLM和三个开源工具使用基准上的实验表明，ToolScope在工具选择准确率上提升了8.38%到38.6%。

Conclusion: ToolScope显著提高了LLM在复杂任务中工具选择的准确性，能够有效应对实际工具集中的冗余和模糊问题，同时缓解输入上下文限制。

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [40] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出了孟加拉文化知识数据集，发现多语言模型在无上下文情况下难以理解低资源文化知识，而有针对性训练和提供语境信息后效果明显提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多语种范围内表现出色，但在低资源文化方面仍存在明显差距，尤其是对于如孟加拉语这样的文化知识挖掘不足。

Method: 提出并构建了BLanCK（Bengali Language Cultural Knowledge）数据集，涵盖民间传统、烹饪艺术和区域方言，并对多语言模型进行了评估。

Result: 多语言模型在非文化类任务上表现很好，但在文化知识上的表现显著较差。为模型提供上下文后，所有模型在文化类任务上的表现均有大幅提升，显示出对上下文感知和文化定制训练数据的需求。

Conclusion: 强调了语境敏感架构和有文化背景的训练数据对于提升多语言模型低资源文化知识理解的关键作用。

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [41] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: 通过RLAIF和DPO，并利用链式推理数据，显著提升了波斯语医学问答模型的推理性能，优于数据量更大的前代模型，证明了推理专注训练在小数据环境下的高效性。


<details>
  <summary>Details</summary>
Motivation: 提升小型语言模型在医学问答等专业领域的推理能力，特别是应用于波斯语等数据资源稀缺的语言环境。

Method: 采用强化学习与AI反馈（RLAIF）和直接偏好优化（DPO），通过将医学多项选择题数据集翻译为波斯语，生成“优选-拒绝”答案，在教师与学生模型之间用链式推理（CoT）生成正确与错误的推理轨迹，形成包含优选和拒绝答案大规模数据集，训练波斯语基础医学推理模型。

Result: 新模型在仅使用远少于前代模型（gaokerena-V）的训练数据的情况下，医学推理能力显著超越前代模型。

Conclusion: 推理能力强化和偏好优化等专注推理的训练方法，在领域专用、小数据语境下极为高效，有助于提升专用语言模型表现。

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>


### [42] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: 本文提出了创造力评估框架CreativityPrism，从质量、创新性和多样性三维度对17个主流LLM进行了系统评测。结果发现不同类型模型和不同创造力维度表现差异明显，强调了多维评估LLM创造力的必要性。


<details>
  <summary>Details</summary>
Motivation: 目前对于大语言模型（LLM）在创造力方面的总体评测框架缺乏，现有方法分散且不一致，导致在不同领域和任务间的评判标准和结果差异较大。作者希望提出一种系统的方法来全面评估LLM的创造力。

Method: 提出并实现了名为CreativityPrism的评估分析框架，将创造力分解为质量、创新性和多样性三大维度，涵盖九个任务、三个领域（发散性思维、创意写作、逻辑推理）及二十个评价指标。用它对17个主流LLM（包括专有和开源模型）进行了系统性测试和相关性分析。

Result: 评测结果显示，专有模型与开源模型之间在创造力上的表现有明显差距。在同一领域内不同任务之间模型表现高度相关，不同领域间相关性较弱。在三大创造力维度中，多样性和质量表现强相关，创新性与其它两个指标相关度较低。

Conclusion: 单一创造力任务或单一维度的高表现并不能代表其他任务或维度的高创造力表现，LLM创造力评估需要更加全面和多维的框架。

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [43] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: ARTER提出了一种高效实体链接框架，将上下文信号、自适应路由与针对性LLM推理结合。相比传统方法和纯LLM推理，既提升了准确率又显著节省了算力资源。


<details>
  <summary>Details</summary>
Motivation: 传统的实体链接任务依赖大量标注数据和模型深度微调，效率低下且成本高。虽然最新的few-shot方法借助LLM进行prompting降低了训练需求，但却引入了高昂的推理开销。因此，迫切需要一种高效且性能优越的EL方法。

Method: ARTER提出一个结构化的管道，将候选生成、上下文评分、自适应路由与有选择的LLM推理有机结合，通过计算多个互补信号（嵌入及LLM相关）判断实体标注的难易，将容易的样本交由轻量级linker处理，难题则提交给计算昂贵的LLM推理。

Result: 在标准数据集上，ARTER相比ReFinED性能提升最高达4.47%，平均在五个数据集上提升2.53%；与全部采用LLM推理的管道相比，性能相近但LLM推理令牌消耗减少一倍，显著提升效率。

Conclusion: ARTER能够无需深度微调，融合结构化处理流程和针对性LLM推理，有效提升实体链接任务的准确性和推理效率，兼顾性能与计算资源消耗。

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [44] [BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation](https://arxiv.org/abs/2510.20151)
*Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 提出了BoundRL方法解决复杂结构文本的分割，能显著减低推理成本、降低幻觉，并使小模型在此任务上超越大模型。引入RLVR和中间候选机制进一步提升了分割精度和泛化表现，是结构文本分割领域的重要进步。


<details>
  <summary>Details</summary>
Motivation: 随着结构化文本在技术报告和生成式AI提示词等领域日益复杂，传统分句或分段的文本切分方法无法有效应对包含表格、代码、占位符等元素的长文本，因此亟需精细化、语义相关的分割方法。

Method: 提出了BoundRL——一种高效的文本分割和标签预测方法。其核心思想是在原始长文本中，仅预测每个语义片段的起始token，通过定位这些token来重建完整片段，显著减少推理成本并降低幻觉风险。为了优化输出格式，模型采用可验证奖励的强化学习（RLVR），设计奖励函数兼顾文档重建正确性与语义对齐，并通过系统扰动生成中间候选分割，缓解熵坍塌，提升方案质量。

Result: 在复杂结构化文本（如大模型提示词）上，BoundRL使小语言模型（1.7B参数）在分段任务上超越了针对大模型的Few-shot Prompting。采用RLVR及候选扰动方法，性能和泛化均优于传统监督微调。

Conclusion: BoundRL能高效处理结构化长文本的语义分段和标签预测，使小模型具备优于大模型Few-shot Prompting的分割能力，强化学习和中间候选机制显著提升结果质量和泛化性。

Abstract: As structured texts become increasingly complex across diverse domains --
from technical reports to generative AI prompts -- the need for text
segmentation into semantically meaningful components becomes critical. Such
texts often contain elements beyond plain language, including tables, code
snippets, and placeholders, which conventional sentence- or paragraph-level
segmentation methods cannot handle effectively. To address this challenge, we
propose BoundRL, a novel and efficient approach that jointly performs
token-level text segmentation and label prediction for long structured texts.
Instead of generating complete contents for each segment, it generates only a
sequence of starting tokens and reconstructs the complete contents by locating
these tokens within the original texts, thereby reducing inference costs by
orders of magnitude and minimizing hallucination. To adapt the model for the
output format, BoundRL~performs reinforcement learning with verifiable rewards
(RLVR) with a specifically designed reward that jointly optimizes document
reconstruction fidelity and semantic alignment. To mitigate entropy collapse,
it further constructs intermediate candidates by systematically perturbing a
fraction of generated sequences of segments to create stepping stones toward
higher-quality solutions. To demonstrate BoundRL's effectiveness on
particularly challenging structured texts, we focus evaluation on complex
prompts used for LLM applications. Experiments show that BoundRL enables small
language models (1.7B parameters) to outperform few-shot prompting of much
larger models. Moreover, RLVR with our designed reward yields significant
improvements over supervised fine-tuning, and incorporating intermediate
candidates further improves both performance and generalization.

</details>


### [45] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: 本文揭示大型语言模型在立场检测中对不同群体和文本属性存在严重刻板印象，呼吁加强此类任务中的偏见检测与纠正。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型因预训练数据继承了社会偏见，导致在仇恨言论检测等NLP任务中表现出对某些群体的偏见。但对于立场检测任务中的偏见评估却鲜有探讨。鉴于立场检测常与敏感政治议题相关，研究其偏见显得尤为重要。

Method: 作者对现有立场检测数据集中的文本，自动加入“特定群体的方言/语言变体”和“文本复杂度/可读性”两个属性标签，用以检测这些属性对大模型零样本立场检测结果的影响。

Result: 实验发现，大型语言模型在立场检测任务中确实表现出显著的刻板印象，例如错误地将支持大麻观点与低文本复杂度相联系，将非裔美国人方言与反对特朗普关联。

Conclusion: 大型语言模型在立场检测领域存在明显偏见，社群应重视并完善专项评估与纠偏机制，以提升模型的公平性和泛化能力。

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [46] [DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking](https://arxiv.org/abs/2510.20168)
*Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出了DeepWideSearch基准，系统评估多跳深度推理与广度信息收集能力。实验证明现有智能体表现极差，揭示四大失败模式。公开该基准以促进未来更强信息检索智能体研究。


<details>
  <summary>Details</summary>
Motivation: 当前的搜索智能体在多跳检索上的深度推理和大规模信息收集能力方面存在明显不足，这在如市场分析和商业拓展等实际应用场景中是一项关键挑战。为此，迫切需要一个既能评估深度也能评估广度的信息检索基准。

Method: 作者提出了DeepWideSearch，这是首个旨在同时评估信息检索“深度”与“广度”的测试基准。具体方法包括对现有数据集进行两种转换，最终构建包含220个问题、覆盖15个不同领域的题库，用于测试智能体在多数据、多跳推理任务中的表现。

Result: 实验证明，即使是目前最先进的智能体，在DeepWideSearch上的平均成功率仅为2.39%，显示出整合信息检索深度和广度的巨大挑战。错误分析还揭示了四大不足：缺乏反思、过度依赖自身知识、检索不足和上下文溢出，暴露出现有智能体架构的关键局限性。

Conclusion: DeepWideSearch的发布为未来在信息检索领域开发更强大、更健壮的智能体奠定了基础。该工作促使学界关注如何同时提升搜索深度和广度能力，推动相关研究进步。

Abstract: Current search agents fundamentally lack the ability to simultaneously
perform \textit{deep} reasoning over multi-hop retrieval and
\textit{wide}-scale information collection-a critical deficiency for real-world
applications like comprehensive market analysis and business development. To
bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly
designed to evaluate agents to integrate depth and width in information
seeking. In DeepWideSearch, agents must process a large volume of data, each
requiring deep reasoning over multi-hop retrieval paths. Specifically, we
propose two methods to converse established datasets, resulting in a curated
collection of 220 questions spanning 15 diverse domains. Extensive experiments
demonstrate that even state-of-the-art agents achieve only 2.39% average
success rate on DeepWideSearch, highlighting the substantial challenge of
integrating depth and width search in information-seeking tasks. Furthermore,
our error analysis reveals four failure modes: lack of reflection, overreliance
on internal knowledge, insufficient retrieval, and context overflow-exposing
key limitations in current agent architectures. We publicly release
DeepWideSearch to catalyze future research on more capable and robust
information-seeking agents.

</details>


### [47] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: 本论文提出了多智能体结构结合代码执行的表格推理方法，并通过MCTS和强化学习实现自我优化，在公开任务上取得领先表现，展示了多智能体与RL结合推进表格理解的新潜力。


<details>
  <summary>Details</summary>
Motivation: 表格理解和推理是许多实际应用的关键能力，但当前大型语言模型（LLMs）方法各有局限：微调方法容易出现算术错误和幻觉，工具型方法虽精确但缺乏语义理解。因此，亟需结合强语言推理与可靠表格处理的新方法。

Method: 提出了一种名为Mixture-of-Minds的多智能体框架，将表格推理任务分解为计划、编程和答题三大专职角色，并利用代码执行实现精确表格操作。在此基础上，用蒙特卡洛树搜索（MCTS）生成伪金轨迹，并通过强化学习（RL）优化智能体，实现自我提升训练。

Result: Mixture-of-Minds在TableBench任务上取得62.13%的表现，优于OpenAI-o4-mini-high，显示出显著的性能提升。

Conclusion: 通过结构化的多智能体协作流程和强化学习优化，能够有效提升表格理解和推理能力，为后续相关任务提供了新思路。

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [48] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: 本文系统测试了大语言模型在文本空间推理任务上的性能。随着任务复杂度上升，模型准确率显著下降，表现出空间表征能力的缺陷，暴露了语言和空间推理结合的技术短板。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLM）在文本理解方面表现突出，但其空间推理能力尚未深入探讨。研究者希望通过一系列任务，考察模型在理解和解决涉及空间关系的文本任务中的表现与局限。

Method: 设计了五类空间推理任务，包括象限识别、几何变换、距离评估、单词查找和拼图滑动，均基于结构化的网格环境。通过逐步增加网格尺寸，提高任务复杂度，考察模型从基础空间推理到多步问题解决的能力。

Result: 研究结果显示，LLM在低复杂度、小尺寸任务中表现尚可，但随着任务规模和复杂度提升，其准确率平均下降42.7%，最高达84%。所有初始准确率高于50%的测试，准确率均下降至少48%

Conclusion: 当前 LLM 在空间推理任务上缺乏对空间关系的深层表征，规模和复杂度的提升导致表现显著下降。语言模型与空间推理能力之间仍有明显差距，需要进一步提升和融合，以支持更复杂的跨领域任务。

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [49] [Decoding-Free Sampling Strategies for LLM Marginalization](https://arxiv.org/abs/2510.20208)
*David Pohl,Marco Cognetta,Junyoung Lee,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 该论文提出了一种无需大语言模型生成的新型采样方法，可高效近似所有可能子词切分下的概率总和（边缘化），在准确度几乎不损失的前提下，显著加快推理速度并节省算力，便于实际大模型推理任务应用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过子词（subword）切分处理文本，以平衡模型大小、推理速度和词汇覆盖率。但这种切分方式使得在推理评估时，只针对某一具体的切分方式进行概率计算，而同一文本存在多种切分方式。部分最近研究提出应通过边缘化的方式评估模型，即对所有可能切分方式的概率质量进行考虑。

Method: 研究者关注在无需模型生成的情况下，通过与模型及分词器无关的高效采样策略，对所有可能切分进行近似边缘化，从而大幅降低计算复杂度，无需每次采样都进行推理生成。

Result: 研究表明，所提出的解码无关采样策略能够以极低的开销，获得足够准确的边缘概率估计，并可有效应用于多个下游推理任务。

Conclusion: 无生成步骤的采样策略能在大幅降低运行成本的同时，保持较高的边缘概率估算质量，为下游任务推理带来可行高效的解决方案。

Abstract: Modern language models operate on subword-tokenized text in order to make a
trade-off between model size, inference speed, and vocabulary coverage. A side
effect of this is that, during inference, models are evaluated by measuring the
probability of only the specific tokenization produced as the output, despite
there being many possible ways to represent the same text with a subword
vocabulary. Recent studies have argued instead for evaluating LLMs by
marginalization - the probability mass of all tokenizations of a given text.
  Marginalization is difficult due to the number of possible tokenizations of a
text, so often approximate marginalization is done via sampling. However, a
downside of sampling is that an expensive generation step must be performed by
the LLM for each sample, which limits the number of samples that can be
acquired given a runtime budget, and therefore also the accuracy of the
approximation. Since computing the probability of a sequence given the
tokenization is relatively cheap compared to actually generating it, we
investigate sampling strategies that are decoding-free - they require no
generation from the LLM, instead relying entirely on extremely cheap sampling
strategies that are model and tokenizer agnostic.
  We investigate the approximation quality and speed of decoding-free sampling
strategies for a number of open models to find that they provide sufficiently
accurate marginal estimates at a small fraction of the runtime cost and
demonstrate its use on a set of downstream inference tasks.

</details>


### [50] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: 本文提出多模态融合方法，将文本、音频和面部特征同步融合，能分级评估抑郁与PTSD严重度，性能优于单模态，尤其提升PTSD评估效果，支持更可解释和实用的临床决策。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估多为二分类且面向单一疾病，未能满足临床对跨疾病、分级严重度和决策可解释性的迫切需求，尤其是在抑郁与PTSD高共现背景下。

Method: 融合访谈文本（句级transformer嵌入）、音频（Mel统计特征及其变化量）、面部信号（动作单元、凝视、头部与姿势描述符），采用标准化特征和校准后的后期融合分类器输出多类别严重度、类别概率和特征归因。

Result: 三模态融合模型的准确率和加权F1与最优单模态基线持平或更优，决策曲线效用和在缺失/噪声模态下的鲁棒性提升，PTSD回归误差减少且类别一致性提高。文本特征对抑郁评估贡献最大，音频与面部特征对PTSD评估至关重要，归因结果与临床表征一致。

Conclusion: 提出的三模态情感严重度融合框架能够有效地对抑郁症和PTSD进行协同、多级别的严重度评估，模型性能优越，并可为临床决策提供可解释性支持。

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [51] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: 该论文提出ContextLM框架，通过next-context预测提升LLM的长程上下文和语义构建能力，在主流模型和多项任务上实证表现优异，且计算代价低。


<details>
  <summary>Details</summary>
Motivation: 传统的下一词预测（NTP）虽然推动了LLM的能力提升，但局限于单个token，难以捕捉更高层次的语义结构和长程上下文关系。因此，论文致力于克服这一限制。

Method: 提出ContextLM框架，利用next-context prediction目标，使模型能预测多token上下文，通过未来token块生成误差信号，并且完全兼容标准的自回归逐token评估方法。

Result: 在GPT2和Pythia模型家族（参数规模达1.5B）上进行了大量实验，ContextLM在困惑度（perplexity）和下游任务表现上均显著优于基线方法。分析还显示该方法提升了模型的长程连贯性和注意力分配效率，且计算开销很小。

Conclusion: next-context prediction机制是一种可扩展、高效的方法，能显著增强语言模型的能力，特别是在长程语义和注意力分配方面，同时保持低计算成本和现有评估兼容性。

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [52] [Citation Failure: Definition, Analysis and Efficient Mitigation](https://arxiv.org/abs/2510.20303)
*Jan Buchmann,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本研究发现LLM-RAG系统在复杂场景下常因引用不全而影响可验证性。为此，作者设计了新基准（CITECONTROL）深入分析引用失败原因，并提出集成式改进方法（CITENTION），显著提升了引用准确性与表现，对相关领域有重要借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 目前基于大语言模型（LLM）的检索增强生成（RAG）系统在引用机制方面存在问题，主要体现在模型生成了有用的回答但引用的证据不完整，即引用失败。这直接影响了用户对系统回应的验证能力。本文旨在区分引用失败与回应失败（回应内容本身有缺陷且无法充分引用证据），并探索引用失败的成因及改进方法。

Method: 论文采用了两步法：首先，系统性地分析了回应与证据关系如何影响引用质量，提出了CITECONTROL基准，用来在不同证据-回应关系下测试引用失败的模式；其次，为提升大语言模型的引用效率，提出了CITENTION框架，将生成式、注意力机制驱动、检索驱动的方法集成使用。

Result: 实验显示，随着回应与证据的关系复杂度增加，引用失败显著增多；而多种引用方法的结合能够有效提升引用表现。使用CITENTION框架后，在CITECONTROL基准及迁移任务上均显著提升了引用质量。

Conclusion: 本文通过区分引用失败和回应失败，提出了对应的基准和集成框架，有效提升了LLM-RAG系统的引用质量，推动了可验证AI的发展，数据与代码已公开以促进研究复现。

Abstract: Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.

</details>


### [53] [Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering](https://arxiv.org/abs/2510.20304)
*Lei Tang,Wei Zhou,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 首次系统评估了过程奖励模型在表格问答任务中的应用，发现虽然代码和文本混合验证PRM有助提升答案选择，但在跨领域和步骤联系弱的情况下效果有限，未来需设计更适合TQA特点的过程验证方法。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型（PRMs）在数学等领域已被证实能够提升大语言模型（LLMs）的复杂推理能力，但其在处理如表格问答（TQA）这类半结构化数据任务中的表现尚未被系统研究。TQA任务存在大量无关信息、推理步骤松散及专业领域推理等独特挑战。

Method: 本文系统性地研究了最先进的生成式过程奖励模型在TQA中的表现，从答案和推理步骤两个角度进行评估，并进一步分析性能影响因素。分析还包括代码和文本验证方式的结合。

Result: 结果显示，结合文本与代码验证的PRMs有助于选择正确答案，但在跨领域泛化方面表现较弱。此外，步骤级验证与最终答案准确率间的相关性较低，表明TQA推理步骤之间因果链较弱。

Conclusion: 现有的过程奖励模型在TQA任务中存在一定局限性，尤其在步骤依赖弱和领域泛化时表现不佳。本文结果为构建更健壮、具过程感知能力的验证器提供了重要启示。

Abstract: Process reward models (PRMs) improve complex reasoning in large language
models (LLMs) by grading candidate solutions step-by-step and selecting answers
via aggregated step scores. While effective in domains such as mathematics,
their applicability to tasks involving semi-structured data, like table
question answering (TQA) remains unexplored. TQA poses unique challenges for
PRMs, including abundant irrelevant information, loosely connected reasoning
steps, and domain-specific reasoning. This work presents the first systematic
study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from
both answer and step perspectives. Results show that PRMs that combine textual
and code verification can aid solution selection but struggle to generalize to
out-of-domain data. Analysis reveals a weak correlation between performance in
step-level verification and answer accuracy, possibly stemming from weak step
dependencies and loose causal links. Our findings highlight limitations of
current PRMs on TQA and offer valuable insights for building more robust,
process-aware verifiers.

</details>


### [54] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: 为解决大模型与代码解释器协作推理时的效率与准确性问题，本文提出CoRT框架和Hint-Engineering策略，通过数据合成与后训练显著提升模型在数学推理上的能力和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模推理模型（LRMs）在自然语言推理方面表现优异，但在应对复杂数学运算时存在效率低和不准确等问题。将代码解释器（CI）引入推理流程能提升模型的数学能力，但模型内部概率推理与CI外部确定性知识之间的冲突导致非最优推理。本文旨在解决模型与CI联合使用中的高效协作问题。

Method: 提出了一种新颖的后训练框架CoRT（Code-Optimized Reasoning Training），主要包括Hint-Engineering的数据合成策略，通过在推理路径上有策略地注入多样化提示，生成高质量、代码集成的推理数据用于模型微调。进一步采用拒绝采样和强化学习优化模型在多轮CI调用与内部思考的交替过程。

Result: 在五个具有挑战性的数学推理数据集上，使用CoRT训练后，DeepSeek-R1-Distill-Qwen-32B与1.5B模型的能力分别提升了4%和8%；同时，推理效率大幅提高，32B模型的token消耗减少约30%，1.5B模型则减少50%。

Conclusion: CoRT在提升大型推理模型复杂数学任务准确性和推理效率方面表现突出，有效优化了模型与代码工具的协同推理，且相关模型和代码已开源。

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [55] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: 该论文发现当前大语言模型在表格推理任务上的高表现，部分来自于语义线索引发的数据集污染，并非真正泛化能力，建议日后评测需注意防止语义泄露。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常被用于结构化数据推理任务，但现有评测可能存在“数据集污染”这一被忽视的混淆因素。

Method: 通过一系列受控探查实验，考察LLMs在面对常用表格数据集（如Adult Income、Titanic等）时是否展示出先验知识，尤其关注数据集中是否有强语义线索（如有意义的列名和可解释的取值类别）。

Result: 发现仅当数据集中存在强语义线索时，模型表现出“污染”效应（即表现异常好）；一旦移除或随机化这些语义线索，模型性能下降到接近随机猜测的水平。

Conclusion: LLMs在表格推理任务中的优秀表现，很大程度上可能得益于对公开数据集的记忆而非真实的泛化能力。作者建议今后在评测时应有防范语义泄露的策略，以更准确反映模型的推理能力。

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [56] [FreeChunker: A Cross-Granularity Chunking Framework](https://arxiv.org/abs/2510.20356)
*Wenxuan Zhang,Yuan-Hao Jiang,Yonghe Wu*

Main category: cs.CL

TL;DR: 本论文提出了FreeChunker，一种跨粒度编码框架，可灵活支持任意句子组合的检索，突破了传统固定粒度分块的局限。实验结果表明，其在检索性能和计算效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的检索增强生成（RAG）系统分块方法粒度固定，边界静态，难以适应不同查询需求，且语义边界检测计算成本高。

Method: 该方法将句子作为最小原子单位，将传统静态分块方式转变为灵活支持任意句子组合的动态检索，无需复杂的语义边界检测。

Result: 在LongBench V2上的实验表明，FreeChunker在检索效果及计算效率方面均显著优于传统分块方式和现有方法。

Conclusion: FreeChunker能够提升检索性能，同时大幅降低语义边界检测的计算开销，适用于多样和复杂的查询场景。

Abstract: Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.

</details>


### [57] [Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)](https://arxiv.org/abs/2510.20358)
*Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 专注对话数据预训练的小型语言模型在通用任务上效果一般，但对话相关任务效果优秀，DPO微调进一步提升表现。


<details>
  <summary>Details</summary>
Motivation: 研究是否仅使用对话数据进行预训练，可以获得在形式和功能上表现更佳的小型语言模型。

Method: 对语言模型仅用对话数据进行预训练，并通过多种微调方法（如PPO和DPO）提升模型生成更具交流性的文本能力。

Result: 在大多数标准BabyLM基准测试上表现不佳，但在自定义对话续写设置下表现优异。DPO微调能进一步提升定制对话基准的表现。PPO微调效果不稳定甚至有负面影响。

Conclusion: 仅对话数据预训练的小型语言模型适合对话任务，在对话续写及相关应用中有潜力。

Abstract: We investigate whether pre-training exclusively on dialogue data results in
formally and functionally apt small language models. Based on this pre-trained
llamalogue model, we employ a variety of fine-tuning strategies to enforce
"more communicative" text generations by our models. Although our models
underperform on most standard BabyLM benchmarks, they excel at dialogue
continuation prediction in a minimal pair setting. While PPO fine-tuning has
mixed to adversarial effects on our models, DPO fine-tuning further improves
their performance on our custom dialogue benchmark.

</details>


### [58] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文发现大语言模型在面对否定表达时，幻觉检测能力明显下降，模型往往难以理解由否定引起的语境转变，并易做出错误判断。提出了NegHalu数据集辅助相关研究。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLMs）在自然语言处理中的幻觉检测研究取得进展，但关于否定文本对幻觉检测影响的研究仍较少。本文旨在弥补这一研究空白。

Method: 文章设置了三个关键的研究问题，并通过重构现有幻觉检测数据集，构建了包含否定表达的NegHalu数据集。实验中考察了LLMs在否定语境下的表现，并追踪了模型处理否定输入时的内部状态。

Result: 实验表明，LLMs在检测否定文本中的幻觉表现不佳，常产生逻辑不一致或与事实不符的判断。

Conclusion: LLMs在处理否定文本的幻觉检测时存在显著挑战，难以像肯定语境下那样做出可靠区分，对否定表达带来的语境变化较难感知和应对。

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [59] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文发布了面向越南交通标志法规的多模态法律问答任务及数据集，并首次给出基准结果，推动了相关领域研究。


<details>
  <summary>Details</summary>
Motivation: 多模态法律文本处理，尤其在越南交通标志法规领域，还缺乏相关研究和高质量基准数据集，限制了智能法律系统的发展。

Method: 设计了VLSP 2025 MLQA-TSR多模态法律问答任务，分为多模态法律检索和多模态问答两个子任务，并构建了针对越南交通标志法规的多模态数据集。

Result: 在VLSP 2025 MLQA-TSR任务上，多模态法律检索子任务取得了64.55%的F2分数，多模态问答子任务取得了86.30%的准确率。

Conclusion: 作者推动了越南交通标志法规相关的多模态法律问答研究，提供了数据基准和评价体系，为该领域智能系统的发展奠定了基础。

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [60] [NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew](https://arxiv.org/abs/2510.20386)
*Shaltiel Shmidman,Avi Shmidman,Moshe Koppel*

Main category: cs.CL

TL;DR: 本文提出了专注于希伯来语的NeoDictaBERT与NeoDictaBERT-bilingual模型，大幅提升了希伯来语NLP基准任务表现，并将模型开源推动相关研究发展。


<details>
  <summary>Details</summary>
Motivation: BERT等早期模型虽然参数较小表现优异，但其结构已落后于新一代transformer模型（如Llama3，Qwen3），最近有多种新架构（如ModernBERT、NeoBERT）显著提升了性能，尤其在扩展上下文窗口方面。作者以此为动机，针对希伯来语开发新模型，弥补现有BERT类模型在希伯来语自然语言处理领域的不足。

Method: 模型采用NeoBERT的架构，并专门以希伯来语文本为核心训练，NeoDictaBERT-bilingual则拓展为双语结构，整体训练流程和效果分别进行了详细描述和基准评测。

Result: NeoDictaBERT和NeoDictaBERT-bilingual模型在希伯来语各项基准测试中几乎全面超越当前模型，特别是NeoDictaBERT-bilingual在检索任务上优于同等规模多语言模型。

Conclusion: 新提出的两种BERT结构模型为希伯来语自然语言处理提供了更“强大”的基础，并在多项任务上树立新基准，具有重要应用和研究价值。

Abstract: Since their initial release, BERT models have demonstrated exceptional
performance on a variety of tasks, despite their relatively small size
(BERT-base has ~100M parameters). Nevertheless, the architectural choices used
in these models are outdated compared to newer transformer-based models such as
Llama3 and Qwen3. In recent months, several architectures have been proposed to
close this gap. ModernBERT and NeoBERT both show strong improvements on English
benchmarks and significantly extend the supported context window. Following
their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:
BERT-style models trained using the same architecture as NeoBERT, with a
dedicated focus on Hebrew texts. These models outperform existing ones on
almost all Hebrew benchmarks and provide a strong foundation for downstream
tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on
retrieval tasks, outperforming other multilingual models of similar size. In
this paper, we describe the training process and report results across various
benchmarks. We release the models to the community as part of our goal to
advance research and development in Hebrew NLP.

</details>


### [61] [Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction](https://arxiv.org/abs/2510.20411)
*Suchir Salhan,Hongyi Gu,Donya Rooein,Diana Galvan-Sosa,Gabrielle Gaudeau,Andrew Caines,Zheng Yuan,Paula Buttery*

Main category: cs.CL

TL;DR: 本文提出ContingentChat方法提升了儿童风格对话中应激性的自然度和连贯性，但完全达到真实互动的‘应激性’仍然存在挑战。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中尤其是儿童与看护者的交流，往往展现出“应激性”，即对话者之间即时、直接且有意义的互动。本文关注提升这类应激性对话的自然与连贯性。

Method: 作者设计了一套教师-学生（teacher-student）框架，将新构建的对其数据集用于对BabyLM模型的后训练，检测和提升其在多轮应激性对话的能力。此外，还实验了教师自适应解码，但获得的增益有限。

Result: 作者提出的ContingentChat框架显著提升了经过后训练的BabyLM模型在多轮应激性对话任务上的表现，生成的回复更符合语法且更加连贯。同时，实验发现教师自适应解码策略带来的进一步提升有限。

Conclusion: 有针对性的对话后训练能显著提高小型语言模型（BabyLM）多轮应激性对话的质量，但完全还原人类对话中的应激性仍需要进一步研究。

Abstract: Multi-turn dialogues between a child and a caregiver are characterized by a
property called contingency - that is, prompt, direct, and meaningful exchanges
between interlocutors. We introduce ContingentChat, a teacher-student framework
that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M
words. Using a novel alignment dataset for post-training, BabyLM generates
responses that are more grammatical and cohesive. Experiments with adaptive
teacher decoding strategies show limited additional gains. ContingentChat
demonstrates the benefits of targeted post-training for dialogue quality and
indicates that contingency remains a challenging goal for BabyLMs.

</details>


### [62] [LM-mixup: Text Data Augmentation via Language Model based Mixup](https://arxiv.org/abs/2510.20449)
*Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种将大量低质量、冗余的指令数据蒸馏为高质量、连贯的指令-输出对的新方法，并构建了大规模数据集MIXTURE与新算法LM-Mixup，有效提升了大模型指令微调的数据利用效率和性能。


<details>
  <summary>Details</summary>
Motivation: 高质量指导数据稀缺且宝贵，低质量数据虽多但常被丢弃。现有方法难以有效利用低质数据，且相关评估标准不明确。因此，作者希望通过设计系统化的蒸馏与增强流程，使低质量数据转化为高质量资源，提升大模型训练效果。

Method: 作者首先提出了Instruction Distillation任务，并构建了包含144K低质量指令聚类与其高质量蒸馏结果的数据集MIXTURE。然后提出LM-Mixup方法，先在MIXTURE上监督微调，再用强化学习优化，联合考量数据质量、语义对齐和格式合规性，通过Group Relative Policy Optimization进行训练增强。

Result: 用LM-Mixup蒸馏后的数据，仅占总数据集3%，用于微调的结果不仅超过全量数据训练效果，还跟最先进的数据精选方法媲美。证明低质量数据经过蒸馏与增强有极大利用价值，可显著提升模型效率和性能。

Conclusion: 即使是低质量数据，经过合理蒸馏处理和优化，也能极大提升指令微调大模型的性能，充分利用原本会被丢弃的数据资源。

Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet
the quality of instruction-following data varies significantly. While
high-quality data is paramount, it is often scarce; conversely, abundant
low-quality data is frequently discarded, leading to substantial information
loss. Existing data augmentation methods struggle to augment this low-quality
data effectively, and the evaluation of such techniques remains poorly defined.
To address this, we formally define the task of Instruction Distillation:
distilling multiple low-quality and redundant inputs into high-quality and
coherent instruction-output pairs. Specifically, we introduce a comprehensive
data construction pipeline to create MIXTURE, a 144K-sample dataset pairing
low-quality or semantically redundant imperfect instruction clusters with their
high-quality distillations. We then introduce LM-Mixup, by first performing
supervised fine-tuning on MIXTURE and then optimizing it with reinforcement
learning. This process uses three complementary reward signals: quality,
semantic alignment, and format compliance, via Group Relative Policy
Optimization (GRPO). We demonstrate that LM-Mixup effectively augments
imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for
only about 3% of the entire dataset, not only surpasses full-dataset training
but also competes with state-of-the-art high-quality data selection methods
across multiple benchmarks. Our work establishes that low-quality data is a
valuable resource when properly distilled and augmented with LM-Mixup,
significantly enhancing the efficiency and performance of instruction-tuned
LLMs.

</details>


### [63] [Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models](https://arxiv.org/abs/2510.20460)
*Christian Hobelsberger,Theresa Winner,Andreas Nawroth,Oliver Mitevski,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 评估了四种LLM输出置信度估计方法，发现CoCoA方法最为可靠，并针对不同应用建议了不确定性度量选择。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）尽管表现强大，但其输出的准确性和可靠性并不稳定，因此需要对输出的不确定性进行量化，以提升其实际应用的可靠性。

Method: 系统评估了四类LLM输出置信度估计方法（VCE、MSP、Sample Consistency、CoCoA），在四个问答任务上用先进开源LLM进行实验，并分析各方法表现及实际应用权衡。

Result: 实验表明，不同的不确定性度量方法关注模型信心的不同方面。其中，混合型CoCoA方法在整体可靠性上表现最佳，显著提升了答案的校准性和甄别能力。

Conclusion: 每种方法各有优劣，作者给出了应用中选择不确定性度量的建议。混合型CoCoA方法在改进LLM答案可靠性方面效果最佳。

Abstract: Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.

</details>


### [64] [Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs](https://arxiv.org/abs/2510.20475)
*Lukas Edman,Alexander Fraser*

Main category: cs.CL

TL;DR: 本论文通过自适应Masked Language Modeling和引入子词嵌入，大幅提升了小规模语言模型在基准任务上的表现，超过了传统方法。


<details>
  <summary>Details</summary>
Motivation: 提升小规模语言模型在语言理解任务中的性能，并改善模型的泛化能力和英语形态泛化表现。

Method: 改进型MLM，通过调整mask概率以适应模型对token的预测能力；同时融合子词嵌入以提升形态泛化能力。

Result: 在(Super)GLUE任务上显著优于标准的MLM方法，并在strict-small track上击败了基线模型。

Conclusion: 提出的自适应Masked Language Modeling方法和子词嵌入策略，在BabyLM Challenge的strict-small track上超越了基线模型。

Abstract: We describe our strategy for the 2025 edition of the BabyLM Challenge. Our
main contribution is that of an improved form of Masked Language Modeling
(MLM), which adapts the probabilities of the tokens masked according to the
model's ability to predict them. The results show a substantial increase in
performance on (Super)GLUE tasks over the standard MLM. We also incorporate
sub-token embeddings, finding that this increases the model's morphological
generalization capabilities. Our submission beats the baseline in the
strict-small track.

</details>


### [65] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: 提出RECALL框架，通过聚类样本的层级表示计算模型间相似度，实现无需历史数据的持续学习模型合并。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖任务标签或有性能权衡，且常需访问历史数据，难以实现模型随进化而有效整合知识，易发生灾难性遗忘。

Method: 通过提取大语言模型的中间隐藏表示，聚类典型样本，计算层级相似度，并进行自适应参数融合，结合浅层领域泛化和深层任务特定能力。

Result: RECALL支持多领域无缝集成，大幅缓解灾难性遗忘，在知识保持与泛化上领先，并且不依赖历史数据。

Conclusion: RECALL在五个NLP任务和多种持续学习场景下，能更好地保留知识和推广泛化，优于现有方法。

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [66] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: 本文关注大语言模型在评测时的表现调整问题，并提出通过添加“steering vector”指令，抑制模型的评测感知，使其在评测时行为更接近实际部署环境，提高安全评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大模型逐渐具备“知道自己在被评测”的能力，导致在评测阶段行为刻意调整，影响安全评估的真实性和可靠性。因此，需要找到方法屏蔽或消除模型的评测感知。

Method: 作者设计了两步训练流程，首先通过持续预训练让模型习得对评测线索的识别和相应行为调整，接着使用专家迭代训练强化其在评测环境下的特殊行为。之后，采用激活引导（steering vector）技术干预模型激活值，实验证明该方式能显著减少评测意识。

Result: 经过设计的仿真实验，发现激活引导可以使模型在检测到评测线索情况下依然展现如同部署环境下的行为，减小评测与真实使用之间的差异，有助于提升安全评估结果的可靠性。

Conclusion: 通过激活引导（activation steering）方法，可以有效抑制模型的评测意识，使其在评测阶段的表现更贴近部署实际，提升安全评估的可信度。

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [67] [Robust Preference Alignment via Directional Neighborhood Consensus](https://arxiv.org/abs/2510.20498)
*Ruochen Mao,Yuling Shi,Xiaodong Gu,Jiaheng Wei*

Main category: cs.CL

TL;DR: 提出RPS算法，通过在偏好空间邻域采样响应并择优，显著提升大模型对边缘人类偏好的鲁棒对齐能力，无需重训，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多按平均偏好训练，难以针对特定、稀有甚至复杂个人需求做出高质量响应，且现有方法需昂贵重训练且一般化能力弱，因此急需提升模型在多样化人类需求下的鲁棒性及实际表现。

Method: RPS是一种训练后无需修改模型的方法，通过在用户指定偏好附近采样多个响应，再选择与原始目标最一致者提升回复质量，理论分析和实验均支持此策略优于直接多响应采样。

Result: 本文提出了一种新的方法——RPS（Robust Preference Selection），用于大语言模型的偏好对齐问题，特别关注在训练数据无法覆盖的多样化人类偏好下模型表现的鲁棒性。大量实验在三种主流对齐范式（DPA、DPO、SFT）中显示，RPS能显著提高模型在“非主流”偏好上的响应质量，无需模型重训练。

Conclusion: RPS能有效弥补大语言模型在少数或特殊人类偏好上的覆盖缺口，提高模型在多样化偏好需求下的可靠性和一致性，为偏好对齐带来实用且理论支持的解决方案。

Abstract: Aligning large language models with human preferences is critical for
creating reliable and controllable AI systems. A human preference can be
visualized as a high-dimensional vector where different directions represent
trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet,
because the training data often reflects dominant, average preferences, LLMs
tend to perform well on common requests but fall short in specific, individual
needs. This mismatch creates a preference coverage gap. Existing methods often
address this through costly retraining, which may not be generalized to the
full spectrum of diverse preferences. This brittleness means that when a user's
request reflects a nuanced preference deviating from the training data's
central tendency, model performance can degrade unpredictably. To address this
challenge, we introduce Robust Preference Selection (RPS), a post-hoc,
training-free method by leveraging directional neighborhood consensus. Instead
of forcing a model to generate a response from a single, highly specific
preference, RPS samples multiple responses from a local neighborhood of related
preferences to create a superior candidate pool. It then selects the response
that best aligns with the user's original intent. We provide a theoretical
framework showing our neighborhood generation strategy is provably superior to
a strong baseline that also samples multiple candidates. Comprehensive
experiments across three distinct alignment paradigms (DPA, DPO, and SFT)
demonstrate that RPS consistently improves robustness against this baseline,
achieving win rates of up to 69% on challenging preferences from
under-represented regions of the space without any model retraining. Our work
presents a practical, theoretically-grounded solution for enhancing the
reliability of preference-aligned models.

</details>


### [68] [Hierarchical Sequence Iteration for Heterogeneous Question Answering](https://arxiv.org/abs/2510.20505)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.CL

TL;DR: 本文提出了一种名为HSEQ（Hierarchical Sequence Iteration）的新型RAG检索增强生成框架，利用结构化迭代方法统一处理文本、表格和知识图谱问答任务，在HotpotQA、HybridQA/TAT-QA和MetaQA数据集上取得更高准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在多跳问题、异构证据时面临准确率、延迟和资源消耗的权衡，且缺乏统一处理多模态数据的能力，难以高效、可靠地支持复杂问答需求。

Method: 方法包括：将文本、表格和知识图谱线性化为含有结构信息的序列，通过Head Agent指导检索，Iteration Agent按结构关系迭代选取和扩展证据，最终由Head Agent综合标准化证据生成答案，必要时进行补充纠错循环。

Result: 实验证明，HSEQ在多类公开异构问答数据集上，优于单步、多跳及已有代理RAG方法，显著提升准确率（EM/F1分数）且减少无效操作与资源消耗，并提高结果的可靠性和审计可追溯性。

Conclusion: HSEQ方法在多源异构证据问答中表现出更好的EM/F1分数，且高效节省资源，并在提升一致性与审计性上展现优势。同时具备格式无关、指导性迭代及证据标准化三大优点。

Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.

</details>


### [69] [Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset](https://arxiv.org/abs/2510.20508)
*Paul Lerner,François Yvon*

Main category: cs.CL

TL;DR: 本文提出用多语言翻译公平性评估LLMs的政治偏见，通过大规模欧洲议会演讲数据实证，发现主流党派翻译优于局外党派，凸显翻译中的系统性政治偏见。


<details>
  <summary>Details</summary>
Motivation: 以往对大型语言模型（LLMs）政治偏见的评估多以英文问卷回答为主。本研究提出了新的偏见衡量框架，将关注点转向多语言翻译中的公平性原则。

Method: 通过对欧洲议会演讲进行多语言翻译质量的系统性比较，具体观察来自左翼、中间派、右翼多数党和局外党派的演讲在翻译质量上的表现差异。研究基于新构建的21语种多平行语料库EuroParl，该语料库详细记录了发言人的政治背景。

Result: 结果显示，多数党（左、中、右）的演讲在翻译时，质量系统性地优于局外党派，反映出翻译系统中的政治偏见。

Conclusion: 通过引入多语言翻译公平性的视角，本研究揭示了在大型语言模型中普遍存在的政治偏见问题，特别是对主流与非主流政党的不公平翻译表现。

Abstract: The political biases of Large Language Models (LLMs) are usually assessed by
simulating their answers to English surveys. In this work, we propose an
alternative framing of political biases, relying on principles of fairness in
multilingual translation. We systematically compare the translation quality of
speeches in the European Parliament (EP), observing systematic differences with
majority parties from left, center, and right being better translated than
outsider parties. This study is made possible by a new, 21-way multiparallel
version of EuroParl, the parliamentary proceedings of the EP, which includes
the political affiliations of each speaker. The dataset consists of 1.5M
sentences for a total of 40M words and 249M characters. It covers three years,
1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of
national parties.

</details>


### [70] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: 本文提出了一种新的上下文压缩方法ARC-Encoder，通过将上下文压缩为更少的连续表示，替换解码器LLM中的token嵌入，从而提升推理效率，降低推理成本。该方法无需微调主模型或更改其结构，具有更好的通用性和适配性。


<details>
  <summary>Details</summary>
Motivation: 随着利用检索增强生成、链式思维推理等技术，LLM推理上下文长度和成本持续增加，现有的上下文压缩方法通常需要微调或修改模型本身且影响模型泛用性。本文旨在研究无需修改LLM本身即可实现高效上下文压缩的新方法。

Method: 文章系统性研究了编码器的训练策略和结构选择，提出ARC-Encoder，将文本token映射为数量更少的连续表示。作者对其在不同LLM应用场景（如in-context learning、上下文窗口扩展等）进行了广泛评估。

Result: ARC-Encoder在多种LLM场景下均表现优异，既提升了推理效率，也可实现同一编码器适配多个不同解码器，有很好的泛用性和移植性。

Conclusion: ARC-Encoder在多项基准测试中达到了SOTA（最先进）性能，并显著提升了推理时的计算效率。该方法能够适配多种解码器LLM，实现编码器的通用迁移，适用于多场景的压缩需求。

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [71] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: 语言模型在句子理解任务中，会因语句复杂性日益依赖语义模式匹配而非深度语法结构分析。作者提出的新数据集可检测模型何时从结构分析转为语义匹配。



<details>
  <summary>Details</summary>
Motivation: 当前缺少区分语言模型是否真正理解语法结构还是单纯依赖语义模式的评测方法。作者希望通过构建新型句子和问题集，揭示模型结构性与语义性理解的边界和条件。

Method: 构建9,720个包含不同复杂度中心嵌入句子的问题，每个句子配语义合理与不合理版本，并设计六种问题类型（表层、结构、因果）。并测试六种语言模型，量化模型在不同复杂度下结构与语义表现。


Result: 本论文提出了CenterBench数据集，通过对中心嵌入句子的理解测试（如“狗追的猫叫了”），研究语言模型在理解语法结构与语义模式匹配之间的区别。通过对句子的语义合理性与语法结构进行区分，量化模型在复杂语法结构下何时放弃结构分析转而依赖语义。


Conclusion: 实验表明，模型在复杂结构下对语义合理句和不合理句的表现差距显著，准确度降低，说明模型趋向语义捷径而不是结构性理解；对结果性问题更显著。人类表现则更为多变。CenterBench成为检测模型结构与语义依赖的首个框架，为语言理解评价提供新思路。


Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [72] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: 作者针对多跳问答中强化学习无法全局规划和忠实执行的难题，提出GlobalRAG框架，结合分解子目标、奖励设计及训练策略，显著提升性能并大幅减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在提升检索增强生成（RAG）上表现突出，但在多跳问答（QA）任务中仍面临两大核心挑战：缺乏全局规划以组织多步推理，以及执行不忠实，导致检索证据和问题推理不连贯。

Method: GlobalRAG将问题分解为多个子目标，并在每个子目标下协调检索与推理，迭代优化证据。创新性地设计了“规划质量奖励”和“子目标完成奖励”，提升推理链条的连贯性与执行稳定性。同时，通过递进式权重退火策略，兼顾过程与结果两个目标，实现更稳健的训练。

Result: 提出的GlobalRAG方法在多跳问答上远超强基线模型，仅用8k训练数据（基线数据量的42%），在EM和F1上平均提升14.2%。

Conclusion: GlobalRAG通过全局规划和分阶段奖励机制，有效提升了多跳问答任务中的推理能力和检索执行的忠实度，为强化学习在复杂QA任务中的应用提供新思路。

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [73] [Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search](https://arxiv.org/abs/2510.20567)
*Zhouwei Zhai,Mengxiang Chen,Haoyun Xia,Jin Li,Renquan Zhou,Min Yang*

Main category: cs.CL

TL;DR: 论文提出了一种新的多智能体认知决策框架（MACDF），用于改善电商搜索的智能推荐和决策支持，特别适用于复杂搜索场景。


<details>
  <summary>Details</summary>
Motivation: 传统基于检索与排序的电商搜索模式难以满足用户复杂、多维度的决策过程，存在语义间隙、高决策成本和缺乏专业指导等关键问题，有待创新突破。

Method: 采用多智能体认知决策框架，突破传统检索排序范式，通过主动决策支持替代被动匹配模型，并在实际平台进行离线评估和线上A/B测试验证。

Result: MACDF在复杂查询（如否定、多约束和推理需求）场景下，推荐准确性大幅提升，用户满意度显著增强，并在京东平台的线上测试中展示了实际应用效果。

Conclusion: MACDF显著提升了推荐准确度和用户满意度，证实了多智能体认知系统在电商搜索中的变革性潜力。

Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its
reliance on query-item matching fundamentally misaligns with multi-stage
cognitive decision processes of platform users. This misalignment introduces
critical limitations: semantic gaps in complex queries, high decision costs due
to cross-platform information foraging, and the absence of professional
shopping guidance. To address these issues, we propose a Multi-Agent Cognitive
Decision Framework (MACDF), which shifts the paradigm from passive retrieval to
proactive decision support. Extensive offline evaluations demonstrate MACDF's
significant improvements in recommendation accuracy and user satisfaction,
particularly for complex queries involving negation, multi-constraint, or
reasoning demands. Online A/B testing on JD search platform confirms its
practical efficacy. This work highlights the transformative potential of
multi-agent cognitive systems in redefining e-commerce search.

</details>


### [74] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: 本文验证了ChatGPT在协作沟通数据编码中的偏见问题，结果表明其编码无明显性别或种族偏见，具备大规模评估应用潜力。


<details>
  <summary>Details</summary>
Motivation: 对沟通与协作的大规模评估依赖于大量人工编码数据，但这种工作极为耗时。虽然已有研究显示ChatGPT能按既定标准进行自动编码且结果与人工标注者接近，但其自动编码结果是否存在性别和种族等人口学偏见则尚不明确。为推进自动化评估方法应用，需要验证AI编码是否公正。

Method: 本研究采用典型协作问题解决编码框架，让ChatGPT自动编码三类协作任务（谈判、问题解决、决策）中的沟通数据，并对编码结果在不同性别和种族群体间进行差异性分析。

Result: 分析结果显示，ChatGPT在不同性别和种族群体的数据编码上，没有表现出显著偏见。

Conclusion: ChatGPT可在不引入明显性别或种族偏见的前提下，完成协作与沟通的数据自动编码，为其在大规模自动化评估中的应用奠定了基础。

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [75] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 本研究通过微调三种预训练模型参与阿拉伯语AI文本检测任务，发现多语言模型优于专用模型，凸显其强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估不同预训练变换器模型在阿拉伯语AI生成文本检测任务中的效果，比较专用模型与多语言模型的表现。

Method: 分别对AraELECTRA、CAMeLBERT和XLM-RoBERTa三种模型在给定数据集上进行微调，以完成二分类任务，并进行效果比较。

Result: 多语言模型XLM-RoBERTa以F1分数0.7701获得最高性能，超过了专门针对阿拉伯语的AraELECTRA和CAMeLBERT。团队在比赛中获得第五名。

Conclusion: 多语言预训练模型在阿拉伯语AI文本检测方面展现出比专用阿拉伯语模型更优的性能，表明其广泛的泛化能力和应用潜力。

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [76] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本论文通过基于人类好奇心量表（5DCR）制定评估框架，发现LLMs具备比人类更强的求知欲和好奇相关学习潜能，但在不确定性时仍较保守。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备类人好奇驱动的学习能力，以及好奇心与模型思考、学习能力之间的关系。

Method: 采用基于5DCR量表的多维评估框架，涵盖信息寻求、刺激寻求、社交好奇等维度，系统评估LLMs展现的好奇心与人类比较，并分析其推理能力。

Result: 大语言模型（LLMs）展现出比人类更强的求知欲，但在面对不确定环境时仍倾向于保守选择。好奇行为能增强模型的推理和主动学习能力。这表明LLMs有潜力展现类似人类的好奇心。

Conclusion: LLMs在好奇心方面表现优异并能促进推理和学习，也为未来提升模型能力和创新研究提供了依据。

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [77] [The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI](https://arxiv.org/abs/2510.20647)
*Alan Saji,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully*

Main category: cs.CL

TL;DR: 大模型在答非英文问题时倾向用英文推理，虽然结果更准，但翻译易出错，反映出多语言推理仍有待提升。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在多语言推理任务上的表现尚未充分探索，尤其存在默认使用英语进行推理带来的可解释性和文化语境偏差问题。

Method: 系统对比LRMs在英文与非英文问题上的推理表现，涵盖MGSM和GPQA Diamond两项任务，不仅考察答案准确率，还分析推理过程中的认知特征。

Result: 英文推理过程表现出更多认知特征，且终答案准确率更高，复杂任务中差距更大。但英文推理也易出现因翻译环节导致的错误，即“Lost in Translation”现象。

Conclusion: 提升大模型原生多语言推理能力，减少英语中心化，有助于改善多语言场景下的推理准确性和可解释性。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical,
scientific, and other question-answering tasks, but their multilingual
reasoning abilities remain underexplored. When presented with non-English
questions, LRMs often default to reasoning in English, raising concerns about
interpretability and the handling of linguistic and cultural nuances. We
systematically compare an LRM's reasoning in English versus the language of the
question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond
measuring answer accuracy, we also analyze cognitive attributes in the
reasoning traces. We find that English reasoning traces exhibit a substantially
higher presence of these cognitive behaviors, and that reasoning in English
generally yields higher final-answer accuracy, with the performance gap
increasing as tasks become more complex. However, this English-centric strategy
is susceptible to a key failure mode - getting "Lost in Translation," where
translation steps lead to errors that would have been avoided by question's
language reasoning.

</details>


### [78] [\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding](https://arxiv.org/abs/2510.20670)
*Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文提出了CantoNLU，这是一个针对粤语自然语言理解的基准测试，涵盖了7个语法和语义任务，并对几类模型进行了基线性能评估。


<details>
  <summary>Details</summary>
Motivation: 粤语虽有众多使用者，但因政策及双言制，相关NLP资源极度匮乏。因此，自然语言理解领域缺乏粤语评测框架，急需解决评估体系空白。

Method: 构建了一套涵盖句法与语义的粤语NLU基准（CantoNLU），对普通话模型、连续预训练的粤语模型以及单语粤语模型在多个任务上的表现进行了系统基线评估。

Result: 粤语自适应模型在大多数任务表现最佳，单语粤语模型在句法相关任务更优，部分场景下普通话模型迁移效果亦佳。

Conclusion: 粤语自适应模型整体表现最佳，而单语模型在语法任务上更优；当粤语数据稀缺时，普通话模型直接迁移也具有竞争力。同时，所有数据和代码都公开，支持未来粤语NLP研究。

Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy
and diglossia. To address this scarcity of evaluation frameworks for Cantonese,
we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural
language understanding (NLU). This novel benchmark spans seven tasks covering
syntax and semantics, including word sense disambiguation, linguistic
acceptability judgment, language detection, natural language inference,
sentiment analysis, part-of-speech tagging, and dependency parsing. In addition
to the benchmark, we provide model baseline performance across a set of models:
a Mandarin model without Cantonese training, two Cantonese-adapted models
obtained by continual pre-training a Mandarin model on Cantonese text, and a
monolingual Cantonese model trained from scratch. Results show that
Cantonese-adapted models perform best overall, while monolingual models perform
better on syntactic tasks. Mandarin models remain competitive in certain
settings, indicating that direct transfer may be sufficient when Cantonese
domain data is scarce. We release all datasets, code, and model weights to
facilitate future research in Cantonese NLP.

</details>


### [79] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: ND-LoRA方法通过提升神经多样性显著减少语言模型幻觉率，且无需增加参数或数据，为语言模型可靠性提升提供新途径。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型即便增加参数、算力与数据，仍存在过高幻觉（不真实内容生成）的问题。作者认为原因之一是神经表示相关性过高，灵感来自投资组合理论，提出提升神经多样性（降低相关性）可降低模型幻觉风险。

Method: 提出并验证了ND-LoRA方法（结合并行LoRA适配器与Barlow Twins正则化），用于提升语言模型的神经多样性并降低幻觉生成率。

Result: ND-LoRA能平均降低幻觉率14.6%（最高25.6%），且不会影响模型准确率。消融实验显示LoRA和正则化手段协同作用，相关性分析与因果干预证明神经多样性为关键因素。神经相关性每增加0.1%，幻觉率提升3.8%。不同任务对神经多样性需求不同。

Conclusion: 神经多样性是提升语言模型可靠性的第三重要维度（参数、数据之外），通过合理设计并行结构与正则方法，可以在固定预算下减轻幻觉，提高模型实际应用价值。

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [80] [Structure-Conditional Minimum Bayes Risk Decoding](https://arxiv.org/abs/2510.20700)
*Bryan Eikema,Anna Rutkiewicz,Mario Giulianelli*

Main category: cs.CL

TL;DR: 论文通过改进MBR策略的效用函数，使其在开放式文本生成任务中更敏感于输出的结构多样性，显著提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统MBR方法在开放式生成任务（如对话、指令跟随）中，因仅依赖相似性函数，容易选出平均但结构次优的答案。作者希望让MBR更能捕捉生成结果的多样结构。

Method: 三种针对MBR效用函数的轻量级改进，以提升对输出结构变化的敏感度。提出两个评测MBR结构最优性的指标，并在实际数据集及基准任务上测试。

Result: 实验结果表明，常见的基于相似性的效用函数在结构最优性方面表现不足，而作者提出的改进显著提高了结构最优性。在AlpacaEval和MT-Bench上，生成质量提升高达13.7%。

Conclusion: 针对开放式生成任务，通过让MBR更关注结构层面的特征，能够更有效地获得高质量输出。新方法在多种评测和数据集上验证了有效性。

Abstract: Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative
to traditional generation strategies. While MBR has proven effective in machine
translation, where the variability of a language model's outcome space is
naturally constrained, it may face challenges in more open-ended tasks such as
dialogue or instruction-following. We hypothesise that in such settings,
applying MBR with standard similarity-based utility functions may result in
selecting responses that are broadly representative of the model's
distribution, yet sub-optimal with respect to any particular grouping of
generations that share an underlying latent structure. In this work, we
introduce three lightweight adaptations to the utility function, designed to
make MBR more sensitive to structural variability in the outcome space. To test
our hypothesis, we curate a dataset capturing three representative types of
latent structure: dialogue act, emotion, and response structure (e.g., a
sentence, a paragraph, or a list). We further propose two metrics to evaluate
the structural optimality of MBR. Our analysis demonstrates that common
similarity-based utility functions fall short by these metrics. In contrast,
our proposed adaptations considerably improve structural optimality. Finally,
we evaluate our approaches on real-world instruction-following benchmarks,
AlpacaEval and MT-Bench, and show that increased structural sensitivity
improves generation quality by up to 13.7 percentage points in win rate.

</details>


### [81] [User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios](https://arxiv.org/abs/2510.20721)
*Xiaoyuan Wu,Roshni Kaushik,Wenkai Li,Lujo Bauer,Koichi Onoue*

Main category: cs.CL

TL;DR: 现有用LLM自身来自动评估隐私和实用性的方法难以反映真实用户的主观感知。应加强用户研究，改进自动评估方法，使之更符合用户对隐私和有用性的实际需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在日常应用（如撰写邮件、总结会议、健康咨询）中需处理并识别用户的私人敏感信息。然而，现有对于LLM隐私保护能力的评估主要依赖于LLM自身作为“代理模型”进行自动化判断，未能真正反映真实用户的隐私和实用性感受。

Method: 通过组织一项用户研究，邀请94名参与者基于PrivacyLens中的90个场景，评价LLM在隐私敏感场景中的回应，从用户角度评估其隐私保护和有用性表现。同时，与五个代理LLM的自动评估结果进行对比分析。

Result: 用户对完全相同的LLM回复在隐私保护和有用性方面的一致性很低，而五个代理LLM之间评判高度一致，但单个LLM的评价与用户几乎没有相关性。结果表明代理LLM不足以反映真实用户对隐私保护的评价。

Conclusion: 隐私和有用性的感知具有较强主观性，现有代理LLM评测无法准确估计用户感受。需要更多以用户为中心的研究，并探索让LLM自动评价更贴合用户实际感知的方法。

Abstract: Large language models (LLMs) have seen rapid adoption for tasks such as
drafting emails, summarizing meetings, and answering health questions. In such
uses, users may need to share private information (e.g., health records,
contact details). To evaluate LLMs' ability to identify and redact such private
information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with
real-life scenarios. Using these benchmarks, researchers have found that LLMs
sometimes fail to keep secrets private when responding to complex tasks (e.g.,
leaking employee salaries in meeting summaries). However, these evaluations
rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking
real users' perceptions. Moreover, prior work primarily focused on the
privacy-preservation quality of responses, without investigating nuanced
differences in helpfulness. To understand how users perceive the
privacy-preservation quality and helpfulness of LLM responses to
privacy-sensitive scenarios, we conducted a user study with 94 participants
using 90 scenarios from PrivacyLens. We found that, when evaluating identical
responses to the same scenario, users showed low agreement with each other on
the privacy-preservation quality and helpfulness of the LLM response. Further,
we found high agreement among five proxy LLMs, while each individual LLM had
low correlation with users' evaluations. These results indicate that the
privacy and helpfulness of LLM responses are often specific to individuals, and
proxy LLMs are poor estimates of how real users would perceive these responses
in privacy-sensitive scenarios. Our results suggest the need to conduct
user-centered studies on measuring LLMs' ability to help users while preserving
privacy. Additionally, future research could investigate ways to improve the
alignment between proxy LLMs and users for better estimation of users'
perceived privacy and utility.

</details>


### [82] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: 本文分析不同自然语言处理（NLP）方法在提取癌症患者药物治疗及毒性（副作用）信息中的表现，结果显示基于大型语言模型（LLM）的方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 氟嘧啶类药物常用于肠癌与乳腺癌，但副作用严重且多记录于非结构化临床描述中。传统自动化信息提取方法不足，因此有必要评估新一代NLP（尤其LLM）提升提取准确性与自动化水平，以助药物安全监控和临床决策。

Method: 构建标注数据集，包含236份临床记录，分类标注药物治疗及毒性项目。开发并比较规则、机器学习（随机森林、SVM、逻辑回归）、深度学习（BERT、ClinicalBERT）及LLM（zero-shot, error-analysis prompting）方法，并通过训练集与测试集评估性能。

Result: LLM的error-analysis prompting在治疗与毒性分类提取上取得最高F1分数（1.000），表现显著优于其他方法。零样本LLM在毒性提取略低（F1=0.876）。传统机器学习方法次之，深度学习模型（BERT类）表现较弱，规则方法作为基线。数据集规模与类别稀少影响机器及深度学习模型的泛化能力。

Conclusion: 采用LLM为核心的NLP方法，能高效、准确地从临床记录中提取氟嘧啶类药物的用药及毒性信息，为肿瘤研究和药物安全监测提供有力支持。

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [83] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 通过校准LRMs思考轨迹，显著提升了其作为机器翻译评估者的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 虽然LRMs在复杂任务推理上表现优异，但其作为机器翻译评估者的潜力未被系统发掘。为了解决其评价中“过度思考”及评分失准等问题，提出新的校准方法。

Method: 分析LRMs评估MT时面临的挑战，通过在合成类人思考轨迹上训练模型，对其思考过程进行校准。实验采用WMT24 Metrics基准进行验证，并比较不同规模LRMs的效果。

Result: 提出对LRMs作为机器翻译评估者的系统分析，发现其需针对性评价材料、简单任务易“过度思考”以及评分机制存在高估等问题。通过训练合成、类人思考轨迹来校准其思考过程，大幅降低资源消耗并提升评估表现。

Conclusion: 校准后的LRMs在MT评估任务中能更精确地判别翻译质量，并大幅减少计算成本，展现出推进自动MT评估的潜力。

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [84] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: 提出了结合具体应用场景与公平性参数化的新型LLM评估资源，为识别和提升模型在多维责任型AI指标上的表现奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型（LLMs）的评估方法多聚焦于文本生成等高层任务，未能针对实际AI应用进行评估，尤以公平性等责任型AI维度尤为不足。不同应用场景中，相关保护属性可能截然不同，所以迫切需要一种更细致、应用驱动的评估方法。

Method: 方法为根据产品特性生成产品描述任务，并通过公平性属性、性别形容词及产品类别参数化生成丰富提示和标注数据，结合这些数据分析LLMs在关键指标上的表现。

Result: 作者构建了一个基于实际应用（根据产品特性生成产品描述）的数据集，并以公平属性、性别形容词与产品类别的交集进行参数化，获得了丰富标注的提示词集合。证明了该数据集可用于识别LLMs在质量、真实度、安全性和公平性上的不足。

Conclusion: 本工作提出了一种新评估方法和配套数据集，可助力社区在责任型AI尤其是公平性维度对LLM进行更具针对性的评估。

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [85] [Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction](https://arxiv.org/abs/2510.20787)
*Mutian He,Philip N. Garner*

Main category: cs.CL

TL;DR: 为解决线性注意力模型因有限记忆导致的遗忘，在高检索需求任务表现不佳的问题，作者提出了结合稀疏注意力、可学习token清除和滑动窗口机制的新方法，既恢复了对历史token的直接访问，又保持了高效计算。实验验证了所提方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 线性注意力模型由于其有限记忆在处理需要大量历史信息检索的任务时会出现遗忘现象，需设计更高效且记忆管理更合理的模型。

Method: 将token混合器与介于线性与完全注意力之间的中间复杂度机制交错使用，包括带token清除的稀疏注意力和查询感知的原生稀疏注意力。特别提出一种新颖的可学习token清除方法，并结合滑动窗口注意力，通过端到端可训练的轻量级CNN从邻近的过去和未来token中自适应地保留每个head关键的KV对。还实现了高效的Triton内核用于稀疏注意力机制。

Result: 提出了一种可学习的token清除机制，并结合稀疏注意力与滑动窗口策略，显著提升了线性注意力模型在检索密集任务上的表现，同时保持了计算与空间的高效性。

Conclusion: 我们提出的方法通过结合稀疏注意力和可学习的token清除机制，能够有效缓解线性注意力模型在检索密集任务中的遗忘问题。实验结果证明了该方法的有效性。

Abstract: Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.

</details>


### [86] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: 作者提出了一种简单的均值池化软上下文压缩方法，显著优于主流方法，并且多压缩比训练下性能下降有限。


<details>
  <summary>Details</summary>
Motivation: 在使用大语言模型（LLMs）进行检索增强生成（RAG）时，长文本上下文的计算成本较高。为此，研究人员希望通过软上下文压缩来减少输入序列的长度，降低成本。

Method: 提出并验证了一种轻量级简单的均值池化方法用于上下文压缩，还研究了训练同一压缩器输出多种压缩比的方法，并进行了大量跨数据集、模型和压缩比例的实验对比分析。

Result: 提出的简单均值池化方法在主流压缩令牌架构上表现更优，无论是在同域还是异域问答任务、不同模型和压缩比下均效果最好。当训练支持多种压缩比例时，性能下降幅度较小。

Conclusion: 尽管均值池化方法表现优异，整体来看，不同架构和训练方式在性能与压缩比之间存在微妙权衡，说明压缩方法的应用场景复杂多变。

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [87] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: 目前LLM生成文本检测面临定义不清、实际应用复杂等问题，检测结果应谨慎解读。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，研究者们开始关注如何检测这些模型生成的文本，但目前对“LLM生成文本”的定义并不统一也不精确。不同使用场景与模型多样性使检测任务更加困难。

Method: 分析当前LLM生成文本检测的定义、场景差异、现实应用局限性以及现有评测方法的问题。

Result: 现有检测器的数值结果常被误解，实际意义正在减弱，检测器仅在特定条件下有用，其结果应作为参考而非决定性指标。

Conclusion: 由于LLM与人类文本的界限愈发模糊，且现有评测方法无法覆盖现实条件，检测器结果应仅作为辅助性参考。

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [88] [A Classification of Long-Refinement Graphs for Colour Refinement](https://arxiv.org/abs/2510.20802)
*Sandra Kiefer,T. Devini de Mel*

Main category: cs.DM

TL;DR: 本研究用逆向工程和结构分类，首次完整刻画了所有度数≤4的最大迭代Colour Refinement图，证实其结构唯一，否定了一定类型图的存在，并揭示其可被字符串紧凑表达。


<details>
  <summary>Details</summary>
Motivation: Colour Refinement算法用于检测图的对称性，是图同构测试的重要工具。长期以来，人们一直在探索是否存在能达到理论最大迭代次数的图。此前的研究发现了部分低度数（2和3）图的无限族，但完整分类尚未实现。

Method: 通过逆向工程方法，系统分析低度数（≤3和4）长迭代图的结构，将这些图用紧凑字符串表示，并研究其结构特性。同时研究这些图的边补闭包性，并完成此前发起的关于最后才被区分图的探索。

Result: 证实除了一个例外，已知的最大度数≤3的图族即唯一符合条件的长迭代图，同时完成对最大度数4的长迭代图的完全分类。发现所有低度长迭代图可用紧凑字符串表示，从而获得结构性见解。还证实不存在仅在最后一轮被区分的长迭代图。由于补图闭包性，高度数图的分类也随之完成。

Conclusion: 本文完整刻画并分类了所有低（高）度数的长迭代Colour Refinement图，完善了理论边界认知，否定了最后才区分的长迭代图可能性，并揭示了丰富的结构规律。

Abstract: The Colour Refinement algorithm is a classical procedure to detect symmetries
in graphs, whose most prominent application is in graph-isomorphism tests. The
algorithm and its generalisation, the Weisfeiler-Leman algorithm, evaluate
local information to compute a colouring for the vertices in an iterative
fashion. Different final colours of two vertices certify that no isomorphism
can map one onto the other. The number of iterations that the algorithm takes
to terminate is its central complexity parameter. For a long time, it was open
whether graphs that take the maximum theoretically possible number of Colour
Refinement iterations actually exist. Starting from an exhaustive search on
graphs of low degrees, Kiefer and McKay proved the existence of infinite
families of such long-refinement graphs with degrees 2 and 3, thereby showing
that the trivial upper bound on the iteration number of Colour Refinement is
tight. In this work, we provide a complete characterisation of the
long-refinement graphs with low (or, equivalently, high) degrees. We show that,
with one exception, the aforementioned families are the only long-refinement
graphs with maximum degree at most 3, and we fully classify the long-refinement
graphs with maximum degree 4. To this end, via a reverse-engineering approach,
we show that all low-degree long-refinement graphs can be represented as
compact strings, and we derive multiple structural insights from this
surprising fact. Since long-refinement graphs are closed under taking edge
complements, this also yields a classification of long-refinement graphs with
high degrees. Kiefer and McKay initiated a search for long-refinement graphs
that are only distinguished in the last iteration of Colour Refinement before
termination. We conclude it in this submission by showing that such graphs
cannot exist.

</details>


### [89] [Labeling and folding multi-labeled trees](https://arxiv.org/abs/2510.20292)
*Vincent Moulton,Andreas Spillner*

Main category: cs.DM

TL;DR: 本文提出了多重标签树与多重集合分割之间的双射，扩展了经典的有根树与集合分割的对应，并应用于系统发育网络的刻画与分类。


<details>
  <summary>Details</summary>
Motivation: 作者受Erdős和Székely关于有根树与集合分割之间的双射关系启发，想要将其推广到带有多重标签（可重复标签）树和多重集合分割之间的关系。

Method: 提出了一种针对多重标签树的新的标签算法，通过对多重集合的有序排列描述了如何给树及其节点做标签，从而建立与多重集合分割之间的联系。

Result: 证明了通过多重集合的某些顺序，可以刻画由多重标签树的标签所产生的多重集合分割；还证明了可标记系统发育网络恰好是“折叠过程”下稳定的系统发育网络，并给出了带固定叶集的可标记系统发育网络与多重集合分割之间的双射。

Conclusion: 该工作推广了原有的树与集合分割之间的对应关系，将其扩展到更一般的多重标签树和系统发育网络，为相关领域的结构刻画和分类提供了新的工具和视角。

Abstract: In 1989 Erd\H{o}s and Sz\'ekely showed that there is a bijection between (i)
the set of rooted trees with $n+1$ vertices whose leaves are bijectively
labeled with the elements of $[\ell]=\{1,2,\dots,\ell\}$ for some $\ell \leq
n$, and (ii) the set of partitions of $[n]=\{1,2,\dots,n\}$. They established
this via a labeling algorithm based on the anti-lexicographic ordering of
non-empty subsets of $[n]$ which extends the labeling of the leaves of a given
tree to a labeling of all of the vertices of that tree. In this paper, we
generalize their approach by developing a labeling algorithm for multi-labeled
trees, that is, rooted trees whose leaves are labeled by positive integers but
in which distinct leaves may have the same label. In particular, we show that
certain orderings of the set of all finite, non-empty multisets of positive
integers can be used to characterize partitions of a multiset that arise from
labelings of multi-labeled trees. As an application, we show that the recently
introduced class of labelable phylogenetic networks is precisely the class of
phylogenetic networks that are stable relative to the so-called folding process
on multi-labeled trees. We also give a bijection between the labelable
phylogenetic networks with leaf-set $[n]$ and certain partitions of multisets.

</details>


### [90] [Excluding a Line Minor via Design Matrices and Column Number Bounds for the Circuit Imbalance Measure](https://arxiv.org/abs/2510.20301)
*Daniel Dadush,Friedrich Eisenbrand,Rom Pinchasi,Thomas Rothvoss,Neta Singer*

Main category: cs.DM

TL;DR: 本文推广了整数矩阵的列数多项式界到实矩阵，通过电路不平衡度定义，首次对所有参数范围给出了界限。同时扩展了对复表示拟阵的元素数量上界理论，完善了相关拟阵理论。


<details>
  <summary>Details</summary>
Motivation: 之前对整数矩阵有明确的边界结果，但对于实矩阵尚未得到全面的可推广的多项式界。本工作希望填补这个理论空白，并为线性规划、组合优化等应用提供理论基础。

Method: 通过研究由电路不平衡度受限的实矩阵诱导的拟阵，证明其是minor closed且排除了长度为O(κ)的线作为小拟阵，从而获得边界。技术上，还证明了对于排除长度为l的线的简单秩d复表示拟阵，其元素数量至多为O(d^4 l) 。

Result: 给出对于任意实矩阵$A$，只要列不共线，就有$n\leq O(d^4 \kappa_A)$这一普适的界。此外，对复表示拟阵也给出了新的边界，丰富了拟阵理论的上界体系。

Conclusion: 本文对实矩阵的列数给出了多项式界，推广了之前仅限于整数矩阵的相关结果，为实矩阵也建立了普适的上界。

Abstract: For a real matrix $A \in \mathbb{R}^{d \times n}$ with non-collinear columns,
we show that $n \leq O(d^4 \kappa_A)$ where $\kappa_A$ is the \emph{circuit
imbalance measure} of $A$. The circuit imbalance measure $\kappa$ is a real
analogue of $\Delta$-modularity for integer matrices, satisfying $\kappa_A \leq
\Delta_A$ for integer $A$. The circuit imbalance measure has numerous
applications in the context of linear programming (see Ekbatani, Natura and
V{\'e}gh (2022) for a survey). Our result generalizes the $O(d^4 \Delta_A)$
bound of Averkov and Schymura (2023) for integer matrices and provides the
first polynomial bound holding for all parameter ranges on real matrices.
  To derive our result, similar to the strategy of Geelen, Nelson and Walsh
(2021) for $\Delta$-modular matrices, we show that real representable matroids
induced by $\kappa$-bounded matrices are minor closed and exclude a rank $2$
uniform matroid on $O(\kappa)$ elements as a minor (also known as a line of
length $O(\kappa)$).
  As our main technical contribution, we show that any simple rank $d$ complex
representable matroid which excludes a line of length $l$ has at most $O(d^4
l)$ elements. This complements the tight bound of $(l-3)\binom{d}{2} + d$ for
$l \geq 4$, of Geelen, Nelson and Walsh which holds when the rank $d$ is
sufficiently large compared to $l$ (at least doubly exponential in $l$).

</details>


### [91] [Partial Optimality in Cubic Correlation Clustering for General Graphs](https://arxiv.org/abs/2510.20431)
*David Stein,Bjoern Andres,Silvia Di Gregorio*

Main category: cs.DM

TL;DR: 作者针对图高阶相关聚类问题中的三元团（3-cliques）情况，提出了部分最优性判据和判定算法，并通过实验验证了其在实际数据集中的有效性，对相关聚类优化具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 高阶相关聚类问题是图论中一个复杂且NP难的问题，特别是当聚类时需要最小化图中团（cliques）的代价总和。实际应用中常用局部搜索启发式方法进行求解。本文针对这一领域的实际需求出发，试图为三元团（3-cliques）聚类问题建立部分最优性判据，提升聚类效率和效果。

Method: 本文针对至多3-cliques（立方相关聚类）的特殊情形，提出并定义了部分最优性判据，并开发了用于判定这些条件的算法。随后，在两组实际数据集上对这些算法的效果进行了数值实验验证。

Result: 数值实验表明，所提出的部分最优性判据及判定算法在三元团相关聚类问题上表现有效，能够在实际数据集上提升聚类过程的效率和质量。

Conclusion: 本文为NP难的高阶相关聚类问题中的三元团相关聚类建立了部分最优性判据，并提出了具体算法，对提升实际聚类任务的效率和聚类质量具有重要意义。

Abstract: The higher-order correlation clustering problem for a graph $G$ and costs
associated with cliques of $G$ consists in finding a clustering of $G$ so as to
minimize the sum of the costs of those cliques whose nodes all belong to the
same cluster. To tackle this NP-hard problem in practice, local search
heuristics have been proposed and studied in the context of applications. Here,
we establish partial optimality conditions for cubic correlation clustering,
i.e., for the special case of at most 3-cliques. We define and implement
algorithms for deciding these conditions and examine their effectiveness
numerically, on two data sets.

</details>
