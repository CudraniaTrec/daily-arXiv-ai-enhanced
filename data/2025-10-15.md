<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.CL](#cs.CL) [Total: 62]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language](https://arxiv.org/abs/2510.11751)
*Jan Pedersen,Kevin Chalmers*

Main category: cs.PL

TL;DR: 本文利用FDR工具，对协作式调度环境下共享通信通道的并发行为进行规格和实现建模，发现实现正确行为取决于资源充足，强调了对并发组件运行时环境建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 并发行为的正确性对于理解组件在特定条件下的运作至关重要，尤其是在共享通信通道的环境下。

Method: 采用FDR（Refinement Checking and Modelling Tool）工具，对共享通信通道在协作式调度模型下的行为进行规格说明与实现建模，具体分析ProcessJ语言中这些通道的实现。

Result: 研究发现，虽然可以实现正确行为，但结果依赖于是否有足够资源来执行所有相关进程。

Conclusion: 需要对并发组件的运行时环境进行建模，以确保它们在现实中如规格描述般运行。

Abstract: Correct concurrent behaviour is important in understanding how components
will act within certain conditions. In this work. we analyse the behaviour of
shared communicating channels within a coorporatively scheduled runtime. We use
the refinement checking and modelling tool FDR to develop both specifications
of how such shared channels should behave and models of the implementations of
these channels in the cooperatively scheduled language ProcessJ. Our results
demonstrate that although we can certainly implement the correct behaviour of
such channels, the outcome is dependant on having adequate resources available
to execute all processes involved. We conclude that modelling the runtime
environment of concurrent components is necessary to ensure components behave
as specified in the real world.

</details>


### [2] [AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework](https://arxiv.org/abs/2510.11759)
*Hongyu Lin,Haolin Pan,Haoran Luo,Yuchen Li,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.PL

TL;DR: 本文提出的AwareCompiler通过整合知识和数据，显著提升了编译器优化的效果和效率，成功解决了自动化优化面临的多项关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动化编译器优化面临语义错位、交互效率低下和奖励稀疏等多重挑战，需要新的方法加以解决。

Method: 提出了一个基于Agent的编译器优化框架AwareCompiler，包含结构化知识集成与数据集构建、基于知识的自适应优化序列生成，以及数据驱动的混合训练流程。

Result: 实验表明，AwareCompiler在程序优化任务中表现出更高的效率和优化效果，优于现有基线方法。

Conclusion: AwareCompiler在标准测试基准上的表现显著优于现有方法，无论在性能还是优化效率方面均取得了更好的结果。

Abstract: Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (LLMs)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward sparsity from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.

</details>


### [3] [Functional Reasoning for Distributed Systems with Failures](https://arxiv.org/abs/2510.12131)
*Haobin Ni,Robbert van Renesse,Greg Morrisett*

Main category: cs.PL

TL;DR: 本文通过设计Sync与Async两种语言，为分布式系统（含拜占庭容错）提供形式化推理框架，并通过工具实现与实际协议验证，保证了从理论到代码的安全正确性。


<details>
  <summary>Details</summary>
Motivation: 分布式系统理论中常用霍尔式的非正式推理方法，但这种方法并非总是严密无误，并且其与正式证明之间的关系存在疑问。

Method: 本文通过语言设计与元分析，将这种直观推理与标准形式化方法连接起来，提出了双语法（Sync和Async）以形式化并组合推理分布式系统（包括拜占庭容错）的方法。

Result: 我们实现了Sync到Async的编译过程，并证明了Sync程序中在语义下证明的安全性在Async模型下依然成立。此外，使用实现的Rocq工具，对两个容错一致性协议（BOSCO和SeqPaxos）进行安全性验证。

Conclusion: 本文提出了一种新的组合式形式推理方法和工具链，形式化地支持了分布式系统中直观的Hoare式推理，能够保证安全性证明从同步模型到异步实际系统的传递。

Abstract: Distributed system theory literature often argues for correctness using an
informal, Hoare-like style of reasoning. While these arguments are intuitive,
they have not all been foolproof, and whether they directly correspond to
formal proofs is in question. We formally ground this kind of reasoning and
connect it to standard formal approaches through language design and
meta-analysis, which leads to a functional style of compositional formal
reasoning for a class of distributed systems, including cases involving
Byzantine faults. The core of our approach is twin languages: Sync and Async,
which formalize the insight from distributed system theory that an asynchronous
system can be reduced to a synchronous system for more straightforward
reasoning under certain conditions. Sync describes a distributed system as a
single, synchronous, data-parallel program. It restricts programs syntactically
and has a functional denotational semantics suitable for Hoare-style formal
reasoning. Async models a distributed system as a collection of interacting
monadic programs, one for each non-faulty node in the system. It has a standard
trace-based operational semantics, modeling asynchrony with interleaving. Sync
compiles to Async and can then be extracted to yield executable code. We prove
that any safety property proven for a Sync program in its denotational
semantics is preserved in the operational semantics of its compiled Async
programs. We implement the twin languages in Rocq and verify the safety
properties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos.

</details>


### [4] [Operational methods in semantics](https://arxiv.org/abs/2510.12295)
*Roberto M. Amadio*

Main category: cs.PL

TL;DR: 本文介绍并推广了操作语义作为编程语言语义分析的主流方法，具有理论和实践上的双重优势，适用于语言实现、属性验证和语义等价分析等任务。


<details>
  <summary>Details</summary>
Motivation: 本文关注于编程语言的操作语义，通过抽象建模来描述程序的计算步骤，并以此为基础探讨语义等价、规范语言以及静态分析方法。动机在于寻找一种有效且通用的方式来理解和分析编程语言的语义。操作语义因其对数学要求适中且能适应多种编程特性，是优选方案。

Method: 采用抽象模型，从程序的计算步骤着手，逐步建立操作语义的理论框架，并将其应用于语义等价判定、规范语言设计及静态分析。强调操作语义在规范实现语言、测试属性、以及用于编译器或静态分析器正确性证明方面的实际作用。

Result: 提出了操作语义作为编程语言分析和实现的有效理论基础，能够支持可移植的语言实现、程序属性规范，甚至能处理编译器和静态分析器的正确性证明。证明了操作语义在理论与实践上的适用性和便利性。

Conclusion: 操作语义为编程语言的理论和实际应用提供了坚实和灵活的基础，使得分析、规范和实现各种语言特性变得更加高效可靠。它既适合语言实现的实际需求，也支持更高层次的程序分析和验证任务。

Abstract: The focus of these lecture notes is on abstract models and basic ideas and
results that relate to the operational semantics of programming languages
largely conceived. The approach is to start with an abstract description of the
computation steps of programs and then to build on top semantic equivalences,
specification languages, and static analyses. While other approaches to the
semantics of programming languages are possible, it appears that the
operational one is particularly effective in that it requires a moderate level
of mathematical sophistication and scales reasonably well to a large variety of
programming features. In practice, operational semantics is a suitable
framework to build portable language implementations and to specify and test
program properties. It is also used routinely to tackle more ambitious tasks
such as proving the correctness of a compiler or a static analyzer.

</details>


### [5] [GUPPY: Pythonic Quantum-Classical Programming](https://arxiv.org/abs/2510.12582)
*Mark Koch,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: Guppy是一个嵌入于Python的专用语言，使用户能用高级Python编写并控制可运行于真实量子硬件的混合量子程序，改善表达能力和易用性。


<details>
  <summary>Details</summary>
Motivation: 量子程序开发受限于控制流表达和与传统托管语言集成，为改善用户易用性及实际应用，开发者推出Guppy使高级量子程序能在Python中高效描述并运行于量子硬件。

Method: Guppy采用嵌入式DSL设计，结合Python语法实现复杂控制流的量子程序表达。

Result: Guppy提供了用Python语法编写高阶混合量子程序的能力，简化量子开发流程并提升控制流表达能力。

Conclusion: Guppy是一种嵌入在Python中的领域特定语言，能让用户用Python风格编写高层次的混合量子程序，并目标运行于真实量子硬件。

Abstract: We present ongoing work on Guppy, a domain-specific language embedded in
Python that allows users to write high-level hybrid quantum programs with
complex control flow in Pythonic syntax, aiming to run them on actual quantum
hardware.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该文提出了eye2vec，通过分布式向量把眼动注视转化为语法元素转换，实现了对程序员阅读代码时眼动数据的高效、自动化分析，减少了手工操作。


<details>
  <summary>Details</summary>
Motivation: 现有的基于眼动追踪的程序理解研究，需要研究者预先选定分析目标（如控件流程、语法元素等），并根据这些目标设计数据处理方法。这一流程耗时且依赖手动操作，并且通过不同的兴趣区（AOI）划分会导致不同结果，增加了分析的不确定性。

Method: 提出了一种基础设施eye2vec，其将连续的两次注视转化为语法元素之间的转换，并利用分布式表示方式表达这些转换。基于分布式表示，可以灵活采用多种数据分析方法，实现更为丰富的语义解释。

Result: eye2vec可以基于分布式向量更高效、更灵活地分析程序员在读代码时的眼动数据，为语义相关的自动分析方法提供了基础。减少了人工预设和人工分析的工作量。

Conclusion: eye2vec为分析开发者阅读代码时的眼动行为，提供了一套自动化、能适应多样语义层级且高效的方法和基础设施。它提高了眼动分析的灵活性和可复用性，也为后续相关的研究提供了便利。

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [7] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文提出，应将LLM的token预算作为注意力预算，通过输入端任务感知的文本精简来提升模型效率和可用性，从而实现更高效、可持续的数据密集型应用，并讨论了相关研究挑战和实现路径。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在数据库查询和开发者可观测性等数据密集型工作流程中的广泛应用，现实世界中的文本丰富型数据（如日志、遥测、监控流）的体量、冗余和噪声，限制了模型的有效性。直接将这类原始数据输入LLM既昂贵又难以持续，并常常不符合任务目标，因此需要更加高效的数据处理方法。

Method: 作者提出将LLM的“token预算”视为“注意力预算”，将任务感知的文本精简提升为语言-数据系统设计中的核心原则。该方法强调输入端的文本精简不是压缩，而是注意力分配——优先保留与下游任务最相关的信息。文中还阐明了建立基准测试、构建自适应精简管道、将预算感知的预处理集成到数据库及检索系统中的研究挑战。

Result: 提出了一种新的视角和设计原则，即通过上游输入精简，将有限的注意力资源集中在工作流中的关键信息上，以提升LLM在数据密集型环境中的可扩展性、准确性和可持续性。

Conclusion: 构建任务感知的输入精简机制可极大提升LLM处理噪声和冗余数据的效率，推动数据驱动工作流与大模型深度融合，实现可持续与高效的信息抽取与利用。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [8] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi通过历史修复数据提取程序性知识，指导agent高效解决仓库级软件问题，比现有技术表现好很多，尤其“设计模式与编码实践”知识作用突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于agent的方法在软件工程任务中取得了进步，但在复杂的仓库级问题解决上仍面临挑战。主要原因是缺乏可指导问题修复流程的程序性知识，以及依赖大量计算资源进行盲目探索。作者希望通过引入程序性知识来提升问题解决效率和效果。

Method: 提出了Lingxi，一个结合从历史问题修复数据中提取的程序性知识指导agent解决仓库级问题的框架。Lingxi通过分层抽象机制离线构建知识库，提供问题修复步骤及原因，并在实际应用时利用知识驱动的方法多角度智能分析问题，区别于以往盲目探索的方法。

Result: 在SWE-bench Verified基准测试的Past@1设置下，Lingxi成功解决74.6%的漏洞，显著超过五种最新技术（优势5.4%-14.9%）。消融研究显示程序性知识才是性能提升关键，仅靠扩展计算规模没有显著收益。定性研究进一步表明，“设计模式与编码实践”是最关键的知识类型，不同知识在分析、规划和修复各阶段角色有所变化。

Conclusion: Lingxi通过引入程序性知识为agent赋能，在仓库级问题解决上取得了突破性进展，其方法较传统暴力计算方式更加高效和智能。程序性知识尤其在不同阶段发挥不同作用，是性能提升的核心。

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [9] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: 本文提出了用于简化分布式多智能体AI应用开发和部署的DMAS-Forge框架，通过自动生成代码和配置减少人工工作，提高开发效率，并展示了该框架的前景及后续研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能应用对多智能体协作的需求增加，特别是在复杂任务解决和服务架构场景下，现有的开发框架和协议迅速变化，部署和测试分布式AI系统变得复杂且费力。

Method: 提出了DMAS-Forge框架，用以将应用逻辑与部署策略解耦，并自动生成多智能体分布式系统所需的粘合代码和配置方案，减少人工干预。论文描述了设计理念、整体愿景，并展示了原型系统。

Result: 框架能够在多种部署场景下简化分布式多智能体应用的开发和运行，实现高效、灵活的部署过程。

Conclusion: DMAS-Forge初步验证了通过自动化和解耦设计能有效降低多智能体系统部署的门槛，为未来分布式AI应用开发提供了新的路径，并展望了进一步完善和扩展的方向。

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [10] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: 本文提出了TorchCor，一个可在普用GPU上高效运行的心脏电生理模拟库，不仅加速了复杂模拟，也降低了研究和临床应用的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 心脏电生理模拟在理解心律失常和临床决策中越来越重要，但传统模拟依赖高性能计算资源，许多研究和临床机构难以获取。

Method: 提出了TorchCor，一个基于PyTorch开发的高性能Python库，用有限元方法在通用GPU上实现心脏电生理模拟。

Result: TorchCor显著加快了尤其是大型三维网格的心脏电生理模拟速度，并通过分析解和基准问题验证了其求解器的精度。

Conclusion: TorchCor为学术及商业用途免费开放，无限制使用，降低了高性能心脏电生理模拟的门槛。

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [11] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 在深度学习代码理解任务中，加入版本历史与结构上下文显著提升了代码克隆检测和摘要质量，人工与量化评测均证明其有效性，建议后续神经软件工程模型加大对上下文信号的利用。


<details>
  <summary>Details</summary>
Motivation: 目前深度学习模型在自动化程序理解任务（如代码摘要、克隆检测）中取得了较好成绩，但大多只利用源代码，忽略了如版本历史、结构关系等上下文信息，限制了对代码演化与运行的理解能力。

Method: 作者开展了实证研究，通过为代码表示增加上下文信号（如版本历史、调用关系），比较了五个代表性模型（CodeBERT、GraphCodeBERT、CodeT5、PLBART、ASTNN）在代码克隆检测和代码摘要两个任务上的表现。实验在SeSaMe和CodeSearchNet数据集上，分别在仅用代码与增加上下文两种条件下微调模型，并进行量化与人工评测。

Result: 引入上下文信息整体提升了模型性能。其中，版本历史对克隆检测（如CodeT5提升15.92% F1）和代码摘要（如GraphCodeBERT提升5.56% METEOR）有持续增益，调用图对性能的提升依模型和任务而异。多种上下文组合可获得更大提升（macro-F1提升至21.48%）。人工评测显示，上下文增强摘要在准确性和内容充分性方面显著优于基线（p<=0.026，delta最高达0.55）。

Conclusion: 通过整合版本历史、调用关系等上下文信号，可以显著提升基于深度学习的代码理解任务效果，为优化神经网络模型的上下文编码提供了新方向。

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [12] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: SEMAP协议在多智能体LLMs自动化软件开发任务中，通过结构化协作和验证机制，大幅降低了任务失败率，提升了系统整体性能。


<details>
  <summary>Details</summary>
Motivation: 随着对软件开发需求的增加，学界和业界越来越关注使用大型语言模型（LLMs）自动化软件工程任务。近期的研究尝试将LLMs扩展为多智能体系统（MAS），以模拟协作开发流程，但这些系统经常因缺乏基础性的软件工程结构原则而失败，主要暴露出规格不足、协作不协调和验证不当三大问题。

Method: 本文提出了软件工程多智能体协议（SEMAP），作为协议层的方法，具体实现了多智能体LLMs的三大核心软件工程设计原则：(1) 明确的行为契约建模；(2) 结构化消息传递；(3) 生命周期引导的执行与验证。SEMAP基于谷歌的Agent-to-Agent (A2A)基础设施进行实现。

Result: 使用多智能体系统失败分类（MAST）框架进行实证评估，结果显示SEMAP能够有效减少不同软件工程任务中的失败。在代码开发任务中，函数级开发的总失败率最多下降69.6%，部署级开发的总失败率下降56.7%。在漏洞检测任务中，SEMAP对Python任务的失败数减少了47.4%，对C/C++任务的失败数减少了28.2%。

Conclusion: SEMAP通过引入核心的软件工程设计原则，有效减少了多智能体LLMs系统在各类软件工程任务中的失败率，提高了自动化开发的可行性和可靠性。

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [13] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: 作者提出NL2Contract任务，用LLM从自然语言推断完整功能合约（包含前置和后置条件），并用新指标系统评估，结果表明相比仅生成后置条件的方法，NL2Contract能更高效、准确地辅助自动软件验证，减少误报并检测实际缺陷。


<details>
  <summary>Details</summary>
Motivation: 自动软件验证器在检查代码是否符合（形式）规范方面越来越有效，但实际应用受到现实代码中规范缺乏的限制。大语言模型（LLM）有望通过从代码中的自然语言提示（如函数名、注释等）自动推断规范，然而仅利用推断出的后置条件作为规范，在验证时会导致许多误报。为此，作者提出重新思考规范推断问题。

Method: 提出NL2Contract任务，利用LLM将非正式的自然语言信息翻译为正式的功能合约（包括前置条件和后置条件）。作者设计了多维度评估指标，包括规范生成的正确性、区分缺陷能力，以及在自动软件验证中的可用性，并系统评估不同LLM在NL2Contract的表现，同时与仅生成后置条件的任务进行对比。

Result: 实验表明：1）LLM普遍能够生成对所有输入都可靠的功能合约；2）生成的合约对区分有缺陷与正确行为具有足够表达力；3）基于LLM生成的功能合约验证器的误报数量明显少于仅用后置条件验证；另外LLM推断出的前置条件通常与开发者意图一致，能帮助自动验证器发现真实缺陷。

Conclusion: 基于LLM的NL2Contract方法能有效自动补全软件规范，包括前置与后置条件，提高了自动软件验证的准确性和实际可用性，减少误报，并能更好地辅助现实代码缺陷检测。

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


### [14] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: 本文提出了iCodeReviewer，通过融合多提示专家和路由算法提升自动安全代码审查的覆盖率和准确性，实验结果显示在安全问题识别和审查意见接受率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动化安全代码审查方法存在精度和覆盖率有限，以及评估不全面等问题，急需更高效且准确的方法帮助开发团队识别安全问题。

Method: 提出了一种基于大型语言模型（LLM）的自动安全代码审查方法，采用了混合提示架构和动态提示管道，根据代码特征激活必要的提示专家，并引入路由算法以减少误报。

Result: iCodeReviewer在内部数据集上的F1值达63.98%，在生产环境中审查意见的接受率高达84%。

Conclusion: iCodeReviewer能够有效识别和定位安全问题，并且生成的审查意见在实际生产环境中的接受率很高。

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [15] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: 本文综述了软件工程与心理学交叉领域中对语言化方法的研究，利用GPT筛查大量文献，发现主流研究侧重技术而非人本主题，大语言模型在文献筛选中表现良好。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的思维、决策和行为一直是软件工程领域的重要挑战，而语言化技术为研究这些认知过程提供了一种低成本且易于实施的方法。本文希望通过跨学科（软件工程与心理学）文献综述，揭示相关研究主题，并探索大语言模型在文献筛选中的有效性。

Method: 作者进行了一项横断面式的文献综述，结合软件工程与心理学领域，聚焦于口头或书面语言数据的研究。他们利用GPT等大语言模型在基于标题对9000多篇论文进行相关性筛选，并与人工评审结果进行对比验证。

Result: 大语言模型（如GPT）辅助筛选在标题级别上与人工结果高度一致，不一致率为13%。主流研究主题集中在软件工程实践本身，更“以人为本”的主题则较少。同时，软件工程研究频繁借用心理学方法，但心理学领域较少反向引用软件工程方法。

Conclusion: 语言化方法为理解开发者的认知过程提供了有力工具，且大语言模型可有效支持大规模跨学科文献评审。当前研究主题偏重于技术范畴，人本主题有待加强。心理学方法在软件工程领域应用广泛，但软件工程对此的反哺有限。

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [16] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: 本文提出并分析了Vibe Coding（VC）这种结合直觉与即兴创作的AI辅助编程新范式，通过访谈探讨其优势及挑战，并认为VC推动了编程文化的转型，值得进一步研究。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能和大语言模型的进步带来了新的编程可能性，促使作者研究新的以直觉、情感驱动和即兴交互为核心的Vibe Coding范式，并探讨其与传统编程方式的区别。

Method: 通过对10位有经验的软件从业者进行5次半结构化访谈，分析并总结了VC的五个主题维度。

Result: VC带来了新的表达方式和快速原型设计的可能，同时也引发了可复现性、可扩展性和包容性等挑战。VC强调了开发者角色的重构，模糊了专业开发者与非开发者之间的界限。作者提出VC值得在HCI与软件工程领域持续探索。

Conclusion: Vibe Coding (VC)代表了一种有意义的编程文化转变，值得在人机交互和软件工程领域进行进一步研究。

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [17] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: 论文通过大规模长期基准测试，系统分析了云环境性能波动特性，发现变异虽存在但不足以严重影响实验可靠性，并揭示了周期性模式与特殊事件可能的影响。


<details>
  <summary>Details</summary>
Motivation: 云环境基准测试结果因性能高变异性备受质疑，影响可复现性和可信度。为科学认识变异性并提升实验可靠性，有必要量化并分析其实际影响。

Method: 通过长时间、多时段重复执行流处理应用的基准测试，并分析结果，研究了云环境下性能变异性及其模式。并扩展研究考察特殊事件（如黑色星期五）对性能的影响。

Result: 证实了云中应用级性能确有波动，但不如以往认为的剧烈；发现了日常和周期性规律，并正考察全球大型事件（如黑五）对性能的影响。

Conclusion: 云环境下基准测试存在应用级别的性能波动，但幅度比预期小。且可观察到日常和每周的性能模式。

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [18] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: 本文评估了SysMLv2语言扩展机制对开发领域专用语言（如数字孪生DarTwin DSL）的支持能力，展示了实际开发过程及应用前景，同时指出当前工具对图形符号的支持有限，为数字孪生模型驱动工程的集成提供了新思路。


<details>
  <summary>Details</summary>
Motivation: SysMLv2引入了面向领域专用概念与语言扩展的机制，有望简化领域专用语言（DSL）的创建，并提升与现有系统描述及技术设计的兼容性。作者希望评估这些新特性在实际中的效果，特别是其在数字孪生演化管理中的应用潜力。

Method: 作者对SysMLv2的新机制进行回顾，并通过具体用例进行评估。他们通过SysMLv2开发了DarTwin DSL，该DSL形式化了现有的DarTwin数字孪生演化符号，并用于测试SysMLv2工具对该DSL的支持。

Result: 作者成功展示了DarTwin DSL的开发过程和应用，但也指出目前SysMLv2在图形符号表达能力上存在工具层面的局限性。

Conclusion: 论文推动了数字孪生模型驱动工程（MDE）领域发展，将SysMLv2的发布与数字孪生的系统性演化管理相结合，尽管工具仍有不足，但方法展现了集成潜力。

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [19] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: 本文提出了面向代码diff理解的Diff-XYZ基准，涵盖三项任务，揭示了不同diff格式在不同任务和模型下的适用性，为大模型代码编辑能力的评估与提升提供了重要工具。


<details>
  <summary>Details</summary>
Motivation: 在大规模编辑和重构代码库时，如何可靠地处理代码差异（diff）对于智能体非常关键。但现有缺乏专门测试代码diff理解的基准数据集，且对不同diff表示格式在不同任务和模型规模下的优缺点认识不足。

Method: 提出了Diff-XYZ基准数据集，包含apply、anti-apply和diff生成三项有监督任务，样本采自真正的代码提交。通过自动化指标和清晰评测协议，系统性比较统一diff格式和其他diff表示格式在不同模型和任务中的表现。

Result: 实验表明，diff表示格式的最佳选择依赖于具体任务和模型规模。例如，search-replace格式适合大型模型完成diff生成任务，但不适合diff分析和小模型。

Conclusion: Diff-XYZ为评估和改进大模型处理代码diff能力提供了通用可复用的基准，有助于推动代码编辑模型及diff格式的发展。数据集已开放发布。

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [20] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 针对软件工程实践，作者开发并验证了两种同理心量表，首次实现了该领域的同理心有效测量，对促进团队协作和用户导向设计具有实践意义。


<details>
  <summary>Details</summary>
Motivation: 现有的同理心量表无法有效衡量软件工程师在特定社会技术情境下的同理心，因为这些量表的语言、场景和假设未必适用于软件工程领域。软件工程中的同理心有着不同于临床或日常语境的特殊表达形式，如理解非技术用户的困惑或同事的技术限制。

Method: 作者提出并验证了两种领域特定的同理心量表：EmpathiSEr-P（面向实践者）和EmpathiSEr-U（实践者对用户），量表基于实践者视角构建，包括认知同理心、情感同理心和同理性反应三个维度。采用多阶段严格方法，包括专家评估、认知访谈和两轮实践者调查，确保量表的实证有效性。

Result: 成功开发并验证了针对软件工程领域的两个同理心量表。这些工具实现了同理心在SE情境下的有效测量，具备良好的心理测量特性。

Conclusion: 本文首次提出并验证了适用于软件工程环境的同理心量表，为研究者和实践者提供了评估同理心及设计提升同理心干预的工具，有助于提升软件团队协作和用户交互质量。

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [21] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: 现有网络服务可持续性报告常用的能耗模型并不准确，不同网站类别和设备类型下模型误差明显，建议引入更细致的参数以提升报告质量。


<details>
  <summary>Details</summary>
Motivation: 当前网络服务的可持续性报告越来越多地依赖于简化的能耗与碳排放模型（如Digst、DIMPACT），但这些模型的准确性和精密性尚未被充分验证。本研究旨在评估这些模型在真实用户交互下的有效性。

Method: 通过预定义的用户操作流程，分别在四种笔记本电脑平台上，对购物、预订、导航和新闻类网站的能耗进行实际测量，对比常用能耗近似模型（P * t）的结果。

Result: 实验结果表明，常用的恒定功率近似模型（P * t）与实际测量能耗之间存在显著差异，这一差异受网站类别、设备类型及具体任务影响。模型偏差具有系统性，而非随机分布。

Conclusion: 现有简化能耗模型在实际应用中存在系统性误差。可持续性报告框架需引入类别感知和设备相关功率参数，以提高报告的可复现性和准确性。

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [22] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 通过系统性文献综述，分析了动态环境下系统组系统（SoSs）运行时组合的挑战与解决方案，指出目前工具链互操作性不足及评估标准缺失，呼吁业界关注跨领域框架和标准化评估体系以促进SoSs的可持续发展。


<details>
  <summary>Details</summary>
Motivation: 现代系统（SoSs）越来越多地应用于智能城市、自动驾驶等动态环境，要求系统能够在运行时动态组合以增强适应性。然而，关于动态SoSs运行时组合的系统性综述与统一结论尚缺乏。

Method: 采用系统性文献综述（SLR）方法，筛选了2019至2024年间共1774篇文献，最终选取80篇核心文献进行主题分析。

Result: 主要挑战分为四类：建模与分析、弹性运行、系统编排、以及组成系统异质性。解决方案涉及七方面：协同仿真与数字孪生、语义本体、集成框架、自适应架构、中间件、形式化方法及AI驱动的弹性。服务化框架在工具中占主导地位，仿真平台用于评估，但工具互操作性和跨工具链流程有限，缺乏标准化基准。评估方法包括仿真、实现驱动与以人为中心的研究。

Conclusion: 该综述揭示了自主性与协调性、建模与现实之间的差距，以及社会技术融合的紧张关系，呼吁建立标准化评估指标、可扩展的分布式架构及跨领域框架，并为后续研究与动态组合同类系统的实施提供指引。

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Flavors of Quantifiers in Hyperlogics](https://arxiv.org/abs/2510.12298)
*Marek Chalupa,Thomas A. Henzinger,Ana Oliveira da Costa*

Main category: cs.LO

TL;DR: 本文扩展了hypertrace逻辑，引入非约束轨迹变量的量化机制，系统分析了不同量化模式对可满足性问题的影响，证明了与多种逻辑的等价关系，并对相关超逻辑的判定性进行了总结。


<details>
  <summary>Details</summary>
Motivation: 现有的超逻辑（如HyperLTL）仅允许对约束轨迹变量进行量化，缺乏对所有可能轨迹量化的能力，限制了对超属性表达的灵活性和对逻辑性质的研究。本文旨在通过扩展现有的hypertrace逻辑，引入对所有可能轨迹的量化机制，从而研究量化模式对可满足性判定的影响。

Method: 提出扩展的hypertrace逻辑，支持两类轨迹变量量化：约束轨迹变量（限定于模型定义的轨迹集合）和非约束轨迹变量（可为任意轨迹）。以此为基础，分析不同量化模式下可满足性问题的判定，并与其他逻辑（如S1S、HyperQPTL）进行等价性证明。

Result: 证明了无约束轨迹变量的hypertrace逻辑与单后继的单变量二阶逻辑（S1S）等价且可满足；轨迹前缀片段等价于HyperQPTL。此外，约束轨迹量化仅在存在到全称交替时，其可满足性等价于无约束情形，由此可判定。对时刻前缀超逻辑提供了新可判定与不可判定性结果。

Conclusion: 扩展后的hypertrace逻辑通过更加丰富的量化机制，提升了对超属性的表达与分析能力，系统阐述了不同量化模式下的可满足性，并统一了与相关超逻辑的关系，为研究超逻辑的理论基础与实际应用打下了坚实基础。

Abstract: Hypertrace logic is a sorted first-order logic with separate sorts for time
and execution traces. Its formulas specify hyperproperties, which are
properties relating multiple traces. In this work, we extend hypertrace logic
by introducing trace quantifiers that range over the set of all possible
traces. In this extended logic, formulas can quantify over two kinds of trace
variables: constrained trace variables, which range over a fixed set of traces
defined by the model, and unconstrained trace variables, which can be assigned
to any trace. In comparison, hyperlogics such as HyperLTL have only constrained
trace quantifiers. We use hypertrace logic to study how different quantifier
patterns affect the decidability of the satisfiability problem. We prove that
hypertrace logic without constrained trace quantifiers is equivalent to monadic
second-order logic of one successor (S1S), and therefore satisfiable, and that
the trace-prefixed fragment (all trace quantifiers precede all time
quantifiers) is equivalent to HyperQPTL. Moreover, we show that all hypertrace
formulas where the only alternation between constrained trace quantifiers is
from an existential to a universal quantifier are equisatisfiable to formulas
without constraints on their trace variables and, therefore, decidable as well.
Our framework allows us to study also time-prefixed hyperlogics, for which we
provide new decidability and undecidability results

</details>


### [24] [Ground Stratification for a Logic of Definitions with Induction](https://arxiv.org/abs/2510.12297)
*Nathan Guermond,Gopalan Nadathur*

Main category: cs.LO

TL;DR: 本文分析了Abella证明助理的逻辑基础中固定点定义的分层机制，扩展了地面分层理论，指出简单弱化对归纳定义会产生不一致，提出了兼容实际应用的改进方式，为系统后续发展铺路。


<details>
  <summary>Details</summary>
Motivation: 原有的严格分层条件过于严格，限制了某些如逻辑关系法等重要应用。已有工作提出了更灵活的地面分层，但针对归纳定义的适应性和一致性问题尚未解决。

Method: 对Abella证明助理底层逻辑中固定点定义的分层条件（stratification）进行理论分析，通过对比原始分层、Tiu提出的地面分层，并对其在归纳定义中的应用进行一致性验证和理论拓展。

Result: 理论上将地面分层扩展到更广泛的定义，但发现直接应用于归纳定义会引发不一致，因此给出一种更加契合实际逻辑关系应用的推广形式，并为Abella系统未来更灵活定义机制奠定基础。

Conclusion: 将地面分层（ground stratification）理论扩展到包含归纳定义的情况时，发现简单弱化分层条件会导致不一致，因此需谨慎推广，以保证逻辑系统的合理性和一致性。

Abstract: The logic underlying the Abella proof assistant includes mechanisms for
interpreting atomic predicates through fixed point definitions that can
additionally be treated inductively or co-inductively. However, the original
formulation of the logic includes a strict stratification condition on
definitions that is too restrictive for some applications such as those that
use a logical relations based approach to semantic equivalence. Tiu has shown
how this restriction can be eased by utilizing a weaker notion referred to as
ground stratification. Tiu's results were limited to a version of the logic
that does not treat inductive definitions. We show here that they can be
extended to cover such definitions. While our results are obtained by using
techniques that have been previously deployed in related ways in this context,
their use is sensitive to the particular way in which we generalize the logic.
In particular, although ground stratification may be used with arbitrary
fixed-point definitions, we show that weakening stratification to this form for
inductive definitions leads to inconsistency. The particular generalization we
describe accords well with the way logical relations are used in practice. Our
results are also a intermediate step to building a more flexible form for
definitions into the full logic underlying Abella, which additionally includes
co-induction, generic quantification, and a mechanism referred to as nominal
abstraction for analyzing occurrences of objects in terms that are governed by
generic quantifiers.

</details>


### [25] [On the Formal Metatheory of the Pure Type Systems using One-sorted Variable Names and Multiple Substitutions](https://arxiv.org/abs/2510.12300)
*Sebastián Urciuoli*

Main category: cs.LO

TL;DR: 作者提出了一种用传统语法和Stoughton多重替换机制形式化Church风格带Pi类型的lambda项的方法，在Agda中机检并验证了其可行性，对比已有方案后，证明了不辨认alpha等价项也能实现依赖类型理论的机械化。


<details>
  <summary>Details</summary>
Motivation: 旨在开发和验证无须将alpha-可转换lambda项归为同类条件下的依赖类型理论机械化方法。

Method: 采用一阶语法、单类型变量名和Stoughton的多重替换，形式化了带Pi类型的Church风格lambda项的转换理论，并在Agda系统中进行了机检。

Result: 成功形式化了纯类型系统（PTS）及其基本元理论属性，并实现了用Agda进行的全程机检，且与其他形式化方案进行了比较。

Conclusion: 本工作证明了利用常规语法并且无需将alpha-可转换的lambda项归为一类，可以实现依赖类型理论的机械化。

Abstract: We develop formal theories of conversion for Church-style lambda-terms with
Pi-types in first-order syntax using one-sorted variables names and Stoughton's
multiple substitutions. We then formalize the Pure Type Systems along some
fundamental metatheoretic properties: weakening, syntactic validity, closure
under alpha-conversion and substitution. Finally, we compare our formalization
with others related. The whole development has been machine-checked using the
Agda system. Our work demonstrates that the mechanization of dependent type
theory by using conventional syntax and without identifying alpha-convertible
lambda-terms is feasible.

</details>


### [26] [CoLF Logic Programming as Infinitary Proof Exploration](https://arxiv.org/abs/2510.12302)
*Zhibo Chen,Frank Pfenning*

Main category: cs.LO

TL;DR: 本文提出并实现了在支持无限对象证明的逻辑框架CoLF$^\omega_1$上，以通信信道和并发消息传递为核心的证明构造式并行编程模型，并给出对应编译器，推动了推理系统与并行程序的结合。


<details>
  <summary>Details</summary>
Motivation: 众多逻辑框架（如Automath和LF）旨在作为金属语言，以规范无基础承诺的演绎系统，最初用于通用证明检查器。后续，这种抽象层被用来实现定理证明器、类型检查器等算法，但早期框架无法直接表达无限对象或证明。

Method: 论文着重介绍了在新发展出的CoLF$^\omega$系统的一阶片段（CoLF$^\omega_1$）上，探索“计算即为构造证明”的范式，该系统已经支持无限对象与证明。具体方法包括将逻辑变量解释为通信通道，计算过程看作并发消息传递，并实现了一个将CoLF$^\omega_1$编译到并行编程语言Sax的具体编译器。

Result: 实现了CoLF$^\omega_1$到Sax的编译器，能够利用无限对象与证明，支持并发消息传递的计算模式。展现了以证明归约为灵感的并行编程方法。

Conclusion: 首次实现在带有无限对象/证明的一阶逻辑框架中，将证明构造、并发计算、通信通道等思想结合，并给出了实际编译器实现，推动了逻辑框架与并行编程的结合。

Abstract: Logical Frameworks such as Automath [de Bruijn, 1968] or LF [Harper et al.,
1993] were originally conceived as metalanguages for the specification of
foundationally uncommitted deductive systems, yielding generic proof checkers.
Their high level of abstraction was soon exploited to also express algorithms
over deductive systems such as theorem provers, type-checkers, evaluators,
compilers, proof transformers, etc. in the paradigm of
computation-as-proof-construction. This has been realized in languages such as
$\lambda$-Prolog [Miller et al., 1991] or Elf [Pfenning, 1991] based on
backward chaining, and LolliMon [Lopez et al., 2005] or Celf [Schack-Nielsen
and Schuermann, 2008], which integrated forward chaining. None of these early
frameworks supported the direct expression of infinitary objects or proofs,
which are available in the recently developed CoLF$^\omega$ [Chen, 2023]. In
this work-in-progress report, we sketch an approach to
computation-as-proof-construction over the first-order fragment of
CoLF$^\omega$ (called CoLF$^\omega_1$ ) that already includes infinitary
objects and proofs. A key idea is the interpretation of logic variables as
communication channels and computation as concurrent message-passing. This is
realized in a concrete compiler from CoLF$^\omega_1$ to Sax, a
proof-theoretically inspired parallel programming language based on the
proof-reduction in the semi-axiomatic sequent calculus [DeYoung et al., 2020].

</details>


### [27] [Type Theory with Single Substitutions](https://arxiv.org/abs/2510.12303)
*Ambrus Kaposi,Szumi Xie*

Main category: cs.LO

TL;DR: 本文提出将类型论定义为单一替换演算（SSC），从而用更简单和极简的方法替代现有复杂的并行替换模型。证明该方法与主流CwF语法等价，并对类型论基础研究带来简化和推广的可能。


<details>
  <summary>Details</summary>
Motivation: 现有类型论的代数定义（如CwFs，Contextual categories等）大多依赖于并行替换演算，这些替换构成一个范畴。但并行替换有时会导致复杂性。为了解决这一问题，本文提出只使用单一替换和弱化的替换演算。

Method: 本文定义了一种单一替换演算（SSC），只包含单一替换和弱化操作，并给出相关类型和变量的八个方程。同时，分析SSC与CwF之间的关系，证明其语法是同构的。

Result: SSC提供了一个比并行替换演算或B-systems更简洁、极简的类型论定义方法。且对于具有依赖函数空间和宇宙层次的类型论，SSC语法与CwF语法是等价的，并证明增加特定类型构造后，SSC模型可转化为CwF。

Conclusion: SSC是定义类型论的一种简约且有效的新方法，与现有主流并行替换模型在语法上等价，同时具备更多模型与更简单结构。它为类型论的结构研究和模型构造提供了新的方向。

Abstract: Type theory can be described as a generalised algebraic theory. This
automatically gives a notion of model and the existence of the syntax as the
initial model, which is a quotient inductive-inductive type. Algebraic
definitions of type theory include Ehrhard's definition of model, categories
with families (CwFs), contextual categories, Awodey's natural models,
C-systems, B-systems. With the exception of B-systems, these notions are based
on a parallel substitution calculus where substitutions form a category. In
this paper we define a single substitution calculus (SSC) for type theory and
show that the SSC syntax and the CwF syntax are isomorphic for a theory with
dependent function space and a hierarchy of universes. SSC only includes single
substitutions and single weakenings, and eight equations relating these: four
equations describe how to substitute variables, and there are four equations on
types which are needed to typecheck the other equations. SSC provides a simple,
minimalistic alternative to parallel substitution calculi or B-systems for
defining type theory. SSC relates to CwF as extensional combinatory calculus
relates to lambda calculus: there are more models of the former, but the
syntaxes are equivalent. If we have some additional type formers, we show that
an SSC model gives rise to a CwF.

</details>


### [28] [Substitution Without Copy and Paste](https://arxiv.org/abs/2510.12304)
*Thorsten Altenkirch,Nathaniel Burke,Philip Wadler*

Main category: cs.LO

TL;DR: 作者针对简单类型λ演算的替换定义繁琐问题，提出了一种简化方法，并通过Agda脚本实现了与原始结构同构的范畴结构，有效减少了理论和实现中的重复。


<details>
  <summary>Details</summary>
Motivation: 在具有绑定符号的语言（如简单类型λ演算）中，替换操作通常需要重复工作，包括分别定义替换和重命名。这不仅繁琐，也导致在验证该演算的范畴属性时需要重复相同的论证。作者希望解决这一重复问题，简化构造和验证过程。

Method: 作者提出了一种轻量级方法，能够避免替换和重命名的重复定义，并且简化范畴属性的论证过程。论文同时以可读的Agda脚本展现其方法，强调系统化和实现方面。

Result: 该方法成功地给出了一个与初始简单类型CwF同构的简单类型CwF（类别伴随家庭），实现了方法论上的简化。

Conclusion: 通过新的轻量级方法，减少了定义替换和重命名时的重复工作，同时在范畴论属性验证方面也实现了简化，提升了论证效率。

Abstract: Defining substitution for a language with binders like the simply typed
$\lambda$-calculus requires repetition, defining substitution and renaming
separately. To verify the categorical properties of this calculus, we must
repeat the same argument many times. We present a lightweight method that
avoids repetition and that gives rise to a simply typed category with families
(CwF) isomorphic to the initial simply typed CwF. Our paper is a literate Agda
script.

</details>


### [29] [Dependently Sorted Nominal Signatures](https://arxiv.org/abs/2510.12305)
*Maribel Fernández,Miguel Pagano,Nora Szasz,Álvaro Tasistro*

Main category: cs.LO

TL;DR: 本文扩展了名义多排序签名，引入广义具体化消除算子，提出一种依赖型排序系统，对表达式进行分类和形式化处理。系统能表达多种推理演算，并为将表达式的 alpha- 等价完全形式化打下了坚实基础。


<details>
  <summary>Details</summary>
Motivation: 解决现有名义多排序签名在表达式抽象和 alpha- 转换处理中存在的局限性，并为各种推理规则与判断形式提供通用的表达与运算机制。

Method: 扩展名义多排序签名，引入名为广义具体化（generalised concretion）的消除算子，利用依赖型排序系统（类似依赖型 lambda 演算）对表达式进行分类。

Result: 提出了相关系统的规则和性质，并通过实验展示了可表示多种有趣演算。系统能够完全形式化处理带有 alpha- 等价的原始表达式，为类型理论的发展奠定了基础。

Conclusion: 该系统为形式化处理 alpha- 等价的原始表达式提供了理论基础，可以用于构建下一步的类型理论。

Abstract: We investigate an extension of nominal many-sorted signatures in which
abstraction has a form of instantiation, called generalised concretion, as
elimination operator (similarly to lambda-calculi). Expressions are then
classified using a system of sorts and sort families that respects
alpha-conversion (similarly to dependently-typed lambda-calculi) but not
allowing names to carry abstraction sorts, thus constituting a first-order
dependent sort system. The system can represent forms of judgement and rules of
inference of several interesting calculi. We present rules and properties of
the system as well as experiments of representation, and discuss how it
constitutes a basis on which to build a type theory where raw expressions with
alpha-equivalence are given a completely formal treatment.

</details>


### [30] [Proceedings of the International Workshop on Verification of Scientific Software](https://arxiv.org/abs/2510.12314)
*Stephen F. Siegel,Ganesh Gopalakrishnan*

Main category: cs.LO

TL;DR: VSS 2025工作坊汇聚学者，共同探讨科学软件验证的挑战与进展，涵盖多种验证手段与测试方法，推动领域合作，促进科学软件的可靠性和正确性。


<details>
  <summary>Details</summary>
Motivation: 科学软件日益成为各类研究和应用的基础，但其正确性和可靠性常因规模庞大、复杂的计算特性而面临挑战。为此，VSS工作坊旨在汇聚软件验证和科学计算领域的专家，共同探讨如何提升大规模科学代码的验证技术，保障其在实际使用中的可信度。

Method: 本工作坊以多种形式开展，包括五篇同行评议论文、三项邀请报告及若干挑战问题。具体主题覆盖了演绎式验证技术、浮点误差分析、耦合模型的规范化及面向领域的测试方法，并设置实际挑战问题促进工具协同。

Result: 展示了一系列科学软件验证领域的新进展、不同观点与解决方案，挑战问题推动了分散验证工具的合作。工作坊也延续了前期相关活动（如Correctness Workshop系列和NSF/DOE报告），为该领域提供了阶段性总结与前瞻。

Conclusion: VSS 2025工作坊成功促进了科学软件验证领域的交流及合作，提出了多种方法与挑战，为提升科学代码的正确性与可靠性奠定了基础，并展望了工具整合及问题联合攻关的未来发展趋势。

Abstract: This volume contains the proceedings of the Verification of Scientific
Software (VSS 2025) workshop, held on 4 May 2025 at McMaster University,
Canada, as part of ETAPS 2025. VSS brings together researchers in software
verification and scientific computing to address challenges in ensuring the
correctness and reliability of large-scale scientific codes. The program
featured five peer-reviewed papers, three invited contributions, and a set of
challenge problems, covering themes such as deductive verification,
floating-point error analysis, specification of coupled models, and
domain-aware testing. VSS builds on the Correctness Workshop series at
Supercomputing and the 2023 NSF/DOE report on scientific software correctness.
It serves as yet another snapshot of this important area, showcasing a wide
range of perspectives, problems and their solutions in progress, with the
challenge problems having the potential to bring together separate verification
tools into concerted action.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [31] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: LLMs解决逻辑谜题多靠模板记忆，对题目细节的扰动极度敏感。PHANTOM RECALL基准展示模型在改变场景下逻辑推理能力严重不足，亟需提示和工具干预改善。这凸显了当前LLMs在真正推理和语言流畅性上的关键差距。


<details>
  <summary>Details</summary>
Motivation: 现有证据表明LLMs在解决逻辑谜题时大多依赖记忆模板，面对题目扰动时易失效。作者希望系统性检测和分析这一问题，评估模型在多变场景下推理能力的真实水平，并探寻提升方法。

Method: 作者提出了“PHANTOM RECALL”基准，包括25个经典逻辑谜题和149种保持推理结构但改变表面细节的扰动版本。他们评价了11种主流LLM，并引入三项工具：自动逻辑等价评判器、细粒度推理错误分类法和基于这一分类的提示优化方法。

Result: 在未修改的谜题中，主流LLMs能达到近乎完美的准确率，但在人类轻松应对的扰动谜题中则明显落后。模型常出现“幻影回忆”和过度详述，提示其逻辑推理未能根据上下文真正调整。

Conclusion: 大型语言模型（LLMs）在面对经过扰动的逻辑谜题时表现不佳，容易发生“幻影回忆”——即套用记忆中模板而非真正推理。这暴露了其在逻辑理解与语言流畅性间的显著差距。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [32] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: 本文发现单靠LLM世界建模受限于幻觉和知识陈旧，长流程下仿真能力急剧下降。引入检索增强技术（R-WoM）有效提升了大模型对复杂环境的推理能力，尤其在需长时序推理的任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型（LLM）能否作为世界模型来提升智能体在数字环境中的决策能力，尤其在无需大量试错探索的情况下进行环境模拟和动作结果预测。当前LLM由于幻觉和静态知识库的限制，导致长时序模拟受阻。

Method: 通过三个任务来系统性评估LLM在世界建模的可行性：1）下一状态识别，2）全流程规划一致性测试，3）里程碑状态转变识别。同时，提出了一种检索增强型世界模型（R-WoM），在LLM推理过程中引入动态检索的最新真实知识以增强模拟能力。

Result: LLM在近期状态和关键性状态转变识别上表现较好，但在长流程规划中表现急剧下降，难以稳定模拟环境动态。R-WoM通过补充最新外部知识显著提升了模拟性能，在OSWorld和WebArena上分别提升了25.3%和18.1%。

Conclusion: 基础LLM作为世界模型在长程环境仿真存在显著缺陷，通过引入检索增强（R-WoM）可显著提升其对复杂环境动态的建模能力。

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [33] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: 本文发现大语言模型区分真假陈述的能力对输入表面形式极为敏感，模型内部知识表征不够稳健，导致泛化能力有限，亟需探索更鲁棒的知识表征方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在实际应用中需要具备鲁棒性，以便在多样且往往与训练环境不同的场景中可靠地应用。然而，已有大量研究发现，LLM对输入的微小变化过度敏感，表现出脆弱性。作者希望探究这种脆弱性是否源自模型内部知识表征的不稳定性。

Method: 研究建立在之前工作的基础上，利用LLM的内部表征对陈述真实与否进行区分。通过对样本进行表层但语义保持一致的扰动（如错别字、重述），评估表征的可分离性，即模型能否持续区分真实和虚假陈述。该评估在四个LLM系列、五个评测数据集、三种知识探测方法上进行。

Result: 实验显示，随着样本经过更多表层扰动并逐渐偏离模型预训练时接触的分布，LLM内部关于陈述真实与否的表征会崩溃。即，模型仅在样本与训练数据形式相似时能有效地区分真假陈述，这种能力高度依赖于表面表达。

Conclusion: LLM学习到的知识表征较为浅层且不够鲁棒，因而泛化能力有限。这是造成模型在基准测试中表现脆弱的一个潜在原因，并对基于真伪探测的评估提出了挑战。作者呼吁进一步研究以提升知识表征的鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [34] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 在机器翻译任务中，直接引入思考过程（如链式思考或中间token）对于大模型翻译提升有限。最有效的方式是扩展高质量平行数据或优化目标翻译，而非单纯模仿人类推理步骤。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型（LRMs）在数学和编程任务领域表现突出，通过自然语言推理提升了问题求解能力，但这些思考机制在机器翻译（MT）任务中的作用尚未被深入探究。

Method: 作者提出在机器翻译任务中引入“中间思考token”（thinking tokens），并对比模型在常规输入输出微调与结合翻译步骤解释的链式思考（CoT）微调中的表现。同时也探讨了利用模块化翻译策略输出聚合中间token的方案。

Result: 仅加入链式思考的解释与中间token，并未提升翻译质量，且微调中引入推理解释并不优于标准微调。但如果中间token中包含实际的翻译过程内容（如多个翻译尝试），则能够改进表现。

Conclusion: 单纯模仿人类翻译的思考过程用于微调，对机器翻译模型帮助有限。与其蒸馏CoT推理解释，不如专注用教师模型优化目标翻译或扩展平行语料，对提升翻译性能更有效。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [35] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 该论文提出了MIND用户参与的多语言问答事实核查框架，能够自动检测并标记事实和文化差异，在多领域表现优异，推动问答系统更精准和具文化敏感性。


<details>
  <summary>Details</summary>
Motivation: 多语言问答系统需要确保跨语言的事实一致性，尤其对于客观问题，并且对主观问题需考虑文化差异，因此有必要检测和处理事实与文化上的不一致。

Method: 提出了MIND，一个结合用户反馈的事实核查流程，通过检测多语言问答中的事实和文化差异来发现不一致，重点关注如“谁协助分娩”等文化敏感问题，并在母婴健康领域的双语问答系统及其他领域数据集上进行评估。

Result: MIND在所有测试领域均表现出可靠的差异识别能力，同时发布了已注释的双语问题数据集，验证了系统的通用性和有效性。

Conclusion: MIND系统能够有效识别并标记多语言问答知识库中的事实和文化不一致，从而推动更加准确和具备文化敏感性的问答系统发展。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [36] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 本文提出TopoAlign框架，使现有代码可以产生结构类似于正式数学语句的数据，用于训练数学大模型。实验证明该方法显著提升了自动形式化的准确性，对通用和专用模型均有益处。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在非正式和正式数学推理方面表现出色，但在自动形式化（从非正式数学语句转化为正式语句）任务上仍有困难。当前用于训练数学LLM的大规模数据集稀缺，特别是包含非正式与正式语句对的数据。现有模型虽能将自然语言转为代码，但代码与正式数学语句在结构和语法上存在显著差异，限制了迁移学习的效果，因此急需新的方法利用现有资源提高数学LLM的自动形式化能力。

Method: 提出了TopoAlign框架，将广泛现有的代码仓库解构为文档字符串、主函数和依赖函数，并重组为与正式数学语句结构相似的样本，从而获得结构对齐的训练数据，无需额外人工标注。作者用TopoAlign数据训练了DeepSeek-Math和Herald两个模型，并在minif2f、Putnam与ProofNet等基准上评测。

Result: TopoAlign显著提升了DeepSeek-Math的表现，BEq@10指标提升了17.77%，typecheck@10提升了68.82%。对于Herald（已为数学任务专门设计的模型），TopoAlign仍带来0.12%和1.09%的提升，证明结构对齐的代码数据对训练专业数学模型同样有益。

Conclusion: TopoAlign能够极大扩展可用于Math LLM训练的数据资源，无需人工标注，显著提升模型在自动形式化任务上的表现，对通用和专用数学模型均有效。

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [37] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY通过生成基于用户画像的合成偏好数据，减少人力任务成本，显著提升了LLM多文化个性化内容的生成效果和用户满意度，是LLM个性化的高效可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化依赖高成本的人类反馈或交互日志，无法大规模扩展且忽略了更深层次的用户属性。

Method: 提出GRAVITY框架，通过生成基于用户概要（兴趣、价值观、信仰、人格特质等）的合成偏好数据，利用人口统计、文化和心理特征（如Hofstede文化维度、Schwartz价值观、世界价值观调查和OCEAN人格）来指导个性化内容生成。

Result: 在对400名亚马逊用户的图书描述生成中，GRAVITY相较于传统提示、标准微调和简单合成方法表现更优。尤其在多文化环境（美国、巴西、日本、印度）下，用户偏好多超过4个百分点提升，用户调查中GRAVITY生成内容有86%以上优选率。

Conclusion: 基于场景和概要的合成数据能够更好地捕捉用户差异，降低人工标注成本，并显著提升LLM个性化内容生成的质量和用户体验，提供了极具扩展性的个性化新途径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [38] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 作者开发了一种自动生成高难度、不可作弊、多跳查询的流程，从而更真实地评估和提升RAG系统性能，在主流数据和模型上实验效果显著优于以往基准。


<details>
  <summary>Details</summary>
Motivation: 当前实际应用中，RAG系统面对复杂查询时，常常遇到语料库缺失或信息不完整的问题，但现有基准测试很少涵盖真实复杂、多跳或超出范围的问题，容易被“作弊”或仅要求简单事实回忆，无法全面暴露现有系统的局限。

Method: 提出了自动化、可控难度生成不可作弊、真实、不可回答和多跳查询（CRUMQs）的新流程，可适配任意语料库与领域，并用于现有RAG数据集进行实验评测。

Result: 在两个主流RAG数据集上生成了CRUMQs，并在主流检索增强大模型上进行了基准测试，结果显示新基准对RAG系统挑战性更高，作弊率最多减少81%。

Conclusion: 该流程可显著提升基准测试的难度和真实感，推动RAG系统能力的发展。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [39] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: DMTD方法只用transformer模型的后期层进行多token生成，显著加速推理且损失很小，未来有更大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大型语言模型（LLMs）多采用decoder-only transformer架构。已有研究发现，模型的不同层级在推理中扮演不同角色：早期层负责输入理解，中间层进行任务处理，晚期层生成输出。作者提出，既然早中层已充分处理和提炼信息，是否有必要每次生成新token时反复经过这些层？动机是提升推理速度，简化计算流程。

Method: 提出了一种新的推理范式——Direct Multi-Token Decoding（DMTD）。具体做法是在生成多个token时直接利用晚期层进行推理，无需每次重复经过早中层。与现有的speculative decoding方法不同，DMTD不引入额外参数、辅助流程或后验校验。通过微调Qwen3-4B模型并在有限数据集上训练和实验，分析了方法的可行性。

Result: 在有限数据集微调下，DMTD的Qwen3-4B模型能够实现高达2倍的推理速度提升，同时性能损失很小。通过扩展性分析，作者认为随着训练数据的增加，该方法的表现还可以继续提升。

Conclusion: DMTD是一种简化、加速语言模型推理的新方法，无需结构改动或额外复杂流程，已在实验中验证其有效性和性能潜力。后续若扩大训练数据量，预计能进一步提升质量和速度。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [40] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 该论文针对LLM在长序列任务受限于上下文长度的问题，提出Context-Folding框架和FoldGRPO强化学习方法，实现更高效的任务拆解和上下文管理，在多个复杂任务上取得了优异效果，显著减少了所需的活动上下文。


<details>
  <summary>Details</summary>
Motivation: 长序列任务中，LLM受限于上下文长度，如何有效管理和利用上下文资源成为瓶颈。

Method: 提出Context-Folding框架，让智能体通过程序化分支和折叠总结完成的子任务，并引入FoldGRPO强化学习框架以学习这种行为，配合过程奖励促进任务拆解和上下文管理。

Result: 在复杂任务（如Deep Research和SWE）中，该智能体能用10倍更小的活动上下文达到或超越ReAct基线，并远超仅用总结管理上下文的模型。

Conclusion: Context-Folding技术提升了LLM在长序列任务中的效率和能力，优于现有基线方法。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [41] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 本文指出自动化形式化不仅是语言翻译，还需先对数学结论进行猜测。研究创建了新评测工具ConjectureBench和改进方法Lean-FIRe，发现当前LLMs的形式化表现被高估，未来提升需重视猜测任务。


<details>
  <summary>Details</summary>
Motivation: 当前自动化形式化（autoformalisation）任务主要关注将非正式的数学表述转换为形式化语言，但忽略了在此之前必要的“猜测（conjecturing）”步骤。许多数学问题无法直接形式化，必须先猜测结论，如显式答案或具体界限。同时大语言模型（LLMs）在自动化形式化表现有限，且猜测能力评价常与形式化或证明混合，难以单独考察，因此难以理解猜测环节的影响和局限性。

Method: 该研究扩充了现有数据集，创建了ConjectureBench，并重新设计了评价框架和指标，专门用于测量LLMs在猜测环节的能力，无论作为独立任务还是自动化形式化流程中的一环。作者还提出了一种推断时方法Lean-FIRe，用于提升LLMs的猜测与形式化表现。

Result: 实验证明主流基础模型（如GPT-4.1、DeepSeek-V3.1）在评估时若直接提供猜测，自动化形式化的表现被严重高估。Lean-FIRe方法首次实现了GPT-4.1在PutnamBench数据集上的13道题端到端自动化形式化，DeepSeek-V3.1完成了7题。结果显示：LLMs具备生成准确猜测的知识，但要提高自动化形式化效果，必须将猜测任务独立出来，并进一步研究如何与形式化正确整合。

Conclusion: 自动化形式化任务要想取得突破，不能忽略猜测步骤。本文提供了专门的评测工具和改进方法，前瞻性地指引未来应更加重视猜测环节，以推动形式化数学推理的进步。

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [42] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: 本研究提出用于多轮智能体评估的SAGE模拟框架，结合业务场景特有知识生成更真实的用户行为，显著提升智能体评估发现效率。


<details>
  <summary>Details</summary>
Motivation: 多轮交互式智能体的评估依赖人工，成本高，主观性强，现有的模拟用户方法通常局限于通用用户建模，缺乏对具体业务场景的真实行为建模。

Method: 提出SAGE框架，结合顶层业务知识（如理想客户画像）与底层业务知识（如产品目录、FAQ、知识库），用于业务场景下的多轮智能体用户模拟与评估。

Result: SAGE生成的交互更真实、更具多样性，能够发现多达33%更多的智能体错误，有助于相关系统的缺陷发现和迭代优化。

Conclusion: SAGE作为评估工具在提升交互的真实性和发现智能体缺陷方面效果显著，适用于业务场景下的智能体评测和改进。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [43] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 提出了一种高效的逻辑等价题自动生成系统，生成题难度和准确率与教材相当，可大量生成独特题目减少抄袭，助力离散数学教学。


<details>
  <summary>Details</summary>
Motivation: 在线教学下学术不端问题日益严重，现有自动化生成离散数学逻辑等价题的方法效率低且题目难度不均，亟需更优解决方案以提升教学效果和公平性。

Method: 提出了一种基于形式语言定义和线性时间算法的逻辑等价题自动生成方法，并将其与教材题目、大模型生成题目进行实验对比分析。

Result: 实验结果显示，该系统生成题目的正确率和难度均与教材题目相近，验证了系统生成题目的质量。

Conclusion: 本文提出的AQG系统生成的逻辑等价题在难度和准确性上与教材习题相当，有效提升了题目的多样性和防抄袭能力。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [44] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 论文实测农业领域信息抽取，两种方法各有优缺点：LLM系统性能更好但速度慢且受控难，NS系统更快可控但泛化差且维护难。实际应用需综合考虑性能、效率和控制等多方面因素。


<details>
  <summary>Details</summary>
Motivation: 目前信息抽取领域主流趋势为过度依靠大型语言模型，忽略了符号或统计方法多年来积累的专业经验。作者希望通过实证比较，揭示不同方法各自的权衡与应用场景。

Method: 分别在三个农业子领域（猪、奶制品、作物）的九次访谈数据上，对神经符号系统和基于大型语言模型的系统进行了比较评测，并对关键性能（如全部信息抽取和核心细节）进行F1分数的量化对比。

Result: LLM系统F1成绩全面优于NS系统（69.4对52.7，及核心63.0对47.2），但NS系统在运行速度、控制力和特定环境下的准确性上有优势。LLM系统则表现为高性能、快速部署、易维护，但存在运行缓慢、控制有限、模型依赖和幻觉风险。最后总结项目部署时存在超出准确率的“隐藏成本”。

Conclusion: 虽然LLM系统在实际性能上优于NS系统，但在实际应用中需要兼顾性能、效率与控制。二者各有优势与不足，尤其在真实世界应用时存在诸多隐藏成本。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [45] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文针对大语言模型因不良提示词导致的幻觉问题，提出CPR框架，显著提升生成质量且减少幻觉，实验显示CPR提示词优于原始提示词。


<details>
  <summary>Details</summary>
Motivation: 大模型在生产回复时经常因为用户输入的语句结构不佳或表述模糊，导致生成出似是而非的错误内容（幻觉），影响模型的可信度。本文认为提升提示词质量可以大幅减少该问题。

Method: 提出Curative Prompt Refinement (CPR) 框架，通过一个微调的小型语言模型对用户输入的句子进行清洗和补充描述，以更好地对齐用户意图。

Result: 实验结果显示，采用CPR处理过的提示词，其生成结果在无需外部知识情况下，超过90%胜率（表现优于原始提示词）。

Conclusion: CPR有效提升了语言模型的生成质量，并显著减少了幻觉现象。

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [46] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文提出多阶段提示词优化（MPR）方法，分阶段修正不规范的提示词，通过小型语言模型和自反思机制提升提示词质量。在实验中，MPR显著减少了大语言模型的幻觉，提升了输出准确率，且可与现有缓解框架并用，是一种高效且实用的提升方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在自然语言理解和生成任务方面表现出色，但仍面临幻觉（即生成表面合理但实际错误的信息）问题。在已有工作中，导致幻觉的众多因素之一——提示词的不规范（如语法错误、歧义、信息不完整）——却较少被深入探讨。

Method: 提出多阶段提示词优化（MPR）框架，针对提示词的不规范问题分阶段改进。每一阶段通过小型语言模型（SLM）修正特定错误（如标点、拼写、关键词误用），并逐步增强提示词的清晰度与上下文信息，还引入自反思机制进行输入优先级排序。

Result: 在幻觉基准测试上，经过MPR优化的提示词相比原始提示词的表现超过85%的胜率，有效减少了幻觉并提升LLM输出的准确性。此外，MPR可与已有的后处理幻觉缓解框架结合，进一步提高效率和适用性。

Conclusion: MPR是一种高效、轻量且可适配的框架，可明显提升大语言模型在多领域的输出准确性与可靠性，且易与现有方法结合，增强其应用价值。

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [47] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 研究发现：采用包含人类标签变异（HLV）的训练方法，无需专门去偏，也能提升AI模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 探讨人类标签变异（HLV）对模型公平性的影响，以及各种HLV方法与多数投票标签训练的对比，因相关研究匮乏。

Method: 通过比较多数投票标签和不同HLV方法训练模型，分析其公平性表现。

Result: 实验结果显示，多种HLV训练方法在未进行去偏处理时，能正向影响模型公平性。

Conclusion: HLV训练方法在没有明确去偏措施的情况下，可以提升模型的公平性。

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [48] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文综述了大语言模型幻觉检测中的不确定性量化方法，系统分类和实证分析了代表性方法，总结了现有挑战并展望未来方向。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型（LLM）在自然语言处理领域取得突破性进展，被广泛应用于问答、机器翻译和文本摘要等任务，但在实际应用中存在可靠性与可信度问题，尤其是幻觉现象，即生成事实错误但表述合理的内容。为提高模型输出的可信性，亟需有效手段评估并量化其不确定性。

Method: 首先介绍了不确定性量化（UQ）的基础理论，包括其定义以及知识性与随机性不确定性的区分，随后分析此理论在大语言模型背景下的适配与应用。进一步探讨了UQ在幻觉检测中的重要作用，系统性地对现有方法进行了多维度分类，并展示了部分代表性方法的实证结果。此外还就当前局限与未来研究方向进行了讨论。

Result: 系统梳理了LLM领域中用于幻觉检测的不确定性量化方法，分析了各类方法的优缺点，并通过实验评估了典型方法在检测幻觉方面的效果，揭示了UQ方法有助于提升模型可靠性，但依然存在一些待解决的问题。

Conclusion: 不确定性量化为大语言模型幻觉检测提供了有效途径，有助于提升生成内容的可靠性，但现有方法在适应性与实际应用中仍有不足。未来研究需进一步优化UQ方法并扩展其应用范围。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [49] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 本文提出用大语言模型重写用户输入提示，从而提升文本生成图像系统的表现，包括图文对齐、画面美学及泛化性，无需额外标注数据且对不同T2I模型都有效。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像（T2I）模型面对简单或不明确的提示时，容易出现图文对应度低、图像美学性或质量欠佳的问题。作者希望改进T2I模型对提示信息的理解和生成效果。

Method: 提出一种基于大语言模型（LLM）的提示重写框架，通过设计奖惩系统和迭代的直接偏好优化（DPO）训练流程，在无需人工标注监督数据的条件下提升用户输入提示的质量。该方法与不同T2I骨干模型结合进行评估。

Result: 实验结果表明，提示重写器在图文对齐、图像质量和美学性方面均超过现有强基线模型且有较强的泛化能力——在不同T2I骨干模型上迁移无需重新训练。同时，系统性评估了LLM模型容量对性能提升的影响。

Conclusion: 提示重写是一种高效、可扩展且与模型无关的策略，有助于显著提升各类T2I系统的生成效果。作者计划公开代码和训练好的重写器模型。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [50] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 本文提出层级对齐方法，将DPO针对模型不同功能层精细化应用，从而提升语法、逻辑和事实一致性，且规避传统方法中的副作用。实验表明分层精调比整体优化更高效、更可控，适合未来先进LLM的开发。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）对齐方法通常将模型视为整体，忽视了Transformer架构中不同层的功能分工。本文希望挑战这种一刀切的范式，探讨更细粒度、更结构化的对齐方法。

Method: 提出层级对齐（Hierarchical Alignment）方法，将DPO策略分别针对模型的局部（句法）、中间（逻辑）和全局（事实）功能块进行精细化调优。采用LoRA进行小范围微调，并通过LLM-as-Judge进行效果评估。

Result: 局部层对齐提升语法流畅性，全局层对齐不仅提升事实一致性，还在逻辑连贯性上效果最佳，超越传统基线。所有分层策略均避免了传统DPO中的“对齐税”，即流畅性提升但逻辑推理下降的问题。

Conclusion: 层级对齐为模型结构感知、定制化调优提供了更高效、可控和可解释的路径，有助于打造更强大可靠的LLMs。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [51] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 本文针对长上下文Transformer模型在存储和性能两个方面的挑战，提出了APCE方法，通过智能选择重要输入片段，既减少了内存需求，又维持甚至提升了摘要性能，是一种兼容性好、有效的长文本处理解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文Transformer模型（LCTMs）在实际部署时面临两大挑战：随着输入序列长度增加，二次自注意力和线性KV缓存带来巨大的内存消耗；以及模型在处理超长上下文时表现明显下降（ContextRot现象）。

Method: 提出了一种名为APCE的机制，通过低维语义相似度匹配，主动选择与当前查询最相关的输入片段，直接作用在输入层，降低对硬件依赖，并提升部署灵活性。

Result: APCE方法在长文本摘要任务上，仅使用50%-70%的输入片段，就达到了与全输入Dense模型相当甚至更优的性能，同时显著改善了KV缓存和自注意力的内存效率。

Conclusion: APCE能够有效缓解LCTM内存瓶颈和上下文退化现象，为长上下文任务提供了一种高效、易部署的解决方案。希望能启发更多面向LCTMs高效性的后续研究。

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [52] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: 本文验证了Verily行为健康安全过滤器（VBHSF）在精神健康危机识别上的优异表现，其在两个真实数据集上的灵敏度和准确性远超主流开源防护工具，有望成为医疗健康领域内容安全的重要技术。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在精神健康危机应对上常常表现不佳，可能导致有害或不适当建议，甚至助长破坏性行为。因此，亟需一种更安全有效的内容防护机制来应对这类紧急情况。

Method: 评估Verily行为健康安全过滤器（VBHSF），使用由临床医生标注的两个数据集（Verily Mental Health Crisis Dataset 和 NVIDIA Aegis AI Content Safety Dataset），并与两个开源内容防护工具（OpenAI Omni Moderation Latest、NVIDIA NeMo Guardrails）进行比较。衡量指标包括灵敏度、特异性、F1分数等。

Result: VBHSF在Verily数据集上取得了极高的灵敏度（0.990）和特异性（0.992），F1分数为0.939，表现均衡且能够准确识别不同危机类别。在NVIDIA数据集上灵敏度和准确度仍高（0.982、0.921），特异性略有降低（0.859）。与NVIDIA NeMo和OpenAI Omni Moderation Latest相比，VBHSF在所有评估中灵敏度显著更高，部分特异性也更优，而其他工具在部分危机类型识别能力极低（敏感度<0.10）。

Conclusion: VBHSF作为内容安全过滤器，表现出强大的、可泛化的精神健康危机识别能力，尤其优先保障高灵敏度，有效减少关键危机漏检，极适合医疗健康场景使用。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [53] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 提出PACE方法以高效评估LLMs创造力，结果与现有排行榜一致。LLMs虽接近普通人类表现，但低于专业人士，人类联想更具多样性和特色。


<details>
  <summary>Details</summary>
Motivation: LLMs创造力评价很重要，但面临数据污染和人工评估昂贵等难题。需要一种高效、低污染的新评估方法。

Method: 借鉴人类创意测评，提出PACE方法：让LLMs生成平行联想链，并以此评估其创造力。通过与Chatbot Arena创意写作排名的相关性（Spearman's ρ=0.739, p<0.001）验证其有效性，并进行与人类比较及语言模式分析。

Result: PACE能高效评估LLM创造力，结果与现有排行榜高度相关。LLMs整体创意表现接近普通人类但落后于专业人士，人类展现出更丰富联想模式。

Conclusion: 高性能的LLMs在创造力评估中与普通人类表现相当，但专业人类始终优于LLMs。语言分析显示，双方在联想中具体性递减，但人类的联想模式更加多样。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [54] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本研究针对多语言领域适应在大模型中的知识迁移机制展开，提出了自适应评估方法AdaXEval，实验证明即使优质双语语料存在，知识跨语言迁移依然困难。


<details>
  <summary>Details</summary>
Motivation: 目前关于多语言领域适应（ML-DA）在大型语言模型（LLM）上的知识跨语言迁移机制尚未深入研究，特别是在低资源条件下模型表现不佳。此研究旨在探索领域知识如何在单一语言内学习并跨语言迁移，弥补现有研究的不足。

Method: 提出了AdaXEval，一种自适应评估方法，从用于训练的双语领域语料中构建多项选择问答数据集，直接分析多语言知识获取过程。通过对LLM进行持续训练，结合多样化数据方案，追踪模型的知识习得及转化机制。

Result: 实验基于13B规模的英日双语LLM进行，发现即便使用高质量双语语料，跨语言知识迁移依然具有挑战性。

Conclusion: 跨语言领域知识迁移机制复杂，在现有方法下仍难以实现理想的迁移效果。AdaXEval方法可更直接地评估和分析知识获取过程。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [55] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 本文系统分析了语音与文本在大型语音语言模型中的表现差距，发现深层表示的方向趋同但幅度差异加大，并提出用于度量单词级对齐质量的新指标。通过针对关键token的干预策略，有效提高了语音输入的正确率，首次为模态差距优化提供了理论和方法支持。


<details>
  <summary>Details</summary>
Motivation: 大型端到端语音语言模型（LSLMs）在对话生成方面表现优异，但在语义理解基准测试上，相较于传统流水线系统表现不佳，存在语音与文本输入性能差异的问题。研究动机主要是针对这一“模态差距”进行系统性分析和优化。

Method: 论文通过系统性实验分析LSLMs语音与文本输入的表现差距。从粗粒度和细粒度两个层次分析文本与语音的表示：在粗粒度层面，考察了深层语音与文本表示的方向和幅度差异；在细粒度层面，分析了单词级别的表示对齐，并提出了新的‘Alignment Path Score’用于量化对齐质量。结合分析结果，论文提出了角度投影和长度归一化策略，对关键token进行干预。

Result: 研究发现语音与文本输入在深层表示上方向更加对齐，但幅度差异加大，而表示相似性与模态差距高度相关。单词级别的自然对齐模式出现，Alignment Path Score与模态差距有更强相关性。通过角度投影和长度归一化干预，语音输入准确性得到改善。

Conclusion: 该工作首次系统性地揭示了LSLMs中的模态差距及对齐机制，并提出了有效的干预方法，为未来LSLMs优化提供理论和方法指导。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [56] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文针对MLLM的多轮对话安全问题，提出了SafeMT基准和Safety Index用于量化评估，并开发了能有效检测多轮恶意意图的对话安全调节器，实验验证提升了多轮对话下的模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型(MLLMs)的广泛应用，其安全性问题日益突出。尤其是在多轮对话中，安全风险比单轮提问更高，但现有基准往往忽视了多轮对话这一危险场景。作者希望推动社区关注MLLMs在多轮对话中的安全问题。

Method: 作者提出了SafeMT这一基准，涵盖了10000个实例、17种场景及4种越狱方法，内容包括含有有害查询和图像的多轮对话数据。同时提出了Safety Index（SI）用于量化评估MLLM在对话过程中的安全性。最后，作者提出了一个对话安全调节器，以侦测隐藏在对话中的恶意意图并向MLLMs提供安全策略。

Result: 对17个主流模型的测试发现，多轮有害对话回合数越多，模型被攻击的风险越高，说明当前模型安全机制尚不足以应对复杂对话场景。此外，用于安全调节的moderator在多个开源模型下都比现有守卫模型更加有效地降低了多轮攻击成功率(ASR)。

Conclusion: SafeMT基准和SI指标能够有效促进MLLM在多轮对话场景下的安全性研究，提出的安全调节器也为提升多模态大模型的安全性提供了实用的路径。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [57] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: 本文提出替代Transformer标准注意力机制的新方法，通过显式量化模型不确定性，减少了大语言模型的自信幻觉和错误，增强了AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）容易产生“幻觉”，即以自信口吻生成事实错误的信息。作者认为这主要由于Transformer结构中Softmax函数在每一层都会丢失关于不确定性的关键信息，造成“人工确定性”。

Method: 本文提出Credal Transformer，用基于证据理论的Credal Attention Mechanism (CAM) 替换标准注意力结构。CAM生成的是一组分布(credal set)而非单一注意力向量，通过将注意力得分重新定义为Dirichlet分布的证据质量，从而显式地衡量模型不确定性。证据充分时可还原标准注意力；证据不足时生成更分散的分布，反映模型的不确定性。

Result: 实验表明，Credal Transformer能有效识别分布外的输入，量化不确定性，并在无法回答的问题上通过选择“放弃”而极大减少自信的错误回答。

Conclusion: 本文提出一种新型架构和范式，将不确定性量化直接集成到大模型结构中，为减少AI幻觉、提升其可靠性提供基础。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [58] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型中的并行推理范式，定义并区分了相关概念，总结了主要技术类型与应用，并指出挑战与未来方向，为研究者提供系统性参考。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLMs）的推理能力不断提升，但传统顺序推理方法在鲁棒性方面存在不足。因此，为了提升推理的健壮性与实用表现，学术界开始关注并行推理，将其作为新型推理范式。本文旨在梳理并总结并行推理领域的最新进展与挑战。

Method: 本文首先给出了并行推理的形式化定义，并澄清了其与链式思维（Chain-of-Thought）等相关概念的区别。随后，作者基于全新分类法，对先进并行推理技术展开梳理，包括非交互式推理、交互式推理及注重效率的解码策略等。此外，文章还系统探讨了并行推理在复杂问题求解、提升LLM输出可靠性等应用场景中的作用。

Result: 本文系统阐述了并行推理的分类、典型技术和应用案例，归纳了当前的核心挑战，并提出了未来研究的潜在方向，期望为领域新手和研究者提供参考与启发。

Conclusion: 并行推理为LLMs带来推理鲁棒性的提升和更丰富的应用潜力，但仍面临若干挑战。本文通过归纳现有技术路线和问题，为后续研究提供了有价值的导航和展望。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [59] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 论文探讨了离散空间推理促进方法（采样+重排序）在COCONUT连续空间推理模型上的适应性。结果显示，采样与重排序提升有限，主要受限于连续空间缺乏可判别正误的归纳偏置。作者建议未来需在训练阶段显式构建这类归纳偏置以突破性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 已有的基于采样和重排序（如PRM、ORM）的方法在大语言模型离散文本推理中表现优秀，但是否能迁移到连续空间推理仍是未知且具有挑战性的课题。

Method: 以COCONUT连续空间推理语言模型为基础，通过dropout采样生成多路线推理样本，并进行Pass@N分析，同时尝试采用离散空间常用的数据生成和PRM、ORM训练方法；深入分析样本几何、轨迹动态等方面，寻找表现差异原因。

Result: 证实多样推理路径的生成在连续空间是可行的，但数据生成和模型训练策略仅带来有限性能提升，无法像离散空间那样显著提升模型准确率，PRM/ORM正确推理与错误推理的判别效果较弱。

Conclusion: 连续空间推理当前受限于缺乏关键的归纳偏置，常见方法迁移效果有限。今后的训练框架应在优化准确率的同时，主动引入可用于推理时判别正误的归纳偏置，以进一步提升连续推理模型的效果。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [60] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 本文提出了融合大语言模型与知识图谱嵌入的新框架LLaDR，有效提升了药物重定位的准确性，尤其适用于复杂和罕见疾病，并在多个实验和案例中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 药物重定位对加速复杂和罕见疾病的治疗发现至关重要。虽然生物医学知识图谱广泛应用于此任务，但现有方法忽略了真实实验室里常识性生物医学知识，如某些药物与特定治疗基本不兼容的机制。为解决这一问题，研究提出了新的方法。

Method: 提出了LLaDR框架，利用大语言模型（LLM）提取生物医学实体的语义丰富文本表示，并用于微调知识图谱嵌入模型（KGE），将治疗相关知识注入KGE中。

Result: LLaDR显著提高了生物医学概念的表示能力，增强了对复杂或研究较少适应症的语义理解。在多个基准场景下表现优异，并通过阿尔茨海默症案例验证了其稳健性和有效性。

Conclusion: LLaDR方法能增强知识图谱的概念表达和语义理解，提升药物重定位任务的性能，在罕见和复杂疾病上尤其有效。代码已公开。

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [61] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文首次系统研究了大型音频语言模型在事件时间定位任务上的时间偏差，发现该问题普遍且严重，提出量化指标TBI，并呼吁开发更具时间鲁棒性的模型架构。


<details>
  <summary>Details</summary>
Motivation: 尽管LALMs广泛应用于音频理解与多模态推理，但对事件精准定位时间（即事件发生的具体时间点）的能力尚未得到系统性研究。准确定位事件时间对于很多实际应用至关重要，因此有必要揭示并量化相关模型在此任务上的潜在不足。

Method: 通过在带有时间戳的数据集上进行对照实验，量化不同模型和数据集下的时间偏差，并提出时间偏差指数（TBI）及可视化工具来评估和展示模型的时间定位误差。

Result: 研究发现：（1）时间偏差在不同数据集和模型之间普遍存在；（2）音频片段越长，时间偏差越大，最长可累计到数十秒；（3）不同事件类型及位置的偏差程度不同。此外作者提出TBI指标系统性量化时间对齐失误。

Conclusion: 目前的大型音频语言模型（LALMs）在时间戳预测上存在系统性时间偏差（temporal bias），即对事件发生时间的识别常常向前或向后偏移，受录音长度、事件类型等因素影响。需要新的模型结构来提升时间定位的鲁棒性。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [62] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种利用偏好微调的大型语言模型进行语音翻译分割的新方法，比现有分割模型精度更高、译文更自然且延迟更低，推动人工对齐的自适应同传技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有即时语音翻译中的分割方法依赖于预训练模型（如SHAS）或启发式规则，但这些方法未能充分结合人工偏好对分割点的选择，在自然实时口译场景下有所局限。

Method: 提出一种基于大型语言模型（LLM）并通过直接偏好优化（DPO）训练的分割框架，让模型自身依据人类偏好预测更自然的分割点，提升分割的适应性和质量。以SeamlessM4T v2作为翻译骨干，在三种语言对（英-日、英-中、英-德）进行评估。

Result: DPO微调的LLM分割模型在分割准确率上超过SHAS，在翻译质量（BLEU, COMET）和延迟（Average Lagging）也表现出持续提升。系统能直接与IWSLT基线比较，验证其优越性。

Conclusion: 基于偏好微调的大型语言模型能够超越现有预训练分割模型，有效提升自适应和以人为本的同声传译表现，显示出在自然口译应用中的巨大潜力。

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [63] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文提出了一套全新的、能根据伤害严重性对大型语言模型公平性进行评测的部署对齐框架HALF。通过在多个真实应用领域评测多款LLM，发现当前LLM尚未做好实际应用的公平性准备，且现有表现与实用需求之间存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在医疗、法律、招聘和教育等高影响领域的广泛应用，模型的公平性和偏见评估变得至关重要。目前的评估方法缺乏与实际场景的结合，也没有考虑不同偏见造成的伤害程度差异。

Method: 提出HALF（Harm-Aware LLM Fairness）框架，该框架以部署为导向，评估模型在真实应用中的偏见，并根据伤害严重性对结果加权。HALF将九大应用领域分为三层（严重、中等、轻微），并设计了五阶段评测流程，系统性评估大型语言模型的公平性。

Result: 通过对八种LLMs的评测发现：（1）模型在不同领域的公平性表现不一致；（2）更大的模型或更好的性能并不能保证公平性提升；（3）在医疗决策支持领域推理型模型表现更好，但在教育领域表现更差。

Conclusion: HALF框架揭示出现有基准测试取得的成功与实际部署准备之间存在明显差距，强调了在真实场景和差异化伤害下评估模型公平性的必要性。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [64] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 本文提出了一种通过“层修补”方法定位和消除LLM中微调带来的道德偏见，发现该偏见集中于少量关键层，通过替换激活可去除，有助于更高效地解释和处理模型偏见。


<details>
  <summary>Details</summary>
Motivation: 已有研究发现，大型语言模型在微调过程中会吸收人类类似的社会偏见，但这些偏见在模型中的具体表现机制尚不清楚。作者希望探索LLM中具体的道德偏见（如Knobe效应）的生成机制及其定位方法。

Method: 作者提出使用Layer-Patching分析法，对3个开源权重的LLM进行研究。通过将预训练模型的激活替换到微调模型的特定关键层，分析偏见在模型中的定位及其消除方式。

Result: 研究发现，模型的Knobe效应不仅是在微调中习得，而且能够明确定位于特定的一组层；只需将预训练模型的激活补丁应用到少数关键层，即可消除这种道德偏见。

Conclusion: LLMs中的社会偏见可以被解释、定位并通过有针对性的层级干预加以缓解，无需对模型进行整体重新训练。这为偏见消除提供了新的思路和方法。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [65] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 本文提出双阶段自适应锐化（DSAS），有效解决了LLMs在多文档问答中的长距离依赖和“信息迷失在中间”问题，显著提升了主流LLMs的表现，且无需改动模型结构或增加新参数，实用性强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多文档问答（Multi-doc QA）任务中表现不佳，主要面临长距离依赖建模困难和“信息迷失在中间”（lost-in-the-middle）的问题，现有方法要么截断全局依赖，要么需高成本的微调，缺乏通用、简单的解决方案。

Method: 提出了一种双阶段自适应锐化（DSAS）方法，包括两个模块：（1）上下文门控加权（CGW）模块，通过层次化注意力追踪和位置感知加权，缓解“信息迷失在中间”问题；（2）互逆注意力抑制（RAS）模块，通过抑制关键信息与无关信息之间的信息交换，加强对关键段落的关注。这一方法无需修改模型结构或增加训练参数，具备即插即用的优点。

Result: 在四个基准测试上，DSAS在主流LLMs（如Llama、Qwen、Mistral和Deepseek）上均取得了显著效果，Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct在Multi-doc QA任务的平均F1分数提升了4.2%。消融实验也证实了CGW和RAS两个模块的关键贡献。

Conclusion: DSAS是一种无需模型结构修改和额外参数、即插即用且有效提升大型语言模型多文档问答性能的新方法，具备良好的通用性、鲁棒性和可扩展性。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [66] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 本研究提出了MedQA-Followup框架，用于系统评估大模型在医疗领域多轮问答下的鲁棒性。实验显示，当前模型在多轮互动和间接干预下准确率大幅下降，凸显了其在临床部署时的重大风险。多轮鲁棒性应成为未来医疗LLM安全评估的核心关注。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）日益应用于医疗临床领域，但其在现实多轮交互中的可靠性尚不清楚。以往评测方法多以单轮提问为主，未能涵盖医疗咨询中复杂的多轮互动情境，例如信息冲突、误导语境与权威影响等问题。因此，需要新的框架系统评估医疗领域LLM在多轮问答下的鲁棒性。

Method: 作者提出了MedQA-Followup评测框架，系统性评估医疗问答中多轮互动下的大模型鲁棒性。该框架区分了浅层鲁棒性（抵抗误导性初始信息）和深层鲁棒性（在多轮挑战下保持准确性），并引入间接-直接干预轴（间接为语境设置，直接为明确建议）。作者基于MedQA数据集设计了控制性干预，对五种主流LLM进行了评估分析。

Result: 实验发现，大模型在浅层扰动下表现尚可，但在多轮问答（深层鲁棒性）情形下准确率大幅下降。例如Claude Sonnet 4的准确率从91.2%降至最低13.5%。此外，间接、基于语境的干预往往比直接建议导致更大准确率下降，暴露出部署在医疗场景下的显著漏洞。进一步分析还发现模型间存在差异，部分模型在反复干预下表现更差，而部分则有一定恢复能力。

Conclusion: 多轮鲁棒性是医疗大模型安全可靠应用中一个关键但被低估的维度。现有LLM在实际多轮医疗咨询中存在尚待解决的严重脆弱性，需要深入关注和改进。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [67] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: 针对中文的编码器模型Chinese ModernBERT，结合定制词表、动态掩码、扩展上下文和优化训练策略，在各项任务上取得高效能表现，超越现有强劲中文模型，并计划开源以推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有的Encoder-only Transformer在英语上的表现优异，但由于中文的词法和分词方式与英语显著不同，这些改进并未完全转移到中文领域。需要针对中文特殊性对模型架构和训练方式进行优化。

Method: 提出Chinese ModernBERT，从零开始设计中文专用编码器：（1）基于硬件优化的32k BPE词表，结合常见词缀与复合词，降低嵌入预算；（2）采用整体词掩码（WWM）和动态掩码策略（掩码比例随训练进度从30%降至15%）；（3）两阶段预训练，将上下文窗口从1024扩展到8192（用RoPE和交替局部/全局注意力）；（4）阻尼余弦学习率调度以稳定长程优化。预训练使用约1.2万亿中文token，并在CLUE、SimCLUE等数据集上验证。

Result: Chinese ModernBERT在CLUE上达到了与主流中文编码器竞争的性能，在bf16下，既保证了长序列吞吐率，又兼顾了短序列速度。检索性能方面，加入对比数据后在SimCLUE测试集上取得了领先Qwen-0.6B-embedding的表现，Pearson 0.505，Spearman 0.537，展现出良好的扩展潜力。

Conclusion: Chinese ModernBERT通过针对中文语言特点的模型设计和训练优化，在准确率、速度和内存效率等多方面实现新的Pareto改进，并在主流评测和检索任务上取得领先成绩，为中文编码模型的未来扩展提供了清晰路径。Tokenizer和权重将开源，促进可复现研究。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [68] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 本文提出了一套使用大型语言模型自动化语法标注的大规模流程，显著提高了标注效率和准确率（98%+），展示了LLMs在语料库处理上的潜力，同时强调了实施时需关注成本与伦理等问题。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言语料库规模迅速扩增，人工标注成为语料库语言学研究中的主要瓶颈。因此，研究亟需自动化方法以实现大规模语法标注，提高效率并降低人工成本。

Method: 本文提出了一种利用大型语言模型（LLMs）的可扩展、无监督的自动化语法标注流程。该流程包含四个阶段：提示工程、事前评估、自动批量处理和事后验证。通过调用GPT-5，对大规模文本进行高效自动标注。

Result: 在英语“consider”结构历时变异的案例分析中，作者使用该流程在60小时内自动标注143,933条历史美语语料库（COHA）中的句子，在两项复杂标注任务中准确率均超过98%。

Conclusion: 大型语言模型可以在大规模数据准备和语料标注中实现高度自动化和高准确率，极大减少了人工干预。但在实际应用时应注意成本、授权和伦理等问题。

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [69] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出了结合权威知识库和检索增强生成技术的反击言论生成框架，有效提升了反击言论的可信度和质量，已在多个评测中表现优异，为应对仇恨言论等有害内容提供了更专业与可靠的自动化解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的反击言论（counter-speech）生成方法主要依赖大语言模型或专家人工撰写，但在可靠性、一致性和可扩展性方面存在明显不足。为解决这些问题，亟需一种能够结合权威知识库、提升生成内容可信度的新方法。

Method: 提出了一个创新性的知识增强反击言论生成框架，把反击言论的生成过程建模为知识驱动的文本生成。该框架融合了先进的检索增强生成（RAG）技术，通过整合来自联合国数字图书馆、EUR-Lex与欧盟基本权利机构的知识库（共32,792份文档），服务于仇恨言论文献中识别的8大主要被攻击群体。

Result: 通过利用MultiTarget-CONAN数据集进行实证评估，采用JudgeLM标准指标及人工评测，结果显示该框架在生成可信、连贯的反击言论方面，明显优于传统LLM基线以及其他竞争方法。

Conclusion: 该成果为仇恨言论等领域的可信反击言论自动生成提供了高效路径，利用权威知识库支持文本生成，有效提升内容可靠性和多样性，也为后续相关研究奠定了坚实基础。

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [70] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 本研究提出细粒度归因方法，发现脑对齐和下一词预测在依赖的词汇特征上有根本差异，从而加深了我们对LLM与人类大脑语言处理关联的理解，并为认知相关任务提供新工具。


<details>
  <summary>Details</summary>
Motivation: LLM与人类大脑活动的对齐机制尚不清楚，探明二者在语言处理中的关联可揭示语言处理的计算原理，尤其是BA与NWP之间的关系仍有争议。

Method: 提出了一种细粒度的输入归因方法，用于识别与脑-LLM对齐最相关的具体词汇，并利用该方法分析BA与NWP的关系。

Result: NWP依赖于最近出现和最早出现的语法相关词（有顺序偏差），而BA则更关注语义和语篇层次信息，对最近词有更精准的偏向。该发现揭示了LLM与人脑在语言处理特征上的不同依赖方式。

Conclusion: 大脑对LLM的对齐（BA）与下一词预测（NWP）所依赖的单词子集主要不同，两者在关注信息类型和单词位置时存在显著差异。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [71] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: MoBiLE通过灵活分配专家数量和高效切换机制显著提升了MoE模型推理速度，兼顾了模型精度和内存效率，易于集成于现有MoE体系。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在推理过程中由于CPU-GPU带宽有限，导致模型专家的异构存储方案效果受限。预取机制虽然有助于缓解瓶颈，但训练成本高且在细粒度专家分割的近期模型上效果下降。

Method: 提出了MoBiLE（一种即插即用的基于异构专家的MoE推理框架），通过混合大专家与小专家（mixture of big-little experts）的方式：对于不重要的token减少专家数量加速推理，对于重要token则保持专家全量以保障模型质量，同时设计了专用的回退与预取机制提升内存效率。

Result: 在四种主流MoE架构和具挑战性的生成任务上评估，MoBiLE在消费级GPU系统上实现了1.60x-1.72x的加速，且准确率几乎无损。

Conclusion: MoBiLE无需额外训练，极大优化了MoE推理的效率与资源利用，同时在保证精度的前提下提升推理速度，适用于现代MoE模型。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [72] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 本研究通过模拟学术工作流程，揭示了LLMs作为评审存在明显偏见，偏好LLM生成的语言风格并回避批判性内容，可能威胁学术公平；但其作为修稿工具也能提升论文质量，需谨慎部署。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，研究人员开始将其深度集成到学术工作流程中，尤其在研究实践和评审环节带来变革。然而，LLMs在学术流程中的双重角色及其复杂影响风险尚未被充分探索。

Method: 设计了一个模拟实验系统，分别设置了研究代理（生成和修改论文）和评审代理（评估论文）。通过模拟由LLMs担任作者和评审者的学术流程，并结合人工标注分析LLMs与人类评审结果的差异。

Result: 模拟结果显示，LLMs作为评审者时，对LLMs撰写的论文系统性打高分，对人类撰写且包含批判性内容（如风险与公平讨论）的论文则持续低估分数。深入分析确认原因主要为LLMs在语言风格上的偏好及对批判性内容的排斥。

Conclusion: 如果LLMs直接参与学术评审环节，可能造成对人类作者的风险和公平性问题；但LLMs指导下的修稿可提升论文质量，对初级研究者和低质量论文有正面作用。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [73] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本文通过大规模多语言实验发现，大模型分词在拉丁语系与非拉丁语系、复杂语言之间存在显著效率差异，非拉丁语种等面临更高的计算成本和上下文效率损失。研究建议未来应开发体现语言多样性的新分词策略，促进AI系统的公平性。


<details>
  <summary>Details</summary>
Motivation: 当前主流人工智能系统在不同语言之间的分词效率存在显著差异，这些差异可能导致在多语言环境下，大模型对不同语言用户的实际可用性和计算资源利用率不公平。本文旨在系统量化和揭示这些分词层面的结构性不平等问题。

Method: 采用统一的预处理和归一化流程，将超过200种语言的数据用tiktoken库进行统一分词处理，并通过Tokens Per Sentence (TPS) 和 Relative Tokenization Cost (RTC)等标准化指标进行统计分析，同时以英文为基线进行对比。

Result: 拉丁文系语言分词效率更高，而非拉丁文及形态复杂语言出现显著的分词膨胀现象，其RTC值通常是英文的3-5倍。这直接导致低资源语言或非拉丁语种使用者面临更高计算消耗与上下文利用率降低。

Conclusion: 目前大模型分词策略存在结构性语言不公平，亟需发展更能体现语言类型多样性的分词方法和自适应词表构建策略，以实现多语言系统的计算公平性。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [74] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PRoH框架通过动态规划和推理有效解决现有KH-RAG多跳问答局限，实现了显著的性能提升，并展现高效的知识检索与跨实体推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识超图（KH）在RAG框架下多实体关系建模能力有限，主要面临静态检索规划、不可适应性检索执行，以及对结构和语义利用浅薄的问题，这严重影响了复杂多跳问答能力。

Method: 提出了PRoH，一种动态规划与推理知识超图框架。它包含三大创新：（i）上下文感知的规划模块，探索局部超图邻域以指导推理路径生成；（ii）结构化问题分解，将子问题以动态演化的有向无环图（DAG）方式组织，实现自适应、多轨路径探索；（iii）实体加权重叠（EWO）引导的推理路径检索算法，优先选择语义连贯的超边。

Result: PRoH在多个领域实验中表现卓越，平均F1分数较前沿模型HyperGraphRAG提升19.73%，生成评估分数（G-E）提升8.41%。在长距离多跳推理任务中表现出强大鲁棒性。

Conclusion: PRoH显著提升了知识超图驱动的RAG多跳问答性能，有效解决了以往模型的三大核心瓶颈，并为复杂知识检索与生成提供了更强的多轨动态推理能力。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [75] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出CLEAR框架，通过分析和优化LLM内部隐藏状态，精准定位和处理知识冲突，显著提升RAG系统输出的事实性与准确度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统容易在模型输出与检索上下文证据之间出现矛盾，且多依赖外部干预，缺乏对LLM内部知识整合机制和冲突处理过程的系统理解。

Method: 利用隐藏状态探测分析知识整合过程，将上下文拆分为句子级别的信息，定位冲突，并通过冲突感知微调优化模型。

Result: 在三个基准测试中，CLEAR在准确率和语境真实性上显著优于主流方法，尤其在多种知识冲突场景下效果稳定。

Conclusion: CLEAR框架通过识别和优化大模型内部知识冲突，大幅提升了RAG系统的准确性与事实性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [76] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 大语言模型在形态学泛化任务中表现接近人类，但其准确率更多依赖于数据资源丰富度，而非语言结构复杂度。表面上类似于人类能力，实则机制不同。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在新词形态学泛化任务中的表现，分析其准确率是否接近人类水平，并考察模型表现受语言复杂性还是训练数据数量影响更大。

Method: 采用多语言版Wug测试，对六个大语言模型在加泰罗尼亚语、英语、希腊语和西班牙语四种语言上的形态学泛化能力进行评估，并与人类说话者进行对比。

Result: 模型能以接近人类的准确率将形态学规则泛化到未见过的新词，但模型表现与社区规模和数据可用性更相关，而不是与语言结构复杂度有关。西班牙语和英语等资源丰富语言表现更好。

Conclusion: 模型的表现主要由语言资源的丰富程度驱动，而非对语法复杂性的敏感度。因此模型仅表面上表现出类人语言能力，真实机制与人类不同。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [77] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 论文提出SMEC新框架，有效压缩LLM嵌入维度，在保持性能甚至提升的同时缓解计算和存储压力，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入虽然信息丰富，但导致计算复杂度和存储成本剧增，影响实际部署。急需能有效降维同时保持表现的方法。

Method: SMEC框架包括三个关键模块：顺序式Matryoshka表示学习（SMRL）以缓解训练过程中的梯度变化、自适应维度选择（ADS）以减少降维时的信息损失、可选跨批记忆（S-XBM）以增强高低维嵌入的无监督学习。

Result: 在多种数据集上进行实验，SMEC在大幅降维（如压缩到256维）时仍能保持甚至提升性能。例如在BEIR数据集上，SMEC比Matryoshka-Adaptor高1.1分、比Search-Adaptor高2.7分。

Conclusion: 提出的SMEC方法能有效减少大模型嵌入的维度，同时保持甚至提升模型性能。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [78] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 论文首次提出个性化机器生成文本检测问题，构建了相应数据集并展示检测器在此场景下的显著性能下降。提出的新方法可有效预测检测器性能变化，相关性高达85%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成更加流畅且具有个人风格的文本，身份冒充风险加剧，因此急需研究个性化机器生成文本的检测方法。

Method: 构建了个性化文本检测基准数据集，并提出了一种新方法来评估和预测检测器在个性化场景下的表现，即通过识别被反转的特征方向并生成探测数据集验证检测器对这些特征的依赖性。

Result: 实验证明，当前先进检测器在个性化文本场景下有较大性能差异，有些检测器表现大幅下降。所提方法能以85%的相关性准确预测性能变化的方向和幅度。

Conclusion: 作者发现当前文本检测器在检测个性化机器生成文本时性能显著下降，并提出了能预测这种性能变化的方法。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [79] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 该文首次系统性探讨‘测试时扩展’方法在注解分歧领域上的迁移表现，实验证明模型平均与多数投票有效但Best-of-N采样方法效果欠佳，揭示了数学/编码领域方法迁移到主观领域的局限。


<details>
  <summary>Details</summary>
Motivation: 以往‘测试时扩展（test-time scaling）’主要用于数学、编程等有唯一正确答案的领域，尚未在主观性强、标注有分歧的领域广泛应用。论文想探究该技术在此类任务中的可行性与表现。

Method: 将‘测试时扩展’的三种方法（模型平均、少数服从多数投票、Best-of-N采样法）应用于LeWiDi-2025任务，评估这些有主观分歧的数据集上LLM输出的改进情况。并对表现表现不佳的Best-of-N方法进行机理分析。

Result: 模型平均与多数投票能稳定提升LLM在LeWiDi任务中的表现，但Best-of-N采样法未能迁移成功。

Conclusion: 在含有标注分歧的主观性任务中，并非所有测试时扩展方法都能带来提升。Best-of-N方法在这些任务上表现不理想，需进一步分析与改进。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [80] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: 视觉语言模型在处理不一致的文本和图片输入时，表现受到明显影响；作者通过VISaGE数据集实验，发现一致性假设对于模型理解能力影响更大。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在训练时学习到了广义的知识表征，但实际评估时多针对单个实例，尤其是遇到不典型的实例时，模型面临输入间一致性与类别一般性之间的权衡。

Method: 作者构建了一个新的评测数据集VISaGE，包含典型与异常的图片，通过精心设计的平衡实验来分析视觉语言模型在不同情境下的表现。

Result: 实证表明，当文本与图片不一致（违背模型假设），视觉语言模型的概念理解降幅显著，甚至超过类别一般性语义先验所造成的影响。

Conclusion: 当视觉语言模型遇到不一致的输入（文本与图片不相关）时，模型的概念理解能力会下降；这种下降比模型基于类别的普遍语义倾向还要明显。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [81] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: 提出了一种简单的微调方法FUT，使大模型能更真实地表达答案不确定性，降低信任差距而不影响准确性，具有良好的泛化性和实用性。


<details>
  <summary>Details</summary>
Motivation: 大模型在表达不确定性时常存在“信任差距”：同一问题多次提问会得到不同答案，但回答往往用语模糊或未准确反映这种不一致，导致输出内容对知识不确定性的表达不真实。

Method: 提出了一种称为Faithful Uncertainty Tuning（FUT）的微调方法，通过对模型输出样本添加与一致性相关的不确定性提示词（如“可能”、“很可能”等），训练模型更诚实地表达其不确定性，无需外部监督数据。

Result: FUT大幅减少模型在表达不确定性时的信任差距，同时保持问答准确性，并且语义分布几乎未发生偏移。方法对不同推理策略、提示词类型和不确定性表达（包括数值类）均表现出鲁棒性。

Conclusion: FUT为大模型表达不确定性提供了一个简单且有效的解决方案，提升了其表达内容的可信度和透明度。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [82] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 本文提出StyleDecipher方法，通过量化风格差异，实现了无需模型内部访问的高精度、可解释、跨领域的机器文本检测，在各种文本类型和敌对环境下效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）广泛应用于开放领域写作，检测机器生成文本成为确保内容真实性和可信度的重要任务。目前方法受限于泛化能力弱、易受篡改、缺乏可解释性等问题，尤其在风格多样或人机混合创作场景下表现不佳。

Method: 提出了StyleDecipher检测框架，通过结合离散风格指标和连续的语义嵌入风格表征，共同建模文本风格差异，将人类与LLM产出的风格特征映射到统一空间内，实现领域无关、无需模型内部信息或标注段落的鲁棒解释型检测。

Result: 在新闻、代码、论文等五个不同领域的大量实验表明，StyleDecipher在域内检测达最优精度，跨域场景下比现有方法最高提升36.30%，同时在面对对抗扰动和人机混合文本时依然保持鲁棒性。定性和定量分析进一步证实风格信号可作为区分依据。

Conclusion: StyleDecipher用风格差异实现了鲁棒、可解释、领域无关的机器文本检测，大幅超越已有方法，并提供了内容真实性的风格证据。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [83] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: 该论文推出大规模学术平行数据集ACADATA，并用其显著提升了大模型在学术翻译与长文本翻译的表现，数据与模型均开放，有力推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 当前学术翻译领域缺乏高质量、多语言的平行数据集，导致相关机器翻译系统和大语言模型在学术文献翻译方面表现不佳。论文作者旨在填补该领域空白，推动学术和长文本翻译技术进步。

Method: 作者构建了ACADATA，一个涵盖96种语言方向的高质量学术平行数据集，包括1.5百万段落对，用于训练（ACAD-TRAIN）与接近6000个翻译对的评估集（ACAD-BENCH）。利用ACAD-TRAIN对大语言模型（LLMs）进行微调，并在ACAD-BENCH上与专用机器翻译系统、通用开源LLMs以及主流商用模型进行对比评测。

Result: 通过ACAD-TRAIN微调后，7B和2B模型在学术语域翻译评价上平均提升了6.1和12.4个d-BLEU分。同时，在通用长文本翻译任务上，非英文翻译最多提升24.9%，表现超越了现有最佳的商用及开源模型。

Conclusion: ACADATA数据集及微调模型为学术领域和长文本翻译研究提供了关键资源，实现了大幅度的质量提升。作者公开数据和模型，助力社区进一步研究。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [84] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: COSTAR-A 对小型本地优化 LLM 有时能提升指令性和输出结构，对资源受限环境有意义，但效能因模型和任务而异。


<details>
  <summary>Details</summary>
Motivation: 常规提示方法对大模型有效，但本地小模型指令性和一致性不足，需开发更适合本地小模型的提示工程技术。COSTAR 框架已提升大模型表现，但在小模型上的一致性不足。因此提出 COSTAR-A 框架。

Method: 通过对参数最多为8B的小型、微调模型进行一系列受控的提示输出评估，比较了 COSTAR 与 COSTAR-A 框架带来的输出效果差异。

Result: COSTAR-A 框架提升了部分小型本地 LLM 的输出结构和果断性，且在 Llama 3.1-8B 上表现突出，但其效果随模型和任务不同而变化。

Conclusion: COSTAR-A 作为一种新的提示工程框架，能提升小型、本地优化的LLM在某些任务上的输出结构和决定性，尤其适合在资源受限硬件的高效 AI 部署。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [85] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 作者发现对于模式化推理任务，推理表现主要源自模型学习到的推理模式，而非人类标注轨迹的数量和质量。提出了PARO框架，用LLM自动标注推理轨迹，并用有限人工监督推理模式，从而极大降低了标注开销，效果与大规模人工标注相当。


<details>
  <summary>Details</summary>
Motivation: 目前主流的SFT+RLVR范式，虽然能显著提升大语言模型的推理能力，但SFT阶段需要大量高质量的人类标注推理轨迹（rationales），导致成本高昂。作者旨在寻找能大幅降低这种推理标注成本的方法。

Method: 作者提出分析模式化推理任务（patterned reasoning tasks）, 发现推理过程本质是固定的、可泛化的推理模式。然后提出PARO（Pattern-Aware LLMs as Rationale AnnOtators）框架，让LLM无须人工标注，依据预设推理模式自动生成推理过程。

Result: 实验显示，用PARO方法自动生成的推理轨迹对模型进行SFT+RLVR微调后，其推理效果可媲美规模大10倍的人类标注推理轨迹。

Conclusion: 人类大规模的推理标注完全可以用基于LLM、只需少量人工监督的自动化推理轨迹生成所取代，大幅降低应用成本。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [86] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 本文提出了生成空间规模（GSS）和GSSBench任务集，利用模型内部指标有效衡量输出多样性，对提升大模型创意与事实任务的表现具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在开放式生成任务中输出多样性的需求各异，但表现出输出同质化或虚假多样性的问题。作者希望统一分析并解决这两类失败模式。

Method: 提出了“生成空间规模（GSS）”的概念，开发了GSSBench任务集，用于评测各种指标与大模型输出与期望行为的差异，并用内部指标如EigenScore进行有效性分析。

Result: 发现传统的多样性和不确定性度量方法表现不佳，而EigenScore等基于模型内部的度量能更好检测并解释模型输出行为。展示了GSS在三方面（歧义检测、推理解释、空间扩展）应用效果。

Conclusion: 引入GSS和GSSBench有效统一并监测生成任务中的输出多样性问题，配合模型内部指标可推动更合理、多样且高质量的生成。为大模型输出的定量分析提供了新思路。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [87] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 该工作系统性分析和提升了OLM模型在多模态细粒度信息感知上的能力，提出高细节低幻觉的自动数据生成与填空式评测方法，并训练了领先的细粒度描述模型，取得了多项新纪录。


<details>
  <summary>Details</summary>
Motivation: 现有的全模态语言模型（OLM）在多模态信息细粒度感知方面仍存在描述细节和幻觉共生的问题，限制了其在人机交互中的能力提升。

Method: 提出了Omni-Detective代理式数据生成流水线，结合工具调用，自动生成高细节并且低幻觉的多模态数据。据此训练了两个模型：Audio-Captioner（用于音频细粒度感知）和Omni-Captioner（音视听细粒度感知）。同时，构建了Omni-Cloze新型填空式评测方法来补足细粒度感知评测的空白。

Result: Audio-Captioner在MMAU与MMAR评测上超越了所有开源模型，包括Gemini 2.5 Flash，并接近Gemini 2.5 Pro的表现。Omni-Captioner在VDC上刷新了最佳纪录，并在video-SALMONN 2测试集上实现了细节与幻觉的最佳平衡。Omni-Cloze评测方法表现出高效、稳定和可靠的特性。

Conclusion: 本文提出的Omni-Detective流水线和Omni-Cloze评测体系，有效提升了OLM多模态细粒度感知的数据生成与评测能力，相关模型在现有任务和新基准上均实现了优异表现。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [88] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 本文扩展了人工语言的研究框架，采用更强表达力的语法生成形式，并着重考察模型在未见长句上的泛化能力。结果表明，语言模型在类型学上常见的词序结构上更容易泛化，说明其归纳偏置倾向于自然语言的常态。


<details>
  <summary>Details</summary>
Motivation: 前人在研究语言模型是否对常见语法属性有归纳偏置时，主要采用了人工语言，并且有限于上下文无关语言的形式化。本文希望扩展此研究，涵盖更丰富、更自然的语言结构，实现更贴近真实语言的归纳偏置分析。

Method: 采用Generalized Categorial Grammar (GCG) 构造人工语言，使其能够表示现实语言中的更多句法结构（如无界依存和温和的上下文相关结构），并设计实验评估语言模型在处理未见加长测试句上的泛化能力。

Result: 实验结果显示，当人工语言的词序更符合语言类型学上的常态（更自然、更常见），语言模型更易于有效泛化处理这些结构。

Conclusion: 语言模型在更自然、类型学上合理的词序条件下表现出更强的归纳泛化能力，这表明其归纳偏置更倾向于常见、合理的语法属性。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [89] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 提出DGRC方法系统分析对话自然性，发现语言模型更偏向延续核心话语内容，instruct-tuning增强了该效应，并能对提示信息敏感变化。


<details>
  <summary>Details</summary>
Motivation: 当前对语言模型生成对话自然性的评估标准尚不统一，且缺乏可扩展的量化指标。

Method: 提出了一种新的评估方法DGRC（Divide, Generate, Recombine, and Compare），核心流程为：对对话内容拆分作为提示、针对子部分用语言模型生成续写、重组对话与续写片段，最后对比重组后不同序列的概率。

Result: 通过DGRC发现，语言模型更倾向于延续“at-issue”内容，经过instruct-tuning训练的模型对这一倾向更显著；在出现诸如“Hey, wait a minute”提示时，该偏好有所减弱，但调优并未进一步增强这一变化特性。

Conclusion: DGRC方法能够系统性地检验语言模型在对话中的话语灵敏度，并揭示其对对话自然性要素（如at-issue）的建模能力，这也反映了良好对话动态的特征。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [90] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 本文批判了主流生成语言理论对LLMs的质疑，提出用Mańczak的语言经验主义和频率原则作为评价和设计LLMs的新视角，强调实际语言使用频率比深层结构更关键，为语言模型的发展提供了新的理论支持。


<details>
  <summary>Details</summary>
Motivation: 目前针对大型语言模型（LLMs）的语言学评论多基于de Saussure和Chomsky的理论，多为推测性且难以产生实际价值。批评者质疑LLMs是否能真正建模语言，强调需要“深层结构”或“语义基础”才能达到理想的语言“能力”。本文试图寻找新的视角。

Method: 采用Witold Mańczak的经验主义原则，将语言定义为所有言说与书写的总和，并强调语言元素使用频率是最关键的规律。作者用此框架重新审视LLMs相关争议，并给出设计、评价和解释语言模型的建设性指南。

Result: 借助Mańczak频率论的框架，作者能够对既有关于LLMs的批评进行有力回应，并提出更适用的理论基础，从而为语言模型的设计和应用提供了新的评判标准。

Conclusion: 以频率为中心的经验主义视角，比传统的结构主义或生成语法视角更适合分析和发展LLMs，有助于克服以往批评中对“深层结构”和“能力”的执念，实现语言模型新的理论突破。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [91] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM 是一个可用于现有大语言模型的“智能路由”扩展，无需大改模型结构或再训练，仅靠训练轻量路由器就能大幅提升推理效率且提升甚至保持准确率，在多个问答/逻辑/数学任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在每次推理时都要将每个 token 经过所有 transformer 层，无论任务复杂度如何，导致简单任务计算浪费，而复杂任务则可能计算不够灵活。现有自适应深度方法效率虽提升，但需高昂的推理耗时、结构改变或大规模再训练，且可能损失准确率。

Method: 提出 Dr.LLM 框架，在现有预训练模型中为每一层添加轻量路由器，按需跳过、执行或重复特定网络层。通过蒙特卡洛树搜索（MCTS）获得高质量层配置，并用显式监督训练路由器，设计包含窗口池化、带有类别均衡的 focal loss 及瓶颈 MLP 路由器以提升鲁棒性。

Result: 在 ARC（逻辑）与 DART（数学）任务上，Dr.LLM 在平均每个样本节省 5 层计算的同时，将准确率提升最高 3.4%。路由器泛化能力好，跨多个任务仅损失 0.85% 准确率，并在效率和准确率上明显优于其他路由方法（提升最高 7.7%）。

Conclusion: Dr.LLM 能无需改变原模型参数，通过显式监督路由器，实现大模型在推理时更精细的计算分配，在兼顾效率的同时提升推理准确率。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [92] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 本文针对Bambara语等口语主导的低资源语言进行田野实验，发现人工高质量语音转写成本极高，为1小时语音需约30-36小时人工，结果为同类语言资源开发提供基准和实践指引。


<details>
  <summary>Details</summary>
Motivation: 低资源语言语音数据缺乏，且人工转写的成本尚未被充分理解，该研究旨在量化并揭示转录过程的复杂性和人力成本。

Method: 通过为期一个月的田野研究，组织十位母语为Bambara的转写员，对ASR生成的53小时语音进行人工校正和转录，统计转写所需时间。

Result: 实验室条件下转录1小时语音平均需30小时人工劳动，田野条件下为36小时。为类似语言开发NLP资源提供了参考基准和实际经验。

Conclusion: 为低资源语言，如Bambara，创建高质量语音数据需要大量人工劳动时间。

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [93] [Bringing Algebraic Hierarchical Decompositions to Concatenative Functional Languages](https://arxiv.org/abs/2510.12481)
*Attila Egri-Nagy*

Main category: cs.FL

TL;DR: 本文将有限状态自动机的代数分解理论推广到编程语言设计领域，初步在级联型函数式编程语言中实现了显式的代数结构，推动了理论成果的实际应用。


<details>
  <summary>Details</summary>
Motivation: 编程语言不断吸收理论计算机科学的概念，但与纯数学之间仍存在差距。许多理论成果在实际中的应用尚未实现。Krohn-Rhodes理论（有限状态自动机的代数分解）为理解和控制计算过程提供了新方法，但迄今应用仅限于理论领域。作者对此现状表示关注，希望推动理论结果真正应用于编程语言。

Method: 作者将代数分解推广到范畴层面（从半群到半群oid），并在特殊类别的级联型函数式编程语言中进行了探索。通过应用半群oid分解理论，尝试在这些编程语言中实现明确的代数结构表示。

Result: 初步设计出一类具有显式半群oid表示的编程语言家族，展示了理论代数分解在实际编程语言构建中的应用可能性。

Conclusion: 论文探讨了如何将理论计算机科学中的代数分解方法引入编程语言设计中，并通过对级联型函数式语言的研究，初步实现了代数分解的实际应用，为理论成果落地提供了新方向。

Abstract: Programming languages tend to evolve over time to use more and more concepts
from theoretical computer science. Still, there is a gap between programming
and pure mathematics. Not all theoretical results have realized their promising
applications. The algebraic decomposition of finite state automata
(Krohn-Rhodes Theory) constructs an emulating hierarchical structure from
simpler components for any computing device. These decompositions provide ways
to understand and control computational processes, but so far the applications
were limited to theoretical investigations. Here, we study how to apply
algebraic decompositions to programming languages. We use recent results on
generalizing the algebraic theory to the categorical level (from semigroups to
semigroupoids) and work with the special class of concatenative functional
programming languages. As a first application of semigroupoid decompositions,
we start to design a family of programming languages with an explicit
semigroupoid representation.

</details>
