<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dual-Numbers Reverse AD for Functional Array Languages](https://arxiv.org/abs/2507.12640)
*Tom Smeding,Mikołaj Konarski,Simon Peyton Jones,Andrew Fitzgibbon*

Main category: cs.PL

TL;DR: 针对反向自动微分的多维数组支持问题，本文提出了新的代码变换和算法提升方法，通过消除高阶特性，实现了高效的“dual arrays”自动微分，显著优化了性能和实用性。


<details>
  <summary>Details</summary>
Motivation: 标准的双数（dual-numbers）在正向自动微分（AD）中表现优秀，简单易用；但应用于反向AD、特别是针对数组程序时，性能却不尽如人意，激发了改进的需求。

Method: 提出了一种在反向自动微分中为多维数组提供优质支持的新算法。其核心由三部分组成：（1）保持语义不变的向量化代码变换（bulk-operation transform, BOT）；（2）将基础双数反向AD算法提升至大致一阶数组语言；（3）通过符号解释达成端到端的编译流程。

Result: 实现了在反向AD中支持多维数组，且性能基本无开销。虽然牺牲了一部分双数AD对高阶代码的普适性，只支持一小部分选定的高阶数组组合器（如build、gather、scatter），但输入程序的高阶特性可通过BOT消除，使AD只需处理一阶数组程序，实现了高效的“dual arrays”扩展。

Conclusion: 本文提出的算法允许双数反向AD在多维数组上无性能损失地工作，适用范围虽略受限，但极大提升了数组类程序的实际表现。

Abstract: The standard dual-numbers construction works well for forward-mode automatic
differentiation (AD) and is attractive due to its simplicity; recently, it also
has been adapted to reverse-mode AD, but practical performance, especially on
array programs, leaves a lot to be desired. In this paper we introduce
first-class support for multidimensional arrays in dual-numbers reverse-mode AD
with little to no performance overhead. The algorithm consists of three
loosely-coupled components: a semantics-preserving vectorisation code
transformation (the bulk-operation transform or BOT), a fairly straightforward
lifting of the basic dual-numbers reverse AD algorithm to a mostly first-order
array language, and symbolic interpretation to achieve an end-to-end
compilation pipeline. Unfortunately, we lose some of the nice generalisable
aspects of dual-numbers AD in the process, most importantly support for
higher-order code.
  We do support some higher-order array combinators, but only a
carefully-chosen set: 'build' (elementwise array construction), 'gather' and
'scatter'. In return, the BOT can eliminate the essential (for AD)
higher-orderness of the input program, meaning that AD gets essentially
presented with a first-order program. This allows the naive trick of lifting
dual numbers to "dual arrays" to work without much modification.

</details>


### [2] [Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications](https://arxiv.org/abs/2507.13091)
*Aurèle Barrière,Victor Deng,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 本文提出首个对ECMAScript正则表达式进行机制化且经过忠实性证明的完整语义，支持捕捉所有可能匹配的回溯树，并在正则优化和引擎验证上取得实际效果。


<details>
  <summary>Details</summary>
Motivation: 现代正则表达式的语义定义常常不完整或缺乏形式化证明，特别是在回溯语义和真实语言支持（如ECMAScript的JavaScript正则表达式）方面。缺乏机制化且经验证为忠实于官方规范的理论模型，也阻碍了相关工具和算法的形式化分析。

Method: 作者开发了首个针对现代正则表达式语言（具有回溯语义）且经过机制化、精炼、实用、完整并经正式证明忠实的语义。他们通过与ECMAScript规范的逐行嵌入进行等价性证明，确保语义的忠实性。并应用于两个实际案例，包括上下文等价性的新定义，以及对PikeVM算法的形式化证明。整个工作基于Rocq证明助手实现了全部定义与结果的机制化证明。

Result: 该工作提供了一个完整机制化且与官方规范等价的现代正则表达式语义模型，首次能够捕捉全回溯树而非仅最优匹配，并将其成功应用于正则表达式优化与发动机算法的形式化验证。

Conclusion: 本工作为现代正则表达式（含回溯机制）提供了强有力的形式化理论基础，可支撑进一步的分析、优化和引擎实现，对语法形式化和实际应用都有重要意义。并填补了相关规范与实用工具之间的机制化验证空白。

Abstract: We present the first mechanized, succinct, practical, complete, and
proven-faithful semantics for a modern regular expression language with
backtracking semantics. We ensure its faithfulness by proving it equivalent to
a preexisting line-by-line embedding of the official ECMAScript specification
of JavaScript regular expressions. We demonstrate its practicality by
presenting two real-world applications. First, a new notion of contextual
equivalence for modern regular expressions, which we use to prove or disprove
rewrites drawn from previous work. Second, the first formal proof of the PikeVM
algorithm used in many real-world engines. In contrast with the specification
and other formalization work, our semantics captures not only the top-priority
match, but a full backtracking tree recording all possible matches and their
respective priority. All our definitions and results have been mechanized in
the Rocq proof assistant.

</details>


### [3] [Towards Formal Verification of LLM-Generated Code from Natural Language Prompts](https://arxiv.org/abs/2507.13290)
*Aaron Councilman,David Fu,Aryan Gupta,Chengxiao Wang,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.PL

TL;DR: 本文提出并实现了一种结合形式化查询语言与符号验证器的系统，能有效验证LLM生成的Ansible代码是否符合用户意图，并在基准测试中表现良好，有助于普及安全、可信的AI代码助手。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽然能根据自然语言描述生成代码，但常会生成错误代码，用户难以察觉并修正。为了提升AI代码助手的可用性和对非程序员更友好的自然语言编程体验，亟需对LLM生成代码提供形式化正确性保障。

Method: 提出并实现了一个结合形式化查询语言与代码验证的系统（Astrogator）：通过让用户以接近自然语言但有形式定义的查询表达意图，并借助符号解释器对LLM生成的Ansible代码做行为验证，确保生成代码与用户意图一致。

Result: 在21个代码生成任务的基准测试中，该验证工具能在83%的情况下验证正确代码，并能在92%的情况下识别出错误代码。

Conclusion: 结合形式化查询语言和自动验证过程，能显著提升LLM生成代码的正确性保障，提高用户对AI编程助手的信任度和生产力。

Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by
taking natural language descriptions and generating code based on it. However,
LLMs often generate incorrect code that users need to fix and the literature
suggests users often struggle to detect these errors. In this work we seek to
offer formal guarantees of correctness to LLM generated code; such guarantees
could improve the experience of using AI Code Assistants and potentially enable
natural language programming for users with little or no programming knowledge.
To address this challenge we propose to incorporate a formal query language
that can represent a user's intent in a formally defined but natural
language-like manner that a user can confirm matches their intent. Then, using
such a query we propose to verify LLM generated code to ensure it matches the
user's intent. We implement these ideas in our system, Astrogator, for the
Ansible programming language which includes such a formal query language, a
calculus for representing the behavior of Ansible programs, and a symbolic
interpreter which is used for the verification. On a benchmark suite of 21
code-generation tasks, our verifier is able to verify correct code in 83% of
cases and identify incorrect code in 92%.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 系统性综述LLMs在AIOps领域的研究进展，分析数据源、任务演进、方法应用和评估，将现状、趋势、空白和未来方向一一给出。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）日益成熟并广泛应用，其在IT运维智能（AIOps）领域的作用受到越来越多关注。然而，目前LLMs对AIOps的影响、潜力和局限性尚未被系统梳理。为弥补这一研究空白，作者决定进行全面综述。

Method: 本研究对2020年1月至2024年12月发表的183篇与LLMs应用于AIOps相关的论文进行了系统性调研和分析。围绕四大研究问题（数据源、AIOps任务演进、LLM方法、评估方式）进行深入探讨。

Result: 作者分析了AIOps中LLMs用于处理多样化失败数据源的方法，探讨了AIOps任务的演化与新型任务产生，归纳了LLMs应对AIOps挑战的多种策略，并梳理了当前AIOps-LLMs结合方法的评估手段。最后，指出了最新进展、研究趋势、存在的空白和未来值得探索的方向。

Conclusion: 本文系统总结了LLMs在AIOps领域中的应用现状与挑战，明确了现有研究的不足之处，并为未来研究提供了方向建议。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [5] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本论文提出利用大型语言模型（LLMs）自动完成不同量子开发工具包之间的代码转换，避免人工编写复杂规则，有效提升了量子软件的跨平台移植性和开发效率。


<details>
  <summary>Details</summary>
Motivation: 目前有多种为不同量子计算平台定制的软件开发工具包（Quantum SDK/QSDK），如Qiskit、Cirq和PennyLane。这种多样性导致混合量子-经典软件系统的跨平台开发和互操作性面临重大挑战。传统的基于规则的代码翻译（transpiler）方式设计和维护成本高、需要高深专业知识、映射刚性，难以适应快速变化的需求。

Method: 本研究将大型语言模型（LLMs）作为一种灵活、自动化的解决方案，利用其预训练知识和上下文推理能力，把LLMs用于作为编程语言无关的代码翻译器，可以在不同QSDK之间自动转换量子程序，同时保持功能等价。

Result: 我们的方案无需手工定义转换规则，提供了一种可扩展的量子软件移植性解决方案。结果显示，该方法有效推动了智能化、通用化的量子编程代码自动转换。

Conclusion: 利用大型语言模型作为量子SDK间代码转换器，不但提升了量子编程生态系统的互操作性，也能大幅降低开发和维护成本，为未来灵活、智能的量子计算软件开发奠定基础。

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [6] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: Kodezi Chronos打破上下文窗口限制，通过多层记忆与检索创新，提升了大规模代码库的理解与维护能力，新基准下性能领先，能自动支持实际的软件维护场景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在代码生成和软件自动化领域虽取得进展，但受限于推理时上下文窗口和缺乏显式代码结构推理，无法有效处理超大规模代码库的理解与维护任务。

Method: 提出了一种名为Kodezi Chronos的新型架构，使用多层嵌入记忆引擎，将向量和图索引与持续的代码感知检索相结合，以支持无固定窗口限制的大规模代码理解、重构与自我修复。评估中引入了新颖的Multi Random Retrieval基准，要求模型在代码工件间解决远距离、复杂、模糊的关联。

Result: Kodezi Chronos在现实漏洞检测方面优于现有LLM和代码模型，提升23%，调试周期减少最多40%。

Conclusion: 该架构能够在IDE和CI/CD中无缝接入，实现自动化软件维护，提高代码可靠性与开发效率，推动自我维持、持续优化的软件生态系统发展。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [7] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文首次系统梳理RL在软件工程中的应用现状，总结了研究热点、方法及面临的挑战，为后续研究与实践提供路线图，并公开相关数据与工具。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习（DRL）的推进，将强化学习（RL）与大语言模型（LLM）相结合成为趋势，软件工程领域对自动化和智能化的需求不断上升，激发研究者将RL应用于各类软件工程任务，但缺乏系统性综述。

Method: 回顾并系统梳理2015年DRL问世以来22个顶级SE会议/期刊的115篇同行评议论文，分析发表趋势、SE应用主题与RL算法分类、数据集与模型、优化和评估做法，并提出挑战与未来研究方向。

Result: 论文首次系统映射RL在软件工程领域的应用，分析了现有研究状态、分类、面临的挑战并指明未来发展方向，成果已在GitHub开源。

Conclusion: 本综述填补了RL应用于SE领域系统综述的空白，为研究者和从业者了解现状与发展趋势提供了参考。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [8] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: RAGSum方法通过将代码片段检索和注释生成一体化，大大提升了多语言自动注释的准确性和质量，适合后续自动化开发和研究应用。


<details>
  <summary>Details</summary>
Motivation: 现有的代码注释自动生成方法常将代码检索和注释生成分开优化，导致无关代码片段被引入，降低自动生成注释的准确性和实用性。作者希望提升检索与生成的耦合效果，提高注释自动化的效率和质量。

Method: 提出了一种融合检索和生成的新方法RAGSum，采用单一的CodeT5模型骨干实现检索和生成相结合。首先使用对比式预训练优化代码嵌入用于最近邻检索，然后以优化检索准确率和注释生成的复合损失进行端到端训练，并引入轻量级自我优化环节进一步提升生成质量。

Result: 在Java、Python、C三种主流编程语言的基准测试上，RAGSum在BLEU、METEOR和ROUGE-L等评价指标上均显著优于三种权威基线方法。

Conclusion: 将代码片段检索与注释生成紧密耦合能够有效提升自动注释的表现，未来可以继续在开发者实际应用和定性研究方面深入探索。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [9] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文利用CodeBERT和CodeT5模型，自动为检测到的架构异味推荐重构方法。CodeT5准确率96.9%，显著优于其他方法。该研究推动了异味检测到修复建议的自动化，且全部资源开源。


<details>
  <summary>Details</summary>
Motivation: 现有的架构异味检测工具虽然能发现如God Class、循环依赖、Hub-like依赖等架构异味，但很少提供如何修复这些异味的建议。为提升软件质量和可维护性，需要自动化地提出纠正建议。

Method: 本文将推荐合适重构方法的问题建模为三分类任务，利用CodeBERT和CodeT5两种预训练transformer模型，基于11,149个开源Java项目中挖掘的200多万个重构实例进行微调训练。

Result: CodeT5模型在样本上的准确率达96.9%，F1值为95.2%，优于CodeBERT与传统基线方法。表明transformer模型在从检测异味到提出修复建议方面极为有效。

Conclusion: transformer类模型可以有效地在架构异味检测和可操作修复建议之间建立桥梁，为未来自动重构推荐系统奠定基础。相关所有代码、模型和数据均开源，促进可重复性和进一步研究。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [10] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 论文提出采用GRPO和ORPO两种RL算法微调大模型，有效提升Qiskit代码生成准确率，优于通用大模型，但高难度任务表现仍待提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（如Granite-20B-Code和StarCoder）生成的Qiskit量子编程代码存在大量错误。为了实现更为可靠的量子电路编程代码生成，需要提升模型在该领域的表现和容错能力。

Method: 该论文对一个32B大模型进行了微调，采用了两种强化学习方法：Group Relative Policy Optimization（GRPO）和Odds-Ratio Preference Optimization（ORPO），并利用丰富注释的合成数据集进行训练。

Result: 在Qiskit HumanEval基准测试上，ORPO方法获得了56.29%的Pass@1分数（比Granite-8B-QK提升约10个百分点），GRPO方法达到49%，两者均优于通用模型基线；在人类原始HumanEval上，两者得分分别为65.90%和63.00%。GRPO在基础任务表现突出（42/54），ORPO在中级任务更佳（41/68），但两者都未能解决五个高级任务，说明仍有进步空间。

Conclusion: 通过强化学习优化方法微调大模型，能够显著提升AI生成量子程序代码的正确性，但面对高难度任务，当前AI尚未达到完全可靠水平。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [11] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 智能电网中新开发数据模型缺乏系统化评估流程。本文提出结合显式与隐式的三阶段评估方法，提升模型设计质量，并通过实际案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前，在智能电网领域，针对新开发的信息与数据模型缺乏明确、系统化的评估流程。已有的显式评估方法过于宽泛，而隐式方法多将模型视为既定且只关注系统功能测试，两者均难以在设计阶段发现问题。因此，亟需一套适用于设计阶段、明确且结合两类评估的流程。

Method: 采用设计科学研究方法（Design Science Research），设计了三阶段的评估流程，将显式与隐式评估方法结合，用于新开发的信息与数据模型。以工业柔性描述为例，通过实际模型开发对方法进行验证和完善。

Result: 提出并验证了一套针对智能电网中新开发信息与数据模型的三阶段评估方法，通过实例应用展示了其有效性。此外，总结了实施过程中的经验教训。

Conclusion: 本文提出了一种结合显式与隐式评估方法的三阶段评估流程，有助于在智能电网中新开发信息与数据模型设计时进行有效评估。经过实际应用验证，该方法可以提前发现模型缺陷，提升模型质量。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [12] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 本文提出用分层Mamdan模糊系统取代传统量表评价项目成功，更精准反映用户持续获益，适应复杂应用场景，后续将开展实证验证。


<details>
  <summary>Details</summary>
Motivation: 传统李克特量表评价法往往忽视项目成功的环境依赖性与多元性，难以全面、客观地反映实际成果，因此引入能够处理模糊和复杂性的逻辑手段。

Method: 构建了分层Type-1 Mamdani模糊系统，将模糊推理融入项目成功评价流程，对比传统李克特量表法。

Result: 方法能更侧重用户持续正向影响，减少对次要结果（如干系人满意度和内部成功）的权重，表现出更高的适应性与合理性。

Conclusion: 提出了一种将模糊逻辑融入项目成功评价的新方法，认为能更动态、准确地评估项目成功，未来将通过实证研究进一步验证和应用该方法。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [13] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 提出用单一对话贯穿项目全流程的方法（SCM），让开发者主导，提升结构化和主动性，优化了未来基于LLM的软件开发。


<details>
  <summary>Details</summary>
Motivation: 目前在使用大型语言模型（LLMs）进行软件开发时，普遍存在依赖模型生成的随意性和被动性，缺乏结构化和有效的开发管理方法。本文提出SCM方法以解决这些问题。

Method: 提出Single Conversation Methodology（SCM），即贯穿整个项目生命周期的单一、持久对话开发方式，涵盖需求、架构、实现等全过程，并结合认知清晰性、可追溯性、模块化、文档等原则。详细定义各阶段和最佳实践，并阐述其背后的哲学立场。

Result: SCM方法通过一套结构化的开发流程和理念，能够提升开发过程的主动性和项目管理的系统性，强调开发者主导LLM工具，纠正了当前在软件开发中对LLM依赖的被动态度。

Conclusion: SCM为基于LLM的软件开发方式提供了系统性的框架，重申了开发者作为架构师和监督者的角色，提升了开发过程的效率与可控性，对现有实践形成正面修正。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [14] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 小语言模型可高效检测手工测试用例中的测试异味，并自动给出解释和改进建议，提升测试质量且易于应用。


<details>
  <summary>Details</summary>
Motivation: 手工测试在发现自动化难以捕捉的问题方面仍不可或缺，但这类测试用例易出现如歧义、冗余、检查缺失等测试异味，影响可靠性与可维护性。现有检测工具需手动定义规则，扩展性不足。

Method: 评估三种小语言模型（SLM）——Gemma3、Llama3.2和Phi-4，在143个真实Ubuntu测试用例中自动检测七类测试异味的表现。

Result: Phi-4在检测带有测试异味的句子时pass@2指标达97%，Gemma3和Llama3.2约为91%。SLM还能自主解释问题并给出改进建议，无需详细的提示或手动规则。

Conclusion: SLM可以低成本、概念驱动地识别多种测试异味，无需大量规则定义和复杂语法分析，有助于保护数据隐私并提升实际场景下测试质量。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [15] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出知识驱动的多智能体需求开发框架iReDev，能高效支持需求开发并优于现有方法，增强人机协同，拓展智能需求开发新方向。


<details>
  <summary>Details</summary>
Motivation: 需求开发是软件开发中至关重要的阶段，但其过程繁琐，需耗费大量时间和人力。目前多智能体系统虽受关注，但对需求开发的支持有限，尤其在于未能有效注入人类知识及实现人-智能体协同。

Method: 提出了一个知识驱动的多智能体需求开发框架iReDev。该框架由六个知识驱动智能体组成，涵盖整个需求开发流程。采用基于artifact池（人工物库）的事件驱动通信机制及人类参与机制，实现高效的人-智能体协作与知识注入。

Result: iReDev生成的工件经过评估，在多个方面优于现有基线方法，验证了其有效性。

Conclusion: iReDev能够更高效、智能地支持需求开发，促进人工与智能体的协同工作，推动智能需求开发的发展。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [16] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 本论文指出大规模预训练模型给需求工程带来新挑战，提出了新的理论框架和研究方向，推动需求工程方法的创新和发展。


<details>
  <summary>Details</summary>
Motivation: 随着大规模预训练模型成为现代软件的核心组件，它们表现出与传统软件截然不同的特性，如能力边界模糊、行为受上下文影响、持续演化等，这些现象冲击了需求工程的传统假设。

Method: 提出了一个面向预训练模型驱动软件系统的需求工程概念框架，并指出了多个有前景的研究方向。

Result: 文章没有具体实验结果，而是提供了一个理论框架，为研究者和从业者指引在此类系统需求工程上的挑战及研究路线。

Conclusion: 本文提出当前预训练模型广泛融入现代软件系统后，对需求工程带来了前所未有的挑战，需重新思考和调整现有方法。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [17] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 本文创新性地提出了一种动态分析递归下降解析器以恢复属性文法（含语义动作）的方法，能全面恢复系统的输入处理规范，并已在初步实验中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有软件常缺失完整、及时的输入规范，已有文法挖掘方法多关注语法，输入处理的语义尚未充分研究。因此亟需一种能够恢复输入处理语义的方案。

Method: 该方法首先利用输入文法，并对递归下降解析器进行动态分析。然后，通过观察程序执行并将运行时行为映射至文法，对文法规则系统性地提取并嵌入语义动作，生成带属性的文法规范。

Result: 初步实验在一批程序上验证了该方法的可行性，生成的属性文法能准确反映和复现原程序的行为。

Conclusion: 文中提出的方法能够从递归下降解析器实现中自动推断出带属性的文法，可以准确重现程序行为，为全面恢复输入处理规范提供了新手段。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [18] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 为提升针对经手工修改的LLM生成代码的检测能力，作者提出了CodeGPTSensor+，利用对抗训练和MIST模块生成对抗样本，实验验证其在保持高准确率的基础上，鲁棒性和检测能力均明显优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在代码生成领域的广泛应用，带来了代码溯源、版权争议和代码质量等新问题，尤其是如何检测并规范LLM生成代码成为亟需解决的问题。实际情况中，LLM生成代码常被手动微调（如重命名、结构调整），现有检测方法在面对这些改动时鲁棒性不足。

Method: 本文提出CodeGPTSensor+，在原有CodeGPTSensor基础上引入对抗训练，通过Multi-objective Identifier and Structure Transformation（MIST）模块系统化生成高质量和具代表性的对抗样本，以提升模型对输入扰动的鲁棒性。

Result: 实验表明，在HMCorp数据集上，CodeGPTSensor+在原始测试集保持高检测准确率的同时，在对抗测试集上的检测准确率有显著提升，鲁棒性优于CodeGPTSensor。

Conclusion: CodeGPTSensor+通过整合对抗训练与MIST模块，显著提升了对经过微调的LLM生成代码的检测鲁棒性，有效应对实际应用中的挑战。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [Dependency Pairs for Expected Innermost Runtime Complexity and Strong Almost-Sure Termination of Probabilistic Term Rewriting](https://arxiv.org/abs/2507.12918)
*Jan-Christoph Kassing,Leon Spitzer,Jürgen Giesl*

Main category: cs.LO

TL;DR: 本论文首次针对概率项重写系统提出了依赖对框架，可自动分析其复杂性和终止性，在AProVE工具实验中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 自动证明概率项重写系统(PTRS)的终止性已取得突破，但对其复杂性分析的自动化研究还很稀缺。

Method: 提出了首个针对PTRS的依赖对(DP)框架，可用于分析期望复杂性和证明内部重写下的正或强几乎确定终止性(SAST)。

Result: 该框架已在AProVE工具中实现，并通过实验证明在SAST证明上优于现有技术。

Conclusion: 借助新提出的依赖对框架，可以实现PTRS的期望复杂性分析和SAST自动证明，推动了该领域自动化分析的发展。

Abstract: The dependency pair (DP) framework is one of the most powerful techniques for
automatic termination and complexity analysis of term rewrite systems. While
DPs were extended to prove almost-sure termination of probabilistic term
rewrite systems (PTRSs), automatic complexity analysis for PTRSs is largely
unexplored. We introduce the first DP framework for analyzing expected
complexity and for proving positive or strong almost-sure termination (SAST) of
innermost rewriting with PTRSs, i.e., finite expected runtime. We implemented
our framework in the tool AProVE and demonstrate its power compared to existing
techniques for proving SAST.

</details>


### [20] [Cyclic proof theory of positive inductive definitions](https://arxiv.org/abs/2507.13057)
*Gianluca Curzi,Lukas Melgaard*

Main category: cs.LO

TL;DR: 本文证明了扩展Peano算术的μPA系统中，循环和归纳证明在理论上等价，并推广了非良基础证明分析技术。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在探讨μPA中循环证明系统与归纳证明系统之间的证明论强度关系，通过分析其对算术系统的影响以及与二阶算术片段之间的等价性。当前循环证明理论中的有效性及其与归纳证明比较尚存在理论空白。

Method: 作者首先将循环证明翻译为基于Sprenger和Dam处理一阶μ-演算的带注释变体，并利用该变体更强的有效性条件给出简化的可靠性证明；接着，作者在Π^1_2-CA_0系统下对该论证进行形式化，借助Möllefled守恒性定理及Curzi和Das关于Knaster-Tarski定理逆向数学的前期工作。

Result: 作者证明了循环和归纳μPA具有相同的证明论强度。此外，获得副产品：即使采用更强的有效性条件，带注释和“普通”μPA循环证明可以证明同样的定理。

Conclusion: 本文为通过二阶算术不良基础片段对算术理论进行非良基础证明论分析提供了新进展，显示出循环和归纳证明在μPA下具有相同的理论力量，增强了对这两种证明方式本质等效性的理解。

Abstract: We study cyclic proof systems for $\mu\mathsf{PA}$, an extension of Peano
arithmetic by positive inductive definitions that is arithmetically equivalent
to the (impredicative) subsystem of second-order arithmetic
$\Pi^1_2$-$\mathsf{CA}_0$ by M\"{o}llefeld. The main result of this paper is
that cyclic and inductive $\mu\mathsf{PA}$ have the same proof-theoretic
strength. First, we translate cyclic proofs into an annotated variant based on
Sprenger and Dam's systems for first-order $\mu$-calculus, whose stronger
validity condition allows for a simpler proof of soundness. We then formalise
this argument within $\Pi^1_2$-$\mathsf{CA}_0$, leveraging M\"{o}llerfeld's
conservativity properties. To this end, we build on prior work by Curzi and Das
on the reverse mathematics of the Knaster-Tarski theorem. As a byproduct of our
proof methods we show that, despite the stronger validity condition, annotated
and "plain" cyclic proofs for $\mu\mathsf{PA}$ prove the same theorems. This
work represents a further step in the non-wellfounded proof-theoretic analysis
of theories of arithmetic via impredicative fragments of second-order
arithmetic, an approach initiated by Simpson's Cyclic Arithmetic, and continued
by Das and Melgaard in the context of arithmetical inductive definitions.

</details>


### [21] [Monotone weak distributive laws over the lifted powerset monad in categories of algebras](https://arxiv.org/abs/2507.13058)
*Quentin Aristote*

Main category: cs.LO

TL;DR: 本文探究单调弱分配律在集合与紧致Hausdorff空间中的关系，刻画了其在更一般代数范畴的存在性边界，指出该律无法在许多一般情形下自动获得，但在某些特定范畴（如几率与非确定性结合）下给出了具体构造。


<details>
  <summary>Details</summary>
Motivation: 在集合和紧致Hausdorff空间中，两个层次的非确定性之间的单调弱分配律表现出相似性。作者希望探究这种相似性背后的本质，并判定这种分配律在更广泛的范畴中是否能够自动获得或提升。

Method: 通过范畴论方法分析分配律、幺半群、幂集单子等结构之间的关系，并给出一系列存在性和不可存在性的刻画与证明。具体地讨论了在不同代数范畴下的单调弱分配律存在性条件。

Result: 在紧致Hausdorff空间的情形下，部分情况下可以自动获得该弱分配律。同时，具体刻画了哪些代数范畴可以存在此类律，哪些则不可能存在。此外，展示了结合概率与非确定性的新型分配律。

Conclusion: 该文部分证明了在紧致Hausdorff空间中，可以通过弱提升自动获得分层幺半群中的单调弱分配律，但在很多其他代数范畴下这一做法并不可行。文中还刻画了在代数范畴下，在何种条件下关于Powerset Monad的单调弱分配律存在。

Abstract: Noticing the similarity between the monotone weak distributive laws combining
two layers of nondeterminism in sets and in compact Hausdorff spaces, we study
whether the latter law can be obtained automatically as a weak lifting of the
former. This holds partially, but does not generalize to other categories of
algebras: we then characterize when exactly monotone weak distributive laws
over powerset monads in categories of algebras exist, exhibiting a law
combining probabilities and non-determinism in compact Hausdorff spaces and
showing on the other hand that such laws do not exist in a lot of other cases.

</details>


### [22] [Impact and Performance of Randomized Test-Generation using Prolog](https://arxiv.org/abs/2507.13178)
*Marcus Gelderie,Maximilian Luff,Maximilian Peltzer*

Main category: cs.LO

TL;DR: 论文提出两种基于Prolog的随机化测试用例生成策略，通过理论分析和实验对比，验证了随机化提升测试覆盖与效率的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着系统测试复杂度的提升，需要自动生成具备复杂逻辑依赖关系的测试用例序列。传统生成方式面对大量或无限的测试集时效率低下，如何高效、随机地产生测试序列成为一个挑战。

Method: 提出了两种在Prolog中引入随机化的策略：一种是在标准Prolog语义之上实现随机化，另一种则直接修改SLD（Selective Linear Definite clause）选择函数以实现随机化。两种方法均在马尔可夫链理论下分析平均达到某测试用例的时间和生成的用例数量，并通过实证数据对比两种策略的效果。

Result: 在理论分析（马尔可夫链）和实证评价下，对比了两种随机化生成测试序列的方法，各自优缺点明确。

Conclusion: 随机化和SLD分辨机制结合，可以显著改善用Prolog自动生成复杂依赖测试用例时的覆盖效果和效率。两种方案均可用，但根据实际需求可选。

Abstract: We study randomized generation of sequences of test-inputs to a system using
Prolog. Prolog is a natural fit to generate test-sequences that have complex
logical inter-dependent structure. To counter the problems posed by a large (or
infinite) set of possible tests, randomization is a natural choice. We study
the impact that randomization in conjunction with SLD resolution have on the
test performance. To this end, this paper proposes two strategies to add
randomization to a test-generating program. One strategy works on top of
standard Prolog semantics, whereas the other alters the SLD selection function.
We analyze the mean time to reach a test-case, and the mean number of generated
test-cases in the framework of Markov chains. Finally, we provide an additional
empirical evaluation and comparison between both approaches. Under
consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [23] [Just Verification of Mutual Exclusion Algorithms](https://arxiv.org/abs/2507.13198)
*Rob van Glabbeek,Bas Luttik,Myrthe Spronck*

Main category: cs.LO

TL;DR: 论文通过模型检测分析多类基于共享寄存器的互斥算法，着重验证活性和安全性，发现部分算法在特定寄存器模型下失效，并建议改进。


<details>
  <summary>Details</summary>
Motivation: 互斥算法是并发计算中的基础，但复杂的通信机制和不同的寄存器模型带来验证难题。尤其是活性属性验证时需避免伪反例，因此需合理的完整性标准和对不同并发假设的分析。

Method: 通过模型检测的方法，分析基于共享读写寄存器（包括原子和非原子寄存器）的互斥算法，对其安全性和活性属性进行形式化验证。采用justness作为完整性标准，并考察了多种并发关系。

Result: 通过形式化建模和模型检测，揭示了部分常见互斥算法在假定条件下存在正确性问题，并对某些问题提出了改进建议。

Conclusion: 论文验证了多种互斥算法的正确性，发现某些算法在特定条件或寄存器假设下违反了正确性，并提出了部分改进建议。

Abstract: We verify the correctness of a variety of mutual exclusion algorithms through
model checking. We look at algorithms where communication is via shared
read/write registers, where those registers can be atomic or non-atomic. For
the verification of liveness properties, it is necessary to assume a
completeness criterion to eliminate spurious counterexamples. We use justness
as completeness criterion. Justness depends on a concurrency relation; we
consider several such relations, modelling different assumptions on the working
of the shared registers. We present executions demonstrating the violation of
correctness properties by several algorithms, and in some cases suggest
improvements.

</details>


### [24] [Solving SAT By Computing A Stable Set Of Points In Clusters](https://arxiv.org/abs/2507.13282)
*Eugene Goldberg*

Main category: cs.LO

TL;DR: 作者提出将CNF公式的SSP（稳定点集）进行簇处理，提高计算效率，同时也有利于并行化SAT求解。


<details>
  <summary>Details</summary>
Motivation: 直接点对点计算SSP在实际中的CNF公式上通常规模极大且不可行，需寻找高效的方法。

Method: 提出了按簇（clusters）同时处理大量点的方法来计算CNF公式的稳定点集（SSP），而不是逐一计算每个点。

Result: 通过簇计算能够更好利用公式结构，提高SAT算法效率，并且便于并行计算。

Conclusion: 通过聚类计算SSP可以提高处理效率，并为并行化SAT求解提供新途径。

Abstract: Earlier we introduced the notion of a stable set of points (SSP). We proved
that a CNF formula is unsatisfiable iff there is a set of points (i.e. complete
assignments) that is stable with respect to this formula. Experiments showed
that SSPs for CNF formulas of practical interest are very large. So computing
an SSP for a CNF formula point by point is, in general, infeasible. In this
report, we show how an SSP can be computed in clusters, each cluster being a
large set of points that are processed simultaneously. The appeal of computing
SSPs is twofold. First, it allows one to better take into account formula
structure and hence, arguably, design more efficient SAT algorithms. Second,
SAT solving by SSPs facilitates parallel computing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [25] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 本文提出“模型合成架构（MSA）”，结合语言模型和概率程序，能更好地拟合和解释人类在复杂新颖情境下的推理方式，比现有语言模型更接近人类判断水平。


<details>
  <summary>Details</summary>
Motivation: 人类在面对新颖情境时，能够从丰富的背景知识中调用相关信息并用以推理和预测，目前仍不清楚其机制。本文旨在探索人类如何实现全局相关信息的整合与连贯推理。

Method: 作者提出了“模型合成架构”（Model Synthesis Architecture，MSA），结合分布式和符号表示，利用语言模型做全局相关检索与模型合成，用概率程序实现定制化、连贯的世界模型。通过一个围绕体育小故事的‘模型奥运’推理数据集，评估MSA对因果结构与背景知识整合推理的拟人能力。

Result: MSA方法在拟合和再现实验中，比单独依赖语言模型的基线方法更准确地捕捉了人类判断，无论是在直接生成还是链式思维方式下都具有更好表现。

Conclusion: 模型合成架构能更好地模拟人类对全局相关变量且本地连贯的推理能力，为理解与复制人类在开放性情境下的推理提供了新途径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [26] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 作者通过机制可解释性方法，揭示了语言模型内部存在可靠区分模态类别的表征，且这些表征与人类的模态分类行为高度吻合，挑战了此前对LM模态类别判断能力的质疑。


<details>
  <summary>Details</summary>
Motivation: 此前研究对LM区分句子模态的能力提出质疑。作者希望揭示LM内部是否存在高效表征模态类别的机制，以及这些机制是否与人类的判断方式存在对应关系，为理解人机模态分类机制提供新视角。

Method: 作者通过在多种语言模型中识别可区分模态类别的线性表征—模态差异向量，并分析这些向量的表现，探究其与LM训练步骤、层数及参数量的关系。同时，通过与人类被试的可解释特征评分进行相关性分析，比较LM内部表征与人类判断的接近程度。

Result: （1）在多种LM中均发现了能够良好区分模态类别的线性表征；（2）这些模态差异向量随模型能力的提升逐步显现（与训练进展、网络层数、模型参数量相关）；（3）投影在这些向量上的信息可有效模拟人类的模态细分类行为，与人类对可解释特征的评分高度相关。

Conclusion: 本文发现，语言模型（LMs）在区分不同模态类型的能力比之前研究认为的要强，通过辨识模态差异向量（modal difference vectors），LMs不但可以进行更加可靠的模态判断，还能较好模拟人类的细致模态分类行为。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [27] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 本文首次公开了车臣语与俄语的开源机器翻译模型和数据。通过在NLLB-200基础上微调实现，取得了初步的双语互译效果，并配套发布了针对车臣语的多语句编码器和平行语料资源，为低资源语言机器翻译提供新工具和数据。


<details>
  <summary>Details</summary>
Motivation: 车臣语属于濒危语言，缺乏相应的机器翻译资源，因此亟需发展相关的翻译模型与数据，以促进车臣语的保存和使用。

Method: 作者收集并整理了车臣语与俄语的平行语料，并利用NLLB-200大语言模型，通过微调实现了对车臣语的支持，并进行多语言翻译模型训练与评估。

Result: 模型俄语到车臣语的BLEU/ChrF++得分为8.34/34.69，车臣语到俄语方向得分为20.89/44.55。论文还发布了包含词、短语和句子的平行语料库及多语句编码器，提升了车臣语在多语言模型中的适应性。

Conclusion: 论文首次构建了可以将车臣语和俄语互译的开源模型，并公开了相关数据集及工具，为低资源语言的翻译和技术发展提供了有力支持。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [28] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 论文使用多种NLP方法分析美国法医自由文本死因报告，BioClinicalBERT表现最优，实现药物过量自动判读；相比以往人工或传统机器方法，显著提升准确率和时效，助力药物危机监测。


<details>
  <summary>Details</summary>
Motivation: 美国因芬太尼等致死的药物过量事件上升，人工ICD-10编码效率低且易丢失关键信息，急需用自动化、可扩展的方法提升死因监测的时效和准确性。

Method: 使用包含多地区3万余条2020年死亡记录的数据集进行模型训练与内部测试，并利用来自2023-2024年的3千余条新记录进行外部验证。评估了单/多标签分类器、BERT类模型、BioClinicalBERT及最新大语言模型（如Qwen 3、Llama 3）在无结构文本下药物涉案分类任务的性能，以宏F1分数和95%置信区间为指标进行对比。

Result: BioClinicalBERT等微调临床预训练模型在内部测试中宏F1分数达0.998，外部验证得分0.966，显著优于通用模型、传统机器学习和其它大语言模型，解决了ICD-10人工编码的滞后及信息损耗问题。NLP方法极大提升了死因判读速度和准确性，可用于实时发现药物趋势。

Conclusion: 经过微调的临床领域NLP模型（如BioClinicalBERT）能够高准确率地从文本化验单中判定具体药物过量涉案情况，显著优于传统机器学习和大模型（如Llama 3等），是自动化药物过量监测的有效工具。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [29] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: AdaptiSent通过自适应跨模态注意机制，极大提升了多模态情感分析和方面词抽取的精度，在推特数据集上超越了现有方法，成为新基准。


<details>
  <summary>Details</summary>
Motivation: 多模态（文本+图片）下的细粒度情感分析任务（MABSA）目前面临准确提取情感与方面词、理解文本与视觉信息交互的挑战。现有方法对跨模态特征融合适应性较差，导致效果有限。

Method: 提出AdaptiSent模型，采用自适应跨模态注意力机制，实现动态模态加权与上下文自适应注意，以充分挖掘文本与图像之间的互补信息，用于提升细粒度情感分类和方面词抽取的效果。

Result: 在标准推特数据集上，AdaptiSent在准确率、召回率及F1分数上均优于传统文本模型及其他多模态方法，尤其在细致捕捉跨模态关系上效果突出。

Conclusion: AdaptiSent显著提升了多模态细粒度情感分析的效果，尤其在复杂多模态信息互补方面表现出色，树立了新的MABSA基线。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [30] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: 本研究提出了利用大音频模型AudioJudge作为统一语音评估框架，显著提升了自动语音评估与人类偏好的相关性，支持多任务、多特性的语音质量与内容评判。该系统在多项任务中表现优异，但需解决输出冗余和偏置问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语音评估方法存在两大主要问题：一是需针对不同音频特性设计专用系统，过程复杂且难以泛化；二是自动评估结果与人类偏好相关性较差。为解决这两个核心挑战，提出了统一化评估框架的需求。

Method: 本研究系统性地研究了大音频模型（Large Audio Model, LAM）作为评判者的可行性，即AudioJudge，通过不同的提示工程策略（如音频拼接与上下文学习提升性能），并引入多方面集成评判机制，将语音评估分解为词汇内容、语音质量和副语言特征等多领域评判。

Result: 提出的AudioJudge在各类音频特性检测任务（发音、语速、说话人识别、语音质量）与系统级人类偏好模拟评价任务均取得了显著效果。采用集成评判后，在系统排名基准上达到最高0.91的Spearman相关性，表现优异。同时，系统在噪声环境下表现稳定，但存在冗长和位置偏置问题。

Conclusion: AudioJudge作为统一化大音频模型评判工具，实现了多方面、多特征的语音自动评估，极大提升了自动评估与人类偏好相关性，并证明了通过分域集成判决提升通用性的可行性。但仍需关注冗余和位置偏置等新挑战。

Abstract: Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [31] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

TL;DR: FLEXITOKENS提出基于可学习分词器的方法，有效缓解语言模型在新分布适应时的分词碎片化和性能下滑问题，在多项下游任务中带来高达10%的表现提升。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在适应新的数据分布时，仅通过简单微调很难应对，主要因为其子词分词器较为僵化，适应性差，导致在处理新的领域或语种时出现过度碎片化问题，影响任务表现。

Method: 提出了一种基于字节的语言模型，分词器可以自适应学习，由一个子模块学习预测字节序列的分界，将输入编码为可变长度的分段。尤其提出FLEXITOKENS方法，通过简化的训练目标，使分词器在适应时更具灵活性，而非像现有方法强制压缩率固定。

Result: FLEXITOKENS在多个多语言基准、结构复杂任务和不同领域测试中表现出更低的过度碎片化问题，且下游任务性能相比传统子词分词器和其他梯度分词器提升最高可至10%。

Conclusion: FLEXITOKENS为多语言、多领域任务提供了一种更灵活高效的分词与模型适应方案，相较传统分词方法在多种评测中实现了显著性能提升。

Abstract: Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [32] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: TransEvalnia是一个基于prompt的翻译评测系统，结果与人工高度一致，维度细致，超越或媲美现有最优方法，并解决位置偏差。


<details>
  <summary>Details</summary>
Motivation: 近年来机器翻译系统需要更精细和具有人类判断一致性的评测工具。现有评测系统如MT-Ranker虽表现良好，但对翻译排序存在位置偏差，同时缺乏基于推理的质量维度细分评估。

Method: 提出TransEvalnia系统，基于prompting方法结合大模型（Claude-3.5-Sonnet与Qwen-2.5-72B-Instruct），按多维质量标准（MQM子集）对翻译进行细颗粒度评估与排序，输出具体维度与整体分数，并针对翻译顺序敏感性提出修正方法。

Result: TransEvalnia在英文-日文及多个WMT竞赛语言对上与MT-Ranker表现相当或更优，Sonnet等大模型的评分与人类评价高度相关，系统结果被人工普遍认可。

Conclusion: TransEvalnia能高效、精细地评测和排序机器翻译结果，结果与人工评价相关性强，且已发布全部数据和代码；同时提出了缓解位置偏差的方法。

Abstract: We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [33] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 本研究通过显式转换策略提升狼人杀代理的适应性与表现，实验优于传统隐式或固定策略方法。


<details>
  <summary>Details</summary>
Motivation: 现有狼人代理大多以隐式定义有效策略，缺乏适应性，难以应对不断变化的游戏局势。该研究为提升狼人代理适应性和表现，提出显式策略选择方法。

Method: 基于对其他玩家态度和对话情境，动态切换预设策略，明确选择与游戏情境和玩家角色估计相适应的策略，并与隐式或固定策略代理进行性能对比。

Result: 提出的方法使狼人代理能根据实际对局情境选择合适策略，实验验证其在应对变化环境和整体表现上优于传统方法。

Conclusion: 通过明确选择策略，狼人代理能更好地适应游戏上下文并提升表现。实验结果显示，该方法优于隐式或固定策略的基线代理。

Abstract: This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [34] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: 提出了一种新的解码方法和配套训练策略，显著提升了大模型的长推理能力，无需额外大规模训练，方法高效且迁移性强。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在长链式思维（CoT）中展现复杂推理能力，这些能力通常依赖显式训练。论文关注能否无需额外训练即激发模型的长推理能力，以降低计算成本。

Method: 提出了一种解码时的方法ThinkLogit，利用logits算术，通过一个显著更小的模型作为指导者，调整目标大模型以支持长推理。进一步提出ThinkLogit-DPO，通过对正确/错误推理对进行偏好优化训练提升指导模型效果。

Result: 在四个数学数据集上，Qwen2.5-32B模型在R1-Distill-Qwen-1.5B指导下，ThinkLogit和ThinkLogit-DPO方法的pass@1比分别提升了26%和29%。ThinkLogit还可以迁移通过强化学习获得的长推理能力，使pass@1提升13%。

Conclusion: 该方法在无需或最小额外训练的情况下，有效提升了大型模型的长推理能力，并具有良好的计算效率。

Abstract: Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [35] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

TL;DR: 本文提出Synergy模型，实现无需手动分词的端到端字节建模，性能优异且具备更高抽象能力，展现了分词器自由架构的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有主流大语言模型多依赖于手工设计或预训练的分词器，造成一定局限。本文旨在寻找端到端、自适应并能动态抽象的建模框架。

Method: 提出Synergy模型，并以端到端方式通过学习的路由机制跨越不同抽象层次，重点将其训练为字节级语言模型，并与Llama3等模型对比。

Result: Synergy能够自发学习出比传统字节级BPE分词器更少的概念token，同时在同等规模下性能不弱于Llama3，并展现位置无关性的概念抽象能力。

Conclusion: Synergy证明了无需预训练分词器的架构是可行的，为更强健、灵活的模型流程提供了新方案。

Abstract: In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [36] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 通过从大模型合成否定数据、用对比学习微调文本编码器，小模型对否定句的理解显著增强，且常规任务性能不受影响，该法也能提升大模型对否定语句的处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前小型文本编码器在具体理解任务中依然重要，但其对“否定”语义处理不佳，影响嵌入向量在下游场景的应用，需要提高其对句子否定信息的敏感度与表达能力。

Method: 从大型语言模型中合成包含多种否定和模糊表达的数据，结合对比学习方法对BERT等强力文本编码器进行微调，同时探索方法对大语言模型的适用性及效果。

Result: 微调后的BERT等模型在否定理解能力上大幅提升，同时在通用理解基准任务上依旧表现优异。对照方法也可提升LLM的否定理解任务表现。

Conclusion: 采用对比学习方法将大模型中蕴含的否定和模糊语句多样模式蒸馏到小型文本编码器中，大幅提升了其对否定语义的理解力，并且在通用基准上保持了竞争力。相关方法亦可扩展用于大语言模型自身，对否定测试表现也有提升。

Abstract: Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [37] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

TL;DR: LLMs可以在没有专门音乐训练的情况下根据文本生成具备某些音乐结构的MIDI音乐，揭示了它们对音乐符号隐式建模的能力和局限。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在自然语言、代码和数学等符号领域表现突出，但它们在音乐等符号音乐领域的建模能力尚未被充分探索。本文旨在研究LLMs对音乐概念的内在表征能力。

Method: 作者通过从描述流派和风格组合的文本提示生成符号音乐数据（MIDI文件），构建数据集，并在该数据集上训练神经网络以执行音乐流派和风格分类、旋律补全等任务，与传统模型进行对比。

Result: 结果显示，LLMs能够根据文本推断出基本的音乐结构和时间关系，能隐式地编码某些音乐模式，但由于缺乏明确的音乐上下文，其能力有限。

Conclusion: LLMs有潜力作为符号音乐生成和理解工具，但尚存在需要明确音乐知识补充的不足，对LLMs在音乐领域的生成能力有了更深入的认识。

Abstract: Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [38] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文系统分析了多语言模型在事实知识方面的跨语言一致性，发现代码切换训练与跨语言词对齐策略对提升模型一致性和性能效果突出，建议未来多关注这两方面方法。


<details>
  <summary>Details</summary>
Motivation: 跨语言一致性对于评估模型知识迁移能力、维持各语言间知识真实性和模型性能均衡具有重要意义。作者希望深入分析和解释多语言模型在事实知识上的跨语言一致性。

Method: 通过分析代码混合下具有相同核心信息的共指陈述，结合可解释性方法，系统性分析和评估多语言模型在不同语言和语言家族下的知识一致性；并测试提升多语言模型性能的常用策略对跨语言知识一致性的影响。

Result: 多语言模型在不同语言家族和语言因素下表现出不同层次的一致性，同时在特定层具有跨语言一致性的瓶颈。主流提升多语言性能的做法中，代码切换训练和跨语言词对齐最显著提升了一致性。

Conclusion: 代码切换训练和跨语言词对齐对提升多语言模型性能和跨语言事实知识一致性都有显著贡献，说明进行跨语言对齐监督和代码切换训练对多语言模型尤为重要。

Abstract: Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [39] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 通过让语言模型的不同层同时进行文本生成，并微调中间层，作者提出了一种新的分层解码架构，在多项任务上取得了优异表现，为分层推理模型的研发提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有的解码器仅使用最后一层进行文本生成，而人类思维通常具备层次性。因此，作者受到人类分层思维的启发，提出探索多层同时解码的可能性。

Method: 在预训练语言模型的不同中间层上复制最后一层的语言头，并通过不同任务输入进行微调，从而适配为层次化的解码器结构。通过实验评估多层解码产生的效果。

Result: 实验显示，选择性的中间层可以生成有意义且合理的文本内容，层次化解码模型在层次文本分类、分类指导生成和层次文本生成等多个任务上达到了最新最优表现。

Conclusion: 该工作展示了多层次同时解码的可行性和有效性，为从零预训练通用分层推理模型的研究提供了可能方向。

Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [40] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 本文提出结合LLM与代码生成技术，分步处理西班牙语表格问答任务，方案准确率达85%。


<details>
  <summary>Details</summary>
Motivation: IberLEF 2025 PRESTA 任务主要关注西班牙语表格上的问答问题，旨在开发能够理解和处理表格数据并回答相关问题的自动化系统。本文的动机是提升针对表格问答任务的自动化解答能力，推动LLM在实际信息处理应用领域的表现。

Method: 本研究提出一种基于LLM的Python代码生成方法，分为多个步骤：首先分析和理解表格内容，然后选择有用的列，接着生成自然语言指令，翻译成Python代码，运行代码并处理错误或异常。每一步均结合了开源LLM和优化提示词。该方案在Semeval 2025相关任务中的MRT实现基础上演化而来。

Result: 提出的方法在PRESTA任务上取得了85%的准确率，体现了其优越性。

Conclusion: 基于LLM的逐步表格数据处理及代码生成方法能有效提升西班牙语表格问答任务的自动化解答能力，具有实用价值。

Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


### [41] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
*Quentin Goux,Nadira Lammari*

Main category: cs.CL

TL;DR: 本文提出一种基于UML的攻击场景正式建模方法，用于支持网络安全攻击分析和自动化训练脚本生成，有助于网络安全流程自动化。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全威胁不断演变，组织需要不断加强自动化水平以保护资产。然而，自动化流程需要结构化、形式化的数据输入，当前缺少专门针对攻击情境输入的正式化建模方法。

Method: 采用UML类模型进行攻击情境及上下文建模，定义出攻击输入数据的正式模型，并通过两个用例（攻击分析、攻击脚本生成）展示其实用性。

Result: 提出了基于UML类图的攻击情境正式建模方法，并通过案例展示其在攻击分析和自动生成训练脚本中的应用价值。

Conclusion: 本文提出的正式模型能够有效促进网络安全相关流程的数据标准化和自动化，提升攻击分析和训练的效率。该模型能够支持上游攻击分析流程并实现自动化的攻击脚本生成。

Abstract: Organizations face an ever-changing threat landscape. They must continuously
dedicate significant efforts to protect their assets, making their adoption of
increased cybersecurity automation inevitable. However, process automation
requires formalization of input data. Through this paper, we address this need
for processes that use attack scenarios as input. Among these processes, one
can mention both the generation of scripts for attack simulation and training
purposes, as well as the analysis of attacks. Therefore, the paper's main
research contribution is a novel formal model that encompasses the attack's
context description and its scenario. It is abstracted using UML class model.
Once the description of our model done, we will show how it could serve an
upstream attack analysis process. We will show also its use for an automatic
generation of attack scripts in the context of cybersecurity training. These
two uses cases constitute the second contribution of this present research
work.

</details>


### [42] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: SemCSE方法利用LLM摘要和对比学习，显著提升科学文本语义嵌入效果，在新老基准上取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有的科学文本表示方法大多依赖于引用关系，这些方法并不能真正反映文本的语义相似性。为了更好地捕捉科学文本的语义内容，需要一种新的无监督表示学习方法。

Method: 提出了一种名为SemCSE的无监督科学文本语义嵌入学习方法。其核心做法是利用大型语言模型（LLM）生成的科学摘要，进行对比学习，将语义相关的摘要在嵌入空间中调整得更接近。通过新的评价基准及对SciRepEval数据集的实验，评估方法有效性。

Result: SemCSE在新提出的评测方法和标准科学文本嵌入评测基准SciRepEval上，均表现出在同尺寸模型中的最优表现，并且相比传统方法，其语义聚合和分离能力更强。

Conclusion: 基于LLM摘要和对比学习的SemCSE方法可以更好地捕捉科学文本的真实语义，优于基于引文的传统方法，且在科学文本嵌入任务中取得了最佳结果。

Abstract: We introduce SemCSE, an unsupervised method for learning semantic embeddings
of scientific texts. Building on recent advances in contrastive learning for
text embeddings, our approach leverages LLM-generated summaries of scientific
abstracts to train a model that positions semantically related summaries closer
together in the embedding space. This resulting objective ensures that the
model captures the true semantic content of a text, in contrast to traditional
citation-based approaches that do not necessarily reflect semantic similarity.
To validate this, we propose a novel benchmark designed to assess a model's
ability to understand and encode the semantic content of scientific texts,
demonstrating that our method enforces a stronger semantic separation within
the embedding space. Additionally, we evaluate SemCSE on the comprehensive
SciRepEval benchmark for scientific text embeddings, where it achieves
state-of-the-art performance among models of its size, thus highlighting the
benefits of a semantically focused training approach.

</details>


### [43] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
*Jaya Caporusso,Matthew Purver,Senja Pollak*

Main category: cs.CL

TL;DR: 本提案计划开发NLP框架自动识别文本中的自我面向，构建本体和标注数据集，评估多类模型，并应用于心理健康和现象学案例，旨在拓展NLP对自我研究的深度和应用广度。


<details>
  <summary>Details</summary>
Motivation: 自我（Self）是心理学、认知科学等多个学科的重要研究对象，但其与语言的关系在自然语言处理领域尚未深入探索。很多自我相关内容与心理健康等已知现象密切相关，因此有必要用NLP方法对自我各面向进行系统分析。

Method: 提出建立关于自我面向的本体和人工标注文档数据集，并在此基础上开发和评估判别式模型、生成式大语言模型及基于嵌入的检索方法，从可解释性、与真实标签一致性、准确率和计算效率四方面进行比较。随后将最佳模型应用于心理健康及经验现象学案例分析。

Result: 尚未给出具体实验结果，但预期开发出能够有效识别文本中自我面向的模型，并能够在心理健康和经验现象学实际应用中验证其有效性。

Conclusion: 本工作预计将填补NLP领域自我面向分析的空白，提供理论和数据基础，并以实际心理健康应用展示模型价值。

Abstract: This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.

</details>


### [44] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
*Hadi Mohammadi,Tina Shahedi,Pablo Mosteiro,Massimo Poesio,Ayoub Bagheri,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 文本内容是决定性别歧视检测标注结果的主要因素，模拟人口统计角色对生成式AI标注无益，建议聚焦于提升标注协议和内容分析以促进公平性。


<details>
  <summary>Details</summary>
Motivation: 在NLP系统（如性别歧视检测）中，标注结果的变异性可能受到标注者人口统计特征的影响，因此了解这些变异性的来源对于系统公平性至关重要。

Method: 采用广义线性混合模型（GLMM）分析了标注者人口统计特征与文本内容对标注决策的影响，并用生成式AI模型模拟标注过程，检验引入人口统计角色设定能否提高自动标注与人工标注的一致性。同时，使用可解释性AI（XAI）技术分析模型决策基础。

Result: 1. 标注者人口统计特征虽然有统计影响，但仅占总方差的8%，文本内容是主要决定因素。
2. 生成式AI带入人口统计角色设定通常未能提高表现，有时反而降低性能。
3. 可解释性分析表明模型主要依据性别歧视相关的内容词汇做判断，而非人口统计属性。

Conclusion: 确保NLP系统公平性的更有效方法是关注内容驱动的解释与健壮的标注协议，而不是依赖于角色模拟策略。

Abstract: Understanding the sources of variability in annotations is crucial for
developing fair NLP systems, especially for tasks like sexism detection where
demographic bias is a concern. This study investigates the extent to which
annotator demographic features influence labeling decisions compared to text
content. Using a Generalized Linear Mixed Model, we quantify this inf luence,
finding that while statistically present, demographic factors account for a
minor fraction ( 8%) of the observed variance, with tweet content being the
dominant factor. We then assess the reliability of Generative AI (GenAI) models
as annotators, specifically evaluating if guiding them with demographic
personas improves alignment with human judgments. Our results indicate that
simplistic persona prompting often fails to enhance, and sometimes degrades,
performance compared to baseline models. Furthermore, explainable AI (XAI)
techniques reveal that model predictions rely heavily on content-specific
tokens related to sexism, rather than correlates of demographic
characteristics. We argue that focusing on content-driven explanations and
robust annotation protocols offers a more reliable path towards fairness than
potentially persona simulation.

</details>


### [45] [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)
*Emma Sharratt,Annelien Smith,Retief Louw,Daleen Klop,Febe de Wet,Herman Kamper*

Main category: cs.CL

TL;DR: 本文运用机器学习，分析南非四至五岁儿童（南非荷兰语和科萨语）叙述故事的音频，证实词汇多样性和句子长度是叙事能力的有效指标，提出目标动词和助动词的使用可辅助早期识别发展风险，对多语言环境下的学前评估具有启示意义。


<details>
  <summary>Details</summary>
Motivation: 口语叙事能力被认为是后续阅读能力的重要预测指标，但目前针对需要介入儿童的叙事评估特征仍不清晰，尤其在多语言环境下缺乏相关研究。本文旨在探讨不同语言背景下早期叙事能力与发展风险之间的关联特征。

Method: 研究通过录音收集了南非四至五岁以南非荷兰语（Afrikaans）和科萨语（isiXhosa）为母语的儿童的口头故事叙述。运用简单的机器学习方法，对故事中的词汇多样性、平均语句长度等语言特征进行分析，并比较两种语言下的表现特征。

Result: 结果显示，词汇多样性（如独特词数）和基于长度的特征（如平均语句长度）可有效区分典型发展的儿童，而语音表达速度等特征区分作用较弱。此外，目标导向故事中使用特定动词和助动词，能降低被判定为需介入儿童的概率。不同语言间既有特有预测因素，也发现了共同的语言标志。

Conclusion: 对两种语言背景的分析揭示了叙事能力的共通与差异特征，对于多语境下学前儿童早期识别与评估有实际意义，可优化多语种儿童发展的筛查工具。

Abstract: Oral narrative skills are strong predictors of later literacy development.
This study examines the features of oral narratives from children who were
identified by experts as requiring intervention. Using simple machine learning
methods, we analyse recorded stories from four- and five-year-old Afrikaans-
and isiXhosa-speaking children. Consistent with prior research, we identify
lexical diversity (unique words) and length-based features (mean utterance
length) as indicators of typical development, but features like articulation
rate prove less informative. Despite cross-linguistic variation in
part-of-speech patterns, the use of specific verbs and auxiliaries associated
with goal-directed storytelling is correlated with a reduced likelihood of
requiring intervention. Our analysis of two linguistically distinct languages
reveals both language-specific and shared predictors of narrative proficiency,
with implications for early assessment in multilingual contexts.

</details>


### [46] [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)
*Jisoo Lee,Raeyoung Chang,Dongwook Kwon,Harmanpreet Singh,Nikhil Verma*

Main category: cs.CL

TL;DR: 本文提出GEMMAS框架，通过对多智能体协作过程的图建模和两个新指标（IDS和UPR），发现同等准确率下，系统内部协作质量差异显著，强调需关注推理过程以提升协作AI的解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在协作推理任务上表现突出，但现有评估方法仅关注最终结果的正确性，忽视了系统内部通信效率低下和协作不佳带来的冗余推理和算力浪费。作者希望解决对协作过程缺乏深入评估的问题。

Method: 提出GEMMAS图结构评估框架，将多智能体系统内部交互建模为有向无环图。设计了两个进程级指标：信息多样性分数（IDS，衡量智能体间消息的语义差异）和冗余路径比例（UPR，量化不必要的推理路径），以捕捉协作过程的质量。

Result: 在五个基准数据集上评估GEMMAS，并重点展示了GSM8K数据集上的结果：系统最终准确率仅有2.1%的差异，但IDS有12.8%的差异，UPR更是相差80%，说明内部协作过程表现存在巨大差异。

Conclusion: 仅关注推理结果的指标不足以全面衡量多智能体系统的性能，过程级的诊断和评估对于设计更可解释、资源高效的协作式AI系统至关重要。

Abstract: Multi-agent systems built on language models have shown strong performance on
collaborative reasoning tasks. However, existing evaluations focus only on the
correctness of the final output, overlooking how inefficient communication and
poor coordination contribute to redundant reasoning and higher computational
costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes
the internal collaboration process by modeling agent interactions as a directed
acyclic graph. To capture collaboration quality, we propose two process-level
metrics: Information Diversity Score (IDS) to measure semantic variation in
inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant
reasoning paths. We evaluate GEMMAS across five benchmarks and highlight
results on GSM8K, where systems with only a 2.1% difference in accuracy differ
by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal
collaboration. These findings demonstrate that outcome-only metrics are
insufficient for evaluating multi-agent performance and highlight the
importance of process-level diagnostics in designing more interpretable and
resource-efficient collaborative AI systems.

</details>


### [47] [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)
*R. Louw,E. Sharratt,F. de Wet,C. Jacobs,A. Smith,H. Kamper*

Main category: cs.CL

TL;DR: 提出了利用自动语音识别和大语言模型自动评估学前儿童口述叙事能力的系统，有效辅助教师发现需要干预的学生，实现了自动化、可扩展的早期读写能力筛查。


<details>
  <summary>Details</summary>
Motivation: 学前阶段叙事与理解能力对后续读写发展至关重要，但面对大班教学，教师难以及时、准确发现需要干预的学生，因此亟需自动化工具辅助评估。

Method: 系统首先利用自动语音识别（ASR）对学前儿童的口头叙事（使用Afrikaans和isiXhosa两种语言）进行转录，然后用机器学习模型（包括线性模型和大语言模型）来预测叙事及理解分数，对比两种模型的效果。

Result: 大语言模型（LLM）在大多数情况下表现优于线性模型，且简易线性模型也有竞争力。LLM系统在需要干预儿童的标记上效果与人类专家相当，为自动化口头评估提供了基础。

Conclusion: 本文提出的基于自动语音识别与大语言模型（LLM）的口头叙事评分系统，在检测需要干预的儿童方面，其表现可与人类专家媲美，有助于提升教师发现学习困难儿童的效率。

Abstract: Developing narrative and comprehension skills in early childhood is critical
for later literacy. However, teachers in large preschool classrooms struggle to
accurately identify students who require intervention. We present a system for
automatically assessing oral narratives of preschool children in Afrikaans and
isiXhosa. The system uses automatic speech recognition followed by a machine
learning scoring model to predict narrative and comprehension scores. For
scoring predicted transcripts, we compare a linear model to a large language
model (LLM). The LLM-based system outperforms the linear model in most cases,
but the linear system is competitive despite its simplicity. The LLM-based
system is comparable to a human expert in flagging children who require
intervention. We lay the foundation for automatic oral assessments in
classrooms, giving teachers extra capacity to focus on personalised support for
children's learning.

</details>


### [48] [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)
*Xinyu Tang,Zhihao Lv,Xiaoxue Cheng,Junyi Li,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 该论文提出了CAST，通过操控模型内部激活状态实现跨任务迁移，无需调整模型参数或增加输入，实验证明其在效率和效果上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的提示方法在面对数据稀缺和新任务时表现不佳，且跨任务的高效知识迁移面临稳健性、可扩展性及效率等问题。作者希望寻找一种无需模型参数更新也无需扩展输入的方法，实现有效的跨任务迁移。

Method: 分析大语言模型（LLM）潜在空间中的激活模式，发现上下文示例激活模式在不同任务中有一致性。基于这一发现，提出CAST框架：首先从高资源任务中挑选具影响力和多样性的样本，使用这些样本的对比表示增强的激活模式，进而操控模型激活状态，实现向低资源任务的迁移。

Result: CAST框架在跨领域和跨语言迁移任务中，效果均优于竞争性基线方法，同时展现出更强的可扩展性和更低的计算资源消耗。

Conclusion: 提出的CAST方法通过在大模型的潜在空间中操控激活状态，能够在不调整模型参数或扩展输入的情况下，有效实现跨任务迁移。实验表明，该方法优于现有方法，具备更好的可扩展性和更低的计算成本。

Abstract: Large language models (LLMs) have shown impressive abilities in leveraging
pretrained knowledge through prompting, but they often struggle with unseen
tasks, particularly in data-scarce scenarios. While cross-task in-context
learning offers a direct solution for transferring knowledge across tasks, it
still faces critical challenges in terms of robustness, scalability, and
efficiency. In this paper, we investigate whether cross-task transfer can be
achieved via latent space steering without parameter updates or input
expansion. Through an analysis of activation patterns in the latent space of
LLMs, we observe that the enhanced activations induced by in-context examples
have consistent patterns across different tasks. Inspired by these findings, we
propose CAST, a novel Cross-task Activation Steering Transfer framework that
enables effective transfer by manipulating the model's internal activation
states. Our approach first selects influential and diverse samples from
high-resource tasks, then utilizes their contrastive representation-enhanced
activations to adapt LLMs to low-resource tasks. Extensive experiments across
both cross-domain and cross-lingual transfer settings show that our method
outperforms competitive baselines and demonstrates superior scalability and
lower computational costs.

</details>


### [49] [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)
*Ashray Gupta,Rohan Joseph,Sunny Rai*

Main category: cs.CL

TL;DR: 本文构建印地语类比测试集，发现多语种LLM用英文提示表现优于印地语，并提出新推理方法提高了模型得分。


<details>
  <summary>Details</summary>
Motivation: 现有类比测试多针对英语，且印地语等印度语言的类比推理能力评估研究不足，导致我们难以了解大型语言模型（LLMs）跨语言的泛化能力。

Method: 作者构建了405题印地语类比测试集（HATS），涵盖自印度政府考试收集的问题，并用多种提示策略对多语种LLM进行基准测试。同时，提出结合认知理论的Chain of Thought（链式推理）提示方法以提升推理能力。

Result: 引入的链式推理方法提高了模型在印地语类比测试中的表现。但无论用何种提示方法，模型在英文提示下表现最佳。

Conclusion: HATS测试集填补了印地语类比推理评测资源缺失的空白，为评估多语种LLM推理能力提供了工具。模型对于印地语推理仍有提升空间。

Abstract: Analogies test a model's ability to infer implicit relationships between
concepts, making them a key benchmark for evaluating reasoning capabilities.
While large language models (LLMs) are widely evaluated for reasoning in
English, their abilities in Indic languages remain understudied, limiting our
understanding of whether these models generalize across languages. To address
this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405
multiple-choice questions sourced from Indian government exams. We benchmark
state-of-the-art multilingual LLMs using various prompting strategies and
introduce a grounded Chain of Thought approach that leverages cognitive
theories of analogical reasoning. This approach improves model performance on
Hindi analogy questions. Our experiments show that models perform best with
English prompts, irrespective of the prompting strategy. Our test set addresses
the lack of a critical resource to evaluate LLM reasoning capabilities in
Hindi.

</details>


### [50] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: 提出了无需微调即可提升MLLMs安全性的AutoSteer，通过多组件组合，能在保障模型能力的同时，大幅降低对抗性安全威胁。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大型语言模型（MLLMs）能力的增强，其跨模态推理能力变强的同时，安全风险也随之增加，尤其是在面对对抗性多模态输入时。提升MLLMs在推理阶段的安全性成为亟需解决的问题。

Method: 提出了AutoSteer，这是一种模块化且自适应的推理时干预技术。其不需要对基础模型进行微调，核心包括三个部分：(1) 安全感知分数（SAS），自动识别模型内部层与安全相关的关键差异；(2) 适应性安全探测器，估算中间表示产生有毒输出的概率；(3) 轻量级拒绝头，当检测到安全风险时对此进行选择性干预和生成调控。

Result: 在LLaVA-OV和Chameleon等安全关键基准上实验表明，AutoSteer在文本、视觉以及跨模态威胁下显著降低了攻击成功率（ASR），且不损失模型的通用能力。

Conclusion: AutoSteer具备实用性、可解释性及有效性，是多模态人工智能系统安全部署的一个有前景框架。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


### [51] [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
*Jiazheng Li,Hong Lu,Kaiyue Wen,Zaiwen Yang,Jiaxuan Gao,Hongzhou Lin,Yi Wu,Jingzhao Zhang*

Main category: cs.CL

TL;DR: 本文通过提出问题增强（QuestA）方法，在RL训练中引入部分解题过程，大幅提升了大语言模型在数学推理等多步复杂任务中的能力，实现了新的SOTA，并且该方法具备广泛适用性和理论依据。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习已广泛用于训练大语言模型（LLM）的推理能力，但近年来研究发现其对多步复杂推理提升有限，尤其在难题上效果不佳。因此，研究者希望找到提升RL在多步推理、尤其是应对高难问题时效果的方法。

Method: 提出了一种基于问题增强（Question Augmentation）的方法QuestA，在RL训练时引入部分解题步骤作为中间信号，从而降低问题难度并为学习提供更丰富的信息。该方法在数学推理任务中与强化学习相结合，提升了训练效率与模型能力，并附有理论解释其为何有效。

Result: 在多个数学基准（AIME24、AIME25、HMMT25）上，1.5B参数量级模型用QuestA方法分别提升了5.3%、10.0%、4.0%的成绩（分别达到67.1%、59.5%、35.5%），超越了如DeepScaleR、OpenMath Nemotron等强力开源模型。

Conclusion: 通过在RL训练中引入部分解题步骤的增强方法，能够显著提升大语言模型在复杂推理任务中的表现，并带来理论上的采样效率提升，对拓展推理能力具有良好的通用性和实用价值。

Abstract: Reinforcement learning (RL) has become a key component in training large
language reasoning models (LLMs). However, recent studies questions its
effectiveness in improving multi-step reasoning-particularly on hard problems.
To address this challenge, we propose a simple yet effective strategy via
Question Augmentation: introduce partial solutions during training to reduce
problem difficulty and provide more informative learning signals. Our method,
QuestA, when applied during RL training on math reasoning tasks, not only
improves pass@1 but also pass@k-particularly on problems where standard RL
struggles to make progress. This enables continual improvement over strong
open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing
their reasoning capabilities. We achieve new state-of-the-art results on math
benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)
on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical
explanations that QuestA improves sample efficiency, offering a practical and
generalizable pathway for expanding reasoning capability through RL.

</details>


### [52] [Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management](https://arxiv.org/abs/2507.13275)
*Luis Gasco,Hermenegildo Fabregat,Laura García-Sardiña,Paula Estrella,Daniel Deniz,Alvaro Rodrigo,Rabih Zbib*

Main category: cs.CL

TL;DR: TalentCLEF 2025 是首个面向职位和技能智能的多语言测评，发布了公开基准，吸引大量团队参与，并推动了劳动力市场领域大语言模型和信息检索技术的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理和大语言模型在人力资本管理领域变革巨大，但缺乏公开数据和基准阻碍了该领域公平、可靠模型的开发和进步，因此亟需相关评测活动和基准。

Method: 组织了TalentCLEF 2025评测，设计了两个任务：多语言职位名称匹配（涵盖英语、西班牙语、德语、中文）和基于职位名称的技能预测（英语），并基于真实工作申请数据构建了人工标注语料，评估体系覆盖单语、跨语种和性别偏见场景。

Result: 共有76支队伍、280余次提交参与，系统多采用基于多语种编码器的对比学习和大语言模型的数据增强或排序，结果表明训练策略比模型规模更具影响力。TalentCLEF成为本领域首个公开基准，促进了相关技术发展。

Conclusion: 本文通过TalentCLEF 2025评测活动，为劳动力市场提供了首个公开基准，推进了构建稳健、公平并具有可迁移性的语言技术，为人力资源管理中的智能系统发展起到了关键推动作用。

Abstract: Advances in natural language processing and large language models are driving
a major transformation in Human Capital Management, with a growing interest in
building smart systems based on language technologies for talent acquisition,
upskilling strategies, and workforce planning. However, the adoption and
progress of these technologies critically depend on the development of reliable
and fair models, properly evaluated on public data and open benchmarks, which
have so far been unavailable in this domain.
  To address this gap, we present TalentCLEF 2025, the first evaluation
campaign focused on skill and job title intelligence. The lab consists of two
tasks: Task A - Multilingual Job Title Matching, covering English, Spanish,
German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.
Both corpora were built from real job applications, carefully anonymized, and
manually annotated to reflect the complexity and diversity of real-world labor
market data, including linguistic variability and gender-marked expressions.
  The evaluations included monolingual and cross-lingual scenarios and covered
the evaluation of gender bias.
  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most
systems relied on information retrieval techniques built with multilingual
encoder-based models fine-tuned with contrastive learning, and several of them
incorporated large language models for data augmentation or re-ranking. The
results show that the training strategies have a larger effect than the size of
the model alone. TalentCLEF provides the first public benchmark in this field
and encourages the development of robust, fair, and transferable language
technologies for the labor market.

</details>


### [53] [Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis](https://arxiv.org/abs/2507.13285)
*Wang Xi,Quan Shi,Tian Yu,Yujie Peng,Jiayi Sun,Mengxing Ren,Zenghui Ding,Ningguang Yao*

Main category: cs.CL

TL;DR: 提出了RCPS框架显著提升自动演示生成质量，并研发PREVAL评估系统与人类判断吻合，解决行业痛点。


<details>
  <summary>Details</summary>
Motivation: 现有自动化高质量媒体演示文稿生成方法难以实现内容一致、结构合理、版式优良的专业输出，常出现逻辑不一致与排版不佳的问题，无法满足专业需求。

Method: 提出RCPS（反思式连贯演示文稿合成）框架，包括深度结构化叙事规划、自适应版式生成及迭代优化三大核心组件；另外，提出名为PREVAL的基于偏好、理据增强的多维度演示质量评估框架，从内容、一致性、设计三个维度自动评估演示文稿质量。

Result: 实验表明，RCPS在所有质量维度上均优于现有基线方法，生成的演示文稿接近人类专家水平。PREVAL的评价结果与人工判断高度相关，验证其作为自动化评估工具的可靠性。

Conclusion: RCPS框架能够有效提升自动生成演示文稿的整体质量，实现结构一致、布局优良的输出，同时配合PREVAL能够可靠、自动地评估演示文稿的质量。

Abstract: Automated generation of high-quality media presentations is challenging,
requiring robust content extraction, narrative planning, visual design, and
overall quality optimization. Existing methods often produce presentations with
logical inconsistencies and suboptimal layouts, thereby struggling to meet
professional standards. To address these challenges, we introduce RCPS
(Reflective Coherent Presentation Synthesis), a novel framework integrating
three key components: (1) Deep Structured Narrative Planning; (2) Adaptive
Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose
PREVAL, a preference-based evaluation framework employing rationale-enhanced
multi-dimensional models to assess presentation quality across Content,
Coherence, and Design. Experimental results demonstrate that RCPS significantly
outperforms baseline methods across all quality dimensions, producing
presentations that closely approximate human expert standards. PREVAL shows
strong correlation with human judgments, validating it as a reliable automated
tool for assessing presentation quality.

</details>


### [54] [AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300)
*Yilun Zhao,Weiyuan Chen,Zhijian Xu,Manasi Patwardhan,Yixin Liu,Chengye Wang,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了首个用于评估大模型设计科学消融实验能力的基准AbGen，揭示了现有模型与专家之间的表现差距，并发现在该任务下自动化评价体系并不可靠。


<details>
  <summary>Details</summary>
Motivation: 目前大模型（LLMs）在科学研究中被广泛应用，但其设计消融实验方案的能力尚缺考核手段和数据集，实际表现距离专家有明显差距，所以需要一个专门的benchmark来评估与改进LLM在该任务上的表现。

Method: 构建了AbGen基准，包含1500个人工标注案例，来源于807篇NLP论文，要求LLM基于给定研究背景为特定模块生成详细的消融实验设计。同时提出AbGen-Eval，用于评估自动化评价工具对LLM消融实验设计能力打分的可靠性。

Result: 主流大模型如DeepSeek-R1-0528和o4-mini在消融实验设计的重要性、真实性和科学性上与人类专家有明显差距。现有自动化评价体系对该任务结果与人工评价存在较大偏差。

Conclusion: AbGen首次提出用于评估LLM消融实验设计的基准，并揭示了现有自动化评价在此类复杂科学任务上的不足。为未来构建更有效、可靠的LLM评价机制提供了重要数据和方向。

Abstract: We introduce AbGen, the first benchmark designed to evaluate the capabilities
of LLMs in designing ablation studies for scientific research. AbGen consists
of 1,500 expert-annotated examples derived from 807 NLP papers. In this
benchmark, LLMs are tasked with generating detailed ablation study designs for
a specified module or process based on the given research context. Our
evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a
significant performance gap between these models and human experts in terms of
the importance, faithfulness, and soundness of the ablation study designs.
Moreover, we demonstrate that current automated evaluation methods are not
reliable for our task, as they show a significant discrepancy when compared to
human assessment. To better investigate this, we develop AbGen-Eval, a
meta-evaluation benchmark designed to assess the reliability of commonly used
automated evaluation systems in measuring LLM performance on our task. We
investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for
future research on developing more effective and reliable LLM-based evaluation
systems for complex scientific tasks.

</details>


### [55] [HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals](https://arxiv.org/abs/2507.13318)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 该论文提出并公开了首个大规模人工标注的触觉震动-文本对数据集HapticCap, 并提出新的检索任务。实验发现T5+AST模型在该任务表现最佳。这将推动触觉与文本跨模态研究和实际应用。


<details>
  <summary>Details</summary>
Motivation: 触觉信号（如智能手机震动、虚拟现实触觉反馈）能够有效传达信息并增强真实感，但设计出能与用户产生有意义共鸣的触觉信号具有挑战性。当前主要难题包括缺乏带有文本描述的大规模震动数据集，以及现有模型难以用文本描述震动信号。

Method: 作者构建了首个由人工完整标注的触觉-文本数据集HapticCap，包含92,070对触觉振动与用户描述（涵盖感官、情感及联想属性）。基于该数据集，提出了触觉描述检索任务，并采用监督对比学习框架训练模型，将文本与震动信号在特定类别下进行关联。还比较了不同模型组合的表现。

Result: T5语言模型与AST音频模型的组合在触觉描述检索任务中表现最佳，尤其是在每个描述类别分别训练时效果最优。

Conclusion: HapticCap数据集极大地丰富了触觉信号的文本描述资源，为触觉-文本跨模态研究提供了支撑。结合对比学习框架和多模态模型，有效提升了触觉信号与文本描述的匹配能力。这个方向对触觉数据理解及交互设计意义重大。

Abstract: Haptic signals, from smartphone vibrations to virtual reality touch feedback,
can effectively convey information and enhance realism, but designing signals
that resonate meaningfully with users is challenging. To facilitate this, we
introduce a multimodal dataset and task, of matching user descriptions to
vibration haptic signals, and highlight two primary challenges: (1) lack of
large haptic vibration datasets annotated with textual descriptions as
collecting haptic descriptions is time-consuming, and (2) limited capability of
existing tasks and models to describe vibration signals in text. To advance
this area, we create HapticCap, the first fully human-annotated
haptic-captioned dataset, containing 92,070 haptic-text pairs for user
descriptions of sensory, emotional, and associative attributes of vibrations.
Based on HapticCap, we propose the haptic-caption retrieval task and present
the results of this task from a supervised contrastive learning framework that
brings together text representations within specific categories and vibrations.
Overall, the combination of language model T5 and audio model AST yields the
best performance in the haptic-caption retrieval task, especially when
separately trained for each description category.

</details>


### [56] [Social and Political Framing in Search Engine Results](https://arxiv.org/abs/2507.13325)
*Amrit Poudel,Tim Weninger*

Main category: cs.CL

TL;DR: 搜索引擎与用户意识形态检索共同加剧了搜索结果的偏见和信息极化，不同搜索引擎在内容和来源排序上差异明显。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注搜索引擎偏见的多种表现，但对于“搜索引擎本身及用户意识形态取向的检索”是如何共同作用于结果偏见，研究不足。该文旨在弥补这一空白。

Method: 收集与政治、社会相关主题的数据集，分析主流搜索引擎的检索输出，对比不同搜索引擎在内容排序和来源分配上的表现，考察用户意识形态取向对结果偏见的影响。

Result: 发现搜索引擎在反映和加剧意识形态偏见方面具有重要作用，用户具有倾向性的检索进一步加深了这些偏见，各大搜索引擎在所优先展示的信息来源上有较大差别。

Conclusion: 搜索引擎不仅自身存在内容优先级的偏见，还会因用户带有意识形态倾向的检索行为进一步加剧这种偏见，对信息极化和公众观念有放大作用。各大搜索引擎在内容来源的优先级上存在显著差异。

Abstract: Search engines play a crucial role in shaping public discourse by influencing
how information is accessed and framed. While prior research has extensively
examined various dimensions of search bias -- such as content prioritization,
indexical bias, political polarization, and sources of bias -- an important
question remains underexplored: how do search engines and
ideologically-motivated user queries contribute to bias in search results. This
study analyzes the outputs of major search engines using a dataset of political
and social topics. The findings reveal that search engines not only prioritize
content in ways that reflect underlying biases but also that
ideologically-driven user queries exacerbate these biases, resulting in the
amplification of specific narratives. Moreover, significant differences were
observed across search engines in terms of the sources they prioritize. These
results suggest that search engines may play a pivotal role in shaping public
perceptions by reinforcing ideological divides, thereby contributing to the
broader issue of information polarization.

</details>


### [57] [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)
*Yulu Qin,Dheeraj Varghese,Adam Dahlgren Lindström,Lucia Donatelli,Kanishka Misra,Najoung Kim*

Main category: cs.CL

TL;DR: VL训练不会改变语言模型的分类知识本质，却能提升模型在需要调用这些知识的任务中的实际表现。


<details>
  <summary>Details</summary>
Motivation: 既往文献显示，视觉-语言联合训练（VL训练）对语言模型的语言表征影响不明显或不一致，作者希望进一步探究VL训练是否会在词汇-概念知识，特别是分类组织领域带来显著影响。

Method: 选取文本-only语言模型（LMs）与其VL训练版本作最小对比实验，采用要求分类理解的文本问答任务，并进行系统的行为分析和表征分析，考察二者对于包含分类与非分类关系概念问题的表示差异。

Result: VL模型在依赖分类理解的问答任务表现优于文本-only模型，但在分类知识本身表征上没有显著差异，差异主要体现在处理包含特定概念关系问题时的表征方式。

Conclusion: VL训练并未显著改变模型的分类知识表征，但能提升模型在具体任务中调用这些知识的能力，即便任务纯为语言形式。

Abstract: Does vision-and-language (VL) training change the linguistic representations
of language models in meaningful ways? Most results in the literature have
shown inconsistent or marginal differences, both behaviorally and
representationally. In this work, we start from the hypothesis that the domain
in which VL training could have a significant effect is lexical-conceptual
knowledge, in particular its taxonomic organization. Through comparing minimal
pairs of text-only LMs and their VL-trained counterparts, we first show that
the VL models often outperform their text-only counterparts on a text-only
question-answering task that requires taxonomic understanding of concepts
mentioned in the questions. Using an array of targeted behavioral and
representational analyses, we show that the LMs and VLMs do not differ
significantly in terms of their taxonomic knowledge itself, but they differ in
how they represent questions that contain concepts in a taxonomic relation vs.
a non-taxonomic relation. This implies that the taxonomic knowledge itself does
not change substantially through additional VL training, but VL training does
improve the deployment of this knowledge in the context of a specific task,
even when the presentation of the task is purely linguistic.

</details>


### [58] [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)
*Zhouqi Hua,Wenwei Zhang,Chengqi Lyu,Yuzhe Gu,Songyang Gao,Kuikun Liu,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出通过模仿图灵机执行生成链式思维数据（TAIL），显著提升大模型在超长推理任务上的泛化能力，并表现优异，推动了基于合成数据提升算法推理能力的发展。


<details>
  <summary>Details</summary>
Motivation: 现有关于Transformer大语言模型在序列长度泛化能力上的研究多侧重于针对特定任务的数据驱动方法，这些方法在整体性能和通用性上受到限制。为了解决这一普遍性难题，论文尝试从图灵机理论视角出发，探索更具普适性的算法推理能力提升方法。

Method: 论文提出了一种名为TAIL（图灵机模仿学习，Turing Machine Imitation Learning)的方法，使用程序生成的链式思维（CoT）数据模拟图灵机的执行过程，将推理步骤线性展开为原子状态，降低模型采用捷径学习的可能性，并引入显式记忆读取机制缓解长距离动态数据访问的难题。

Result: TAIL方法在涵盖8类算法和18个任务的合成基准测试集上，显著提升了Qwen2.5-7B及其它模型在长度泛化和任务性能上的表现，超越了现有的数据驱动方法和DeepSeek-R1。实验还表明，TAIL赋予模型的读写能力符合图灵机特性。

Conclusion: TAIL为提升大模型推理泛化能力，尤其在面对超出训练长度的新问题时，提供了有效可行的训练方法。基于模拟图灵机的思路，生成式合成数据能够让模型学习到本质的算法执行机制，对推动LLM reasoning从合成数据中学习具有重要启示。

Abstract: Length generalization, the ability to solve problems of longer sequences than
those observed during training, poses a core challenge of Transformer-based
large language models (LLM). Although existing studies have predominantly
focused on data-driven approaches for arithmetic operations and symbolic
manipulation tasks, these approaches tend to be task-specific with limited
overall performance. To pursue a more general solution, this paper focuses on a
broader case of reasoning problems that are computable, i.e., problems that
algorithms can solve, thus can be solved by the Turing Machine. From this
perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to
improve the length generalization ability of LLMs. TAIL synthesizes
chain-of-thoughts (CoT) data that imitate the execution process of a Turing
Machine by computer programs, which linearly expands the reasoning steps into
atomic states to alleviate shortcut learning and explicit memory fetch
mechanism to reduce the difficulties of dynamic and long-range data access in
elementary operations. To validate the reliability and universality of TAIL, we
construct a challenging synthetic dataset covering 8 classes of algorithms and
18 tasks. Without bells and whistles, TAIL significantly improves the length
generalization ability as well as the performance of Qwen2.5-7B on various
tasks using only synthetic data, surpassing previous methods and DeepSeek-R1.
The experimental results reveal that the key concepts in the Turing Machine,
instead of the thinking styles, are indispensable for TAIL for length
generalization, through which the model exhibits read-and-write behaviors
consistent with the properties of the Turing Machine in their attention layers.
This work provides a promising direction for future research in the learning of
LLM reasoning from synthetic data.

</details>


### [59] [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)
*Lingrui Mei,Jiayu Yao,Yuyao Ge,Yiwei Wang,Baolong Bi,Yujun Cai,Jiazhi Liu,Mingyu Li,Zhong-Zhi Li,Duzhen Zhang,Chenlin Zhou,Jiayi Mao,Tianze Xia,Jiafeng Guo,Shenghua Liu*

Main category: cs.CL

TL;DR: 本文提出并系统化了“上下文工程”领域，详细分解其关键组件和系统集成方式，发现LLM虽善于理解复杂信息，但生成长文本水平有待提升，为后续研究提供了明确方向。


<details>
  <summary>Details</summary>
Motivation: LLM的推理效果高度依赖上下文信息。本综述提出了“上下文工程”这一正式学科，旨在系统化优化LLM的输入信息，而不只是简单的提示词设计。

Method: 作者对1300多篇相关论文进行了系统梳理，将上下文工程分解为基本组件（如上下文检索、生成、处理和管理），并分析了这些组件如何集成到复杂系统（如RAG、记忆系统、工具化推理和多智能体系统）中。

Result: 建立了上下文工程的全新分类体系，并指出当前模型在理解复杂上下文上很强，但在生成复杂、长文本方面存在明显短板。同时为未来工作指出关键研究方向。

Conclusion: 本综述为上下文感知AI领域提供了统一框架和技术路线图，明确了当前技术瓶颈，强调未来应着重提升模型生成复杂长文本的能力。

Abstract: The performance of Large Language Models (LLMs) is fundamentally determined
by the contextual information provided during inference. This survey introduces
Context Engineering, a formal discipline that transcends simple prompt design
to encompass the systematic optimization of information payloads for LLMs. We
present a comprehensive taxonomy decomposing Context Engineering into its
foundational components and the sophisticated implementations that integrate
them into intelligent systems. We first examine the foundational components:
context retrieval and generation, context processing and context management. We
then explore how these components are architecturally integrated to create
sophisticated system implementations: retrieval-augmented generation (RAG),
memory systems and tool-integrated reasoning, and multi-agent systems. Through
this systematic analysis of over 1300 research papers, our survey not only
establishes a technical roadmap for the field but also reveals a critical
research gap: a fundamental asymmetry exists between model capabilities. While
current models, augmented by advanced context engineering, demonstrate
remarkable proficiency in understanding complex contexts, they exhibit
pronounced limitations in generating equally sophisticated, long-form outputs.
Addressing this gap is a defining priority for future research. Ultimately,
this survey provides a unified framework for both researchers and engineers
advancing context-aware AI.

</details>


### [60] [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 该研究揭示大语言模型难以全面解释不同类型的幽默，尤其是复杂和需要世界知识的笑话，反映了计算幽默领域研究的狭隘，强调了未来在更复杂幽默类型解释上的研究需求。


<details>
  <summary>Details</summary>
Motivation: 幽默是一种复杂的语言形式，但现有的计算幽默研究几乎仅关注于短小的双关笑话。本文动机在于探究大语言模型（LLM）解释不同幽默类型的能力，填补对复杂幽默类型研究的空白。

Method: 作者整理了一个包含600个笑话的新数据集，覆盖异形词双关、同形词双关、互联网幽默和涉及现实世界事件的时事笑话，并为每个笑话手动编写高质量解释。然后，让多种LLM模型在零样本（zero-shot）情况下生成解释，分析其对不同幽默类型解释的准确性和全面性。

Result: 实验结果显示，无论是常规模型还是推理增强型模型，都无法对全部幽默类型生成令人满意的解释，尤其在涉及世界知识和复杂推理的笑话类型上表现欠佳。

Conclusion: 当前LLM在幽默解释任务上存在显著不足，尤其是对复杂和需要世界知识的幽默类型，显示出当前计算幽默研究过于聚焦于简单形式的局限。

Abstract: Humour, as a complex language form, is derived from myriad aspects of life,
whilst existing work on computational humour has focussed almost exclusively on
short pun-based jokes. In this work, we investigate whether the ability of
Large Language Models (LLMs) to explain humour depends on the particular humour
form. We compare models on simple puns and more complex topical humour that
requires knowledge of real-world entities and events. In doing so, we curate a
dataset of 600 jokes split across 4 joke types and manually write high-quality
explanations. These jokes include heterographic and homographic puns,
contemporary internet humour, and topical jokes, where understanding relies on
reasoning beyond "common sense", rooted instead in world knowledge regarding
news events and pop culture. Using this dataset, we compare the zero-shot
abilities of a range of LLMs to accurately and comprehensively explain jokes of
different types, identifying key research gaps in the task of humour
explanation. We find that none of the tested models (inc. reasoning models) are
capable of reliably generating adequate explanations of all joke types, further
highlighting the narrow focus of most works in computational humour on overly
simple joke forms.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [61] [Spectral Moment of Order Four and the Uniqueness of the CCZ class of Dublin APN Permutation](https://arxiv.org/abs/2507.12853)
*Valérie Gillot,Philippe Langevin,Abdoulaye Lo*

Main category: cs.DM

TL;DR: 论文通过对6比特布尔函数分类，提出了寻找6比特APN函数的新方法，促进了密码学安全函数研究。


<details>
  <summary>Details</summary>
Motivation: 寻找更安全的密码函数是密码学的重要方向，尤其是APN函数在抵抗差分攻击中有重要应用。6比特APN函数的结构复杂，现有分类和搜索方法有限。

Method: 该论文基于6比特布尔函数的分类，提出了寻找6比特APN函数的新方法和方案。

Result: 获得了6比特APN函数搜索的新进展和结果，丰富了现有的6比特APN函数资料库。

Conclusion: 通过分类和创新方法，推动了6比特APN函数的研究，并对安全布尔函数的结构理解有所加深。

Abstract: The note provides new apparoaches and results for the search of 6-bit
APN-functions based on the classification of 6-bits Boolean functions.

</details>
