<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: TroVE工具箱机制本身贡献有限，MATH基准下提升主要由计算量增加带来，经修正后仅有1%的微弱改进。


<details>
  <summary>Details</summary>
Motivation: 可重用性的定理和公式在数学问题求解中非常关键。TroVE认为，类似的方法可以提升大模型在MATH基准测试中的表现，但有声音质疑其实际作用。本文旨在重新评估TroVE在MATH任务中的有效性。

Method: 重新评估了TroVE方法在MATH数据集上的表现，具体分析其三种模式（直接生成代码、创建工具、复用工具）对表现的影响，并进行了实现上的小修正以提升性能，此外匹配计算消耗，比较TroVE和PRIMITIVE的实际改进幅度。

Result: TroVE模型提升主要源自分配了更高的计算量，而非工具箱机制本身。实现修正后MATH基准表现提升了3%，但在严格匹配计算量后，TroVE仅比直接生成方法提升1%。

Conclusion: TroVE的工具箱机制对提升MATH基准表现并无显著作用，主要优势只是源自于消耗了更多计算资源。

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: 本文提出结合大语言模型和定向模糊测试的新框架RandLuzz，通过智能生成种子和定制变异器，显著提升漏洞检测效率，在主流方法对比测试中取得2~5倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 模糊测试（Fuzzing）依赖随机性有效发现漏洞，但随机性也导致效率低下，使得漏洞暴露过程耗时。即使定向模糊测试（Directed Fuzzing）部分减弱了随机性，但仍未完全解决效率与随机性的矛盾，特别是种子（seed）和变异器（mutator）这两个关键组件中的随机性问题。

Method: 本文提出RandLuzz，创新性地结合大语言模型（LLMs）与定向模糊测试以去除或减少种子和变异器中的随机性。RandLuzz利用LLM优势生成针对目标位置可到达的种子，并根据具体漏洞分析结果定制bug-specific变异器，从而提升模糊测试效率。RandLuzz会分析函数调用链或功能来引导LLM生成可到达种子，并依据漏洞原因与变异建议生成定制化代码变异器。

Result: RandLuzz与AFLGo、Beacon、WindRanger、SelectFuzz四个当前最先进的定向模糊器对比，使用其生成的种子，整体发现漏洞的平均速度提升2.1至4.8倍；在单漏洞评测时，相比第二快方法提升至2.7倍。有8个漏洞可在60秒内被暴露。

Conclusion: 将大语言模型融合进模糊测试能够显著减少相关核心环节（如种子与变异器）随机性，大幅提升检测漏洞的速度与效率。RandLuzz方法在实际多项评价中都显出明显优势。

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [3] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: 提出了一种针对企业 protobuf 复杂嵌套结构的自动化测试数据生成框架，结合 Python 元类机制和日志统计分析，有效提升测试效率与覆盖率，能快速生成大规模、高质量测试数据。


<details>
  <summary>Details</summary>
Motivation: 在大型企业系统中，基于 Protocol Buffers (protobuf) 的复杂嵌套数据结构使得性能测试，尤其是面向中间业务接口的测试数据生成工作极端困难。传统方法无法有效应对企业级 protobuf 架构中复杂的层级和类图结构。

Method: 提出了一种新的测试数据生成框架，利用 Python 的元类（metaclass）机制实现动态类型增强，并通过对生产日志的统计分析，抽取真实的值域信息。该方法结合了自动架构自省、统计值分布分析和递归下降算法，以应对深度嵌套结构。

Result: 在三个真实企业系统中的实验表明，该框架在测试数据准备时间上最多可缩短 95%，测试覆盖率提升 80%。系统可处理多达 15 层嵌套的 protobuf 结构，并在数秒内生成超过 10 万条测试用例的完整测试集。

Conclusion: 该框架极大提升了基于 protobuf 的企业系统性能测试的效率与有效性，为复杂数据结构的自动测试提供了有力支持。

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [4] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: 本文提出TypyBench基准，系统评测大模型在Python全仓库类型推断上的能力，发现模型在复杂类型和一致性方面表现欠佳，建议未来聚焦类型一致性问题，工具与数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 动态语言如Python类型推断困难，现有LLMs在代码理解有进展，但其类型推断能力尚未深入系统评估。

Method: 提出TypyBench基准，包含TypeSim和TypeCheck两种新评测指标，系统评测多种大模型在50个高质量Python仓库上的类型推断能力。

Result: LLMs在TypeSim指标上表现尚可，但在复杂嵌套类型、代码库级别类型一致性方面仍有较大提升空间。TypyBench揭示了模型在不同类型复杂度和场景下的表现差异。

Conclusion: LLMs在Python类型推断上能取得不错的相似度分数，但在复杂嵌套类型和库级别的一致性上表现不足，未来应关注提升类型一致性。TypyBench为研究提供了新的基准和方向。

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [5] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: 该论文提出RedCoder系统，利用多Agent博弈和策略库，微调LLM自动与目标代码生成模型多轮交互，诱发其输出含漏洞代码，实验效果优于现有红队方法，提升了安全评估效率和广度。


<details>
  <summary>Details</summary>
Motivation: 当前的代码生成大型语言模型（Code LLMs）在AI辅助软件开发与测试领域表现突出，但研究发现其在对抗性场景下易生成存在安全漏洞甚至恶意的代码。现有红队测试方式依赖大量人工干预，难以扩展且忽视了现实AI编程多轮交互的特点。

Method: 提出RedCoder红队智能体，通过多Agent对抗游戏模拟攻击交互，收集对话原型与攻击策略。再以对话原型微调LLM，打造可自动与目标模型多轮交互、动态调用策略以引导模型输出存在漏洞代码的红队智能体。

Result: RedCoder可自主与多种Code LLMs进行多轮对话，动态利用攻击策略引导目标模型生成易受攻击代码。实验显示RedCoder在诱发代码漏洞能力上优于现有单轮与多轮红队测试方案。

Conclusion: RedCoder提供了一种可扩展、自动化的方式，显著提升了对现代代码生成模型安全边界的评估能力。

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [6] [Infinite Traces by Finality: a Sheaf-Theoretic Approach](https://arxiv.org/abs/2507.22536)
*Marco Peressotti*

Main category: cs.LO

TL;DR: 本文提出了一种系统性的sheaaf理论方法，结合Kleisli范畴和guarded递归，有效捕捉并刻画了系统的无限轨迹语义，完善了线性语义的理论基础。


<details>
  <summary>Details</summary>
Motivation: 在Kleisli范畴中，建模各种系统的线性行为是一个重要但尚未完全解决的问题，尤其是在利用终余代数（final coalgebra）捕捉无限轨迹语义方面还存在困难。以往的工作大多局限于有限轨迹语义，对无限情形缺乏系统方法。

Method: 提出一种基于层叠理论（sheaf-theoretic）的框架，将Kleisli范畴、序数上的层叠、保卫（guarded）(共同)递归结合，从有限近似的合并中系统性地构造捕捉无限轨迹语义的终余代数。特别引入了保卫行为函子，并分析其在温和条件下的性质。

Result: 在所提框架和适当的假设条件下，构造出的终余代数可以直接刻画系统的无限轨迹语义，从而为Kleisli范畴中无限轨迹提供了一般性的刻画方法。

Conclusion: 该工作首次提出并验证了一种系统性的方法，将Kleisli范畴与层叠理论及保卫(共)递归相结合，能够自动构造并刻画无限轨迹语义的终余代数。拓展了既有有限语义的理解，为系统的无限行为分析提供了新手段。

Abstract: Kleisli categories have long been recognised as a setting for modelling the
linear behaviour of various types of systems. However, the final coalgebra in
such settings does not, in general, correspond to a fixed notion of linear
semantics. While there are well-understood conditions under which final
coalgebras capture finite trace semantics, a general account of infinite trace
semantics via finality has remained elusive. In this work, we present a
sheaf-theoretic framework for infinite trace semantics in Kleisli categories
that systematically constructs final coalgebras capturing infinite traces. Our
approach combines Kleisli categories, sheaves over ordinals, and guarded
(co)recursion, enabling infinite behaviours to emerge from coherent families of
finite approximations via amalgamation. We introduce the notion of guarded
behavioural functor and show that, under mild conditions, their final
coalgebras directly characterise infinite traces.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [7] [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)
*Vanessa Rebecca Wiyono,David Anugraha,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文提出了首个由印尼语母语者创作、专用于LLM偏好评测的多领域数据集IndoPref，为印尼语和多语言大模型的评估与提升提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 印尼语有超过2亿使用者，但在大语言模型（LLMs）的偏好性研究中却严重缺乏代表性。现有多语言数据集大多来源于英文翻译，导致内容缺乏文化与语言的真实性。因此，研究动机是解决印尼语在LLM偏好数据中的不足，并提供本地化高质量的数据集。

Method: 本研究提出了IndoPref，这是首个完全由人类创作、涵盖多个领域的印尼语偏好数据集，用于评估LLM生成文本的自然性与质量。所有标注均由印尼语母语人士书写，并通过Krippendorff's alpha进行一致性验证。此外，作者还在多个LLM模型上进行了基准测试，并评估了各模型的输出质量。

Result: IndoPref数据集的标注显示了很强的一致性，能有效评估LLM的文本生成表现。基于该数据集，不同LLM在印尼语生成内容的质量上有了比较明显的对比和量化数据。

Conclusion: IndoPref填补了印尼语LLM评测数据集的空白，为后续提升LLM在印尼语及多语言场景下的能力提供了高质量标准和实测基线。该成果也为多语言AI公平性、真实性研究提供了有力支持。

Abstract: Over 200 million people speak Indonesian, yet the language remains
significantly underrepresented in preference-based research for large language
models (LLMs). Most existing multilingual datasets are derived from English
translations, often resulting in content that lacks cultural and linguistic
authenticity. To address this gap, we introduce IndoPref, the first fully
human-authored and multi-domain Indonesian preference dataset specifically
designed to evaluate the naturalness and quality of LLM-generated text. All
annotations are natively written in Indonesian and evaluated using
Krippendorff's alpha, demonstrating strong inter-annotator agreement.
Additionally, we benchmark the dataset across multiple LLMs and assess the
output quality of each model.

</details>
