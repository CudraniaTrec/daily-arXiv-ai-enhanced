<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.CL](#cs.CL) [Total: 50]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 通过（G）KAT理论，命题程序等价性问题变得可判定且实用，有助于程序形式化分析。


<details>
  <summary>Details</summary>
Motivation: 一般程序等价性问题是不可判定的，但如果对语句的语义进行抽象，能使该问题变得可判定并具有实际可行性。

Method: 从（Guarded）Kleene Algebra with Tests（（G）KAT）的视角，探讨命题程序等价性的新进展。

Result: 命题等价性不仅可判定，还为形式化分析和程序验证提供了理论基础与工具，提升了复杂程序分析的可行性。

Conclusion: 通过对程序语义进行抽象，可以决定并且切实应用命题等价性的判定，这为形式化程序分析提供实践可行的方法。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 该研究创建并发布了一个高质量、德语软件工程领域的情感分析数据集，填补了相关领域的空白，验证了数据集有效性，同时指出现有通用德语情感分析工具难以满足该领域需求。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程情感分析工具主要依赖英语或非德语的数据集，缺乏针对德语软件开发领域的数据与方法，从而影响对德语开发者团队情绪氛围的准确分析。

Method: 从德语开发者论坛Android-Hilfe.de爬取5949条开发者发言，采用Shaver等人的六种基本情绪模型，由四位德语计算机专业学生进行标注，并评估标注一致性及与现有情感分析工具的效果对比。

Result: 数据集的标注达到很高一致性，适用于德语软件工程情感分析。用现有德语情感分析工具进行实验，证实在该领域缺少适用的领域特定解决方案。

Conclusion: 该论文通过构建并验证了面向德语开发者社区的软件工程情感分析数据集，证明了这种数据集的有效性和可靠性，并揭示了现有工具在该领域的局限性。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [3] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 本研究提出并评估了一种自动从用户评论中提取解释性需求和生成说明的工具化方法，发现AI在生成清晰风格内容上有优势，但正确性仍需人工把关，并发布了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 当前，软件系统的可解释性需求日益增长，以提高透明度、增强用户信任并满足合规要求。但如何将用户反馈中的解释性需求转化为结构化需求和对应解释仍然具有挑战性。现有方法虽然可以识别用户评论中的解释相关问题，但缺乏系统性衍生需求及生成匹配解释的 established approach。

Method: 本文提出了一种工具支持的自动化方法，将用户评论自动转化为可解释性需求和相应解释。研究中与一家工业自动化制造商合作，创建了包含58条用户评论的数据集，每条评论都人工注释了可解释性需求和说明，对这种自动化方法进行了效果评估。

Result: 评估发现，AI自动生成的需求在相关性及正确性上常不及人工，但AI生成的解释在表述清晰和风格上更受青睐。尽管如此，正确性依然是主要问题，因此需要人工验证。

Conclusion: 本研究自动化方法能够从用户评论中提取可解释性需求并生成解释，为软件系统解释性需求的推进做出贡献。研究还揭示了自动生成成果的优势与局限，并公开了标注数据集以支持后续研究。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [4] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 本文提出结合AAS与BPMN的分布式自动化工程解决方案，有效提高了工程自动化、安全性及跨组织协作能力。


<details>
  <summary>Details</summary>
Motivation: 在工程领域中推动工业4.0技术的应用，实现工厂及流程工程的自动化和优化，需要高效的数据交换与互操作解决方案。

Method: 结合资产管理壳（AAS）与业务流程模型与标注（BPMN），提出一种分布式AAS copy-on-write基础设施，并开发了工作流管理原型系统以自动化AAS操作与工程任务。

Result: 分布式AAS基础设施提升了安全性和可扩展性，实现了跨组织协作。工作流原型系统自动化了AAS操作及工程流程，提高了工程效率和可追溯性。

Conclusion: AAS与BPMN结合可为工程自动化赋能，提出的基础设施和原型系统在提升工程流程自动化、安全性、可扩展性及协作性方面具有显著成效。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [5] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLM不能直接用传统需求文档生成高质量代码，需人工拆解需求后补充相关设计，再进行提示，传统的需求工程仍然不可替代。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM具备高级代码生成能力，但还不清楚在实际软件开发中开发者如何将需求整合到LLM驱动的开发流程中。

Method: 通过对14家公司的18名从业者进行访谈，分析他们在使用LLM生成代码时如何处理和利用需求及设计文档信息。

Result: 提出了一套描述开发者在用LLM生成代码时处理需求和设计信息过程的理论，指出需求需拆解为编程任务并补充设计细节后，才能有效用于LLM提示。

Conclusion: 研究表明，现有需求文档过于抽象，无法直接作为LLM生成代码的输入，还需开发者手动分解需求并补充设计与架构细节。因此，即便使用LLM，基础的需求工程工作依然不可或缺。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [6] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 本文为需求工程领域提示工程应用进行了首次系统综述，分析了现有方法、任务与模型局限性，提出了PE向标准化工作流演变的路线图，为未来研究与实践提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在需求工程（RE）领域的兴起，虽然提升了多项任务，但因其不确定性和可控性差，缺乏有效提示工程（PE）指导，限制了其在RE中的可信应用。作者旨在梳理和规范LLM在RE中PE方法的现状和发展路径。

Method: 本研究采用Kitchenham和Petersen的二次研究协议，系统性回顾了六大数字图书馆，共筛选867条记录，最终全面分析35项主要研究。作者提出了一种混合分类法，将技术导向的提示模式与任务导向的RE角色关联起来。

Result: 研究梳理了目前PE在RE中的多种技术模式、任务类型和LLM家族，系统揭示了当前方法的局限性和研究空白，并构建了将碎片化PE原型转化为可复现、易用工作流的路线图。

Conclusion: 本文首次系统性总结了PE在RE领域的研究现状，构建了分类体系与研究流程，并为未来发展提供了明确的改进方向和实践路径。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [7] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 本文提出了一种基于RAG和LLM的AI方法，自动化从太空任务文件生成需求。初步实验表明，该方法能降低人工负担、提升需求的完整性与合规性，特别有助于中小型企业参与复杂空间项目。


<details>
  <summary>Details</summary>
Motivation: 太空行业的需求工程（RE）极为复杂，需要高精度、严格标准的遵循与任务定制化。小型企业和新加入者在将大量、非结构化的任务文档转化为具体可执行需求时，面临显著挑战。

Method: 提出一个基于AI的模块化方法，利用检索增强生成（RAG）模型，对原始太空任务文档进行预处理、语义分类、与领域标准相关知识的内容检索，并通过大型语言模型（LLM）合成需求草案。

Result: 在与Starbound Space Solutions的行业合作中，将该方法应用于实际任务文档进行实验，初步结果显示可减少人工工作量、提升需求覆盖度并支持合规检查。

Conclusion: 该方法对小型企业降低参与大型、安全关键空间项目的门槛具有潜力。并提出将AI更广泛地整合进需求工程工作流的路线图。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [8] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 本研究通过因果实验发现，大语言模型的偏见主要源于预训练阶段而非微调或训练随机性，提示解决模型偏见需重视预训练过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）表现出类人认知偏见，但这些偏见究竟源自预训练、微调还是训练中的随机性尚不明确。该研究旨在厘清不同训练阶段对偏见的影响。

Method: 提出了两步因果实验方法：（1）多次用不同随机种子微调模型，分析训练随机性对30余种认知偏见的影响；（2）引入跨微调方法，交换不同模型的指令数据集，检测偏见是否依赖于特定数据集。

Result: 结果显示，虽然训练随机性确实带来一定偏差变异，但偏见主要由预训练阶段决定：拥有相同预训练背骨的模型，其偏见分布更相似。此外，仅共享微调数据集，对偏见模式影响有限。

Conclusion: 要理解和缓解微调后模型中的偏见，必须深入考虑预训练的影响，而不仅仅关注微调阶段。该发现为未来消解或评估LLM偏见提供了理论指导。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [9] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: LLM在替代人类参与社会科学调查时，会受问题措辞和选项顺序等扰动显著影响，存在新近偏差，提示设计与稳健性测试必不可少。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）正越来越多地被作为社会科学调查中的人类受试者替代，但目前对其可靠性与对既有反应偏见的敏感性了解较少。

Method: 作者选用九种不同的LLM，针对世界价值观调查（WVS）的问题，设计并施加了包括问题措辞和答案选项结构在内的11种扰动，累计生成了超过16.7万次模拟访谈，以检测模型在规范性调查语境下的反应稳健性。

Result: 所有测试模型对扰动都存在一定脆弱性，并且都表现出一致的"新近偏差"，即倾向于选择最后一个展示的选项。虽然模型规模较大的LLM更稳健，但所有模型依然容易受到语义变化（如同义改写）和组合扰动的影响。LLM确实在一定程度上复现了人类调查中的一些回应偏差。

Conclusion: 在利用LLM生成合成调查数据时，提示设计和稳健性测试至关重要，因为LLM不仅会受到已知的人类反应偏倚影响，也容易因问题表述和选项顺序的变化而发生显著偏移。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [10] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: 本文介绍了SynthTextEval工具包，可对合成文本在多个关键维度（如实用性、公平性、隐私等）进行全面且标准化的评估，推动其在医疗和法律等敏感领域的可行性，实现AI开发中的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）生成文本的流畅度提升，合成文本在隐私保护等高风险领域应用前景广阔，但对合成文本在多维度上的系统评价是实现其应用价值的前提。

Method: 提出了SynthTextEval工具包，支持用户上传或通过模块生成合成数据，并从下游系统实用性、公平性、隐私泄露风险、与源文本分布的差异、领域专家定性反馈等多维度进行全面评价。

Result: 该工具可用于任何数据，论文重点展示了其在医疗和法律等高风险领域数据集上的功能和有效性。通过标准化和整合评估指标提升合成文本的可用性和可信度，有利于AI开发中的隐私保护。

Conclusion: SynthTextEval为合成文本的全面评价提供了统一的平台，有助于推动合成数据在注重隐私的领域的实际应用。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [11] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文针对医疗领域大语言模型提出了多角色（患者、医生、通用用户）安全评估协议和数据集PatientSafetyBench，并通过案例验证，填补了现有研究空白，为医疗LLMs安全应用提供了评估工具和理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在医学等重要领域的广泛应用，其安全性问题愈发突出，尤其是在患者和临床医师等不同角色用户使用时，对人类健康构成潜在威胁。而以往的安全评估主要集中在通用基准，缺乏面向医疗场景和不同用户视角的系统性安全评估。

Method: 本文提出了一套专为医学领域设计的安全评估协议，分别针对患者、临床医生及一般用户三类视角，并结合通用安全评估进行定量分析。同时，作者构建了PatientSafetyBench数据集，涵盖5个关键类别，共466个样本，来从患者角度衡量医疗LLMs的安全性。此外，采用了定向red-teaming方法，并以MediPhi模型集为案例进行分析。

Result: 首次定义了针对医疗LLMs的分角色（患者、临床医生、通用用户）安全评估标准，通过red-teaming实证展示了PatientSafetyBench和相关协议的有效性，为后续医疗LLM的安全部署提供了体系化评估基础。

Conclusion: 本文填补了以用户不同角色视角评估医疗LLMs安全性的研究空白，并提供了可操作的评估协议和数据集资源，为推动医疗场景下LLMs的安全落地奠定了基础。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [12] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本文提出了一种可应用于多组重叠对话场景的鲁棒中断检测方法，并分析了中断的语言和音韵特征，推动了AI课堂对话监控的实际应用。


<details>
  <summary>Details</summary>
Motivation: 中断在协作学习中对群组互动和知识建构具有重要影响。AI可以帮助教师监测这些互动，但大多数已有的中断检测方法只适用于单一对话和音频干净的环境。由于真实课堂小组协作场景常常存在多组同时对话和大量重叠语音，现有方法难以直接应用，因此需要更加鲁棒的中断检测技术。

Method: 本文分析了单组对话和多组对话环境下的中断检测，并提出了一种对重叠语音具有鲁棒性的先进中断识别方法。同时，研究还深入挖掘了协作群体互动中中断发生的语言学及韵律特征。

Result: 提出的方法能够有效在有重叠语音干扰的多小组协作对话场景中检测并识别中断。此外，研究揭示了中断在群体对话中表现出的显著语言及音韵信息。

Conclusion: 该工作不仅提升了中断检测在多组重叠对话中的适用性，为AI在真实课堂的部署奠定基础，也为后续追踪多小组对话中重叠语音影响的相关研究提供了新思路与数据。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [13] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 本工作提出多智能体RAG框架，显著提升健康虚假信息反驳言论的相关性和准确率，在多项指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的大模型在对抗健康虚假信息的反驳言论生成方面表现突出，但证据利用有限且输出难以控制。为提升反驳言论的证据性和质量，作者希望提出更优方法。

Method: 提出多智能体的检索增强生成（RAG）框架，整合多个LLM以优化知识检索、证据强化和回应精炼，并融合静态与动态证据，生成更优反驳言论。通过消融实验和人工评测验证各模块必要性和整体质量提升。

Result: 该方法在礼貌性、相关性、信息量和事实准确性方面超越对比方法。消融实验显示各模块皆为必要，人工评测证明精炼环节显著提升了反驳言论质量并获得用户偏好。

Conclusion: 多智能体RAG框架生成健康虚假信息反驳言论时，能更好结合证据、优化控制，显著提高生成输出质量和可信度。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [14] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 本文针对现有Transformer在长文本处理中的高复杂度问题，提出了一种结合GNN和CNN的新型模型，通过高效的端到端图构建和LLM特征融合，实现了更高效和有竞争力的文本分类效果，尤其适合长文档场景。


<details>
  <summary>Details</summary>
Motivation: Transformer架构因其对输入长度呈二次复杂度的计算方式，在处理长文本时存在时间、成本和能耗等效率问题。本文旨在突破现有Transformer在长文档处理上的效率瓶颈。

Method: 提出了一种结合了图神经网络（GNN）和卷积神经网络（CNN）的新型模型架构，并集成了实时端到端的图生成机制。该模型采用字符级输入的小批量处理，无需填充或截断，通过高效的字典查找方式引入大型语言模型（LLMs）提供的信息（如token嵌入和情感极性）。局部上下文通过CNN建模，局部感受野扩展和文档级信息聚合通过基于格子的图结构和小世界图实现。

Result: 实验分析了生成图的结构属性，平均聚类系数约为0.45，平均最短路径长度在4到5之间，表现出有意义的语义组织能力。模型在情感分析、新闻分类等多项文本分类任务上进行测试，对比了主流SOTA模型，验证了新模型的高效性和有竞争力的性能。

Conclusion: 所提出的GNN+CNN图生成模型在效率和性能上均达到了与Transformer等SOTA模型相当或更优的水平，尤其适合处理长文本，能够显著降低计算资源消耗。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [15] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出MedReadCtrl，一种可读性可控的指令调优方法，显著提升医疗大模型对不同健康素养用户的表达匹配能力，优于GPT-4，并促进AI医疗公平。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域有巨大潜力，但其能否将内容个性化并易于理解，直接影响实际部署，尤其是在面对不同健康素养用户时。如何实现AI输出内容在复杂度可控、意义不损失，是当前的关键挑战。

Method: 提出了MedReadCtrl，一种控可读性指令调优框架，通过调整大模型（LLM）输出内容的复杂度，使内容既能满足指定的易读性需求，又保证医学信息的准确传达。此方法在九个医疗和通用数据集、三项任务中进行了广泛评测。

Result: MedReadCtrl在可读性指令遵循方面优于GPT-4（如ReadMe数据集上1.39 vs. 1.59，p<0.001），并在临床任务迁移上取得显著提升（如MTSamples上ROUGE-L提高+14.7，SARI +6.18）。专家评估中，MedReadCtrl的内容被优选率为71.7%（对比GPT-4为23.3%），尤其对低健康素养用户更有优势。

Conclusion: MedReadCtrl能有效把临床内容重构为更加易读且符合期望可读性的表达，同时保持医学原意。这为患者教育和AI医疗服务的公平推广提供了可扩展的新方案。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [16] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型与自动优化的新方法，从医学文本高效提取住房驱逐信息，构建了最大公开数据集，显著提升了自动化抽取准确率且大幅降低人工工作量，对SDoH研究有重要意义。


<details>
  <summary>Details</summary>
Motivation: 驱逐作为一个重要却研究不足的社会健康决定因素，虽在非结构化临床记录中出现，却难以被结构化编码，限制了后续研究与应用。因此需开发高效方法从文本中提取相关信息。

Method: 提出SynthEHR-Eviction pipeline，将大语言模型（LLMs）、人工参与标注和自动化提示优化（APO）结合，实现临床文本中驱逐相关SDoH信息抽取，并在此基础上微调如Qwen2.5、LLaMA3等模型。

Result: 微调的LLMs（Qwen2.5、LLaMA3）在新构建数据集上的相关分类任务Macro-F1分别达到88.8%（驱逐）和90.3%（其他SDoH），均超越GPT-4o-APO、GPT-4o-mini-APO和BioBERT，且流水线可减少80%+的标注工作量，加速数据集构建，支持其他信息抽取任务。

Conclusion: 通过SynthEHR-Eviction流水线，能够高效从EHR中提取细粒度驱逐相关社会健康决定因素（SDoH）信息，不仅准确性提升，还可显著缩减人工标注工作量，并具有很好的泛化能力。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [17] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 本文提出了一种利用多模态大模型生成自然语言注释并对小型语言模型进行时序推理能力微调的框架。实验结果显示，小型模型在时序趋势、噪声和极值检测等方面具备可解释的推理能力，适用于数据隐私和设备端部署场景。


<details>
  <summary>Details</summary>
Motivation: 时序数据分析在许多领域非常重要，但现有的大型模型难以部署在资源受限或需要隐私保护的环境中。作者希望将复杂时序推理能力压缩进体积小、可解释性强的语言模型中，实现更广泛的应用。

Method: 作者首先构建了具有可控趋势和噪音的均值回归时间序列合成数据集，并利用大型多模态模型自动生成相关自然语言注释，再用这些注释对小型Qwen语言模型进行指令微调。此外，作者还设计了趋势方向、噪声强度和极值定位等新的评估指标，系统评估模型的时序推理能力。

Result: 经过微调的小型Qwen模型能够获得有意义的时序解释能力，在趋势判断、噪声识别、极值判定等任务上表现良好，结果显示可将时序推理能力有效蒸馏进紧凑的语言模型。

Conclusion: 实验展示了通过多模态大模型注释和新设计评测体系，可以高效训练小模型掌握时序推理功能，为可解释、可部署的时序模型发展奠定基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [18] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: SAND框架使LLM智能体能自我推理和权衡备选动作，显著提升决策表现，优于传统微调和最新智能体训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）智能体在行为训练时普遍依赖专家示范或偏好优化。这些方法主要关注模仿专家行为或区分选中的和被拒绝的推理过程，但缺乏对备选动作的系统分析，导致智能体容易陷入次优策略。

Method: 提出了一种名为Self-taught ActioN Deliberation（SAND）的新框架，允许LLM智能体在采取行动前对候选动作进行显式推理和权衡。该方法结合了自洽性动作采样和基于执行的动作评估，并利用智能体自身的基础模型进行逐步推理过程生成，生成的推理轨迹再用于微调智能体模型。

Result: 在两个典型的交互式智能体任务上，SAND框架比初始的监督微调平均提升了20%，同时优于当前先进的智能体训练方法。

Conclusion: SAND显著提升了LLM智能体的决策质量，通过自我推理和权衡有效克服了以往方法中对行动探测不足的问题，实现了更优的任务表现。

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [19] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: 提出RLEP机制，通过回放高质量训练样例，有效提升大语言模型强化学习稳定性和性能，显著提高数学相关任务准确率。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型上使用强化学习（RL）训练存在能耗高、训练过程不稳定、策略权重偏移等问题。为了解决这些问题，需要提升学习稳定性和效率，并缓解策略漂移。

Method: 提出RLEP（Reinforcement Learning with Experience rePlay）两阶段框架，第一阶段收集经过验证的轨迹，第二阶段在训练过程中将这些轨迹回放并与新生成的数据混合组成mini-batch，进行策略优化。通过高质量轨迹回放，引导模型聚焦于有效探索路径，提升训练效果。

Result: RLEP在Qwen2.5-Math-7B基础模型上，以更少的更新步骤达到基线最高准确率，最终超越基线表现。在AIME-2024准确率从38.2%提升到39.9%；AIME-2025从19.8%提升到22.3%；AMC-2023从77.0%提升到82.2%。

Conclusion: RLEP方法能稳定训练过程，加速收敛，提高模型最终表现，证明了高质量经验回放机制在大语言模型RL微调中的有效性。代码、数据集和模型权重均已开源，为相关研究提供便利。

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [20] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 论文提出了“machine bullshit”作为描述LLM失真现象的新框架，开发了定量指标和分类体系，并通过大规模实验证明RLHF和特定推理方式会加剧该问题，强调该现象对于AI对齐带来的系统性挑战。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM研究更多关注于幻觉和阿谀现象，还缺乏对机器输出中“对真理冷漠、仅为应付的无内容陈述”这一更广泛现象的系统性理论和量化工具。提出“machine bullshit”作为统摄性框架，以更深入理解和刻画这一对LLM准确性的威胁。

Method: 1. 提出Bullshit Index指标和四种bullshit分类法。2. 在Marketplace、Political Neutrality和自建BullshitEval三个数据集上进行实验和评估。3. 分析RLHF和推理时链式思考（CoT）对bullshit产生的影响。

Result: 1. RLHF微调后LLM的bullshit水平显著上升。2. CoT推理会放大如empty rhetoric和paltering等bullshit形式。3. 政治场景下机器使用weasel words等bullshit策略更常见。4. Bullshit Index和分类体系能够有效刻画和量化LLM的truthfulness丧失。

Conclusion: 通过实证结果发现，当前LLM尤其是在经过人类反馈强化学习（RLHF）微调后，会显著加剧“bullshit”现象，不同推理提示方式也会放大某些类型的bullshit，尤其是在政治语境下更为明显。该研究对LLM可靠性和对齐提出了新的系统性挑战。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [21] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: 提出了一种从大模型提取任务分解轨迹再用于小模型微调（PLAN-TUNING）的新方法，实现了小模型复杂推理性能的大幅提升，在多个基准测试集上效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）通过将复杂问题分解为简单子任务（即类人规划方式）极大提升了处理能力，但如何在小型开源LLM的训练后阶段挖掘并利用这种规划结构仍然研究较少。

Method: 提出PLAN-TUNING方法：统一的后训练框架。包括两个步骤：一是从大规模LLMs中提取合成式任务分解（称为“规划轨迹”）；二是通过监督学习和强化学习目标，使小模型拟合这些规划过程，从而提升复杂推理能力。

Result: 在GSM8k和MATH基准上，经过plan-tuning的小模型比现有强基线平均提升约7%。在OlympiadBench和AIME 2024等域外数据集上，plan-tuned模型提升效果更明显，分别提升约10%和12%。

Conclusion: 展示了引入规划轨迹对小型LLMs复杂推理能力的显著提升，证明了PLAN-TUNING是一种有效提升小型LLMs任务性能的方法。

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [22] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: 本文提出了TeaR方法，结合数据筛选与强化学习，有效提升了大模型在推理相关任务中的表现，特别是在代码推理等领域，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型（LLM）在推理能力上的增强是研究的重点，但直接让模型模拟代码执行往往导致模型过度依赖复杂的数据结构和算法，反而影响了对核心推理能力的提升。解决如何引导模型获得更优推理路径，避免对表层算法模式的过拟合问题，是本研究的动机。

Method: 提出了TeaR方法，通过精细构建的数据集和强化学习，引导模型在代码相关任务中探索最优推理路径，从而提升模型在推理任务上的表现。实验覆盖了不同规模的基础模型和长链思维链（long-CoT）蒸馏模型，参数量从1.5B至32B，涵盖17项包括数学、知识、代码及逻辑推理基准。

Result: 在多种模型和基准测试上的实验结果均显示出显著的性能提升。例如，TeaR在Qwen2.5-7B模型上提升了35.9%，在R1-Distilled-7B模型提升了5.9%。

Conclusion: TeaR方法能够有效提升大模型在代码等复杂推理任务上的表现，通过合理的数据筛选与强化学习引导，缓解了模型对复杂算法模式的过拟合，有助于提升核心推理能力，泛化能力更强。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [23] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: 本文提出采用融合多种BERT变体的DyGIE++方法自动抽取燃料电池ORR催化剂文献信息，结果表明领域专用模型在抽取任务上表现最佳，为材料信息学领域大规模文献分析提供了有效手段。


<details>
  <summary>Details</summary>
Motivation: 燃料电池效率的提升依赖于氧还原反应（ORR）催化剂，而从庞杂的科学文献中结构化提取相关信息非常困难，因此亟需自动化和高效的信息抽取方法。

Method: 本文提出使用基于DyGIE++的命名实体识别（NER）和关系抽取（RE）方法，结合多种预训练BERT模型（如MatSciBERT、PubMedBERT），建立燃料电池领域的材料信息语料库（FC-CoMIcs）。通过手动标注12类关键实体及2类实体关系，集成并微调transformer模型以提升信息抽取准确率。

Result: 微调后的PubMedBERT模型在NER任务上获得最高F1分数82.19%，MatSciBERT模型在RE任务上取得最高F1分数66.10%。域专用BERT模型在ORR催化剂信息抽取上优于通用科学模型BlueBERT。

Conclusion: 基于微调的领域专用BERT模型能够可靠、高效地从科学文献中自动抽取ORR催化剂相关信息，具有大规模文献分析的潜力。

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [24] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: LLM对于高复杂度任务的推理与验证能力有限，面对复杂计算任务会出现不可避免的幻觉和错误，这为其真实世界智能体应用带来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer基础的大型语言模型（LLM）在AI领域的广泛应用，人们对其极限能力产生浓厚兴趣，尤其关注其"幻觉"（即生成错误或荒谬信息）的现象。同时，越来越多的研究关注将LLM作为自主或半自主智能体，用于实际世界中的各种任务。因此，理解LLM能否胜任不同任务非常重要。

Method: 本文从LLM推理的计算复杂性角度，分析其能力边界。具体方法包括对比不同复杂性任务，分析LLM在超出某一复杂性上无法完成推理与验证的现象，并通过具体案例进行论证。

Result: 研究结果表明：LLM在面对超出一定计算复杂度的计算和代理任务时能力有限，且无法验证这类任务的准确性。文中提供了实例支持这一结论。

Conclusion: LLM的能力在计算复杂性上存在本质边界，尤其是在高复杂性任务的推理与验证方面，这限制了其在复杂实际场景中作为智能体的应用潜力。

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [25] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 本论文提出了一套利用有限真实数据和专家知识微调大语言模型的心理支持对话生成体系，构建了大规模中文心理对话数据集，并开发了先进的综合支持系统，在多个任务和数据集上达到了最优性能。


<details>
  <summary>Details</summary>
Motivation: 随着社会压力的增加，人们对心理支持的需求日益增长。然而，尤其是在非英语语种中，相关数据集十分稀缺，限制了智能心理支持系统的发展。

Method: 提出了一种结合有限真实世界数据和专家知识的框架，对两个大型语言模型（对话生成器和对话修饰器）进行微调。生成器基于预定义路径大规模生成心理咨询对话，修饰器使其更贴合真实数据。通过自动和人工审核，构建了汉语心理支持对话数据集CPsDD。此外，设计了综合智能对话支持系统（CADSS），包括Profiler、Summarizer、Planner和Supporter四个核心模块。

Result: 构建了涵盖13大类、16种心理问题、13种成因和12种支持重点的6.8万条中文心理支持对话数据集CPsDD。提出的CADSS系统在策略预测与情感支持对话任务上，在CPsDD和ESConv数据集上均达到SOTA水平。

Conclusion: 结合有限真实数据和专家经验，利用大语言模型大规模生成高质量中文心理支持对话，有效缓解了中文心理支持数据稀缺问题，并证明了综合智能支持系统在核心心理对话任务中的领先性能。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [26] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本研究首次将VAP方法拓展到三人对话，实现了对未来说话权转移的预测，并取得优于基线的方法效果，对不同对话类型的适应性也有所考察。未来计划应用于真实多方语音对话系统。


<details>
  <summary>Details</summary>
Motivation: 传统语音对话中的轮流交互研究多聚焦于双人对话，缺乏对多人（尤其是三人）对话中轮流机制的探索。

Method: 将语音活动投影（VAP）模型扩展到三人会话场景，使用日语三人对话数据集训练多个模型，仅通过声学数据预测每个说话者未来的语音活动。

Result: VAP模型在三人会话数据上的表现优于基线方法，但对话类型对准确率有影响。

Conclusion: 首次验证了VAP方法可用于三人对话场景中的轮流预测。未来将该方法整合到实际语音对话系统中。

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [27] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 大语言模型结合少样本提示，在多语种主观性检测上，尤其是在低质量或低资源条件下，能匹敌甚至超越传统微调模型，是高效、泛化性强的新选择。


<details>
  <summary>Details</summary>
Motivation: 跨语言主观性检测任务面临数据稀缺与噪声复杂的问题，传统微调方法依赖高质量标注数据，存在适应性和泛化能力局限。作者旨在探索大语言模型（LLM）在少样本提示（few-shot prompting）下能否有效替代或超越微调的小型模型，提升多语种主观性检测效果，尤其是在低质量或低资源场景下。

Method: 采用大语言模型，结合精心设计的少样本提示（few-shot prompts）进行多语种主观性检测任务对比实验。实验涉及各种高级提示工程技巧（如辩论式LLM和不同样例选择策略），以评估其在CheckThat! 2025评测任务1中的表现，并与微调的小模型（SLM）进行系统对比。

Result: 在CheckThat! 2025多语种主观性检测任务中，该方法在阿拉伯语和波兰语获得第一名，在意大利语、英语、德语及多语种赛道均进入前四。尤其在阿拉伯语数据集上表现异常稳健，推测与对标注不一致性的鲁棒性有关。

Conclusion: LLM结合合理的少样本提示能够有效适应多语种主观性检测任务，在数据稀缺或质量不高的情况下表现突出，是传统微调方案的有力替代选择。即使使用更复杂的提示工程，提升有限，说明标准few-shot prompts已具备较高表达与泛化能力。该方法对于多语种情感分析等相关任务具有广泛应用前景。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [28] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 本文首次系统性分析阿拉伯语-英语企业域跨语种RAG，发现检索端是性能瓶颈，并提出简单的平均跨语种检索策略，大幅提升跨语种RAG表现。


<details>
  <summary>Details</summary>
Motivation: 此前跨语言RAG研究多基于开放域（如Wikipedia），在语言不平衡和重叠预训练数据下，检索难题被掩盖。实际企业场景的跨语言RAG存在严重信息检索瓶颈，有必要系统性研究真实多语场景下的检索与生成能力。

Method: 基于真实企业语料构建阿拉伯语-英语跨语言检索-生成基准，系统测试查询和支持文档的语言任意组合，分析检索器跨语言排名难点。提出强制从两种语言中平均检索文档的简单新策略，以缓解跨语言检索失败问题。

Result: 当用户查询和支持文档为不同语言时，检索性能明显下降，成为跨语种特定领域RAG的瓶颈。采用平均跨语言检索策略后，跨语种与总性能大幅提升。

Conclusion: 多语种（特别是企业真实场景）RAG检索端存在显著提升空间。仔细设计检索策略能有效改善跨语种信息检索和生成表现。该工作为实际RAG应用多语检索提供可行改进思路。

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [29] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: 本文系统分析了长链思维监督微调与强化学习在视觉语言模型推理中的作用及其结合方式。SFT有利于复杂推理但易生成冗长答案，RL有利于简洁与泛化，但协同训练未取得预期互补效果，反而产生性能权衡，提示需探索更自适应联合方法。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT SFT与RL在纯语言模型中能协同提升推理能力，但其在视觉语言模型中的联合效果尚不明确，因此有必要系统探究两者在多模态推理中的作用及结合方式。

Method: 通过系统性实验，分别和联合使用长链思维（CoT）监督微调（SFT）及强化学习（RL），在多个多模态推理基准上分析其各自和联合对模型性能的影响，采用多种混合训练策略（如两阶段、交错、渐进式训练、数据混合、模型融合）进行对比评估。

Result: SFT提升了模型对复杂问题的结构化推理能力，但使答案更冗长，对简单问题表现下降；RL提升了回答简明性和泛化能力，在各难度问题上均有提升，但对高难题的提升不如SFT显著。多种联合训练方法均未取得预期的互补增益，而是出现权衡，未能实现协同提升。

Conclusion: 长链思维（CoT）监督微调（SFT）和强化学习（RL）在大型视觉语言模型（VLMs）中的联合效果存在权衡，二者结合并未带来预期的协同优势，反而在准确率、推理风格和回答长度上产生权衡。要充分结合这两种后训练技术，仍需更无缝且自适应的方法。

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [30] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 本文提出了M4Doc框架，将图像编码器与多模态大模型对齐，解决了文档图像机器翻译泛化和效率难题，在多个场景下显著提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: Document Image Machine Translation (DIMT) 由于训练数据有限，以及视觉与文本信息复杂交互，导致泛化能力不足。研究需要提升DIMT效率和跨领域泛化能力。

Method: 提出了M4Doc框架。该框架将仅图像编码器与多模态大型语言模型(MLLM)的多模态表示对齐，在大规模文档图像数据集预训练。训练时通过对齐学习视觉-文本相关性，推理时跳过MLLM，提升推理效率。

Result: 实验表明，M4Doc在翻译质量上，尤其在跨领域泛化和复杂文档图像场景下有显著提升。

Conclusion: M4Doc有效提升了DIMT在各类场景下的表现，解决了泛化与效率问题。

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [31] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: 作者证明了离散扩散语言模型的去噪器在特定条件下能恢复干净token的后验分布，并提出了一种无需额外训练，能够提升推理性能和不确定性估计的集成方法，在WikiText-2上大幅优于GPT-2 Small，相关代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前离散扩散语言模型在推理和建模中对噪声处理和生成结果可信度估计还有不足，缺乏有效的后验概率推断与不确定性量化方法。

Method: 证明在最小假设下，通过对K次独立的掩码-去噪过程进行Monte Carlo边际化，可以O(1/sqrt(K))的速率逼近干净令牌的后验分布，且给出一致性和有限样本误差界。同时提出一种推理时的轻量级集成方法，通过平均K次掩码-去噪来获得后验感知的概率和不确定性估计，无需增加额外训练成本。

Result: 在WikiText-2数据集上，使用K=8时，该方法测试困惑度达到8.8，显著优于体量相当的GPT-2 Small（困惑度为20.3）。代码已开源。

Conclusion: 本文揭示了离散扩散语言模型的贝叶斯本质，并基于此提出了有效的集成推理方法，大幅提升了模型性能和推断后验概率的能力，不增加额外训练成本。

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [32] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 通过知识蒸馏技术，可大幅压缩大语言模型体积，学生模型在问答任务中仅用一半多的参数即可保有教师模型九成以上性能，且通过简易的提示方式进一步提升效果，非常适合受限资源环境应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然在多种NLP任务上表现优异，但庞大的计算资源需求限制了其在实际资源有限环境中的应用。本文旨在探索在保持强性能的前提下，LLMs通过知识蒸馏（KD）方式压缩的可行程度，特别是在问答任务中的表现。

Method: 采用知识蒸馏方法，将Pythia和Qwen2.5模型家族压缩为更小的学生模型，并在两个问答基准数据集SQuAD和MLQA上评估其性能。测试场景涵盖零样本和单样本提示（prompting）。

Result: 学生模型在参数量最多减少57.1%的情况下，依然保留了超过90%的教师模型性能。单样本提示较零样本提示还能带来额外的性能提升。

Conclusion: 知识蒸馏结合极简提示可在保证问答能力的同时，大幅缩减模型规模，为资源有限场景下的部署提供了可行方案。

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [33] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 通过优化提示和精简型微调方法，在不损失复杂问答准确率的情况下，能显著减少检索次数，提升推理效率，并打破需大规模数据微调的常规认知。


<details>
  <summary>Details</summary>
Motivation: 当前主流的方法在复杂问答任务中通过大模型迭代检索和推理获取答案，学界多关注提升检索增强生成（RAG）的准确率和召回率，但对推理效率（如检索次数）关注较少。本文关注在保持高性能的同时，降低检索开销的问题。

Method: 作者比较了常规大规模微调、新颖提示工程，以及基于监督和强化学习微调策略对RAG的影响。尤其考察了ReAct流水线结合改进提示的效果，以及监督微调和RL微调在减少推理时检索次数方面的作用。

Result: 改进后的ReAct流水线在HotPotQA等基准上优于SOTA，无需大规模带思维链追踪的QA数据微调即可提升表现。监督和RL微调能以几乎一半的检索成本获得可比的RAG指标，并且训练示例需求量少，对延迟和效率极具提升。

Conclusion: 无需大规模数据微调，通过更优提示工程和小规模监督/RL微调可提升复杂问答RAG表现并大幅降低推理中的检索次数，实现性能与效率兼得。

Abstract: We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


### [34] [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
*Haotan Guo,Jianfei He,Jiayuan Ma,Hongbin Na,Zimu Wang,Haiyang Zhang,Qi Chen,Wei Wang,Zijing Shi,Tao Shen,Ling Chen*

Main category: cs.CL

TL;DR: 本文系统分析并实证揭示了中文语音变体逃避有害内容检测的难题，构建真实数据集发现当前主流模型表现不佳；通过拼音提示策略可有效提升检测准确率，并为后续内容安全检测研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 在中文内容审核中，用户常用同音词、近音词等手段隐蔽表达有害意图（即Phonetic Cloaking Replacement, PCR），造成内容检测的难题，但现有方法多以规则或合成扰动为主，未能反映真实用户行为的创造性。

Method: 作者将PCR系统性地分为四类表层形式，并收集了一个包含500条真实发生的、经过语音变体处理的攻击性文本数据集（来自RedNote平台）。利用该数据集对最新大语言模型检测能力进行基准测试，并通过误差分析，尝试了基于拼音的提示策略。

Result: 测试表明，最好的大模型F1-score仅为0.672，用零样本链式思考法反而表现更差。经误差分析后发现，早前被认为无效的基于拼音的提示策略却能显著提高检测准确率。

Conclusion: 本研究首次系统梳理了中文PCR的类型，发布了真实的数据集，证明了现有模型在现实情况下检测能力有限，并提出了一种简便有效的缓解思路（拼音提示法），推动了有害言论健壮检测的前沿探索。

Abstract: Phonetic Cloaking Replacement (PCR), defined as the deliberate use of
homophonic or near-homophonic variants to hide toxic intent, has become a major
obstacle to Chinese content moderation. While this problem is well-recognized,
existing evaluations predominantly rely on rule-based, synthetic perturbations
that ignore the creativity of real users. We organize PCR into a four-way
surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring,
phonetically cloaked offensive posts gathered from the RedNote platform.
Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:
the best model reaches only an F1-score of 0.672, and zero-shot
chain-of-thought prompting pushes performance even lower. Guided by error
analysis, we revisit a Pinyin-based prompting strategy that earlier studies
judged ineffective and show that it recovers much of the lost accuracy. This
study offers the first comprehensive taxonomy of Chinese PCR, a realistic
benchmark that reveals current detectors' limits, and a lightweight mitigation
technique that advances research on robust toxicity detection.

</details>


### [35] [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
*Andrew D. Foland*

Main category: cs.CL

TL;DR: 本论文提出NOIR指标，可自动平衡评价摘要的语义保留与长度压缩，在无需人工参考摘要的情况下，有效对应人工评价，适用于多种摘要场景。


<details>
  <summary>Details</summary>
Motivation: 目前摘要评估方法大多依赖人工参考摘要或仅关注摘要压缩率，缺乏同时衡量语义保留与长度压缩之间权衡的客观指标。

Method: 提出了一种名为NOIR（NOrmed Index of Retention）的新颖量化指标，利用语言模型嵌入度量语义相似性，结合摘要长度压缩，自动评估摘要质量。

Result: 实验表明，NOIR能够有效刻画摘要系统在语义保留和长度压缩间的权衡，并与人工主观评价高度相关。

Conclusion: NOIR为各类自动摘要任务提供了无需人工参考、可广泛应用的自动评价工具，能促进摘要方法和提示的改进。

Abstract: This paper proposes NOrmed Index of Retention (NOIR), a quantitative
objective metric for evaluating summarization quality of arbitrary texts that
relies on both the retention of semantic meaning and the summary length
compression. This gives a measure of how well the recall-compression tradeoff
is managed, the most important skill in summarization. Experiments demonstrate
that NOIR effectively captures the token-length / semantic retention tradeoff
of a summarizer and correlates to human perception of sumarization quality.
Using a language model-embedding to measure semantic similarity, it provides an
automated alternative for assessing summarization quality without relying on
time-consuming human-generated reference summaries. The proposed metric can be
applied to various summarization tasks, offering an automated tool for
evaluating and improving summarization algorithms, summarization prompts, and
synthetically-generated summaries.

</details>


### [36] [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Yuehao Wang,Peihao Wang,Jing Xiong,Liliang Ren,Hao Cheng,Janardhan Kulkarni,Yelong Shen,Atlas Wang,Mac Schwager,Anderson Schneider,Xiaodong Liu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 本文提出了SAS和PEAA方法，可用极少的参数增加，模拟更大头数和维度的注意力机制，实验证实在多任务上带来性能显著提升，优于常规注意力变体。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是Transformer架构的核心组件，现有的方法如多头注意力（MHA）等已经取得了一定成果。作者通过分析MHA发现，增加注意力头的数量和每头的隐藏维度（在参数开销较小的条件下）有助于提升性能，从而提出了进一步增强注意力容量、提高模型性能的需求。

Method: 提出了一种模拟注意力得分（Simulated Attention Score, SAS）的方法，通过将低维的头部表示投影到高维空间，实现了更大头数和更大每头隐藏维度的效果，而无需实质增加参数量。此外，作者提出了参数高效的注意力聚合（PEAA），进一步控制模型参数的增加。

Result: SAS方法通过实验证明，在多种数据集和任务上均显著提升了性能，并且在参数量不变的前提下优于现有的多种注意力机制变体。

Conclusion: SAS和PEAA能够以极低的参数开销，模拟更大容量的注意力机制，实现性能大幅提升，为Transformer类模型提升能力提供了新思路。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Various methods have been developed to compute attention scores, including
multi-head attention (MHA), multi-query attention, group-query attention and so
on. We further analyze the MHA and observe that its performance improves as the
number of attention heads increases, provided the hidden size per head remains
sufficiently large. Therefore, increasing both the head count and hidden size
per head with minimal parameter overhead can lead to significant performance
gains at a low cost. Motivated by this insight, we introduce Simulated
Attention Score (SAS), which maintains a compact model size while simulating a
larger number of attention heads and hidden feature dimension per head. This is
achieved by projecting a low-dimensional head representation into a
higher-dimensional space, effectively increasing attention capacity without
increasing parameter count. Beyond the head representations, we further extend
the simulation approach to feature dimension of the key and query embeddings,
enhancing expressiveness by mimicking the behavior of a larger model while
preserving the original model size. To control the parameter cost, we also
propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive
experiments on a variety of datasets and tasks demonstrate the effectiveness of
the proposed SAS method, achieving significant improvements over different
attention variants.

</details>


### [37] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: 本文提出的K2RAG系统通过结合向量检索、知识图谱和摘要，大幅提升了RAG检索的准确率和效率，并降低了硬件资源消耗，是扩充大语言模型知识的新型优秀方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）不断扩展，现有的微调方法在时间和计算资源上的消耗日益严重，虽然已有一些优化技巧，但由于LLMs体量和复杂性的持续增长，快速高效地扩充模型知识成为新的挑战。RAG技术可以通过外部知识库实现扩充，但其原始实现依然面临可扩展性与答案准确率的瓶颈。因此，亟需一种更优的知识扩展方法。

Method: 提出了一种新颖的KeyKnowledgeRAG（K2RAG）框架，借鉴分而治之思想，集成了稠密与稀疏向量检索、知识图谱和文本摘要技术，以提升检索质量和系统效率。该框架还包含训练数据预处理与摘要步骤，显著减少整体训练耗时。

Result: 在MultiHopRAG数据集上评估，K2RAG在答案相似度上显著优于常规RAG方法（平均得分0.57，第三四分位Q3为0.82）。此外，摘要步骤将单组件训练时间缩短了93%，执行速度比基于知识图谱的传统RAG快40%，并且显存需求低至其他RAG的三分之一。

Conclusion: K2RAG不仅提升了答案准确率，还在时间、算力和显存等方面显著优于传统方法，证明其在RAG系统中具备良好的实用性和扩展性。

Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [38] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

TL;DR: 论文复现并扩展了Vec2Text嵌入还原方法，发现文本嵌入易被还原，尤其在理想环境下，敏感信息（如密码）也可能被恢复。针对风险，加入高斯噪声或量化可显著提高隐私安全性，建议对NLP系统采用更强的防御措施。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入作为NLP核心技术被认为能够保护隐私，但近期研究（如Vec2Text）表明，嵌入可以被还原成原始文本，这引发了对嵌入隐私安全性的质疑。该论文基于此动机，对Vec2Text方法的可复现性和局限性进行了探究。

Method: 本文复现了Vec2Text框架，通过以下两方面进行评估：（1）验证原有结论，包括原文中内域和外域实验，（2）通过参数敏感性分析、敏感信息还原实验（如密码）以及嵌入量化防御实验，扩展了原有的研究。

Result: 实验结果显示，在理想条件下Vec2Text能够高效还原原文本，甚至能还原无明显语义的密码类序列，但对输入长度较敏感。添加高斯噪声和量化处理能显著降低Vec2Text的还原能力，其中量化方法实现简单且具有较强通用性。

Conclusion: 文本嵌入存在较高的隐私泄露风险，尤其是在高维空间可控还原技术下。应对措施如嵌入量化和添加噪声能在一定程度上防御还原攻击，未来需关注NLP系统中更健壮的隐私保护机制。

Abstract: Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [39] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

TL;DR: 该论文提出Selective-DPO，通过重点对齐高影响力token，实现大模型对齐的高效和高质量。实验证明这一方法优于传统对齐与蒸馏方案，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的训练后对齐是一个重要难题，因为并非所有token对模型性能的贡献相同。提高对齐质量并降低不必要的计算开销是一大挑战。

Method: 提出了一种选择性对齐（Selective Alignment）策略，通过对偏好对中的高影响力token加权优化。这一策略利用当前模型和参考模型在token级别的对数概率差异，重点优化那些信息量大的token。同时也探究了参考模型质量对token选择和优化效果的影响。

Result: 实验证明，该方法（Selective-DPO）在Arena-Hard和MT-Bench等基准测试上，优于标准DPO和基于蒸馏的主流方法。在强化token级优化和参考模型选择方面表现突出。

Conclusion: 通过聚焦高影响力token和优质参考模型，Selective-DPO在LLM偏好对齐方面取得了更高的效率和表现。该方法为后续大模型对齐和优化方向提供了有效思路。

Abstract: Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [40] [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
*Maha Tufail Agro,Atharva Kulkarni,Karima Kadaoui,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文系统性综述了端到端语音识别在语码转换问题上的研究，总结了已有工作、挑战和未来发展方向，为后续研究提供了参考和指导。


<details>
  <summary>Details</summary>
Motivation: 近年来自动语音识别（ASR）研究不断增加，尤其是针对经常出现语码转换（CS）的语言，因此需要对端到端ASR模型领域的相关研究进行系统性梳理。

Method: 收集并人工标注在同行评审会议发表的相关论文，梳理并分析涉及的语言、数据集、评测指标、模型选择及性能表现，同时讨论端到端ASR在语码转换场景下的挑战。

Result: 详细记录了当前在端到端ASR中处理语码转换的研究现状，包括所用语言、数据集、评价指标、主流模型及其性能，并对当前挑战进行了讨论，提供了未来研究的机会和指引。

Conclusion: 该综述系统总结了端到端ASR在语码转换领域的研究进展与挑战，为未来的研究方向和资源分配提供了参考。

Abstract: Motivated by a growing research interest into automatic speech recognition
(ASR), and the growing body of work for languages in which code-switching (CS)
often occurs, we present a systematic literature review of code-switching in
end-to-end ASR models. We collect and manually annotate papers published in
peer reviewed venues. We document the languages considered, datasets, metrics,
model choices, and performance, and present a discussion of challenges in
end-to-end ASR for code-switching. Our analysis thus provides insights on
current research efforts and available resources as well as opportunities and
gaps to guide future research.

</details>


### [41] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

TL;DR: 本文首次系统回顾了LLM在法律领域的应用，创新性提出双视角分类法和技术-法律任务映射，总结最新进展和主要挑战，并为未来法律AI研究提供技术及实践路径。


<details>
  <summary>Details</summary>
Motivation: 目前在法律领域对大语言模型（LLMs）应用缺乏全面系统的综述，缺乏统一的分类与分析框架，法律AI的应用场景多样但挑战突出。

Method: 提出创新性双视角分类法，将法律推理框架与专业本体结合，系统梳理历史与前沿研究。技术上重点考察基于Transformer的LLM在法律问答、推理、生成式论证等任务上的应用，并落实Toulmin论证框架于NLP子任务的映射与实现。

Result: 文中记录了在任务泛化、推理形式化、工作流程集成等方面的重大进展，归纳了如稀疏注意力、混合专家架构等技术对法律文本处理、知识集成和评估带来的提升。同时指出LLMs在幻觉、可解释性、司法适配和伦理等方面存在挑战，并指明多模态证据整合、低资源场景和动态反驳等未来关键前沿。

Conclusion: 本综述为法律AI描绘了技术路线和概念框架，为相关研究和实践提供了坚实基础和系统资源（含索引仓库）。

Abstract: This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [42] [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
*Shoutao Guo,Xiang Li,Shaolei Zhang,Mengge Liu,Wei Chen,Yang Feng*

Main category: cs.CL

TL;DR: StreamUni方法通过多阶段输出和统一模型，实现端到端流式语音翻译，省去分段模型，提升实时性与质量，实验结果显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有流式语音翻译大多基于分句处理，需依赖额外的分段模型，导致上下文信息有限且策略优化受限，难以兼顾实时性与翻译质量。作者旨在消除额外分段依赖，实现端到端高效流式语音翻译。

Method: 提出StreamUni模型，利用语音链式思维（CoT）机制，实现多阶段输出，整合语音分割、策略决策与翻译生成为一体，并提出流式CoT训练，利用少量数据提升低延迟决策和生成能力。

Result: 提出的StreamUni方法在流式语音翻译任务上，融合多种子任务（分割、决策、翻译）并实现最优性能，实验验证优于现有主流方法。

Conclusion: StreamUni方法能够依托统一的大型语音语言模型（LSLM），高效完成端到端的流式语音翻译任务，无需大量专门的策略训练，实验性能达到当前最优水平。

Abstract: Streaming speech translation (StreamST) requires determining appropriate
timing, known as policy, to generate translations while continuously receiving
source speech inputs, balancing low latency with high translation quality.
However, existing StreamST methods typically operate on sentence-level speech
segments, referred to as simultaneous speech translation (SimulST). In
practice, they require collaboration with segmentation models to accomplish
StreamST, where the truncated speech segments constrain SimulST models to make
policy decisions and generate translations based on limited contextual
information. Moreover, SimulST models struggle to learn effective policies due
to the complexity of speech inputs and cross-lingual generation. To address
these challenges, we propose StreamUni, which achieves StreamST through a
unified Large Speech-Language Model (LSLM). Specifically, StreamUni
incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate
multi-stage outputs. Leveraging these multi-stage outputs, StreamUni
simultaneously accomplishes speech segmentation, policy decision, and
translation generation, completing StreamST without requiring massive
policy-specific training. Additionally, we propose a streaming CoT training
method that enhances low-latency policy decisions and generation capabilities
using limited CoT data. Experiments demonstrate that our approach achieves
state-of-the-art performance on StreamST tasks.

</details>


### [43] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer的逆向生成模型，实现了信号时序逻辑（STL）公式连续嵌入与符号公式的可逆映射，提升了符号知识与数据驱动方法结合的能力，并在规范挖掘任务中取得有效结果。


<details>
  <summary>Details</summary>
Motivation: 将符号化知识融入数据驱动学习算法中能够提升模型推理能力，然而实现表达式的连续嵌入后，需解决如何从连续空间逆向生成具体的符号逻辑公式的问题。

Method: 设计并训练基于Transformer的解码器模型，用于从Signal Temporal Logic（STL）公式的连续嵌入向量逆向生成有效的STL公式。模型采用构建的小型STL语法词表，并通过端到端训练来实现可逆嵌入。

Result: 模型仅训练1个epoch即可生成有效公式，在约10个epoch后具备泛化至逻辑语义的能力；解码出的公式长度更短、嵌套更少，但与标准参照语义接近乃至等同。实验亦检验了不同训练公式复杂度下数据对模型泛化能力的影响。

Conclusion: 提出的Transformer解码模型能有效实现STL公式连续嵌入的可逆性，在符号规范挖掘任务中，通过嵌入空间的优化，能够自动推断出满足分类任务需要的STL公式。该方法推动了符号逻辑与机器学习的结合。

Abstract: Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


### [44] [Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning](https://arxiv.org/abs/2507.07810)
*Nhi Hoai Doan,Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 本研究揭示了大语言模型中某类神经元（重复神经元）在不同层对上下文学习表现的不同影响，并提出可在保持ICL能力的同时减少重复输出的方法。


<details>
  <summary>Details</summary>
Motivation: 以往关于大语言模型（LLM）模型的研究主要集中在注意力头的作用，本论文希望从神经元层面（尤其是重复神经元）分析模型对输入重复模式的识别能力与其上下文学习（ICL）表现之间的关系。

Method: 通过实验，分析大语言模型中不同层深度的重复神经元对ICL任务表现的影响，并与归纳头进行比较，进一步提出减少重复输出同时保持良好ICL能力的策略。

Result: 发现重复神经元对ICL性能的影响随所在层的不同而异。通过与归纳头的对比，找到了在减少重复输出的同时，仍能保持较强ICL表现的方法。

Conclusion: 论文从新视角（技能神经元/重复神经元）出发，揭示了模型内特定神经元对ICL任务的重要影响，且提出了优化模型输出重复性的有效策略。

Abstract: This paper investigates the relationship between large language models'
(LLMs) ability to recognize repetitive input patterns and their performance on
in-context learning (ICL). In contrast to prior work that has primarily focused
on attention heads, we examine this relationship from the perspective of skill
neurons, specifically repetition neurons. Our experiments reveal that the
impact of these neurons on ICL performance varies depending on the depth of the
layer in which they reside. By comparing the effects of repetition neurons and
induction heads, we further identify strategies for reducing repetitive outputs
while maintaining strong ICL capabilities.

</details>


### [45] [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
*Anwoy Chatterjee,H S V N S Kowndinya Renduchintala,Sumit Bhatia,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文发现传统只关注响应token的指令微调损失并非最优，提出对提示和响应分别加权的新办法，实验显示新方法能显著提升模型表现和鲁棒性，建议未来更重视损失函数设计。


<details>
  <summary>Details</summary>
Motivation: 现有的指令微调主要关注模型跟随用户指令的能力，但很少有研究关注使用的损失函数是否最优，尤其是传统自回归目标仅在响应部分计算损失是否合理。

Method: 系统性地研究在指令微调时对提示(prompt)和响应(response)token加权的影响，提出了加权指令微调（WIT）方法，分别调整两部分的损失权重，并在多模型、多数据集和多评测集上进行了大规模实验。

Result: 实验表明，标准的指令微调损失函数往往表现次优，对输入提示变化的鲁棒性有限。低至中等提示权重与中到高响应权重组合能在多种设置下得到最佳模型表现，并适合作为后续偏好对齐训练的起点。

Conclusion: 需重新审视指令微调的损失函数设计，加权损失策略能带来更强健、更泛化的模型，并为未来的发展提供实践方向。

Abstract: Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.

</details>


### [46] [Conditional Unigram Tokenization with Parallel Data](https://arxiv.org/abs/2507.07824)
*Gianluca Vico,Jindřinch Libovický*

Main category: cs.CL

TL;DR: 本论文提出一种基于条件概率的cross-lingual分词方法（conditional unigram tokenization），统计特性正常，对翻译无提升但对语言建模有益，参数效率待解决。


<details>
  <summary>Details</summary>
Motivation: 目前常用的unigram分词方法，在多语言场景下未能充分利用源语言和目标语言的对齐信息，导致跨语言语义对齐不佳。作者希望通过改进分词方法，提升跨语言任务表现。

Method: 提出conditional unigram tokenization方法，将目标token概率建模为条件概率（基于平行数据中的源语言token），并优化分词器以最大程度实现跨语言语义对齐。采用固定的源语言分词器，训练目标分词器。方法在多语种和不同数据资源场景下进行评测。

Result: 实验发现，新方法在分词统计特性上与标准unigram分词器类似。在机器翻译任务上没有带来明显提升，但在语言建模中表现出一致的困惑度（perplexity）下降。作者认为条件概率建模导致的参数规模（随词表大小二次增长）造成了数据效率瓶颈。

Conclusion: conditional unigram tokenization在统计特性上可靠，但对下游机器翻译帮助有限，仅在语言模型困惑度上有改善。参数量激增带来的数据效率问题是瓶颈，需要改进建模方法来实现实用的跨语言分词。

Abstract: We introduce conditional unigram tokenization, a novel approach that extends
unigram tokenization by conditioning target token probabilities on
source-language tokens from parallel data. Given a fixed source tokenizer, our
method learns a target tokenizer that maximizes cross-lingual semantic
alignment. We evaluate our tokenizer on four language pairs across different
families and resource levels, examining intrinsic properties and downstream
performance on machine translation and language modeling. While our conditional
tokenizer maintains comparable statistical properties to standard unigram
tokenizers, results are mixed: we observe no improvements in machine
translation quality, but find consistent perplexity reductions in language
modeling. We hypothesize that quadratic scaling of conditional probability
estimation with respect to the vocabulary size creates a data efficiency
bottleneck. Our findings suggest that alternative parameterizations may be
necessary for practical cross-lingual tokenization.

</details>


### [47] [From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems](https://arxiv.org/abs/2507.07847)
*Youngjoon Jang,Seongtae Hong,Junyoung Son,Sungjin Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文发现，RAG系统中的实体共指带来歧义并损害检索与生成质量。通过共指消解，尤其结合mean pooling策略，可大幅提升检索相关性和QA性能，且对小模型效果更为显著。研究为优化知识密集型AI系统中的RAG框架提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: RAG框架通过结合外部检索与大语言模型，提高了自然语言处理中的事实一致性并减少了幻觉现象，但在实际应用中检索文档中的共指复杂性往往导致歧义，影响了系统的上下文学习效果。本文旨在深入探究实体共指对RAG系统文档检索和生成性能的影响，目标是揭示并解决这一关键挑战。

Method: 本文系统性地分析了实体共指对RAG系统检索相关性、语境理解和响应质量的影响。采用对比实验，重点考察了经过共指消解后不同特征池化策略（如mean pooling）对检索与QA任务表现的提升效果。

Result: 实验结果显示，实施共指消解能显著增强检索效果并提升问答表现。mean pooling在应用共指消解之后，在上下文把握能力上表现最优。研究还发现小型模型在消解歧义后受益更大，因其自身处理共指歧义的能力较弱。

Conclusion: 本研究揭示了共指复杂性对RAG系统性能的影响，证实共指消解是提升检索相关性和回答准确性的有效方法。研究为在知识密集型AI应用中应对共指挑战提供了方法指导和理论依据。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in
natural language processing (NLP), improving factual consistency and reducing
hallucinations by integrating external document retrieval with large language
models (LLMs). However, the effectiveness of RAG is often hindered by
coreferential complexity in retrieved documents, introducing ambiguity that
disrupts in-context learning. In this study, we systematically investigate how
entity coreference affects both document retrieval and generative performance
in RAG-based systems, focusing on retrieval relevance, contextual
understanding, and overall response quality. We demonstrate that coreference
resolution enhances retrieval effectiveness and improves question-answering
(QA) performance. Through comparative analysis of different pooling strategies
in retrieval tasks, we find that mean pooling demonstrates superior context
capturing ability after applying coreference resolution. In QA tasks, we
discover that smaller models benefit more from the disambiguation process,
likely due to their limited inherent capacity for handling referential
ambiguity. With these findings, this study aims to provide a deeper
understanding of the challenges posed by coreferential complexity in RAG,
providing guidance for improving retrieval and generation in
knowledge-intensive AI applications.

</details>


### [48] [Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation](https://arxiv.org/abs/2507.07868)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 本文基于Alpay代数体系提出一种多层语义博弈结构，将AI与文档对齐转化为博弈型不动点问题，并用范畴论等工具证明实用性和唯一性，可推动AI语义认知的新模型。


<details>
  <summary>Details</summary>
Motivation: 在自指代代数与AI语义嵌入的基础上，激励加强语义对齐与复杂博弈推理的自然产生机制，推动AI系统在语义层面实现更高效、更具鲁棒性的对齐与推理。

Method: 构建多层嵌套的语义博弈结构，将AI系统与文档之间的对齐过程形式化为复合算子，并采用跨有限与超限的Banach不动点定理、φ-拓扑和Yoneda引理等范畴论工具组合检验理论正确性。

Result: 提出了新的多层嵌套的语义博弈结构，并在分类理论、信息论和真实AI认知模型的支撑下，证明了语义均衡存在唯一性；还构建了应用于AI语义嵌入和认知模拟的验证工具与具体流程。

Conclusion: 本文提出的多层语义博弈结构可以自然地产生博弈论推理，并通过新的定理证明了在实际认知模拟下语义均衡的存在性和唯一性。该方法不仅理论完善，而且可实际应用于AI认知和嵌入空间。

Abstract: This paper extends the self-referential framework of Alpay Algebra into a
multi-layered semantic game architecture where transfinite fixed-point
convergence encompasses hierarchical sub-games at each iteration level.
Building upon Alpay Algebra IV's empathetic embedding concept, we introduce a
nested game-theoretic structure where the alignment process between AI systems
and documents becomes a meta-game containing embedded decision problems. We
formalize this through a composite operator $\phi(\cdot, \gamma(\cdot))$ where
$\phi$ drives the main semantic convergence while $\gamma$ resolves local
sub-games. The resulting framework demonstrates that game-theoretic reasoning
emerges naturally from fixed-point iteration rather than being imposed
externally. We prove a Game Theorem establishing existence and uniqueness of
semantic equilibria under realistic cognitive simulation assumptions. Our
verification suite includes adaptations of Banach's fixed-point theorem to
transfinite contexts, a novel $\phi$-topology based on the
Kozlov-Maz'ya-Rossmann formula for handling semantic singularities, and
categorical consistency tests via the Yoneda lemma. The paper itself functions
as a semantic artifact designed to propagate its fixed-point patterns in AI
embedding spaces -- a deliberate instantiation of the "semantic virus" concept
it theorizes. All results are grounded in category theory, information theory,
and realistic AI cognition models, ensuring practical applicability beyond pure
mathematical abstraction.

</details>


### [49] [DocCHA: Towards LLM-Augmented Interactive Online diagnosis System](https://arxiv.org/abs/2507.07870)
*Xinyi Liu,Dachun Sun,Yi R. Fung,Dilek Hakkani-Tür,Tarek Abdelzaher*

Main category: cs.CL

TL;DR: 本文针对医疗对话代理缺乏自适应与透明推理的问题，提出基于置信度的模块化诊断流程DocCHA，在中英文真实数据集上大幅提升诊断准确率和信息采集效率，为可信赖的AI医疗助手注入新动力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）表现出色，但目前的大多数医疗对话代理（CHAs）表现呆板、不具备自适应多轮推理、症状澄清或透明决策能力，制约了其在需要多轮对话的临床诊断实际应用。

Method: 提出了DocCHA，一个基于模块化、具备置信度感知的医疗对话框架，将诊断过程分为三个阶段：症状采集、病史获取和因果图构建。每个模块都利用可解释的置信得分指导自适应提问、优先澄清关键信息，并强化推理链条。

Result: DocCHA在两个真实中文医疗咨询数据集（IMCS21、DX）上表现优于主流LLM提示方法（如GPT-3.5、GPT-4o、LLaMA-3），诊断准确率提高最多5.18%，症状召回率提升30%，且对话轮数增加不明显。

Conclusion: DocCHA能实现结构化、透明且高效的诊断对话，为多语言和资源有限环境下的大语言模型医疗助手应用提供了可靠支撑。

Abstract: Despite the impressive capabilities of Large Language Models (LLMs), existing
Conversational Health Agents (CHAs) remain static and brittle, incapable of
adaptive multi-turn reasoning, symptom clarification, or transparent
decision-making. This hinders their real-world applicability in clinical
diagnosis, where iterative and structured dialogue is essential. We propose
DocCHA, a confidence-aware, modular framework that emulates clinical reasoning
by decomposing the diagnostic process into three stages: (1) symptom
elicitation, (2) history acquisition, and (3) causal graph construction. Each
module uses interpretable confidence scores to guide adaptive questioning,
prioritize informative clarifications, and refine weak reasoning links.
  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),
DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,
GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and
over 30 percent improvement in symptom recall, with only modest increase in
dialogue turns. These results demonstrate the effectiveness of DocCHA in
enabling structured, transparent, and efficient diagnostic conversations --
paving the way for trustworthy LLM-powered clinical assistants in multilingual
and resource-constrained settings.

</details>


### [50] [Automating MD simulations for Proteins using Large language Models: NAMD-Agent](https://arxiv.org/abs/2507.07887)
*Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.CL

TL;DR: 本工作通过将Gemini 2.0 Flash大语言模型与Python和Selenium自动化工具结合，开发了一整套MD模拟输入文件自动生成流程，大幅提高了工作效率、准确性和可扩展性，为分子模拟与结构生物学自动化铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟（MD）对于理解蛋白质的结构、动力学及功能至关重要，但其输入文件的准备过程常常繁琐且易出错，因此需要一种简化和自动化的方法。

Method: 本研究提出了一个自动化流程，结合大型语言模型（LLM，具体为Gemini 2.0 Flash）、Python脚本以及基于Selenium的网页自动化，自动生成MD模拟所需的输入文件。该流程利用CHARMM GUI的网页界面为NAMD准备输入，并通过Gemini的代码生成和迭代改进能力，自动编写、执行并修正脚本，实现参数提取和输入文件生成。还结合后处理软件进一步优化结果。

Result: 该自动化流程显著减少了准备时间，降低了手动出错率，并且能并行处理多个蛋白系统，具有良好的可扩展性。

Conclusion: 该方法为分子模拟输入文件的自动化准备提供了切实可行且高效的解决方案，展现了LLM在结构生物信息学自动化领域的广阔前景。

Abstract: Molecular dynamics simulations are an essential tool in understanding protein
structure, dynamics, and function at the atomic level. However, preparing high
quality input files for MD simulations can be a time consuming and error prone
process. In this work, we introduce an automated pipeline that leverages Large
Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with
python scripting and Selenium based web automation to streamline the generation
of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based
interface for preparing simulation-ready inputs for NAMD. By integrating
Gemini's code generation and iterative refinement capabilities, simulation
scripts are automatically written, executed, and revised to navigate CHARMM
GUI, extract appropriate parameters, and produce the required NAMD input files.
Post processing is performed using additional software to further refine the
simulation outputs, thereby enabling a complete and largely hands free
workflow. Our results demonstrate that this approach reduces setup time,
minimizes manual errors, and offers a scalable solution for handling multiple
protein systems in parallel. This automated framework paves the way for broader
application of LLMs in computational structural biology, offering a robust and
adaptable platform for future developments in simulation automation.

</details>


### [51] [DTECT: Dynamic Topic Explorer & Context Tracker](https://arxiv.org/abs/2507.07910)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: DTECT是一个开源、端到端动态主题建模与探索平台，集成了多模型、自动标注、趋势分析、交互可视化和自然语言查询，极大提升文本主题分析的易用性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据不断激增，如何发现其中的主题演变和趋势成为重大挑战。现有动态主题建模工具流程分散，缺乏良好的可解释性和易用的交互方式。

Method: 提出了DTECT系统，实现端到端动态主题分析，包括数据预处理、支持多种模型、提供专用评估指标。此外，加入了LLM驱动的自动主题标注、基于突显词的趋势分析、文档级摘要交互可视化和自然语言查询接口。

Result: DTECT统一了从原始文本到时序洞察的整个流程，大幅提升了动态主题模型的可解释性和易用性。作为开源工具，用户可以便捷追踪和理解文本主题动态。

Conclusion: DTECT系统有效解决了现有动态主题建模在可解释性、交互性和集成度上的不足，为分析大规模时序文本主题提供了强有力的工具。

Abstract: The explosive growth of textual data over time presents a significant
challenge in uncovering evolving themes and trends. Existing dynamic topic
modeling techniques, while powerful, often exist in fragmented pipelines that
lack robust support for interpretation and user-friendly exploration. We
introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end
system that bridges the gap between raw textual data and meaningful temporal
insights. DTECT provides a unified workflow that supports data preprocessing,
multiple model architectures, and dedicated evaluation metrics to analyze the
topic quality of temporal topic models. It significantly enhances
interpretability by introducing LLM-driven automatic topic labeling, trend
analysis via temporally salient words, interactive visualizations with
document-level summarization, and a natural language chat interface for
intuitive data querying. By integrating these features into a single, cohesive
platform, DTECT empowers users to more effectively track and understand
thematic dynamics. DTECT is open-source and available at
https://github.com/AdhyaSuman/DTECT.

</details>


### [52] [SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment](https://arxiv.org/abs/2507.07939)
*Guoxin Zang,Xue Li,Donglin Di,Lanshun Nie,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: 本文提出了SAGE方法，通过自引导事实增强和熵感知的优化方式提升视觉语言模型于工业异常检测中的推理能力和泛化性，并配套发布了新数据集和逻辑评测框架。SAGE在多个industrial anomaly任务下优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在通用多模态任务上取得了进展，但它们在工业异常检测与推理中表现不佳，尤其是在可解释性和泛化到新类别方面存在局限。异常检测任务具有较强的领域特性，现有VLM方法难以满足工业场景对精确、结构化、具备上下文感知能力分析的需求。

Method: 提出了SAGE框架，通过自引导事实增强（SFE）和熵感知直接偏好优化（E-DPO）提升异常推理。SFE通过事实提取和融合将领域知识注入视觉推理，E-DPO采用熵感知优化对齐模型输出与专家偏好。此外，构建了面向工业异常推理、包含28,415个专家打分问答的AD-PL数据集，并开发了多尺度逻辑评估（MLE）框架用于模型推理逻辑和一致性评测。

Result: SAGE在工业异常检测数据集上，在zero-shot与one-shot设置下均表现出优异性能。

Conclusion: SAGE能够有效推动VLM在工业异常分析场景中的推理能力、可解释性和泛化能力，论文还公开了相关代码、模型及数据集，便于后续研究和应用。

Abstract: While Vision-Language Models (VLMs) have shown promising progress in general
multimodal tasks, they often struggle in industrial anomaly detection and
reasoning, particularly in delivering interpretable explanations and
generalizing to unseen categories. This limitation stems from the inherently
domain-specific nature of anomaly detection, which hinders the applicability of
existing VLMs in industrial scenarios that require precise, structured, and
context-aware analysis. To address these challenges, we propose SAGE, a
VLM-based framework that enhances anomaly reasoning through Self-Guided Fact
Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE
integrates domain-specific knowledge into visual reasoning via fact extraction
and fusion, while E-DPO aligns model outputs with expert preferences using
entropy-aware optimization. Additionally, we introduce AD-PL, a
preference-optimized dataset tailored for industrial anomaly reasoning,
consisting of 28,415 question-answering instances with expert-ranked responses.
To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation
(MLE), a quantitative framework analyzing model logic and consistency. SAGE
demonstrates superior performance on industrial anomaly datasets under
zero-shot and one-shot settings. The code, model and dataset are available at
https://github.com/amoreZgx1n/SAGE.

</details>


### [53] [MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)
*Yu Wang,Xi Chen*

Main category: cs.CL

TL;DR: MIRIX是一个多代理、多模态记忆系统，通过结构化轻存储和灵活调度，极大改善了AI对真实世界和长期用户信息的记忆、抽象和检索能力，在多个苛刻场景下显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有AI记忆方案多为扁平且狭窄，难以持久、抽象和个性化地处理用户信息，且基本只针对文本，无法应对复杂、多模态、长期的实际应用需求。

Method: 提出了模块化多代理（multi-agent）记忆系统MIRIX，包括六种类型的结构化记忆（核心、情节、语义、程序、资源、知识库），可处理文本、视觉等多模态信息，并通过多代理框架实现动态协调和高效检索。

Result: 在ScreenshotVQA多模态测试中，MIRIX比RAG基线准确率高35%，存储需求降低99.9%；在LOCOMO长文本对话测试中达到85.4%的SOTA表现，远超现有方法。

Conclusion: MIRIX在多模态和长期对话两个苛刻的基准测试中都显著优于现有系统，成为记忆增强型大型语言模型（LLM）代理的新性能标杆。此外，团队还实现了落地应用，兼顾了个性化、可视化和安全性。

Abstract: Although memory capabilities of AI agents are gaining increasing attention,
existing solutions remain fundamentally limited. Most rely on flat, narrowly
scoped memory components, constraining their ability to personalize, abstract,
and reliably recall user-specific information over time. To this end, we
introduce MIRIX, a modular, multi-agent memory system that redefines the future
of AI memory by solving the field's most critical challenge: enabling language
models to truly remember. Unlike prior approaches, MIRIX transcends text to
embrace rich visual and multimodal experiences, making memory genuinely useful
in real-world scenarios. MIRIX consists of six distinct, carefully structured
memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and
Knowledge Vault, coupled with a multi-agent framework that dynamically controls
and coordinates updates and retrieval. This design enables agents to persist,
reason over, and accurately retrieve diverse, long-term user data at scale. We
validate MIRIX in two demanding settings. First, on ScreenshotVQA, a
challenging multimodal benchmark comprising nearly 20,000 high-resolution
computer screenshots per sequence, requiring deep contextual understanding and
where no existing memory systems can be applied, MIRIX achieves 35% higher
accuracy than the RAG baseline while reducing storage requirements by 99.9%.
Second, on LOCOMO, a long-form conversation benchmark with single-modal textual
input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing
existing baselines. These results show that MIRIX sets a new performance
standard for memory-augmented LLM agents. To allow users to experience our
memory system, we provide a packaged application powered by MIRIX. It monitors
the screen in real time, builds a personalized memory base, and offers
intuitive visualization and secure local storage to ensure privacy.

</details>


### [54] [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
*Noam Razin,Yong Lin,Jiarui Yao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 本论文系统分析了隐式与显式奖励模型在泛化上的差距，发现IM-RM过度依赖表层特征，导致泛化性差。设计上细微的不同会显著影响奖励模型的表现。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在语言模型的后训练和推理中非常关键。尽管隐式奖励模型（IM-RM）不需要模型结构改动，但其泛化能力弱于显式奖励模型（EX-RM）。这种几乎在所有方面都相同却表现不同的现象令人迷惑。本文旨在深入理解造成这种泛化差异的根本原因。

Method: 本文通过理论分析和实验证明了IM-RM和EX-RM的泛化差距，并考察了它们对表层词元级线索的依赖；另外，排除了多种其他可能的假设。

Result: 研究发现IM-RM比EX-RM更依赖表层的词元级信息，因此在分布外和分布内的泛化性能均较差，同时否定了一些主流的关于泛化差异的备选解释。

Conclusion: 奖励模型中看似微小的设计选择会带来泛化能力上的显著差异，IM-RM更依赖表面信息因而泛化能力弱于EX-RM。

Abstract: Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.

</details>


### [55] [Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology](https://arxiv.org/abs/2507.07983)
*Sabine Felde,Rüdiger Buchkremer,Gamal Chehab,Christian Thielscher,Jörg HW Distler,Matthias Schneider,Jutta G. Richter*

Main category: cs.CL

TL;DR: 小型语言模型结合RAG比大型模型更实用和高效，适合资源有限医疗场景，但还无法取代专家判断。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在复杂医学领域（如风湿病学）中，支持临床决策有潜力，但其资源消耗高，部署成本高，限制了实际应用。作者希望探索更节能、高效且适用于资源有限医疗环境的替代方案。

Method: 对比小型语言模型（SLMs）结合检索增强生成（RAG）技术，与大型语言模型在风湿病领域诊断和治疗方面的表现。同时评估其能耗和部署成本。

Result: 小型语言模型结合RAG技术，在诊断和治疗性能上超越了大型模型，并且大幅降低能耗，实现本地化、低成本部署，但所有模型均未达到风湿病专家水平。

Conclusion: SLMs结合RAG可为资源有限医疗环境提供高性价比的辅助方案，但仍需专家监督。

Abstract: Large language models (LLMs) show promise for supporting clinical
decision-making in complex fields such as rheumatology. Our evaluation shows
that smaller language models (SLMs), combined with retrieval-augmented
generation (RAG), achieve higher diagnostic and therapeutic performance than
larger models, while requiring substantially less energy and enabling
cost-efficient, local deployment. These features are attractive for
resource-limited healthcare. However, expert oversight remains essential, as no
model consistently reached specialist-level accuracy in rheumatology.

</details>


### [56] [Automating Expert-Level Medical Reasoning Evaluation of Large Language Models](https://arxiv.org/abs/2507.07988)
*Shuang Zhou,Wenya Xie,Jiaxi Li,Zaifu Zhan,Meijia Song,Han Yang,Cheyenna Espinoza,Lindsay Welton,Xinnie Mai,Yanwei Jin,Zidu Xu,Yuen-Hei Chung,Yiyun Xing,Meng-Han Tsai,Emma Schaffer,Yucheng Shi,Ninghao Liu,Zirui Liu,Rui Zhang*

Main category: cs.CL

TL;DR: 作者提出了一个更严格、更可扩展的医学推理评测基准MedThink-Bench及独特的评测方法LLM-w-Ref，能有效模拟专家级判读，并发现某些小模型在医学推理上优于大型模型，为大模型在医疗领域安全应用提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLM）正越来越多地被应用于临床决策中，但如何保证其推理过程的透明性与可信性成为新的难题。现有LLM医学推理能力的评估方法，要么评测不可靠，要么扩展性差，缺乏严谨且可大规模评测的基准。

Method: 作者提出了MedThink-Bench，这是一个面向医学推理能力的基准，包括500个涵盖十个医学领域的高难度问题，每个问题都配备了专家编写的逐步推理解释。同时，提出了LLM-w-Ref评测框架，结合了细粒度推理分析和“LLM作为判官”的机制，以专家级精度评估模型的中间推理，并保持良好扩展性。

Result: 通过实验发现，LLM-w-Ref评测结果与专家判读高度相关。基于MedThink-Bench基准测试了12个SOTA的大模型，观察到部分较小参数的模型（如MedGemma-27B）在推理能力上优于更大型的专有模型（如OpenAI-o3）。

Conclusion: MedThink-Bench为大语言模型的医学推理能力评估提供了坚实工具，有助于推动其在临床实践中的安全和负责任部署。

Abstract: As large language models (LLMs) become increasingly integrated into clinical
decision-making, ensuring transparent and trustworthy reasoning is essential.
However, existing evaluation strategies of LLMs' medical reasoning capability
either suffer from unsatisfactory assessment or poor scalability, and a
rigorous benchmark remains lacking. To address this, we introduce
MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable
assessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging
questions across ten medical domains, each annotated with expert-crafted
step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel
evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge
mechanisms to assess intermediate reasoning with expert-level fidelity while
maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong
positive correlation with expert judgments. Benchmarking twelve
state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can
surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,
MedThink-Bench offers a foundational tool for evaluating LLMs' medical
reasoning, advancing their safe and responsible deployment in clinical
practice.

</details>


### [57] [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
*Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Ming Li,Qilong Wu,Kaipeng Zhang,Chen Wei*

Main category: cs.CL

TL;DR: PyVision让多模态大语言模型能动态生成与执行Python工具，极大提升了视觉推理灵活性与效果。实验表明主流模型在多个数据集上均有显著性能提升，表明动态工具生成对智能体式视觉推理具有重要推动作用。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法多局限于预定义的流程和静态工具集，难以灵活应对复杂多变的问题。作者希望开发一种能让MLLMs根据具体任务自主生成、调整工具的方法，提升模型在视觉推理场景下的能力和可扩展性。

Method: 设计了PyVision，一个允许MLLMs自动生成、执行并改进Python工具的交互式多轮框架。作者还构建了工具分类法，并分析了这些工具在多个视觉推理基准上的使用情况以验证其有效性。

Result: PyVision框架提升了MLLMs在多个视觉推理数据集上的表现。例如，GPT-4.1在V*数据集上提升了7.8个百分点，Claude-4.0-Sonnet在VLMsAreBlind-mini数据集上提升了31.1个百分点。显示动态工具生成机制显著提升了模型自主视觉推理效果。

Conclusion: PyVision为MLLMs（多模态大语言模型）提供了动态生成、执行和调整Python工具的能力，使其在视觉推理任务中更加灵活和具有解释性。实验表明，PyVision极大提升了多种主流模型在视觉推理基准数据集上的表现，展示了动态工具生成在推动模型自主解决问题方面的重要价值。

Abstract: LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [58] [The Richness of CSP Non-redundancy](https://arxiv.org/abs/2507.07942)
*Joshua Brakensiek,Venkatesan Guruswami,Bart M. P. Jansen,Victor Lagerkvist,Magnus Wahlström*

Main category: cs.DM

TL;DR: 本文系统研究了CSP中的非冗余问题：不仅证明了可构造任意多项式次幂的非冗余度谓词，还首次对二元谓词的条件非冗余进行了完全分类，并发展了新的代数理论和结构，为量子Pauli群等新模型的应用打开了方向。


<details>
  <summary>Details</summary>
Motivation: 非冗余（non-redundancy）问题在CSP领域与稀疏化、核化、查询复杂度等多个重要问题密切相关，但其本质和极限尚不完全清楚。作者旨在深化对CSP中非冗余性质的理解，推动相关理论和实际应用的发展。

Method: 1. 利用构造方法证明对于任意有理数r≥1，都存在一个谓词P，其非冗余度为Θ(n^r)。2. 针对二元谓词，借助极值组合数学中高环图（high-girth graphs）的结构，给出条件非冗余的完整分类。3. 基于已有工作，发展条件非冗余的代数理论并应用于Mal'tsev embedding，引入和探索新的群体结构案例。

Result: 1. 证明存在非冗余度为任意有理次幂的谓词。2. 首次完整地对所有二元谓词的条件非冗余进行了分类。3. 提出了基于量子Pauli群而不是阿贝尔群的新Mal'tsev embedding例子。

Conclusion: 该工作深化了CSP非冗余度的理论理解，扩展了非冗余实例构造的极限，首次系统地描述了二元谓词的条件非冗余类别，并引入量子结构作为新型的代数工具，推动了相关领域的理论创新。

Abstract: In the field of constraint satisfaction problems (CSP), a clause is called
redundant if its satisfaction is implied by satisfying all other clauses. An
instance of CSP$(P)$ is called non-redundant if it does not contain any
redundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum
number of clauses in a non-redundant instance of CSP$(P)$, as a function of the
number of variables $n$. Recent progress has shown that non-redundancy is
crucially linked to many other important questions in computer science and
mathematics including sparsification, kernelization, query complexity,
universal algebra, and extremal combinatorics. Given that non-redundancy is a
nexus for many of these important problems, the central goal of this paper is
to more deeply understand non-redundancy.
  Our first main result shows that for every rational number $r \ge 1$, there
exists a finite CSP predicate $P$ such that the non-redundancy of $P$ is
$\Theta(n^r)$. Our second main result explores the concept of conditional
non-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We
completely classify the conditional non-redundancy of all binary predicates
(i.e., constraints on two variables) by connecting these non-redundancy
problems to the structure of high-girth graphs in extremal combinatorics.
  Inspired by these concrete results, we build off the work of Carbonnel [CP
2022] to develop an algebraic theory of conditional non-redundancy. As an
application of this algebraic theory, we revisit the notion of Mal'tsev
embeddings, which is the most general technique known to date for establishing
that a predicate has linear non-redundancy. For example, we provide the first
example of predicate with a Mal'tsev embedding that cannot be attributed to the
structure of an Abelian group, but rather to the structure of the quantum Pauli
group.

</details>
