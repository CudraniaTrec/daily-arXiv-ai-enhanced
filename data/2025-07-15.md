<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.FL](#cs.FL) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams](https://arxiv.org/abs/2507.09539)
*Anna Bolotina,Christoph M. Kirsch,Stefanie Muroya Lei,Matthias Pleschinger*

Main category: cs.PL

TL;DR: 本文提出了一种将符号执行推理下沉至机器码，并结合BDD与SMT求解器的技术路线，通过rotor和bitme工具实验，展示了BDD优化在缓解状态爆炸和加速SMT求解方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 符号执行是一种强大的软件分析技术，但由于控制和数据流的状态爆炸，其可扩展性一直面临挑战。传统工具往往在保持可控性的同时，牺牲了完整性，并依赖SMT求解器进行数据流推理。此外，这些推理通常发生在源代码或中间表示级别，导致机器码生成也成为信任基的一部分。作者希望探索将推理下沉到机器码级别，并完全依赖SMT及其他高效求解器技术进行推理的可行性。

Method: 作者开发了两个工具rotor和bitme，分别用于模型生成和有界模型检测。rotor将RISC-V整数算术作为建模对象，因为其语义与SMT中比特向量非常相似。bitme实现并推广了两种二元决策图（BDD）：代数决策图（ADDs）和上下文无关语言有序二元决策图（CFLOBDDs），通过BDD对程序输入进行传播，从而将恒定传播概括为领域传播，只有在无法传播时才交给SMT求解器处理。

Result: 实验表明，当前主流SMT求解器在本工作中的表现有限，但作者通过BDD的引入显著提升了SMT求解效率，并发现CFLOBDDs在缓解状态爆炸方面具有比ADDs更好的可扩展性。

Conclusion: 在符号执行的可扩展性挑战下，本文展示了在机器码级别利用BDD与SMT结合的新路径，增强了符号执行在工业级软件分析中的潜力。BDD优化有效缓解了状态爆炸问题，对大规模软件分析具有积极意义。

Abstract: Symbolic execution is a powerful technique for analyzing the behavior of
software yet scalability remains a challenge due to state explosion in control
and data flow. Existing tools typically aim at managing control flow
internally, often at the expense of completeness, while offloading reasoning
over data flow to SMT solvers. Moreover, reasoning typically happens on source
code or intermediate representation level to leverage structural information,
making machine code generation part of the trust base. We are interested in
changing the equation in two non-trivial ways: pushing reasoning down to
machine code level, and then offloading reasoning entirely into SMT solvers and
other, possibly more efficient solver technology. In more abstract terms, we
are asking if bit-precise reasoning technology can be made scalable on
software, and not just hardware. For this purpose, we developed two tools
called rotor and bitme for model generation and bounded model checking,
respectively. We chose RISC-V restricted to integer arithmetic as modeling
target for rotor since RISC-V integer semantics is essentially equivalent to
established SMT semantics over bitvectors and arrays of bitvectors. While
state-of-the-art SMT solvers struggle in our experiments, we have evidence that
there is potential for improvement. To show the potential, we have slightly
generalized and then implemented in bitme two types of binary decision diagrams
(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered
binary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input
through models, essentially generalizing constant propagation to domain
propagation. SMT solvers only get involved when model input cannot be
propagated, significanly speeding up SMT solving. We then study the impact on
state explosion of CFLOBDDs, which are potentially more scalable than ADDs.

</details>


### [2] [BeePL: Correct-by-compilation kernel extensions](https://arxiv.org/abs/2507.09883)
*Swarn Priya,Frédéric Besson,Connor Sughrue,Tim Steenvoorden,Jamie Fulford,Freek Verbeek,Binoy Ravindran*

Main category: cs.PL

TL;DR: 现有 eBPF 校验逻辑有安全与兼容性不足，BeePL 通过形式化类型系统和可验证工具链，确保加载到内核的扩展安全且正确。


<details>
  <summary>Details</summary>
Motivation: eBPF 是一种可以扩展内核功能但需极高安全性的技术，现有的 eBPF 校验器（verifier）过于保守，有时会拒绝合法程序，有时又会放行不安全程序。如何保证安全且高效地扩展内核，是亟需解决的问题。

Method: 提出 BeePL，一种为 eBPF 设计的领域特定语言，结合形式化验证的类型系统，通过静态验证保证内存访问、指针安全、循环终止和结构化控制流等安全特性。此外，对于无法完全静态检查的属性，BeePL 在编译时插入运行时检查，并扩展 CompCert 编译器以正确定向 BPF 字节码。

Result: BeePL 保证类型正确的程序在内存安全、终止性和控制流等方面满足 eBPF 的关键安全属性。形式化的类型健全性证明为这些安全提供了理论保障，并通过扩展 CompCert 成功实现了一个可验证的全流程安全工具链。

Conclusion: BeePL 用形式化方法和工具链建设，显著提升了 eBPF 安全扩展的可用性与可靠性，为内核扩展提供了更高的安全保障和理论支撑。

Abstract: eBPF is a technology that allows developers to safely extend kernel
functionality without modifying kernel source code or developing loadable
kernel modules. Since the kernel governs critical system operations and
enforces isolation boundaries between user space and privileged data, any
mechanism that modifies its behavior must meet the highest standards of safety
and correctness. To this end, the eBPF toolchain includes a verifier, which
statically checks safety properties such as memory access validity, bounded
loops, and type correctness before loading the program into the kernel.
However, the existing verifier is both overly conservative in some
cases-rejecting valid programs-and unsound in others, permitting unsafe
behavior that violates the intended semantics of the kernel interface.
  To address these challenges, we introduce BeePL, a domain-specific language
for eBPF with a formally verified type system. The BeePL type system, along
with the language design, statically enforces key safety properties such as
type-correct memory access, safe pointer usage, absence of unbounded loops, and
structured control flow. These guarantees are backed by formal type soundness
proofs, ensuring that well-typed programs satisfy the safety invariants
required by the eBPF execution environment. BeePL also proves that well-typed
source programs meet critical eBPF-specific properties related to memory
safety, termination, and control flow, enabling high-level reasoning prior to
compilation. For properties not fully enforceable statically-such as dynamic
bounds and undefined behavior-BeePL inserts semantics-preserving runtime checks
during compilation. We develop a verified compilation strategy that extends
CompCert to generate BPF bytecode from BeePL programs, establishing a
principled foundation for an end-to-end verifiable toolchain for safe kernel
extensions.

</details>


### [3] [Rows and Capabilities as Modal Effects](https://arxiv.org/abs/2507.10301)
*Wenhao Tang,Sam Lindley*

Main category: cs.PL

TL;DR: 该论文提出了一个通用的效应系统对比分析新框架，通过将行基与能力基效应系统编码进该框架，揭示了它们的本质区别，并为编程语言的效应系统设计提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 目前主流的效应系统大多以行多态(row polymorphism)或能力(capabilities)为基础，但两者之间精确关系尚不清楚，且效应追踪与函数等其他特性经常纠缠，缺乏统一对比分析框架。

Method: 提出一个统一的框架，利用并推广模态效应类型(modal effect types)，通过模态性将效应追踪与函数解耦。采用宏翻译(macro translation)方法，将已有的行基和能力基效应系统编码到该框架中，并验证类型与语义的保持性。

Result: 成功构建了一个能够编码、分析和对比不同效应系统的统一框架；通过编码，展示了行基和能力基效应系统在效应追踪机制上的本质异同。该框架可以直接分析不同效应系统的差异，并为程序语言设计提供了新见解。

Conclusion: 通过提出和实现一个基于模态效应类型的统一框架，有效地将多种基础的效应系统进行了编码和对比分析，揭示了其机制本质，并在理论与实际语言设计上均具有指导意义。

Abstract: Effect handlers allow programmers to model and compose computational effects
modularly. Effect systems statically guarantee that all effects are handled.
Several recent practical effect systems are based on either row polymorphism or
capabilities. However, there remains a gap in understanding the precise
relationship between effect systems with such disparate foundations. The main
difficulty is that in both row-based and capability-based systems, effect
tracking is typically entangled with other features such as functions.
  We propose a uniform framework for encoding, analysing, and comparing effect
systems. Our framework exploits and generalises modal effect types, a recent
novel effect system which decouples effect tracking from functions via
modalities. Modalities offer fine-grained control over when and how effects are
tracked, enabling us to express different strategies for effect tracking. We
give encodings as macro translations from existing row-based and
capability-based effect systems into our framework and show that these
encodings preserve types and semantics. Our encodings reveal the essence of
effect tracking mechanisms in different effect systems, enable a direct
analysis on their differences, and provide valuable insights on language
design.

</details>


### [4] [Orthologic Type Systems](https://arxiv.org/abs/2507.10482)
*Simon Guilloud,Viktor Kunčak*

Main category: cs.PL

TL;DR: 利用正交逻辑，论文统一解决了类型系统中交、并、否定及子类型假设的表达与决定性检验，提出高效判定与规范化算法，具备理论创新和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有类型系统在结合交、并、否定类型以及子类型假设时存在理论与算法挑战。该论文旨在借用orthologic（正交逻辑）理论，提升形式化表达能力及算法效率。

Method: 作者提出以orthologic为基础设计类型系统，使其支持交、并及否定类型，并处理子类型假设。进一步扩展orthologic以支持单调和反单调函数，为类型构造子提供理论基础。同时，提出带函数符号的orthologic证明系统，并分析其部分cut消去性质。

Result: 作者基于这些理论，提出了$ O(n^2(1+m))$的子类型判定算法（m为假设数），以及$ O(n^2)$的多项式时间规范化算法，实现对类型的最简规范化。

Conclusion: 通过正交逻辑，本文为同时支持交、并、否定及子类型假设的类型系统，给出了统一且高效的理论与算法解决方案。

Abstract: We propose to use orthologic as the basis for designing type systems
supporting intersection, union, and negation types in the presence of subtyping
assumptions. We show how to extend orthologic to support monotonic and
antimonotonic functions, supporting the use of type constructors in such type
systems. We present a proof system for orthologic with function symbols,
showing that it admits partial cut elimination. Using these insights, we
present an $\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation
under $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization
algorithm, allowing simplification of types to their minimal canonical form.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: 本文通过访谈与问卷研究分析了巴西开发者使用Git工作流（主干式与分支式）的适用情境与影响因素，发现主干式适合小而精的高效团队，分支式更适合大团队但管理更难，为团队选择工作流提供科学建议。


<details>
  <summary>Details</summary>
Motivation: 当前关于Git工作流的选择主要集中在论坛和博客等非科学性渠道，缺乏学术界对其系统性研究。该文旨在填补对不同Git协作开发模式影响因素与适用情境的实证研究空白，帮助团队根据自身特点选择合适的工作流。

Method: 作者采用半结构化访谈和问卷调查的方法，收集了巴西软件开发者关于Git工作流使用的实际经验和偏好，系统分析影响工作流选择的关键因素。

Result: 研究发现，trunk-based development更适合高节奏、经验丰富且团队规模较小的项目，而branch-based development则更适合经验较少且规模较大的团队，但分支管理带来了更多挑战。

Conclusion: 选择合适的Git工作流需要权衡项目节奏、团队规模和成员经验。Trunk-based能提升敏捷性但要求高协作基础，而branch-based提升可控性但需面对管理复杂性。作者提供了实证证据，为团队工作流决策提供了科学参考。

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [6] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 本论文针对低资源语言R，提出并验证了基于小型语言模型和上下文逐行分析的自动代码分割方法；实验表明该方法优于传统与LLM方案，且具通用性。


<details>
  <summary>Details</summary>
Motivation: 随着代码库规模的增长，传统的手动和语法分析方法在源代码分割上变得不切实际，尤其是在像R语言这样的低资源语言及其相关领域。为提升知识检索和维护效率，亟需自动化且适用于特定领域的方法。

Method: 提出了两种基于大语言模型(LLM)和小型语言模型(SLM)的自动化R代码分割方法：一是带上下文信息的逐行分析法，二是基于代码范围的分段法。构建了人工标注数据集StatCodeSeg，并对计算机科学领域的Python代码也进行了实验以验证方法的通用性。

Result: 上下文相关的逐行分析方法优于基于范围的分割法。在模型选择上，像CodeBERT和编码器版CodeT5+这种较小的语言模型在分割表现上优于LLM。最值得注意的是，这两个最佳模型在预训练时未见过R代码，只在4,130行人工标注代码上微调，仍能取得优异效果。

Conclusion: 提出的方法——特别是基于上下文的逐行分析加上小型语言模型——为低资源语言（如R）的自动代码分割提供了高效、通用性的解决方案，对大型代码库的知识检索与维护具有实际价值。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [7] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: 本文提出并实现了基于多智能体的药物发现自动化系统Tippy，显著加快并优化了药物设计-合成-测试-分析全过程，对提升制药研发效率具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 制药行业在新药开发过程中面临前所未有的挑战，传统方法难以满足现代治疗开发需求。需要通过创新手段提升实验室自动化和提升药物发现效率。

Method: 本文提出了一个新颖的AI框架——Tippy，采用多智能体系统，在DMTA（设计-合成-测试-分析）循环中实现实验室自动化。该系统包含Supervisor、Molecule、Lab、Analysis和Report五个专门智能体，并由Safety Guardrail监管，各自负责药物发现流程中的不同阶段。

Result: Tippy作为首个可生产化的AI多智能体DMTA自动化实现，显著提升了实验室工作流程的效率、决策速度和跨学科协调能力。

Conclusion: Tippy系统展示了AI智能体在药物发现实验室流程中的转型潜力，为AI辅助药物开发提供了新的范式和生产级应用示例。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [8] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 该文提出一种基于深度学习的命名实体识别方法显著提升了从移动应用评论中提取用户需求的效果，尤其在大规模数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 移动应用评论包含大量关于用户需求和软件改进的信息。然而，评论通常内容非正式、存在语法拼写错误且含有大量无关信息，现有技术难以有效提取有价值的需求。

Method: 本文将需求提取任务重新表述为命名实体识别（NER）问题，提出了一种基于序列到序列（Seq2seq）生成模型的框架，包含BiLSTM编码器、LSTM解码器，并结合了自注意力机制、GloVe词嵌入及CRF模型。

Result: 在两个数据集上进行评估：手工标注数据集（1,000条）和众包数据集（23,816条）。该方法在大数据集上F1达到0.96，手工标注的数据集F1为0.47，均优于或可比现有方法。

Conclusion: 将需求提取建模为NER任务并引入Seq2seq等深度学习技术，可有效提升从非结构化用户评论中识别需求的准确性和鲁棒性。

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Computability of Equivariant Gröbner bases](https://arxiv.org/abs/2507.08990)
*Arka Ghosh,Aliaume Lopez*

Main category: cs.LO

TL;DR: 本文分析了带群作用的无限变量多项式环中等变理想的Gröbner基和理想成员判定问题。在满足希尔伯特基性质时，上述判定问题可解，否则给出一类情形下的不可判定性条件。


<details>
  <summary>Details</summary>
Motivation: 等变理想和群作用在代数研究与实际应用中常见，但相关Gröbner基及成员性判定的可计算性长期是未解问题，作者旨在揭示何时这些问题可判定，以及不可判定的条件。

Method: 作者研究了在无限变量情形下，结合群作用的等变理想和其Gröbner基的可计算性，通过判定希尔伯特基性质与不可判定性的条件，对理想成员问题进行理论分析。

Result: 证明了在满足希尔伯特基性质时，等变Gröbner基可计算、理想成员问题可判定，同时刻画了不可判定问题的充分条件。

Conclusion: 当群作用和不定元集合满足希尔伯特基性质时，等变理想的Gröbner基可计算，相应的理想成员判定问题可判定。反之，作者还给出了等变理想成员性问题不可判定的充分条件，这一条件适用于常见的不满足希尔伯特基性质的例子。

Abstract: Let $\mathbb{K}$ be a field, $\mathcal{X}$ be an infinite set (of
indeterminates), and $\mathcal{G}$ be a group acting on $\mathcal{X}$. An ideal
in the polynomial ring $\mathbb{K}[\mathcal{X}]$ is called equivariant if it is
invariant under the action of $\mathcal{G}$. We show Gr\"obner bases for
equivariant ideals are computable are hence the equivariant ideal membership is
decidable when $\mathcal{G}$ and $\mathcal{X}$ satisfies the Hilbert's basis
property, that is, when every equivariant ideal in $\mathbb{K}[\mathcal{X}]$ is
finitely generated. Moreover, we give a sufficient condition for the
undecidability of the equivariant ideal membership problem. This condition is
satisfied by the most common examples not satisfying the Hilbert's basis
property.

</details>


### [10] [A Simple and Effective ASP-Based Tool for Enumerating Minimal Hitting Sets](https://arxiv.org/abs/2507.09194)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.LO

TL;DR: 本文提出用ASP方法建模和枚举最小命中集，开发了MinHit-ASP工具，并在多领域基准测试中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 最小命中集问题是计算机科学和数学中的基础问题，广泛应用于实际场景。对于给定的集合族，有效枚举所有最小命中集极为重要。

Method: 运用Answer Set Programming（ASP）对最小命中集枚举问题进行建模，利用现有ASP求解器实现高效枚举。开发了一个ASP工具MinHit-ASP。

Result: 实验证明，MinHit-ASP工具能够高效地枚举不同问题领域基准数据上的最小命中集。

Conclusion: 本文提出的基于ASP的方法和工具（MinHit-ASP）能有效完成所有最小命中集的枚举，并在多个领域基准测试集上取得良好效果。

Abstract: The hitting set problem is a fundamental problem in computer science and
mathematics. Given a family of sets over a universe of elements, a minimal
hitting set is a subset-minimal collection of elements that intersects each set
in the family. Enumerating all minimal hitting sets is crucial in various
real-world applications.
  In this paper, we address the full enumeration of all minimal hitting sets
for a given family of sets. We formulate the problem using Answer Set
Programming (ASP) and leverage existing ASP solvers for efficient enumeration.
We propose an ASP-based tool, MinHit-ASP, and our empirical evaluation shows
that it effectively enumerates minimal hitting sets across benchmarks from
diverse problem domains.

</details>


### [11] [Recovering Commutation of Logically Constrained Rewriting and Equivalence Transformations (Full Version)](https://arxiv.org/abs/2507.09326)
*Kanta Takahata,Jonas Schöpf,Naoki Nishida,Takahito Aoto*

Main category: cs.LO

TL;DR: 作者提出最一般约束重写方法，有效解决LCTRS实现过程中的搜索空间过大问题，并通过理论证明与原有系统兼容，提升了工具的理论基础和实现效率。


<details>
  <summary>Details</summary>
Motivation: 当前逻辑约束项重写系统（LCTRSs）在实现上存在搜索空间过大等瓶颈，且传统约束重写方式将重写规约与等价变换紧密结合，影响理论分析与实际应用。作者旨在提出一种更高效和理论上优良的重写模式。

Method: 作者定义了左线性、无值左项(LVF)的LCTRS类，并提出在存在性约束项上工作的最一般约束重写方法。通过构造和理论证明，展示这一方法可嵌入原有的约束重写形式，并验证其与等价变换的可交换性。

Result: 提出的最一般约束重写在定义的LCTRSs类别上能够与等价变换可交换，从而支持等价变换在重写规则应用后进行，有效降低实现时的搜索空间。同时，该方法可兼容原有约束重写形式，对提升LCTRS工具效率和正确性有重要意义。

Conclusion: 本文提出了一种新的约束重写方法——最一般约束重写，可有效与等价变换配合，实现更高效的重写系统实现。作者证明了这种重写与等价变换可交换，能够减少实现中的搜索空间，推动LCTRS工具更高效、正确地工作。

Abstract: Logically constrained term rewriting is a relatively new rewriting formalism
that naturally supports built-in data structures, such as integers and bit
vectors. In the analysis of logically constrained term rewrite systems
(LCTRSs), rewriting constrained terms plays a crucial role. However, this
combines rewrite rule applications and equivalence transformations in a closely
intertwined way. This intertwining makes it difficult to establish useful
theoretical properties for this kind of rewriting and causes problems in
implementations -- namely, that impractically large search spaces are often
required. To address this issue, we propose in this paper a novel notion of
most general constrained rewriting, which operates on existentially constrained
terms, a concept recently introduced by the authors. We define a class of
left-linear, left-value-free LCTRSs that are general enough to simulate all
left-linear LCTRSs and exhibit the desired key property: most general
constrained rewriting commutes with equivalence. This property ensures that
equivalence transformations can be deferred until after the application of
rewrite rules, which helps mitigate the issue of large search spaces in
implementations. In addition to that, we show that the original rewriting
formalism on constrained terms can be embedded into our new rewriting formalism
on existentially constrained terms. Thus, our results are expected to have
significant implications for achieving correct and efficient implementations in
tools operating on LCTRSs.

</details>


### [12] [Non-Termination of Logic Programs Using Patterns](https://arxiv.org/abs/2507.09390)
*Etienne Payet*

Main category: cs.LO

TL;DR: 本论文提出并实现了一种自动检测逻辑程序非循环非终止性的新方法，通过新unfolding技术和工具NTI验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 过去已有方法可以通过规则模式自动检测术语重写系统中的非循环非终止性，但这些方法尚未直接应用于逻辑编程。动机是将这种检测方法引入逻辑编程领域，以自动识别潜在的非终止问题。

Method: 将术语重写系统中用于检测非循环非终止性的模式检测方法，经过改造后应用到逻辑编程中。具体做法是设计了一种新的unfolding技术，用以生成能够描述可能无限多个有限重写序列的模式。并在自研工具NTI中进行了实现和实验评估。

Result: 通过实验评估验证了该方法的可行性和有效性，工具NTI在自动检测逻辑程序中的非循环非终止性方面表现良好。

Conclusion: 将术语重写系统中的非循环非终止性检测方法成功转化并应用于逻辑编程，提出的新unfolding技术和工具NTI能够有效检测逻辑程序中的相关问题。

Abstract: In this paper, we consider an approach introduced in term rewriting for the
automatic detection of non-looping non-termination from patterns of rules. We
adapt it to logic programming by defining a new unfolding technique that
produces patterns describing possibly infinite sets of finite rewrite
sequences. We present an experimental evaluation of our contributions that we
implemented in our tool NTI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 本文提出结合空间嵌入和transformer的Spatial ModernBERT模型，通过多任务token分类识别表格和键值信息，并通过创新的后处理方法，高效提升了金融文档信息提取的准确率，具有现实应用价值。


<details>
  <summary>Details</summary>
Motivation: 金融文档表格及关键字段的自动提取对于审计、数据分析、发票自动化等商务流程至关重要，现有方法在处理此类复杂排版场景时精度有限，亟需结合空间布局与文本信息的模型。

Method: 提出了Spatial ModernBERT模型，在传统BERT基础上加入空间嵌入，采用三头结构（标签头、列号头、行号头）进行token分类，通过交叉熵损失进行训练；提出后处理方法基于B-I-IB标注合并tokens并重建表格布局、提取键值。先在PubTables-1M数据集预训练，再在金融文档数据集微调。

Result: 该模型在真实的金融文档数据集上表现出强健的性能，显著提升了表格和关键值提取的准确率。

Conclusion: Spatial ModernBERT 能有效结合文本和空间特征，实现了高精度的表格与键值提取，适用于实际的金融文档场景。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [14] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: 许多现有LLM安全防护措施在面对多语言尤其是低资源语言时效果有限。该文提出并验证了多语言守护系统SEALGuard，并构建了大规模数据集，显著提升主流系统在多种语言下的安全检测能力，对提升LLM国际化部署的安全性具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前大多数针对大型语言模型（LLM）安全的守护机制主要集中在英文输入，处理多语言（尤其是低资源语言）安全检测能力不足，因此系统容易被非英文的非法或越狱提示词攻破。

Method: 提出SEALGuard，一种多语言的守护机制，通过低秩适应（LoRA）将通用多语言模型适配成多语言安全守护系统。同时，构建了包含10种语言、26万多条安全与非法及越狱提示的大型数据集SEALSBench，并基于该基准进行评测。

Result: 对比现有的主流守护系统（如LlamaGuard），SEALGuard在多语言非法及越狱提示检测任务上，分别提升了48%的防护成功率（DSR），并获得了最佳的DSR、精准率和F1分数。消融实验也证明了模型适配策略和规模提升对最终性能有积极贡献。

Conclusion: SEALGuard解决了多语言语境下LLM的安全对齐难题，在检测不同语言下的非法及越狱提示时，相较主流方法表现显著提升，有助于LLM系统更全面的安全防护。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [15] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 医学领域常用的LLM评估数据集存在很多局限，需要更具代表性和标准化的数据集，以及政策协作以保障评估质量。


<details>
  <summary>Details</summary>
Motivation: 目前用于医学问答的大型语言模型（LLM）评估数据集存在局限性，有影响真实临床应用的风险，因此需要分析这些评估基础的质量问题。

Method: 对广泛使用的基准数据集（如MedQA、MedMCQA、PubMedQA、MMLU）进行了回顾，评估其在严谨性、透明度和临床相关性方面的表现；同时分析医学期刊中的挑战性问题，讨论其作为无偏评估工具的潜力。

Result: 现有的大多数数据集缺乏临床真实性、透明度和有效的验证流程。公开的挑战性问题虽然带来一些益处，但因数量少、范围窄，并且部分已被LLM训练暴露而限制了实用性。

Conclusion: 必须建立标准化的评估框架，要求各机构和政策制定者协作，保证数据集和方法的严谨性、公正性以及能够反映临床复杂性。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [16] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 本文提出并公开了两个更准确、符合韩国专业领域实际需求的大型语言模型评测基准，有助于推动模型在实际工作场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的开发需要不仅涵盖学术领域，还要覆盖工业领域的高质量基准测试，以评估其在实际场景中的适用性。目前针对韩国专业领域的公开基准较少，存在内容和可靠性等问题。

Method: 作者构建了两个韩语专家级基准：KMMLU-Redux（对原有KMMLU重新整理、移除关键错误，以提高可靠性）和KMMLU-Pro（基于韩国国家职业资格考试题目，反映韩国专业知识）。

Result: 实验结果表明，这两个基准能够全面代表韩国的工业知识领域。

Conclusion: 作者提出并公开了两个新的韩国专业领域基准，为大型语言模型在实际工业应用中的评测提供了更合适的工具。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [17] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS无监督自我改进模型引导方法可适应上下文，效果优于传统有监督方法，未来值得深入研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）推理时通过模型引导动态对齐人类偏好，但常用引导方法依赖外部注释数据，这限制了适应性和效果。

Method: 提出SIMS框架，无需外部监督，通过自我生成和迭代优化对比样本自我提升模型引导效果。同时，引入提示排序和对比采样等新策略。

Result: 在多种LLM和基准上，SIMS在引导效果和适应性方面均明显优于现有方法。

Conclusion: 自我改进型模型引导在推理时LLM对齐方面表现优秀，是未来研究的有前景方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [18] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 该研究通过分析MIMIC-III电子健康记录，发现特定患者群体（如黑人、政府保险、自费等）更容易在病历中遭遇污名化和质疑性描述，这种偏见由不同类型医护人员共同维持。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）作为医疗团队之间沟通的重要媒介，可能助长了对患者的污名化。研究旨在揭示EHR中污名化和质疑性语言在不同患者群体及医护人员中的分布情况。

Method: 通过词汇匹配及监督学习分类器，识别MIMIC-III电子健康记录中的质疑性词语和污名化标签，并用泊松回归模型评估不同特征影响其出现频率的预测因子。

Result: 发现黑人或非裔美国人、医保/医疗补助或政府保险、自费患者，以及患有特定疾病和心理健康问题的患者，其病历中污名化标签的出现率更高；男性患者的质疑性词语频率亦较高。护士和社会工作者记录的污名化标签和质疑性词语也更多。

Conclusion: 医学记录中的语言偏见在历史上易受污名化的患者群体中更常见，并由多类医务人员在无形中延续。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [19] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 人们在Ganzflicker诱发下的幻觉体验内容，受自身视觉意象能力影响：意象强的人看到更复杂的内容，用词更生动；意象弱的人看到更简单的图形。


<details>
  <summary>Details</summary>
Motivation: 不同个体的视觉意象能力存在差异，但尚不清楚这些差异是否会影响个体在Ganzflicker诱发下所产生的视觉幻觉内容的复杂性。本文旨在探究视觉意象能力的个体差异如何影响内在视觉体验的丰富性。

Method: 通过让4000多名参与者自由描述他们在Ganzflicker诱发下体验到的视觉幻觉，并应用自然语言处理工具（包括视觉-语言模型和纯语言模型）分析这些文本文本描述。同时比较了不同视觉意象能力（强、弱）的参与者在幻觉内容描述上的差异。

Result: 强视觉意象者描述了复杂、具象、自然的场景内容，而弱意象者多以简单的几何图形为主。视觉-语言嵌入模型能更好地捕捉这些差异，并发现强意象者使用了更具感官和动作关联的语言。

Conclusion: 视觉意象的个体差异会显著影响Ganzflicker诱发的内在视觉体验内容的复杂性和丰富性，这可能反映了早期视觉区与高阶区域之间协同调控能力的个体变化。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [20] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard提出了一种能支持无限上下文、同时高效和灵活的Transformer结构，在各种任务上表现优异，是长文本生成的一种有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大语言模型（LLMs）在处理长文本时面临内存和计算上的瓶颈，主要原因是softmax注意力机制复杂度为二次方并且KV缓存随文本长度增长而变大。现有的线性化方法通常受限于模型结构，难以集成门控模块，导致在灵活性和表现上存在不足。

Method: 提出Lizard框架，通过引入亚二次复杂度的注意力机制，近似softmax注意力，同时采用受最新线性模型启发的门控模块，实现自适应内存控制和常量内存推理。Lizard结合了全局上下文压缩的门控线性注意力与带有meta memory的滑动窗口注意力，形成混合机制，兼顾长距离依赖和局部细节。同时，提出硬件友好的训练加速算法。

Result: Lizard在标准语言建模任务上几乎无损地恢复了教师模型的性能，并且在与以往线性化方法的对比中表现显著优越。在5-shot MMLU基准测试上，相比于此前模型提升了18分，在关联回溯任务上也有显著改进。

Conclusion: Lizard显著缓解了Transformer模型在处理超长上下文时的内存和计算压力，在保证性能的前提下实现了更好的灵活性、推广性和效率，优于现有线性化方案。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [21] [Multiplicative Modular Nim (MuM)](https://arxiv.org/abs/2507.08830)
*Satyam Tyagi*

Main category: cs.DM

TL;DR: 作者提出并系统研究了一种用模乘法取代异或的新型Nim游戏，定理完整，支持分解与扩展，为非加法型组合游戏提供新框架，有助于相关代数教育。


<details>
  <summary>Details</summary>
Motivation: 本文提出MuM（乘法模Nim）这一Nim游戏变种，旨在用模m下的乘法代替传统的二进制异或运算，从而探究非加法型组合游戏的理论结构。同时也出于教育目的，提升对AES S盒代数性质的理解。

Method: 作者采用类似Bouton的直接分析方法，结合Sprague-Grundy定理类比，定义游戏状态的值mumber，并使用multiplicative mex递归求值。针对合成模数，借助中国剩余定理将游戏分解至质因数次幂子游戏。最后将该理论扩展至有限域F(p^n)，并引入Canonical Heap Model来消解整数堆与域元素的多对一映射。

Result: （1） 对素数模数，理论完整，游戏取值mumber等于堆积大小乘积模m；（2） 对复合模数，可分解为独立子游戏；（3） 扩展到有限域时，通过规范堆模型保证理论健全；（4） 组合性上MuM通过模乘而非常规Nim的异或。

Conclusion: 本文首次系统分析了乘法模Nim及其组合博弈代数，建立了完整的理论体系，为非加法型组合游戏的进一步研究及相关数学教育提供了新途径。

Abstract: We introduce Multiplicative Modular Nim (MuM), a variant of Nim in which the
traditional nim-sum is replaced by heap-size multiplication modulo m. We
establish a complete theory for this game, beginning with a direct,
Bouton-style analysis for prime moduli. Our central result is an analogue of
the Sprague-Grundy theorem, where we define a game-theoretic value, the mumber,
for each position via a multiplicative mex recursion. We prove that these
mumbers are equivalent to the heap-product modulo m, and show that for
disjunctive sums of games, they combine via modular multiplication in contrast
to the XOR-sum of classical nimbers. For composite moduli, we show that MuM
decomposes via the Chinese Remainder Theorem into independent subgames
corresponding to its prime-power factors. We extend the game to finite fields
F(pn), motivated by the pedagogical need to make the algebra of the AES S-box
more accessible. We demonstrate that a sound game in this domain requires a
Canonical Heap Model to resolve the many-to-one mapping from integer heaps to
field elements. To our knowledge, this is the first systematic analysis of a
multiplicative modular variant of Nim and its extension into a complete,
non-additive combinatorial game algebra.

</details>


### [22] [m-Eternal Domination and Variants on Some Classes of Finite and Infinite Graphs](https://arxiv.org/abs/2507.09283)
*Tiziana Calamoneri,Federico Corò,Neeldhara Misra,Saraswati G. Nanoti,Giacomo Paesani*

Main category: cs.DM

TL;DR: 本文提出并系统研究了图论中的m-永恒支配问题，证明了其在多类图中的NP-困难性，并在四类无限网格结构上给出了支配数的严密界限，丰富了图防御策略的理论基础，对相关实际应用具有参考价值。


<details>
  <summary>Details</summary>
Motivation: 本论文研究了图论中的m-永恒支配问题，该问题源于防御者与攻击者在图上进行的博弈。该问题的动机在于理解和优化图结构中防御策略的最小需求，具有重要的理论和应用价值。

Method: 作者通过理论分析的方法，证明了m-永恒支配问题和其变种在一些特定类别的图中是NP-困难的。此外，作者还分析了四种无限规则网格（方格、八边形、六边形和三角形）下的支配数与m-永恒支配数，并给出了严格的界限。

Result: 主要结果包括：m-永恒支配问题在某些特殊类别图中是NP-hard，且在四种常见无限规则网格中，获得了关于支配数和m-永恒支配数的结构性界和精确界。

Conclusion: m-永恒支配问题及其若干变体不仅计算复杂度高（NP-hard），并且在不同类型的网格图中具有详细的界限特征，对图的安全防御等应用研究有直接借鉴意义。

Abstract: We study the m-Eternal Domination problem, which is the following two-player
game between a defender and an attacker on a graph: initially, the defender
positions k guards on vertices of the graph; the game then proceeds in turns
between the defender and the attacker, with the attacker selecting a vertex and
the defender responding to the attack by moving a guard to the attacked vertex.
The defender may move more than one guard on their turn, but guards can only
move to neighboring vertices. The defender wins a game on a graph G with k
guards if the defender has a strategy such that at every point of the game the
vertices occupied by guards form a dominating set of G and the attacker wins
otherwise. The m-eternal domination number of a graph G is the smallest value
of k for which (G,k) is a defender win.
  We show that m-Eternal Domination is NP-hard, as well as some of its
variants, even on special classes of graphs. We also show structural results
for the Domination and m-Eternal Domination problems in the context of four
types of infinite regular grids: square, octagonal, hexagonal, and triangular,
establishing tight bounds.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [23] [Algebraic Closure of Matrix Sets Recognized by 1-VASS](https://arxiv.org/abs/2507.09373)
*Rida Ait El Manssour,Mahsa Naraghi,Mahsa Shirmohammadi,James Worrell*

Main category: cs.FL

TL;DR: 本文研究了由上下文无关语言定义的矩阵集合的Zariski闭包计算问题。提出了one-counter语言情况下的计算方法，并证明了indexed语言情况下该问题不可判定，对提升仿射程序多项式不变量理论具有意义。


<details>
  <summary>Details</summary>
Motivation: 现有研究已经能够计算由有限生成矩阵幺半群或由正则语言指定的矩阵集合的Zariski闭包。这一结果已被用于求解给定仿射程序所有多项式不变量。然而，带递归过程调用的仿射程序多项式不变量的普遍可判定性仍是未解问题。其数学核心为如何计算由上下文无关语言定义的矩阵集合的Zariski闭包。本文试图推进这个难题的可判定性研究。

Method: 本文从两个方面入手：一方面，针对一类特定的上下文无关语言（即one-counter语言，对应一维向量加法系统和零测试的自动机），提出计算矩阵集合Zariski闭包的具体过程；另一方面，证明了对于indexed语言（上下文无关语言的自然扩展，等价于嵌套下推自动机）时，该问题是不可判定的。主要技术方法之一是将Simon的因式分解森林方法新颖地应用于无限矩阵幺半群。

Result: 本文提出了一种能够对由one-counter语言描述的矩阵集合计算其Zariski闭包的方法，并证明了对于indexed语言，该问题是不可判定的。

Conclusion: 对于特定类型的上下文无关语言（one-counter语言），可以有效计算其对应矩阵集合的Zariski闭包；但对于更广泛的indexed语言，该问题是不可判定的。该研究推进了递归仿射程序多项式不变量这一领域的理论边界。

Abstract: It is known how to compute the Zariski closure of a finitely generated monoid
of matrices and, more generally, of a set of matrices specified by a regular
language. This result was recently used to give a procedure to compute all
polynomial invariants of a given affine program. Decidability of the more
general problem of computing all polynomial invariants of affine programs with
recursive procedure calls remains open. Mathematically speaking, the core
challenge is to compute the Zariski closure of a set of matrices defined by a
context-free language. In this paper, we approach the problem from two sides:
Towards decidability, we give a procedure to compute the Zariski closure of
sets of matrices given by one-counter languages (that is, languages accepted by
one-dimensional vector addition systems with states and zero tests), a proper
subclass of context-free languages. On the other side, we show that the problem
becomes undecidable for indexed languages, a natural extension of context-free
languages corresponding to nested pushdown automata. One of our main technical
tools is a novel adaptation of Simon's factorization forests to infinite
monoids of matrices.

</details>


### [24] [A Divide and Conquer Algorithm for Deciding Group Cellular Automata Dynamics](https://arxiv.org/abs/2507.09761)
*Niccolo' Castronuovo,Alberto Dennunzio,Luciano Margara*

Main category: cs.FL

TL;DR: 本文提出一种新算法，将复杂群元胞自动机动力学分析问题分解为更简单的子问题。这使得诸如单射性、满射性、敏感性等性质变为可判定，证明了非阿贝尔群上无强遍历与正膨胀自动机，并将拓扑熵的计算简化为阿贝尔与特定非阿贝尔群情形。


<details>
  <summary>Details</summary>
Motivation: 群上的元胞自动机由于定义在一般有限群上，并且其全局规则为群自同态，具备复杂的动力学性质。这些动力学性质（如满射性、单射性、初始状态敏感性等）难以直接判定，因此需要简化分析的方法。

Method: 提出一种新的算法性技术，将待研究的群元胞自动机分解成有限个更简单的群元胞自动机。这些分解后的自动机有一部分定义在阿贝尔群上，另一部分则定义在若干个简单非阿贝尔同构群的乘积上。分解后所得的群完全由原始的群确定，与自动机的具体规则和待判定的性质无关。借此，原动力学性质在分解后的自动机中得以保留，并变得更易分析。

Result: 证明了分解后所得的群元胞自动机便于分析，有助于判定原始自动机的重要动力学性质。具体结果包括：可判定单射性、满射性及初始条件敏感性，并证明定义在非阿贝尔群上的这类自动机不存在强遍历性及正膨胀性。此外，只要可计算特定类型下（如简单非阿贝尔群乘积、阿贝尔群）的拓扑熵，则原自动机的拓扑熵也能被计算。

Conclusion: 通过将一般群元胞自动机分解为结构更简单的子问题，显著降低了相关动力学性质的判定难度，部分关键动力学性质成为可判定问题，并澄清了非阿贝尔群情形下动力学性质的结构特征。

Abstract: We prove that many dynamical properties of group cellular automata (i.e.,
cellular automata defined on any finite group and with global rule which is an
endomorphism), including surjectivity, injectivity, sensitivity to initial
conditions, strong transitivity, positive expansivity, and topological entropy,
can be decided by decomposing them into a set of much simpler group cellular
automata. To be more specific, we provide a novel algorithmic technique
allowing one to decompose the group cellular automaton to be studied into a
finite number of group cellular automata, some of them defined on abelian
groups, while others, if any, defined on products of simple non-abelian
isomorphic groups.
  It is worth noting that the groups resulting from the decomposition only
depend on the original group and therefore they are completely independent of
both the automaton and the property under investigation. As a result, they do
not inherit any aspect of the complexity of the automaton under investigation.
  We prove that the group cellular automata obtained by the decomposition
preserve dynamical properties and turn out to be much easier to analyze if
compared to the original cellular automaton. As a consequence of these results,
we show that injectivity, surjectivity and sensitivity to initial conditions
are decidable properties and that no strongly transitive, and therefore no
positively expansive, group cellular automata defined on non-abelian groups
exist. Moreover, we prove that the topological entropy of a group cellular
automaton can be computed, provided we know how to compute the topological
entropy for group cellular automata defined on products of simple non-abelian
isomorphic groups and on abelian groups.

</details>


### [25] [Rule-based Generation of de Bruijn Sequences: Memory and Learning](https://arxiv.org/abs/2507.09764)
*Francisco J. Muñoz,Juan Carlos Nuño*

Main category: cs.FL

TL;DR: 本文提出了一种结合规则缩减和神经网络分类的新方法，有效加速了长记忆条件下de Bruijn序列的寻找。


<details>
  <summary>Details</summary>
Motivation: 传统的de Bruijn序列生成方法存在搜索空间大和效率低下的问题，因此需要开发一种更高效的生成方法。

Method: 提出了一种新的de Bruijn序列生成方法，该方法结合了基于序列生成规则的性质分析以缩小候选空间，以及神经网络分类器来判别哪些规则能够生成de Bruijn序列。

Result: 在较大记忆长度μ下的实验表明，该方法能够有效、快速地生成de Bruijn序列，验证了其计算效率和实际效益。

Conclusion: 通过引入规则筛选和深度学习分类的联合方法，极大提升了de Bruijn序列生成的效率，为相关领域的序列设计提供了新思路。

Abstract: We investigate binary sequences generated by non-Markovian rules with memory
length $\mu$, similar to those adopted in Elementary Cellular Automata. This
generation procedure is equivalente to a shift register and certain rules
produce sequences with maximal periods, known as de Bruijn sequences. We
introduce a novel methodology for generating de Bruijn sequences that combines:
(i) a set of derived properties that significantly reduce the space of feasible
generating rules, and (ii) a neural network-based classifier that identifies
which rules produce de Bruijn sequences. Experiments for large values of $\mu$
demonstrate the approach's effectiveness and computational efficiency.

</details>
