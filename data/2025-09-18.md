<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.DM](#cs.DM) [Total: 3]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Catalpa: GC for a Low-Variance Software Stack](https://arxiv.org/abs/2509.13429)
*Anthony Arnold,Mark Marron*

Main category: cs.PL

TL;DR: 本文提出Catalpa垃圾回收器，针对Bosque语言，利用其语法特性实现低尾延迟、恒定内存开销和高吞吐，无需与应用同步，极大优化了应用在工业场景下的可用性和响应表现。


<details>
  <summary>Details</summary>
Motivation: 应用程序的响应速度对用户体验至关重要，工业界更关注响应时间的尾分位数（如95或99百分位），而不是平均响应时间。现有的运行时和内存管理设计难以满足对于极低尾延迟的需求。该论文旨在通过改进垃圾回收器的设计，满足工业对尾延迟的极致要求。

Method: 论文提出了一种新型的垃圾回收器（Catalpa collector），专为Bosque编程语言和运行时设计。该回收器利用Bosque的不可变性和无引用环等语言特性，实现了有界的回收暂停时间、恒定固定的内存开销，并且无需任何额外的栅栏或与应用代码的同步操作。

Result: Catalpa垃圾回收器能够在保持高吞吐量和较小内存开销的同时，显著减少延迟和响应时间的波动。此外，其实现方式无需对应用层代码增加同步或屏障等额外负担。

Conclusion: 通过与Bosque语言特性的结合，Catalpa回收器展示了面向极低尾延迟优化的垃圾回收方案，可满足现代工业应用对可用性和响应速度的严苛需求。

Abstract: The performance of an application/runtime is usually conceptualized as a
continuous function where, the lower the amount of memory/time used on a given
workload, then the better the compiler/runtime is. However, in practice, good
performance of an application is viewed as more of a binary function - either
the application responds in under, say 100 ms, and is fast enough for a user to
barely notice, or it takes a noticeable amount of time, leaving the user
waiting and potentially abandoning the task. Thus, performance really means how
often the application is fast enough to be usable, leading industrial
developers to focus on the 95th and 99th percentile tail-latencies as heavily,
or moreso, than average response time. Our vision is to create a software stack
that actively supports these needs via programming language and runtime system
design. In this paper we present a novel garbage-collector design, the Catalpa
collector, for the Bosque programming language and runtime. This allocator is
designed to minimize latency and variability while maintaining high-throughput
and incurring small memory overheads. To achieve these goals we leverage
various features of the Bosque language, including immutability and
reference-cycle freedom, to construct a collector that has bounded collection
pauses, incurs fixed-constant memory overheads, and does not require any
barriers or synchronization with application code.

</details>


### [2] [Extended Abstract: Towards a Performance Comparison of Syntax and Type-Directed NbE](https://arxiv.org/abs/2509.13489)
*Chester J. F. Gould,William J. Bowman*

Main category: cs.PL

TL;DR: 论文开发了一个平台，首次可以直接对比依赖类型检查中的两种主流类型相等判断方法，从性能角度分析其优劣并探索改进潜力。


<details>
  <summary>Details</summary>
Motivation: 在依赖类型系统中，类型相等检测方法至关重要，但目前对于基于语法导向和类型导向的相等检测孰优孰劣缺乏精确对比。

Method: 开发一个真实的对比平台，能够直接量化两种类型相等检测方法下的性能差异，并分析其背后的原因及改进途径。

Result: 能够直接对比语法导向和类型导向方法的效率，测量出类型导向检查的性能劣势，并为改进提供依据。

Conclusion: 提供了一种实用平台，以量化和分析依赖类型检查中两种主要类型相等检测方法的表现，有助于后续实现和理论上的改进。

Abstract: A key part of any dependent type-checker is the method for checking whether
two types are equal. A common claim is that syntax-directed equality is more
performant, although type-directed equality is more expressive. However, this
claim is difficult to make precise, since implementations choose only one or
the other approach, making a direct comparison impossible. We present some
work-in-progress developing a realistic platform for direct, apples-to-apples,
comparison of the two approaches, quantifying how much slower type-directed
equality checking is, and analyzing why and how it can be improved.

</details>


### [3] [CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing](https://arxiv.org/abs/2509.13982)
*Boyu Zhang,Ping He,Tianyu Du,Xuhong Zhang,Lei Yun,Kingsum Chow,Jianwei Yin*

Main category: cs.PL

TL;DR: 本文提出的CLMTracing框架实现了对开源代码语言模型在黑盒环境下的用户级追踪水印，具有优良鲁棒性和实用性，明显超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着开源代码语言模型（code LMs）的大规模应用，知识产权保护成为一个日益重要的问题。当前的水印技术在复杂、实际需求下，特别是在黑盒环境中对单用户追踪存在局限。

Method: 提出了CLMTracing框架，这是一种黑盒代码语言模型水印方案，通过基于规则的水印与保持实用性的注入方法，实现用户级别的模型追踪。同时，采用对鲁棒水印敏感的参数选择算法和对抗训练来提升对水印移除攻击的鲁棒性。

Result: 在多种最先进的代码语言模型（SOTA code LMs）上的综合评测显示，CLMTracing在无明显副作用的情况下，相比现有方法有显著提升，并对各种水印移除攻击表现出强大鲁棒性。

Conclusion: CLMTracing能有效满足用户级黑盒模型追踪需求，显著增强了开源代码LMs的知识产权保护性能。

Abstract: With the widespread adoption of open-source code language models (code LMs),
intellectual property (IP) protection has become an increasingly critical
concern. While current watermarking techniques have the potential to identify
the code LM to protect its IP, they have limitations when facing the more
practical and complex demand, i.e., offering the individual user-level tracing
in the black-box setting. This work presents CLMTracing, a black-box code LM
watermarking framework employing the rule-based watermarks and
utility-preserving injection method for user-level model tracing. CLMTracing
further incorporates a parameter selection algorithm sensitive to the robust
watermark and adversarial training to enhance the robustness against watermark
removal attacks. Comprehensive evaluations demonstrate CLMTracing is effective
across multiple state-of-the-art (SOTA) code LMs, showing significant harmless
improvements compared to existing SOTA baselines and strong robustness against
various removal attacks.

</details>


### [4] [Parallelizable Feynman-Kac Models for Universal Probabilistic Programming](https://arxiv.org/abs/2509.14092)
*Michele Boreale,Luisa Collodi*

Main category: cs.PL

TL;DR: 本文提出并形式化定义了概率程序图的期望型语义，理论证明并提升了粒子滤波推理一致性，设计了具备高性能的VPF算法，实验结果优于主流工具。


<details>
  <summary>Details</summary>
Motivation: 现有的概率程序推理方法在处理采样与条件重赋值，特别是在无界循环中的一致性和高效性方面存在理论与实际缺陷，本文旨在系统地提升理论上的可证明性并改善算法效率。

Method: 作者基于概率程序的形式化操作语义，提出了一种期望型语义，并将其嵌入到Feynman-Kac模型框架下，通过有限近似定理对该语义进行了理论界定，同时设计出VPF（向量化粒子滤波）算法并进行实验验证。

Result: 有限近似定理为无限轨迹语义提供了实用界限，VPF算法在实验中相比主流概率程序推理工具表现出更优性能，理论完善且实践有效。

Conclusion: 本文证明了粒子滤波（PF）算法在概率程序图（PPG）和他们定义的期望型语义下是一致的，并引入了新算法VPF，提升了推理效率。

Abstract: We study provably correct and efficient instantiations of Sequential Monte
Carlo (SMC) inference in the context of formal operational semantics of
Probabilistic Programs (PPs). We focus on universal PPs featuring sampling from
arbitrary measures and conditioning/reweighting in unbounded loops. We first
equip Probabilistic Program Graphs (PPGs), an automata-theoretic description
format of PPs, with an expectation-based semantics over infinite execution
traces, which also incorporates trace weights. We then prove a finite
approximation theorem that provides bounds to this semantics based on
expectations taken over finite, fixed-length traces. This enables us to frame
our semantics within a Feynman-Kac (FK) model, and ensures the consistency of
the Particle Filtering (PF) algorithm, an instance of SMC, with respect to our
semantics. Building on these results, we introduce VPF, a vectorized version of
the PF algorithm tailored to PPGs and our semantics. Experiments conducted with
a proof-of-concept implementation of VPF show very promising results compared
to state-of-the-art PP inference tools.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: 本文讨论了研究软件科学（RSS）是否应被归类为元科学，并通过概念比较与论证，认为RSS作为独立跨学科领域，并与元科学目标高度一致，其归类对学科发展和科研质量提升具有积极意义。


<details>
  <summary>Details</summary>
Motivation: 科学研究日益依赖计算方法，因此研究软件的质量、可复现性和透明度对科学结果的可靠性至关重要。如何定义及分类“研究软件科学”（RSS），关系到其在科学界的认可程度、资金支持和在科研改进中的整合。

Method: 论文通过界定元科学（metascience）和研究软件科学（RSS）的概念，比较它们的原则与目标，并探讨两者的交集，提出支持和反对RSS作为元科学分支的观点。

Result: 分析结果显示，RSS极大推动了元科学的核心目标，尤其是在计算可复现性方面，有效融合了技术、社会和认知的多重研究侧面。其最终分类取决于对元科学定义的宽泛或狭窄程度。

Conclusion: RSS最佳理解为独立且跨学科的领域，与元科学高度契合，在某些定义下甚至属于元科学范畴。将其作为元科学的一部分，有助于提升其在科研中的地位、促进资金投入，并提升研究机构对软件开发的重视。无论分类如何，为研究软件应用科学严谨性能保障科学发现的可靠性。

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [6] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文提出多智能体LLM方法，自动将报税法律文本转为可执行程序，并通过创新的元变测试生成与检测，显著提升了在复杂法规任务中的表现，展示了利用LLM构建法律关键软件的可行性与优势。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在将自然语言法规转化为可执行逻辑方面展现出巨大潜力，但其在法律关键场景中存在不可靠性（如歧义和幻觉）的问题，亟需提升其在实际敏感领域中的鲁棒性与可信度。

Method: 提出一种基于多智能体的代理型（agentic）方法，并以美国联邦报税为案例。该方法围绕元变测试（metamorphic testing）进行，创新引入更高阶的元变关系，通过比较模拟个体在结构性变动下的系统输出，实现对泛化和逻辑一致性的检查。此外，采用基于LLM的角色分工框架，自动化生成测试用例和代码。系统拥有负责翻译税法与进行元变测试以搜索反例的多个智能体。

Result: 在复杂税法任务中的实验表明，所提出的框架，使用较小的模型（GPT-4o-mini）时最差通过率为45%，明显优于更强大模型（GPT-4o和Claude 3.5，9-15%）。

Conclusion: 基于代理型LLM的多智能体方法，为从自然语言法规规范自动生成稳健、可信的法律关键软件指明了道路。该方法提升了自动化法律推理的鲁棒性，有助于高风险领域的真实应用。

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [7] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: 提出Prompt2DAG方法，将自然语言转为Airflow DAG。混合生成法大幅提升成功率和效率，成为自动化数据管道开发的优选。


<details>
  <summary>Details</summary>
Motivation: 数据管道的自动化生成对工程师要求很高，目前缺乏简单易用的方法来将自然语言需求转化为可执行的工作流代码，阻碍了数据管道开发的普及。

Method: 提出Prompt2DAG方法，将自然语言描述自动转化为可执行的Apache Airflow DAGs。共评估四种生成方式（直接、仅用LLM、混合、模板），涵盖13个LLM和5个案例，并用综合性评分体系（SAT、DST、PCT）进行对比分析。

Result: 实验表明，混合方法在成功率和代码质量等方面都优于其它方法（成功率78.5%，SAT: 6.79, DST: 7.67, PCT: 7.76），显著高于仅用LLM（66.2%）和直接方法（29.2%），且性价比高。

Conclusion: 高可靠性的混合方法在工作流生成自动化中效果最佳，强调结构化和灵活性兼顾，有助于推动数据管道开发的民主化。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [8] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 用大型语言模型增强crash报告，显著提升问题定位和修复推荐效果，对开发者更有用。


<details>
  <summary>Details</summary>
Motivation: 传统的crash报告经常缺乏开发者定位问题和修复bug所需的详细诊断信息。这导致软件维护和调试效率低下。

Method: 作者提出并研究了两种利用大型语言模型（LLM）增强crash报告的方法：Direct-LLM（单步基于堆栈跟踪上下文的增强）和Agentic-LLM（迭代式结合仓库代码寻找更多证据的增强）。在真实开源项目的crash报告数据集上进行了实验评估。

Result: 通过在492份真实crash报告上实验，LLM增强后的报告将Top-1问题定位准确率由原始报告的10.6%提升到40.2%-43.1%。建议性修复的质量与开发者实际补丁高度相似（CodeBLEU分数56-57%）。实验证明Agentic-LLM模型可以生成更好的根因分析与可操作的修复建议。16名开发者的用户研究也证实增强报告使理解和解决crash更容易，特别在修复指导方面提升最大。

Conclusion: 将大型语言模型应用于crash报告的增强，结合堆栈信息和代码仓库，可以显著提升问题定位和修复建议的质量，对开发者调试有极高应用价值。

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [9] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: AI代码审查工具Copilot在安全漏洞检测方面表现不佳，未能识别多种关键安全问题，主要指出低严重性问题。安全编码依然离不开专用安全工具和人工审核。


<details>
  <summary>Details</summary>
Motivation: 伴随AI工具在软件开发中的普及，确保这些工具能有效支持安全编码成为重要课题。评估AI工具对安全审查能力有现实需求。

Method: 使用多种编程语言和应用域的开源项目中的标记漏洞代码样本系统评估Copilot对常见安全缺陷的发现及反馈能力。

Result: Copilot常常无法识别关键安全漏洞（如SQL注入、XSS、不安全反序列化），其反馈更偏向低级别问题。安全开发仍需专用工具与人工审查。

Conclusion: GitHub Copilot的代码审查功能在发现关键安全漏洞方面效果有限，主要关注低严重性问题，如代码风格和拼写错误。

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [10] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest大幅提升了ML notebook回归测试能力，可自动生成单元级断言，有效检测bug和避免性能回退，并已在主流ML库CI应用，被用户证实方便且有用。


<details>
  <summary>Details</summary>
Motivation: 目前的数据科学家和机器学习工程师常用notebook进行模型管道的原型设计和实验，但notebook在测试方面支持有限，缺乏有效的回归测试，导致一些细微但有害的错误难以被发现，进而影响性能和结果。

Method: 提出了NBTest，首个可在notebook单元级编写断言并在pytest或CI流水线中运行的回归测试框架。NBTest包含断言API库和JupyterLab插件，同时开发了针对关键组件（如数据处理、模型构建和评估）的单元级断言自动生成方法，并运用统计方法减少断言的非确定性（flakiness）。

Result: 在592个Kaggle notebook上评估NBTest，自动生成21163个断言，平均每个notebook产生35.75个断言。断言对ML特定变异的杀伤分数为0.57。NBTest能够用最新断言检测旧版本notebook的回归错误，通过统计技术降低了非确定性，同时保持高错误检测能力。NBTest已被某主流ML库CI采用，17人用户调查结果显示易用性评分4.3/5，实用性评分4.24/5。

Conclusion: NBTest提升了ML notebook的可靠性和可维护性，无需增加开发者负担。它能自动生成有效的回归测试断言，检测回归错误，易于集成到现有开发流程中，并在实际和用户研究中表现良好。

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [11] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 本文提出PromptSE框架，量化代码生成模型对提示表达敏感性的稳定性，发现性能与稳定性大多解耦，并用创新指标AUC-E支持模型比较，为AI软件开发工具的稳健性评估提供新维度。


<details>
  <summary>Details</summary>
Motivation: 现有的代码生成模型在软件开发中应用广泛，但对于模型对不同提示表达的敏感性缺乏深入分析。不同情绪或沟通风格表达的同一需求会导致模型输出显著不同，而现有评测只关注模型的最优性能。

Method: 提出PromptSE框架，利用情感和个性模板生成语义等价的提示变体，并提供基于概率的连续评分和二元通过率的稳定性评估方法，当不可获取logits时也可使用。结果通过提出的AUC-E指标进行聚合，可以进行模型间的横向比较。

Result: 对来自三个家族的14个模型进行评测，发现性能和稳定性实际上是基本解耦的优化目标，且展示了与模型架构和规模相关的规律，挑战了关于模型鲁棒性的常见假设。

Conclusion: PromptSE框架为闭源模型的快速筛查和研究环境下的详细稳定性分析提供支持，将提示稳定性作为补充评估维度，帮助实践者在部署和模型选择时量化性能-稳定性权衡，有助于建立更可信的AI辅助软件开发工具。

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [12] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 针对代码语言模型泄露敏感信息的问题，作者提出了无需整体再训练的敏感记忆删除方法CodeEraser，并在主流模型上验证了其实用性和高效性，为提升代码模型安全性提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 现有的代码语言模型（CLM）在代码生成和摘要等任务中表现优异，但会无意中记忆并在特定提示下泄露训练数据中的敏感信息，这对隐私构成严重威胁。现有如数据去重、差分隐私等防护手段都需要模型整体再训练，代价高昂。此文旨在探讨能否有效且高效地消除CLM中记忆的敏感信息。

Method: 本文利用机器去学习（machine unlearning）技术对CLM进行后处理，以在不重新训练整个模型的前提下，去除已训练模型中特定敏感信息。具体做法包括量化模型对敏感数据的记忆风险，构建5万个高风险敏感样本作为去学习目标，并分析了两种主流基于梯度上升的去学习方法（标准法与约束法），提出了改进方法CodeEraser，能够选择性删除代码中的敏感片段，同时保持代码结构与功能正确。

Result: 大量实验表明，在CodeParrot、CodeGen-Mono和Qwen2.5-Coder三类主流CLM上，CodeEraser能有效且高效地抹除目标敏感记忆，同时保持模型的实用性。

Conclusion: 机器去学习是应对CLM敏感信息泄露问题的一种高效且有效的方法，提出的CodeEraser方法能在不损害模型整体表现的前提下，精准消除模型对敏感代码片段的记忆。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [13] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 本文深入分析了大推理模型(LRMs)在代码生成时的推理流程，构建了推理行为分类法，发现不同模型各有推理特点，并证明合理利用推理信息可提升代码生成准确率，为自动编程等应用带来新优化思路。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在代码生成等软件工程任务中的应用广泛，但对于能够进行多步推理的前沿大推理模型（LRMs），其推理模式及其对代码生成的影响缺乏系统性分析。

Method: 对多个主流LRMs进行代码生成测试，采用开放编码手动注释推理过程，从而制定推理行为分类法。通过经验研究，分析不同模型的推理差异、与代码正确性的关系等，并尝试基于推理特点优化提示策略。

Result: 提出涵盖4个阶段15种推理行为的分类法，发现LRMs具有人类般的编程流程，有的任务额外触发如搭建骨架、发现缺陷等动作。Qwen3展现迭代推理，DeepSeek-R1-7B更像瀑布式流程。测试发现诸如生成单元测试、骨架等动作有助于代码正确性，并提出基于推理模式优化的提示方法能提升生成质量。

Conclusion: 首次系统性揭示了LRMs在代码生成中的推理行为模式，探明推理与代码产出的关系，并以此指导有效优化自动生成策略，提高代码质量，对后续模型设计和实践具有重要启示价值。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [14] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: 本文提出FAMAS，通过系统性轨迹分析和创新的怀疑度算法，有效提升了多智能体系统的失败归因能力，并在多个基准上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）虽然在复杂问题自动化上表现出色，但其失败归因（定位具体导致失败的代理动作）依然困难且费时，严重影响系统调试和改进。现有手段对此问题关注不足。

Method: 作者提出了FAMAS，一种光谱分析法驱动的MAS失败归因方法。其核心为：通过系统性轨迹重放与抽象，利用轨迹变化进行光谱分析，并创新设计了结合代理与动作激活模式的怀疑度计算公式，更精准估计失败归因。

Result: 在Who and When基准测试集上与12个基线方法对比，FAMAS在失败归因表现上全面超越所有对手，显示了该方法的有效性和优越性。

Conclusion: FAMAS为MAS提供了自动化、高效的失败归因方案，显著提升了系统可调试性和改进效率。其计算怀疑度公式和整体框架对MAS研发具有重要参考价值。

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [15] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了基于span级采样的Autoscope方法，既保障了追踪数据结构，又显著减少了存储压力，并在实验中展现出比现有采样方法更好的表现，提升了根因分析能力。


<details>
  <summary>Details</summary>
Motivation: 在微服务系统中，分布式追踪是重要的诊断工具，但由于追踪日志数据量大，后端存储压力极大。常见的应对方法是追踪采样，但已有采样方法常常丢弃有用的正常追踪数据，影响后续分析。

Method: 提出Trace Sampling 2.0，在span层面（而非整条追踪）进行采样并保持追踪结构一致。基于此方法实现了Autoscope，利用静态分析提取关键执行逻辑，保证重要span被保留。

Result: 在两个开源微服务上评估，Autoscope可减少81.2%的追踪存储量，同时保留98.1%的故障span，明显优于现有追踪级采样方法。在根因分析任务中，准确率提升平均8.3%。

Conclusion: Autoscope可大幅提升微服务系统的可观测性和存储效率，具备实际应用价值。

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [16] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: 本文发现，基于Prompt的大型语言模型（LLM）无需依赖大量标注数据就能实现软件需求分类，并且few-shot及persona类提示方法可实现甚至超越微调模型的效果，提升任务泛化与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的需求分类方法大多依赖于监督学习，但这种方法需要大量带标签的数据，数据标注成本高、耗时久且依赖领域，泛化能力弱并且每次任务常需重新训练。

Method: 本研究测试基于Prompt的大型语言模型（LLM）是否能减少对数据的需求。对比了多种LLM模型和不同的提示方式（zero shot, few shot, persona, chain of thought），并在PROMISE和SecReq两个英文数据集上进行了多任务实验，将最佳LLM设置与强大的微调Transformer基线做比较。

Result: 结果显示，基于Prompt的LLMs，尤其是采用few-shot prompt时，效果可以等同甚至优于基线。增加persona或persona结合chain of thought还能进一步提升表现。

Conclusion: 基于Prompt的LLMs是一个实际且可扩展的选项，能够降低对大规模数据标注的依赖，提高在多任务间的泛化能力。

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [17] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: 本文系统评估了GenAI在软件建模教育中的伦理议题，发现相关研究极其稀缺，仅有三篇文献涉及。作者呼吁急需发展系统性的伦理框架，以规范和指导GenAI在建模教育中的应用，保障其负责任、公平和透明地融入教学实践。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）在软件建模教育中的应用日益普及，但相关的伦理影响尚未得到充分探讨。目前在教育实践中缺乏对公平性、责任性、透明度和多样性等伦理原则的详细指导与规范，这促使作者希望系统性梳理和审视这一领域的伦理研究现状。

Method: 作者通过系统性文献综述的方法，检索了ACM Digital Library、IEEE Xplore、Scopus、ScienceDirect、SpringerLink、Web of Science等六大计算机科学数据库，对涉及GenAI在软件建模教育中伦理问题的相关研究进行筛选和分析。

Result: 在初步检索的1386篇独立论文中，仅有3篇明确讨论了GenAI在软件建模教育中的伦理问题。通过对这三篇论文的详细分析，作者总结了现有研究在责任、公平、透明和多样性等方面的不足及挑战，进一步指出了该领域伦理讨论的严重缺失和研究机遇。

Conclusion: 当前关于生成式人工智能在软件建模教育中伦理议题的研究极其稀少，存在显著研究空白和实践指导缺失。作者呼吁在未来对GenAI在教育领域的负责任整合过程中，建立完善的伦理框架和规范。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [18] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本论文系统分析了SWE-Bench自动化代码修复工具失败根因，提出专家-执行者协作框架，大幅提升原系统对棘手问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 自动化代码问题修复技术虽然取得进展，但当前模型在许多任务上仍然失败，且现有评估只给出整体的解决率，缺乏对失败根本原因的深入分析，难以发现模型弱点或指导改进。

Method: 分析了三款主流SOTA工具（涵盖流水线和智能体架构）在SWE-Bench-Verified基准下的性能和效率，并对150个失败案例进行系统人工分析，建立了失败类型的详细分类体系。随后，提出了包含专家-执行者双智能体的协作架构框架，以专家监督和校正执行智能体的推理过程。

Result: 系统性地揭示了不同架构下失败类型的分布特征，特别指出智能体架构多数失败因推理与认知死锁。新提出的协作架构成功解决了22.2%的原单智能体无法解决的问题。

Conclusion: 通过诊断性评估和协作架构设计能有效提升自动化代码修复工具的健壮性和问题解决能力。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [19] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 本研究评估了将传统软件流程应用于LLM多智能体系统的效果。结果显示，不同流程对效率、代码质量和成本有显著影响，建议根据具体项目目标选择相应流程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）驱动的多智能体系统（MAS）正在改变软件开发格局，但如何协调这些智能体的合作是个挑战。传统的软件开发流程（如瀑布模型、V模型和敏捷方法）已在项目管理中证明有效，因此本文探讨将这些流程结构应用于LLM多智能体系统的可行性及其带来的影响。

Method: 作者在三种流程模型（瀑布、V模型、敏捷）下，结合四种不同的GPT模型，执行了11个多样化软件项目，总共进行了132次实验，并用标准化指标评估了每次输出，包括代码规模（文件数、代码行数）、成本（执行时间、令牌消耗）和质量（代码异味、AI和人工检测的bug）。

Result: 实验结果表明，流程模型和LLM的选择都会显著影响系统性能。瀑布模式效率最高，V模型生成的代码最冗长，敏捷方法则获得了最高的代码质量，但计算消耗较大。

Conclusion: 传统开发流程可以有效迁移到LLM多智能体系统中，但在质量、成本和适应性之间存在权衡。因此，流程选择应根据项目目标进行调整，如优先考虑效率、健壮性或结构化验证等。

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [20] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: 本论文发现CoT推理长度过长会带来实际性能损失，提出SEER框架以自适应压缩推理过程，在软件工程等领域显著提升了准确性与效率，是资源受限环境下提升LLM实用性的有效方案。


<details>
  <summary>Details</summary>
Motivation: 虽然Chain-of-Thought（CoT）推理可以提升大型语言模型在算术、逻辑和常识任务上的表现，但输出长度增加带来计算消耗上升，延迟、内存和KV缓存的压力变大。在软件工程等对输出简洁且确定性要求高的场景，这些问题尤为突出。作者希望探究CoT长度与性能之间的权衡，并提出解决方案。

Method: 作者通过代码生成基准的实证研究分析了CoT长度对任务表现的影响。进一步提出SEER（Self-Enhancing Efficient Reasoning）框架，结合Best-of-N采样与任务感知的自适应过滤机制，根据推理前的输出动态调整阈值，实现对CoT压缩和节省计算资源。

Result: 实验显示，过长的CoT反而导致截断、准确性下降，甚至提高最多5倍延迟，且失败的输出长度都比成功的长。使用SEER后，CoT平均缩短42.1%，减少了截断现象，提高了准确性，并消除了多数无限循环。

Conclusion: CoT并非越长越好，需动态调控。SEER框架可在保持准确率的前提下，大幅减少CoT长度与资源消耗，使增强CoT的LLMs更高效稳健，尤其适合受限资源场景。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [21] [Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement](https://arxiv.org/abs/2509.13699)
*Max Barth,Marie-Christine Jakobs*

Main category: cs.LO

TL;DR: 本文提出并实现了一种针对软件模型检测的并行抽象方法，显著提高了验证效率，并优于现有的并行方法。


<details>
  <summary>Details</summary>
Motivation: 自动软件验证对于软件质量保证非常重要，但自动验证（特别是软件模型检测）耗时较长，影响其实用性，例如在持续集成中的应用。因此，急需加快自动验证流程以提升其实用价值。一个解决思路是利用多核CPU并行化验证流程。

Method: 提出了一种并行化trace abstraction（基于抽象的软件模型检测）的方法。具体为并行分析不同可能违反安全属性的trace（程序路径），从而并行化抽象细化过程。此方法已在软件验证工具Ultimate Automizer中实现并评估。

Result: 实验评估表明，该并行化trace abstraction方法相比传统的顺序方法在许多耗时任务上能更快提供结果，其效果也优于最新的并行抽象模型检测方法DSS。

Conclusion: 文中提出的并行化trace abstraction方法能够显著提升软件模型检测在多核环境下的效率，有助于自动软件验证的实际应用，特别是在需要快速反馈的场景。

Abstract: Automatic software verification is a valuable means for software quality
assurance. However, automatic verification and in particular software model
checking can be time-consuming, which hinders their practical applicability
e.g., the use in continuous integration. One solution to address the issue is
to reduce the response time of the verification procedure by leveraging today's
multi-core CPUs.
  In this paper, we propose a solution to parallelize trace abstraction, an
abstraction-based approach to software model checking. The underlying idea of
our approach is to parallelize the abstraction refinement. More concretely, our
approach analyzes different traces (syntactic program paths) that could violate
the safety property in parallel. We realize our parallelized version of trace
abstraction in the verification tool Ulti mate Automizer and perform a thorough
evaluation. Our evaluation shows that our parallelization is more effective
than sequential trace abstraction and can provide results significantly faster
on many time-consuming tasks. Also, our approach is more effective than DSS, a
recent parallel approach to abstraction-based software model checking.

</details>


### [22] [Algorithmic Perspective on Toda's Theorem](https://arxiv.org/abs/2509.13871)
*Dror Fried,Etay Segal,Gad E. Yaron*

Main category: cs.LO

TL;DR: 作者将Toda定理的QBF到模型计数的理论归约转变为具体算法，并不断优化。新算法超过原型，但仍有实际可用性上的挑战需突破。


<details>
  <summary>Details</summary>
Motivation: Toda定理在计算复杂性理论中非常重要，其核心在于将具有常量数量量词的QBF问题归约到模型计数问题。随着模型计数工具的发展，研究者开始探索能否利用这种理论归约实现高效解决QBF问题的算法。本文基于该动机，从算法角度重新审视Toda归约。

Method: 本文首先将Toda归约转化为具体算法，可以在给定QBF公式和概率测度下，以相应置信度输出正确结果。之后，作者分析算法细节，并对原型算法进行理论和算法上的改进。

Result: 改进后的算法在效率和精度方面均优于原始的朴素原型。但论文也指出，要将Toda归约转化为具有竞争力的求解器仍面临诸多挑战。

Conclusion: 本文把Toda的理论归约具体化为实际算法，并完成了相关的改进和分析，取得了一定进展。但最终实用化仍需解决不少难题。

Abstract: Toda's Theorem is a fundamental result in computational complexity theory,
whose proof relies on a reduction from a QBF problem with a constant number of
quantifiers to a model counting problem. While this reduction, henceforth
called Toda's reduction, is of a purely theoretical nature, the recent progress
of model counting tools raises the question of whether the reduction can be
utilized to an efficient algorithm for solving QBF. In this work, we address
this question by looking at Toda's reduction from an algorithmic perspective.
We first convert the reduction into a concrete algorithm that given a QBF
formula and a probability measure, produces the correct result with a
confidence level corresponding to the given measure. Beyond obtaining a naive
prototype, our algorithm and the analysis that follows shed light on the fine
details of the reduction that are so far left elusive. Then, we improve this
prototype through various theoretical and algorithmic refinements. While our
results show a significant progress over the naive prototype, they also provide
a clearer understanding of the remaining challenges in turning Toda's reduction
into a competitive solver.

</details>


### [23] [The Complexity of Deciding Characteristic Formulae Modulo Nested Simulation (extended abstract)](https://arxiv.org/abs/2509.14089)
*Luca Aceto,Antonis Achilleos,Aggeliki Chalki,Anna Ingólfsdóttir*

Main category: cs.LO

TL;DR: 本论文证明了嵌套模拟语义下的特征公式判定问题复杂性随嵌套层级增加而从coNP-完全升至PSPACE-完全，进一步明确了不同逻辑语义环境下模型验证任务的理论复杂性边界。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析判定模态逻辑公式（用于刻画嵌套模拟语义）是否为某一过程特征公式的问题的复杂性。这涉及公式的可满足性和素性判定，与进程理论和模型验证相关。

Method: 作者主要通过复杂性理论分析，对不同嵌套层数的模拟语义下，公式判定问题归约至NP、coNP和PSPACE相关复杂性类，并使用逻辑与复杂性论证。

Result: 当嵌套层数为2时，判定公式为素公式的问题属于coNP-完全；而嵌套层数为3及以上时，该判定问题为PSPACE-完全。对于2嵌套模拟语义的特征公式判定，复杂性落在DP类（NP与coNP语言的交集）。

Conclusion: 判定n嵌套模拟语义特征公式的复杂性随嵌套层数的增加显著提升，2层时复杂度较低（coNP-完全），而n≥3层时达到PSPACE-完全，这为相关模型验证算法的可行性带来影响。

Abstract: This paper studies the complexity of determining whether a formula in the
modal logics characterizing the nested-simulation semantics is characteristic
for some process, which is equivalent to determining whether the formula is
satisfiable and prime. The main results are that the problem of determining
whether a formula is prime in the modal logic characterizing the
2-nested-simulation preorder is coNP-complete and is PSPACE-complete in the
case of the n-nested-simulation preorder, when n>=3. This establishes that
deciding characteristic formulae for the n-nested simulation semantics is
PSPACE-complete, when n>=3. In the case of the 2-nested simulation semantics,
that problem lies in the complexity class DP, which consists of languages that
can be expressed as the intersection of one language in NP and of one in coNP.

</details>


### [24] [An Automaton-based Characterisation of First-Order Logic over Infinite Trees](https://arxiv.org/abs/2509.14090)
*Massimo Benerecetti,Dario Della Monica,Angelo Matteo,Fabio Mogavero,Gabriele Puppis*

Main category: cs.LO

TL;DR: 本文通过两种犹豫树自动机，刻画了无限无序树上一阶逻辑（FO）的表达能力，证明了其与两种分支时序逻辑（polcCTLp和cCTL*[f]）是等价的，指出FO只能表达树分支上的安全性或余安全性性质。这为相关逻辑与自动机理论的互相转化提供了基础。


<details>
  <summary>Details</summary>
Motivation: 研究一阶逻辑（FO）在无序无限树上的表达能力，以及其与分支时序逻辑（如CTL类逻辑）的联系。

Method: 引入并定义两类犹豫树自动机，并证明其精确刻画了两种分支时序逻辑（polcCTLp 和 cCTL*[f]）的表达能力，后者已知等价于无限树上的FO。通过自动机方法对逻辑进行了刻画和分析。

Result: 证明了所提的两类树自动机分别与 polcCTLp 和 cCTL*[f] 的表达能力等价，而这两种时序逻辑又与无限树上的FO等价。同时，自动机的方法给出了这两类逻辑的范式，并揭示了FO在树分支上仅能表达安全性（safety）或余安全性（co-safety）性质。

Conclusion: 通过自动机的理论刻画，不仅明确了无限树上FO的表达能力，还建立了与分支时序逻辑的紧密联系，促进了时序逻辑与自动机理论的融合发展。

Abstract: In this paper, we study First Order Logic (FO) over (unordered) infinite
trees and its connection with branching-time temporal logics. More
specifically, we provide an automata-theoretic characterisation of FO
interpreted over infinite trees. To this end, two different classes of hesitant
tree automata are introduced and proved to capture precisely the expressive
power of two branching time temporal logics, denoted polcCTLp and cCTL*[f],
which are, respectively, a restricted version of counting CTL with past and
counting CTL* over finite paths, both of which have been previously shown
equivalent to FO over infinite trees. The two automata characterisations
naturally lead to normal forms for the two temporal logics, and highlight the
fact that FO can only express properties of the tree branches which are either
safety or co-safety in nature.

</details>


### [25] [Metric Equational Theories](https://arxiv.org/abs/2509.14094)
*Radu Mardare,Neil Ghani,Eigil Rischel*

Main category: cs.LO

TL;DR: 本论文通过结合量化等价理论与富Lawvere理论，提出并完善了用于度量空间代数结构的证明系统，实现了从有限元数到度量空间元数的理论升级。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在将量化等价理论（QET）与富Lawvere理论结合，提出针对度量空间上的代数结构的合理且完备的证明系统。

Method: 论文将QET扩展为度量等价理论（MET），其中运算的元数不再是有限集合，而是来自可数度量空间。同时借鉴富Lawvere理论，将运算的元数视为底层λ-可达范畴中的λ-可呈现对象，并对QET的证明系统进行适应性扩展。

Result: 成功构建了适用于MET的完备且可靠的证明系统，并解决了MET中项的有效性无法独立于等式有效性的问题，充分利用了度量空间的特殊结构。

Conclusion: 该方法为度量空间上的代数结构提供了合理且完备的证明系统，实现了从QET到MET的理论扩展。

Abstract: This paper proposes appropriate sound and complete proof systems for
algebraic structures over metric spaces by combining the development of
Quantitative Equational Theories (QET) with the Enriched Lawvere Theories. We
extend QETs to Metric Equational Theories (METs) where operations no longer
have finite sets as arities (as in QETs and the general theory of universal
algebras), but arities are now drawn from countable metric spaces. This
extension is inspired by the theory of Enriched Lawvere Theories, which
suggests that the arities of operations should be the lambda-presentable
objects of the underlying lambda-accessible category. In this setting, the
validity of terms in METs can no longer be guaranteed independently of the
validity of equations, as is the case with QET. We solve this problem, and
adapt the sound and complete proof system for QETs to these more general METs,
taking advantage of the specific structure of metric spaces.

</details>


### [26] [The Complexity of Generalized HyperLTL with Stuttering and Contexts](https://arxiv.org/abs/2509.14095)
*Gaëtan Regaud,Martin Zimmermann*

Main category: cs.LO

TL;DR: 本文分析了一种可表达异步超属性的广义HyperLTL逻辑，证明其可满足性复杂度与HyperLTL相同，但模型检测显著更难，达到了二阶算术的复杂度。


<details>
  <summary>Details</summary>
Motivation: HyperLTL只适用于同步超属性，不能表达异步超属性。本文提出的广义HyperLTL旨在扩展表达能力，以便刻画异步超属性，并探究其理论复杂性。

Method: 对带stuttering和contexts的HyperLTL进行了复杂性分析，包括可满足性和模型检测的上下界证明。

Result: 带stuttering与contexts的HyperLTL的可满足性为Σ₁¹-完全，复杂度不高于普通HyperLTL，但模型检测问题等价于二阶算术的真值问题，远比HyperLTL模型检测要难。这一模型检测的下界即便只允许stuttering或contexts也成立。

Conclusion: 本文确定了带stuttering和contexts的广义HyperLTL的可满足性和模型检测问题的复杂度，并指明了其与现有逻辑之间的本质区别。

Abstract: We settle the complexity of satisfiability and model-checking for generalized
HyperLTL with stuttering and contexts, an expressive logic for the
specification of asynchronous hyperproperties. Such properties cannot be
specified in HyperLTL, as it is restricted to synchronous hyperproperties.
Nevertheless, we prove that satisfiability is $\Sigma_1^1$-complete and thus
not harder than for HyperLTL. On the other hand, we prove that model-checking
is equivalent to truth in second-order arithmetic, and thus much harder than
the decidable HyperLTL model-checking problem. The lower bounds for the
model-checking problem hold even when only allowing stuttering or only allowing
contexts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文系统评估了多个LLM在意大利语性别中立重写任务上的表现，并提出了衡量性别中立性和语义保真度的二维框架。结果显示微调后的小型LLM能高效达到或超过大型模型的效果，为今后的模型训练和任务评估提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 在意大利语等有语法性别的语言中，实现性别中立的文本重写（Gender-neutral rewriting, GNR）非常具有挑战性。当前针对意大利语GNR的LLM系统性评估和优化手段有限。

Method: 提出了一个二维评价框架，同时衡量性别中立性和语义保真度。作者采用多种LLM模型进行few-shot提示、模型微调以及针对性数据清洗，并开展模型间系统对比。

Result: 开源大型语言模型在意大利语GNR任务上优于现有唯一专用模型。经过微调的小模型在效果上达到甚至超过最佳开源LLM，并且模型体量更小。还发现提升训练数据的性别中立性和语义保真度之间存在权衡。

Conclusion: 开源LLM经微调后在意大利语性别中立重写任务可实现性能与效率兼具的提升，同时提出的方法为多维度评估该任务提供了标准。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [28] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: FOMC会议纪要分析难度大，Op-Fed数据集通过创新分层标注和主动学习提升数据质量，有望促进自动化分析与后续研究。


<details>
  <summary>Details</summary>
Motivation: FOMC会议经常影响大量人的经济决策，但现有数据集面临类别不均衡和句间依赖强的问题，影响自动化分析。

Method: 提出五阶段分层标注方案以精细区分观点、货币政策相关内容和立场，同时通过主动学习筛选实例以增加阳性样本数量。

Result: 发布Op-Fed数据集，包含1044条经人工标注的句子及上下文。采用现有闭权重大模型进行零样本测试，在观点分类上达到0.80准确率，在货币政策立场分类上为0.61，低于人工基线0.89。

Conclusion: Op-Fed数据集可助力模型训练、信心校准，并为后续注释扩展提供种子数据。

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [29] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: 论文通过DSTC12 Track 1两个子任务反映，当前对话系统评估在多维度与文化安全性方面均存在不足，多数方法在文化相关安全检测上亟待提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs发展迅速，对对话系统的评估需求增加，但现有评估常受限于传统指标，对安全性及多文化识别不充分，急需完善。

Method: 组织了DSTC12 Track 1比赛，包括多维度自动评估和多语言、多文化安全检测两项任务。各任务设定不同的基线模型，并评估参赛队伍的表现。

Result: 在多维度对话评估任务中，Llama-3-8B基线得分低，说明有很大提升空间；在多语言安全检测上，参赛队优于基线，但在文化安全子任务上基线表现反而更好，凸显文化安全检测的挑战。

Conclusion: 现有的对话系统评估方法在多维度评估和文化安全性方面仍有明显不足，尤其是在文化相关安全检测领域亟需改进。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [30] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 针对迁移学习中的跨任务泛化挑战，作者提出了框架并对10个大模型进行了细致分析，发现源数据的隐含统计及语言特征对迁移效果影响显著，为更好地适配和优化大模型提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在实际应用中常常面临无法穷举所有高质量训练数据及跨领域迁移学习的挑战，因此需要分析模型跨任务泛化能力与迁移学习带来的副作用。

Method: 提出了一个分析框架，通过构建迁移学习矩阵与降维方法，对不同任务间的交互进行剖析。在实验中，训练并分析了10个模型，识别模型的潜在能力，并探索迁移学习的副作用。

Result: 实验发现模型性能提升并不完全依赖数据集的表层相似性和数据质量，而是受到源数据集的隐含统计特征（如类别分布、生成长度倾向等）以及具体语言特征的影响更大。

Conclusion: 本研究揭示了迁移学习中的复杂动态机制，有助于未来更可预测、更高效地调整和应用大语言模型。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [31] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 本研究揭示歧义信息在LLM内部由少量神经元编码，通过操作这些神经元，可实现问题歧义的检测与回答控制，为模型的可解释性和实用性提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现实中的问题经常带有歧义，但大语言模型（LLMs）通常会自信地给出答案，而不是寻求澄清。作者希望探究LLMs是否能内部表示、检测和控制问题歧义。

Method: 作者分析了LLMs的内部表征，发现歧义信息在极少数神经元内以线性方式编码。通过在模型的预填充阶段定位这些“歧义编码神经元” (AENs)，并训练探针检测其输出，进行模型层级分析及神经元操控实验。

Result: 用AENs输出训练的探针能有效检测歧义，并在多个数据集上超越其他主流基线。AENs主要在浅层出现，可通过操控其激活来控制模型回答与否。

Conclusion: LLMs在内部分布着紧凑且可控的歧义表征，有助于模型更可解释和可控。可以通过极少数神经元调控LLM的回答策略。

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [32] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 本文提出CL$^2$GEC多学科持续学习语法纠错基准，覆盖10个领域、10,000句。实验证明正则化方法能更好应对遗忘，为多领域语法纠错研究打下基础。


<details>
  <summary>Details</summary>
Motivation: 当前学术领域对于自动写作辅助系统的需求不断增长，尤其是在多学科背景下。现有中文语法错误纠正（CGEC）研究缺乏针对不同学科学术写作的专用基准，且未充分关注持续学习（Continual Learning, CL）应对领域特异性语言变化及避免灾难性遗忘的潜力。

Method: 本文提出了CL$^2$GEC，这是首个面向中国文学语法错误纠正的持续学习基准。基准包含一万句人工标注、覆盖十个学科的句子，具备各学科独特语言风格及错误类型。利用连续学习设定，模拟对不同学科的逐步接触，并评估大语言模型在顺序微调、参数高效适配及四种代表性持续学习算法下的表现。评价指标涵盖标准语法纠错指标及针对任务变化调整的持续学习指标。

Result: 实验结果显示，基于正则化的方法比重放法和普通顺序方法更有效地缓解遗忘问题。CL$^2$GEC基准能够全面评估跨学科适应性语法纠错，为相关方向后续研究提供了严谨的数据基础。

Conclusion: 正则化方式在持续学习场景下缓解灾难性遗忘表现优异，CL$^2$GEC基准为多领域适应性中文语法纠错研究提供了重要参考。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [33] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: 本文提出的AgentCTG新框架，通过多智能体协作实现了更精细的文本生成控制，并在多个任务和实际场景中取得了领先效果，验证了其实用性和创新性。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成（CTG）在实现细粒度条件控制、可伸缩性、领域知识学习等方面仍存在不少挑战，且实际应用下需考虑成本和更精确的控制。

Method: 提出了一个新颖且可扩展的框架AgentCTG，通过模拟多智能体协作机制，实现对文本生成过程的精细且复杂的控制，并引入了自动提示模块提升生成效果。

Result: AgentCTG在多个公开数据集上取得了SOTA（最新最优）效果，提出并验证了一个面向实际应用的挑战性任务（Character-Driven Rewriting），成功提升在线导航和角色扮演交互体验。

Conclusion: AgentCTG框架能够更好地实现高精度和复杂的文本生成控制，促进了在线社区个性化和用户参与度，展示了良好的实际应用前景。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [34] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: 本文提出了CARE检索增强推理框架，用少量标注数据显著提升大模型在知识密集型问答中的一致性和准确率，效果优于主流微调和检索方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在根据给定信息回答问题时，往往存在上下文一致性差、答案前后不一的问题。现有方法要么依赖高昂的有监督微调生成证据，要么让模型学会网络搜索，但并不能切实提升模型对已给上下文的利用能力。该论文旨在解决LLMs在知识密集型任务中上下文利用不足与答案不一致的问题。

Method: 提出了一种新的原生检索增强推理框架CARE，训练LLMs在推理过程中明确结合场内检索到的证据，并用模型自带的检索能力进行信息整合。该方法只需少量标注证据数据，通过策略性地检索推理链中的in-context tokens，提升了检索和生成答案的准确度。

Result: 在多个真实世界及反事实问答基准上，大量实验结果表明，CARE在检索准确率和答案生成表现上，均显著优于有监督微调、传统检索增强生成及外部检索的方法。

Conclusion: CARE方法大幅提升了LLMs在知识密集型任务中的准确性、可靠性与效率，是该领域的一个基础性进展。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [35] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在日语比较级自然语言推理中的表现，发现模型受Prompt和标签影响很大且对日语独有现象处理不佳，引入逻辑语义表征可提升模型推理效果。


<details>
  <summary>Details</summary>
Motivation: 主流大语言模型在自然语言推理（NLI）方面表现良好，但在处理包含数值和逻辑表达的推理任务、尤其是比较级时仍存在困难，尤其对训练语料中并未充分覆盖的语言。文章旨在揭示在日语等非英语语境下模型推理能力的短板。

Method: 构建了聚焦于日本语比较级的NLI数据集，采用零样本（zero-shot）和少样本（few-shot）设置，对多种大型语言模型进行了评估，并比较Prompt格式和标签影响，同时测试逻辑语义表示对结果的提升。

Result: 模型在零样本情况下对Prompt格式极为敏感，少样本设置时又受标签影响明显。对于日语独有的语言现象，模型处理能力不足。逻辑语义表征能助力模型在困难推理任务中的准确性提升。

Conclusion: 通过构建日本语比较级相关的NLI数据集，发现大语言模型在面对非主流训练语言、特别是日语中的比较级推理任务时表现不佳，且Prompt格式和少量示例中的标签都会显著影响模型表现。逻辑语义表示能提升模型在难题上的推理准确性。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [36] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 将LLMs通过DSPy-based prompt优化后，能高效且灵活地处理临床文本与结构化数据分类任务，表现与专业系统相当但更简便。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLMs）在文本生成方面表现突出，但它们处理临床分类任务中的结构化数据（如时间序列）的能力尚未得到充分探索。该工作旨在解决LLMs在处理临床结构化数据上的不足，提升其在医疗领域的应用价值。

Method: 本文通过DSPy-based prompt optimization方法，对指令微调的LLMs进行适配，使其能够联合处理临床笔记和结构化电子健康记录（EHR）输入，从而应用于临床分类任务。

Result: 实验结果显示，该方法在性能上与专业的多模态系统相当，同时方法复杂度更低，并具备更强的任务适应性。

Conclusion: 利用DSPy-based prompt优化，可使LLMs有效联合处理文本和结构化数据，简化多模态医疗任务的实现。

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [37] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: 论文提出了DSCC-HS方法，在解码过程中用两个小型代理模型动态引导大型语言模型，有效抑制幻觉且无需修改主模型，在多个基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在实际部署中因幻觉问题（hallucination）而可靠性受限，现有如RAG等方法主要是反应式的，难以根本解决幻觉问题。

Method: 提出了一种新颖的前瞻性方法——动态自增强校准幻觉抑制方法（DSCC-HS），灵感来自双过程认知理论。该方法训练了两个代理模型：事实对齐代理（FAP）和幻觉检测代理（HDP），并在推理过程中，通过FAP和HDP的logits差值生成实时导向向量，动态引导目标模型，无需修改目标模型，属于即插即用式。

Result: 在TruthfulQA上Factual Consistency Rate达到了99.2%；在长文本BioGEN基准上，FActScore取得了46.50的最高分，均为最先进（state-of-the-art）表现。

Conclusion: DSCC-HS为提升LLM事实性提供了高效、可行的新途径，能够有效抑制幻觉并提升内容的真实可靠性。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [38] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 研究团队开发了利用NLP筛查高严重度医疗事件报告的工具，特别是在放射肿瘤领域。通过迁移学习等方法显著提升了模型跨机构的泛化能力，模型的表现与专业人员接近，显示出自动化筛查的现实可行性。


<details>
  <summary>Details</summary>
Motivation: 事件报告在医疗安全和质量改进中非常重要，但人工审核需要耗费大量时间和专业知识，因此亟需自动化工具。本文提出利用自然语言处理（NLP）技术自动筛查放射肿瘤科中的高严重度事件报告。

Method: 作者收集并标注了两个放射肿瘤事件报告数据库，分别来自本院（7,094份）及IAEA SAFRON（571份），采用支持向量机（SVM）和预训练大语言模型BlueBERT进行分类，并通过迁移学习提高跨机构通用性。部分报告还进行了人工编辑以评估模型与人类专家的对比表现。

Result: 本院数据测试集上，SVM和BlueBERT的AUROC分别为0.82和0.81。直接应用到IAEA SAFRON数据，模型性能较弱，SVM为0.42，BlueBERT为0.56。但通过迁移学习微调BlueBERT_TRANSFER，性能提升至0.78。经人工编辑后的报告，模型（SVM: 0.85，BlueBERT_TRANSFER: 0.74）与人工专家（0.81）的表现相近。

Conclusion: 该研究开发出可跨机构应用的NLP自动筛查工具，能在放射肿瘤事件报告中筛查高严重度报告，效果接近人类专家水平，具备临床应用潜力。

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [39] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 本文提出了一种分阶段、无需训练的高效提示压缩方法（DSPC），通过句子筛选与token级重要性分析，大幅减少token数，且实验效果优于当前最佳方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在自然语言处理领域的成功，促使输入模型的提示（prompt）越来越长，从而带来计算成本增加的问题。为解决这一“提示膨胀”问题，需要对提示进行压缩。现有方法多需额外训练小规模模型，增加了计算负担。

Method: 提出了一种无需训练、分为两阶段的提示压缩方法：Dual-Stage Progressive Compression（DSPC）。首先，粗粒度阶段通过TF-IDF过滤语义相关性低的句子；接着，细粒度阶段依据注意力贡献、跨模型损失变化和位置重要性评估token的重要性，剪除低效词元，同时保持语义完整。

Result: 在LLaMA-3.1-8B-Instruct 和 GPT-3.5-Turbo等模型、受限token预算下，DSPC均取得一致性的效果提升。例如，在Longbench数据集的FewShot任务中，DSPC仅用三分之一token，即获得49.17分，较当前最优的LongLLMLingua方法提升7.76分。

Conclusion: DSPC方法实现了无训练、有效的prompt压缩，能显著降低token消耗，并在多项任务中优于现有最先进方法。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [40] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 针对日语比较级NLI任务，作者提出了基于组合语义的逻辑推理系统ccg-jcomp，并在相关数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于英语和日语比较级在形态和语义上存在显著差异，现有针对英文的逻辑推理系统难以直接应用于日文，因此有必要开发适用于日语的推理方法。

Method: 提出并实现了ccg-jcomp，一个逻辑推理系统，专门针对日语比较级表达并基于组合语义。通过与现有大型语言模型进行准确度对比评估。

Result: ccg-jcomp系统在含有比较级的日语NLI数据集上表现优异，其准确性优于部分现有的大型语言模型。

Conclusion: 该研究提出了一个基于组合语义的日语比较级逻辑推理系统，并在日语NLI数据集上有效性得到证实。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [41] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 作者系统评估了阿拉伯方言识别任务中多种高效方法，发现LoRA微调模型在识别准确性上超越了传统全量微调和其他软提示技术，是最优选择。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言识别（ADI）任务对数据和参数的效率要求高，当前大语言模型（LLM）在方言细粒度识别方面存在挑战，需要探索更高效的方法。

Method: 探讨多种高效策略，包括软提示（prefix-tuning, prompt-tuning, P-tuning及其V2版本）、LoRA参数重构；数据高效方面分析硬提示结合zero-shot及few-shot推理，参数高效方面利用阿语特定encoder模型和开源decoder-only模型进行实验。

Result: LLM在zero-shot或few-shot下方言识别效果较差，软提示encoder模型表现更佳，LoRA微调模型性能最佳，甚至优于全量微调。

Conclusion: 参数高效和数据高效技术提升了阿拉伯方言识别性能，尤其是LoRA微调方法，表现优于其它方法。

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [42] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文提出了能力感知多视角课程指令微调框架CAMPUS，克服了传统方法的刚性不足，显著提升了大语言模型微调效能。


<details>
  <summary>Details</summary>
Motivation: 现有的教学法在微调大语言模型时，过于依赖静态的难度评估，忽略了模型能力的动态变化，从而限制了模型训练效果。

Method: 提出了CAMPUS框架，通过能力感知和多视角动态课程安排，实现子课程的动态选择和课程表的灵活调整，并结合多种难度标准进行调度。

Result: 大量实验显示CAMPUS在高效指令微调任务中明显优于现有主流方法。

Conclusion: CAMPUS通过动态、能力感知的课程调度，提升了指令微调大语言模型的效率和性能。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [43] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 本研究针对具语法性别的职业名称，提出了针对性别排序公平性的比较度量，并构建了多语言测试集，通过评估现有多语言模型，发现这些模型在职位排序上均存在性别偏见。


<details>
  <summary>Details</summary>
Motivation: 自动职位排名系统可能因职位名称的语法性别分配而表现出性别偏见，因此需要系统化分析和评价这些模型的性别公正性。

Method: 提出了一套用于控制性别比较排序的度量指标，特别是RBO排序偏好重叠指标；并在四种具备语法性别的语言中，生成了包含男性和女性职业形式及相关性标注的职位名称测试集，并用这些测试集评估若干现成多语言模型的性别偏见。

Result: 所有被评估的现成多语言模型在职位名称排序任务中都表现出不同程度的性别偏见。

Conclusion: 现有的多语言自动职位排序模型在处理带有语法性别的职位名称时，都不同程度存在性别偏见。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [44] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 本文提出利用几何方法实现黑盒大语言模型的全局与局部不确定性估算，有效检测并降低幻觉出现，在医疗等关键场景表现尤为优越。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在多任务上表现优异，但仍存在“幻觉”问题，即生成语法上合理但错误的答案。现有的不确定性量化方法用于检测这种幻觉，但黑盒方法无法同时估计全局和局部不确定性。作者希望解决这一缺陷，提出更好的黑盒估算方法。

Method: 该论文采用了一种基于几何的框架，通过对模型生成的批量响应进行原型分析，并仅凭黑盒访问来操作。全局上提出了“几何体积”，测量响应嵌入的原型的凸包体积。局部上提出了“几何怀疑”，可以对单个响应进行可靠性排序并优先选取。与以前只给出全局单一分数的扩散方法不同，新方法能为个体响应分配语义边界点，从而归因其可靠性。理论上还证明了凸包体积与熵之间的联系。

Result: 在短文本问答和医学数据集上进行了实验。结果显示，该框架在常规数据集上的表现与主流方法相当或更好，而在医疗领域数据集（幻觉风险极高）上的表现明显优于现有方法。

Conclusion: 作者提出的基于几何的黑盒不确定性估算方法，既能给出全局评价，又能细致裁量个体输出，对幻觉检测与可靠性评估均有优越表现，尤其在高风险领域（如医疗）优势明显。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [45] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: 本文介绍了AutoMin 2025自动会议纪要与问答共享任务，尽管参赛队伍减少，但通过多语言、多领域设计和基线系统，系统评测了当前主流大语言模型在会议纪要和问答上的性能。


<details>
  <summary>Details</summary>
Motivation: 会议纪要和基于会议内容问答是实际场景中的关键需求，现有自动生成系统面临多语种、多领域和问答精度等挑战；本项工作旨在推动领域发展、测试最新大模型性能，并促进行业交流。

Method: 以AutoMin共享任务为平台，设定会议纪要自动生成和基于会议文本的问答两大任务，涵盖英、捷两种语言和项目会议、欧盟议会两个领域。同时设计单语及跨语问答实验，并附加多种基线系统供评测。

Result: 只有一支团队参与会议纪要任务，两支团队参与问答任务，但通过组织者的基线方案，得以系统性地对比和评估各大模型在不同语言、领域与任务上的表现。

Conclusion: 虽然参与度下降，但组织者通过加入多个基线系统，系统性地评估了当前大语言模型在自动会议纪要与会议问答上的表现，为未来相关研究提供了重要参考。

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [46] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文发现，大型语言模型在处理德国方言时，表现出显著负面偏见。模型倾向于将方言使用者与负面形容词相关联，不仅在判断任务中重现社会刻板印象，而且标明方言身份时偏见更明显。这提示未来在语言公正性方面应加强模型设计与评估。


<details>
  <summary>Details</summary>
Motivation: 方言是人类文化的重要组成部分，但方言使用者经常面临社会负面刻板印象。本文动机是探究这些社会偏见是否同样存在于大型语言模型（LLMs）之中。

Method: 依托社会语言学对方言感知的研究文献，分析与方言使用者相关的常见性格特征。设计了两个任务：关联任务和决策任务，分别评估LLMs的方言命名偏见和方言使用偏见。此外，作者新建了一个评估语料库，将七种德国地区方言与标准德语句子配对，以检测模型在方言使用上的偏见。

Result: 在关联任务中，所有评测的LLM对德国方言使用者表现出显著的方言命名和方言使用偏见，具体表现为与负面形容词的关联。在决策任务中，所有模型也重现了这些偏见。此外，作者发现，明确标注方言人口信息会比仅通过方言使用的隐性提示更强化模型的偏见，与以往研究结论相反。

Conclusion: 当前主流LLM对德国方言使用者存在显著的负面偏见，无论在关联还是决策任务中均有体现。明确标注方言人口信息反而会加剧这种偏见，这对方言或其他语言变体的处理提出了警示。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [47] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 本文系统分析了12个LLM在不同社会偏见场景下的价值观对齐表现，发现参数规模并非决定对齐优劣的关键，同家族模型一致性高，对齐表现受场景类型影响。小型模型在可读性和一致性之间表现出权衡，研究为LLM安全性和公平性提供了更细致的理解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在与人类价值观对齐上会出现偏差，尤其在涉及复杂和敏感社会偏见的场景中。以往研究发现LLM存在价值观对齐的问题，但尚不清楚模型在不同类型场景下的表现是否有差异。本文动机是深入探索LLM在不同社会偏见场景中的价值观对齐情况。

Method: 本文分析了来自四个模型家族的12个大型语言模型，并在四个数据集上进行了广泛实验，比较了不同类型偏见场景（如负面与非负面问题）下LLM与人类价值观的对齐情况。同时，通过模型生成的解释分析了LLM在理解社会偏见上的能力，并比较了通过微调赋能的小型语言模型的解释效果。

Result: 大规模参数的LLM不一定具有更低的对齐偏差率和攻击成功率。LLM在特定类型场景下存在对齐偏好，同一模型家族的模型在判断上更为一致。不同LLM对社会偏见的理解能力差异不显著，且LLM更偏爱自身生成的解释。微调后的小型语言模型生成的解释更具可读性，但一致性较低。

Conclusion: LLM的参数规模不必然决定其价值观对齐能力；模型在不同类型场景下对齐表现存在明显偏好，且同家族模型一致性高。赋能小型模型可以提升其解释可读性但可能影响一致性。整体上，对齐与理解能力需多维度考量。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [48] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER框架结合证据检索和大语言模型推理，显著提升了生物医学事实核查的效果，在多个数据集上表现优异，代码和数据已开放。


<details>
  <summary>Details</summary>
Motivation: 当前医疗健康领域存在虚假信息，如疫苗犹豫和未经证实的治疗方法，这威胁到公共健康和医疗系统的信任。自动化事实核查取得进展，但生物医学领域核查仍因术语复杂、需专家知识及证据基础的重要性而面临挑战。

Method: 提出了CER框架，将科学证据检索、大语言模型推理与监督式真实性预测相结合。利用先进的证据检索和语言模型生成技术，有效降低输出的幻觉风险，确保内容基于可验证的科学证据。

Result: 在多个专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上，CER框架取得了最新最优的性能，并展现出良好的跨数据集泛化能力。

Conclusion: CER能有效集成证据和推理，提升生物医学事实核查的准确性和可靠性，优于现有技术，并且代码和数据已公开以促进透明和复现。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [49] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 本文提出CER框架，结合证据检索和大模型推理，有效提升了生物医学事实核查的准确性和泛化能力，并在多个数据集上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的虚假信息对公共健康和医疗信任构成威胁。尽管自动事实核查技术不断进步，但由于生物医学术语复杂、需要专业知识以及对科学证据依赖性强，医学主张的自动核查仍极具挑战性。

Method: 提出了CER（Combining Evidence and Reasoning）框架，结合了科学证据检索、大语言模型进行推理、以及有监督的真实性预测。通过将大语言模型的文本生成能力与高质量生物医学证据检索技术结合，保证生成内容基于可验证的科学证据，减少‘幻觉’风险。

Result: 在专家标注数据集（HealthFC、BioASQ-7b、SciFact）上评估，CER取得了业界领先的性能，并在不同数据集间表现出良好的泛化能力。代码和数据已开源，以增强透明度与可复现性。

Conclusion: CER框架为生物医学事实核查提供了有效的新方案，通过证据检索与大语言模型推理的结合，实现了高性能和可靠性，有助于提升医疗虚假信息自动检测的水平。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [50] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型对词义消歧及生成型词义理解的能力，发现其不仅与专门WSD系统性能持平，在生成任务中还能以高准确率解释词义，特别是在自由解释方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在理解词语上下文含义方面的能力极为重要，但现有研究对其真正理解词义的程度仍欠缺深入探索。本文旨在填补这一研究空白。

Method: 本文评估了经过指令微调的LLM在词义消歧（WSD）任务中的表现，并与专为该任务设计的最优系统进行对比。同时分析了当前顶级开源及闭源LLM在三个生成型任务（词义定义生成、自由解释和举例）中的表现。

Result: 在WSD任务上，主流模型如GPT-4o和DeepSeek-V3表现与专门系统相当，同时在不同领域和难度下展现更佳的鲁棒性。在生成型任务中，LLM可以高达98%准确率解释上下文中的词义，尤其在自由解释任务上表现最好，与其生成能力最为契合。

Conclusion: LLM不仅能在结构化词义消歧任务中与专用系统媲美，在开放性生成任务中也展现出极强的上下文理解能力，未来有望在更多实际语言处理场景中应用。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [51] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 论文发现多语种检索增强生成系统倾向优先引用与查询相同语言的文档，尤其是英文，对低资源语言偏见更明显，有时甚至会因语言偏好忽略文档相关性，这提示多语种模型在生成与引用过程有系统性语言偏向。


<details>
  <summary>Details</summary>
Motivation: 当前多语种检索增强生成（mRAG）系统可以支持跨语言带引用的回答，但尚不清楚混合不同语言的文档是否会以意想不到的方式影响生成与引用行为。

Method: 提出了一种受控方法，通过分析模型内部，固定文档相关性，测量模型对语言的偏好。覆盖八种语言与六个开源模型，系统性研究引用倾向。

Result: 模型在英文查询时更偏好引用英文内容，且这种偏好在低资源语言及文档处于中间位置时更强烈。模型有时会为语言偏好牺牲文档相关性，说明引用行为不仅由信息量驱动。

Conclusion: 多语种检索增强生成系统在处理多语境时存在语言偏好，引用行为受语言影响，而非仅依赖文档信息度，相关发现有助于理解多语种语言模型的语境利用与引用机制。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [52] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 论文提出将多个人工评估数据集融合并引入长上下文，显著提升了机器翻译质量自动评估与人工得分的相关性。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在提升机器翻译质量自动评估的方法，尤其关注结合更长上下文以更好地贴合人工判分。

Method: 基于COMET框架，训练多语种回归模型以预测段落级误差标注(ESA)分数。通过拼接人工标注的同领域句子生成长上下文训练数据，并将MQM、SQM与DA三个人工评估数据集归一化后融合训练。

Result: 实验表明，结合长上下文的信息训练出来的模型，其与人工判分的相关度高于仅用短句训练的模型。

Conclusion: 采用长上下文和多数据集融合的方法，能够更有效地提升自动翻译质量评估系统与人工评分的一致性。

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [53] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC通过逐步剪枝冗余推理链，大幅降低了SC方法在大模型推理时的计算资源和延时，精度不但未下降还有提升，是一种更高效的Test-Time Scaling技术。


<details>
  <summary>Details</summary>
Motivation: 动机是现有Self-Consistency（SC）方法在提升大模型推理能力时计算成本过高，且加速方法有限，急需高效替代方案。

Method: 提出Slim-SC，一种基于推理链间相似性进行逐步剪枝的新策略，用于删除冗余的推理链。

Result: Slim-SC在三个STEM数据集和两种LLM架构上的实验结果显示，推理延时降低最多45%，KVC使用量减少最多26%，并且准确率不降反增。

Conclusion: Slim-SC方法能有效降低SC的计算开销，同时在保证或提升准确率的前提下，加速推理过程。

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [54] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT通过检测LLMs推理过程中的答案收敛，实现提前终止生成，平均节省约41%的推理token，但准确率基本不降，是高效实用的推理改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）通过生成较长的思维链（CoT）能更好地解决复杂问题，但这种长链带来了高推理成本。

Method: 提出ES-CoT方法，在推理过程中通过检测模型答案的收敛性和提前终止来缩短CoT的长度。在每一步推理后，将模型当前的最终答案作为step answer，并追踪连续相同step answer的长度。当长度剧增且超过某一阈值时，即认为模型已收敛，提前终止生成。通过实证和理论分析论证其有效性。

Result: 在3个大语言模型的5个推理数据集上，ES-CoT平均减少约41%的推理token数，同时准确率与标准CoT相当。此外，ES-CoT能够无缝结合自洽提示，并且对超参数设置具有鲁棒性。

Conclusion: ES-CoT是一种实用且高效的推理方法，能以较小性能损失大幅降低推理成本，并适用于不同任务和模型。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [55] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: 提出Hala系列阿拉伯语指令与翻译模型，通过FP8压缩、指令语料扩充及多参数规模训练，实现推理性能和任务表现双提升，在阿拉伯语NLP领域达到新SOTA，并开放资源以加速研究进展。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语自然语言处理领域缺少高质量的指令及翻译模型，且现有大模型推理成本较高。该研究旨在提升阿拉伯语相关任务的表现，并降低模型部署和推理的资源消耗。

Method: 提出了名为Hala的阿拉伯语指令与翻译模型家族，使用translate-and-tune流程：首先将强力的AR↔EN教师模型压缩到FP8，实现约2倍吞吐量提升；利用该教师构建高保真的双语数据用于监督；之后用轻量级语言模型LFM2-1.2B微调，并将高质量英文指令集翻译为阿拉伯语，形成大规模指令语料；最终分别训练含350M、700M、1.2B和9B参数的Hala模型，通过slerp合并方法平衡阿拉伯语专长与基础模型能力。

Result: 在阿拉伯语相关基准测试中，Hala在nano（≤2B）和small（7-9B）两类模型中均达到领域内的最新效果，并显著超越其基础模型。

Conclusion: 通过高效压缩与大规模指令语料生成，有效提升了阿拉伯语模型的实际表现和推理效率，对推动阿拉伯语NLP研究具有重要意义，并公开了模型、数据及工具链以支持后续研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [56] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 该研究发现机器翻译系统的语音评估结果与文本评估基本一致，但能揭示部分系统的显著差异，说明语音评估更自然、有效，强调今后需将语音纳入机器翻译质量评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译（MT）评估方法过于依赖文本，未能充分反映语音等实际应用情境下的翻译质量。现实中机器翻译常通过语音展示，如Google翻译语音模式等，因此仅依赖文本评估可能不能全面反映系统性能。

Method: 本研究从WMT General MT Shared Task选取10个机器翻译系统，分别以文本和音频两种方式进行评估。评测数据通过Amazon Mechanical Turk众包收集，采用统计显著性检验及自我复现实验验证音频评估方法的可靠性和一致性。

Result: 众包获得的音频评估结果与文本评估总体一致，但在部分机器翻译系统上可识别出显著的质量差异。此差异归因于语音作为更丰富和自然的评估方式，有助于发现文本中难以察觉的问题。

Conclusion: 建议未来机器翻译评价体系引入语音或多模态评估，提高对实际应用中机器翻译系统质量的全面把握。

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [57] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文实验证实，训练数据中上下文相关实例的稀缺限制了翻译模型的上下文利用能力，不同现象改进难以互相转移。提出的两种训练策略可有效提升翻译准确率，单语和多语设置下分别提升6和8个百分点。


<details>
  <summary>Details</summary>
Motivation: 为了实现接近人类水平的翻译能力，需要有效利用上下文信息以保证文本连贯性并妥善处理如代词消歧等复杂语言现象。然而，标准训练数据中丰富上下文实例的稀缺被认为是阻碍翻译模型利用上下文能力提升的主要原因。本文旨在系统性验证这一假设。

Method: 通过构建包含不同比例上下文相关实例的训练集，在单语和多语环境下对比实验，分析上下文稀疏性对模型表现的影响。此外，提出并实证两种提升数据利用效率的训练策略。

Result: 实验结果表明训练数据中上下文丰富实例的稀疏性与模型利用上下文能力和表现显著相关，印证了稀疏性是提升译文连贯性的瓶颈。不同上下文现象的提升无法互相泛化，跨语言迁移效果有限。此外，两种训练策略在相关评价指标（ctxPro）下分别带来6和8个百分点的准确率提升。

Conclusion: 数据中上下文丰富实例的不足严重限制了翻译模型的上下文利用能力，是实现高质量机器翻译的重要瓶颈。针对性的数据扩充和策略能有效提升模型上下文理解和表达能力。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [58] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 通过在多智能体大语言模型辩论系统中集成置信度表达机制，提升了模型之间交流的有效性和整体任务表现，提出了ConfMAD框架并通过实验进行验证和分析。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论系统中的大语言模型在表达自身知识优势时缺乏置信度表达，导致交流不充分、错误坚持或过早收敛至次优答案，影响系统效能。

Method: 开发了ConfMAD框架，将置信表达机制整合至多智能体辩论全过程，通过实验进行效果验证并分析辩论动态。

Result: 引入置信度表达能显著提升多智能体辩论系统的任务表现，并对辩论机制有积极影响。

Conclusion: 在多智能体辩论系统中引入置信度表达，能够提升系统性能和辩论有效性。实验验证了方法的有效性，并深入分析了置信度对辩论过程的影响。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [59] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 本文提出了基于问题对话的手语翻译（QB-SLT）新任务，并设计了跨模态自监督学习及自注意力加权特征融合方法，在新数据集上显著提升了翻译效果，证明对话辅助比传统gloss更易用且更高效。


<details>
  <summary>Details</summary>
Motivation: 手语翻译（SLT）旨在弥合聋人与听力正常人之间的沟通鸿沟，而对话能为翻译提供重要的上下文信息。以往方法更多依赖手语转录（gloss）标注，但这种标注难度较大，而对话信息天然易于获取和标注，因此作者希望探索如何有效整合对话信息提升手语翻译效果。

Method: 提出了一种基于问题的手语翻译（QB-SLT）新任务，并设计了跨模态自监督学习与Sigmoid自注意力加权（SSL-SSAW）的融合方法。在该方法中，利用对比学习对齐手语与文本（问题）的多模态特征，再通过SSAW模块自适应地从问题和手语序列中提取特征，还通过自监督学习提升表征和翻译能力。

Result: 在新构建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上，提出的方法SSL-SSAW取得了SOTA（最新最优）的翻译性能。研究发现，与传统难以获取的gloss辅助相比，易于获取的问题信息同样可达到甚至超过原有性能。同时可视化结果也证明了对话信息能有效提升翻译质量。

Conclusion: 基于问题的手语翻译任务能够有效结合对话上下文，提出的SSL-SSAW方法实现了不同模态的特征融合和自适应提取，在多个数据集上的实验结果表现优异，显示对话辅助对手语翻译具有实际价值和应用前景。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [60] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一款支持25种语言的快、强、多语种ASR/AST模型，表现优于同类大型系统且速度更快，并发布了参数更小的新版本。


<details>
  <summary>Details</summary>
Motivation: 为实现高效、准确且多语种支持的自动语音识别与语音到文本翻译系统，兼顾速度与模型鲁棒性，尤其在计算资源有限时仍能保持性能。

Method: 使用FastConformer编码器和Transformer解码器，支持25种语言；采用包括Granary和NeMo ASR Set 3.0在内的1.7M小时数据进行两阶段预训练和微调，动态数据均衡，并添加非语音音频减少幻觉。时间戳方面使用NeMo Forced Aligner结合CTC辅助模型。

Result: Canary-1B-v2在英语ASR任务上比Whisper-large-v3快10倍且识别更优，在多语种ASR和AST任务上与大型模型（如Seamless-M4T-v2-large、LLM-based系统）具有竞争性能。

Conclusion: Canary-1B-v2在多语种ASR和AST任务上表现优越，尤其在英语ASR方面超过Whisper-large-v3，并且速度更快，同时与更大规模模型在多语种任务上有竞争力。还发布了更小参数量的Parakeet-TDT-0.6B-v3。

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [61] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS数据集是首个大规模、多语言、低资源定位的代码切换语音数据集，覆盖113种语言对和52种语言，为代码切换语音识别与翻译研究极大拓展了语料基础。


<details>
  <summary>Details</summary>
Motivation: 目前在语音识别和翻译研究中，代码切换（code-switched）的语料库主要集中在资源丰富的语言，而低资源语言的相关数据较少。该论文旨在扩展相关研究到更多语言，尤其是低资源语言。

Method: 作者构建了CS-FLEURS数据集，包含52种语言的113种代码切换语言对，并提供了4个测试集和一个训练集。测试集包含真实人声及多种语音合成技术生成的代码切换句子，训练集则以生成式语音合成为主。

Result: CS-FLEURS数据集涵盖大量低资源语言的代码切换语音数据集，具有多样化的数据来源（真实人声、合成语音等），大大扩展了现有研究可涉及的语言范围。

Conclusion: CS-FLEURS数据集的构建为未来代码切换语音识别和翻译研究打开了新局面，尤其用于低资源语言，为相关系统开发和评估提供了有力支持。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [62] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 本文提出了针对多模态大语言模型关联能力的新基准测试AssoCiAm，通过区分内外部模糊性并采用混合计算方案，提升了评估的准确性和可靠性。实验证明认知与关联相关，新的方法可避免模型输出的随机性，提高评估的科学性。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态大型语言模型（MLLMs）在通向通用人工智能（AGI）过程中备受关注，而创造力作为AGI的重要能力，需通过模型的关联能力来体现。然而，现有评估关联能力的框架常忽略关联任务中的模糊性，导致评估可靠性下降。

Method: 本文将关联任务中的模糊性分为两类——内部模糊性与外部模糊性，提出了名为AssoCiAm的基准测试，通过混合计算的方法，实现了绕开模糊性准确评估关联能力。

Result: 大量实验显示，MLLM的认知能力与关联能力之间存在强正相关。同时，存在模糊性的评价会导致模型的输出更随机。新方法可有效提升评估的准确度和可靠性。

Conclusion: AssoCiAm基准测试能够更好地避开关联任务中的模糊性，为MLLMs关联能力提供可靠评估工具，对提升模型创造力和理解关联有重要意义。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [63] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本研究利用行为金融学和财务情境，构建个性化财务顾问数据集，并微调Qwen-3-8B模型，实现与更大模型同等水平的个性化财务建议，显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 目前的个性化财务建议需要综合考虑用户目标、约束、风险容忍度及所在司法地区。过去，大模型应用主要聚焦于投资者和财务规划师的支持系统，但这些基于代理的流程成本高昂且实际财务回报率不足25%。因此，亟需更加高效且可复现的个性化财务建议方法。

Method: 本研究提出一个新颖且可复现的框架，将相关财务情境与行为金融学结合，生成端到端的财务顾问监督数据。基于此框架，作者创建了包含1.9万条推理样本的数据集，并对Qwen-3-8B模型进行了全面微调。随后，通过分割测试集和盲评LLM评审，验证模型表现。

Result: 经过数据精细整理和行为因素整合，微调后的Qwen-3-8B在事实准确性、流畅性和个性化指标上，与更大规模的基线模型（14-32B参数）表现相当，同时相比大模型成本降低80%。

Conclusion: 所提出的框架和中型模型能够高性价比地提供高度个性化的财务建议，具备实际应用潜力，有助于降低个性化财务顾问系统的部署与维护成本。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [64] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 本研究用大语言模型分析英美议会75年移民话语，发现美国两极分化加剧，英国党派分歧虽持续但一致性较高，且叙事框架逐渐安全化。方法验证了LLMs可用于大规模细粒度政治话语分析。


<details>
  <summary>Details</summary>
Motivation: 当前对移民议题在英国和美国议会话语中的长时段动态缺乏系统性、细粒度的量化分析，尤其是通过自动化技术揭示其立场、叙事框架及演变趋势。

Method: 利用开源大语言模型（LLMs），对75年来英国议会及美国国会辩论中涉及移民的话语进行自动立场标注和情感追踪，并在英国部分进一步开发半自动框架提取更细致的叙事框架。

Result: 美国的移民相关话语日趋两极分化，而英国议会内不同党派态度相对保持一致，但工党和保守党之间的意识形态差距始终存在。2025年该差距达到历史最大。英国移民话语正从社会融合等长期整合类叙事，转向边境安全、非法移民等安全化叙事，且移民相关法律的讨论由国内法逐步转向国际法与人权。

Conclusion: 大语言模型能够实现对大规模历史政治文本中复杂、细粒度话语结构的可扩展自动分析，为理解议题演变提供技术与实证支撑。

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [65] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是完整开放的多语言大模型，数据合规和多语种覆盖优异，性能接近或超过同类开放模型，科研产出完全开放，推动开放AI生态发展。


<details>
  <summary>Details</summary>
Motivation: 当前开放模型生态系统存在两个主要问题：数据合规性和多语言表示能力不足。许多模型发布了权重，但没有可复现的数据处理流程，也忽略了内容所有者权益。

Method: 预训练仅使用公开数据，严格遵守robots.txt及内容过滤（排除不允许和有毒、涉及隐私的内容）；采用Goldfish训练目标以减少数据记忆并保持下游任务性能；扩大多语言覆盖，使用1800多种语言的15万亿token进行训练，其中约40%为非英语数据。所有相关科研产出，包括数据准备脚本、检查点、评估套件和训练代码，均以宽松许可发布，确保透明性和可扩展性。

Result: 发布了8B和70B规模的两个Apertus模型，在多语言基准测试中接近或超过其它开放权重模型的SOTA表现，尤其多语言能力突出。并且所有成果完全开放可获取。

Conclusion: Apertus套件解决了开放大模型数据合规和多语种覆盖两大难题，在保障内容权益与提升多语能力的同时，达到了领先的多语言效果，实现了完整开放和可审计性。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [66] [Hyper-Zagreb Indices of Hypergraphs with Application in Drug Design](https://arxiv.org/abs/2509.13787)
*Abdulkafi Sanad*

Main category: cs.DM

TL;DR: 该文首次提出并推广了超图上的 Hyper-Zagreb 指数，系统研究其数学性质及界限，并成功应用于QSAR药物活性预测，拓展了化学信息学中的超图建模工具。


<details>
  <summary>Details</summary>
Motivation: 传统的 Zagreb 指数作为分子结构描述符广泛用于图的研究和应用，然而对于复杂系统（如化学分子）自然建模为超图时，缺乏合适的结构描述工具。本文通过推广 Zagreb 指数到超图，以更全面分析和应用于化学与生物信息学。

Method: 定义和推广了第一、第二类 Hyper-Zagreb 指数，从图扩展到超图，并对一般超图、弱二部超图、超树、k-均匀超图等，讨论这些指数的界、极值结构和其性质。进一步，将这些指数应用于QSAR模型，进行药物设计和生物活性预测。

Result: 推广了 Hyper-Zagreb 指数至多种超图类别，系统刻画其数学界、极值，确认其在药物结构-活性分析（QSAR建模）中的应用与有效性。

Conclusion: 本文提出的 Hyper-Zagreb 指数能够有效刻画超图的结构特性，并展示其在药物设计和生物活性预测中的实际应用价值。

Abstract: Let $\mathcal{H}$ be a hypergraph on the non-empty finite vertex set
$V(\mathcal{H})$ with the hyperedge set $E(\mathcal{H})$, where each hyperedge
$e \in E(\mathcal{H})$ is a subset of $V(\mathcal{H})$ with at least two
vertices. This paper introduces the first and second Hyper-Zagreb indices for
hypergraphs, extending these well-known graph indices to hypergraphs. We
discuss bounds on these indices for general hypergraphs, weak bipartite
hypergraphs, hypertrees, $k$-uniform hypergraphs, $k$-uniform weak bipartite
hypergraphs, and $k$-uniform hypertrees, characterizing the extremal
hypergraphs that achieve these bounds. Additionally, we present a novel
application of these indices in drug design and bioactivity prediction,
demonstrating their utility in quantitative structure-activity relationship
(QSAR) modeling.

</details>


### [67] [4-uniform Maker-Breaker and Maker-Maker games are PSPACE-complete](https://arxiv.org/abs/2509.13819)
*Florian Galliot*

Main category: cs.DM

TL;DR: 本文证明了先手在两类顶点选择博弈中是否有必胜策略的问题，在4-均匀超图上的情形是PSPACE完全的，改进并收紧了相关复杂性结果。


<details>
  <summary>Details</summary>
Motivation: 此前对于类似博弈的复杂性界定并不精确，部分均匀性条件下存在复杂性间隙，作者旨在厘清4-均匀超图下的复杂性。

Method: 分析了这两类博弈，并通过复杂性理论证明了其决定性问题的PSPACE完全性，尤其关注于对超图的结构限制。

Result: 将现有结果推进到4-均匀超图，Maker-Maker游戏相较于先前rank 4超图获得了提升，Maker-Breaker游戏则缩小了3到5间的复杂性鸿沟。

Conclusion: 对于两种顶点选择游戏，判断先手是否有必胜策略在4-均匀超图上的情形是PSPACE完全的。

Abstract: We study two positional games where two players take turns picking a
previously unpicked vertex of a hypergraph $H$. We say a player fills an edge
of $H$ if that player has picked all the vertices of that edge. In the
Maker-Maker game, whoever first fills an edge wins, or we get a draw if no edge
is filled. In the Maker-Breaker game, the first player aims at filling an edge
while the second player aims at preventing the first player from filling an
edge. We show that, for both games, deciding whether the first player has a
winning strategy is a PSPACE-complete problem even when restricted to 4-uniform
hypergraphs. For the Maker-Maker game, this improves on a previous result for
hypergraphs of rank 4. For the Maker-Breaker game, this improves on a previous
result for 5-uniform hypergraphs, and closes the complexity gap as the problem
for hypergraphs of rank 3 is known to be solvable in polynomial time.

</details>


### [68] [Gremban Expansion for Signed Networks: Algebraic and Combinatorial Foundations for Community-Faction Detection](https://arxiv.org/abs/2509.14193)
*Fernando Diaz-Diaz,Karel Devriendt,Renaud Lambiotte*

Main category: cs.DM

TL;DR: 文章提出利用Gremban扩展和谱聚类，能够有效区分和检测有符号网络中的社群与派别结构，并为后续多划分及网络动力系统分析奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 有符号网络中社群与派别结构的识别在理论和应用中都具有重要意义。传统方法难以同时处理有符号关系的复杂性以及这两种中尺度结构的区分。

Method: 采用Gremban扩展将有符号图转换为更大的无符号图，从而可以将无符号图的标准分析技术应用到有符号网络。通过分析扩展图的对称性及其割集与原图社群、派别结构之间的双射联系，引入尊重对称性的谱聚类方法进行社群与派别检测。

Result: 提出了利用Gremban扩展区分和检测有符号网络社群与派别的新方法，并证明了扩展图割集与原图目标结构之间的数学联系。新方法能够自然区分两种结构，并支持多划分场景及拓展到动力网络系统。

Conclusion: 本文提出了一种基于Gremban扩展的谱聚类方法，用于区分和检测有符号网络中的社群和派别结构。该方法利用了Gremban扩展的对称性，实现了社群与派别的精确识别。

Abstract: This article deals with the characterization and detection of community and
faction structures in signed networks. We approach the study of these mesoscale
structures through the lens of the Gremban expansion. This graph operation
lifts a signed graph to a larger unsigned graph, and allows the extension of
standard techniques from unsigned to signed graphs. We develop the
combinatorial and algebraic properties of the Gremban expansion, with a focus
on its inherent involutive symmetry. The main technical result is a bijective
correspondence between symmetry-respecting cut-sets in the Gremban expansion,
and regular cut-sets and frustration sets in the signed graph (i.e., the
combinatorial structures that underlie communities and factions respectively).
This result forms the basis for our new approach to community-faction detection
in signed networks, which makes use of spectral clustering techniques that
naturally respect the required symmetries. We demonstrate how this approach
distinguishes the two mesoscale structures, how to generalize the approach to
multi-way clustering and discuss connections to network dynamical systems.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [69] [How Concise are Chains of co-Büchi Automata?](https://arxiv.org/abs/2509.14087)
*Rüdiger Ehlers*

Main category: cs.FL

TL;DR: COCOA可指数级提升表达简洁性，但在布尔运算下自动机规模会急剧膨胀，使用时需权衡其优缺点。


<details>
  <summary>Details</summary>
Motivation: COCOA（共Büchi自动机链）是近年来被提出用于表示任意ω-正则语言的规范模型，并能多项式时间最小化。虽然已知如何从确定性奇偶自动机构建COCOA，但其与更早自动机模型的关系尚不清楚。作者希望研究COCOA在表达上是否更为简洁，以及其在布尔运算下的表现。

Method: 分析不同模型之间表达能力的区别，重点比较COCOA与确定性奇偶自动机在表达同一语言时的自动机大小，并考察它们在布尔运算（如析取和合取）下的自动机规模增长情况。

Result: 作者证明即使链中的自动机全部是确定性的，COCOA依然可以相比确定性奇偶自动机更为简洁，具有指数级的表达优势。然而，在对COCOA进行布尔运算时，自动机的规模可能会指数级膨胀，而同样操作在确定性奇偶自动机上则仅产生多项式规模的膨胀。

Conclusion: COCOA在直接语言表示时比确定性奇偶自动机更为简洁，但这种简洁性在布尔运算下无法保持，表明COCOA虽然适用于部分场景，但并非对所有自动机操作都优于经典模型。

Abstract: Chains of co-B\"uchi automata (COCOA) have recently been introduced as a new
canonical model for representing arbitrary omega-regular languages. They can be
minimized in polynomial time and are hence an attractive language
representation for applications in which normally, deterministic omega-automata
are used. While it is known how to build COCOA from deterministic parity
automata, little is currently known about their relationship to automaton
models introduced earlier than COCOA.
  In this paper, we analyze the conciseness of chains of co-B\"uchi automata.
We show that even in the case that all automata in the chain are deterministic,
chains of co-B\"uchi automata can be exponentially more concise than
deterministic parity automata. We then answer the question if this conciseness
is retained when performing Boolean operations (such as disjunction and
conjunction) over COCOA by showing that there exist families of languages for
which these operations lead to an exponential growth of the sizes of the
automata. The families have the property that when representing them using
deterministic parity automata, taking the disjunction or conjunction of them
only requires a polynomial blow-up, which shows that Boolean operations over
COCOA do not retain their conciseness in general.

</details>
