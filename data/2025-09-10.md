<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)
*Youjie Li,Cheng Wan,Zhiqi Lin,Hongyu Zhu,Jiacheng Yang,Ziang Song,Xinyi Di,Jiawei Wu,Huiyao Shu,Wenlei Bao,Yanghua Peng,Haibin Lin,Li-Wen Chang*

Main category: cs.PL

TL;DR: veScale系统以SPMD范式改进了LLMs分布式训练，解决了随机数一致性、通信效率等问题，达到更快、更简洁的训练体验和一致的模型结果。


<details>
  <summary>Details</summary>
Motivation: LLMs训练规模与复杂性快速增长，当前复杂的分布式并行方式调试困难，因此需要更简单可调试的SPMD编程范式，但在eager模式下存在一致性和性能难题。

Method: 提出veScale，一个采用SPMD范式并支持eager模式的分布式训练系统，包括分布式随机数生成的新算法及优化通信效率。

Result: veScale相比先进系统如TorchTitan，训练速度提升最高达2.2倍，代码复杂度降低78.4%，并保证与单设备执行结果一致。

Conclusion: veScale系统能够在简化代码复杂度的同时实现与单设备一致的分布式训练结果，并显著提升训练性能。

Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.

</details>


### [2] [Fast and Extensible Hybrid Embeddings with Micros](https://arxiv.org/abs/2509.07551)
*Sean Bocirnea,William J. Bowman*

Main category: cs.PL

TL;DR: 采用微嵌入（IR为核心）替代宏嵌入，结合多种设计模式，实现了高效、可扩展的静态类型语言嵌入，比宏嵌入方法编译更快，并提出了新的抽象包装方案。


<details>
  <summary>Details</summary>
Motivation: 宏嵌入广泛用于定义可扩展语言嵌入，但其编译时性能较低。作者希望通过微嵌入以提升编译时效率，同时维持语言可扩展性。

Method: 提出以语法到中间表示（IR）为核心的微嵌入方法，而不是传统的宏嵌入（语法到语法转换），并将IR和其相关函数设计为可扩展，通过模式和新抽象实现灵活扩展性。

Result: 实现了静态类型语言的可扩展混合嵌入，编译时效率显著优于宏嵌入方案，并开发了新的抽象以打包相关设计模式。

Conclusion: 微嵌入结合多种设计模式可以实现高效可扩展的静态类型语言实现，与宏嵌入相比显著提升了编译时性能。

Abstract: Macro embedding is a popular approach to defining extensible shallow
embeddings of object languages in Scheme like host languages. While macro
embedding has even been shown to enable implementing extensible typed languages
in systems like Racket, it comes at a cost: compile-time performance. In this
paper, we revisit micros - syntax to intermediate representation (IR)
transformers, rather than source syntax to source syntax transformers (macros).
Micro embedding enables stopping at an IR, producing a deep embedding and
enabling high performance compile-time functions over an efficient IR, before
shallowly embedding the IR back into source syntax. Combining micros with
several design patterns to enable the IR and functions over it to be
extensible, we achieve extensible hybrid embedding of statically typed
languages with significantly improved compile-time compared to macro-embedding
approaches. We describe our design patterns and propose new abstractions
packaging these patterns.

</details>


### [3] [What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)](https://arxiv.org/abs/2509.07609)
*Yichen Xu,Oliver Bračevac,Cao Nguyen Pham,Martin Odersky*

Main category: cs.PL

TL;DR: 提出了System Capless，为Scala捕获类型提供新理论和rcaps机制，突破泛型数据结构能力追踪难题，实现对标准库的全面支持，在保证类型安全和易用性的前提下，几乎无需更改用户代码即可推广捕获检查。


<details>
  <summary>Details</summary>
Motivation: 解决Scala中捕获类型无法在泛型数据结构中有效追踪embedded capabilities的问题，因现有系统的盒类型无法命名能力，限制了在标准集合库的应用，是广泛采用捕获分析的瓶颈。

Method: 提出了System Capless理论，包括存在量化和全域量化的能力捕获机制，以Lean完全形式化证明类型安全性和作用域安全性，并通过Scala 3中的完整重实现及迁移标准集合库和异步库进行实证评估。

Result: System Capless可无缝支持轻量级捕获类型写法，极大提升表达能力，Scala 3库和生产代码迁移后只需极少甚至零额外符号变更，即可实现能力捕获检查。

Conclusion: System Capless通过引入reach capabilities（rcaps）为Scala捕获类型提供了理论基础，实现了对泛型数据结构中能力的跟踪，可以显著推广捕获检查到标准库及生产代码且几乎无符号负担。

Abstract: Capturing types in Scala unify static effect and resource tracking with
object capabilities, enabling lightweight effect polymorphism with minimal
notational overhead. However, their expressiveness has been insufficient for
tracking capabilities embedded in generic data structures, preventing them from
scaling to the standard collections library -- an essential prerequisite for
broader adoption. This limitation stems from the inability to name capabilities
within the system's notion of box types.
  This paper develops System Capless, a new foundation for capturing types that
provides the theoretical basis for reach capabilities (rcaps), a novel
mechanism for naming "what's in the box." The calculus refines the universal
capability notion into a new scheme with existential and universal capture set
quantification. Intuitively, rcaps witness existentially quantified capture
sets inside the boxes of generic types in a way that does not require exposing
existential capture types in the surface language. We have fully mechanized the
formal metatheory of System Capless in Lean, including proofs of type soundness
and scope safety. System Capless supports the same lightweight notation of
capturing types plus rcaps, as certified by a type-preserving translation, and
also enables fully optional explicit capture-set quantification to increase
expressiveness.
  Finally, we present a full reimplementation of capture checking in Scala 3
based on System Capless and migrate the entire Scala collections library and an
asynchronous programming library to evaluate its practicality and ergonomics.
Our results demonstrate that reach capabilities enable the adoption of capture
checking in production code with minimal changes and minimal-to-zero notational
overhead in a vast majority of cases.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications](https://arxiv.org/abs/2509.07449)
*Mterorga Ukor*

Main category: cs.SE

TL;DR: AOP实现Web安全机制比传统方式模块化、易维护且性能损耗很小，是提升Web应用安全开发质量的有效手段。


<details>
  <summary>Details</summary>
Motivation: 现代Web应用面临诸多安全问题，如未经授权访问、数据泄露与注入攻击，但传统面向对象编程(OOP)常将安全逻辑与业务功能混杂，导致代码耦合高、可维护性差。因此，研究如何通过新范式改善安全开发成为必要。

Method: 采用案例研究方法，对比分析AOP（面向切面编程）与传统OOP或中间件实现的安全功能（如认证、授权、输入验证、加密、日志和会话管理）。通过收集代码质量指标（代码行数、耦合度、内聚度、模块化指数、可复用性）、性能指标（响应时间、吞吐量、内存使用）和可维护性指标，辅以开发者反馈，并应用ISO/IEC 25010软件质量模型和统计方法进行评估。

Result: AOP提升了安全机制的模块化、可复用性和可维护性，同时仅带来极小的性能开销。

Conclusion: AOP能有效模块化安全相关横切关注点，为Web应用开发中的安全与软件质量平衡提供了实用参考。

Abstract: Security remains a critical challenge in modern web applications, where
threats such as unauthorized access, data breaches, and injection attacks
continue to undermine trust and reliability. Traditional Object-Oriented
Programming (OOP) often intertwines security logic with business functionality,
leading to code tangling, scattering, and reduced maintainability. This study
investigates the role of Aspect-Oriented Programming (AOP) in enhancing secure
software development by modularizing cross-cutting security concerns. Using a
case study approach, we compare AOP-based implementations of security features
including authentication, authorization, input validation, encryption, logging,
and session management with conventional OOP or middleware-based approaches.
Data collection involves analyzing code quality metrics (e.g., lines of code,
coupling, cohesion, modularity index, reusability), performance metrics
(response time, throughput, memory usage), and maintainability indicators.
Developer feedback is also incorporated to assess integration and debugging
experiences. Statistical methods, guided by the ISO/IEC 25010 software quality
model, are applied to evaluate differences across implementations. The findings
demonstrate that AOP enhances modularity, reusability, and maintainability of
security mechanisms, while introducing only minimal performance overhead. The
study contributes practical insights for software engineers and researchers
seeking to balance security with software quality in web application
development.

</details>


### [5] [CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum](https://arxiv.org/abs/2509.07498)
*Hai Dinh-Tuan*

Main category: cs.SE

TL;DR: 本文提出面向工业4.0的云原生架构CRACI，解决了传统分层架构的灵活性和可扩展性不足等问题。通过理论对比分析和实测数据评估，证明了该架构的有效性与先进性。


<details>
  <summary>Details</summary>
Motivation: 随着工业4.0的发展，信息技术（IT）与运营技术（OT）的融合暴露出传统分层架构（如ISA-95和RAMI 4.0）的局限性。这些架构过于刚性，存在数据孤岛，且缺乏对云原生技术的支持，难以满足可扩展和互操作的工业系统需求。

Method: 提出了一种新的云原生工业计算连续体参考架构（CRACI），采用解耦和事件驱动模型，实现跨层灵活、非分层的数据流。CRACI将信任、治理与政策、可观测性及生命周期管理作为架构核心基石。通过两个方面验证：（1）与既有标准、操作模型和学术方案进行理论对比分析；（2）基于真实智能制造场景的性能数据进行定量评估。

Result: 结果表明CRACI能够充分利用计算连续体，克服传统架构的结构性局限，实现可扩展、现代化的工业系统。

Conclusion: CRACI是一种可行且先进的云原生工业架构，为工业系统带来更高的可扩展性和现代化能力。

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical
architectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,
and lack of support for cloud-native technologies impair the development of
scalable and interoperable industrial systems. This paper addresses this issue
by introducing CRACI, a Cloud-native Reference Architecture for the Industrial
Compute Continuum. Among other features, CRACI promotes a decoupled and
event-driven model to enable flexible, non-hierarchical data flows across the
continuum. It embeds cross-cutting concerns as foundational pillars: Trust,
Governance & Policy, Observability, and Lifecycle Management, ensuring quality
attributes are core to the design. The proposed architecture is validated
through a two-fold approach: (1) a comparative theoretical analysis against
established standards, operational models, and academic proposals; and (2) a
quantitative evaluation based on performance data from previously published
real-world smart manufacturing implementations. The results demonstrate that
CRACI provides a viable, state-of-the-art architecture that utilizes the
compute continuum to overcome the structural limitations of legacy models and
enable scalable, modern industrial systems.

</details>


### [6] [PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings](https://arxiv.org/abs/2509.07540)
*Huu Hung Nguyen,Anh Tuan Nguyen,Thanh Le-Cong,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: PatchSeeker利用大模型和语义增强方法，大幅提高了漏洞描述与修补提交的自动关联准确率，可有效助力漏洞定位与代码修复。


<details>
  <summary>Details</summary>
Motivation: NVD 记录虽然权威且广泛用于漏洞管理，但往往缺乏与实际修补代码（VFC）的直接链接，导致漏洞定位和分析困难。现有自动映射方法依赖不完整、噪音较大的提交信息，未能有效利用描述深层语义。亟需更有效的语义映射机制。

Method: 提出PatchSeeker方法，利用大型语言模型（LLM）将NVD漏洞描述和实际修补提交（VFC）进行语义嵌入。对于简短或不具信息性的commit message，通过模型生成摘要以丰富内容，用于弥合自然语言报告与底层代码变更间的语义鸿沟。用丰富的语义嵌入实现高效精准的漏洞映射。

Result: PatchSeeker在基准测试集上相较最佳现有方法Prospector，MRR提升了59.3%，Recall@10提升了27.9%。扩展实验和消融研究显示：commit message生成和LLM主干模型选择对性能均有正面影响。

Conclusion: PatchSeeker依靠语义增强和大模型，有效提升了NVD与VFC的自动映射性能，为漏洞管理和程序修复提供数据基础。同时揭示方法局限及未来改进空间。

Abstract: Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on sparse, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone LLMs make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.

</details>


### [7] [Bridging the Gap Between Binary and Source Based Package Management in Spack](https://arxiv.org/abs/2509.07728)
*John Gouwar,Gregory Becker,Tamara Dahlgren,Nathan Hanford,Arjun Guha,Todd Gamblin*

Main category: cs.SE

TL;DR: 本文提出了splicing扩展，用于解决Spack在二进制混用和兼容性上的短板，使HPC等场景下软件部署更灵活高效，无需频繁重构即可支持快速安装。


<details>
  <summary>Details</summary>
Motivation: 二进制包管理器虽然能快速安装软件，但因严格的ABI兼容性设置限制了软件可配置性；而源码包管理器则灵活但编译缓慢，尤其在高性能计算（HPC）场景更突出。Spack虽能兼容二进制与源码包，却缺少二进制兼容性模型，无法混用非同时构建的二进制包。

Method: 提出了一种名为splicing的扩展，通过为Spack引入二进制兼容性建模，增强其包描述语言和依赖决策引擎，实现对可兼容二进制包的重用，同时保留源码构建灵活性。

Result: splicing极大提升了二进制包的复用与混用能力，实现了在无需频繁重构的情况下快速部署，包括诸如MPI等对ABI非常敏感的依赖。其对安装时间的影响很小。

Conclusion: splicing为Spack带来了基于二进制兼容的配置与安装新模式，在保证灵活性的同时兼顾了高效，特别适合HPC环境。

Abstract: Binary package managers install software quickly but they limit
configurability due to rigid ABI requirements that ensure compatibility between
binaries. Source package managers provide flexibility in building software, but
compilation can be slow. For example, installing an HPC code with a new MPI
implementation may result in a full rebuild. Spack, a widely deployed,
HPC-focused package manager, can use source and pre-compiled binaries, but
lacks a binary compatibility model, so it cannot mix binaries not built
together. We present splicing, an extension to Spack that models binary
compatibility between packages and allows seamless mixing of source and binary
distributions. Splicing augments Spack's packaging language and dependency
resolution engine to reuse compatible binaries but maintains the flexibility of
source builds. It incurs minimal installation-time overhead and allows rapid
installation from binaries, even for ABI-sensitive dependencies like MPI that
would otherwise require many rebuilds.

</details>


### [8] [What's Coming Next? Short-Term Simulation of Business Processes from Current State](https://arxiv.org/abs/2509.07747)
*Maksym Avramenko,David Chapela-Campa,Marlon Dumas,Fredrik Milani*

Main category: cs.SE

TL;DR: 本文提出了从事件日志中提取当前流程状态初始化仿真的方法，比传统“热身”仿真更适用于突发变化环境下短期性能预测。


<details>
  <summary>Details</summary>
Motivation: 现有的业务流程仿真方法主要用于战术决策，即从空状态出发，预测流程变更的长期影响，但对短期性能预测和临时干扰分析支持不足，如需求激增或资源短缺等场景。为满足运营决策需在已发生事件的基础上分析，需探索新的仿真初始化方法。

Method: 提出一种从事件日志初始化仿真的方法。通过分析当前正在进行的业务和资源状态，将其作为仿真的初始状态，解决：(1)如何定义足够的当前状态信息以初始化仿真模型；(2)如何从事件日志中提取这些状态信息。并开发一个可以输入仿真模型和当前案例日志进行短期仿真的引擎。

Result: 实验表明，该短期仿真方法比传统的从空状态热身的长周期仿真在有概念漂移或突发性能变化时能产生更准确的短期性能预测。

Conclusion: 文中提出的基于当前状态初始化的短期仿真方法能更有效地支持业务流程的运营决策，尤其在应对流程突发变化时，准确性优于传统方法。

Abstract: Business process simulation is an approach to evaluate business process
changes prior to implementation. Existing methods in this field primarily
support tactical decision-making, where simulations start from an empty state
and aim to estimate the long-term effects of process changes. A complementary
use-case is operational decision-making, where the goal is to forecast
short-term performance based on ongoing cases and to analyze the impact of
temporary disruptions, such as demand spikes and shortfalls in available
resources. An approach to tackle this use-case is to run a long-term simulation
up to a point where the workload is similar to the current one (warm-up), and
measure performance thereon. However, this approach does not consider the
current state of ongoing cases and resources in the process. This paper studies
an alternative approach that initializes the simulation from a representation
of the current state derived from an event log of ongoing cases. The paper
addresses two challenges in operationalizing this approach: (1) Given a
simulation model, what information is needed so that a simulation run can start
from the current state of cases and resources? (2) How can the current state of
a process be derived from an event log? The resulting short-term simulation
approach is embodied in a simulation engine that takes as input a simulation
model and a log of ongoing cases, and simulates cases for a given time horizon.
An experimental evaluation shows that this approach yields more accurate
short-term performance forecasts than long-term simulations with warm-up
period, particularly in the presence of concept drift or bursty performance
patterns.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Contradictions](https://arxiv.org/abs/2509.07026)
*Yang Xu,Shuwei Chen,Xiaomei Zhong,Jun Liu,Xingxing He*

Main category: cs.LO

TL;DR: 本文提出并完善了标准矛盾构建和基于矛盾分离的自动推理方法，突破了经典二元归结的局限，为提升自动推理系统的效率和表达能力提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 传统的自动定理证明（ATP）依赖二元归结，步骤局限，每步仅处理两子句、消除两文字，限制了推理系统的能力。因此，需要新的方法突破此瓶颈。

Method: 通过理论分析系统性地构建了两类主要标准矛盾，引入了判定子句集可满足性与不可满足性的流程，并推导了标准子矛盾数量的计算公式。

Result: 提出了最大三角标准矛盾和三角型标准矛盾的结构与构建方法，发展了通过最大标准矛盾判定子句集可满足性的方法，推导出两类结构下标准子矛盾的数量计算公式。

Conclusion: 本文提出并系统地构建了标准矛盾，特别研究了最大三角标准矛盾和三角型标准矛盾的构造方法，为基于矛盾分离的动态多子句自动推理提供了方法论基础，从而扩展了自动推理系统的表现力和推理能力。

Abstract: Trustworthy AI requires reasoning systems that are not only powerful but also
transparent and reliable. Automated Theorem Proving (ATP) is central to formal
reasoning, yet classical binary resolution remains limited, as each step
involves only two clauses and eliminates at most two literals. To overcome this
bottleneck, the concept of standard contradiction and the theory of
contradiction-separation-based deduction were introduced in 2018. This paper
advances that framework by focusing on the systematic construction of standard
contradictions. Specially, this study investigates construction methods for two
principal forms of standard contradiction: the maximum triangular standard
contradiction and the triangular-type standard contradiction. Building on these
structures, we propose a procedure for determining the satisfiability and
unsatisfiability of clause sets via maximum standard contradiction.
Furthermore, we derive formulas for computing the number of standard
sub-contradictions embedded within both the maximum triangular standard
contradiction and the triangular-type standard contradiction. The results
presented herein furnish the methodological basis for advancing
contradiction-separation-based dynamic multi-clause automated deduction,
thereby extending the expressive and deductive capabilities of automated
reasoning systems beyond the classical binary paradigm.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)
*Ruggero Marino Lazzaroni,Alessandro Angioi,Michelangelo Puliga,Davide Sanna,Roberto Marras*

Main category: cs.CL

TL;DR: 该论文发布并验证了意大利医学院入学考试首个大规模LLM评测集，全面分析了多个模型的表现和相关影响因素，为意大利医学AI教育和NLP领域提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育领域潜力巨大，但专门面向非英语语种和专业领域的基准极为稀缺，特别是在医学与意大利语领域。该研究旨在弥补意大利医学院入学考试评估工具的空白。

Method: 研究构建了MedBench-IT数据集，涵盖6个学科、3个难度级别，共17410道多选题，来源于权威出版机构。作者评估了GPT-4o、Claude等专有LLM及<30B参数的开源模型，还对模型结果稳定性、顺序偏置、推理提示和可读性与性能的相关性进行了全面测试分析。

Result: 主要发现包括：模型在17410题目的整体表现（具体分数未详列）；模型对同一问题的重复作答一致率高达88.86%；答案顺序变化对结果基本无影响；题目可读性较差时模型表现略有下降，两者呈现微弱负相关。

Conclusion: MedBench-IT为意大利NLP、EdTech和实践领域提供了首个大规模、标准化的医学教育评测基准，有助于推动本地语言AI应用与学术研究发展。该资源也为模型可靠性与多方面性能评估树立了新标准。

Abstract: Large language models (LLMs) show increasing potential in education, yet
benchmarks for non-English languages in specialized domains remain scarce. We
introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on
Italian medical university entrance examinations. Sourced from Edizioni Simone,
a leading preparatory materials publisher, MedBench-IT comprises 17,410
expert-written multiple-choice questions across six subjects (Biology,
Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty
levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude
series) and resource-efficient open-source alternatives (<30B parameters)
focusing on practical deployability.
  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response
consistency, varying by subject), ordering bias analysis (minimal impact), and
reasoning prompt evaluation. We also examined correlations between question
readability and model performance, finding a statistically significant but
small inverse relationship. MedBench-IT provides a crucial resource for Italian
NLP community, EdTech developers, and practitioners, offering insights into
current capabilities and standardized evaluation methodology for this critical
domain.

</details>


### [11] [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)
*William Chen,Chutong Meng,Jiatong Shi,Martijn Bartelds,Shih-Heng Wang,Hsiu-Hsuan Wang,Rafael Mosquera,Sara Hincapie,Dan Jurafsky,Antonis Anastasopoulos,Hung-yi Lee,Karen Livescu,Shinji Watanabe*

Main category: cs.CL

TL;DR: 该论文介绍了一项覆盖200+语言的ASR评测挑战，通过创新数据集和灵活评测，激励多语种ASR模型的公平与高效发展。


<details>
  <summary>Details</summary>
Motivation: 近期多语种自动语音识别（ASR）技术虽有进步，但进步在不同语言及方言间分布不均。亟需提升面向多语种和多口音环境下ASR模型的表现与公平性。

Method: 提出Interspeech 2025 ML-SUPERB 2.0挑战赛，搭建覆盖200多种语言、口音及方言的新测试集，同时引入基于DynaBench的在线评测服务器，支持参赛团队灵活选择模型设计与结构。

Result: 共有3支队伍5个提交，全部优于基线模型。最佳模型在多语测试集上，语言识别（LID）准确率提升23%、字错误率（CER）降低18%。在有口音和方言的数据上，CER降低30.2%、LID准确率提升15.7%。

Conclusion: 社区挑战能有效推动语音技术在多语环境下的包容性和整体性能提升。

Abstract: Recent improvements in multilingual ASR have not been equally distributed
across languages and language varieties. To advance state-of-the-art (SOTA) ASR
models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a
new test suite that consists of data from 200+ languages, accents, and dialects
to evaluate SOTA multilingual speech models. The challenge also introduces an
online evaluation server based on DynaBench, allowing for flexibility in model
design and architecture for participants. The challenge received 5 submissions
from 3 teams, all of which outperformed our baselines. The best-performing
submission achieved an absolute improvement in LID accuracy of 23% and a
reduction in CER of 18% when compared to the best baseline on a general
multilingual test set. On accented and dialectal data, the best submission
obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance
of community challenges in making speech technologies more inclusive.

</details>


### [12] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 这篇论文提出并验证了一个基于大语言模型的主题模型评价体系，能更好识别语义缺陷，推动动态数据环境下主题模型的精细化评估。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模自动化指标（如coherence和diversity）过于依赖统计特征，难以解释语义上的缺陷，且对主题模型在动态数据中的实际表现评估有限。数字图书馆系统和相关领域亟需更准确、能揭示模型语义弱点的评价机制。

Method: 作者设计了一套基于大语言模型（LLM）的自动化评估框架，涵盖词汇有效性、主题内部语义合理性、主题间结构合理性以及文档-主题对齐四个维度，共九项指标。该框架通过对抗性和采样协议验证，并应用于多类型数据集与多种主题建模方法，实现了多场景测试。

Result: 框架实验显示，基于LLM的九项指标不仅具备可解释性和鲁棒性，还能从任务相关视角发现传统指标常常遗漏的关键弱点，如主题冗余和语义漂移。这些LLM指标有助于实现对动态数据集中主题模型的细粒度、可扩展评价。

Conclusion: 本研究证明了大型语言模型能显著提升主题模型在动态环境下的评价能力，揭示并克服了传统指标在语义层面上的不足，为维护大规模数据集中的主题相关性提供了可行且有效的工具，并开放了全部代码和数据。

Abstract: This study presents a framework for automated evaluation of dynamically
evolving topic models using Large Language Models (LLMs). Topic modeling is
essential for organizing and retrieving scholarly content in digital library
systems, helping users navigate complex and evolving knowledge domains.
However, widely used automated metrics, such as coherence and diversity, often
capture only narrow statistical patterns and fail to explain semantic failures
in practice. We introduce a purpose-oriented evaluation framework that employs
nine LLM-based metrics spanning four key dimensions of topic quality: lexical
validity, intra-topic semantic soundness, inter-topic structural soundness, and
document-topic alignment soundness. The framework is validated through
adversarial and sampling-based protocols, and is applied across datasets
spanning news articles, scholarly publications, and social media posts, as well
as multiple topic modeling methods and open-source LLMs. Our analysis shows
that LLM-based metrics provide interpretable, robust, and task-relevant
assessments, uncovering critical weaknesses in topic models such as redundancy
and semantic drift, which are often missed by traditional metrics. These
results support the development of scalable, fine-grained evaluation tools for
maintaining topic relevance in dynamic datasets. All code and data supporting
this work are accessible at
https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [13] [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)
*Amal Chebbi,Babajide Kolade*

Main category: cs.CL

TL;DR: 作者针对能源领域开发了专用大语言模型EnergyGPT，利用精选能源文献对LLaMA 3.1-8B模型进行微调。在能源相关任务上，EnergyGPT性能显著超越原始模型，展示了无需大规模算力即可提升领域专业性的可行路径。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在专业领域（如能源领域）中的表现受限，缺乏深度技术和精确领域知识，因此需要专门针对能源领域定制的模型。

Method: 在本研究中，作者通过对LLaMA 3.1-8B模型进行有监督微调，利用高质量、精心筛选的能源相关文献构建语料库，开发了能源领域专用模型EnergyGPT。此外，论文详细介绍了整个开发流程，包括数据收集与处理、模型微调、基准测试设计、评估及部署。

Result: 通过能源领域专用的问答基准评测，EnergyGPT在多数能源相关的语言理解与生成任务中都优于基础模型。该训练策略能够在无需大规模基础设施的情况下提升模型的领域相关性和表现。

Conclusion: EnergyGPT经过定制化微调后，在能源领域表现显著优于原始模型，能够有效提升建模效果和专业知识覆盖。

Abstract: Large Language Models have demonstrated impressive capabilities across
various domains. However, their general-purpose nature often limits their
effectiveness in specialized fields such as energy, where deep technical
expertise and precise domain knowledge are essential. In this paper, we
introduce EnergyGPT, a domain-specialized language model tailored for the
energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised
Fine-Tuning on a high-quality, curated corpus of energy-related texts. We
present a complete development pipeline, including data collection and
curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation
and deployment. Through this work, we demonstrate that our training strategy
enables improvements in domain relevance and performance without the need for
large-scale infrastructure. By evaluating the performance of the model using
domain-specific question-answering benchmarks, our results demonstrate that
EnergyGPT outperforms the base model in most of the energy-related language
understanding and generation tasks.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [14] [On the Convergence of Elementary Cellular Automata under Sequential Update Modes](https://arxiv.org/abs/2509.07797)
*Isabel Donoso-Leiva,Eric Goles,Martín Ríos-Wilson,Sylvain Sené*

Main category: cs.DM

TL;DR: 本文理论分析了带有不动点的元胞自动机在序列更新下的收敛性，并根据其所有初始态是否均能收敛以及与并行更新下的不动点存在性，将这些自动机进行了详细分类。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究最基本的元胞自动机（Elementary Cellular Automata）在具有至少一个不动点的情形下，其在序列更新模式下的收敛性。特别关注不同初始配置及不同更新模式下的行为和规律。

Method: 对具有不动点的元胞自动机进行理论分析，通过分类与推理，探讨在序列更新（与同步/并行模式对比）模式下，不同规则能否使所有初始配置收敛到不动点。

Result: 作者将相关规则进行分类，依据其在所有、部分、单一或无序列更新模式下，所有初始配置是否都能收敛到不动点，并与其在同步（并行）更新模式下是否有不动点相关联。

Conclusion: 论文得出了对于具有不动点的元胞自动机，在不同序列更新模式下，其达到不动点的收敛性结论和分类标准，为理解自动机动力学行为和稳定性提供了理论依据。

Abstract: In this paper, we perform a theoretical analysis of the sequential
convergence of elementary cellular automata that have at least one fixed point.
Our aim is to establish which elementary rules always reach fixed points under
sequential update modes, regardless of the initial configuration. In this
context, we classify these rules according to whether all initial
configurations converge under all, some, one or none sequential update modes,
depending on if they have fixed points under synchronous (or parallel) update
modes.

</details>
