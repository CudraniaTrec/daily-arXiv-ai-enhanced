{"id": "2508.19206", "categories": ["cs.LO", "math.LO", "math.NT", "11U05, 03B10, 03B25, 11J54"], "pdf": "https://arxiv.org/pdf/2508.19206", "abs": "https://arxiv.org/abs/2508.19206", "authors": ["Hera Brown", "Jakub Konieczny"], "title": "Decidability of Extensions of Presburger Arithmetic by Hardy Field Functions", "comment": "17 pages", "summary": "We study the extension of Presburger arithmetic by the class of\nsub-polynomial Hardy field functions, and show the majority of these extensions\nto be undecidable. More precisely, we show that the theory\n$\\mathrm{Th}(\\mathbb{Z}; <, +, \\lfloor f \\rceil)$, where $f$ is a Hardy field\nfunction and $\\lfloor \\cdot \\rceil$ the nearest integer operator, is\nundecidable when $f$ grows polynomially faster than $x$. Further, we show that\nwhen $f$ grows sub-linearly quickly, but still as fast as some polynomial, the\ntheory $\\mathrm{Th}(\\mathbb{Z}; <, +, \\lfloor f \\rceil)$ is undecidable.", "AI": {"tldr": "\u53ea\u8981\u7528\u6210\u957f\u901f\u5ea6\u4e0d\u4f4e\u4e8e\u67d0\u4e9b\u591a\u9879\u5f0f\u7684Hardy\u573a\u51fd\u6570\u6269\u5c55Presburger\u7b97\u672f\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u8fd9\u7c7b\u7406\u8bba\u90fd\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5f15\u5165Hardy\u573a\u51fd\u6570\u6269\u5c55Presburger\u7b97\u672f\uff0c\u7279\u522b\u5173\u6ce8\u8fd9\u4e9b\u6269\u5c55\u540e\u7684\u7406\u8bba\u5728\u53ef\u5224\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u5206\u6790$\text{Th}(\text{Z}; <, +, \rfloor f \rceil)$\u7406\u8bba\uff0c\u5176\u4e2d$f$\u662fHardy\u573a\u51fd\u6570\uff0c\u4f7f\u7528\u6700\u63a5\u8fd1\u6574\u6570\u7684\u8fd0\u7b97\u7b26\uff0c\u5bf9\u4e0d\u540c\u589e\u957f\u901f\u5ea6\u7684Hardy\u573a\u51fd\u6570\u8fdb\u884c\u53ef\u5224\u6027\u7684\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u5f53$f$\u7684\u589e\u957f\u901f\u5ea6\u591a\u9879\u5f0f\u5feb\u4e8e$x$\u65f6\uff0c\u8be5\u7406\u8bba\u4e0d\u53ef\u5224\u5b9a\u3002\u5f53$f$\u589e\u957f\u901f\u5ea6\u6b21\u7ebf\u6027\u4f46\u4f9d\u7136\u8fbe\u5230\u67d0\u79cd\u591a\u9879\u5f0f\u901f\u5ea6\u65f6\uff0c\u7406\u8bba\u540c\u6837\u4e0d\u53ef\u5224\u5b9a\u3002", "conclusion": "\u5927\u591a\u6570\u7531Hardy\u573a\u51fd\u6570\u6269\u5c55\u7684Presburger\u7b97\u672f\u7406\u8bba\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff0c\u5c24\u5176\u662f\u5728$f$\u589e\u957f\u901f\u5ea6\u8fbe\u5230\u591a\u9879\u5f0f\u7ea7\u522b\u65f6\u3002"}}
{"id": "2508.18587", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18587", "abs": "https://arxiv.org/abs/2508.18587", "authors": ["Bar\u0131\u015f Bayaz\u0131t", "Yao Li", "Xujie Si"], "title": "A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants", "comment": "Accepted by LMPL 2025", "summary": "Large language models (LLMs) can potentially help with verification using\nproof assistants by automating proofs. However, it is unclear how effective\nLLMs are in this task. In this paper, we perform a case study based on two\nmature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the\neffectiveness of LLMs in generating proofs by both quantitative and qualitative\nanalysis. Our study finds that: (1) external dependencies and context in the\nsame source file can significantly help proof generation; (2) LLMs perform\ngreat on small proofs but can also generate large proofs; (3) LLMs perform\ndifferently on different verification projects; and (4) LLMs can generate\nconcise and smart proofs, apply classical techniques to new definitions, but\ncan also make odd mistakes.", "AI": {"tldr": "\u672c\u6587\u4ee5\u4e24\u4e2a\u6210\u719f\u9a8c\u8bc1\u9879\u76ee\u4e3a\u6848\u4f8b\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u5316\u751f\u6210\u8bc1\u660e\u65b9\u9762\u7684\u6548\u679c\u3002\u53d1\u73b0\u4e0a\u4e0b\u6587\u3001\u4f9d\u8d56\u4fe1\u606f\u6709\u52a9\u63d0\u9ad8\u8d28\u91cf\uff1bLLMs\u5584\u4e8e\u751f\u6210\u5c0f\u578b\u8bc1\u660e\uff0c\u80fd\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\u4f46\u8868\u73b0\u4f9d\u8d56\u5177\u4f53\u9879\u76ee\uff1b\u540c\u65f6LLMs\u751f\u6210\u8bc1\u660e\u65e2\u6709\u521b\u65b0\u7b80\u6d01\u4e4b\u5904\uff0c\u4e5f\u4f1a\u72af\u5947\u602a\u9519\u8bef\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u81ea\u52a8\u5316\u52a9\u624b\u8bc1\u660e\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5c24\u5176\u662f\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\u80fd\u529b\u548c\u9650\u5236\u3002", "method": "\u4ee5hs-to-coq\u5de5\u5177\u548cVerdi\u4e24\u4e2a\u6210\u719fRocq\u9879\u76ee\u4e3a\u6848\u4f8b\uff0c\u91c7\u7528\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\u8bc4\u4f30LLMs\u81ea\u52a8\u751f\u6210\u8bc1\u660e\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3001\u5916\u90e8\u4f9d\u8d56\u663e\u8457\u63d0\u5347\u8bc1\u660e\u751f\u6210\uff0cLLMs\u80fd\u667a\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u8bc1\u660e\u4f46\u4ecd\u4f1a\u72af\u9519\u3002", "conclusion": "1. \u5916\u90e8\u4f9d\u8d56\u548c\u540c\u6e90\u6587\u4ef6\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u81ea\u52a8\u751f\u6210\u8bc1\u660e\u975e\u5e38\u6709\u5e2e\u52a9\uff1b2. LLMs\u5728\u751f\u6210\u5c0f\u578b\u8bc1\u660e\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u751f\u6210\u5927\u578b\u8bc1\u660e\uff1b3. LLMs\u5728\u4e0d\u540c\u9a8c\u8bc1\u9879\u76ee\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff1b4. LLMs\u80fd\u751f\u6210\u7b80\u6d01\u800c\u806a\u660e\u7684\u8bc1\u660e\uff0c\u5e94\u7528\u7ecf\u5178\u6280\u5de7\u5230\u65b0\u5b9a\u4e49\uff0c\u4f46\u4e5f\u53ef\u80fd\u51fa\u73b0\u5947\u7279\u9519\u8bef\u3002"}}
{"id": "2508.18370", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18370", "abs": "https://arxiv.org/abs/2508.18370", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faCTF-Dojo\u548cCTF-Forge\uff0c\u5927\u5e45\u7b80\u5316\u548c\u6269\u5c55LLM\u53ef\u6267\u884c\u4efb\u52a1\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u5e76\u8bc1\u660e\u6267\u884c\u53cd\u9988\u5bf9\u63d0\u5347\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u5df2\u63a5\u8fd1\u4e1a\u754c\u9876\u5c16\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u53ef\u6267\u884c\u8fd0\u884c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\uff0c\u4f46\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6267\u884c\u73af\u5883\u4ecd\u7136\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u66f4\u5f3a\u5927\u7684\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u7684\u8bad\u7ec3\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86CTF-Dojo\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u53ef\u6267\u884c\u8fd0\u884c\u73af\u5883\uff0c\u5185\u542b658\u4e2a\u53ef\u91cd\u73b0\u7684CTF\u98ce\u683c\u6311\u6218\uff0c\u5e76\u5bb9\u5668\u5316\u5728Docker\u4e2d\u3002\u540c\u65f6\u7814\u53d1\u4e86CTF-Forge\u8fd9\u4e00\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u53ef\u4ee5\u5c06\u516c\u5f00\u8d44\u6e90\u8f6c\u6362\u4e3a\u53ef\u7528\u7684\u8fd0\u884c\u73af\u5883\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5927\u5e45\u63d0\u9ad8\u6269\u5c55\u6548\u7387\u3002", "result": "\u5728CTF-Dojo\u73af\u5883\u4e0b\uff0c\u4ec5\u4f7f\u7528486\u6761\u9ad8\u8d28\u91cf\u8f68\u8ff9\u8bad\u7ec3LLM\u4ee3\u7406\uff0c\u5728\u4e09\u4e2a\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u591a11.6%\u7684\u7edd\u5bf9\u589e\u76ca\u3002\u6700\u4f73\u768432B\u6a21\u578b\u8fbe\u5230\u4e8631.9%\u7684Pass@1\uff0c\u521b\u9020\u4e86\u65b0\u7684\u5f00\u6e90\u6743\u91cd\u7684\u6700\u4f18\u6210\u7ee9\uff0c\u8868\u73b0\u5ab2\u7f8e\u6700\u5148\u8fdb\u7684\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7CTF-Dojo\uff0c\u5c06CTF\u4efb\u52a1\u4f5c\u4e3a\u53ef\u6267\u884c\u4ee3\u7406\u5b66\u4e60\u7684\u57fa\u51c6\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6267\u884c\u53cd\u9988\u7684\u8bad\u7ec3\u4fe1\u53f7\u5bf9\u4e8e\u63d0\u5347ML\u4ee3\u7406\u6027\u80fd\u6781\u4e3a\u6709\u6548\u4e14\u5173\u952e\uff0c\u53ef\u4ee5\u4e0d\u4f9d\u8d56\u9ad8\u6602\u7684\u4e13\u6709\u7cfb\u7edf\u5b9e\u73b0\u7a81\u7834\u3002"}}
{"id": "2508.18798", "categories": ["cs.FL"], "pdf": "https://arxiv.org/pdf/2508.18798", "abs": "https://arxiv.org/abs/2508.18798", "authors": ["Niclas Hertzberg", "Merlijn Sevenhuijsen", "Liv K\u00e5reborn", "Anna Lokrantz"], "title": "CASP: An evaluation dataset for formal verification of C code", "comment": null, "summary": "Recent developments in Large Language Models (LLMs) have shown promise in\nautomating code generation, yet the generated programs lack rigorous\ncorrectness guarantees. Formal verification can address this shortcoming, but\nrequires expertise and is time-consuming to apply. Currently, there is no\ndataset of verified C code paired with formal specifications that enables\nsystematic benchmarking in this space. To fill this gap, we present a curated\nevaluation dataset of C code paired with formal specifications written in\nANSI/ISO C Specification Language (ACSL). We develop a multi-stage filtering\nprocess to carefully extract 506 pairs of C code and formal specifications from\nThe Stack 1 and The Stack 2. We first identify C files annotated with formal\nlanguages. Then, we ensure that the annotated C files formally verify, and\nemploy LLMs to improve non-verifying files. Furthermore, we post-process the\nremaining files into pairs of C code and ACSL specifications, where each\nspecification-implementation pair is formally verified using Frama-C. To ensure\nthe quality of the pairs, a manual inspection is conducted to confirm the\ncorrectness of every pair. The resulting dataset of C-ACSL specification pairs\n(CASP) provides a foundation for benchmarking and further research on\nintegrating automated code generation with verified correctness.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7f3a\u4e4f\u6b63\u786e\u6027\u4fdd\u969c\u7684\u95ee\u9898\uff0c\u6784\u5efa\u4e86 506 \u4e2a C \u4ee3\u7801\u4e0e\u5f62\u5f0f\u5316\u89c4\u8303\uff08ACSL\uff09\u7684\u9ad8\u8d28\u91cf\u914d\u5bf9\u6570\u636e\u96c6 CASP\uff0c\u5e76\u786e\u8ba4\u6bcf\u4e00\u5bf9\u6b63\u786e\u65e0\u8bef\uff0c\u4e3a\u540e\u7eed\u7ed3\u5408\u4ee3\u7801\u751f\u6210\u4e0e\u6b63\u5f0f\u9a8c\u8bc1\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u81ea\u52a8\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u751f\u6210\u7a0b\u5e8f\u7f3a\u4e4f\u4e25\u683c\u7684\u6b63\u786e\u6027\u4fdd\u969c\uff0c\u800c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u867d\u80fd\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u5374\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8017\u65f6\u3002\u76ee\u524d\u5c1a\u65e0\u5df2\u9a8c\u8bc1\u7684 C \u4ee3\u7801\u4e0e\u5f62\u5f0f\u5316\u89c4\u8303\u914d\u5bf9\u7684\u6570\u636e\u96c6\u7528\u4e8e\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8be5\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u9636\u6bb5\u7b5b\u9009\u6d41\u7a0b\uff0c\u4ece The Stack 1 \u548c The Stack 2 \u7cbe\u5fc3\u63d0\u53d6 506 \u5bf9 C \u4ee3\u7801\u4e0e\u5f62\u5f0f\u5316\u89c4\u8303\u3002\u6d41\u7a0b\u5305\u62ec\uff1a\u8bc6\u522b\u5e26\u6709\u5f62\u5f0f\u5316\u8bed\u8a00\u6ce8\u91ca\u7684 C \u6587\u4ef6\uff1b\u786e\u4fdd\u8fd9\u4e9b\u6587\u4ef6\u53ef\u88ab\u6b63\u5f0f\u9a8c\u8bc1\uff0c\u5e76\u5229\u7528 LLMs \u6539\u8fdb\u672a\u80fd\u9a8c\u8bc1\u7684\u6587\u4ef6\uff1b\u5bf9\u5269\u4f59\u6587\u4ef6\u8fdb\u884c\u540e\u5904\u7406\uff0c\u5f62\u6210\u6bcf\u4e00\u5bf9\u4ee3\u7801\u4e0e ACSL \u89c4\u8303\u90fd\u901a\u8fc7 Frama-C \u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u914d\u5bf9\u3002\u6700\u7ec8\u7528\u4eba\u5de5\u68c0\u67e5\u786e\u4fdd\u914d\u5bf9\u8d28\u91cf\u548c\u6b63\u786e\u6027\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86 506 \u5bf9 C-ACSL \u89c4\u8303\u914d\u5bf9\u7684 CASP \u6570\u636e\u96c6\u3002\u6bcf\u4e2a\u914d\u5bf9\u90fd\u901a\u8fc7 Frama-C \u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u5e76\u7ecf\u4eba\u5de5\u68c0\u67e5\u786e\u8ba4\u6b63\u786e\u6027\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u4e0e\u6b63\u786e\u6027\u9a8c\u8bc1\u7ed3\u5408\u7684\u7814\u7a76\u548c\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u53d1\u5e03\u4e86\u914d\u5bf9\u5f62\u5f0f\u5316\u89c4\u8303\u548c\u5df2\u9a8c\u8bc1 C \u4ee3\u7801\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6 CASP\uff0c\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u548c\u6b63\u786e\u6027\u9a8c\u8bc1\u9886\u57df\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u548c\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2508.18290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18290", "abs": "https://arxiv.org/abs/2508.18290", "authors": ["Hans-Joachim Rudolph"], "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u590d\u6742\u503c\u5f20\u91cf\u548c\u8bed\u4e49\u5438\u5f15\u5b50\u7684AGI\u7406\u8bba\u6846\u67b6\uff0c\u7a81\u7834\u73b0\u6709\u6982\u7387\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u9650\uff0c\u5f3a\u8c03\u8bed\u4e49\u5e94\u901a\u8fc7\u9012\u5f52\u6536\u655b\u800c\u751f\u6210\uff0c\u4e3a\u5b9e\u73b0\u5177\u6709\u4e3b\u52a8\u8868\u8fbe\u548c\u7406\u89e3\u80fd\u529b\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u57fa\u4e8etransformer\u6a21\u578b\u7684\u8bed\u8a00\u7cfb\u7edf\uff0c\u4fa7\u91cd\u7edf\u8ba1\u6982\u7387\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\uff0c\u4ecd\u7f3a\u4e4f\u5bf9\u590d\u6742\u8bed\u4e49\uff08\u5982\u8bbd\u523a\u3001\u591a\u4e49\u3001\u6a21\u7cca\u7b49\uff09\u7684\u672c\u8d28\u5efa\u6a21\u3002\u8bba\u6587\u65e8\u5728\u7a81\u7834\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u66f4\u5177\u8868\u8fbe\u6027\u548c\u7a33\u5b9a\u6027\u7684\u8bed\u4e49\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u590d\u6742\u503c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684'\u8bed\u4e49\u5438\u5f15\u5b50'(semantic attractor)\u7406\u8bba\uff0c\u901a\u8fc7\u9012\u5f52\u7684\u5f20\u91cf\u53d8\u6362\u3001\u865a\u6570\u5355\u4f4di\u7684\u5faa\u73af\u64cd\u4f5c\uff0c\u6784\u5efa\u65cb\u8f6c\u8bed\u4e49\u7ed3\u6784\u3002\u8be5\u7ed3\u6784\u901a\u8fc7\u68af\u5ea6\u6d41\u3001\u5f20\u91cf\u53d8\u5f62\u548c\u77e9\u9635\u52a8\u6001\uff0c\u5b9e\u73b0\u5bf9\u8bed\u4e49\u7684\u975e\u6982\u7387\u5316\u3001\u5185\u5728\u5bfc\u5411\u7684\u6536\u655b\u5efa\u6a21\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8bed\u4e49\u5438\u5f15\u5b50\u4e3a\u6838\u5fc3\u7684\u65b0\u578b\u7406\u8bba\u6a21\u578b\uff0c\u53ef\u6709\u6548\u63cf\u8ff0\u8bbd\u523a\u3001\u591a\u4e49\u6027\u548c\u6b67\u4e49\u7b49\u8bed\u8a00\u73b0\u8c61\uff0c\u4e3a\u8bed\u4e49\u7684\u7a33\u5b9a\u6027\u3001\u6e05\u6670\u5ea6\u53ca\u8868\u8fbe\u6df1\u5ea6\u63d0\u4f9b\u4e86\u6570\u5b66\u548c\u54f2\u5b66\u57fa\u7840\u3002", "conclusion": "\u8bed\u4e49\u7684\u771f\u6b63\u751f\u6210\u6e90\u81ea\u9012\u5f52\u6027\u5730\u8d8b\u5411\u4e8e\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u800c\u975e\u7b80\u5355\u6a21\u62df\u6216\u6982\u7387\u9884\u6d4b\uff0c\u5b9e\u73b0\u8fd9\u4e00\u70b9\u9700\u8bbe\u8ba1\u5168\u65b0\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u8ba9\u6a21\u578b\u4e3b\u52a8\u5851\u9020\u8bed\u8a00\u800c\u975e\u4ec5\u9884\u6d4b\u3002"}}
{"id": "2508.18431", "categories": ["cs.SE", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18431", "abs": "https://arxiv.org/abs/2508.18431", "authors": ["K\u00e9rian Fiter", "Louis Malassign\u00e9-Onfroy", "Bentley Oakes"], "title": "DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting", "comment": null, "summary": "With Digital Twin (DT) construction and evolution occurring over time,\nstakeholders require tools to understand the current characteristics and\nconceptual architecture of the system at any time. We introduce DTInsight, a\nsystematic and automated tool and methodology for producing continuous\nreporting for DTs. DTInsight offers three key features: (a) an interactive\nconceptual architecture visualization of DTs; (b) generation of summaries of DT\ncharacteristics based on ontological data; and (c) integration of these outputs\ninto a reporting page within a continuous integration and continuous deployment\n(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT\nDescription Framework (DTDF), DTInsight enables up-to-date and detailed reports\nfor enhanced stakeholder understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDTInsight\uff0c\u5b9e\u73b0\u5bf9\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u67b6\u6784\u53ef\u89c6\u5316\u3001\u7279\u6027\u6458\u8981\u751f\u6210\u53ca\u62a5\u544a\u81ea\u52a8\u96c6\u6210\uff0c\u6709\u6548\u652f\u6301\u7cfb\u7edf\u6f14\u8fdb\u4e2d\u5229\u76ca\u76f8\u5173\u65b9\u7684\u7406\u89e3\u548c\u51b3\u7b56\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u968f\u7740\u65f6\u95f4\u4e0d\u65ad\u53d1\u5c55\uff0c\u5229\u76ca\u76f8\u5173\u65b9\u9700\u8981\u59cb\u7ec8\u80fd\u7406\u89e3\u5176\u5f53\u524d\u7279\u5f81\u548c\u67b6\u6784\uff0c\u4f46\u7f3a\u4e4f\u6301\u7eed\u5316\u3001\u81ea\u52a8\u5316\u62a5\u544a\u5de5\u5177\u3002", "method": "\u63d0\u51faDTInsight\u5de5\u5177\uff0c\u7ed3\u5408DT\u63cf\u8ff0\u6846\u67b6\uff08DTDF\uff09\uff0c\u81ea\u52a8\u53ef\u89c6\u5316DT\u67b6\u6784\u3001\u751f\u6210\u672c\u4f53\u7279\u6027\u6458\u8981\uff0c\u5e76\u5c06\u7ed3\u679c\u96c6\u6210\u5230CI/CD\u7684\u62a5\u544a\u9875\u9762\u4e2d\u3002", "result": "DTInsight\u5b9e\u73b0\u4e86DT\u67b6\u6784\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3001\u7279\u6027\u81ea\u52a8\u6458\u8981\u4e0e\u62a5\u544a\u96c6\u6210\uff0c\u63d0\u5347\u4e86\u62a5\u544a\u81ea\u52a8\u5316\u548c\u7406\u89e3\u6548\u7387\u3002", "conclusion": "DTInsight\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u6301\u7eed\u751f\u6210\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u7cfb\u7edf\u62a5\u544a\uff0c\u63d0\u5347\u4e86\u5229\u76ca\u76f8\u5173\u65b9\u5bf9\u7cfb\u7edf\u72b6\u6001\u548c\u7279\u6027\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2508.18321", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18321", "abs": "https://arxiv.org/abs/2508.18321", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKAIROS\u57fa\u51c6\uff0c\u7cfb\u7edf\u7814\u7a76LLMs\u5728\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u52a8\u6001\u4e0b\u7684\u51b3\u7b56\u3001\u4fe1\u4efb\u548c\u9c81\u68d2\u6027\uff0c\u53d1\u73b0GRPO\u65b9\u6cd5\u867d\u63d0\u5347\u8868\u73b0\u4f46\u5f71\u54cd\u6297\u793e\u4f1a\u5f71\u54cd\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4e2d\uff0c\u4f5c\u4e3a\u534f\u4f5c\u667a\u80fd\u7684\u7ec4\u6210\u90e8\u5206\u3002\u4f46\u8fc7\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ece\u4f17\u504f\u89c1\uff0c\u8f83\u5c11\u7cfb\u7edf\u6027\u63a2\u7d22\u4fe1\u4efb\u5efa\u7acb\u3001\u6297\u5047\u4fe1\u606f\u80fd\u529b\u4ee5\u53ca\u7fa4\u4f53\u4e92\u52a8\u4e0b\u7684\u793e\u4f1a\u52a8\u6001\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51faKAIROS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u5305\u542b\u4e0d\u540c\u53ef\u9760\u6027\u4e2a\u4f53\uff08\u4e13\u5bb6\u3001\u65b0\u624b\u3001\u654c\u5bf9\u8005\u3001\u566a\u58f0\uff09\u7684\u7b54\u9898\u7ade\u8d5b\u73af\u5883\u3002\u901a\u8fc7\u64cd\u7eb5\u5386\u53f2\u4e92\u52a8\u4e0e\u5f53\u524d\u540c\u4f34\u53cd\u9988\uff0c\u6709\u7cfb\u7edf\u5730\u7814\u7a76\u4fe1\u4efb\u3001\u81ea\u4fe1\u548c\u540c\u4f34\u884c\u4e3a\u5bf9LLM\u51b3\u7b56\u7684\u5f71\u54cd\u3002\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u7f13\u89e3\u7b56\u7565\uff1a\u63d0\u793a\u5de5\u7a0b\uff08prompting\uff09\u3001\u6709\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u7b49\u3002", "result": "GRPO\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u3001\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u548c\u65e0\u7ea6\u675f\u63a8\u7406\u65f6\uff0c\u6574\u4f53\u6027\u80fd\u6700\u4f73\uff0c\u4f46\u76f8\u8f83\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u964d\u4f4e\u3002", "conclusion": "LLMs\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u73af\u5883\u4e0b\u7684\u4fe1\u4efb\u5f62\u6210\u3001\u4fe1\u606f\u6574\u5408\u53ca\u51b3\u7b56\u673a\u5236\u590d\u6742\u3002GRPO\u53ef\u6539\u5584\u6574\u4f53\u8868\u73b0\uff0c\u4f46\u9700\u6743\u8861\u5176\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u654f\u611f\u6027\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.18452", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18452", "abs": "https://arxiv.org/abs/2508.18452", "authors": ["Pierre-Emmanuel Goffi", "Rapha\u00ebl Tremblay", "Bentley Oakes"], "title": "Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling", "comment": "Accepted for EDTconf 2025", "summary": "Successfully engineering interactive industrial DTs is a complex task,\nespecially when implementing services beyond passive monitoring. We present\nhere an experience report on engineering a safety-critical digital twin (DT)\nfor beer fermentation monitoring, which provides continual sampling and reduces\nmanual sampling time by 91%. We document our systematic methodology and\npractical solutions for implementing bidirectional DTs in industrial\nenvironments. This includes our three-phase engineering approach that\ntransforms a passive monitoring system into an interactive Type 2 DT with\nreal-time control capabilities for pressurized systems operating at seven bar.\nWe contribute details of multi-layered safety protocols, hardware-software\nintegration strategies across Arduino controllers and Unity visualization, and\nreal-time synchronization solutions. We document specific engineering\nchallenges and solutions spanning interdisciplinary integration, demonstrating\nhow our use of the constellation reporting framework facilitates cross-domain\ncollaboration. Key findings include the critical importance of safety-first\ndesign, simulation-driven development, and progressive implementation\nstrategies. Our work thus provides actionable guidance for practitioners\ndeveloping DTs requiring bidirectional control in safety-critical applications.", "AI": {"tldr": "\u672c\u6587\u62a5\u544a\u4e86\u5728\u5564\u9152\u53d1\u9175\u5de5\u4e1a\u573a\u666f\u4e0b\u5de5\u7a0b\u4e92\u52a8\u578b\u6570\u5b57\u5b6a\u751f\u7684\u5b8c\u6574\u65b9\u6cd5\u4e0e\u6210\u679c\uff0c\u6240\u5efa\u7cfb\u7edf\u53ef\u6301\u7eed\u91c7\u6837\u5e76\u6781\u5927\u51cf\u5c11\u4eba\u5de5\u64cd\u4f5c\u65f6\u95f4\u3002\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u5de5\u7a0b\u65b9\u6cd5\u53ca\u591a\u5c42\u5b89\u5168\u63aa\u65bd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u538b\u5b9e\u65f6\u53cc\u5411\u63a7\u5236\uff0c\u5e76\u603b\u7ed3\u4e86\u8de8\u5b66\u79d1\u96c6\u6210\u7684\u5173\u952e\u7ecf\u9a8c\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u578bDT\u5f00\u53d1\u5177\u53c2\u8003\u4ef7\u503c\u3002", "motivation": "\u76ee\u524d\u5de5\u4e1a\u73af\u5883\u4e0b\u6570\u5b57\u5b6a\u751f(DT)\u7cfb\u7edf\u901a\u5e38\u4ec5\u7528\u4e8e\u88ab\u52a8\u76d1\u63a7\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u548c\u63a7\u5236\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u578b\u573a\u666f\u4e2d\uff0c\u5de5\u7a0b\u5b9e\u73b0\u590d\u6742\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u53ef\u884c\u7cfb\u7edf\u6027\u5de5\u7a0b\u65b9\u6cd5\u4ee5\u63a8\u52a8\u5de5\u4e1aDT\u4ece\u88ab\u52a8\u76d1\u63a7\u5411\u53cc\u5411\u4ea4\u4e92\u4e0e\u63a7\u5236\u8f6c\u53d8\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5c06\u88ab\u52a8\u76d1\u63a7\u7cfb\u7edf\u8f6c\u5316\u4e3a\u652f\u6301\u5b9e\u65f6\u63a7\u5236\u7684\u4e92\u52a8\u578b\u4e8c\u7c7b\u6570\u5b57\u5b6a\u751f\u3002\u5177\u4f53\u5305\u62ec\u591a\u5c42\u5b89\u5168\u534f\u8bae\u8bbe\u8ba1\u3001\u786c\u8f6f\u4e00\u4f53\u5316\uff08Arduino\u4e0eUnity\u53ef\u89c6\u5316\u96c6\u6210\uff09\u3001\u5b9e\u65f6\u540c\u6b65\u65b9\u6848\uff0c\u4ee5\u53ca\u57fa\u4e8e\u661f\u5ea7\u62a5\u544a\u6846\u67b6\u4fc3\u8fdb\u8de8\u9886\u57df\u534f\u4f5c\u3002\u5168\u8fc7\u7a0b\u8bb0\u5f55\u5de5\u7a0b\u6311\u6218\u4e0e\u5e94\u5bf9\u5b9e\u8df5\u3002", "result": "\u901a\u8fc7\u6240\u8ff0\u65b9\u6cd5\u6210\u529f\u5f00\u53d1\u9002\u7528\u4e8e\u9ad8\u538b\u5564\u9152\u53d1\u9175\u73af\u8282\u7684\u5b89\u5168\u5173\u952e\u578b\u4e92\u52a8\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\uff0c\u5b9e\u73b0\u6301\u7eed\u91c7\u6837\uff0c\u5e76\u5c06\u4eba\u5de5\u91c7\u6837\u65f6\u95f4\u51cf\u5c11\u4e8691%\u3002\u7cfb\u7edf\u5728\u591a\u9886\u57df\u96c6\u6210\u548c\u5b89\u5168\u8bbe\u8ba1\u65b9\u9762\u6548\u679c\u826f\u597d\uff0c\u6280\u672f\u8def\u7ebf\u5177\u5907\u53ef\u590d\u5236\u6027\u3002", "conclusion": "\u5b9e\u8df5\u8bc1\u660e\uff0c\u5b89\u5168\u4f18\u5148\u8bbe\u8ba1\u3001\u4eff\u771f\u9a71\u52a8\u5f00\u53d1\u4e0e\u6e10\u8fdb\u5f0f\u5b9e\u65bd\u7b56\u7565\u5bf9\u4e8e\u5b89\u5168\u5173\u952e\u578b\u53cc\u5411\u63a7\u5236\u6570\u5b57\u5b6a\u751f\u81f3\u5173\u91cd\u8981\uff1b\u7ed9\u51fa\u5bf9\u7c7b\u4f3c\u9886\u57df\u5de5\u7a0b\u5b9e\u8df5\u7684\u5177\u4f53\u65b9\u6cd5\u6307\u5bfc\u3002"}}
{"id": "2508.18328", "categories": ["cs.CL", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.18328", "abs": "https://arxiv.org/abs/2508.18328", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "\u672c\u8bba\u6587\u6784\u5efa\u4e86\u5168\u7403\u9996\u4e2a\u805a\u7126\u975e\u62c9\u4e01\u6587\u5b57\u591a\u8bed\u79cd\u7f51\u7ad9\u7684\u5927\u578b\u6570\u636e\u96c6LangCrUX\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u7f51\u9875\u65e0\u969c\u788d\u63d0\u793a\u666e\u904d\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u5c4f\u5e55\u9605\u8bfb\u5668\u4f7f\u7528\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Kizuki\u6269\u5c55\u5de5\u5177\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u8bed\u79cd\u7f51\u9875\u7684\u65e0\u969c\u788d\u8bbf\u95ee\u4f53\u9a8c\u3002", "motivation": "\u5c3d\u7ba1\u7f51\u7edc\u4e0a\u9010\u6e10\u652f\u6301\u591a\u8bed\u79cd\u5185\u5bb9\uff0c\u4f46\u5bf9\u4e8e\u89c6\u89c9\u969c\u788d\u8005\u6765\u8bf4\uff0c\u73b0\u6709\u8f85\u52a9\u6280\u672f\u5982\u5c4f\u5e55\u9605\u8bfb\u5668\u5bf9\u975e\u62c9\u4e01\u6587\u5b57\u652f\u6301\u4e0d\u8db3\uff0c\u5f71\u54cd\u5176\u83b7\u53d6\u4fe1\u606f\u3002\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u591a\u8bed\u79cd\u7f51\u9875\u5185\u5bb9\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u963b\u788d\u5bf9\u76f8\u5173\u65e0\u969c\u788d\u95ee\u9898\u7684\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aLangCrUX\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u6db5\u76d6120,000\u4e2a\u4e3b\u8981\u91c7\u7528\u975e\u62c9\u4e01\u6587\u5b57\u7684\u70ed\u95e8\u7f51\u7ad9\uff0c\u6d89\u53ca12\u79cd\u8bed\u8a00\uff0c\u5e76\u5229\u7528\u8be5\u6570\u636e\u96c6\u7cfb\u7edf\u5206\u6790\u591a\u8bed\u79cd\u7f51\u9875\u7684\u65e0\u969c\u788d\u72b6\u51b5\u3002", "result": "\u6570\u636e\u5206\u6790\u53d1\u73b0\uff0c\u7f51\u9875\u666e\u904d\u5ffd\u89c6\u65e0\u969c\u788d\u63d0\u793a\uff0c\u4e14\u8fd9\u4e9b\u63d0\u793a\u901a\u5e38\u4e0d\u80fd\u53cd\u6620\u9875\u9762\u53ef\u89c1\u5185\u5bb9\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5c4f\u5e55\u9605\u8bfb\u5668\u6548\u679c\u53d7\u9650\uff0c\u964d\u4f4e\u65e0\u969c\u788d\u6027\u3002", "conclusion": "\u591a\u8bed\u79cd\u7f51\u7ad9\u666e\u904d\u5b58\u5728\u65e0\u969c\u788d\u63d0\u793a\u7f3a\u5931\u6216\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u8bbf\u95ee\u4f53\u9a8c\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Kizuki\uff0c\u4e00\u79cd\u8bed\u8a00\u611f\u77e5\u7684\u81ea\u52a8\u65e0\u969c\u788d\u6d4b\u8bd5\u6269\u5c55\uff0c\u4ee5\u63d0\u5347\u591a\u8bed\u79cd\u5185\u5bb9\u7684\u8f85\u52a9\u8bbf\u95ee\u6548\u679c\u3002"}}
{"id": "2508.18547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18547", "abs": "https://arxiv.org/abs/2508.18547", "authors": ["Youssef Abdelsalam", "Norman Peitek", "Anna-Maria Maurer", "Mariya Toneva", "Sven Apel"], "title": "How do Humans and LLMs Process Confusing Code?", "comment": null, "summary": "Already today, humans and programming assistants based on large language\nmodels (LLMs) collaborate in everyday programming tasks. Clearly, a\nmisalignment between how LLMs and programmers comprehend code can lead to\nmisunderstandings, inefficiencies, low code quality, and bugs.\n  A key question in this space is whether humans and LLMs are confused by the\nsame kind of code. This would not only guide our choices of integrating LLMs in\nsoftware engineering workflows, but also inform about possible improvements of\nLLMs.\n  To this end, we conducted an empirical study comparing an LLM to human\nprogrammers comprehending clean and confusing code. We operationalized\ncomprehension for the LLM by using LLM perplexity, and for human programmers\nusing neurophysiological responses (in particular, EEG-based fixation-related\npotentials).\n  We found that LLM perplexity spikes correlate both in terms of location and\namplitude with human neurophysiological responses that indicate confusion. This\nresult suggests that LLMs and humans are similarly confused about the code.\nBased on these findings, we devised a data-driven, LLM-based approach to\nidentify regions of confusion in code that elicit confusion in human\nprogrammers.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0cLLM \u548c\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5bf9\u4ee3\u7801\u56f0\u60d1\u533a\u57df\u7684\u53cd\u5e94\u9ad8\u5ea6\u4e00\u81f4\uff0c\u57fa\u4e8e\u6b64\uff0c\u53ef\u4ee5\u7528 LLM \u8f85\u52a9\u53d1\u73b0\u53ef\u80fd\u4ee4\u7a0b\u5e8f\u5458\u56f0\u60d1\u7684\u4ee3\u7801\u533a\u57df\u3002", "motivation": "\u4eba\u7c7b\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u7a0b\u52a9\u624b\u5df2\u5728\u65e5\u5e38\u7f16\u7a0b\u4efb\u52a1\u4e2d\u534f\u4f5c\uff0c\u63a2\u7a76 LLM \u548c\u4eba\u7c7b\u4e4b\u95f4\u4ee3\u7801\u7406\u89e3\u56f0\u60d1\u7684\u4e00\u81f4\u6027\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb LLM \u5e76\u4f18\u5316\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4 LLM \u7684 perplexity\uff08\u56f0\u60d1\u5ea6\uff09\u4e0e\u4eba\u7c7b\u7a0b\u5e8f\u5458\u7684\u795e\u7ecf\u751f\u7406\u53cd\u5e94\uff08\u5982\u57fa\u4e8e EEG \u7684\u6ce8\u89c6\u76f8\u5173\u7535\u4f4d\uff09\uff0c\u5b9e\u8bc1\u6027\u5206\u6790\u4ed6\u4eec\u5728\u9605\u8bfb\u6e05\u6670\u548c\u6666\u6da9\u4ee3\u7801\u65f6\u7684\u8868\u73b0\u3002", "result": "LLM \u7684 perplexity \u4e0e\u4eba\u7c7b\u56f0\u60d1\u65f6\u7684\u795e\u7ecf\u751f\u7406\u4fe1\u53f7\u5728\u4f4d\u7f6e\u548c\u5e45\u503c\u4e0a\u9ad8\u5ea6\u76f8\u5173\uff0c\u5373\u4ed6\u4eec\u5728\u4ee3\u7801\u7684\u540c\u4e00\u90e8\u5206\u611f\u5230\u56f0\u60d1\u3002\u7531\u6b64\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e LLM\u3001\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0\u4ee4\u7a0b\u5e8f\u5458\u56f0\u60d1\u7684\u4ee3\u7801\u533a\u57df\u3002", "conclusion": "LLMs \u548c\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u7406\u89e3\u4ee3\u7801\u65f6\uff0c\u5bf9\u4e8e\u54ea\u4e9b\u90e8\u5206\u4ea7\u751f\u56f0\u60d1\u9ad8\u5ea6\u4e00\u81f4\uff0c\u53ef\u4ee5\u5229\u7528 LLM \u8f85\u52a9\u68c0\u6d4b\u4ee4\u7a0b\u5e8f\u5458\u56f0\u60d1\u7684\u4ee3\u7801\u533a\u57df\u3002"}}
{"id": "2508.18381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18381", "abs": "https://arxiv.org/abs/2508.18381", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LVLMs\u591a\u8bed\u8a00\u7406\u89e3\u548c\u6d45\u5c42\u795e\u7ecf\u5143\u6fc0\u6d3b\u5bc6\u5207\u76f8\u5173\uff0c\u63d0\u51faPLAST\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u6d45\u5c42\u5fae\u8c03\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u80fd\u529b\u4e14\u4ec5\u970014%\u7684\u53c2\u6570\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u4e0e\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u80fd\u529b\u4e0a\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u5e73\u8861\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22LVLMs\u591a\u8bed\u8a00\u5904\u7406\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u5e76\u63d0\u5347\u5176\u591a\u8bed\u8a00\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPLAST\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u8bc6\u522b\u548c\u5fae\u8c03\u4e0e\u591a\u8bed\u8a00\u7406\u89e3\u76f8\u5173\u7684\u7279\u5b9a\u6d45\u5c42\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u80fd\u529b\u589e\u5f3a\u3002\u5177\u4f53\u505a\u6cd5\u5305\u62ec\u76d1\u6d4b\u795e\u7ecf\u5143\u6fc0\u6d3b\u4ee5\u5b9a\u4f4d\u5173\u952e\u5c42\uff0c\u968f\u540e\u5229\u7528\u95ee\u9898\u7ffb\u8bd1\u5bf9\u8fdb\u884c\u591a\u8bed\u8a00\u5bf9\u9f50\u5fae\u8c03\u3002", "result": "\u5728MM-Bench\u548cMMMB\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\uff0cPLAST\u663e\u8457\u63d0\u5347\u4e86LVLMs\u7684\u591a\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u4ec5\u9700\u5fae\u8c0314%\u7684\u53c2\u6570\u5373\u53ef\u83b7\u5f97\u6709\u6548\u63d0\u5347\u3002\u6b64\u5916\uff0cPLAST\u5728\u4f4e\u8d44\u6e90\u548c\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u5176\u6cdb\u5316\u6027\u5f3a\u3002", "conclusion": "PLAST\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4e14\u53c2\u6570\u5f00\u9500\u4f4e\uff0c\u5177\u6709\u826f\u597d\u7684\u9002\u7528\u6027\u548c\u6269\u5c55\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6d45\u5c42\u4fe1\u606f\u7684\u8bed\u8a00\u7279\u5b9a\u5904\u7406\u3002"}}
{"id": "2508.18636", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18636", "abs": "https://arxiv.org/abs/2508.18636", "authors": ["Yan Wang", "Xinyi Hou", "Yanjie Zhao", "Weiguo Lin", "Haoyu Wang", "Junjun Si"], "title": "LaQual: A Novel Framework for Automated Evaluation of LLM App Quality", "comment": null, "summary": "LLM app stores are quickly emerging as platforms that gather a wide range of\nintelligent applications based on LLMs, giving users many choices for content\ncreation, coding support, education, and more. However, the current methods for\nranking and recommending apps in these stores mostly rely on static metrics\nlike user activity and favorites, which makes it hard for users to efficiently\nfind high-quality apps. To address these challenges, we propose LaQual, an\nautomated framework for evaluating the quality of LLM apps. LaQual consists of\nthree main stages: first, it labels and classifies LLM apps in a hierarchical\nway to accurately match them to different scenarios; second, it uses static\nindicators, such as time-weighted user engagement and functional capability\nmetrics, to filter out low-quality apps; and third, it conducts a dynamic,\nscenario-adaptive evaluation, where the LLM itself generates scenario-specific\nevaluation metrics, scoring rules, and tasks for a thorough quality assessment.\nExperiments on a popular LLM app store show that LaQual is effective. Its\nautomated scores are highly consistent with human judgments (with Spearman's\nrho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in\ntravel planning). By effectively screening, LaQual can reduce the pool of\ncandidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual\nsignificantly outperforms baseline systems in decision confidence, comparison\nefficiency (with average scores of 5.45 compared to 3.30), and the perceived\nvalue of its evaluation reports (4.75 versus 2.25). Overall, these results\ndemonstrate that LaQual offers a scalable, objective, and user-centered\nsolution for finding and recommending high-quality LLM apps in real-world use\ncases.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9LLM\u5e94\u7528\u5546\u5e97\u8bc4\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6LaQual\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e94\u7528\u7b5b\u9009\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u5747\u8bc1\u660e\u5176\u6709\u6548\u6027\u4e0e\u5148\u8fdb\u6027\u3002", "motivation": "\u5f53\u524dLLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5e94\u7528\u5546\u5e97\u4ec5\u4f9d\u8d56\u7528\u6237\u6d3b\u8dc3\u5ea6\u3001\u6536\u85cf\u7b49\u9759\u6001\u6307\u6807\u8fdb\u884c\u5e94\u7528\u6392\u540d\u548c\u63a8\u8350\uff0c\u5bfc\u81f4\u7528\u6237\u96be\u4ee5\u9ad8\u6548\u627e\u5230\u9ad8\u8d28\u91cf\u5e94\u7528\uff0c\u8fd9\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u5e94\u7528\u5546\u5e97\u751f\u6001\u90fd\u6784\u6210\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86LaQual\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\uff081\uff09\u5206\u5c42\u6807\u6ce8\u4e0e\u5206\u7c7bLLM\u5e94\u7528\uff0c\u4ee5\u5339\u914d\u4e0d\u540c\u573a\u666f\uff1b\uff082\uff09\u5229\u7528\u65f6\u5e8f\u52a0\u6743\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u529f\u80fd\u6307\u6807\u7b49\u9759\u6001\u4fe1\u606f\u7b5b\u9009\u4f4e\u8d28\u5e94\u7528\uff1b\uff083\uff09\u5229\u7528LLM\u81ea\u8eab\u751f\u6210\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7684\u8bc4\u4ef7\u6307\u6807\u548c\u4efb\u52a1\uff0c\u8fdb\u884c\u52a8\u6001\u81ea\u9002\u5e94\u7684\u9ad8\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u5728\u4e3b\u6d41LLM\u5e94\u7528\u5546\u5e97\u5b9e\u9a8c\u53d1\u73b0\uff0cLaQual\u81ea\u52a8\u8bc4\u5206\u7ed3\u679c\u4e0e\u4eba\u5de5\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff08\u5728\u6cd5\u5f8b\u54a8\u8be2\u548c\u65c5\u884c\u89c4\u5212\u573a\u666f\u4e0b\uff0c\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u7cfb\u6570\u7ea6\u4e3a0.6\uff0c\u7edf\u8ba1\u663e\u8457\uff09\uff1b\u53ef\u7b5b\u966466.7%-81.3%\u7684\u5019\u9009\u4f4e\u8d28\u5e94\u7528\u3002\u7528\u6237\u7814\u7a76\u4e5f\u8bc1\u5b9eLaQual\u5728\u51b3\u7b56\u4fe1\u5fc3\u3001\u6548\u7387\u548c\u8bc4\u4ef7\u62a5\u544a\u4ef7\u503c\u611f\u77e5\u7b49\u65b9\u9762\u5927\u5e45\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "LaQual\u4e3a\u5b9e\u9645\u573a\u666f\u4e0bLLM\u5e94\u7528\u7684\u53d1\u73b0\u548c\u63a8\u8350\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5ba2\u89c2\u4e14\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u8d28\u91cf\u5e94\u7528\u7b5b\u9009\u4e0e\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18384", "abs": "https://arxiv.org/abs/2508.18384", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3abackprompting\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4eff\u771f\u771f\u5b9eLLM\u8f93\u51fa\u5e76\u914d\u5408\u4eba\u5de5\u805a\u7c7b\u8fdb\u884c\u6807\u6ce8\uff0c\u7528\u4e8e\u8bad\u7ec3\u66f4\u9c81\u68d2\u7684\u5065\u5eb7\u5efa\u8bae\u68c0\u6d4b\u5668\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u68c0\u6d4b\u5668\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u53c2\u6570\u8fdc\u591a\u4e8e\u81ea\u8eab\u7684GPT-4o\u7b49\u65b9\u6848\uff0c\u4e3aLLM\u5b89\u5168\u843d\u5730\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u73af\u5883\u4e0bLLM\u8f93\u5165/\u8f93\u51fa\u6587\u672c\u5b88\u62a4\u68c0\u6d4b\u5668\u96be\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u3001\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5065\u5eb7\u5efa\u8bae\u68c0\u6d4b\u8fd9\u6837\u96be\u5ea6\u9ad8\u4e14\u7ec6\u5fae\u7684\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u6570\u636e\u91c7\u96c6\u548c\u6a21\u578b\u6548\u679c\u5747\u5b58\u5728\u8f83\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3abackprompting\u7684\u6570\u636e\u751f\u6210\u4e0e\u6807\u6ce8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5065\u5eb7\u5efa\u8bae\u5b88\u62a4\u68c0\u6d4b\u5668\u751f\u6210\u66f4\u63a5\u8fd1\u771f\u5b9e\u4ea7\u51fa\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u91c7\u7528\u7a00\u758f\u4eba\u5de5\u53c2\u4e0e\u7684\u805a\u7c7b\u65b9\u5f0f\u6807\u6ce8\u8fd9\u4e9b\u6837\u672c\uff0c\u6700\u540e\u7528\u4e8e\u8bad\u7ec3\u68c0\u6d4b\u5668\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u901a\u8fc7\u5c06\u65b0\u65b9\u6cd5\u4e0e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff08\u5982GPT-4o\uff09\u5bf9\u6bd4\uff0c\u63d0\u51fa\u7684\u68c0\u6d4b\u5668\u63d0\u5347\u4e86\u9ad8\u8fbe3.73%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4e14\u53ea\u97001/400\u7684\u53c2\u6570\u91cf\uff0c\u663e\u793a\u51fa\u660e\u663e\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684backprompting\u65b9\u6cd5\u7ed3\u5408\u7a00\u758f\u4eba\u5de5\u53c2\u4e0e\u805a\u7c7b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u6570\u636e\u7684\u6807\u6ce8\uff0c\u5e76\u7531\u6b64\u5f00\u53d1\u51fa\u7684\u68c0\u6d4b\u5668\u5728\u5065\u5eb7\u5efa\u8bae\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u8fdc\u5c0f\u4e8eGPT-4o\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u63d0\u5347\u660e\u663e\u3002"}}
{"id": "2508.18675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18675", "abs": "https://arxiv.org/abs/2508.18675", "authors": ["Xu Lu", "Weisong Sun", "Yiran Zhang", "Ming Hu", "Cong Tian", "Zhi Jin", "Yang Liu"], "title": "Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision", "comment": null, "summary": "Automated code generation has long been considered the holy grail of software\nengineering. The emergence of Large Language Models (LLMs) has catalyzed a\nrevolutionary breakthrough in this area. However, existing methods that only\nrely on LLMs remain inadequate in the quality of generated code, offering no\nguarantees of satisfying practical requirements. They lack a systematic\nstrategy for requirements development and modeling. Recently, LLM-based agents\ntypically possess powerful abilities and play an essential role in facilitating\nthe alignment of LLM outputs with user requirements. In this paper, we envision\nthe first multi-agent framework for reliable code generation based on\n\\textsc{re}quirements \\textsc{de}velopment and \\textsc{fo}rmalization, named\n\\textsc{ReDeFo}. This framework incorporates three agents, highlighting their\naugmentation with knowledge and techniques of formal methods, into the\nrequirements-to-code generation pipeline to strengthen quality assurance. The\ncore of \\textsc{ReDeFo} is the use of formal specifications to bridge the gap\nbetween potentially ambiguous natural language requirements and precise\nexecutable code. \\textsc{ReDeFo} enables rigorous reasoning about correctness,\nuncovering hidden bugs, and enforcing critical properties throughout the\ndevelopment process. In general, our framework aims to take a promising step\ntoward realizing the long-standing vision of reliable, auto-generated software.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u548c\u5f62\u5f0f\u5316\u9700\u6c42\u5f00\u53d1\u7684\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u6846\u67b6ReDeFo\uff0c\u901a\u8fc7\u5f15\u5165\u6b63\u5f0f\u89c4\u8303\u548c\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\u548c\u8d28\u91cf\uff0c\u4e3a\u8f6f\u4ef6\u81ea\u52a8\u751f\u6210\u8fc8\u5411\u5b9e\u7528\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u4e00\u76f4\u662f\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u91cd\u8981\u76ee\u6807\uff0c\u800c\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u65b9\u6cd5\u5728\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u5bf9\u5b9e\u9645\u9700\u6c42\u7684\u6ee1\u8db3\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u9700\u6c42\u5f00\u53d1\u4e0e\u5efa\u6a21\u7b56\u7565\u662f\u4e3b\u8981\u74f6\u9888\u4e4b\u4e00\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff08ReDeFo\uff09\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e09\u79cd\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u77e5\u8bc6\u4e0e\u6280\u672f\uff0c\u5c06\u9700\u6c42\u5f00\u53d1\u3001\u9700\u6c42\u5f62\u5f0f\u5316\u4e0e\u4ee3\u7801\u751f\u6210\u6d41\u7a0b\u7d27\u5bc6\u7ed3\u5408\u3002\u6838\u5fc3\u601d\u60f3\u662f\u7528\u5f62\u5f0f\u5316\u89c4\u8303\u5c06\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u8f6c\u5316\u4e3a\u51c6\u786e\u7684\u53ef\u6267\u884c\u4ee3\u7801\u3002", "result": "ReDeFo\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u66f4\u4e25\u683c\u7684\u9700\u6c42\u5230\u4ee3\u7801\u7684\u8f6c\u6362\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u8fdb\u884c\u6b63\u786e\u6027\u63a8\u7406\u3001\u53d1\u73b0\u6f5c\u5728\u7684\u9690\u85cfbug\uff0c\u5e76\u4fdd\u8bc1\u5173\u952e\u6027\u8d28\u5728\u5f00\u53d1\u5168\u6d41\u7a0b\u4e2d\u7684\u8d2f\u5f7b\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53+\u5f62\u5f0f\u5316\u9700\u6c42\u65b9\u6cd5\u589e\u5f3a\u4ee3\u7801\u8d28\u91cf\u548c\u4fdd\u969c\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u81ea\u52a8\u751f\u6210\u53ef\u9760\u8f6f\u4ef6\u7684\u957f\u671f\u76ee\u6807\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.18387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18387", "abs": "https://arxiv.org/abs/2508.18387", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIntegral Transformer\u673a\u5236\uff0c\u80fd\u6709\u6548\u53bb\u9664\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u566a\u58f0\u540c\u65f6\u4fdd\u7559\u5bf9\u7279\u6b8atoken\u7684\u4f5c\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u9879\u8bed\u8a00\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5bf9\u4e0d\u540c\u5c42\u6ce8\u610f\u529b\u673a\u5236\u7684\u642d\u914d\u7ed9\u51fa\u65b0\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7684Softmax\u81ea\u6ce8\u610f\u529b\u5bb9\u6613\u5bf9\u65e0\u8bed\u4e49\u4fe1\u606f\u7684token\uff08\u5982\u7279\u6b8a\u7b26\u53f7\u548c\u6807\u70b9\uff09\u5206\u914d\u8fc7\u591a\u6743\u91cd\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u566a\u58f0\u3002\u4ee5\u5f80\u5f15\u5165\u8d1f\u6ce8\u610f\u529b\u5206\u6570\u7684\u65b9\u6cd5\u5219\u6709\u820d\u5f03\u6709\u7528\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u9700\u627e\u5230\u66f4\u597d\u7684\u53bb\u566a\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236Integral Transformer\uff0c\u901a\u8fc7\u5bf9logit\u5206\u5e03\u91c7\u6837\u4fe1\u53f7\u8fdb\u884c\u79ef\u5206\u6765\u53bb\u566a\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u7279\u6b8atoken\u7684\u6709\u6548\u8d21\u732e\u3002\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u5c42\u4f7f\u7528\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6548\u679c\u3002", "result": "\u5728\u591a\u9879\u77e5\u8bc6\u4e0e\u63a8\u7406\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cIntegral Transformer\u7684\u8868\u73b0\u4f18\u4e8evanilla\u3001Cog\u4e0eDifferential\u7b49\u6ce8\u610f\u529b\u673a\u5236\u3002\u540c\u65f6\u5206\u6790\u8868\u660e\u5e95\u5c42Transformer\u4f7f\u7528vanilla\u6ce8\u610f\u529b\u66f4\u4f73\uff0c\u9ad8\u5c42Integral Transformer\u80fd\u6709\u6548\u5e73\u8861\u5206\u5e03\u5e76\u51cf\u5c11\u79e9\u574d\u584c\u3002", "conclusion": "Integral Transformer\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u6ce8\u610f\u529b\u5206\u5e03\u5e76\u51cf\u5c11\u9ad8\u5c42\u7684\u79e9\u574d\u584c\uff0c\u6bd4\u73b0\u6709\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\u6548\u679c\u66f4\u597d\u3002"}}
{"id": "2508.18721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18721", "abs": "https://arxiv.org/abs/2508.18721", "authors": ["Yunrui Pei", "Hongshu Wang", "Wenjie Zhang", "Yun Lin", "Weiyu Kong", "Jin song Dong"], "title": "LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging", "comment": null, "summary": "Dynamic data dependency, answering \"why a variable has this value?\", is\ncritical for debugging. Given a program step `s` reading a variable `v`,\nfinding the dynamic definition of `v` is challenging. Traditional methods\nrequire either (1) exhaustive instrumentation of all possible definitions of\n`v` in one run or (2) replicating the run to re-examine reads/writes - both\ncostly. If `v` is defined in a library, instrumentation becomes expensive; for\nnon-deterministic programs, replication is infeasible.\n  We propose RecovSlicing, which computes dynamic data dependency in a single\nrun with partial instrumentation. We leverage LLMs to infer program behavior\nfrom a partially recorded trace and code context. Given a trace and a slicing\ncriterion (step `s` and variable `v`), RecovSlicing estimates the runtime\ndefinition of `v` by recovering the missing execution.It also supports implicit\nvariables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:\n(1) recovering runtime values and structures, and (2) aligning recovered\nvariables with recorded memory to analyze definitions.\n  We evaluate RecovSlicing on 8,300 data dependencies across three slicing\nbenchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution\nSlicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,\noutperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall\n(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug\nlocalizer, it enables finding 16% more regressions.", "AI": {"tldr": "RecovSlicing\u5229\u7528\u5927\u6a21\u578b\u63a8\u65ad\u90e8\u5206\u8bb0\u5f55\u7a0b\u5e8f\u8f68\u8ff9\u4e0b\u7684\u53d8\u91cf\u52a8\u6001\u5b9a\u4e49\uff0c\u65e0\u9700\u4f20\u7edf\u7e41\u91cd\u63d2\u6869\u548c\u590d\u523b\u6267\u884c\uff1b\u5b9e\u9645\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u663e\u8457\u9ad8\u4e8e\u540c\u7c7b\u65b9\u6cd5\uff0c\u5e76\u80fd\u53d1\u73b0\u66f4\u591a\u7a0b\u5e8f\u56de\u5f52\u7f3a\u9677\uff0c\u63d0\u9ad8\u8c03\u8bd5\u6548\u7387\u3002", "motivation": "\u7a0b\u5e8f\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\uff0c\u7406\u89e3\u53d8\u91cf\u4e3a\u4f55\u5177\u6709\u67d0\u4e2a\u503c\uff08\u5373\u52a8\u6001\u6570\u636e\u4f9d\u8d56\uff09\u5f88\u5173\u952e\uff0c\u7136\u800c\u5728\u5b9e\u9645\u8ffd\u8e2a\u53d8\u91cf\u7684\u52a8\u6001\u5b9a\u4e49\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e0d\u4ec5\u9700\u8981\u5927\u91cf\u63d2\u6869\u6216\u91cd\u590d\u6267\u884c\uff08\u5c24\u5176\u5f53\u53d8\u91cf\u5b9a\u4e49\u5728\u5e93\u91cc\u6216\u7a0b\u5e8f\u975e\u786e\u5b9a\u6027\u65f6\u66f4\u4e3a\u6602\u8d35\u6216\u4e0d\u53ef\u884c\uff09\uff0c\u4e25\u91cd\u5f71\u54cd\u6548\u7387\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "method": "\u63d0\u51fa\u4e86RecovSlicing\u65b9\u6cd5\uff0c\u5728\u4ec5\u90e8\u5206\u63d2\u6869\u7684\u5355\u6b21\u8fd0\u884c\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7LLM\uff08\u5927\u6a21\u578b\uff09\u5bf9\u4ee3\u7801\u53ca\u90e8\u5206\u6267\u884c\u8f68\u8ff9\u8fdb\u884c\u63a8\u65ad\uff0c\u6062\u590d\u7f3a\u5931\u7684\u6267\u884c\u4fe1\u606f\uff0c\u5b9e\u73b0\u52a8\u6001\u6570\u636e\u4f9d\u8d56\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u5982list.get(i)\u7b49\u9690\u5f0f\u53d8\u91cf\uff0c\u5173\u952e\u6280\u672f\u70b9\u5305\u62ec\uff1a\uff081\uff09\u6062\u590d\u8fd0\u884c\u65f6\u503c\u4e0e\u7ed3\u6784\uff0c\uff082\uff09\u5c06\u6062\u590d\u51fa\u7684\u53d8\u91cf\u4e0e\u8bb0\u5f55\u7684\u5185\u5b58\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5b9a\u4e49\u5206\u6790\u3002", "result": "\u5728\u4e09\u4e2a\u5207\u7247\u57fa\u51c6\u6d4b\u8bd5\u30018300\u4e2a\u6570\u636e\u4f9d\u8d56\u4efb\u52a1\u4e0a\uff0cRecovSlicing\u5728\u51c6\u786e\u7387\uff0880.3%\uff0c91.1%\uff0c98.3%\uff09\u548c\u53ec\u56de\u7387\uff0891.1%\uff0c91.1%\uff0c98.3%\uff09\u5747\u5927\u5e45\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff08\u51c6\u786e\u7387\u5206\u522b\u4e3a39.0%\uff0c82.0%\uff0c59.9%\uff1b\u53ec\u56de\u7387\u5206\u522b\u4e3a53.4%\uff0c79.1%\uff0c87.1%\uff09\uff0c\u4e14\u6574\u5408\u8fdb\u56de\u5f52\u7f3a\u9677\u5b9a\u4f4d\u5de5\u5177\u540e\u53ef\u591a\u53d1\u73b016%\u7684\u56de\u5f52\u7f3a\u9677\u3002", "conclusion": "RecovSlicing\u65e0\u9700\u5168\u9762\u63d2\u6869\u6216\u91cd\u590d\u6267\u884c\uff0c\u4ec5\u501f\u52a9\u5927\u6a21\u578b\u548c\u4ee3\u7801\u4e0a\u4e0b\u6587\u5c31\u80fd\u9ad8\u6548\u3001\u9ad8\u51c6\u786e\u7387\u63a8\u65ad\u53d8\u91cf\u5b9a\u4e49\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7a0b\u5e8f\u8c03\u8bd5\u4e0e\u7f3a\u9677\u5b9a\u4f4d\u6548\u7387\u3002"}}
{"id": "2508.18395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18395", "abs": "https://arxiv.org/abs/2508.18395", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLatent Self-Consistency\uff08LSC\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u578bToken\u5d4c\u5165\u9009\u62e9\u8bed\u4e49\u4e00\u81f4\u7b54\u6848\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4fdd\u6301\u9ad8\u6548\u3001\u6613\u90e8\u7f72\u548c\u826f\u597d\u7684\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6216\u957f\u6587\u672c\u95ee\u9898\u65f6\uff0c\u57fa\u4e8e\u6982\u7387\u89e3\u7801\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\u3002\u867d\u7136Self-Consistency\u7b49\u65b9\u6cd5\u80fd\u7f13\u89e3\u77ed\u6587\u672c\u95ee\u9898\uff0c\u4f46\u5728\u957f\u6587\u672c\u6216\u6240\u6709\u683c\u5f0f\u4e0b\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u5747\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u4e86Latent Self-Consistency\uff08LSC\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684Token\u5d4c\u5165\uff0c\u6311\u9009\u8bed\u4e49\u6700\u4e00\u81f4\u7684\u7b54\u6848\u3002\u540c\u65f6\uff0c\u4ec5\u9700\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u6458\u8981Token\u751f\u6210\uff0c\u4fdd\u6301\u63a8\u7406\u6548\u7387\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u3002", "result": "\u57286\u4e2a\u77ed\u6587\u672c\u548c5\u4e2a\u957f\u6587\u672c\u63a8\u7406\u57fa\u51c6\uff08\u5982MATH\u3001MMLU\u548cTruthfulQA\uff09\u4e0a\uff0cLSC\u5728\u6240\u6709\u4efb\u52a1\u5e73\u5747\u8868\u73b0\u4f18\u4e8eSC\u3001USC\u548cWUCS\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\u3002\u6b64\u5916\uff0cLSC\u80fd\u63d0\u4f9b\u6821\u51c6\u826f\u597d\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4f7f\u9884\u671f\u6821\u51c6\u8bef\u5dee\u5728\u5404\u79cd\u7b54\u6848\u683c\u5f0f\u4e0b\u90fd\u8f83\u4f4e\u3002", "conclusion": "LSC\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u7b54\u6848\u4e00\u81f4\u6027\u9009\u62e9\u65b9\u6cd5\uff0c\u65e0\u8bba\u9762\u5bf9\u77ed\u6587\u672c\u8fd8\u662f\u957f\u6587\u672c\u95ee\u9898\u90fd\u8868\u73b0\u7a33\u5b9a\u53ef\u9760\uff0c\u8fd8\u80fd\u8f93\u51fa\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002"}}
{"id": "2508.18771", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18771", "abs": "https://arxiv.org/abs/2508.18771", "authors": ["Kexin Sun", "Hongyu Kuang", "Sebastian Baltes", "Xin Zhou", "He Zhang", "Xiaoxing Ma", "Guoping Rong", "Dong Shao", "Christoph Treude"], "title": "Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions", "comment": null, "summary": "AI-based code review tools automatically review and comment on pull requests\nto improve code quality. Despite their growing presence, little is known about\ntheir actual impact. We present a large-scale empirical study of 16 popular\nAI-based code review actions for GitHub workflows, analyzing more than 22,000\nreview comments in 178 repositories. We investigate (1) how these tools are\nadopted and configured, (2) whether their comments lead to code changes, and\n(3) which factors influence their effectiveness. We develop a two-stage\nLLM-assisted framework to determine whether review comments are addressed, and\nuse interpretable machine learning to identify influencing factors. Our\nfindings show that, while adoption is growing, effectiveness varies widely.\nComments that are concise, contain code snippets, and are manually triggered,\nparticularly those from hunk-level review tools, are more likely to result in\ncode changes. These results highlight the importance of careful tool design and\nsuggest directions for improving AI-based code review systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5927\u89c4\u6a21\u5206\u6790\u4e86AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5728GitHub\u4e0a\u7684\u4f7f\u7528\u548c\u5b9e\u9645\u5f71\u54cd\uff0c\u53d1\u73b0\u6709\u6548\u7684\u8bc4\u8bba\u6709\u52a9\u4e8e\u4ee3\u7801\u6539\u8fdb\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u5177\u4f18\u5316\u63d0\u4f9b\u53c2\u8003\u5efa\u8bae\u3002", "motivation": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5728\u81ea\u52a8\u5316\u5ba1\u67e5\u548c\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u65b9\u9762\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u5176\u5b9e\u9645\u4f5c\u7528\u5c1a\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u5de5\u5177\u5bf9\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u6d41\u7a0b\u7684\u771f\u6b63\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u9009\u53d6\u4e86GitHub\u4e0a\u768416\u79cd\u4e3b\u6d41AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u5bf9178\u4e2a\u4ed3\u5e93\u4e2d\u7684\u8d85\u8fc722,000\u6761\u5ba1\u67e5\u8bc4\u8bba\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002\u7814\u7a76\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7684LLM\u8f85\u52a9\u6846\u67b6\u6765\u5224\u65ad\u8bc4\u8bba\u662f\u5426\u88ab\u91c7\u7eb3\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc6\u522b\u6709\u6548\u6027\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u7684\u91c7\u7528\u7387\u5728\u4e0d\u65ad\u589e\u957f\uff0c\u4f46\u5176\u6709\u6548\u6027\u5dee\u5f02\u8f83\u5927\u3002\u7b80\u660e\u627c\u8981\u3001\u5305\u542b\u4ee3\u7801\u7247\u6bb5\u3001\u624b\u52a8\u89e6\u53d1\u7684\u8bc4\u8bba\uff0c\u4ee5\u53ca\u9488\u5bf9\u4ee3\u7801\u5757\u7684\u5ba1\u67e5\u5de5\u5177\u8bc4\u8bba\uff0c\u66f4\u5bb9\u6613\u4fc3\u6210\u5b9e\u9645\u7684\u4ee3\u7801\u66f4\u6539\u3002", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u9700\u6ce8\u91cd\u5de5\u5177\u4e0e\u8bc4\u8bba\u8bbe\u8ba1\uff0c\u5176\u6709\u6548\u6027\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\u3002\u9488\u5bf9\u5de5\u5177\u8bbe\u8ba1\u7684\u6df1\u5165\u4f18\u5316\uff0c\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347AI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u7684\u8d28\u91cf\u548c\u4f5c\u7528\u3002"}}
{"id": "2508.18407", "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "pdf": "https://arxiv.org/pdf/2508.18407", "abs": "https://arxiv.org/abs/2508.18407", "authors": ["Michal \u0160tef\u00e1nik", "Timothee Mickus", "Marek Kadl\u010d\u00edk", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5206\u5e03\u5916\uff08OOD\uff09\u8bc4\u4f30\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u6cdb\u5316\u4e0e\u73b0\u5b9e\u4e2d\u771f\u6b63\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5efa\u8bae\u91c7\u7528\u66f4\u7ec6\u81f4\u7684\u65b9\u6cd5\u63d0\u5347\u6cdb\u5316\u8bc4\u4f30\u7684\u5065\u58ee\u6027\u3002", "motivation": "\u8d28\u7591\u76ee\u524dAI\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u7684\u4e00\u9879\u6838\u5fc3\u5047\u8bbe\uff0c\u5373\u5206\u5e03\u5916\u6570\u636e\u96c6\u80fd\u591f\u53cd\u6620\u6a21\u578b\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u53ef\u80fd\u5931\u6548\uff0c\u5e76\u5e0c\u671b\u63d0\u51fa\u66f4\u5065\u5168\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06\u73b0\u6709\u5206\u5e03\u5916\u8bc4\u4f30\u7ed3\u679c\u4e0e\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u5177\u4f53\u5931\u8d25\u6a21\u5f0f\uff08\u5c24\u5176\u662f\u6a21\u578b\u5bf9\u865a\u5047\u7279\u5f81\u6216\u9884\u6d4b\u6377\u5f84\u7684\u4f9d\u8d56\uff09\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7684OOD\u6570\u636e\u96c6\u8bc4\u4f30\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8d28\u91cf\u60ac\u6b8a\uff0c\u6709\u4e9b\u751a\u81f3\u6bd4\u7b80\u5355\u7684\u5206\u5e03\u5185\u8bc4\u4f30\u66f4\u5dee\u3002\u8fd9\u90e8\u5206\u662f\u56e0\u4e3a\u865a\u5047\u6377\u5f84\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u7684\u6570\u636e\u96c6\u4e2d\u53ef\u80fd\u5171\u4eab\uff0c\u4e14\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u8d28\u91cf\u53ef\u80fd\u8131\u94a9\u3002", "conclusion": "\u5e38\u7528\u7684\u57fa\u4e8eOOD\uff08\u5206\u5e03\u5916\uff09\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u5fc5\u80fd\u51c6\u786e\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u5931\u6548\u6a21\u5f0f\u3002"}}
{"id": "2508.18816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18816", "abs": "https://arxiv.org/abs/2508.18816", "authors": ["Sabato Nocera", "Davide Fucci", "Giuseppe Scanniello"], "title": "Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study", "comment": "Accepted for ESEM25 NIER track", "summary": "Background: Static Code Analysis (SCA) tools are widely adopted to enforce\ncode quality standards. However, little is known about how open-source projects\nuse and customize these tools. Aims: This paper investigates how GitHub\nprojects use and customize a popular SCA tool, namely SonarQube Cloud. Method:\nWe conducted a mining study of GitHub projects that are linked through GitHub\nActions to SonarQube Cloud projects. Results: Among 321 GitHub projects using\nSonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud\nprojects, while others exhibit misconfigurations or restricted access. Among\n265 accessible SonarQube Cloud projects, 75% use the organization's default\nquality gate, i.e., a set of conditions that deployed source code must meet to\npass automated checks. While 55% of the projects use the built-in quality gate\nprovided by SonarQube Cloud, 45% of them customize their quality gate with\ndifferent conditions. Overall, the most common quality conditions align with\nSonarQube Cloud's \"Clean as You Code\" principle and enforce security,\nmaintainability, reliability, coverage, and a few duplicates on newly added or\nmodified source code. Conclusions: Many projects rely on predefined\nconfigurations, yet a significant portion customize their configurations to\nmeet specific quality goals. Building on our initial results, we envision a\nfuture research agenda linking quality gate configurations to actual software\noutcomes (e.g., improvement of software security). This would enable\nevidence-based recommendations for configuring SCA tools like SonarQube Cloud\nin various contexts.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86GitHub\u9879\u76ee\u4f7f\u7528\u548c\u5b9a\u5236SonarQube Cloud\u7684\u73b0\u72b6\uff0c\u53d1\u73b0\u65e2\u6709\u5927\u91cf\u4f9d\u8d56\u9ed8\u8ba4\u914d\u7f6e\uff0c\u4e5f\u6709\u5927\u91cf\u9488\u5bf9\u5177\u4f53\u9700\u6c42\u8fdb\u884c\u5b9a\u5236\uff0c\u672a\u6765\u53ef\u5c06\u8d28\u91cf\u95e8\u914d\u7f6e\u4e0e\u8f6f\u4ef6\u6210\u679c\u5173\u8054\uff0c\u4e3a\u76f8\u5173\u5de5\u5177\u914d\u7f6e\u63d0\u4f9b\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u9759\u6001\u4ee3\u7801\u5206\u6790\uff08SCA\uff09\u5de5\u5177\u88ab\u5e7f\u6cdb\u7528\u4e8e\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\uff0c\u4f46\u76ee\u524d\u5173\u4e8e\u5f00\u6e90\u9879\u76ee\u5b9e\u9645\u5982\u4f55\u4f7f\u7528\u548c\u5b9a\u5236\u8fd9\u4e9b\u5de5\u5177\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u9488\u5bf9SonarQube Cloud\u7684\u5b9a\u5236\u548c\u4f7f\u7528\u60c5\u51b5\u3002", "method": "\u672c\u6587\u91c7\u7528\u6316\u6398\u5f0f\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u901a\u8fc7GitHub Actions\u5173\u8054\u5230SonarQube Cloud\u7684GitHub\u9879\u76ee\uff0c\u8003\u5bdf\u8fd9\u4e9b\u9879\u76ee\u662f\u5426\u6b63\u786e\u8fde\u63a5\u3001\u8d28\u91cf\u95e8\u7684\u914d\u7f6e\u548c\u5b9a\u5236\u60c5\u51b5\u3002", "result": "\u5728321\u4e2a\u4f7f\u7528SonarQube Cloud\u7684GitHub\u9879\u76ee\u4e2d\uff0c81%\u6b63\u786e\u8fde\u63a5\uff0c\u5176\u4ed6\u5b58\u5728\u914d\u7f6e\u9519\u8bef\u6216\u8bbf\u95ee\u53d7\u9650\u3002\u5728265\u4e2a\u53ef\u8bbf\u95ee\u7684\u9879\u76ee\u4e2d\uff0c75%\u4f7f\u7528\u7ec4\u7ec7\u9ed8\u8ba4\u7684\u8d28\u91cf\u95e8\uff0c55%\u91c7\u7528\u5185\u5efa\u8d28\u91cf\u95e8\uff0c45%\u5219\u6839\u636e\u81ea\u8eab\u9700\u6c42\u8fdb\u884c\u4e86\u5b9a\u5236\u3002\u5b9a\u5236\u5185\u5bb9\u591a\u805a\u7126\u5728\u5b89\u5168\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u53ef\u9760\u6027\u3001\u8986\u76d6\u7387\u7b49\u65b9\u9762\uff0c\u7b26\u5408SonarQube Cloud\u7684\u201cClean as You Code\u201d\u539f\u5219\u3002", "conclusion": "\u8bb8\u591a\u9879\u76ee\u4f9d\u8d56\u9884\u8bbe\u914d\u7f6e\uff0c\u4f46\u76f8\u5f53\u4e00\u90e8\u5206\u4f1a\u6839\u636e\u81ea\u8eab\u8d28\u91cf\u76ee\u6807\u8fdb\u884c\u5b9a\u5236\u3002\u672a\u6765\u5e94\u8fdb\u4e00\u6b65\u7814\u7a76\u8d28\u91cf\u95e8\u914d\u7f6e\u4e0e\u8f6f\u4ef6\u5b9e\u9645\u6210\u679c\u7684\u5173\u8054\uff0c\u4e3a\u4e0d\u540c\u8bed\u5883\u4e0bSCA\u5de5\u5177\u914d\u7f6e\u63d0\u4f9b\u5faa\u8bc1\u5efa\u8bae\u3002"}}
{"id": "2508.18444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18444", "abs": "https://arxiv.org/abs/2508.18444", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u5185\u5bb9\u91cd\u6392\u5e8f\u65f6\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u8bed\u4e49\u7406\u89e3\uff0c\u53d1\u73b0\u8bad\u7ec3\u65b9\u6cd5\u7684\u9009\u62e9\u5bf9\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u63a8\u7406\u80fd\u529b\u5f71\u54cd\u663e\u8457\uff0c\u90e8\u5206\u65b9\u6cd5\u80fd\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6392\u5e8f\u539f\u56e0\u7684\u80fd\u529b\uff0c\u4f46\u6574\u4f53\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u4ecd\u9700\u52a0\u5f3a\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u5bf9\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u5ea6\u589e\u5f3a\uff0c\u4f46\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\u5ea6\u4e0b\u964d\uff0c\u7528\u6237\u96be\u4ee5\u7406\u89e3\u5176\u91cd\u65b0\u6392\u5e8f\uff08re-ranking\uff09\u80cc\u540e\u7684\u539f\u56e0\u3002\u7279\u522b\u662f\u5728\u7528\u6237\u6570\u636e\u532e\u4e4f\u7684\u65b0\u7cfb\u7edf\u4e2d\uff0c\u5185\u5bb9\u51c6\u786e\u6392\u5e8f\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9LLMs\u5728\u91cd\u65b0\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u5f71\u54cd\uff0c\u5e76\u68c0\u9a8c\u8fd9\u4e9b\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u6587\u672c\u63a8\u7406\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u4f5c\u8005\u4f7f\u7528\u6765\u81ea\u73af\u5883\u4e0e\u5730\u7403\u79d1\u5b66\u9886\u57df\u7684\u8f83\u5c0f\u6392\u5e8f\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91cd\u70b9\u8003\u5bdf\u6a21\u578b\u89e3\u91ca\u6392\u5e8f\u539f\u56e0\u7684\u80fd\u529b\u3002", "result": "\u90e8\u5206\u8bad\u7ec3\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u793a\u51fa\u6a21\u578b\u5e76\u975e\u90fd\u5b66\u5230\u4e86\u51c6\u786e\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u662f\u901a\u8fc7\u62bd\u8c61\u77e5\u8bc6\u4f18\u5316\u6392\u5e8f\u8bc4\u4ef7\u3002\u8fd9\u8bf4\u660eLLMs\u7684\u53ef\u9760\u6027\u5b58\u5728\u7591\u95ee\uff0c\u5176\u900f\u660e\u5ea6\u4e0e\u8bad\u7ec3\u65b9\u5f0f\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "LLMs\u5728\u91cd\u65b0\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u53ef\u89e3\u91ca\u6027\u53d7\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u5f71\u54cd\uff0c\u6709\u4e9b\u65b9\u6cd5\u80fd\u63d0\u5347\u6a21\u578b\u63a8\u7406\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u4f46\u5e76\u4e0d\u80fd\u5168\u90e8\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u53ef\u9760\u6027\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2508.18955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18955", "abs": "https://arxiv.org/abs/2508.18955", "authors": ["Yunbo Ni", "Shaohua Li"], "title": "Interleaving Large Language Models for Compiler Testing", "comment": null, "summary": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5c06LLM\u751f\u6210\u7684\u529f\u80fd\u4e30\u5bcc\u7684\u5c0f\u4ee3\u7801\u7247\u6bb5\uff0c\u5206\u79bb\u5230\u79bb\u7ebf\u751f\u6210\u5e76\u5728\u7ebf\u7075\u6d3b\u7ec4\u5408\u7684\u65b0\u6d4b\u8bd5\u6846\u67b6LegoFuzz\uff0c\u6709\u6548\u53d1\u73b0\u4e86\u4e3b\u6d41C\u7f16\u8bd1\u5668\u4e2d\u7684\u5927\u91cf\u9ad8\u4ef7\u503cbug\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4e3aAI\u8f85\u52a9\u8f6f\u4ef6\u6d4b\u8bd5\u5f00\u8f9f\u65b0\u601d\u8def\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7f16\u8bd1\u5668\u6d4b\u8bd5\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u751f\u6210\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u901a\u5e38\u8fc7\u4e8e\u7b80\u5355\u4e14\u5e7f\u6cdb\u3001\u6df1\u5165\u7684\u6d4b\u8bd5\u6d88\u8017\u5927\u91cf\u7b97\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7f16\u8bd1\u5668\u6d4b\u8bd5\u6846\u67b6\uff0c\u5c06\u6d4b\u8bd5\u8fc7\u7a0b\u5206\u4e3a\u79bb\u7ebf\u4e0e\u5728\u7ebf\u4e24\u4e2a\u9636\u6bb5\u3002\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7LLMs\u751f\u6210\u5927\u91cf\u5c0f\u800c\u590d\u6742\u7684\u4ee3\u7801\u7247\u6bb5\uff1b\u5728\u7ebf\u9636\u6bb5\u5219\u901a\u8fc7\u7b56\u7565\u6027\u7ec4\u5408\u8fd9\u4e9b\u7247\u6bb5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6d4b\u8bd5\u7a0b\u5e8f\uff0c\u5bf9\u7f16\u8bd1\u5668\u8fdb\u884c\u6d4b\u8bd5\u3002\u6b64\u5916\u5c06\u6b64\u6846\u67b6\u5b9e\u73b0\u4e3a\u540d\u4e3aLegoFuzz\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u6d4b\u8bd5C\u7f16\u8bd1\u5668\u3002", "result": "LegoFuzz\u5728\u6d4b\u8bd5GCC\u548cLLVM\u65f6\u53d1\u73b0\u4e8666\u4e2abug\uff0c\u5176\u4e2d\u8fd1\u4e00\u534a\u4e3a\u4e25\u91cd\u4e14\u96be\u4ee5\u53d1\u73b0\u7684\u9519\u8bef\u7f16\u8bd1\uff08miscompilation\uff09bug\u3002\u8fd9\u4e9b\u662f\u73b0\u6709LLMs\u5de5\u5177\u672a\u80fd\u68c0\u6d4b\u5230\u7684\u95ee\u9898\u3002", "conclusion": "\u8fd9\u79cd\u9ad8\u6548\u7684\u8bbe\u8ba1\u6781\u5927\u63d0\u5347\u4e86AI\u6a21\u578b\u5728\u8f6f\u4ef6\u6d4b\u8bd5\uff0c\u5c24\u5176\u662fC\u7f16\u8bd1\u5668\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u6709\u6f5c\u529b\u63a8\u5e7f\u81f3\u66f4\u5e7f\u6cdb\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u9886\u57df\u3002"}}
{"id": "2508.18466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18466", "abs": "https://arxiv.org/abs/2508.18466", "authors": ["Alina Wr\u00f3blewska", "Bartosz \u017buk"], "title": "Integrating gender inclusivity into large language models via instruction tuning", "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6ce2\u5170\u8bed\u5927\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u5229\u7528\u6027\u522b\u5305\u5bb9\u6027\u6821\u5bf9\u548c\u6307\u4ee4\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u6027\u522b\u5e73\u7b49\u6027\u3002", "motivation": "\u6ce2\u5170\u8bed\u7684\u8bed\u6cd5\u6027\u522b\u4f53\u7cfb\u56e0\u5386\u53f2\u548c\u653f\u6cbb\u539f\u56e0\u5bfc\u81f4\u7537\u6027\u5f62\u5f0f\u4e3b\u5bfc\uff0c\u9020\u6210\u793e\u4f1a\u6027\u522b\u4e0d\u516c\u3002\u8fd9\u79cd\u504f\u89c1\u4f1a\u88ab\u57fa\u4e8e\u5927\u89c4\u6a21\u6587\u672c\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u7ee7\u627f\u548c\u653e\u5927\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u5229\u7528IPIS\u6570\u636e\u96c6\u2014\u2014\u5305\u542b\u6027\u522b\u5305\u5bb9\u6027\u6821\u5bf9\u548c\u6ce2\u5170\u8bed\u5230\u82f1\u8bed\u7ffb\u8bd1\u6307\u4ee4\uff0c\u5bf9\u591a\u8bed\u8a00\u548c\u6ce2\u5170\u8bed\u4e13\u7528\u5927\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff1b\u7ed3\u5408\u7406\u8bba\u8bed\u8a00\u5b66\u6846\u67b6\uff0c\u8bbe\u8ba1\u7cfb\u7edf\u6027\u63d0\u793a\uff0c\u660e\u786e\u5b9a\u4e49\u6027\u522b\u5305\u5bb9\u6027\u89c4\u8303\u3002", "result": "\u901a\u8fc7IPIS\u5fae\u8c03\uff0c\u6240\u9009\u6a21\u578b\uff08Llama-8B\u3001Mistral-7B\u3001Mistral-Nemo\u3001Bielik\u548cPLLuM\uff09\u5728\u751f\u6210\u6ce2\u5170\u8bed\u6587\u672c\u65f6\u8868\u73b0\u51fa\u660e\u663e\u6539\u5584\u7684\u6027\u522b\u5305\u5bb9\u6027\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684IPIS\u5fae\u8c03\u65b9\u6cd5\u6709\u6548\u5730\u964d\u4f4e\u4e86\u6ce2\u5170\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u6027\u522b\u5305\u5bb9\u6027\u7684\u8bed\u8a00\u751f\u6210\u3002"}}
{"id": "2508.18993", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18993", "abs": "https://arxiv.org/abs/2508.18993", "authors": ["Ziyi Ni", "Huacan Wang", "Shuo Zhang", "Shuo Lu", "Ziyang He", "Wang You", "Zhenheng Tang", "Yuntao Du", "Bill Sun", "Hongzhang Liu", "Sen Hu", "Ronghao Chen", "Bo Li", "Xin Li", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Pin Lyu"], "title": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging", "comment": "Highly practical, Well-motivated, Actionable", "summary": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u63d0\u51faGitTaskBench\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7801\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4ed3\u5e93\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7528\u65b0\u7ecf\u6d4e\u6548\u76ca\u6307\u6807\u91cf\u5316\u6027\u80fd\u3002\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4ecd\u6709\u9650\uff0c\u5173\u952e\u96be\u70b9\u5728\u73af\u5883\u4e0e\u4f9d\u8d56\u7ba1\u7406\u3002\u57fa\u51c6\u5f00\u653e\uff0c\u4fc3\u8fdb\u4ee3\u7801AI\u771f\u5b9e\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u8003\u8651\u771f\u5b9e\u7684\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\u4e2d\u5bf9\u5927\u578b\u4ee3\u7801\u5e93\uff08\u5982GitHub\uff09\u7684\u5b9e\u9645\u4efb\u52a1\u5229\u7528\uff0c\u8fd9\u4e0e\u5b9e\u9645\u5f00\u53d1\u9700\u6c42\u5b58\u5728\u5dee\u8ddd\u3002\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bc4\u6d4b\u65b9\u6cd5\uff0c\u80fd\u66f4\u8d34\u5408\u5de5\u4f5c\u6d41\u9a71\u52a8\u7684\u5b9e\u9645\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86GitTaskBench\uff0c\u4e00\u4e2a\u5305\u542b54\u4e2a\u73b0\u5b9e\u4efb\u52a1\u3001\u8986\u76d67\u79cd\u6a21\u6001\u548c7\u4e2a\u9886\u57df\u7684\u57fa\u51c6\uff0c\u4efb\u52a1\u7ed3\u5408\u6709\u4ee3\u8868\u6027\u7684\u4ee3\u7801\u4ed3\u5e93\u548c\u81ea\u52a8\u5316\u3001\u4eba\u5de5\u5ba1\u6838\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u660e\u786e\u4e86\u5b9e\u7528\u7684\u6210\u529f\u6807\u51c6\u3002\u540c\u65f6\u5f15\u5165\u4e86alpha-value\u6307\u6807\uff0c\u7528\u4ee5\u91cf\u5316\u4ee3\u7406\u6027\u80fd\u7684\u7ecf\u6d4e\u6548\u76ca\uff08\u7efc\u5408\u4efb\u52a1\u6210\u529f\u7387\u3001\u6a21\u578b\u8c03\u7528\u6210\u672c\u53ca\u5f00\u53d1\u8005\u85aa\u8d44\u5e73\u5747\u503c\uff09\u3002\u5e76\u7528\u4e09\u4e2a\u6700\u524d\u6cbf\u4ee3\u7406\u6846\u67b6\u4e0e\u591a\u79cdLLM\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76ee\u524d\u5148\u8fdb\u7cfb\u7edf\u5728\u5229\u7528\u4ed3\u5e93\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65b9\u9762\u4ecd\u5b58\u5728\u8f83\u5927\u96be\u5ea6\uff0c\u5982OpenHands+Claude 3.7\u4ec5\u89e3\u51b3\u4e8648.15%\u7684\u4efb\u52a1\u3002\u8d85\u8fc7\u534a\u6570\u7684\u5931\u8d25\u5f52\u56e0\u4e8e\u73af\u5883\u642d\u5efa\u548c\u4f9d\u8d56\u89e3\u6790\u7b49\u770b\u4f3c\u7b80\u5355\u5374\u81f3\u5173\u91cd\u8981\u7684\u6b65\u9aa4\uff0c\u66b4\u9732\u4e86\u5de5\u4f5c\u6d41\u7ba1\u7406\u548c\u8d85\u65f6\u5e94\u5bf9\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "GitTaskBench\u80fd\u5f15\u5bfc\u793e\u533a\u5173\u6ce8\u548c\u63d0\u5347\u4ee3\u7801\u4ed3\u5e93\u611f\u77e5\u7684\u81ea\u52a8\u63a8\u7406\u3001\u6267\u884c\u548c\u90e8\u7f72\u80fd\u529b\uff0c\u63a8\u52a8\u4ee3\u7801\u667a\u80fd\u4f53\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u590d\u6742\u4efb\u52a1\u89e3\u51b3\u3002\u57fa\u51c6\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u52a9\u529b\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2508.18473", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18473", "abs": "https://arxiv.org/abs/2508.18473", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u5c06\u5927\u6a21\u578b\u5e7b\u89c9\u68c0\u6d4b\u5efa\u6a21\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u91cd\u68c0\u9a8c\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9\u8f93\u51fa\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5b83\u4eec\u5e38\u5e38\u51fa\u73b0\u5e7b\u89c9\u73b0\u8c61\uff0c\u5373\u751f\u6210\u81ea\u4fe1\u4f46\u5b9e\u4e3a\u9519\u8bef\u6216\u65e0\u610f\u4e49\u7684\u5185\u5bb9\u3002\u9274\u4e8e\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u5f71\u54cdLLM\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4e9f\u9700\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5c06\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u5e76\u7c7b\u6bd4\u4e8e\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u3002\u8fdb\u4e00\u6b65\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u91cd\u68c0\u9a8c\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u7684\u6548\u679c\u3002", "result": "\u901a\u8fc7\u4e30\u5bcc\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u4e3b\u6d41\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u597d\u5730\u8bc6\u522bLLM\u7684\u5e7b\u89c9\u8f93\u51fa\u3002"}}
{"id": "2508.19056", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19056", "abs": "https://arxiv.org/abs/2508.19056", "authors": ["S. Panda", "D. Munjal", "D. P. Mohapatra"], "title": "A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs", "comment": null, "summary": "Test case prioritization focuses on finding a suitable order of execution of\nthe test cases in a test suite to meet some performance goals like detecting\nfaults early. It is likely that some test cases execute the program parts that\nare more prone to errors and will detect more errors if executed early during\nthe testing process. Finding an optimal order of execution for the selected\nregression test cases saves time and cost of retesting. This paper presents a\nstatic approach to prioritizing the test cases by computing the affected\ncomponent coupling (ACC) of the affected parts of object-oriented programs. We\nconstruct a graph named affected slice graph (ASG) to represent these affected\nprogram parts.We determine the fault-proneness of the nodes of ASG by computing\ntheir respective ACC values. We assign higher priority to those test cases that\ncover the nodes with higher ACC values. Our analysis with mutation faults shows\nthat the test cases executing the fault-prone program parts have a higher\nchance to reveal faults earlier than other test cases in the test suite. The\nresult obtained from seven case studies justifies that our approach is feasible\nand gives acceptable performance in comparison to some existing techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u5206\u6790\u7684\u6d4b\u8bd5\u7528\u4f8b\u4f18\u5148\u7ea7\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5bf9\u8c61\u7a0b\u5e8f\u4e2d\u53d7\u5f71\u54cd\u7ec4\u4ef6\u7684\u8026\u5408\u5ea6\uff0c\u4f18\u5148\u6267\u884c\u53ef\u80fd\u66f4\u6613\u51fa\u9519\u90e8\u5206\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u66f4\u65e9\u53d1\u73b0\u9519\u8bef\uff0c\u6548\u679c\u4f18\u4e8e\u90e8\u5206\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u7528\u4f8b\u4f18\u5148\u7ea7\u6392\u5e8f\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u52a8\u6001\u4fe1\u606f\u6216\u5386\u53f2\u6570\u636e\uff0c\u7f3a\u4e4f\u6709\u6548\u9759\u6001\u5206\u6790\u624b\u6bb5\uff0c\u4e14\u63d0\u524d\u53d1\u73b0\u7a0b\u5e8f\u9519\u8bef\u53ef\u8282\u7701\u56de\u5f52\u6d4b\u8bd5\u65f6\u95f4\u548c\u6210\u672c\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u9759\u6001\u4f18\u5148\u7ea7\u6392\u5e8f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53d7\u5f71\u54cd\u7247\u6bb5\u56fe\uff08ASG\uff09\uff0c\u5e76\u8ba1\u7b97\u5176\u8282\u70b9\u7684\u53d7\u5f71\u54cd\u7ec4\u4ef6\u8026\u5408\u5ea6\uff08ACC\uff09\uff0c\u636e\u6b64\u4e3a\u6d4b\u8bd5\u7528\u4f8b\u5206\u914d\u4f18\u5148\u7ea7\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0c\u901a\u8fc7\u4f18\u5148\u6267\u884c\u8986\u76d6\u9ad8ACC\u8282\u70b9\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u53ef\u66f4\u65e9\u53d1\u73b0\u7a0b\u5e8f\u4e2d\u7684\u9519\u8bef\u3002\u9488\u5bf9\u4e03\u4e2a\u6848\u4f8b\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u884c\u6027\u4e0a\u5747\u5177\u5907\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5bf9\u4e8e\u63ed\u793a\u7a0b\u5e8f\u65e9\u671f\u9519\u8bef\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u6027\u80fd\uff0c\u4e14\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2508.18549", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18549", "abs": "https://arxiv.org/abs/2508.18549", "authors": ["Maike Z\u00fcfle", "Vil\u00e9m Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u673a\u5668\u7ffb\u8bd1\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u53ea\u53c2\u8003\u5355\u4e00\u8bd1\u6587\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7efc\u5408\u591a\u8bd1\u6587\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4ef7\u4e00\u81f4\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u65b0\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u81ea\u52a8\u5316\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u901a\u5e38\u53ea\u8003\u8651\u6e90\u53e5\u548c\u5355\u4e00\u8bd1\u6587\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u65f6\u53c2\u8003\u591a\u4e2a\u8bd1\u6587\u7684\u505a\u6cd5\u4e0d\u540c\uff0c\u8fd9\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u53ef\u80fd\u9650\u5236\u81ea\u52a8\u5316\u6307\u6807\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\uff1aCOMET-polycand\uff0c\u901a\u8fc7\u5bf9\u6bd4\u540c\u4e00\u6e90\u53e5\u7684\u591a\u4e2a\u5907\u9009\u8bd1\u6587\uff0c\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\uff1bCOMET-polyic\uff0c\u501f\u9274\u68c0\u7d22\u5f0fin-context learning\uff0c\u53c2\u8003\u4e0e\u5f53\u524d\u6e90\u53e5\u7c7b\u4f3c\u7684\u5176\u5b83\u6e90\u53e5\u7ffb\u8bd1\u53ca\u5176\u4eba\u5de5\u8d28\u91cf\u5f97\u5206\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728COMET-polycand\u4e2d\uff0c\u4ec5\u52a0\u5165\u4e00\u4e2a\u989d\u5916\u8bd1\u6587\uff0c\u5206\u6bb5\u7ea7\u8bc4\u4ef7\u6027\u80fd\u5373\u4eceKendall's tau-b\u76f8\u5173\u60270.079\u63d0\u5347\u52300.118\uff0c\u6dfb\u52a0\u66f4\u591a\u8bd1\u6587\u540e\u6548\u679c\u8fdb\u4e00\u6b65\u589e\u5f3a\u3002COMET-polyic\u91c7\u7528\u68c0\u7d22\u793a\u4f8b\u540c\u6837\u5e26\u6765\u6539\u8fdb\uff08\u76f8\u5173\u6027\u4ece0.079\u63d0\u5347\u52300.116\uff09\u3002", "conclusion": "\u4e3a\u673a\u5668\u7ffb\u8bd1\u81ea\u52a8\u8bc4\u4f30\u7684\u6307\u6807\u6ce8\u5165\u591a\u8bd1\u6587\u6216\u591a\u4f8b\u4fe1\u606f\u540e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u7684\u4e00\u81f4\u6027\u3002\u6240\u63d0\u51fa\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2508.18569", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18569", "abs": "https://arxiv.org/abs/2508.18569", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7528\u4e8e\u9690\u55bb\u5bf9\u9f50\u7684\u81ea\u8bc4\u4f30\u89c6\u89c9\u9690\u55bb\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u5e26\u5956\u52b1\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u4e0d\u9700\u5927\u89c4\u6a21\u91cd\u8bad\u4e5f\u80fd\u4f18\u4e8e\u4e3b\u6d41\u5c01\u95ed\u6e90\u65b9\u6cd5\uff0c\u4f46\u4e0e\u4eba\u7c7b\u5ba1\u7f8e\u4ecd\u6709\u5dee\u8ddd\u3002", "motivation": "\u89c6\u89c9\u9690\u55bb\u751f\u6210\u8981\u6c42\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u534f\u540c\uff0c\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u7cbe\u51c6\u5bf9\u9f50\u9690\u55bb\u7684\u610f\u4e49\u4e0e\u89c6\u89c9\u8868\u8fbe\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6846\u67b6\u548c\u8bc4\u4ef7\u673a\u5236\u63d0\u5347\u9690\u55bb\u5bf9\u9f50\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u8bc4\u4f30\u89c6\u89c9\u9690\u55bb\u751f\u6210\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u8bad\u7ec3\u81ea\u7531\u7684S-T-M\u5206\u89e3\u7ba1\u7ebf\u548c\u4e00\u4e2a\u7ed3\u5408\u81ea\u8bc4\u4f30\u5956\u52b1\u673a\u5236\u7684\u8bad\u7ec3\u7ba1\u7ebf\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u9690\u55bb\u5206\u89e3\u5206\u6570\u548c\u610f\u4e49\u5bf9\u9f50\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8bad\u7ec3\u81ea\u7531\u7ba1\u7ebf\u5728\u5206\u89e3\u3001CLIP\u548c\u610f\u4e49\u5bf9\u9f50\u6307\u6807\u4e0a\u4f18\u4e8e\u5f3a\u95ed\u6e90\u57fa\u7ebf\uff08GPT-4o\u3001Imagen\uff09\uff0c\u8bad\u7ec3\u7ba1\u7ebf\u8868\u73b0\u63a5\u8fd1\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cGPT-4o\u6574\u4f53\u504f\u597d\u8f83\u9ad8\uff0c\u8bad\u7ec3\u81ea\u7531\u7ba1\u7ebf\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u9886\u5148\u5e76\u5728\u62bd\u8c61\u9690\u55bb\u4e0a\u4f18\u4e8eImagen\u3002\u5206\u6790\u53d1\u73b0S-T-M\u63d0\u793a\u5bf9\u8f83\u957f\u6216\u62bd\u8c61\u9690\u55bb\u5e2e\u52a9\u663e\u8457\uff0c\u95ed\u6e90\u6a21\u578b\u5728\u77ed\u5c0f\u5177\u4f53\u9690\u55bb\u8868\u73b0\u66f4\u4f18\uff0c\u91c7\u6837\u53c2\u6570\u5bf9\u8868\u73b0\u6709\u654f\u611f\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u5728\u9002\u5ea6\u8ba1\u7b97\u8d44\u6e90\u4e0b\u80fd\u591f\u5f88\u597d\u5730\u5b9e\u73b0\u89c6\u89c9\u9690\u55bb\u7684\u5bf9\u9f50\uff0c\u4f46\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5dee\u8ddd\u4e3b\u8981\u4f53\u73b0\u5728\u7f8e\u5b66\u548c\u91c7\u6837\u65b9\u9762\u3002"}}
{"id": "2508.18598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18598", "abs": "https://arxiv.org/abs/2508.18598", "authors": ["Colin Klein"], "title": "What do language models model? Transformers, automata, and the format of thought", "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u53cd\u6620\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u7edf\u8ba1\u548c\u7ed3\u6784\uff0c\u800c\u4e0d\u662f\u4eba\u7c7b\u8ba4\u77e5\uff1b\u4f5c\u8005\u901a\u8fc7\u5206\u6790\u53d8\u6362\u5668\u67b6\u6784\u7684\u8ba1\u7b97\u7279\u70b9\uff0c\u8bf4\u660e\u5176\u4e0e\u4eba\u8111\u5904\u7406\u8bed\u8a00\u7684\u65b9\u5f0f\u5b58\u5728\u672c\u8d28\u4e0d\u540c\uff0c\u5e76\u63d0\u51faLLMs\u548c\u4eba\u7c7b\u90fd\u80fd\u521b\u65b0\u8bed\u8a00\u8868\u8fbe\uff0c\u4f46\u65b9\u5f0f\u622a\u7136\u4e0d\u540c\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5230\u5e95\u6a21\u62df\u4e86\u4ec0\u4e48\uff0c\u6f84\u6e05\u5b83\u4eec\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u89e3\u91ca\u5b83\u4eec\u5728\u8bed\u8a00\u4e0a\u7684\u672c\u8d28\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u53d8\u6362\u5668\uff08transformer\uff09\u67b6\u6784\u7684\u8ba1\u7b97\u80fd\u529b\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u80fd\u529b\u7684\u683c\u5f0f\uff0c\u91cd\u70b9\u8ba8\u8bba\u67b6\u6784\u4e0d\u53d8\u6027\uff0c\u5e76\u5f15\u7528\u76f8\u5173\u7814\u7a76\uff08\u5982Liu\u7b492022\u5e74\uff09\u5bf9\u201c\u6377\u5f84\u81ea\u52a8\u673a\u201d\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d8\u6362\u5668\u67b6\u6784\u672c\u8d28\u4e0a\u5904\u7406\u7684\u662f\u7ebf\u6027\u683c\u5f0f\uff0c\u800c\u4eba\u7c7b\u8bed\u8a00\u80fd\u529b\u6d89\u53ca\u8d85\u7ebf\u6027\u8ba1\u7b97\u683c\u5f0f\uff1bLLMs\u548c\u4eba\u7c7b\u5b66\u4e60\u751f\u6210\u8bed\u8a00\u7684\u8def\u5f84\u548c\u673a\u5236\u663e\u8457\u4e0d\u540c\uff0c\u4f46\u90fd\u80fd\u5229\u7528\u8bed\u8a00\u4f5c\u4e3a\u201c\u8bdd\u8bed\u673a\u5668\u201d\u521b\u9020\u65b0\u7684\u8bed\u5883\u548c\u8868\u8fbe\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3b\u8981\u662f\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u53cd\u6620\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\uff0c\u5c3d\u7ba1\u8fd9\u5e76\u4e0d\u662f\u4e00\u4e2a\u8d2c\u4f4e\u5b83\u4eec\u4ef7\u503c\u7684\u8bf4\u6cd5\u3002"}}
{"id": "2508.18607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18607", "abs": "https://arxiv.org/abs/2508.18607", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86NOOV\u795e\u7ecf\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408\u81ea\u52a8\u5b66\u4e60\u7684\u53cc\u8bed\u8bcd\u5178\u548c\u751f\u7269\u533b\u5b66\u77ed\u8bed\u67e5\u627e\u8868\uff0c\u663e\u8457\u63d0\u5347\u82f1\u6587EHR\u5230\u897f\u73ed\u7259\u6587\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5e76\u884c\u5bf9\u9f50\u8bed\u6599\u5e93\u53ca\u5927\u91cf\u672a\u767b\u5f55\u8bcd\uff0c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6587\u672c\u4ece\u82f1\u6587\u5230\u897f\u73ed\u7259\u6587\u7684\u7ffb\u8bd1\u975e\u5e38\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u4f46\u5145\u6ee1\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86NOOV\uff08No OOV\uff09\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u7cfb\u7edf\uff0c\u878d\u5408\u4ece\u5e76\u884c\u5bf9\u9f50\u8bed\u6599\u81ea\u52a8\u5b66\u4e60\u7684\u53cc\u8bed\u8bcd\u5178\u4e0e\u4ece\u5927\u578b\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u5e93\u63d0\u53d6\u7684\u77ed\u8bed\u67e5\u627e\u8868\uff0c\u4ee5\u89e3\u51b3\u672a\u767b\u5f55\u8bcd\u548c\u8bcd\u91cd\u590d\u7b49NMT\u5e38\u89c1\u95ee\u9898\u3002", "result": "NOOV\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u63d0\u5347EHR\u6587\u672c\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\uff0c\u751f\u6210\u66f4\u4f18\u8d28\u7684\u7ffb\u8bd1\u7ed3\u679c\u3002", "conclusion": "NOOV\u7cfb\u7edf\u6709\u6548\u7f13\u89e3\u4e86\u9886\u57df\u5185\u5e76\u884c\u8bed\u6599\u4e0d\u8db3\u548c\u672a\u77e5\u8bcd\u6c47\u95ee\u9898\uff0c\u5bf9EHR\u7ffb\u8bd1\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.18609", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18609", "abs": "https://arxiv.org/abs/2508.18609", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6df1\u5165\u63ed\u793a\u4e86\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u5206\u5c42\u7684\u91cf\u5316\u6269\u5c55\u5b9a\u5f8b\u3002\u53d1\u73b0\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u529b\u5bf9\u91cf\u5316\u76f8\u5173\u53c2\u6570\u6781\u4e3a\u654f\u611f\uff0c\u77e5\u8bc6\u5229\u7528\u80fd\u529b\u5219\u66f4\u4e3a\u7a33\u5065\uff0c\u4e3a\u672a\u6765\u91cf\u5316\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u8df5\u53c2\u8003\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u56e0\u5176\u5e9e\u5927\u89c4\u6a21\u5728\u90e8\u7f72\u4e0a\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u6210\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u5bf9PTQ\u5bf9LLM\u591a\u6837\u77e5\u8bc6\u80fd\u529b\u5f71\u54cd\u7684\u7406\u89e3\u4e0d\u5145\u5206\uff0c\u5df2\u6709\u7684\u5173\u4e8e\u91cf\u5316\u6a21\u578b\u7684\u6269\u5c55\u5b9a\u5f8b\u4e5f\u5ffd\u89c6\u4e86PTQ\u76f8\u5173\u53c2\u6570\u548c\u4efb\u52a1\u654f\u611f\u6027\u3002", "method": "\u7efc\u5408\u5b9e\u8bc1\u7814\u7a76\uff0c\u5efa\u7acb\u4ee5\u4efb\u52a1\u5206\u5c42\u7684\u6269\u5c55\u5b9a\u5f8b\u3002\u5c06LLM\u77e5\u8bc6\u80fd\u529b\u533a\u5206\u4e3a\u8bb0\u5fc6\u80fd\u529b\uff08memorization\uff09\u548c\u5229\u7528\u80fd\u529b\uff08utilization\uff09\uff0c\u5e76\u6784\u5efa\u7edf\u4e00\u7684\u91cf\u5316\u6846\u67b6\uff0c\u6db5\u76d6\u6a21\u578b\u5927\u5c0f\u3001\u6709\u6548\u6bd4\u7279\u5bbd\u5ea6\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u5206\u7ec4\u5927\u5c0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u76f8\u6bd4\u66f4\u9c81\u68d2\u7684\u77e5\u8bc6\u5229\u7528\u80fd\u529b\uff0c\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u529b\u5bf9\u6709\u6548\u6bd4\u7279\u5bbd\u5ea6\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5927\u5c0f\u7684\u53d8\u5316\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u7814\u7a76\u83b7\u5f97\u4e86PTQ\u5f71\u54cd\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u53ef\u4e3a\u5f00\u53d1\u66f4\u597d\u5730\u4fdd\u7559\u76ee\u6807\u8ba4\u77e5\u529f\u80fd\u7684\u77e5\u8bc6\u611f\u77e5\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.18648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18648", "abs": "https://arxiv.org/abs/2508.18648", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u63d2\u5165\u4e3b\u52a8\u751f\u6210\u7684\u6d1e\u89c1\u6765\u6539\u5584LLM\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6784\u5efa\u4e86TBYS\u6846\u67b6\u4e0e\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660eTBYS\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u6548\u679c\u4f18\u8d8a\u3002", "motivation": "\u4f20\u7edfLLM\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u4eba\u7c7b\u5b9e\u9645\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5173\u952e\u7684\u5185\u5728\u601d\u8003\u6b65\u9aa4\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e2d\uff0c\u4eba\u7c7b\u901a\u5e38\u4e0d\u4f1a\u8868\u8fbe\u6240\u6709\u63a8\u7406\u7ec6\u8282\uff0c\u5bfc\u81f4\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u5728\u8fde\u7eed\u63a8\u7406\u6b65\u9aa4\u4e4b\u95f4\u63d2\u5165insight\uff08\u6d1e\u89c1\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u6536\u96c6\u548c\u7b5b\u9009in-context\u793a\u4f8b\uff0c\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u5fae\u8c03\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u6570\u636e\u96c6\u4e0a\uff0cTBYS\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u8868\u73b0\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u548c\u5fae\u8c03\u5de5\u4f5c\u91cf\u3002", "conclusion": "TBYS\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.18651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18651", "abs": "https://arxiv.org/abs/2508.18651", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u65b9\u6cd5CoDe\uff0c\u80fd\u52a8\u6001\u7ed3\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u7684\u6982\u7387\u8f93\u51fa\uff0c\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u56de\u590d\u7684\u5fe0\u5b9e\u6027\u800c\u4e0d\u635f\u5931\u8868\u8fbe\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u5728\u878d\u5408\u5916\u90e8\u77e5\u8bc6\u4ee5\u51cf\u5c11\u5e7b\u89c9\u65f6\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u56de\u590d\u7684\u5fe0\u5b9e\u6027\uff08faithfulness\uff09\u548c\u8868\u8fbe\u6027\uff08expressiveness\uff09\uff0c\u9020\u6210\u751f\u6210\u7ed3\u679c\u8981\u4e48\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u652f\u6301\uff0c\u8981\u4e48\u8868\u8fbe\u5197\u957f\u4e14\u4e0d\u81ea\u7136\u3002", "method": "\u5f15\u5165Collaborative Decoding\uff08CoDe\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u6563\u5ea6\u4e0e\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u52a8\u6001\u878d\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u7684\u8f93\u51fa\u6982\u7387\uff1b\u540c\u65f6\u8bbe\u8ba1\u77e5\u8bc6\u611f\u77e5\u91cd\u6392\u5e8f\u673a\u5236\uff0c\u9632\u6b62\u8fc7\u5ea6\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\uff0c\u5e76\u9002\u5f53\u5229\u7528\u5916\u90e8\u4fe1\u606f\u3002", "result": "\u63d0\u51fa\u7684Collaborative Decoding\uff08CoDe\uff09\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u5fe0\u5b9e\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u8868\u8fbe\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u79cd\u5927\u6a21\u578b\u548c\u8bc4\u6d4b\u6307\u6807\u4e0a\uff0cCoDe\u90fd\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "CoDe\u6846\u67b6\u51ed\u501f\u5206\u5e03\u6563\u5ea6\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u52a8\u6001\u878d\u5408\uff0c\u914d\u5408\u77e5\u8bc6\u611f\u77e5\u91cd\u6392\u5e8f\u673a\u5236\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u6a21\u578b\u5728\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65f6\u7684\u8868\u73b0\uff0c\u65e2\u4fdd\u8bc1\u5fe0\u5b9e\u6027\u53c8\u4fdd\u7559\u8868\u8fbe\u6027\uff0c\u5177\u6709\u826f\u597d\u901a\u7528\u6027\u3002"}}
{"id": "2508.18655", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18655", "abs": "https://arxiv.org/abs/2508.18655", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Emotion Omni\uff0c\u4e00\u4e2a\u53ef\u5728\u6709\u9650\u6570\u636e\u548c\u4f4e\u7b97\u529b\u6761\u4ef6\u4e0b\u7406\u89e3\u548c\u56de\u590d\u7528\u6237\u60c5\u611f\u7684\u8bed\u97f3LLM\uff0c\u5e76\u6784\u5efa\u4e8620\u4e07\u6761\u60c5\u611f\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u6781\u5927\u63d0\u5347\u4e86\u8bed\u97f3\u52a9\u624b\u7684\u60c5\u611f\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08Speech LLMs\uff09\u867d\u7136\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u8bed\u97f3\u4e0e\u7528\u6237\u4ea4\u4e92\uff0c\u4f46\u8fd8\u672a\u80fd\u5145\u5206\u7406\u89e3\u7528\u6237\u8bed\u97f3\u4e2d\u4e30\u5bcc\u7684\u60c5\u611f\u548c\u526f\u8bed\u8a00\u4fe1\u606f\u3002\u7531\u4e8e\u60c5\u611f\u7406\u89e3\u5bf9\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u73b0\u6709\u540c\u7406\u5fc3\u8bed\u97f3LLM\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6781\u9ad8\uff0c\u5982\u4f55\u5728\u6709\u9650\u6570\u636e\u4e0b\u751f\u6210\u6709\u540c\u7406\u5fc3\u7684\u8bed\u97f3\u56de\u590d\u6210\u4e3a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\u67b6\u6784Emotion Omni\uff0c\u80fd\u591f\u7406\u89e3\u7528\u6237\u8bed\u97f3\u8f93\u5165\u7684\u60c5\u611f\u5185\u5bb9\u5e76\u751f\u6210\u5177\u5907\u540c\u7406\u5fc3\u7684\u8bed\u97f3\u56de\u590d\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u5f00\u6e90TTS\u6846\u67b6\u8bbe\u8ba1\u4e86\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u5305\u542b20\u4e07\u6761\u60c5\u611f\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u540c\u7406\u5fc3\u8bed\u97f3\u52a9\u624b\u7684\u5f00\u53d1\u3002", "result": "Emotion Omni\u6a21\u578b\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u548c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u60c5\u611f\u5185\u5bb9\u7684\u7406\u89e3\u548c\u540c\u7406\u5fc3\u56de\u590d\u7684\u751f\u6210\u3002\u540c\u65f6\uff0c\u81ea\u5efa\u768420\u4e07\u60c5\u611f\u5bf9\u8bdd\u6570\u636e\u96c6\u4e3a\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5e76\u4e14\u76f8\u5173\u6f14\u793a\u5df2\u4e0a\u7ebf\u5c55\u793a\u3002", "conclusion": "Emotion Omni\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7528\u6237\u60c5\u611f\u7684\u7406\u89e3\u4e0e\u56de\u590d\u80fd\u529b\uff0c\u51cf\u5c0f\u4e86\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u4e0e\u7b97\u529b\u7684\u4f9d\u8d56\uff0c\u4e3a\u6784\u5efa\u60c5\u611f\u5316\u4eba\u673a\u4ea4\u4e92\u5e26\u6765\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2508.18673", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.18673", "abs": "https://arxiv.org/abs/2508.18673", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.", "AI": {"tldr": "\u8be5\u6587\u9488\u5bf9\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u6837\u672c\u9009\u62e9\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u5c06\u63d0\u793a\u9009\u53d6\u89c6\u4e3a\u8bfe\u7a0b\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u53cc\u91cd\u96be\u5ea6\u4fe1\u53f7\u8fdb\u884c\u96be\u5ea6\u5e73\u8861\u91c7\u6837\uff0c\u5927\u5e45\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u8868\u73b0\u7684\u7a33\u5b9a\u6027\u548c\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u8003(MCoT)\u5728\u63d0\u793a\u6837\u672c\u9009\u62e9\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u968f\u673a\u6216\u4eba\u5de5\u9009\u53d6\u7684\u6837\u672c\u672a\u517c\u987e\u6a21\u578b\u672c\u8eab\u7684\u77e5\u8bc6\u5206\u5e03\u548c\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u4e14\u6ce2\u52a8\u5927\u3002", "method": "\u53d7\u6559\u5b66\u7406\u8bba\u201c\u56e0\u6750\u65bd\u6559\u3001\u96be\u6613\u5e73\u8861\u201d\u542f\u53d1\uff0c\u5c06\u63d0\u793a\u9009\u62e9\u91cd\u65b0\u5b9a\u4e49\u4e3a\u63d0\u793a\u8bfe\u7a0b\u8bbe\u8ba1\u95ee\u9898\uff1a\u6839\u636e\u6a21\u578b\u5f53\u524d\u80fd\u529b\u6709\u5e8f\u751f\u6210\u9002\u5b9c\u7684\u8bad\u7ec3\u6837\u672c\u3002\u65b9\u6cd5\u7efc\u5408\u4e24\u7c7b\u96be\u5ea6\u6807\u5fd7\uff1a1) \u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u9884\u6d4b\u5206\u6b67\uff0c\u523b\u753b\u6a21\u578b\u4e3b\u89c2\u611f\u77e5\u96be\u5ea6\uff1b2) \u8bc4\u4f30\u6837\u672c\u672c\u8eab\u590d\u6742\u6027\uff0c\u523b\u753b\u6837\u672c\u56fa\u6709\u96be\u5ea6\u3002\u901a\u8fc7\u4e24\u7ef4\u5ea6\u5206\u6790\uff0c\u63d0\u51fa\u96be\u5ea6\u5e73\u8861\u91c7\u6837\u7b56\u7565\uff0c\u4fdd\u8bc1\u6837\u672c\u5728\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u591a\u6837\u6027\u3002", "result": "\u57285\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u548c\u591a\u4e2a\u6d41\u884c\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e0a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u548c\u7a33\u5b9a\u4e86\u6027\u80fd\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u968f\u673a\u91c7\u6837\u5e26\u6765\u7684\u6027\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u96be\u5ea6\u5e73\u8861\u8bfe\u7a0b\u4e3a\u6838\u5fc3\u7684MCoT\u63d0\u793a\u9009\u62e9\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u6a21\u578b\u611f\u77e5\u96be\u5ea6\u548c\u6837\u672c\u672c\u8eab\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u4e14\u4f18\u5316\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2508.18687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18687", "abs": "https://arxiv.org/abs/2508.18687", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u73b0\u6709Med-VLMs\u5728\u5904\u7406\u8bed\u4e49\u7b49\u4ef7\u95ee\u9898\u65f6\u7b54\u6848\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u533b\u7597\u8bca\u65ad\u53ef\u9760\u6027\u3002\u4f5c\u8005\u6784\u5efaRoMed\u6570\u636e\u96c6\u5e76\u63d0\u51faCCL\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u9886\u5148\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08Med-VLMs\uff09\u5728\u9ad8\u98ce\u9669\u533b\u5b66\u5e94\u7528\u4e2d\uff0c\u9762\u5bf9\u8bed\u4e49\u7b49\u4ef7\u4f46\u8868\u8fbe\u65b9\u5f0f\u4e0d\u540c\u7684\u533b\u5b66\u95ee\u9898\u65f6\uff0c\u5176\u7b54\u6848\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u8bca\u65ad\u7684\u53ef\u9760\u6027\u3002\u52a8\u673a\u662f\u89e3\u51b3\u6a21\u578b\u5bf9\u4e0d\u540c\u95ee\u9898\u8868\u8ff0\u8106\u5f31\u3001\u7b54\u590d\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaRoMed\u6570\u636e\u96c6\uff0c\u5305\u542b144,000\u4e2a\u533b\u7597\u89c6\u89c9\u95ee\u7b54\uff0c\u8986\u76d6\u8bcd\u7ea7\u3001\u53e5\u7ea7\u4e0e\u8bed\u4e49\u7ea7\u6270\u52a8\uff1b\u63d0\u51fa\u4e00\u81f4\u6027\u4e0e\u5bf9\u6bd4\u5b66\u4e60\uff08CCL\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u7684\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u504f\u5dee\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5bf9\u8868\u8fbe\u53d8\u4f53\u7684\u9c81\u68d2\u6027\u3002", "result": "\u539f\u6709SOTA\u6a21\u578b\u5728RoMed\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff08\u5982\u53ec\u56de\u7387\u4e0b\u964d40%\uff09\uff0c\u66b4\u9732\u51fa\u9c81\u68d2\u6027\u7f3a\u9677\u3002CCL\u65b9\u6cd5\u5728\u4e09\u5927\u4e3b\u6d41VQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u5e76\u5728RoMed\u6d4b\u8bd5\u96c6\u4e0a\u5c06\u7b54\u6848\u4e00\u81f4\u6027\u63d0\u534750%\uff0c\u663e\u8457\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u8868\u8ff0\u53d8\u4f53\u65f6\u5b58\u5728\u4e25\u91cd\u4e00\u81f4\u6027\u95ee\u9898\u3002\u6784\u5efa\u7684\u65b0\u6570\u636e\u96c6\u548c\u4e00\u81f4\u6027-\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7b54\u590d\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u533b\u7597AI\u8bca\u65ad\u63d0\u4f9b\u66f4\u53ef\u9760\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2508.18701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18701", "abs": "https://arxiv.org/abs/2508.18701", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAttention2Probability\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u7cfb\u7edf\u7684\u9886\u57df\u672f\u8bed\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u672f\u8bed\u5e72\u9884\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5927\u6a21\u578b\u5728\u901a\u7528\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u5728\u5904\u7406\u7279\u5b9a\u9886\u57df\u672f\u8bed\u6216\u65b0\u8bcd\u65f6\u4ecd\u7136\u5b58\u5728\u51c6\u786e\u6027\u6311\u6218\u3002\u89e3\u51b3\u8fd9\u4e2a\u96be\u9898\u5bf9\u4e8e\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAttention2Probability\u7684\u6ce8\u610f\u529b\u9a71\u52a8\u672f\u8bed\u6982\u7387\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5c06\u8bed\u97f3\u4e0e\u672f\u8bed\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u5316\u4e3a\u672f\u8bed\u51fa\u73b0\u6982\u7387\uff0c\u5e76\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u6784\u5efa\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u5305\u542b\u672f\u8bed\u7684\u8bed\u97f3\u6570\u636e\u96c6\u7528\u4e8e\u76f8\u5173\u4efb\u52a1\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAttention2Probability\u65b9\u6cd5\u5728\u672f\u8bed\u68c0\u7d22\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684VectorDB\u65b9\u6cd5\uff0c\u4e2d\u6587\u6700\u5927\u53ec\u56de\u7387\u4e3a92.57%\uff0c\u82f1\u6587\u4e3a86.83%\uff0c\u5355\u6b21\u67e5\u8be2\u5ef6\u8fdf\u4ec58.71\u6beb\u79d2\u3002\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86SLM\u5728\u8bc6\u522b\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u672f\u8bed\u7684\u51c6\u786e\u73876%-17%\u3002\u540c\u65f6\u63ed\u793a\u5f53\u524dSLM\u5bf9\u672f\u8bed\u5229\u7528\u4ecd\u6709\u5c40\u9650\u6027\u3002", "conclusion": "Attention2Probability\u4e0d\u4ec5\u5728\u672f\u8bed\u8bc6\u522b\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u8f83\u5927\u8fdb\u6b65\uff0c\u8fd8\u901a\u8fc7\u65b0\u7684\u6570\u636e\u96c6\u63a8\u52a8\u8bed\u97f3\u5230\u6587\u672c\u9886\u57df\u672f\u8bed\u5e72\u9884\u7814\u7a76\u53d1\u5c55\u3002\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u5b9e\u9645\u8bed\u97f3\u7cfb\u7edf\u4e2d\u7684\u9886\u57df\u672f\u8bed\u5904\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u548c\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.18709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18709", "abs": "https://arxiv.org/abs/2508.18709", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAOF\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u8fc7\u6ee4\u4e0e\u65b0\u9896\u6027\u7ea6\u675f\u63d0\u5347\u591a\u8bed\u8a00\u8c1c\u9898\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u539f\u521b\u6027\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u548c\u6a21\u578b\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5bb9\u5197\u4f59\u5e76\u63d0\u9ad8\u4e86\u521b\u9020\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u8c1c\u9898\u751f\u6210\u9700\u8981\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65e2\u5177\u5907\u6587\u5316\u9002\u5e94\u529b\uff0c\u53c8\u8981\u80fd\u8fdb\u884c\u521b\u9020\u6027\u62bd\u8c61\uff0c\u4f46\u4f20\u7edf\u63d0\u793a\u7b56\u7565\u5e38\u5e38\u5bfc\u81f4\u6a21\u578b\u53ea\u662f\u590d\u7528\u8bb0\u5fc6\u4e2d\u7684\u8c1c\u9898\u6216\u8fdb\u884c\u6d45\u5c42\u6b21\u6539\u5199\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u539f\u521b\u6027\u8fc7\u6ee4\uff08AOF\uff09\u63d0\u793a\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5254\u9664\u5197\u4f59\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u5f3a\u5236\u8981\u6c42\u8bcd\u6c47\u65b0\u9896\u6027\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "result": "AOF\u589e\u5f3a\u7684GPT-4o\u5728\u65e5\u8bed\u6d4b\u8bd5\u4e2d\u8fbe\u52300.177\u7684Self-BLEU\u548c0.915\u7684Distinct-2\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8bcd\u6c47\u591a\u6837\u6027\u548c\u66f4\u4f4e\u7684\u5197\u4f59\u5ea6\uff1b\u8d85\u8d8a\u4e86\u5176\u4ed6\u63d0\u793a\u65b9\u6cd5\u548c\u8bed\u8a00\u5bf9\u3002", "conclusion": "\u8bed\u4e49\u62d2\u7edd\u673a\u5236\u65e0\u9700\u9488\u5bf9\u5177\u4f53\u4efb\u52a1\u5fae\u8c03\uff0c\u4e5f\u80fd\u5f15\u5bfc\u6a21\u578b\u4ea7\u751f\u5177\u5907\u6587\u5316\u57fa\u7840\u548c\u521b\u9020\u6027\u7684\u5185\u5bb9\u3002"}}
{"id": "2508.18715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18715", "abs": "https://arxiv.org/abs/2508.18715", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86\u89e3\u91ca\u5148\u884c\u7684MGT\u68c0\u6d4b\u65b0\u6846\u67b6EMMM\uff0c\u517c\u987e\u5ef6\u8fdf\u3001\u51c6\u786e\u7387\u548c\u9762\u5411\u975e\u4e13\u5bb6\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6613\u4e8e\u7406\u89e3\u4e14\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5ba2\u670d\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\uff0c\u6076\u610f\u884c\u4e3a\u8005\u53ef\u4ee5\u5229\u7528\u5176\u751f\u6210\u7684\u673a\u5668\u6587\u672c\u8fdb\u884c\u7528\u6237\u5192\u5145\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5728\u7ebf\u5bf9\u8bdd\u573a\u666f\u4e0b\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u88ab\u975e\u4e13\u4e1a\u5ba2\u670d\u4eba\u5458\u4fe1\u8d56\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u89e3\u91ca-\u540e\u68c0\u6d4b\uff08explanation-then-detection\uff09\u6846\u67b6EMMM\uff0c\u4f18\u5316\u4ee5\u975e\u4e13\u5bb6\u6613\u61c2\u89e3\u91ca\u4e3a\u4f18\u5148\uff0c\u7ed3\u5408\u51c6\u786e\u6027\u4e0e\u4f4e\u5ef6\u8fdf\uff0c\u63d0\u5347\u7528\u6237\u5bf9AI\u68c0\u6d4b\u7cfb\u7edf\u7684\u4fe1\u4efb\u5ea6\u3002", "result": "EMMM\u63d0\u4f9b\u4e86\u9002\u5408\u975e\u4e13\u5bb6\u7528\u6237\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u5728\u4eba\u5de5\u8bc4\u6d4b\u4e2d\u670970%\u7684\u8bc4\u5ba1\u66f4\u559c\u6b22\u5176\u8f93\u51fa\u3002\u540c\u65f6\uff0c\u5176\u51c6\u786e\u7387\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u6301\u5e73\uff0c\u4e14\u8f93\u51fa\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\u3002", "conclusion": "EMMM\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u5728\u7ebf\u5ba2\u670d\u573a\u666f\u4e0bMGT\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u5df2\u5f00\u6e90\u76f8\u5e94\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.18739", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18739", "abs": "https://arxiv.org/abs/2508.18739", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.", "AI": {"tldr": "DIVER\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u501f\u52a9\u5927\u6a21\u578b\u3001SFT\u548cRL\uff0c\u80fd\u5728\u5b9e\u9645\u5e73\u53f0\u4e0a\u663e\u8457\u63d0\u5347\u5e7f\u544a\u6807\u9898\u7684\u591a\u6837\u6027\u548c\u70b9\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5e7f\u544a\u6807\u9898\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5316\u4e8e\u6807\u9898\u8d28\u91cf\u6216\u70b9\u51fb\u7387\uff08CTR\uff09\uff0c\u5ffd\u89c6\u4e86\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u540c\u8d28\u5316\u3002\u5e7f\u544a\u884c\u4e1a\u6025\u9700\u65e2\u6709\u9ad8\u8d28\u91cf\u53c8\u6709\u591a\u6837\u6027\u7684\u6807\u9898\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u5438\u5f15\u66f4\u591a\u53d7\u4f17\u7fa4\u4f53\u3002", "method": "\u63d0\u51fa\u4e86DIVER\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u591a\u76ee\u6807\u4f18\u5316\u7ed3\u5408\u6709\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u8054\u5408\u4f18\u5316\u6807\u9898\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u5957\u81ea\u52a8\u751f\u6210\u5305\u542b\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u6807\u9898\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "result": "DIVER\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u6807\u9898\u7684\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u3002\u5728\u4e00\u4e2a\u62e5\u6709\u6570\u4ebf\u7528\u6237\u7684\u5927\u89c4\u6a21\u5185\u5bb9\u5e73\u53f0\u4e0a\u7ebf\u540e\uff0c\u5e7f\u544a\u4e3b\u4ef7\u503c\uff08ADVV\uff09\u63d0\u53474.0%\uff0c\u70b9\u51fb\u7387\uff08CTR\uff09\u63d0\u53471.4%\u3002", "conclusion": "DIVER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u6807\u9898\u751f\u6210\u4e2d\u8fc7\u5ea6\u540c\u8d28\u5316\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u9898\u7684\u591a\u6837\u6027\u53ca\u5e7f\u544a\u6574\u4f53\u6548\u679c\uff0c\u4e3a\u5e7f\u544a\u4e3b\u5e26\u6765\u66f4\u5927\u6536\u76ca\u3002"}}
{"id": "2508.18740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18740", "abs": "https://arxiv.org/abs/2508.18740", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u591a\u573a\u666f\u591a\u6a21\u6001MECTEC\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5f02\u6784\u56fe\u7684\u65b0\u6a21\u578bM3HG\uff0c\u5b9e\u73b0\u60c5\u611f\u56e0\u679c\u4e09\u5143\u7ec4\u62bd\u53d6\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u60c5\u611f\u56e0\u679c\u4e09\u5143\u7ec4\u62bd\u53d6\uff08MECTEC\uff09\u7814\u7a76\u53d7\u5230\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u573a\u666f\u5355\u4e00\u95ee\u9898\u7684\u9650\u5236\uff0c\u7f3a\u4e4f\u80fd\u652f\u6301\u6a21\u578b\u53d1\u5c55\u7684\u591a\u6837\u5316\u5bf9\u8bdd\u6570\u636e\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u9996\u4e2a\u591a\u6a21\u6001\u3001\u591a\u573a\u666fMECTEC\u6570\u636e\u96c6MECAD\uff0c\u5305\u542b\u6765\u81ea56\u90e8\u7535\u89c6\u5267\u7684989\u4e2a\u4f1a\u8bdd\uff0c\u5e76\u63d0\u51fa\u4e86M3HG\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5f02\u6784\u56fe\u7ed3\u6784\u663e\u5f0f\u5efa\u6a21\u60c5\u611f\u4e0e\u56e0\u679c\u8bed\u5883\uff0c\u540c\u65f6\u5728\u8bdd\u8bed\u95f4\u4e0e\u8bdd\u8bed\u5185\u591a\u5c42\u7ea7\u878d\u5408\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM3HG\u6a21\u578b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86MECTEC\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86MECTEC\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u573a\u666f\u5355\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u65b0\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u5148\u8fdb\u6027\u3002"}}
{"id": "2508.18748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18748", "abs": "https://arxiv.org/abs/2508.18748", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "\u63d0\u51faChronoRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u548c\u6392\u5e8f\u6587\u6863\u7247\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u53d9\u4e8b\u957f\u6587\u672c\u95ee\u7b54\u4e2d\u65f6\u5e8f\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u8868\u73b0\uff0c\u6548\u679c\u8d85\u8d8a\u73b0\u6709RAG\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684RAG\u65b9\u6cd5\u5728\u5904\u7406\u53d9\u4e8b\u7c7b\u957f\u6587\u672c\u95ee\u7b54\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u53d9\u4e8b\u6587\u672c\u7684\u7406\u89e3\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u4ee5\u53ca\u7247\u6bb5\u95f4\u7684\u65f6\u5e8f\u5173\u7cfb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b64\u7acb\u7684\u4fe1\u606f\u7247\u6bb5\u3002", "method": "\u63d0\u51fa\u4e86ChronoRAG\uff0c\u4e00\u4e2a\u4e13\u4e3a\u53d9\u4e8b\u6587\u672c\u8bbe\u8ba1\u7684\u65b0\u578bRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u96f6\u6563\u4fe1\u606f\u6574\u5408\u4e3a\u7ed3\u6784\u5316\u7247\u6bb5\uff0c\u5e76\u5728\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u4e2d\u663e\u5f0f\u6355\u6349\u4e0e\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u3002", "result": "\u5728NarrativeQA\u6570\u636e\u96c6\u4e0a\uff0cChronoRAG\u5728\u4e8b\u5b9e\u8bc6\u522b\u548c\u590d\u6742\u65f6\u5e8f\u5173\u7cfb\u7406\u89e3\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ChronoRAG\u5f3a\u8c03\u65f6\u5e8f\u63a8\u7406\u5bf9\u4e8e\u53d9\u4e8b\u7c7bQA\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6709\u6548\u63d0\u5347\u4e86\u957f\u6587\u672c\u53d9\u4e8b\u95ee\u7b54\u7684\u8868\u73b0\u3002"}}
{"id": "2508.18773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18773", "abs": "https://arxiv.org/abs/2508.18773", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ThinkDial\uff0c\u8fd9\u662f\u9996\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3cgpt-oss\u7684\u53ef\u63a7\u63a8\u7406\u6a21\u5f0f\u3002\u5b83\u901a\u8fc7\u591a\u6a21\u5f0f\u9884\u7b97\u63a7\u5236\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u5e45\u51cf\u5c11token\u60c5\u51b5\u4e0b\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff0c\u5c55\u73b0\u4e86\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u7684LLM\u63a8\u7406\u8d44\u6e90\u8c03\u63a7\u65b0\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u901a\u8fc7Chain-of-thought\u63a8\u7406\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u63a7\u5236\u5176\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4ecd\u662f\u5173\u952e\u96be\u9898\u3002\u5c3d\u7ba1\u4e13\u6709\u7cfb\u7edf\uff08\u5982OpenAI\u7684gpt-oss\u7cfb\u5217\uff09\u5df2\u7ecf\u5f15\u5165\u4e86\u79bb\u6563\u64cd\u4f5c\u6a21\u5f0f\u6765\u76f4\u89c2\u8c03\u63a7\u63a8\u7406\u8fc7\u7a0b\uff0c\u5f00\u6e90\u793e\u533a\u5374\u5c1a\u672a\u6709\u7c7b\u4f3c\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86ThinkDial\uff0c\u8fd9\u662f\u9996\u4e2a\u5f00\u6e90\u3001\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3cgpt-oss\u7684\u53ef\u63a7\u63a8\u7406\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e09\u79cd\u72ec\u7acb\u7684\u63a8\u7406\u6a21\u5f0f\uff08\u9ad8\u3001\u4e2d\u3001\u4f4e\uff09\u5b9e\u73b0\u63a8\u7406\u8ba1\u7b97\u7684\u52a8\u6001\u7ba1\u63a7\u3002\u65b9\u6cd5\u6838\u5fc3\u5305\u62ec\u9884\u7b97\u6a21\u5f0f\u7684\u6709\u76d1\u7763\u5fae\u8c03\uff08\u5c06\u53ef\u63a7\u63a8\u7406\u80fd\u529b\u76f4\u63a5\u690d\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff09\u548c\u5206\u4e24\u9636\u6bb5\u7684\u9884\u7b97\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08\u7ed3\u5408\u81ea\u9002\u5e94\u5956\u52b1\u8c03\u6574\uff09\u3002", "result": "ThinkDial\u5728\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\uff0c\u80fd\u7075\u6d3b\u5e73\u8861\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1aMedium\u6a21\u5f0f\u5b9e\u73b050% token\u51cf\u5c11\u4e14\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e10%\uff0cLow\u6a21\u5f0f\u51cf\u5c1175% token\u800c\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e15%\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u54cd\u5e94\u957f\u5ea6\u663e\u8457\u7f29\u77ed\u7684\u540c\u65f6\uff0c\u4f9d\u7136\u8fbe\u5230\u4e86\u65e2\u5b9a\u6027\u80fd\u9608\u503c\uff0c\u5e76\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ThinkDial\u7a81\u7834\u4e86\u5f00\u6e90LLM\u63a8\u7406\u63a7\u5236\u7684\u74f6\u9888\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8fde\u7eed\u96c6\u6210\u9884\u7b97\u611f\u77e5\u8bad\u7ec3\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ee5\u54cd\u5e94\u957f\u5ea6\u548c\u8ba1\u7b97\u6d88\u8017\u4e3a\u6838\u5fc3\u7684\u591a\u6a21\u5f0f\u52a8\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4fdd\u6301\u4e86\u8f83\u5f3a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b83\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2508.18780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18780", "abs": "https://arxiv.org/abs/2508.18780", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f15\u5bfcLLMs\u8fdb\u884c\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\uff0c\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u53ec\u56de\u7387\u548c\u6574\u4f53\u8868\u73b0\u4e0a\u8fbe\u5230SOTA\uff0c\u5c55\u73b0\u51faRL\u673a\u5236\u7684\u4f18\u52bf\u548c\u672a\u6765\u53ef\u63a7\u6027\u3002", "motivation": "\u76ee\u524dGEC\u9886\u57df\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u548c\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60(Rule-Based RL)\u65b0\u6846\u67b6\uff0c\u7528\u4ee5\u5f15\u5bfcLLMs\u8fdb\u884c\u8bed\u6cd5\u7ea0\u9519\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u7528\u76d1\u7763\u7ec6\u8c03\u751f\u6210\u7ea0\u6b63\u540e\u7684\u53e5\u5b50\u3002", "result": "\u5728\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u53d6\u5f97\u4e86\u76ee\u524d\u6700\u4f73\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u53ec\u56de\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u89c4\u5219\u5f15\u5bfc\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6fc0\u53d1LLMs\u7684\u6f5c\u529b\uff0c\u4e3aGEC\u5e26\u6765\u66f4\u53ef\u63a7\u548c\u53ef\u9760\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.18783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18783", "abs": "https://arxiv.org/abs/2508.18783", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "title": "Controllable Conversational Theme Detection Track at DSTC 12", "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86\u5728\u5bf9\u8bdd\u5206\u6790\u4e2d\u8fdb\u884c\u53ef\u63a7\u4e3b\u9898\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u6bd4\u8d5bDSTC12\u9a8c\u8bc1\u76f8\u5173\u6280\u672f\u53ca\u6210\u679c\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55\u548c\u6280\u672f\u521b\u65b0\u3002", "motivation": "\u9762\u5bf9\u5bf9\u8bdd\u6570\u636e\u5206\u6790\u9886\u57df\u65e5\u76ca\u589e\u957f\u7684\u6570\u636e\u91cf\u4e0e\u590d\u6742\u6027\u95ee\u9898\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u603b\u7ed3\u5bf9\u8bdd\u4e3b\u9898\u53ef\u66ff\u4ee3\u7e41\u7410\u7684\u4eba\u5de5\u5206\u6790\u8fc7\u7a0b\uff0c\u63d0\u5347\u6548\u7387\uff0c\u6ee1\u8db3\u7528\u6237\u591a\u6837\u5316\u548c\u5b9a\u5236\u5316\u9700\u6c42\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5ba2\u6237\u652f\u6301\u4e0e\u9500\u552e\u7b49\u573a\u666f\u3002", "method": "\u5c06\u4e3b\u9898\u68c0\u6d4b\u95ee\u9898\u5b9a\u4e49\u4e3a\u5bf9\u8bdd\u8bed\u53e5\u7684\u8054\u5408\u805a\u7c7b\u548c\u4e3b\u9898\u6807\u7b7e\u5206\u914d\uff0c\u901a\u8fc7\u7528\u6237\u504f\u597d\u6570\u636e\u5b9e\u73b0\u4e3b\u9898\u7c92\u5ea6\u7684\u53ef\u63a7\u6027\u3002\u7ec4\u7ec7\u4e86\u516c\u5f00\u6bd4\u8d5b\uff0c\u63d0\u4f9b\u6570\u636e\u96c6\u3001\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u8bc4\u4ef7\u6307\u6807\uff0c\u5e76\u5206\u6790\u53c2\u8d5b\u961f\u4f0d\u65b9\u6848\u3002", "result": "\u672c\u6b21\u516c\u5f00\u8d5b\u4fc3\u8fdb\u4e86\u4e3b\u9898\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63a8\u52a8\u4e86\u8054\u5408\u805a\u7c7b\u4e0e\u53ef\u63a7\u4e3b\u9898\u6807\u7b7e\u5206\u914d\u65b9\u6cd5\u7684\u7814\u7a76\u3002\u516c\u5f00\u7684\u6570\u636e\u548c\u4ee3\u7801\u8d44\u6e90\uff0c\u4e3a\u793e\u533a\u5e26\u6765\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u521b\u65b0\u7684\u5b9e\u7528\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5bf9\u8bdd\u5206\u6790\u4e2d\u8fdb\u884c\u53ef\u63a7\u4e3b\u9898\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u6bd4\u8d5b\uff08DSTC12\uff09\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u548c\u521b\u65b0\u6027\u3002"}}
{"id": "2508.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18791", "abs": "https://arxiv.org/abs/2508.18791", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.", "AI": {"tldr": "LaTeXTrans\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347LaTeX\u6587\u6863\u7684\u683c\u5f0f\u548c\u5185\u5bb9\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5728\u901a\u7528\u6587\u672c\u7684\u7ffb\u8bd1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9762\u5bf9\u7ed3\u6784\u5316\u7684LaTeX\u683c\u5f0f\u6587\u6863\u4ecd\u5b58\u5728\u5f88\u5927\u6311\u6218\u3002\u8fd9\u7c7b\u6587\u6863\u4e2d\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u5b66\u516c\u5f0f\u3001\u8868\u683c\u3001\u56fe\u50cf\u548c\u4ea4\u53c9\u5f15\u7528\u7b49\u9886\u57df\u7279\u5b9a\u8bed\u6cd5\u6df7\u5408\uff0c\u8981\u6c42\u5728\u7ffb\u8bd1\u4e2d\u51c6\u786e\u4fdd\u7559\u683c\u5f0f\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86LaTeXTrans\uff0c\u8fd9\u662f\u4e00\u4e2a\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u516d\u4e2a\u7279\u5b9a\u529f\u80fd\u7684\u667a\u80fd\u4f53\uff1a\u89e3\u6790\u5668\u7528\u4e8e\u5c06LaTeX\u62c6\u5206\u4e3a\u6613\u4e8e\u7ffb\u8bd1\u7684\u5355\u5143\uff1b\u7ffb\u8bd1\u5668\u3001\u9a8c\u8bc1\u5668\u3001\u6458\u8981\u5668\u548c\u672f\u8bed\u63d0\u53d6\u5668\u534f\u4f5c\u4ee5\u5b9e\u73b0\u81ea\u6211\u6821\u6b63\u548c\u4e13\u4e1a\u672f\u8bed\u4e00\u81f4\u6027\u7ffb\u8bd1\uff1b\u751f\u6210\u5668\u7528\u4e8e\u91cd\u6784\u7ed3\u6784\u5316\u7684LaTeX\u6587\u6863\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLaTeXTrans\u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u4e0e\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u4e3b\u6d41\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u4e14\u5b9e\u7528\u5730\u7ffb\u8bd1LaTeX\u6587\u6863\u3002", "conclusion": "LaTeXTrans\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86LaTeX\u7ed3\u6784\u5316\u6587\u6863\u7684\u7ffb\u8bd1\u96be\u9898\uff0c\u4e3a\u4e13\u4e1a\u6587\u6863\u7684\u7f16\u8bd1\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u81ea\u52a8\u5316\u7ffb\u8bd1\u5de5\u5177\u3002"}}
{"id": "2508.18819", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18819", "abs": "https://arxiv.org/abs/2508.18819", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u5408AMR\u8bed\u4e49\u548c\u4f20\u64ad\u7279\u5f81\u7684\u81ea\u76d1\u7763\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u751f\u6210\u8d1f\u6837\u672c\u5e76\u91c7\u7528\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u6570\u5b57\u65f6\u4ee3\u865a\u5047\u4fe1\u606f\u6cdb\u6ee5\u5bf9\u793e\u4f1a\u5e26\u6765\u4e25\u91cd\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u53ca\u793e\u4f1a\u52a8\u6001\uff0c\u540c\u65f6\u5bf9\u6807\u6ce8\u6570\u636e\u96c6\u4f9d\u8d56\u5927\u3001\u8d44\u6e90\u6d88\u8017\u9ad8\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408AMR\u8868\u793a\u7684\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u548c\u65b0\u95fb\u4f20\u64ad\u52a8\u6001\u3002\u521b\u65b0\u6027\u5730\u5f15\u5165LLM\u751f\u6210\u7684\u8d1f\u951a\u70b9\u7684\u56fe\u5bf9\u6bd4\u635f\u5931\uff08LGCL\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u56fe\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u793e\u4ea4\u4f20\u64ad\u7279\u5f81\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\uff0c\u6548\u679c\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u7ed3\u5408\u8bed\u4e49\u548c\u793e\u4ea4\u4f20\u64ad\u7279\u5f81\u7684\u81ea\u76d1\u7763\u68c0\u6d4b\u6846\u67b6\u5728\u865a\u5047\u4fe1\u606f\u8bc6\u522b\u4e0a\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002"}}
{"id": "2508.18824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18824", "abs": "https://arxiv.org/abs/2508.18824", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u7a0b\u5e8f\u8f85\u52a9\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u5b66\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u4fdd\u8bc1\u6570\u636e\u591a\u6837\u6027\u548c\u6b63\u786e\u6027\u3002\u4f7f\u7528\u8be5\u6570\u636e\u5fae\u8c03\u540e\u7684\u5927\u6a21\u578b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6570\u5b66\u63a8\u7406\u80fd\u529b\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u6602\u8d35\u3001\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u53ef\u9760\u6027\u7684\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u7a0b\u5e8f\u8f85\u52a9\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u548c\u9886\u57df\u7279\u5b9a\u5de5\u5177\uff0c\u81ea\u52a8\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u8f6c\u5316\u6210\u81ea\u7136\u8bed\u8a00\u7684\u9898\u76ee\u548c\u89e3\u7b54\u5bf9\u3002\u5229\u7528\u53cc\u5411\u9a8c\u8bc1\u673a\u5236\u4fdd\u8bc1\u6570\u636e\u7684\u6b63\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u751f\u6210\u4e861230\u4e07\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u5b66\u9898\u76ee-\u89e3\u7b54\u4e09\u5143\u7ec4\u3002\u6a21\u578b\u5728\u8fd9\u4e9b\u6570\u636e\u4e0a\u5fae\u8c03\u540e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u63a8\u7406\u80fd\u529b\u6709\u663e\u8457\u63d0\u5347\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "\u7a0b\u5e8f\u8f85\u52a9\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u5b66\u8bed\u6599\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u6570\u5b66\u63a8\u7406\u53ca\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u5907\u5f88\u5f3a\u7684\u6269\u5c55\u6027\u4e0e\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.18847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18847", "abs": "https://arxiv.org/abs/2508.18847", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.", "AI": {"tldr": "\u5927\u6a21\u578b\u5e38\u51fa\u73b0\u9ad8\u7f6e\u4fe1\u5ea6\u5374\u9519\u8bef\u7684\u201c\u8fc7\u5ea6\u81ea\u4fe1\u201d\u73b0\u8c61\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002\u672c\u6587\u63d0\u51fa\u7b80\u5355\u9ad8\u6548\u7684ConfTuner\u65b9\u6cd5\uff0c\u901a\u8fc7tokenized Brier score\u635f\u5931\u51fd\u6570\u5fae\u8c03\uff0c\u5927\u5e45\u63d0\u5347\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u6709\u52a9\u4e8e\u6253\u9020\u53ef\u4fe1\u5927\u6a21\u578b\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\uff08LLMs\uff09\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u79d1\u5b66\u3001\u6cd5\u5f8b\u548c\u533b\u7597\uff09\u5e94\u7528\u4e2d\u5f80\u5f80\u51fa\u73b0\u201c\u8fc7\u5ea6\u81ea\u4fe1\u201d\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5373\u4f7f\u9519\u8bef\u4e5f\u4f1a\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u56de\u7b54\uff0c\u5f71\u54cd\u5176\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\uff08\u5982\u63d0\u793a\u5de5\u7a0b\u6216\u7528\u542f\u53d1\u5f0f\u7f6e\u4fe1\u5ea6\u5fae\u8c03\uff09\u666e\u9002\u6027\u6709\u9650\uff0c\u6025\u9700\u66f4\u7b80\u5355\u9ad8\u6548\u4e14\u666e\u9002\u7684\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faConfTuner\u65b9\u6cd5\uff0c\u5373\u901a\u8fc7\u4e00\u4e2a\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u2014\u2014tokenized Brier score\uff0c\u5bf9LLM\u8fdb\u884c\u7b80\u5355\u9ad8\u6548\u7684\u5fae\u8c03\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u771f\u5b9e\u6216\u4ee3\u7406\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002\u8be5\u635f\u5931\u51fd\u6570\u88ab\u7406\u8bba\u8bc1\u660e\u4e3a\u4e00\u79cd\u6b63\u786e\u7684\u8bc4\u5206\u89c4\u5219\uff0c\u53ef\u4ee5\u5408\u7406\u6fc0\u52b1\u6a21\u578b\u62a5\u544a\u771f\u6b63\u7684\u7f6e\u4fe1\u6982\u7387\u3002", "result": "\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cConfTuner\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u7684\u6821\u51c6\u80fd\u529b\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u9ed1\u76d2\u5927\u6a21\u578b\uff08\u5982GPT-4o\uff09\u3002\u5b9e\u9a8c\u8fd8\u663e\u793a\uff0c\u66f4\u597d\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u81ea\u6211\u7ea0\u9519\u548c\u6a21\u578b\u7ea7\u8054\u7b49\u4e0b\u6e38\u80fd\u529b\u3002", "conclusion": "ConfTuner\u4e3aLLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u901a\u7528\u3001\u7406\u8bba\u6709\u4fdd\u8bc1\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u8d56\u7684\u5927\u6a21\u578b\u7cfb\u7edf\u3002\u540c\u65f6\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e3b\u6d41\u5f00\u6e90/\u95ed\u6e90\u5927\u6a21\u578b\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.18870", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18870", "abs": "https://arxiv.org/abs/2508.18870", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReflectivePrompt\uff0c\u901a\u8fc7\u81ea\u53cd\u8fdb\u5316\u673a\u5236\u6539\u8fdb\u81ea\u52a8\u63d0\u793a\u7b97\u6cd5\uff0c\u5728\u591a\u9879\u4efb\u52a1\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u4e3a\u81ea\u52a8\u63d0\u793a\u9886\u57df\u7684\u6700\u65b0\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u9ad8\u6548\u63d0\u793a\uff08prompt\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9886\u57df\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u73b0\u6709\u7684\u8fdb\u5316\u7b97\u6cd5\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u8fd8\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u548c\u5168\u9762\u7684\u63d0\u793a\u641c\u7d22\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86ReflectivePrompt\uff0c\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\uff08evolutionary algorithms\uff09\u7684\u81ea\u53cd\u5f0f\u8fdb\u5316\u65b9\u6cd5\u3002\u901a\u8fc7\u5728\u4ea4\u53c9\u548c\u7cbe\u82f1\u53d8\u5f02\u64cd\u4f5c\u524d\u5f15\u5165\u77ed\u671f\u4e0e\u957f\u671f\u53cd\u601d\u64cd\u4f5c\uff0c\u79ef\u7d2f\u5e76\u52a8\u6001\u66f4\u65b0\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u63d0\u5347\u6240\u5f97\u63d0\u793a\u7684\u8d28\u91cf\u3002", "result": "ReflectivePrompt\u572833\u4e2a\u5206\u7c7b\u548c\u6587\u672c\u751f\u6210\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5f00\u653e\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982t-lite-instruct-0.1\u548cgemma3-27b-it\uff09\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReflectivePrompt\u5728\u51e0\u4e4e\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff08\u5982EvoPrompt\uff09\uff0c\u5982BBH\u4efb\u52a1\u4e0a\u63d0\u5347\u8fbe28%\u3002", "conclusion": "ReflectivePrompt\u4f5c\u4e3a\u8fdb\u5316\u7b97\u6cd5\u9a71\u52a8\u7684\u81ea\u52a8\u63d0\u793a\u65b0\u65b9\u6cd5\uff0c\u5728\u81ea\u52a8\u63d0\u793a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u662f\u5f53\u524d\u6700\u6709\u6548\u7684\u76f8\u5173\u65b9\u6cd5\u4e4b\u4e00\u3002"}}
{"id": "2508.18872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18872", "abs": "https://arxiv.org/abs/2508.18872", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u5185\u5bb9\u5206\u6790\uff08LACA\uff09\u7684\u65b9\u6cd5\uff0c\u5e2e\u52a9\u8ba1\u7b97\u6559\u80b2\u7814\u7a76\u8005\u9ad8\u6548\u5730\u5206\u6790\u5927\u91cf\u6587\u672c\u6570\u636e\uff0c\u63d0\u5347\u7814\u7a76\u4e25\u8c28\u6027\u548c\u666e\u9002\u6027\uff0c\u5bf9\u5b66\u79d1\u53d1\u5c55\u5177\u6709\u63a8\u52a8\u4f5c\u7528\u3002", "motivation": "\u8ba1\u7b97\u6559\u80b2\u7814\u7a76\uff08CER\uff09\u5f80\u5f80\u7531\u4e00\u7ebf\u6559\u5b66\u5b9e\u8df5\u8005\u63a8\u52a8\uff0c\u65e8\u5728\u6539\u8fdb\u81ea\u8eab\u548c\u6574\u4e2a\u5b66\u79d1\u7684\u6559\u5b66\u65b9\u6cd5\uff0c\u4f46\u666e\u904d\u9762\u4e34\u7f3a\u4e4f\u540c\u4e8b\u3001\u8d44\u6e90\u6216\u80fd\u529b\uff0c\u96be\u4ee5\u5f00\u5c55\u5177\u5907\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u4e25\u8c28\u6027\u7684\u7814\u7a76\uff0c\u96be\u4ee5\u63a8\u52a8\u5b66\u79d1\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5185\u5bb9\u5206\u6790\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5bb9\u5206\u6790\u65b0\u65b9\u6cd5\uff0c\u5373LLM\u8f85\u52a9\u5185\u5bb9\u5206\u6790\uff08LACA\uff09\uff0c\u7528\u4e8e\u5904\u7406\u548c\u5206\u6790\u5927\u91cf\u6587\u672c\u6570\u636e\u3002\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6559\u80b2\u76f8\u5173\u7684\u6570\u636e\u96c6\uff0c\u6f14\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u91cd\u590d\u6027\u53ca\u4e25\u8c28\u6027\u3002", "result": "\u5c55\u793a\u4e86LACA\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5728\u4e0d\u663e\u8457\u589e\u52a0\u7814\u7a76\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u3001\u4e25\u8c28\u5730\u5904\u7406\u548c\u5206\u6790\u6d77\u91cf\u6587\u672c\u6570\u636e\uff0c\u4ece\u800c\u66f4\u597d\u5730\u63d0\u70bc\u548c\u63a8\u5e7f\u7814\u7a76\u6210\u679c\u3002", "conclusion": "LACA\u8fd9\u79cd\u65b9\u6cd5\u5728\u8ba1\u7b97\u6559\u80b2\u7814\u7a76\uff08CER\uff09\u4e2d\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\uff0c\u53ef\u63d0\u5347\u7814\u7a76\u7684\u666e\u904d\u6027\u548c\u8d28\u91cf\uff0c\u5bf9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6559\u5b66\u5b9e\u8df5\u4e0e\u7814\u7a76\u6c34\u5e73\u8fdb\u6b65\u5177\u6709\u79ef\u6781\u610f\u4e49\u3002"}}
{"id": "2508.18916", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18916", "abs": "https://arxiv.org/abs/2508.18916", "authors": ["Bojan Evkoski", "Igor Mozeti\u010d", "Nikola Ljube\u0161i\u0107", "Petra Kralj Novak"], "title": "Affective Polarization across European Parliaments", "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u52a8\u5316\u5206\u6790\u516d\u56fd\u8bae\u4f1a\u53d1\u8a00\uff0c\u53d1\u73b0\u6b27\u6d32\u8bae\u4f1a\u666e\u904d\u5b58\u5728\u654c\u610f\u5316\u3001\u6781\u5316\u4ea4\u6d41\uff0c\u4e0e\u8bae\u5458\u6d3b\u8dc3\u5ea6\u65e0\u5173\uff0c\u4e92\u60e0\u673a\u5236\u52a9\u63a8\u6781\u5316\u3002", "motivation": "\u60c5\u611f\u6781\u5316\uff0c\u5373\u5bf9\u53cd\u5bf9\u96c6\u56e2\u7684\u8d1f\u9762\u60c5\u7eea\u548c\u654c\u610f\u4e0a\u5347\uff0c\u5df2\u6210\u4e3a\u5168\u7403\u653f\u6cbb\u8bdd\u8bed\u7684\u91cd\u8981\u7279\u5f81\u3002\u8be5\u7814\u7a76\u65e8\u5728\u8003\u5bdf\u6b27\u6d32\u8bae\u4f1a\u4e2d\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u60c5\u611f\u6781\u5316\u73b0\u8c61\u3002", "method": "\u7814\u7a76\u5229\u7528\u516d\u4e2a\u6b27\u6d32\u56fd\u5bb6\u8bae\u4f1a\u7684\u5168\u9762\u53d1\u8a00\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u81ea\u52a8\u8bc4\u4f30\u8bae\u5458\u60c5\u611f\uff0c\u901a\u8fc7\u6bd4\u8f83\u8bae\u5458\u5bf9\u5bf9\u7acb\u96c6\u56e2\u6210\u5458\u4e0e\u672c\u96c6\u56e2\u6210\u5458\u7684\u8d1f\u9762\u8a00\u8f9e\uff0c\u5206\u6790\u60c5\u611f\u6781\u5316\u4e92\u52a8\u6a21\u5f0f\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u516d\u4e2a\u56fd\u5bb6\u8bae\u4f1a\u4e2d\u90fd\u5b58\u5728\u6301\u7eed\u7684\u60c5\u611f\u6781\u5316\u73b0\u8c61\u3002\u867d\u7136\u8bae\u5458\u7684\u6d3b\u8dc3\u5ea6\u4e0e\u8d1f\u9762\u60c5\u7eea\u5448\u76f8\u5173\uff0c\u4f46\u9ad8\u6d3b\u8dc3\u4e0e\u4f4e\u6d3b\u8dc3\u8bae\u5458\u5728\u60c5\u611f\u6781\u5316\u7a0b\u5ea6\u4e0a\u5e76\u65e0\u663e\u8457\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u4e92\u60e0\u673a\u5236\u5728\u8bae\u5458\u95f4\u60c5\u611f\u6781\u5316\u4e2d\u8d77\u5230\u4e86\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u6b27\u6d32\u591a\u6570\u8bae\u4f1a\u4e2d\u666e\u904d\u5b58\u5728\u60c5\u611f\u6781\u5316\uff0c\u4e14\u4e0e\u8bae\u5458\u6d3b\u8dc3\u5ea6\u5173\u7cfb\u4e0d\u5927\uff0c\u4e92\u60e0\u673a\u5236\u662f\u5f71\u54cd\u60c5\u611f\u6781\u5316\u7684\u91cd\u8981\u56e0\u7d20\u3002"}}
{"id": "2508.18929", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18929", "abs": "https://arxiv.org/abs/2508.18929", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u517c\u5177\u591a\u6837\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684RAG\u7cfb\u7edf\u8bc4\u6d4b\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u66f4\u5408\u89c4\u3001\u66f4\u5b89\u5168\u7684RAG\u8bc4\u6d4b\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u7684\u8bc4\u4ef7\u6807\u51c6\u5927\u591a\u5173\u6ce8\u6027\u80fd\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u7528\u4e8e\u8bc4\u4ef7\u7684\u6570\u636e\u96c6\u8bbe\u8ba1\u548c\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u5927\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u65f6\u9762\u4e34\u5982\u9690\u79c1\u4fdd\u62a4\u7b49\u73b0\u5b9e\u7ea6\u675f\u3002\u56e0\u6b64\u9700\u5f00\u53d1\u80fd\u517c\u987e\u591a\u6837\u6027\u548c\u9690\u79c1\u5408\u89c4\u6027\u7684\u65b0\u578bRAG\u7cfb\u7edf\u8bc4\u4ef7\u6570\u636e\u96c6\u751f\u6210\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u591a\u6837\u6027\u667a\u80fd\u4f53\u5229\u7528\u805a\u7c7b\u6280\u672f\u63d0\u5347\u4e3b\u9898\u8986\u76d6\u548c\u8bed\u4e49\u53d8\u5316\uff0c2\uff09\u9690\u79c1\u667a\u80fd\u4f53\u8de8\u9886\u57df\u68c0\u6d4b\u5e76\u5c4f\u853d\u654f\u611f\u4fe1\u606f\uff0c3\uff09\u95ee\u7b54\u7b56\u5212\u667a\u80fd\u4f53\u7ed3\u5408\u524d\u4e24\u8005\u751f\u6210\u9002\u5408RAG\u8bc4\u4ef7\u7684\u9ad8\u8d28\u91cf\u95ee\u7b54\u96c6\u3002\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u96c6\u5728\u8bed\u4e49\u591a\u6837\u6027\u548c\u9690\u79c1\u5c4f\u853d\u6548\u679c\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9886\u57df\u4e13\u5c5e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u5408\u6210\u95ee\u7b54\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u8bed\u4e49\u591a\u6837\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u66f4\u5b89\u5168\u3001\u5408\u89c4\u7684RAG\u7cfb\u7edf\u8bc4\u4ef7\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.18988", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18988", "abs": "https://arxiv.org/abs/2508.18988", "authors": ["Hung Ming Liu"], "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u539f\u751f\u690d\u5165\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u7684AI Mother Tongue\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u540c\u65f6\u5177\u5907\u9ad8\u51c6\u786e\u7387\u4e0e\u53ef\u8ffd\u8e2a\u89e3\u91ca\u6027\uff0c\u6709\u671b\u7edf\u4e00\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u7684\u76f4\u89c9\u63a8\u7406\u4e0e\u7b26\u53f7\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ed1\u76d2\u6027\u8d28\u5bfc\u81f4\u96be\u4ee5\u7406\u89e3\u5176\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u91ca\u6027\u8f83\u5f31\u3002\u56e0\u6b64\uff0c\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u8be5\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5728\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u690d\u5165\u7b26\u53f7\u5316\u53ef\u89e3\u91ca\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdAI\u6bcd\u8bed(Symbolic Mother Tongue)\u7684\u6846\u67b6\uff0c\u5c06\u7b26\u53f7\u8bed\u8a00\u539f\u751f\u5185\u5d4c\u5230\u795e\u7ecf\u6a21\u578b\u7684\u8868\u793a\u4e2d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b26\u53f7\u7684\u65b9\u5f0f\u8ffd\u8e2a\u63a8\u7406\u8def\u5f84\u3001\u5229\u7528\u95e8\u63a7\u5f52\u7eb3\u673a\u5236\u8fdb\u884c\u9009\u62e9\u6027\u5173\u6ce8\uff0c\u5e76\u7ed3\u5408\u7b26\u53f7\u7eaf\u5ea6\u548c\u51b3\u7b56\u7a00\u758f\u6027\u7b49\u4e92\u8865\u8bad\u7ec3\u76ee\u6807\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5206\u4e3a\u4e24\u9636\u6bb5\uff0c\u5148\u83b7\u5f97\u5e7f\u6cdb\u7b26\u53f7\u80fd\u529b\u540e\u9010\u6b65\u7ec6\u5316\u4e3a\u76f4\u89c9\u5224\u65ad\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2aAI\u4efb\u52a1\u4e0a\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u517c\u5177\u7ade\u4e89\u6027\u51c6\u786e\u7387\u548c\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u663e\u793a\u51fa\u5728\u795e\u7ecf\u6a21\u578b\u4e2d\u7edf\u4e00\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3001\u76f4\u89c9\u6027\u548c\u7b26\u53f7\u63a8\u7406\u7684\u6f5c\u529b\u3002", "conclusion": "AI Mother Tongue\u6846\u67b6\u4e3a\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u6210\u4e3a\u65b0\u4e00\u4ee3\u53ef\u89e3\u91ca\u795e\u7ecf\u6a21\u578b\u7684\u6838\u5fc3\u673a\u5236\u3002"}}
{"id": "2508.18992", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18992", "abs": "https://arxiv.org/abs/2508.18992", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "Automatic Prompt Optimization with Prompt Distillation", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.", "AI": {"tldr": "DistillPrompt\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u63d0\u793a\u8bcd\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u7684\u6548\u679c\uff0c\u662f\u76ee\u524d\u6700\u4f18\u7684\u975e\u68af\u5ea6\u65b9\u6cd5\u4e4b\u4e00\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548cprompt\u5de5\u7a0b\u7684\u8fc5\u901f\u53d1\u5c55\uff0c\u81ea\u52a8\u5316\u6311\u9009\u6700\u4f18\u63d0\u793a\u8bcd\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u4e0d\u5145\u5206\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faDistillPrompt\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u84b8\u998f\u3001\u538b\u7f29\u548c\u805a\u5408\u64cd\u4f5c\u5206\u591a\u9636\u6bb5\u5c06\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u878d\u5408\u8fdb\u63d0\u793a\u8bcd\uff0c\u5e76\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5728\u4e0d\u540c\u6587\u672c\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u7684\u6d4b\u8bd5\u4e2d\uff0cDistillPrompt\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982Grips\uff09\u5728\u5173\u952e\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u5347\u4e8620.12%\u3002", "conclusion": "DistillPrompt\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u9009\u62e9\u4f18\u5316\u7684\u5927\u6a21\u578b\u63d0\u793a\u8bcd\uff08autoprompting\uff09\u9886\u57df\u8868\u73b0\u5353\u8d8a\uff0c\u662f\u6700\u6709\u6548\u7684\u975e\u68af\u5ea6\u65b9\u6cd5\u4e4b\u4e00\u3002"}}
{"id": "2508.19026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MovieCORE\uff0c\u4e00\u4e2a\u5177\u6709\u66f4\u9ad8\u8ba4\u77e5\u8981\u6c42\u7684\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u95ee\u9898\uff0c\u5f00\u53d1\u8ba4\u77e5\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u5e76\u5f15\u5165\u63a8\u7406\u589e\u5f3a\u6a21\u5757ACE\uff0c\u6709\u6548\u63d0\u5347\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5bf9\u7535\u5f71\u6df1\u5c42\u6b21\u5185\u5bb9\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\u96c6\u901a\u5e38\u53ea\u6d89\u53ca\u8868\u5c42\u7406\u89e3\uff0c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u5bf9\u7535\u5f71\u5185\u5bb9\u7684\u6df1\u5c42\u6b21\u8ba4\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u63a8\u52a8\u89c6\u9891\u7406\u89e3\u5411\u66f4\u9ad8\u9636\u8ba4\u77e5\u4efb\u52a1\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86MovieCORE\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u521b\u65b0\u7684agentic brainstorming\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u534f\u540c\u751f\u6210\u548c\u4f18\u5316\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u8ba4\u77e5\u6d4b\u8bd5\u4f53\u7cfb\uff0c\u7528\u4e8e\u8bc4\u4f30\u95ee\u9898\u7684\u6df1\u5ea6\u3001\u601d\u7ef4\u6fc0\u53d1\u6027\u548c\u53e5\u6cd5\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6df1\u5c42\u8ba4\u77e5\u4efb\u52a1\u7684VQA\u6a21\u578b\u7efc\u5408\u8bc4\u4f30\u65b9\u6848\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86Agentic Choice Enhancement\uff08ACE\uff09\u6a21\u5757\uff0c\u4f5c\u4e3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u540e\u8bad\u7ec3\u589e\u5f3a\u3002", "result": "MovieCORE\u6570\u636e\u96c6\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u6d4b\u548c\u6fc0\u53d1\u6a21\u578b\u5728\u7535\u5f71\u5185\u5bb9\u7406\u89e3\u4e0a\u7684\u6df1\u5c42\u63a8\u7406\u80fd\u529b\u3002ACE\u6a21\u5757\u4f7f\u73b0\u6709\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u63d0\u5347\u6700\u9ad8\u53ef\u8fbe25%\u3002", "conclusion": "MovieCORE\u63a8\u52a8\u4e86AI\u5bf9\u7535\u5f71\u5185\u5bb9\u7684\u6df1\u5c42\u7406\u89e3\uff0c\u4e3a\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u7684\u80fd\u529b\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u8ba4\u77e5\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u3002"}}
{"id": "2508.19076", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19076", "abs": "https://arxiv.org/abs/2508.19076", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.", "AI": {"tldr": "HiPlan\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u548c\u5b9e\u65f6\u6307\u5bfc\uff0c\u7ed3\u6784\u5316\u590d\u7528\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u4f7f\u5f97LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u3001\u957f\u5468\u671f\u4efb\u52a1\u4e0a\u7684\u51b3\u7b56\u80fd\u529b\u5927\u5e45\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u5e94\u5bf9\u590d\u6742\u3001\u957f\u5468\u671f\u7684\u89c4\u5212\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5176\u95ee\u9898\u4e3b\u8981\u5728\u4e8e\u7f3a\u4e4f\u5b8f\u89c2\u6307\u5bfc\uff0c\u5bfc\u81f4\u4efb\u52a1\u8ff7\u5931\u548c\u5931\u8d25\uff0c\u540c\u65f6\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u76d1\u7763\u4e0d\u8db3\uff0c\u65e0\u6cd5\u53ca\u65f6\u54cd\u5e94\u73af\u5883\u53d8\u5316\u3001\u5bb9\u6613\u504f\u79bb\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86HiPlan\uff0c\u4e00\u4e2a\u5206\u5c42\u89c4\u5212\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u91cc\u7a0b\u7891\u5f0f\u7684\u884c\u52a8\u6307\u5bfc\uff08\u5b8f\u89c2\u65b9\u5411\uff09\u548c\u9010\u6b65\u63d0\u793a\uff08\u5fae\u89c2\u884c\u52a8\uff09\uff0c\u5305\u62ec\uff1a1\uff09\u79bb\u7ebf\u9636\u6bb5\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u6784\u5efa\u91cc\u7a0b\u7891\u5e93\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u7ecf\u9a8c\u590d\u7528\uff1b2\uff09\u6267\u884c\u9636\u6bb5\uff0c\u7ed3\u5408\u8fc7\u53bb\u91cc\u7a0b\u7891\u7684\u8f68\u8ff9\uff0c\u6839\u636e\u5f53\u524d\u89c2\u5bdf\u52a8\u6001\u751f\u6210\u4e0e\u91cc\u7a0b\u7891\u76ee\u6807\u4e00\u81f4\u7684\u9010\u6b65\u63d0\u793a\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHiPlan\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u5df2\u6709\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u5206\u5c42\u7ec4\u4ef6\u5404\u81ea\u7684\u4e92\u8865\u6027\u3002", "conclusion": "HiPlan\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u76ee\u6807\u8ff7\u5931\u548c\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u7684\u89c4\u5212\u548c\u63d0\u793a\u673a\u5236\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u51b3\u7b56\u8868\u73b0\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19077", "abs": "https://arxiv.org/abs/2508.19077", "authors": ["Tom R\u00f6hr", "Soumyadeep Roy", "Fares Al Mohamad", "Jens-Michalis Papaioannou", "Wolfgang Nejdl", "Felix Gers", "Alexander L\u00f6ser"], "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues", "comment": "Accepted at ECAI 2025", "summary": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u533b\u751f\u610f\u56fe\u7cbe\u7ec6\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u6807\u6ce8\u5927\u91cf\u533b\u60a3\u5bf9\u8bdd\uff0c\u7528\u4ee5\u8bc4\u6d4b\u533b\u7597\u610f\u56fe\u5206\u7c7b\u6a21\u578b\u8868\u73b0\uff0c\u63ed\u793a\u5bf9\u8bdd\u7ed3\u6784\u8f6c\u53d8\u53ca\u5176\u6a21\u5f0f\uff0c\u5e76\u8bc1\u5b9e\u610f\u56fe\u7279\u5f81\u6709\u52a9\u4e8e\u5bf9\u8bdd\u6458\u8981\u6027\u80fd\u63d0\u5347\u3002\u6570\u636e\u4e0e\u4ee3\u7801\u5168\u90e8\u516c\u5f00\u3002", "motivation": "\u5c3d\u7ba1\u533b\u751f\u5728\u5bf9\u8bdd\u4e2d\u6709\u660e\u786e\u7684\u8bca\u65ad\u53ca\u6cbb\u7597\u5bfc\u5411\uff0c\u73b0\u6709\u7814\u7a76\u9c9c\u6709\u5173\u6ce8\u533b\u751f\u610f\u56fe\u968f\u5bf9\u8bdd\u53d1\u5c55\u800c\u53d8\u5316\u7684\u8f68\u8ff9\uff0c\u7f3a\u5c11\u7ec6\u7c92\u5ea6\u6570\u636e\u548c\u5206\u6790\u65b9\u6cd5\u5236\u7ea6\u4e86\u81ea\u52a8\u5316\u533b\u5b66\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u4ee5Aci-bench\u6570\u636e\u96c6\u4e3a\u57fa\u7840\uff0c\u4e0e\u533b\u5b66\u4e13\u5bb6\u5408\u4f5c\u57fa\u4e8eSOAP\u6846\u67b6\u63d0\u51fa\u533b\u751f\u610f\u56fe\u5206\u7c7b\u4f53\u7cfb\uff0c\u5bf95000\u4f59\u8f6e\u533b\u60a3\u5bf9\u8bdd\u8fdb\u884c\u5927\u89c4\u6a21\u6807\u6ce8\uff0c\u5e76\u7528\u4ee5\u8bc4\u6d4b\u751f\u6210\u5f0f\u548c\u7f16\u7801\u5f0f\u6a21\u578b\u5728\u533b\u7597\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5bf9\u6574\u4f53\u533b\u7597\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5bf9SOAP\u610f\u56fe\u7c7b\u522b\u8f6c\u6362\u7684\u8bc6\u522b\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u9996\u6b21\u63ed\u793a\u4e86\u533b\u7597\u5bf9\u8bdd\u4e2d\u5e38\u89c1\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u4e3a\u81ea\u52a8\u5316\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u533b\u751f\u610f\u56fe\u8fc7\u6ee4\u7528\u4e8e\u533b\u7597\u5bf9\u8bdd\u6458\u8981\u4efb\u52a1\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86\u533b\u60a3\u5bf9\u8bdd\u4e2d\u533b\u751f\u610f\u56fe\u7684\u52a8\u6001\u8f68\u8ff9\uff0c\u5bf9\u533b\u751f\u610f\u56fe\u7684\u7cbe\u7ec6\u5316\u5206\u7c7b\u80fd\u591f\u4fc3\u8fdb\u7b97\u6cd5\u63ed\u793a\u5bf9\u8bdd\u7ed3\u6784\u5e76\u63d0\u5347\u533b\u7597\u6458\u8981\u6027\u80fd\u3002"}}
{"id": "2508.19089", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19089", "abs": "https://arxiv.org/abs/2508.19089", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs", "comment": "Accepted by EMNLP 2025", "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.", "AI": {"tldr": "\u5927\u6a21\u578b\u5bf9\u6781\u4f4e\u8d44\u6e90\u3001\u5c24\u5176\u4e66\u5199\u4f53\u7cfb\u7f55\u89c1\u7684\u8bed\u8a00\u652f\u6301\u8584\u5f31\u3002\u5bf9\u6bd4ICL\u548cPEFT\u4e24\u5927\u7c7b\u9002\u914d\u65b9\u5f0f\uff0c\u53d1\u73b0\u5728\u6781\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u52a0\u5165\u8bed\u8a00\u5bf9\u9f50\u4fe1\u53f7\u7684\u96f6\u6837\u672cICL\u6548\u679c\u6700\u597d\uff0c\u800cPEFT\u548c\u5c11\u6837\u672cICL\u66f4\u9002\u5408\u8f83\u5e38\u89c1\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u5efa\u8bae\u9488\u5bf9\u76ee\u6807\u8bed\u8a00\u7c7b\u578b\u9009\u62e9\u5408\u9002\u9002\u914d\u7b56\u7565\uff0c\u907f\u514d\u7528PEFT\u5904\u7406\u672a\u89c1\u4e66\u5199\u4f53\u7cfb\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5c24\u5176\u662f\u7a00\u6709\u4e66\u5199\u4f53\u7cfb\uff09\u652f\u6301\u6781\u5ea6\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u3002\u7814\u7a76\u5e0c\u671b\u63ed\u79d8ICL\u548cPEFT\u7b49\u65b9\u6848\u5728\u8fd9\u7c7b\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u53ca\u5e94\u7528\u8fb9\u754c\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u5728\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u591a\u8bed\u79cd\u5927\u6a21\u578b\u4e0a\uff0c\u5bf920\u79cd\u4ee3\u8868\u6027\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u6db5\u76d6\u7eafICL\uff08\u6709\u65e0\u8f85\u52a9\u5bf9\u9f50\u4fe1\u53f7\uff09\u3001PEFT\u4ee5\u53ca\u591a\u79cd\u96f6\u6837\u672c/\u5c11\u6837\u672c\u914d\u7f6e\u3002", "result": "PEFT\u5728\u6781\u4f4e\u8d44\u6e90\u4e14\u4e66\u5199\u4f53\u7cfb\u7f55\u89c1\u65f6\u6548\u679c\u53d7\u9650\uff1b\u5e26\u8bed\u8a00\u5bf9\u9f50\u4fe1\u53f7\u7684\u96f6\u6837\u672cICL\u5728\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u5c24\u5176\u6709\u6548\uff1b\u8f83\u9ad8\u4ee3\u8868\u6027\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u5219\u66f4\u9002\u5408\u91c7\u7528\u5c11\u6837\u672cICL\u6216PEFT\u3002", "conclusion": "\u5bf9\u4e8e\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5c24\u5176\u662f\u7f55\u89c1\u4e66\u5199\u4f53\u7cfb\u7684\u8bed\u8a00\uff0c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u8868\u73b0\u6709\u9650\uff0c\u800c\u57fa\u4e8e\u8bed\u8a00\u5bf9\u9f50\u7684\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6548\u679c\u66f4\u663e\u8457\u3002\u5bf9\u4e8e\u7a0d\u5fae\u66f4\u6709\u4ee3\u8868\u6027\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5c11\u6837\u672cICL\u6216PEFT\u6536\u76ca\u66f4\u660e\u663e\u3002\u9488\u5bf9\u8fd9\u7c7b\u8bed\u8a00\u7684\u5b9e\u8df5\u5efa\u8bae\u5305\u62ec\uff1a\u907f\u514d\u5c06\u591a\u8bed\u79cd\u6a21\u578b\u7528\u4e8e\u672a\u89c1\u4e66\u5199\u4f53\u7cfb\u7684\u5fae\u8c03\u3002"}}
{"id": "2508.19093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19093", "abs": "https://arxiv.org/abs/2508.19093", "authors": ["Mathew Henrickson"], "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index", "comment": null, "summary": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faRAG\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u788e\u7247\u5316\u3001\u591a\u8bed\u79cd\u827a\u672f\u54c1\u6eaf\u6e90\u6570\u636e\u9ad8\u6548\u68c0\u7d22\u7684\u96be\u9898\uff0c\u5bf9\u6863\u6848\u5927\u89c4\u6a21\u68c0\u7d22\u548c\u5206\u6790\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u827a\u672f\u54c1\u6eaf\u6e90\u7814\u7a76\u5bf9\u4e8e\u9a8c\u8bc1\u5176\u771f\u5b9e\u6027\u3001\u6cd5\u5f8b\u7ef4\u6743\u548c\u7406\u89e3\u5386\u53f2\u6587\u5316\u80cc\u666f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7531\u4e8e\u6863\u6848\u4fe1\u606f\u788e\u7247\u5316\u4e14\u591a\u8bed\u79cd\uff0c\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u4e14\u73b0\u6709\u5e73\u53f0\u5bf9\u5143\u6570\u636e\u8981\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u6027\u68c0\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u91c7\u7528\u8bed\u4e49\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u6458\u8981\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4e0e\u591a\u8bed\u79cd\u68c0\u7d22\uff0c\u964d\u4f4e\u5bf9\u4e25\u683c\u5143\u6570\u636e\u7ed3\u6784\u7684\u4f9d\u8d56\u3002", "result": "\u4f7f\u7528Getty Provenance Index\uff08\u5fb7\u56fd\u9500\u552e\u8bb0\u5f55\uff09\u4e2d\u76841\u4e07\u6761\u62cd\u5356\u6570\u636e\u6837\u672c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6848\u80fd\u9ad8\u6548\u68c0\u7d22\u548c\u603b\u7ed3\u62cd\u5356\u8bb0\u5f55\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6570\u636e\u5bfc\u822a\u3002", "conclusion": "RAG\u6846\u67b6\u4e3a\u827a\u672f\u54c1\u5e02\u573a\u6863\u6848\u7684\u68c0\u7d22\u548c\u6458\u8981\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u5386\u53f2\u5b66\u8005\u548c\u6587\u5316\u9057\u4ea7\u4e13\u4e1a\u4eba\u58eb\u7684\u654f\u611f\u6027\u7814\u7a76\u3002"}}
{"id": "2508.19099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19099", "abs": "https://arxiv.org/abs/2508.19099", "authors": ["Thomas Compton"], "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic", "comment": "5 pages conference paper, 4 tables", "summary": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u7ed3\u5408\u8bcd\u6c47\u4e0e\u8bed\u4e49\u7684\u65b0\u578b\u900f\u660eQDA\u65b9\u6cd5\uff0c\u514b\u670d\u73b0\u6709\u9ed1\u76d2\u5de5\u5177\u7684\u4e0d\u8db3\u3002\u65b9\u6cd5\u901a\u8fc7\u5f00\u6e90Python\u5de5\u5177\u5b9e\u73b0\uff0c\u63d0\u5347\u4e86\u7814\u7a76\u53ef\u63a7\u6027\u3001\u590d\u73b0\u6027\u4e0e\u591a\u5c42\u89e3\u6790\u80fd\u529b\uff0c\u4e3a\u8bdd\u8bed\u5206\u6790\u63d0\u4f9b\u5e7f\u6cdb\u9002\u7528\u4e14\u900f\u660e\u7684\u8303\u5f0f\u3002", "motivation": "\u91cf\u5316\u8bdd\u8bed\u5206\u6790\uff08QDA\uff09\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8ba1\u7b97\u5de5\u5177\uff0c\u4f46\u8fc7\u4e8e\u4f9d\u8d56\u9ed1\u76d2\u8f6f\u4ef6\u5982MAXQDA\u548cNVivo\uff0c\u53ef\u80fd\u635f\u5bb3\u65b9\u6cd5\u900f\u660e\u6027\u548c\u7814\u7a76\u76ee\u6807\u7684\u4e00\u81f4\u6027\u3002\u4f5c\u8005\u56e0\u6b64\u5e0c\u671b\u63d0\u51fa\u66f4\u900f\u660e\u4e14\u80fd\u4e0e\u7814\u7a76\u76ee\u6807\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u8bcd\u6c47\u4e0e\u8bed\u4e49\u65b9\u6cd5\u7684\u6df7\u5408\u6846\u67b6\u3002\u65b9\u6cd5\u5305\u62ec\u81ea\u5b9a\u4e49\u7684Python\u6d41\u7a0b\uff0c\u5229\u7528NLTK\u3001spaCy\u548cSentence Transformers\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u3001\u8bcd\u5f62\u8fd8\u539f\u548c\u5d4c\u5165\u751f\u6210\u3002\u4f7f\u7528BERTopic\u5efa\u6a21\uff0c\u7ed3\u5408UMAP\u964d\u7ef4\u3001HDBSCAN\u805a\u7c7b\u548cc-TF-IDF\u5173\u952e\u8bcd\u63d0\u53d6\uff0c\u8fdb\u884c\u4e86\u53c2\u6570\u4f18\u5316\u53ca\u591a\u6b21\u8fd0\u884c\u4ee5\u63d0\u5347\u4e3b\u9898\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u8bcd\u6c47\u7cbe\u786e\u641c\u7d22\u4e0e\u8bed\u4e49\u805a\u7c7b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u5c42\u6b21\u5206\u6790\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\uff0c\u80fd\u591f\u5b9e\u73b0\u5bf9\u5386\u53f2\u653f\u6cbb\u8bdd\u8bed\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3001\u9ad8\u65b9\u6cd5\u900f\u660e\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002\u591a\u5c42\u6b21\u7684\u5206\u6790\u65b9\u5f0f\u63d0\u5347\u4e86\u8bdd\u8bed\u4e3b\u9898\u5206\u6790\u7684\u7a33\u5065\u6027\u4e0e\u89e3\u91ca\u6027\uff0c\u5176\u900f\u660e\u7684\u4ee3\u7801\u548c\u6750\u6599\u4e5f\u516c\u5f00\u5206\u4eab\uff0c\u652f\u6301\u4ed6\u4eba\u590d\u73b0\u4e0e\u5e94\u7528\u3002", "conclusion": "\u81ea\u5b9a\u4e49\u7684\u3001\u900f\u660e\u7684QDA\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u9ed1\u76d2\u8f6f\u4ef6\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u65b9\u6cd5\u4e09\u89d2\u9a8c\u8bc1\u63d0\u5347\u7814\u7a76\u53ef\u9760\u6027\uff0c\u5f3a\u8c03\u7814\u7a76\u8005\u4e3b\u5bfc\u6743\u548c\u4ee3\u7801\u5c42\u900f\u660e\u6027\u3002\u4e3a\u8ba1\u7b97\u8bdd\u8bed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u548c\u89e3\u91ca\u6027\u5f3a\u7684\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2508.19111", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19111", "abs": "https://arxiv.org/abs/2508.19111", "authors": ["Zhikai Ding", "Shiyu Ni", "Keping Bi"], "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs", "comment": "EMNLP2025 Findings", "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u7f6e\u4fe1\u5ea6\u8868\u73b0\u5c1a\u53ef\u4f46\u9700\u6539\u8fdb\u3002\u63d0\u51fa\u7684\u65b0\u6821\u51c6\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u81ea\u77e5\u80fd\u529b\uff0c\u8868\u73b0\u4f18\u4e8e\u4ec5\u7528\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u867d\u7136\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u89c6\u89c9\u95ee\u7b54\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u73b0\u8c61\u3002\u4e00\u4e2a\u53ef\u9760\u7684\u6a21\u578b\u5e94\u8be5\u77e5\u9053\u81ea\u8eab\u77e5\u8bc6\u7684\u8fb9\u754c\uff0c\u5373\u201c\u77e5\u4e4b\u4e3a\u77e5\u4e4b\uff0c\u4e0d\u77e5\u4e3a\u4e0d\u77e5\u201d\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76LVLM\u6a21\u578b\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u7684\u80fd\u529b\u3002", "method": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd\u4fe1\u5fc3\u4fe1\u53f7\uff1a\u6982\u7387\u578b\u4fe1\u5fc3\u3001\u7b54\u6848\u4e00\u81f4\u6027\u578b\u4fe1\u5fc3\u548c\u53e3\u5934\u5316\u4fe1\u5fc3\uff0c\u5e76\u5728\u4e09\u4e2aLVLM\u6a21\u578b\u548c\u4e09\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u8fd8\u5f15\u5165\u4e86\u51e0\u79cd\u6765\u81ea\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0LVLMs\u5177\u5907\u4e00\u5b9a\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u63d0\u5347\u7a7a\u95f4\u4ecd\u5927\u3002\u5176\u4e2d\u6982\u7387\u548c\u4e00\u81f4\u6027\u578b\u4fe1\u53f7\u662f\u66f4\u53ef\u9760\u7684\u4fe1\u5fc3\u6307\u6807\uff0c\u800c\u53e3\u5934\u5316\u4fe1\u5fc3\u6613\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u3002\u5c06LLM\u76f8\u5173\u6821\u51c6\u65b9\u6cd5\u7528\u4e8eLVLM\u80fd\u63d0\u5347\u611f\u77e5\u8868\u73b0\u3002\u89c6\u89c9\u4e0e\u6587\u672c\u8f93\u5165\u7684\u8054\u5408\u5904\u7406\u867d\u7136\u7565\u964d\u95ee\u7b54\u8868\u73b0\uff0c\u4f46\u63d0\u9ad8\u4e86\u6a21\u578b\u81ea\u4fe1\u5224\u65ad\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LVLM\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u6709\u521d\u6b65\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u4fe1\u5fc3\u4fe1\u53f7\u9700\u6539\u8fdb\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u63d0\u5347\u5176\u77e5\u4e4b\u4e3a\u77e5\u4e4b\u80fd\u529b\u3002\u8054\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u53ef\u589e\u52a0\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.19202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19202", "abs": "https://arxiv.org/abs/2508.19202", "authors": ["Alan Li", "Yixin Liu", "Arpan Sarkar", "Doug Downey", "Arman Cohan"], "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "comment": "28 pages, 16 figures", "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u79d1\u5b66\u63a8\u7406\u8bc4\u6d4b\u5957\u4ef6SciReas\u4e0e\u5206\u6790\u6846\u67b6KRUX\uff0c\u7cfb\u7edf\u5206\u6790LLM\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u77e5\u8bc6\u4e0e\u63a8\u7406\u89d2\u8272\uff0c\u53d1\u73b0\u77e5\u8bc6\u68c0\u7d22\u662f\u74f6\u9888\uff0c\u5916\u90e8\u77e5\u8bc6\u4e0e\u63d0\u5347\u8bed\u8a00\u5316\u63a8\u7406\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u53d1\u5e03\u6709\u7ade\u4e89\u529b\u7684\u57fa\u7ebf\u6a21\u578bSciLit01\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u65e2\u9700\u8981\u6df1\u539a\u9886\u57df\u77e5\u8bc6\uff0c\u4e5f\u8981\u5177\u5907\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u800c\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u8fc7\u4e8e\u96f6\u6563\uff0c\u96be\u4ee5\u7cfb\u7edf\u5206\u6790\u77e5\u8bc6\u4e0e\u63a8\u7406\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "method": "\u63d0\u51faSciReas\uff0c\u4e00\u4e2a\u7efc\u5408\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u6027\u8bc4\u6d4b\u5957\u4ef6\uff0c\u4ee5\u53ca\u66f4\u6ce8\u91cd\u590d\u6742\u63a8\u7406\u7684\u5b50\u96c6SciReas-Pro\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1KRUX\u6846\u67b6\uff0c\u7528\u4ee5\u5256\u6790\u77e5\u8bc6\u4e0e\u63a8\u7406\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u7ed3\u5408\u4e24\u8005\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002\u6700\u540e\uff0c\u5c06\u6570\u636e\u96c6\u4e0e\u5176\u4ed6\u6700\u65b0\u65b9\u6cd5\u5982\u957f\u94fe\u5f0f\u63a8\u7406SFT\u8fdb\u884c\u5bf9\u6bd4\uff0c\u63a8\u51fa\u5f3a\u529b\u57fa\u7ebf\u6a21\u578bSciLit01\u3002", "result": "\u53d1\u73b0\uff1a\uff081\uff09\u6a21\u578b\u53c2\u6570\u4e2d\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\u662f\u79d1\u5b66\u63a8\u7406\u7684\u74f6\u9888\uff1b\uff082\uff09\u5728\u4e0a\u4e0b\u6587\u4e2d\u52a0\u5165\u5916\u90e8\u77e5\u8bc6\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u7406\u6a21\u578b\u8868\u73b0\uff1b\uff083\uff09\u63d0\u5347\u8bed\u8a00\u5316\u63a8\u7406\u6709\u52a9\u4e8e\u6a21\u578b\u83b7\u53d6\u4efb\u52a1\u76f8\u5173\u77e5\u8bc6\u3002\u5e76\u53d1\u5e03\u4e86SciLit01\u4f5c\u4e3a\u79d1\u5b66\u63a8\u7406\u9886\u57df\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5b8c\u5584\u800c\u7cfb\u7edf\u5316\u7684\u79d1\u5b66\u63a8\u7406\u8bc4\u6d4b\u4e0e\u5206\u6790\u6846\u67b6\u6709\u52a9\u4e8e\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u79d1\u5b66\u63a8\u7406\u7684\u74f6\u9888\u548c\u63d0\u5347\u8def\u5f84\uff0c\u672a\u6765\u53d1\u5c55\u5e94\u8fdb\u4e00\u6b65\u8003\u8651\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u534f\u540c\u4f18\u5316\u3002"}}
{"id": "2508.19205", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19205", "abs": "https://arxiv.org/abs/2508.19205", "authors": ["Zhiliang Peng", "Jianwei Yu", "Wenhui Wang", "Yaoyao Chang", "Yutao Sun", "Li Dong", "Yi Zhu", "Weijiang Xu", "Hangbo Bao", "Zehua Wang", "Shaohan Huang", "Yan Xia", "Furu Wei"], "title": "VibeVoice Technical Report", "comment": null, "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.", "AI": {"tldr": "VibeVoice\u901a\u8fc7\u65b0\u578b\u5206\u8bcd\u5668\u548cnext-token diffusion\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u5730\u751f\u6210\u591a\u89d2\u8272\u3001\u957f\u65f6\u95f4\u7684\u8bed\u97f3\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5408\u6210\u6a21\u578b\u5728\u5904\u7406\u591a\u8bf4\u8bdd\u4eba\u548c\u957f\u65f6\u5e8f\u8bed\u97f3\u65f6\u53d7\u9650\u4e8e\u538b\u7f29\u7387\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5f71\u54cd\u6548\u7387\u548c\u97f3\u8d28\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u65b9\u6cd5\u63d0\u5347\u957f\u6587\u672c\u3001\u591a\u89d2\u8272\u8bed\u97f3\u5408\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528next-token diffusion\u8fdb\u884c\u81ea\u56de\u5f52\u6f5c\u5728\u5411\u91cf\u751f\u6210\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u6bd4Encodec\u6a21\u578b\u63d0\u5347\u4e8680\u500d\u6570\u636e\u538b\u7f29\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u97f3\u9891\u8d28\u91cf\u3002", "result": "VibeVoice\u53ef\u5728\u6700\u957f64K\u7a97\u53e3\uff08\u7ea690\u5206\u949f\uff09\u4e0e\u6700\u591a\u56db\u540d\u8bf4\u8bdd\u4eba\u6761\u4ef6\u4e0b\u751f\u6210\u771f\u5b9e\u5bf9\u8bdd\u8bed\u5883\u7684\u9ad8\u8d28\u91cf\u957f\u7bc7\u8bed\u97f3\uff0c\u97f3\u9891\u4fdd\u771f\u4e14\u8ba1\u7b97\u6548\u7387\u4f18\u5f02\uff0c\u4f18\u4e8e\u76ee\u524d\u5f00\u6e90/\u5546\u7528\u5bf9\u8bdd\u6a21\u578b\u3002", "conclusion": "VibeVoice\u80fd\u9ad8\u6548\u5408\u6210\u591a\u4f4d\u8bf4\u8bdd\u8005\u7684\u957f\u7bc7\u8bed\u97f3\uff0c\u5e76\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u97f3\u8d28\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6570\u636e\u538b\u7f29\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u548c\u5546\u7528\u5bf9\u8bdd\u6a21\u578b\u3002"}}
{"id": "2508.19221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19221", "abs": "https://arxiv.org/abs/2508.19221", "authors": ["Isabel Cachola", "Daniel Khashabi", "Mark Dredze"], "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?", "comment": null, "summary": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.", "AI": {"tldr": "\u8be5\u6587\u53d1\u73b0\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\u96be\u4ee5\u51c6\u786e\u53cd\u6620PLS\u6458\u8981\u7684\u6613\u8bfb\u6027\uff0c\u8bed\u8a00\u6a21\u578b\u8bc4\u4ef7\u7ed3\u679c\u4e0e\u4eba\u7c7b\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u5efa\u8bae\u6539\u7528\u8bed\u8a00\u6a21\u578b\u8861\u91cf\u6458\u8981\u53ef\u8bfb\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u3002", "motivation": "\u9274\u4e8e\u5f53\u524dPLS\u9886\u57df\u666e\u904d\u91c7\u7528\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\u8fdb\u884c\u6458\u8981\u8bc4\u4ef7\uff0c\u4f46\u5c1a\u672a\u6709\u7814\u7a76\u6bd4\u8f83\u8fd9\u4e9b\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63a2\u7d22\u66f4\u4f18\u7684\u8bc4\u4ef7\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u8bc4\u4f30\u4e868\u79cd\u53ef\u8bfb\u6027\u6307\u6807\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u7ed3\u679c\u4e0e\u4eba\u5de5\u53ef\u8bfb\u6027\u5224\u65ad\u8fdb\u884c\u5bf9\u6bd4\uff0c\u540c\u65f6\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u5728PLS\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8003\u5bdf\u5176\u662f\u5426\u80fd\u66f4\u597d\u5730\u8861\u91cf\u6df1\u5c42\u6b21\u7684\u53ef\u8bfb\u6027\u56e0\u7d20\uff08\u5982\u80cc\u666f\u77e5\u8bc6\u9700\u6c42\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u6570\u4f20\u7edf\u6307\u6807\u4e0e\u4eba\u5de5\u53ef\u8bfb\u6027\u5224\u65ad\u7684\u76f8\u5173\u6027\u5f88\u4f4e\uff0c\u800c\u8868\u73b0\u6700\u4f73\u7684\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u4e3a0.56\uff0c\u8bf4\u660e\u540e\u8005\u5728\u6355\u6349\u53ef\u8bfb\u6027\uff08\u5305\u62ec\u6df1\u5c42\u56e0\u7d20\uff09\u4e0a\u66f4\u52a0\u51c6\u786e\uff0c\u4e5f\u80fd\u5f15\u51fa\u4e0d\u540c\u4e8e\u4f20\u7edf\u6307\u6807\u7684\u7ed3\u8bba\u3002", "conclusion": "\u4f5c\u8005\u53d1\u73b0\uff0c\u4f20\u7edf\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807\uff08\u5982FKGL\uff09\u4e0e\u4eba\u5de5\u53ef\u8bfb\u6027\u5224\u65ad\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u800c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4ef7\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u76f8\u5173\u6027\u66f4\u9ad8\uff0c\u63a8\u8350\u5728PLS\u9886\u57df\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u6765\u8bc4\u4f30\u6458\u8981\u7684\u53ef\u8bfb\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u5206\u6790\u4ee3\u7801\u548c\u6570\u636e\u3002"}}
{"id": "2508.19227", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19227", "abs": "https://arxiv.org/abs/2508.19227", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "title": "Generative Interfaces for Language Models", "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u52a8\u751f\u6210\u4efb\u52a1\u76f8\u5173\u754c\u9762\u7684\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u9a8c\u8bc1\uff0c\u5176\u5728\u7528\u6237\u4f53\u9a8c\u548c\u4ea4\u4e92\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5bf9\u8bdd\u5f0f\u7cfb\u7edf\uff0c\u7528\u6237\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u66f4\u559c\u6b22\u751f\u6210\u5f0f\u754c\u9762\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u591a\u4f5c\u4e3a\u52a9\u624b\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u4f46\u4f9d\u7136\u53d7\u9650\u4e8e\u7ebf\u6027\u8bf7\u6c42-\u54cd\u5e94\u683c\u5f0f\uff0c\u7279\u522b\u5728\u591a\u8f6e\u3001\u4fe1\u606f\u5bc6\u96c6\u548c\u63a2\u7d22\u6027\u4efb\u52a1\u4e2d\uff0c\u4e92\u52a8\u6548\u7387\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u6548\u7387\u548c\u4f53\u9a8c\u3002", "method": "\u63d0\u51fa\u4e00\u79cdGenerative Interfaces for Language Models\u8303\u5f0f\uff0c\u8ba9LLM\u4e3b\u52a8\u4e3a\u7528\u6237\u751f\u6210\u4ea4\u4e92\u754c\u9762\uff08UI\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u754c\u9762\u7279\u5b9a\u8868\u793a\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u5316\u4e3a\u7279\u5b9a\u4efb\u52a1\u7684\u754c\u9762\u3002\u7cfb\u7edf\u6027\u5f15\u5165\u591a\u7ef4\u8bc4\u4f30\u4f53\u7cfb\uff0c\u6bd4\u8f83\u751f\u6210\u5f0f\u754c\u9762\u4e0e\u4f20\u7edf\u804a\u5929\u754c\u9762\u5728\u591a\u79cd\u4efb\u52a1\u3001\u4ea4\u4e92\u548c\u67e5\u8be2\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u5f0f\u754c\u9762\u5728\u529f\u80fd\u3001\u4ea4\u4e92\u548c\u60c5\u611f\u4f53\u9a8c\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u5bf9\u8bdd\u5f0f\u754c\u9762\uff0c\u4e14\u572870%\u4ee5\u4e0a\u7684\u6848\u4f8b\u4e2d\u88ab\u7528\u6237\u504f\u597d\u9009\u62e9\u3002", "conclusion": "\u751f\u6210\u5f0f\u754c\u9762\u4e3a\u63d0\u5347LLM\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u660e\u786e\u4e86\u7528\u6237\u504f\u597d\u5176\u7684\u573a\u666f\u4e0e\u539f\u56e0\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u4e0eUI\u672a\u6765\u53d1\u5c55\u94fa\u8def\u3002"}}
