<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 37]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 本文提出B-bound抽象域和领域抽象新方法，大幅提升了递归定义函数不变量推导能力，支持高度非线性、自动化transfer function设计，并推广至一般函数空间，有利于程序分析和混合系统建模。


<details>
  <summary>Details</summary>
Motivation: 递归定义（如算子的不动点或函数方程）函数的闭式界推导难度大，传统数值抽象域很难应对高度非线性不变量的分析，限制了在程序分析和混合系统推导（如成本分析、循环加速等）中的应用。

Method: 1. 构建B-bound domain，利用选定边界函数的集合联合对数值函数进行抽象。2. 引入领域抽象(functor)，实现在函数空间中的Galois连接。3. 基于简单操作符语言，逐步推广到多元、分段、非离散函数领域。4. 分析约束空间中的凸性性质以简化自动化设计。

Result: 1. B-bound domains能有效推导高度非线性数值不变量。2. 约束空间的凸性性质有助于transfer function的更简洁甚至自动化设计。3. 领域抽象框架支持更一般的从符号到数值函数的抽象，并可实现降维处理。

Conclusion: 论文提出了一类新的约束抽象域（B-bound domains）以及一种支持符号到数值函数抽象（领域抽象）的范函，从而在高阶抽象解释下，可以推导递归定义函数的闭式界，并自动化和简化transfer function的设计。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [2] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: 论文提出了Kernel-FFI，一个适用于Jupyter等notebook环境的通用FFI框架，实现了跨语言函数与对象的便捷交互，降低了复杂度，支持OOP特性，将作为开源工具发布。


<details>
  <summary>Details</summary>
Motivation: 现有的外部函数接口（FFI）难以满足如Jupyter这种交互式notebook环境下的动态多语言开发需求，手动配置繁琐且支持有限，亟需改善。

Method: 提出Kernel-FFI框架，采用源代码级转换自动重写跨语言调用，无需手动编写绑定代码，且支持对象引用、自动资源管理，并通过创新的通道机制处理阻塞和递归、一部调用问题。

Result: Kernel-FFI实现了在notebook中多语言函数无缝调用和对象操作，提升了OOP支持，消除了大量样板代码，并增强了对异步与递归调用的处理能力。工具将开源。

Conclusion: Kernel-FFI为交互式notebook环境下的多语言开发提供了透明、高效、语言无关的FFI方案，极大简化了复杂性，提升了开发效率。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: 作者提出自动化大样本评测框架，系统分析LLM生成智能合约代码的质量，发现现有LLM可靠性不足，提出需探索更可靠集成方式，该工作为后续工具评测与开发提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的代码生成方法有诸多局限，近期文献中提出利用LLM根据业务流程描述生成智能合约代码。作者希望填补当前评测多为小样本、依赖人工检查、编译通过但未验证程序正确执行的不足。

Method: 该论文提出并采用了一个自动化评估框架，基于更大规模的数据集，系统评测了不同类型和规模LLM在智能合约生成中实现流程控制、资源分配与数据条件等方面的表现。

Result: 实验证明，LLM虽具备一定能力，但距离智能合约开发“完美可靠”的高要求仍有明显差距。所提出的评测基准为后续更可靠LLM集成工具的开发与评估奠定基础。

Conclusion: 当前的大型语言模型（LLMs）在生成智能合约代码时，尚未达到智能合约开发所需的高度可靠性水平。后续需要探索更负责任的LLM集成方式以提升代码生成的可靠性。

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [4] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: 提出了一种能根据用户样本自动生成ETL转换流程的新架构FlowETL，大幅减少了人工操作，在多个类型数据集上验证了其实用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前ETL流程需要大量人工参与，人工设计和实现特定上下文的转换，缺乏自动化和通用能力，限制了数据仓库和其他数据存储的高效应用。尽管ETL自动化领域已有进展，但仍缺乏能自动设计和应用转换的解决方案。

Method: 提出了FlowETL，这是一种基于示例的自主ETL流水线架构。FlowETL通过用户定义的目标数据集和输入输出样本对，由规划引擎自动生成转换计划，并由ETL worker应用到源数据集。监控和日志系统确保整个流程的可观测性。

Result: FlowETL在包含不同领域、文件结构与文件大小的14个数据集上展示了较好的泛化能力。

Conclusion: FlowETL能够无需人工干预，自动为不同数据集生成和应用上下文相关的数据转换方案，实现了ETL流程的自动化和一定程度的通用性。

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [5] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: 本文提出了“vibe modeling”这一新概念，融合了大语言模型与模型驱动工程优势，旨在提升复杂软件系统的开发效率与质量，同时也指出本方法面临的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 新型用户界面、智能组件需求、可持续性等挑战推动软件开发日益复杂和需求增长，现有开发方法和工具难以应对。模型驱动工程（MDE）虽提升了开发质量和效率，但模型本身也变得越来越难以管理。同时，基于大语言模型（LLM）的“vibe coding”虽然可以通过自然语言生成代码，但带来了安全、可维护性、可扩展性等新问题。

Method: 本文提出一种新的方法——vibe modeling，旨在整合人工智能（如LLM）与模型驱动工程（MDE）的优势，通过新理念和工具，推进高质量复杂系统的开发。文中勾勒了vibe modeling的核心理念，并探讨其未来面临的机会与挑战。

Result: 提出了“vibe modeling”这一新概念，初步定义其关键要素，并分析了其在建模领域的应用潜力和未决问题。

Conclusion: vibe modeling结合了人工智能和模型驱动工程技术，有望加快开发速度，提高复杂系统的可靠性，但在推广和应用过程中还面临诸多挑战，需要进一步研究和完善。

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [6] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: 大部分GitHub CI工具创新有限，多数只是重复已有功能，少数首发工具主导后续发展。本研究为工具开发与管理策略提供了量化分析和数据支持。


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace工具快速扩张，但大量新工具实则重复已有功能，导致创新停滞与资源浪费，亟需系统量化分析这一现象，为平台创新和产品策略提供参考。

Method: 将6,983个CI Actions与3,869个提供者关联，并挖掘其版本历史，通过图模型对每项功能的出现时间、其采纳情况进行建模，并对冗余工具进行聚类分析。

Result: 约65%的新CI Actions在六个月内复制已有功能，只有少数“首创”工具成为后续分支与扩展的核心来源。研究成果可帮助开发者选择最佳推出时机、发现功能空白，并协助维护者减少冗余。数据集与分析图表已开源发布，以促进行业创新与科学研究。

Conclusion: GitHub Marketplace中CI工具的新增很大比例是在重复已有功能，核心创新主要来自于少数首发工具。这为开发者和维护者在业务策略制定和产品管理方面提供了具有数据驱动的指导。

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [7] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge通过自动生成和调试IoT集成代码，把原本复杂且依赖专家的设备集成过程大幅简化，提高了效率、准确性和易用性，在实验和用户测试中表现出超过人类专家的能力。


<details>
  <summary>Details</summary>
Motivation: 多模态物联网（IoT）系统需要集成多种物联网设备，但将新设备集成到中央平台通常需要复杂、耗时且需要专业知识的编程工作。简化和自动化这一过程是提升IoT系统易用性和扩展性的关键。

Method: 提出了AutoBridge系统，采用分而治之的方法：首先自动获取设备特定知识生成控制逻辑，再结合平台特定知识生成可集成的代码。为保证正确性，AutoBridge还配备了多阶段调试流程，包括虚拟设备自动调试器和仅需二元反馈（是/否）的硬件调试机制。

Result: 在两个开源IoT平台、34种IoT设备上的测试显示，AutoBridge能在无人干预下达到93.87%的平均集成成功率和94.87%的功能覆盖率。通过最简答的二元用户反馈，功能覆盖率可达100%。

Conclusion: AutoBridge有效自动化了IoT设备集成代码的生成和调试，显著提升了集成效率和准确性，并在用户研究中超过了专家程序员。

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [8] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: 自主业务流程技术虽具高效优势，但不可解释性引发信任和合规等风险。论文提出可解释自主业务流程（XABP）概念，并系统阐释其结构、特点以及未来BPM领域的研究方向。


<details>
  <summary>Details</summary>
Motivation: 虽ABP能提升效率和自动化水平，但导致信任、可追溯性、问责和合规风险等问题，因此有必要设计让流程“可解释”的理论与方法。

Method: 系统梳理了可解释自主业务流程（XABP）的形式、解释结构，并识别出实现XABP过程中业务流程管理（BPM）领域需要面对的主要研究挑战。

Result: 构建了XABP的框架，并明确了其关键特征以及在业务流程管理研究中的待解挑战。

Conclusion: 论文提出，通过为自主业务流程（ABP）引入可解释性（XABP）能够缓解信任、易用性和合规性等方面的担忧。

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [9] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate 利用多智能体竞争与协作辩论机制，提升了软件工程问题定位与修复能力，在基准测试中效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的问题修复方法多为独立推理，容易陷入局部最优，难以发现跨越不同代码部分的问题模式。需要更高效的方式整合多元推理路径，实现更全面的问题定位和修复。

Method: 1. 基于代码依赖图，生成多条故障传播轨迹。2. 组织三轮、多视角、多智能体辩论，提出并融合不同的推理路径。3. 通过蒙特卡洛树搜索（MCTS）整合最终修复方案，用于代码补丁生成。

Result: 在 SWE-bench 基准测试中，SWE-Debate 实现了当前最优性能（SOTA），大幅超过其它开源智能体框架。

Conclusion: SWE-Debate 提出了竞争性多智能体辩论框架，在SWE-bench基准测试中实现了开源智能体框架的新SOTA（state-of-the-art）结果，显著优于现有基线方法。

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [10] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: 本文提出了一套用于自动化评估COBOL到Java代码转换的系统，结合分析器和LLM评审，实现了高效、多角度的自动质量评价，显著改善了评估效率和管理反馈。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的COBOL到Java代码转换存在评估难题，包括模型不透明和评估质量复杂性大。人工评审开销高、效率低，缺少自动化、高效的评估体系。

Method: 系统结合分析型检测器和LLM作为评委（LaaJ）的方法，实现可扩展、多维度的自动评估。系统集成于持续集成流程，支持大规模基准测试，无需高度依赖人工复查。

Result: 系统实现了对COBOL到Java翻译质量的高效、自动化评价，优化了开发和管理反馈流程，提升了翻译项目的规模化管理和代码质量演进能力。

Conclusion: 作者提出的评估系统显著解决了大模型代码翻译评估难题，为高质量代码现代化和持续集成提供了有力工具。

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [11] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: 本文提出经验增强的SWE-Exp智能体，能有效积累和复用以往问题解决经验，在软件修复任务上取得了新的最佳表现，展示了从试错到经验驱动转变的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在软件问题修复领域取得了显著进展，但现有智能体对每个问题都独立对待，缺乏基于以往经验的学习与知识复用。这导致重复探索失败路径，未能充分利用成功经验解决相似问题。

Method: 提出SWE-Exp，一种结合经验增强的方法：通过建立多层次经验库，系统性提取和积累以往修复中的高层次理解和具体代码变更经验，支持持续学习。该经验库包含对成功和失败修复的总结，实现可复用知识的积累。

Result: SWE-Exp在SWE-bench-Verified基准测试中，基于开源智能体框架取得了41.6%的Pass@1，达到了最先进水平。

Conclusion: SWE-Exp为自动化软件工程智能体提供了一种持续学习和经验驱动的全新范式，相较以往仅靠试错式探索，能够更有策略性地解决问题，推动了领域发展。

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [12] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: 本文提出了Trae Agent，一个基于智能体的集成推理方法，突破了现有方法在大规模集成空间探索和仓库级理解方面的瓶颈，在SWE-bench基准集上取得了业界领先成绩，并已开源支持社区应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的软件问题解决方法在探索大规模集成空间和实现仓库级理解方面存在局限，影响了其总体效果。随着LLM技术快速发展，需要更有效的方案应对实际软件开发中的复杂问题。

Method: 提出了Trae Agent，一种基于智能体(agent)的集成推理方法，将软件仓库中的问题解决建模为最优解搜索问题。设计了模块化智能体，分别负责生成、剪枝和选择，以提高处理大规模集成空间与仓库级理解的能力。

Result: 在SWE-bench基准集上，使用三种主流LLM与四种现有集成推理方法进行了对比。实验结果显示，Trae Agent在Pass@1指标上平均提升10.22%，并以75.20%的Pass@1分数登顶SWE-bench Verified排行榜。

Conclusion: Trae Agent显著提升了LLM在软件问题解决任务上的表现，是首个能在仓库级别处理复杂问题的基于智能体的集成推理方法，并向社区开放源码，推动后续研究和应用。

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [13] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: Kieker框架现在支持Python，结合静态和动态分析，有效提升了对Python应用结构的观测能力。


<details>
  <summary>Details</summary>
Motivation: Kieker原本专为Java设计，随着Python的流行，提供对Python的支持可以帮助更多开发者实现可观测性分析。

Method: 提出了一条结合静态与动态分析方法的Python分析流程，以全面了解系统结构。

Result: 开发了一套用于Python程序的观测分析流程，有助于获得应用的结构洞察。

Conclusion: 这种方法扩展了Kieker框架的应用范围，使其能更好地适应现代多语言的软件开发环境。

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [14] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本文基于大规模GitLab MR数据，将提交后代码调整量作为审查工作量指标，发现大部分MR需调整，且工作量与传统因素无关。通过多维度特征和机器学习模型，可有效预测CR工作量。结果对自动化管理和优化代码审查流程有重要意义。


<details>
  <summary>Details</summary>
Motivation: 代码审查（Code Review, CR）对软件开发至关重要，但其过程涉及大量工作，包括代码调整、回复评审意见等。虽然已有研究关注了CR的延迟和迭代次数，但很少有研究从代码变动量的角度衡量CR的工作量，尤其是在GitLab Merge Requests（MRs）背景下尚未深入探讨。

Method: 作者以CR提交后修改的代码量为工作量指标，收集了四个GitLab项目的23,600多个MR数据。通过分析MR的实际调整量，并训练可解释的机器学习模型，利用文本特征、代码复杂度、开发者经验、评审历史、分支策略等多维度数据进行建模和预测。

Result: 结果显示，多达71%的MR提交后需要调整，其中28%调整量超过200行代码。CR的工作量与评审时长和参与人数无关联。机器学习模型在预测任务中表现良好（AUC 0.84-0.88），并指出复杂度、经验、文本特征对预测有重要作用，历史项目特性也对当前审查努力有影响。

Conclusion: 机器学习能够较好地解释和预测代码审查过程中集成代码所需的努力。多因素（如复杂度、经验、文本特征等）对CR工作量具有重要影响。该研究表明以量化分析和自动化方法辅助CR流程管理是可行的。

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Explanations for Unrealizability of Infinite-State Safety Shields](https://arxiv.org/abs/2507.23603)
*Andoni Rodriguez,Irfansha Shaik,Davide Corsi,Roy Fox,Cesar Sanchez*

Main category: cs.LO

TL;DR: 本文提出用时序公式展开为安全组件 shield 不可实现问题生成简单解释，解决强化学习安全实践中的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 强化学习在实际应用中需要保证策略的安全性，shielding 技术能合成符合逻辑规范的安全组件，但现实中因规范不一致而出现 shield 无法实现的问题，亟需检测与解释这种不可实现性。

Method: 提出了一种利用时序公式展开（temporal formula unrolling）的方法，能够为 shield 的不可实现性生成简单的无条件和有条件解释，并展示了多种该技术的变体及其适用性。

Result: 新方法可以在无限状态空间下检测并解释 shield 不可实现的原因，使 shielding 更适用于真实复杂场景。

Conclusion: 本文流程化地为 shield 不可实现问题生成解释，有助于开发者理解和修正规范，提高强化学习安全组件的实际可用性。

Abstract: Safe Reinforcement Learning focuses on developing optimal policies while
ensuring safety. A popular method to address such task is shielding, in which a
correct-by-construction safety component is synthesized from logical
specifications. Recently, shield synthesis has been extended to infinite-state
domains, such as continuous environments. This makes shielding more applicable
to realistic scenarios. However, often shields might be unrealizable because
the specification is inconsistent (e.g., contradictory). In order to address
this gap, we present a method to obtain simple unconditional and conditional
explanations that witness unrealizability, which goes by temporal formula
unrolling. In this paper, we show different variants of the technique and its
applicability.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
*Sergio Di Meglio,Aniello Somma,Luigi Libero Lucio Starace,Fabio Scippacercola,Giancarlo Sperlì,Sergio Di Martino*

Main category: cs.CL

TL;DR: 本文通过真实案例分析，将两款主流大语言模型应用于在线住宿平台信息一致性提升任务。Mixtral 8x7B表现最佳，但消耗远高于Mistral 7B。结果为实际工程落地如何取舍模型效果与成本提供了有力指导。


<details>
  <summary>Details</summary>
Motivation: 在线房产预订平台依赖外部供应商提供住宿信息，但这些信息常常不完整或不一致，影响用户体验并造成市场损失。亟需提升信息描述的一致性与准确性。

Method: 将大语言模型（LLM）整合入房产预订平台CALEIDOHOTELS，评估两种模型（经过QLoRA微调的Mistral 7B与经过系统提示优化的Mixtral 8x7B），比较它们生成住宿描述的一致性、完整性、精确性和虚假生成（幻觉）率。

Result: Mixtral 8x7B在完整性（99.6%对93%）、精确性（98.8%对96%）和幻觉率（1.2%对4%）方面优于Mistral 7B，且生成内容更短更精炼，但其计算成本更高（50GB显存和$1.61/小时对5GB和$0.16/小时）。

Conclusion: Mixtral 8x7B虽质量优异但资源消耗大，Mistral 7B性价比较高。研究为实际部署LLMs在平台中提供了关于效果与效率平衡的实证参考。

Abstract: Online property booking platforms are widely used and rely heavily on
consistent, up-to-date information about accommodation facilities, often
sourced from third-party providers. However, these external data sources are
frequently affected by incomplete or inconsistent details, which can frustrate
users and result in a loss of market. In response to these challenges, we
present an industrial case study involving the integration of Large Language
Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by
FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,
fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.
Both models were assessed based on their ability to generate consistent and
homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B
outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision
(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet
more concise content (249 vs. 277 words on average). However, this came at a
significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB
and $0.16/hour for Mistral 7B. Our findings provide practical insights into the
trade-offs between model quality and resource efficiency, offering guidance for
deploying LLMs in production environments and demonstrating their effectiveness
in enhancing the consistency and reliability of accommodation data.

</details>


### [17] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
*Jinzhi Wang,Qingke Peng,Haozhou Li,Zeyuan Zeng,Qinfeng Song,Kaixuan Yang,Jiangbo Zhang,Yaoying Wang,Ruimeng Li,Biyi Zhou*

Main category: cs.CL

TL;DR: 针对电力营销客户服务需求，该文提出ElectriQ评测基准，结合知识增强方法，使定制后的小模型在专业性和用户体验上优于主流大模型，为行业应用提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有电力客户服务系统响应慢、流程僵化及行业特定任务表现有限，现有主流大模型通用性强但缺乏专业领域与同理心支持，因此需要专用于电力营销场景的评测基准与优化手段。

Method: 构建了涵盖六类服务的电力营销对话数据集ElectriQ，提出四项评测指标，并结合领域知识库和知识增强方法对13个LLM进行实验评估。

Result: 经过知识增强和微调后，LLama3-8B等小规模模型在专业性和用户友好性上超越了GPT-4o。ElectriQ推动了为电力营销服务定制LLMs的发展。

Conclusion: ElectriQ为电力营销领域大型语言模型的评测和优化奠定了基础，提升了模型的专业性和用户友好性。小参数模型在特定任务上可优于通用大模型。

Abstract: Electric power marketing customer service plays a critical role in addressing
inquiries, complaints, and service requests. However, current systems, such as
China's 95598 hotline, often struggle with slow response times, inflexible
procedures, and limited accuracy in domain-specific tasks. While large language
models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,
they lack the domain expertise and empathy required in this field. To bridge
this gap, we introduce ElectriQ, the first benchmark designed to evaluate and
enhance LLMs in electric power marketing scenarios. ElectriQ consists of a
dialogue dataset covering six key service categories and introduces four
evaluation metrics: professionalism, popularity, readability, and
user-friendliness. We further incorporate a domain-specific knowledge base and
propose a knowledge augmentation method to boost model performance. Experiments
on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and
augmented, can surpass GPT-4o in terms of professionalism and
user-friendliness. ElectriQ establishes a comprehensive foundation for
developing LLMs tailored to the needs of power marketing services.

</details>


### [18] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
*Navid Yazdanjue,Morteza Rakhshaninejad,Hossein Yazdanjouei,Mohammad Sadegh Khorshidi,Mikko S. Niemela,Fang Chen,Amir H. Gandomi*

Main category: cs.CL

TL;DR: 本研究结合深度学习与人工特征，提出新分类系统，有效提升多平台非法内容检测性能，取得了领先于其他主流模型的结果。


<details>
  <summary>Details</summary>
Motivation: 非法市场逐渐转移到互联网隐秘角落，包括深网、暗网及Telegram、Reddit等平台。这些渠道因匿名特性成为毒品、武器、凭证等非法商品交易温床。鉴于标注数据有限、非法语言不断演变及平台结构多样，自动检测和分类该类内容具有很大挑战性。

Method: 提出一个分层分类框架，结合微调语言模型（如ModernBERT）和半监督集成学习。首先使用微调后的ModernBERT从多个平台抽取语义表示，捕捉特殊行话和模糊语言。再结合人工设计的文档结构、内嵌模式（如比特币地址、邮件、IP）及元数据特征。分类流程分两阶段：第一阶段用XGBoost、随机森林和SVM集成模型检测销售类文档，第二阶段进一步细分类别（毒品/武器/凭证）。

Result: 在三个数据集（自建多源语料、DUTA、CoDA）测试，模型优于多种主流基线（如BERT、ALBERT、Longformer等），准确率0.96489，F1分数0.93467，TMCC 0.95388，显示出强泛化能力和适应有限标注下高效检测能力。

Conclusion: 提出的多层级分类体系，综合自然语言处理和人工特征，可以有效、稳健地检测多平台非法市场内容，对实际应用和反网络犯罪有重要意义。

Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the
internet, including the deep and dark web, as well as platforms such as
Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of
illicit goods including drugs, weapons, and stolen credentials. Detecting and
categorizing such content remains challenging due to limited labeled data, the
evolving nature of illicit language, and the structural heterogeneity of online
sources. This paper presents a hierarchical classification framework that
combines fine-tuned language models with a semi-supervised ensemble learning
strategy to detect and classify illicit marketplace content across diverse
platforms. We extract semantic representations using ModernBERT, a transformer
model for long documents, finetuned on domain-specific data from deep and dark
web pages, Telegram channels, Subreddits, and Pastebin pastes to capture
specialized jargon and ambiguous linguistic patterns. In addition, we
incorporate manually engineered features such as document structure, embedded
patterns including Bitcoin addresses, emails, and IPs, and metadata, which
complement language model embeddings. The classification pipeline operates in
two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random
Forest, and SVM with entropy-based weighted voting to detect sales-related
documents. The second stage further classifies these into drug, weapon, or
credential sales. Experiments on three datasets, including our multi-source
corpus, DUTA, and CoDA, show that our model outperforms several baselines,
including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The
model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of
0.95388, demonstrating strong generalization, robustness under limited
supervision, and effectiveness in real-world illicit content detection.

</details>


### [19] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
*Jinyu Liu,Xiaoying Song,Diana Zhang,Jason Thomale,Daqing He,Lingzi Hong*

Main category: cs.CL

TL;DR: 传统ML和LLM单独用于图书馆主题分析各有局限，本文提出两者结合的混合框架，能显著提升生成主题标签的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 本文指出，图书馆管理系统必须对信息资源进行主题分析。目前虽然大型语言模型（LLMs）被广泛用于分类和摘要，但在主题分析上的能力尚未深入探索。传统机器学习的多标签分类方法对未见案例表现不好，而LLMs则容易过度生成和产生幻觉，因此需要更优的解决方案。

Method: 作者提出了一种混合框架：将基于嵌入的ML模型与LLM结合。具体做法是：1）用ML预测最佳的主题标签数量，以指导LLM生成内容；2）用真实的LCSH术语对LLM生成的标签进行后编辑，减少幻觉。

Result: 实验显示，对LLM的生成提供初始预测并对其输出施加后编辑，能够得到更加受控且与词表一致的主题标签输出。

Conclusion: 混合ML和LLM的方法能提升面向LCSH主题分析的生成结果可控性和规范性，相较单独使用LLM可更好满足图书馆信息资源主题分析需求。

Abstract: Providing subject access to information resources is an essential function of
any library management system. Large language models (LLMs) have been widely
used in classification and summarization tasks, but their capability to perform
subject analysis is underexplored. Multi-label classification with traditional
machine learning (ML) models has been used for subject analysis but struggles
with unseen cases. LLMs offer an alternative but often over-generate and
hallucinate. Therefore, we propose a hybrid framework that integrates
embedding-based ML models with LLMs. This approach uses ML models to (1)
predict the optimal number of LCSH labels to guide LLM predictions and (2)
post-edit the predicted terms with actual LCSH terms to mitigate
hallucinations. We experimented with LLMs and the hybrid framework to predict
the subject terms of books using the Library of Congress Subject Headings
(LCSH). Experiment results show that providing initial predictions to guide LLM
generations and imposing post-edits result in more controlled and
vocabulary-aligned outputs.

</details>


### [20] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
*Victor Eiti Yamamoto,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本文提出了一种结合标签匹配与三元组匹配的知识图谱集成方法，显著提升多样化和复杂上下文下的匹配准确性，并表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱（KG）集成主要聚焦于schema和实体匹配，而上下文匹配研究不足。实际应用中KG的数据源、规模及信息密度多样化，当前实体匹配方法难以覆盖复杂上下文，导致在真实场景表现有限。

Method: 提出了一种包含标签匹配与三元组匹配的新型知识图谱集成方法。首先，通过字符串操作、模糊匹配与向量相似性对实体和谓词标签进行对齐，随后识别传递相似信息的三元组映射，并利用这些映射提升实体匹配的准确率。同时还构建了新的测试数据集，以全面评估三元组匹配效果。

Result: 所提方法在OAEI比赛及其他多样化场景下，与主流系统和有监督方法相比表现出较高准确率与竞争力。

Conclusion: 该方法解决了复杂上下文下知识图谱集成问题，在准确性和适应性方面优于现有方案。同时，新增的测试数据集有助于深入评估三元组匹配机制。

Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.

</details>


### [21] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
*Esmail Gumaan*

Main category: cs.CL

TL;DR: 论文系统梳理了大模型幻觉的理论定义、检测和缓解方法及评估流程，为幻觉治理提供了理论与实用方案。


<details>
  <summary>Details</summary>
Motivation: 幻觉现象影响LLM生成内容的可靠性，亟需理论定义、风险衡量、检测缓解方法和评估标准。

Method: 采用理论分析（PAC-Bayes和Rademacher复杂度），全面综述了检测和缓解幻觉的方法，并提出了统一的检测与缓解流程。

Result: 严格区分了内在与外在幻觉，提出并推导了幻觉风险界，归纳和评述了多种检测缓解方法，并制定了评估协议。

Conclusion: 本论文为大语言模型（LLM）中的幻觉问题提供了理论基础和实际指导，为未来减少幻觉现象指明了方向。

Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of
content that is not faithful to the input or the real-world facts. This paper
provides a rigorous treatment of hallucination in LLMs, including formal
definitions and theoretical analyses. We distinguish between intrinsic and
extrinsic hallucinations, and define a \textit{hallucination risk} for models.
We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes
and Rademacher complexity). We then survey detection strategies for
hallucinations, such as token-level uncertainty estimation, confidence
calibration, and attention alignment checks. On the mitigation side, we discuss
approaches including retrieval-augmented generation, hallucination-aware
fine-tuning, logit calibration, and the incorporation of fact-verification
modules. We propose a unified detection and mitigation workflow, illustrated
with a diagram, to integrate these strategies. Finally, we outline evaluation
protocols for hallucination, recommending datasets, metrics, and experimental
setups to quantify and reduce hallucinations. Our work lays a theoretical
foundation and practical guidelines for addressing the crucial challenge of
hallucination in LLMs.

</details>


### [22] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
*Kwun Hang Lau,Ruiyuan Zhang,Weijie Shi,Xiaofang Zhou,Xiaojun Cheng*

Main category: cs.CL

TL;DR: 提出了结合时间逻辑的RAG框架，解决了RAG难以处理时间跨度查询的问题，并在新开发的财金类基准数据集上验证了明显优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: RAG在处理静态事实知识注入方面表现优异，但在需要跨时间追踪实体和现象的问题上存在重大不足，常规方法难以检索到连续且时序一致的相关证据。作者旨在弥补这一盲点。

Method: 提出了一种融入时间逻辑的新型RAG框架，包括：将查询拆解为主题和时间窗口，用既考量语义又兼顾时序相关性的专用检索器进行证据检索，并开发了新基准数据集ADQAB进行测试。

Result: 实验证明，所提方法在ADQAB基准集上显著提高了回答准确率，超过标准RAG 13%～27%。相关数据集与代码已经对外开放。

Conclusion: 本文提出的方法显著提升了RAG系统在处理涉及时间跨度问题上的准确性，比标准RAG提升13%至27%，展示了方法的有效性和现实潜力。数据集和代码均已公开。

Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static,
factual knowledge into Large Language Models (LLMs), it exhibits a critical
deficit in handling longitudinal queries that require tracking entities and
phenomena across time. This blind spot arises because conventional,
semantically-driven retrieval methods are not equipped to gather evidence that
is both topically relevant and temporally coherent for a specified duration. We
address this challenge by proposing a new framework that fundamentally
redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by
disentangling a user's query into its core subject and its temporal window. It
then employs a specialized retriever that calibrates semantic matching against
temporal relevance, ensuring the collection of a contiguous evidence set that
spans the entire queried period. To enable rigorous evaluation of this
capability, we also introduce the Analytical Diachronic Question Answering
Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus
of real and synthetic financial news. Empirical results on ADQAB show that our
approach yields substantial gains in answer accuracy, surpassing standard RAG
implementations by 13% to 27%. This work provides a validated pathway toward
RAG systems capable of performing the nuanced, evolutionary analysis required
for complex, real-world questions. The dataset and code for this study are
publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [23] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
*Daniel Son,Sanjana Rathore,Andrew Rufail,Adrian Simon,Daniel Zhang,Soham Dave,Cole Blondin,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 作者比较了不同尺寸Gemma模型内部特征，经分析发现中间层特征最为接近，支持模型间普适可解释性。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在探究不同规模（如Gemma-2-2B和Gemma-2-9B）的大语言模型内部特征的通用性，即使模型规模相差四倍，是否依然能收敛到相似的内部语义表示。

Method: 采用稀疏自编码器（SAE）词典学习方法，对每个模型的残差流激活进行处理。然后通过激活相关性对齐获得单义特征，并利用SVCCA和RSA两种方法比较特征空间的匹配程度。还初步将分析从单 token 扩展到多 token 子空间。

Result: 在模型的中间层发现最强的特征重叠，而早期和后期层次的相似性较低。初步实验表明，语义相似的多token子空间与语言模型的交互也具有相似性。

Conclusion: 大语言模型尽管规模有差异，依旧会形成普遍相似、可解释的特征空间，支持了跨模型可解释性基于通用特征的观点。

Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B
and Gemma-2-9B), asking whether models with a four-fold difference in scale
still converge on comparable internal concepts. Using the Sparse Autoencoder
(SAE) dictionary-learning pipeline, we utilize SAEs on each model's
residual-stream activations, align the resulting monosemantic features via
activation correlation, and compare the matched feature spaces with SVCCA and
RSA. Middle layers yield the strongest overlap, while early and late layers
show far less similarity. Preliminary experiments extend the analysis from
single tokens to multi-token subspaces, showing that semantically similar
subspaces interact similarly with language models. These results strengthen the
case that large language models carve the world into broadly similar,
interpretable features despite size differences, reinforcing universality as a
foundation for cross-model interpretability.

</details>


### [24] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 研究通过深度预训练语言模型和滑动窗口技术，在临床试验启动前利用注册信息高效预测安全性（严重不良事件）结果，有助于提升临床试验的科学性和安全性。


<details>
  <summary>Details</summary>
Motivation: 目前临床试验在设计时，由于对安全性指标（如严重不良事件 SAE）预期不准确，可能导致试验终止或受试者暴露于不必要风险。研究动机在于利用临床试验注册信息，提前预测试验安全性结果，从而优化试验设计。

Method: 分析了ClinicalTrials.gov上22,107项两臂平行干预性临床试验。采用迁移学习（如ClinicalT5、BioBERT）进行特征提取，结合后续模型（包括分类和回归方法）进行预测。对于文本超长问题，提出滑动窗口方法增强语义嵌入。开发了两种预测模型：一是分类模型预测实验组的SAE率是否高于对照组（指标为AUC）；二是回归模型预测对照组SAE比例（指标为RMSE）。

Result: 最佳模型（ClinicalT5+Transformer+MLP）在预测试验组SAE发生率高低时，AUC为77.6%；预测对照组SAE比例时，RMSE为18.6%。滑动窗口方法相较于无此方法的基线提升显著：平均AUC提高2.00%，平均RMSE降低1.58%。

Conclusion: 利用预训练语言模型和滑动窗口技术，可以对临床试验的严重不良事件结果进行较为准确的预测。该方法为试验设计阶段提供了数据支持，有助于优化受试者保护和提升试验效率。ClinicalTrials.gov等注册平台的数据远未被充分利用。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [25] [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)
*Jindong Li,Yali Fu,Jiahong Liu,Linxiao Cao,Wei Ji,Menglin Yang,Irwin King,Ming-Hsuan Yang*

Main category: cs.CL

TL;DR: 本文首次系统综述了LLM领域中的离散化与向量量化方法，归纳了算法类别、原理和应用难点，对未来高效多模态系统发展具有指导意义。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，如何将连续的多模态数据有效转化为适合语言模型处理的离散表示成为亟需解决的问题。离散化（尤其是向量量化，VQ）既提升了计算效率，也与LLM结构高度兼容，但相关的综述研究尚未形成体系。

Method: 本文首次系统梳理并分析了为LLM设计的离散化方法，提出结构化的分类法，总结了8种具有代表性的VQ变体，从算法原理、训练动态、与LLM集成等角度进行全面比较。同时梳理了VQ在传统任务、LLM单模态和多模态系统中的应用及面临的挑战。

Result: 作者总结了VQ在融合LLM体系时对比对、推理和生成能力的影响，明确指出了现有方法中存在的主要挑战（如码本崩溃、梯度估计不稳定、多模态编码约束等），并展望了动态/自适应量化、统一token化框架、生物启发式码本学习等未来方向。

Conclusion: 本文为LLM相关的离散化和向量量化方法提供了结构化梳理和深度分析，填补了领域综述空白，成为后续高效、多模态LLM研究与实践的重要参考资料。

Abstract: The rapid advancement of large language models (LLMs) has intensified the
need for effective mechanisms to transform continuous multimodal data into
discrete representations suitable for language-based processing. Discrete
tokenization, with vector quantization (VQ) as a central approach, offers both
computational efficiency and compatibility with LLM architectures. Despite its
growing importance, there is a lack of a comprehensive survey that
systematically examines VQ techniques in the context of LLM-based systems. This
work fills this gap by presenting the first structured taxonomy and analysis of
discrete tokenization methods designed for LLMs. We categorize 8 representative
VQ variants that span classical and modern paradigms and analyze their
algorithmic principles, training dynamics, and integration challenges with LLM
pipelines. Beyond algorithm-level investigation, we discuss existing research
in terms of classical applications without LLMs, LLM-based single-modality
systems, and LLM-based multimodal systems, highlighting how quantization
strategies influence alignment, reasoning, and generation performance. In
addition, we identify key challenges including codebook collapse, unstable
gradient estimation, and modality-specific encoding constraints. Finally, we
discuss emerging research directions such as dynamic and task-adaptive
quantization, unified tokenization frameworks, and biologically inspired
codebook learning. This survey bridges the gap between traditional vector
quantization and modern LLM applications, serving as a foundational reference
for the development of efficient and generalizable multimodal systems. A
continuously updated version is available at:
https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

</details>


### [26] [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)
*Lee Harris*

Main category: cs.CL

TL;DR: 提出了一种串联多语言模型的LMC算法，在提升准确率和速度的同时，有效减少了模型幻觉，特别适用于知识抽取场景，如医疗信息提取。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然能够捕捉文本中的复杂关系，但存在高昂成本和幻觉（生成不存在的信息）的问题，错误信息会浪费资源。

Method: 提出并实现了Language Model Chain (LMC)算法：对每个给定文本和提示，语言模型生成的答案只有在候选答案集合中存在时才被认定为正确；对于不正确回答，将相关文本输入更强但较慢的语言模型，依次通过多个模型迭代，直到获得正确答案为止。

Result: LMC算法用于从医疗文档中提取出生日期，多阶段串联的语言模型显著提升了预测速度和准确率，并大幅减少了幻觉现象。

Conclusion: LMC算法对于知识抽取领域具有重要意义，未来有进一步拓展价值。

Abstract: Language models can capture complex relationships in given text, but these
are notorious for being costly and for producing information that does not
exist (i.e., hallucinations). Furthermore, the resources invested into
producing this information would be wasted if it were incorrect. We address
these issues by proposing, implementing, and applying the Language Model Chain
(LMC) algorithm. In this, a language model's response to a given prompt about
given text is only correct if it exists in the collection of possible (i.e.,
candidate) answers, and text corresponding to incorrect responses is fed into a
more predictive (but slower) language model. This process is repeated for a
collection of language models, or until all predictions about the text are
correct. We used the LMC algorithm to extract patient dates of birth from
medical documents, and combining a collection of language models in a
multi-stage cascade significantly increased prediction speed and accuracy over
individual language models, while greatly reducing the number of corresponding
hallucinations. We believe that the novel LMC algorithm significantly
contributes to the knowledge extraction field, and that this should be explored
much further in the future.

</details>


### [27] [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)
*Mateusz Kmak,Kamil Chmurzyński,Kamil Matejuk,Paweł Kotzbach,Jan Kocoń*

Main category: cs.CL

TL;DR: 社交媒体情绪对股价预测作用有限，评论量和搜索热度更具参考价值。传统情感分析难完全反映网络讨论对市场的真实影响。


<details>
  <summary>Details</summary>
Motivation: 近几年，散户投资者通过社交媒体（如Reddit）影响股票价格的现象引发了学术界和实务界对网络情绪与股价关系的关注，尤其是在GameStop事件后。本文旨在探索社交媒体衍生的情绪是否能有效预测股市走势。

Method: 作者选取Reddit r/wallstreetbets下与GameStop（GME）和AMC讨论相关的内容，分别应用两种现有情感分析方法，并创新性地提出基于ChatGPT注释和RoBERTa微调的新模型，以适应社交平台的非正式语言和表情。通过相关性与因果性指标评估分析结果。

Result: 结果显示，社交媒体情绪与股价的相关性较弱。反而评论数量和Google搜索趋势等简单指标，具有更强的预测能力。

Conclusion: 研究表明，传统情感分析难以充分捕捉互联网讨论的市场影响力，网络散户行为的复杂性超出一般情感追踪的解释能力。应更关注其他量化指标和更复杂的非结构化数据解读。

Abstract: The surge of retail investor activity on social media, exemplified by the
2021 GameStop short squeeze, raised questions about the influence of online
sentiment on stock prices. This paper explores whether sentiment derived from
social media discussions can meaningfully predict stock market movements. We
focus on Reddit's r/wallstreetbets and analyze sentiment related to two
companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's
role, we employ two existing text-based sentiment analysis methods and
introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model
designed to better interpret the informal language and emojis prevalent in
social media discussions. We use correlation and causality metrics to determine
these models' predictive power. Surprisingly, our findings suggest that social
media sentiment has only a weak correlation with stock prices. At the same
time, simpler metrics, such as the volume of comments and Google search trends,
exhibit stronger predictive signals. These results highlight the complexity of
retail investor behavior and suggest that traditional sentiment analysis may
not fully capture the nuances of market-moving online discussions.

</details>


### [28] [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)
*Aman Gupta,Yingying Zhuang,Zhou Yu,Ziji Zhang,Anurag Beniwal*

Main category: cs.CL

TL;DR: 本文研究多语言检索增强生成系统中，提示翻译策略对分类任务的影响，发现优化后的提示策略可以明显提升低资源语言的性能，强调跨语言资源和策略优化的价值。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型的性能在不同语言和任务中存在较大差异，且高资源语言知识库经常被用于低资源语言，如何有效利用提示翻译策略以提升模型表现尚无定论。

Method: 系统性地评估了不同提示翻译策略在RAG增强大语言模型多语言系统中对分类任务的影响，通过实验比对不同策略的效果。

Result: 实验表明，合理选择和优化提示翻译策略能有效提升模型在多语言分类中的性能，强化跨语言知识流通。

Conclusion: 优化的提示策略能够显著提升跨语言的知识共享，从而提升下游分类任务的性能。建议在非英语，特别是低资源语言中，更广泛地采用多语言资源共享和跨语言提示优化。

Abstract: Despite advances in the multilingual capabilities of Large Language Models
(LLMs), their performance varies substantially across different languages and
tasks. In multilingual retrieval-augmented generation (RAG)-based systems,
knowledge bases (KB) are often shared from high-resource languages (such as
English) to low-resource ones, resulting in retrieved information from the KB
being in a different language than the rest of the context. In such scenarios,
two common practices are pre-translation to create a mono-lingual prompt and
cross-lingual prompting for direct inference. However, the impact of these
choices remains unclear. In this paper, we systematically evaluate the impact
of different prompt translation strategies for classification tasks with
RAG-enhanced LLMs in multilingual systems. Experimental results show that an
optimized prompting strategy can significantly improve knowledge sharing across
languages, therefore improve the performance on the downstream classification
task. The findings advocate for a broader utilization of multilingual resource
sharing and cross-lingual prompt optimization for non-English languages,
especially the low-resource ones.

</details>


### [29] [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)
*Brittney Exline,Melanie Duffin,Brittany Harbison,Chrissa da Gomez,David Joyner*

Main category: cs.CL

TL;DR: 英语母语与否影响在线CS课程学生互评：非母语者更积极但收到的情感反馈较弱，母语者对反馈满意度低，语言背景的影响虽小但复杂。


<details>
  <summary>Details</summary>
Motivation: 随着美国研究生级别的计算机科学（CS）项目中国际学生人数逐年增加，非母语英语学习者成为在线课程中的主力。论文动机在于探索学生英语母语身份对他们在在线课程中同伴反馈体验的具体影响。

Method: 采用Twitter-roBERTa模型，对500名学生之间的同伴评价文本进行情感分析。分析这些反馈的情绪分数，与学生的语言背景（母语/非母语英语者）、反馈评分等因素关联起来，同时控制性别和年龄等变量。

Result: 研究结果显示：英语母语者对所收到的反馈评价较低，而非母语者写下的反馈更为积极，但他们收到的反馈情感得分反而更低。在控制性别和年龄后，语言背景依然表现出显著而复杂的影响。

Conclusion: 英语母语身份在在线计算机课程中对同伴反馈的体验有一定影响：非母语英语学生更倾向于给出正面反馈，但反而收到的反馈正面程度较低；母语英语学生则对反馈满意度较低。语言背景的影响不大，但具有复杂性。

Abstract: Graduate-level CS programs in the U.S. increasingly enroll international
students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.
students. Many of these students take online courses, where peer feedback is
used to engage students and improve pedagogy in a scalable manner. Since these
courses are conducted in English, many students study in a language other than
their first. This paper examines how native versus non-native English speaker
status affects three metrics of peer feedback experience in online U.S.-based
computing courses. Using the Twitter-roBERTa-based model, we analyze the
sentiment of peer reviews written by and to a random sample of 500 students. We
then relate sentiment scores and peer feedback ratings to students' language
background. Results show that native English speakers rate feedback less
favorably, while non-native speakers write more positively but receive less
positive sentiment in return. When controlling for sex and age, significant
interactions emerge, suggesting that language background plays a modest but
complex role in shaping peer feedback experiences.

</details>


### [30] [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)
*Haoran Sun,Shaoning Zeng*

Main category: cs.CL

TL;DR: 本文提出了H-MEM分层记忆架构，通过结构化、多层次组织和索引检索机制，显著提升了大语言模型代理在长期对话中的表现，优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理（LLM Agents）的推理能力受限于其长期记忆能力。现有工作的存储与检索方法（如相似度搜索、知识图谱等）在结构化与高效检索方面表现不足，亟需新的机制提升记忆模块效能。

Method: 提出一种分层记忆（Hierarchical Memory, H-MEM）架构，根据语义抽象层级，将记忆以多层级方式组织和更新。每个记忆向量都带有指向下层相关子记忆的索引编码。在推理阶段，通过索引路由机制分层检索，避免了高昂的全量相似度计算。

Result: 在LoCoMo数据集的五项任务设置中，H-MEM在长期对话场景下始终优于五种基线方法。

Conclusion: 分层记忆架构能够有效提升LLM Agent长期记忆能力，实现更高效的决策与对话连贯性，在多个任务上取得显著优势。

Abstract: Long-term memory is one of the key factors influencing the reasoning
capabilities of Large Language Model Agents (LLM Agents). Incorporating a
memory mechanism that effectively integrates past interactions can
significantly enhance decision-making and contextual coherence of LLM Agents.
While recent works have made progress in memory storage and retrieval, such as
encoding memory into dense vectors for similarity-based search or organizing
knowledge in the form of graph, these approaches often fall short in structured
memory organization and efficient retrieval. To address these limitations, we
propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that
organizes and updates memory in a multi-level fashion based on the degree of
semantic abstraction. Each memory vector is embedded with a positional index
encoding pointing to its semantically related sub-memories in the next layer.
During the reasoning phase, an index-based routing mechanism enables efficient,
layer-by-layer retrieval without performing exhaustive similarity computations.
We evaluate our method on five task settings from the LoCoMo dataset.
Experimental results show that our approach consistently outperforms five
baseline methods, demonstrating its effectiveness in long-term dialogue
scenarios.

</details>


### [31] [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)
*Nilesh,Atul Gupta,Avinash C Panday*

Main category: cs.CL

TL;DR: 本论文提出一种基于全局上下文的实体输入编码新方法，显著提升了文档级关系抽取的准确性，经多项基准测试验证有效，兼具理论创新和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 以往方法只关注实体出现的句子，无法获得完整的文档上下文信息，导致关系抽取不准确，因此需要一种能整合全文实体位置信息和全局语境的新方法。

Method: 提出了一种新颖的输入嵌入方法，将实体在文档中的多个出现位置整体建模为独立片段、分离于具体出现位置，通过这一方式捕捉全局关系和实现多句推理。

Result: 在DocRED、Re-DocRED和REBEL三项关系抽取基准数据集上，实验结果表明该方法在文档级关系预测任务中表现优异，既推进了理论研究，也具备实际应用价值。

Conclusion: 所提出的方法能在文档级关系抽取任务中更准确地预测实体间的关系，对提升全局上下文建模和多句推理具有积极意义。

Abstract: In document-level relation extraction, entities may appear multiple times in
a document, and their relationships can shift from one context to another.
Accurate prediction of the relationship between two entities across an entire
document requires building a global context spanning all relevant sentences.
Previous approaches have focused only on the sentences where entities are
mentioned, which fails to capture the complete document context necessary for
accurate relation extraction. Therefore, this paper introduces a novel input
embedding approach to capture the positions of mentioned entities throughout
the document rather than focusing solely on the span where they appear. The
proposed input encoding approach leverages global relationships and
multi-sentence reasoning by representing entities as standalone segments,
independent of their positions within the document. The performance of the
proposed method has been tested on three benchmark relation extraction
datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results
demonstrated that the proposed method accurately predicts relationships between
entities in a document-level setting. The proposed research also has
theoretical and practical implications. Theoretically, it advances global
context modeling and multi-sentence reasoning in document-level relation
extraction. Practically, it enhances relationship detection, enabling improved
performance in real-world NLP applications requiring comprehensive entity-level
insights and interpretability.

</details>


### [32] [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)
*Zhehao Tan,Yihan Jiao,Dan Yang,Lei Liu,Jie Feng,Duolin Sun,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出了一套创新的细粒度RAG基准测试框架，系统性评估LLM在RAG中的多种能力，发现主流LLM存在语境忠实性和错误鲁棒性不足的问题。该基准有助于推动更可靠RAG系统的研究与开发。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估RAG（检索增强生成）系统的基准多关注系统整体性能，很少关注LLM（大语言模型）在RAG系统中的具体能力，且缺乏针对文档利用的系统化细粒度评测框架。作者为此提出新方法。

Method: 提出了Placeholder-RAG-Benchmark，多维度、细粒度地评估LLM在RAG中的能力，包括多级过滤能力、组合能力和参考推理能力。创新性地采用占位符方法，解耦LLM的参数化知识与外部知识的贡献。

Result: 实验表明，典型LLM在RAG中的生成能力存在局限，特别是在错误鲁棒性和语境忠实性方面。新基准为研发更可靠高效的RAG系统提供了可复现的评测框架。

Conclusion: 该工作提出了一个细致、创新的RAG基准测试方法，有助于深入理解和提升LLM在RAG系统中的实际效果，对推动更可靠的实际系统开发具有重要意义。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge, where the LLM's ability to generate responses
based on the combination of a given query and retrieved documents is crucial.
However, most benchmarks focus on overall RAG system performance, rarely
assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects
such as noise robustness, but lack a systematic and granular evaluation
framework on document utilization. To this end, we introduce
\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,
emphasizing the following progressive dimensions: (1) multi-level filtering
abilities, (2) combination abilities, and (3) reference reasoning. To provide a
more nuanced understanding of LLMs' roles in RAG systems, we formulate an
innovative placeholder-based approach to decouple the contributions of the
LLM's parametric knowledge and the external knowledge. Experiments demonstrate
the limitations of representative LLMs in the RAG system's generation
capabilities, particularly in error resilience and context faithfulness. Our
benchmark provides a reproducible framework for developing more reliable and
efficient RAG systems. Our code is available in
https://github.com/Alipay-Med/PRGB.

</details>


### [33] [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
*Xi Chen,Aske Plaat,Niki van Stein*

Main category: cs.CL

TL;DR: 本文通过特征层级因果分析，证实链式思考提示令大型语言模型具备更可解释的内部推理结构，尤其在大模型上效果显著，并提出相关分析方法辅助此发现。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思考（CoT）提示能提升大型语言模型在多步任务上的准确率，但这些生成的“思考”是否反映了真实的内部推理过程仍不清楚。作者希望探究CoT的事实性和其对模型内部计算结构的影响。

Method: 作者结合稀疏自编码器和激活修补（activation patching）方法，针对Pythia-70M和Pythia-2.8B模型，在GSM8K数学问题下，用CoT和普通提示（noCoT）分别提取和分析单义（monosemantic）特征，通过特征交换实验研究不同提示方式下的内部表征和模型行为变化。

Result: 在2.8B模型中，将少量CoT推理相关特征嵌入普通noCoT运行，能显著提升答案的对数概率，而在70M模型无显著效果，展示了模型规模阈值。此外，CoT让大型模型表现出更高的激活稀疏度和特征可解释性，内部计算更加模块化，如正确答案置信度从1.2提升到4.3。作者还提出patch-curves及随机特征修补基线，说明有用的CoT信息不仅在Top-K修补，还分布较广。

Conclusion: CoT提示能在大容量语言模型中诱导出更可解释的内部结构，作为一种结构化提示方法，其提升模型推理事实性和解释性的效果得到验证。

Abstract: Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on
multi-step tasks, yet whether the generated "thoughts" reflect the true
internal reasoning process is unresolved. We present the first feature-level
causal study of CoT faithfulness. Combining sparse autoencoders with activation
patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B
while they tackle GSM8K math problems under CoT and plain (noCoT) prompting.
Swapping a small set of CoT-reasoning features into a noCoT run raises answer
log-probabilities significantly in the 2.8B model, but has no reliable effect
in 70M, revealing a clear scale threshold. CoT also leads to significantly
higher activation sparsity and feature interpretability scores in the larger
model, signalling more modular internal computation. For example, the model's
confidence in generating correct answers improves from 1.2 to 4.3. We introduce
patch-curves and random-feature patching baselines, showing that useful CoT
information is not only present in the top-K patches but widely distributed.
Overall, our results indicate that CoT can induce more interpretable internal
structures in high-capacity LLMs, validating its role as a structured prompting
method.

</details>


### [34] [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)
*Xiaoyu Pan,Yang Bai,Ke Zou,Yang Zhou,Jun Zhou,Huazhu Fu,Yih-Chung Tham,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出了眼科领域的大模型评测基准EH-Benchmark，并通过多智能体三阶段框架，有效缓解了医学大模型在诊断中出现的幻觉现象，提高了模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前医学大模型（MLLMs）在眼科诊断中具有巨大潜力，但由于眼科知识有限、视觉定位和推理能力不足以及多模态数据稀缺，导致出现幻觉（hallucinations），影响准确诊断。此外，现有医学基准无法有效评估幻觉类型或提供应对方法。

Method: 提出了EH-Benchmark，这是一个专注于评估医学大模型在眼科诊断中幻觉表现的新型基准。基于任务和错误类型，将幻觉划分为视觉理解和逻辑组合两大类及多个子类。针对MLLMs主要依赖语言推理的问题，设计了三阶段的多智能体评估框架：知识检索阶段、任务案例阶段和结果验证阶段。

Result: 实验结果显示，所提出的多智能体框架能显著减少两类幻觉，提高了模型的准确性、可解释性和可靠性。

Conclusion: EH-Benchmark能够更有效地评估和缓解MLLMs中眼科诊断领域幻觉问题，帮助提升医疗AI系统的实战能力和安全性。

Abstract: Medical Large Language Models (MLLMs) play a crucial role in ophthalmic
diagnosis, holding significant potential to address vision-threatening
diseases. However, their accuracy is constrained by hallucinations stemming
from limited ophthalmic knowledge, insufficient visual localization and
reasoning capabilities, and a scarcity of multimodal ophthalmic data, which
collectively impede precise lesion detection and disease diagnosis.
Furthermore, existing medical benchmarks fail to effectively evaluate various
types of hallucinations or provide actionable solutions to mitigate them. To
address the above challenges, we introduce EH-Benchmark, a novel ophthalmology
benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'
hallucinations based on specific tasks and error types into two primary
classes: Visual Understanding and Logical Composition, each comprising multiple
subclasses. Given that MLLMs predominantly rely on language-based reasoning
rather than visual processing, we propose an agent-centric, three-phase
framework, including the Knowledge-Level Retrieval stage, the Task-Level Case
Studies stage, and the Result-Level Validation stage. Experimental results show
that our multi-agent framework significantly mitigates both types of
hallucinations, enhancing accuracy, interpretability, and reliability. Our
project is available at https://github.com/ppxy1/EH-Benchmark.

</details>


### [35] [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)
*Shalini Jangra,Suparna De,Nishanth Sastry,Saeed Fadaei*

Main category: cs.CL

TL;DR: 本研究开发了可安全分享的合成PII标注数据集和相关方法，助力社交媒体隐私风险研究，并对数据的有效性和安全性进行了系统评估，实现了公开共享。


<details>
  <summary>Details</summary>
Motivation: 当前社交平台（如Reddit）充斥着用户发布的包含个人信息标识符（PIIs）的内容，这些自我披露虽然可能促进社交互动，但也会带来隐私风险和安全威胁。研究如何识别和检索此类风险性自披露行为，却因缺乏公开的标注数据集受到限制。

Method: 提出一种创新方法，安全地合成PII类数据以便共享。具体包括：（1）建立包含19个易受害群体PII披露类别的分类体系；（2）利用三种主流大语言模型（Llama2-7B、Llama3-8B、zephyr-7b-beta）及序列化指令提示方式，生成仿真Reddit原帖风格的PII标注多文本片段数据集；（3）评估数据集的方法包括再现性等价（合成与原始数据模型训练效果相当），数据不可链接性，以及人类辨别难以区分性。

Result: 成功创建并开放了包含19类PII披露类型的合成多文本片段数据集和配套代码。三项指标评估表明：合成数据在模型训练效果与原始数据相当，无法通过常见方式链接到原用户，并且训练有素的人类难以区分其与原始数据的差异。

Conclusion: 本研究提供了一种可安全公开且具再现性的PII文本合成数据集生成方法，有助于推进社交媒体隐私风险相关研究。所发布的数据集与代码为后续PII识别研究提供了基础。

Abstract: Social platforms such as Reddit have a network of communities of shared
interests, with a prevalence of posts and comments from which one can infer
users' Personal Information Identifiers (PIIs). While such self-disclosures can
lead to rewarding social interactions, they pose privacy risks and the threat
of online harms. Research into the identification and retrieval of such risky
self-disclosures of PIIs is hampered by the lack of open-source labeled
datasets. To foster reproducible research into PII-revealing text detection, we
develop a novel methodology to create synthetic equivalents of PII-revealing
data that can be safely shared. Our contributions include creating a taxonomy
of 19 PII-revealing categories for vulnerable populations and the creation and
release of a synthetic PII-labeled multi-text span dataset generated from 3
text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and
zephyr-7b-beta, with sequential instruction prompting to resemble the original
Reddit posts. The utility of our methodology to generate this synthetic dataset
is evaluated with three metrics: First, we require reproducibility equivalence,
i.e., results from training a model on the synthetic data should be comparable
to those obtained by training the same models on the original posts. Second, we
require that the synthetic data be unlinkable to the original users, through
common mechanisms such as Google Search. Third, we wish to ensure that the
synthetic data be indistinguishable from the original, i.e., trained humans
should not be able to tell them apart. We release our dataset and code at
https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster
reproducible research into PII privacy risks in online social media.

</details>


### [36] [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
*Shuyu Guo,Zhaochun Ren*

Main category: cs.CL

TL;DR: ACC-RAG通过动态压缩优化了RAG推理，兼顾准确率和速度，超越当前固定压缩方案。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法为解决推理成本高问题广泛采用上下文压缩，但采用固定压缩率，会对简单问题压缩过度、复杂问题压缩不足，影响整体性能与效率。

Method: 提出ACC-RAG框架，结合分层压缩器和上下文选择器，根据输入复杂度动态调整压缩率。同时在多套问答数据集上进行评测，与固定压缩率及标准RAG方法进行了对比分析。

Result: ACC-RAG在推理速度上较标准RAG快4倍，同时准确率与现有最佳方法相当或更优，在Wikipedia和五个QA数据集上表现突出。

Conclusion: ACC-RAG能够自适应调整压缩率，实现在不降低准确率的情况下大幅提升推理效率，优于传统的固定压缩率RAG方法。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external knowledge but incurs significant inference costs due to lengthy
retrieved contexts. While context compression mitigates this issue, existing
methods apply fixed compression rates, over-compressing simple queries or
under-compressing complex ones. We propose Adaptive Context Compression for RAG
(ACC-RAG), a framework that dynamically adjusts compression rates based on
input complexity, optimizing inference efficiency without sacrificing accuracy.
ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with
a context selector to retain minimal sufficient information, akin to human
skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms
fixed-rate methods and matches/unlocks over 4 times faster inference versus
standard RAG while maintaining or improving accuracy.

</details>


### [37] [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)
*Baptiste Lefort,Eric Benhamou,Beatrice Guez,Jean-Jacques Ohana,Ethan Setrouk,Alban Etienne*

Main category: cs.CL

TL;DR: 本文提出将金融新闻情感与市场数据通过分层强化学习及轻量级大语言模型结合，显著超越传统基准，实现高收益和高稳健性，方法可复现并具推广性。


<details>
  <summary>Details</summary>
Motivation: 在投资组合优化领域，单一信号源（如价格指标）受到限制，金融新闻和市场指标结合能提升决策质量，但如何高效集成两者面临挑战。本文旨在融合自然语言情感信号与传统市场数据，实现更优投资组合管理。

Method: 本文提出一个分层架构，将轻量级大语言模型（LLMs）与深度强化学习（DRL）结合。采用三级结构：基础RL代理处理混合数据，元代理整合其决策，超代理基于市场与情感分析合并最终决策。

Result: 在2018-2024年数据上评估（以2000-2017年为训练集），该框架达到了年化收益率26%、夏普比率1.2，均优于等权重和标普500基准。

Conclusion: 分层RL结合LLMs的方法，可有效整合跨模态信号，提高投资组合优化的稳定性与表现，并支持可复现开放源代码。

Abstract: This paper presents a novel hierarchical framework for portfolio
optimization, integrating lightweight Large Language Models (LLMs) with Deep
Reinforcement Learning (DRL) to combine sentiment signals from financial news
with traditional market indicators. Our three-tier architecture employs base RL
agents to process hybrid data, meta-agents to aggregate their decisions, and a
super-agent to merge decisions based on market data and sentiment analysis.
Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework
achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming
equal-weighted and S&P 500 benchmarks. Key contributions include scalable
cross-modal integration, a hierarchical RL structure for enhanced stability,
and open-source reproducibility.

</details>


### [38] [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)
*Anthony C Davis,Burhan Sadiq,Tianmin Shu,Chien-Ming Huang*

Main category: cs.CL

TL;DR: 本文综述了视觉-语言模型结合符号信息系统提升模型可解释性和推理能力的技术路线，强调神经符号系统的优势和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型虽然表现优秀，但存在解释性差、集成新信息需重训、资源消耗大、逻辑推理有限等缺陷。为解决这些问题，有必要寻求能够兼顾解释性、可扩展性和推理能力的新方法。

Method: 通过系统性文献综述的方法，分类整理了将视觉-语言理解与外部符号信息系统结合以提升模型能力的各种技术路线。

Result: 文献回顾总结了神经符号系统如何结合外部符号推理模块与已训练的视觉-语言模型，提升推理能力、记忆力和输出解释性，并具有无须大规模重训整合新知识的优势。

Conclusion: 神经符号系统通过联用预训练VLM和外部符号系统，在提升视觉-语言理解模型可解释性、推理能力和灵活性方面展现巨大潜力，为未来AI技术发展提供了重要方向。

Abstract: Recent advances in visual-language machine learning models have demonstrated
exceptional ability to use natural language and understand visual scenes by
training on large, unstructured datasets. However, this training paradigm
cannot produce interpretable explanations for its outputs, requires retraining
to integrate new information, is highly resource-intensive, and struggles with
certain forms of logical reasoning. One promising solution involves integrating
neural networks with external symbolic information systems, forming neural
symbolic systems that can enhance reasoning and memory abilities. These neural
symbolic systems provide more interpretable explanations to their outputs and
the capacity to assimilate new information without extensive retraining.
Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural
component, augmented by external systems, offers a pragmatic approach to
realizing the benefits of neural-symbolic integration. This systematic
literature review aims to categorize techniques through which visual-language
understanding can be improved by interacting with external symbolic information
systems.

</details>


### [39] [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)
*Jingwei Zhao,Yuhua Wen,Qifei Li,Minchi Hu,Yingying Zhou,Jingyao Xue,Junyang Wu,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CL

TL;DR: 本文回顾了多模态意图识别领域的发展及主流深度学习方法，分析了现状、应用与挑战，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互对自然性的要求不断提高，意图识别技术需要不仅依赖于文本，还要融合语音、视觉和生理信号等多种模态数据，以更准确地理解用户意图。近年来，深度学习和Transformer模型推动了这一领域的发展。

Method: 本文综述了基于深度学习的意图识别方法，从单模态到多模态的技术转变，涉及相关数据集、技术方法、应用及现有挑战。重点关注于多模态意图识别（MIR）领域的进展。

Result: 总结了多模态意图识别领域的最新成果，评述了各类方法的表现和应用现状，指出进一步研究的方向和存在的问题。

Conclusion: 本综述为研究者提供了多模态意图识别领域的技术全景和未来建议，有助于推动人机交互的自然性和智能化。

Abstract: Intent recognition aims to identify users' underlying intentions,
traditionally focusing on text in natural language processing. With growing
demands for natural human-computer interaction, the field has evolved through
deep learning and multimodal approaches, incorporating data from audio, vision,
and physiological signals. Recently, the introduction of Transformer-based
models has led to notable breakthroughs in this domain. This article surveys
deep learning methods for intent recognition, covering the shift from unimodal
to multimodal techniques, relevant datasets, methodologies, applications, and
current challenges. It provides researchers with insights into the latest
developments in multimodal intent recognition (MIR) and directions for future
research.

</details>


### [40] [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
*Kathleen Mealey,Jonathan A. Karr Jr.,Priscila Saboia Moreira,Paul R. Brenner,Charles F. Vardeman II*

Main category: cs.CL

TL;DR: 论文系统性分析了知识图谱构建中，多种NLP和LLM工具在航空运维情报可靠环境下的零样本表现。结果显示现有工具难以满足机密与集成需求，作者开源了基准数据集并提出提升信任度的建议。


<details>
  <summary>Details</summary>
Motivation: 组织数据往往难以平衡数据机密性和数据集成，同时，NLP工具在诸如运维等特定领域的知识结构中表现有限。本文旨在探讨如何在机密环境下从组织数据中提取知识，并提升运营智能。

Method: 论文将知识抽取过程细分为命名实体识别（NER）、共指消解、实体链接和关系抽取几个功能模块，对16种NLP工具和大型语言模型（LLM）进行了系统评估。评估数据集源自美国联邦航空局公布的运维相关公开数据，验证了各工具的零样本性能，并确保数据处理在受控、机密环境下完成。

Result: 研究发现，现有NLP和LLM工具在受信环境下的性能存在显著局限。论文详细讨论了这些限制及其对航空等关键行业的技术成熟度的影响，与此同时，作者开源了用于基线测试的数据集。

Conclusion: 当前受控环境下的NLP和LLM工具在任务关键行业（如航空）应用时仍有诸多技术瓶颈，需进一步改进以提升可靠性与信任度。由此，论文提出了增强信任机制的建议，并开放数据集以推动后续研究。

Abstract: Deriving operational intelligence from organizational data repositories is a
key challenge due to the dichotomy of data confidentiality vs data integration
objectives, as well as the limitations of Natural Language Processing (NLP)
tools relative to the specific knowledge structure of domains such as
operations and maintenance. In this work, we discuss Knowledge Graph
construction and break down the Knowledge Extraction process into its Named
Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation
Extraction functional components. We then evaluate sixteen NLP tools in concert
with or in comparison to the rapidly advancing capabilities of Large Language
Models (LLMs). We focus on the operational and maintenance intelligence use
case for trusted applications in the aircraft industry. A baseline dataset is
derived from a rich public domain US Federal Aviation Administration dataset
focused on equipment failures or maintenance requirements. We assess the
zero-shot performance of NLP and LLM tools that can be operated within a
controlled, confidential environment (no data is sent to third parties). Based
on our observation of significant performance limitations, we discuss the
challenges related to trusted NLP and LLM tools as well as their Technical
Readiness Level for wider use in mission-critical industries such as aviation.
We conclude with recommendations to enhance trust and provide our open-source
curated dataset to support further baseline testing and evaluation.

</details>


### [41] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 本文系统性比较了5个主流LLM在金融年报分析中的表现，发现GPT表现最佳，不同模型对输入和内容敏感性强，建议根据场景选择合适模型。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLMs）在金融自然语言处理（FinNLP）领域表现突出，但主流LLMs之间的系统性对比尚不充分。同时，LLMs在金融分析中的影响力与日俱增，因此亟需对其进行全面评估。

Method: 本文选取五个主流LLM（GPT、Claude、Perplexity、Gemini和DeepSeek），基于‘Magnificent Seven’科技公司发布的10-K文件，设计领域专属的prompts，并采用三种方法进行评估：人工标注、自动化词汇-语义指标（如ROUGE、Cosine Similarity、Jaccard）、模型输出行为诊断（如提示级别变异性和跨模型相似性）。

Result: 实验显示，GPT在连贯性、语义一致性和上下文相关性方面表现最佳，其次是Claude和Perplexity；Gemini和DeepSeek输出波动较大、一致性较低。此外，不同公司的模型输出相似度和稳定性存在变化，提示其对prompts设计和源材料有一定敏感性。

Conclusion: 主流LLMs在金融NLP任务中表现存在明显差异，GPT整体表现最优；但模型对不同输入和数据具备敏感性，应结合具体应用场景选择和优化LLM。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


### [42] [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)
*Jinkun Zhao,Yuanshuai Wang,Xingjian Zhang,Ruibo Chen,Xingchuang Liao,Junle Wang,Lei Huang,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 提出AIOps领域专家协作新框架CoE-Ops，利用大语言模型和检索增强，显著提升各类AIOps任务的准确性和泛化能力，效果优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，AIOps已成为DevOps中的重要范式，但现有模型受限于特定领域知识，难以胜任多样化运维任务，单一模型适用范围有限。结合多模型可提升性能的理念在集成学习和大模型训练中已得到验证，因此亟需在AIOps领域实现模型的协作优化。

Method: 提出了一种专家协作框架（CoE-Ops），将通用大语言模型作为任务分类器，结合检索增强生成机制以增强对高层次和低层次AIOps任务（如QA、代码、构建、测试、故障分析、异常检测等）的处理能力。该方法在AIOps领域实现，并在DevOps-EVAL数据集进行了大量实验。

Result: 实验表明，CoE-Ops在高层AIOps任务的路由准确率上比现有方法提升72%，单模型AIOps任务解决准确率提升高达8%，相较于大规模专家混合模型（MoE）准确率提升达14%。

Conclusion: 通过引入多专家协作和大语言模型分类，结合检索增强机制，有效提升了AIOps任务的处理能力，在多项任务上显著超越传统模型和其他专家模型范式。

Abstract: With the rapid evolution of artificial intelligence, AIOps has emerged as a
prominent paradigm in DevOps. Lots of work has been proposed to improve the
performance of different AIOps phases. However, constrained by domain-specific
knowledge, a single model can only handle the operation requirement of a
specific task,such as log parser,root cause analysis. Meanwhile, combining
multiple models can achieve more efficient results, which have been proved in
both previous ensemble learning and the recent LLM training domain. Inspired by
these works,to address the similar challenges in AIOPS, this paper first
proposes a collaboration-of-expert framework(CoE-Ops) incorporating a
general-purpose large language model task classifier. A retrieval-augmented
generation mechanism is introduced to improve the framework's capability in
handling both Question-Answering tasks with high-level(Code,build,Test,etc.)
and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed
method is implemented in the AIOps domain, and extensive experiments are
conducted on the DevOps-EVAL dataset. Experimental results demonstrate that
CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps
tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement
over single AIOps models in DevOps problem resolution, and outperforms
larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

</details>


### [43] [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)
*Sumit Soman,H. G. Ranjani,Sujoy Roychowdhury,Venkata Dharma Surya Narayana Sastry,Akshat Jain,Pranav Gangrade,Ayaaz Khan*

Main category: cs.CL

TL;DR: 本文提出将VLM解析得到的流程图结构集成入文本RAG系统，在电信技术文档QA任务上提升了问答性能，并降低了部署推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统的基于文本的提取-生成（RAG）问答系统难以处理答案存在于图形（如流程图）中的技术文档问题，因此需要新的方法结合图形信息提升问答效果。

Method: 利用视觉大语言模型（VLM）解析流程图，获得其图结构表示，并将这些图结构嵌入进文本RAG系统，实现图文联合检索与问答。全过程包括技术文档处理、图像类型分类、图结构建立和文本嵌入管道整合。

Result: 基于专有电信产品文档构建的QA数据集上测试显示，经微调的VLM模型生成的流程图图结构与真实结构的编辑距离更低，证明其健壮性。结合图结构后使用文本嵌入模型（包括领域自适应模型）的问答检索性能良好。此外，推理阶段无需VLM，降低了部署成本。

Conclusion: 基于VLM提取的流程图结构能够增强RAG系统对图文信息联合问答能力，提高电信领域技术文档问答的准确性和检索效率，并兼具推理成本优势。

Abstract: Question-Answering (QA) from technical documents often involves questions
whose answers are present in figures, such as flowcharts or flow diagrams.
Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such
questions. We leverage graph representations of flowcharts obtained from Visual
large Language Models (VLMs) and incorporate them in a text-based RAG system to
show that this approach can enable image retrieval for QA in the telecom
domain. We present the end-to-end approach from processing technical documents,
classifying image types, building graph representations, and incorporating them
with the text embedding pipeline for efficient retrieval. We benchmark the same
on a QA dataset created based on proprietary telecom product information
documents. Results show that the graph representations obtained using a
fine-tuned VLM model have lower edit distance with respect to the ground truth,
which illustrate the robustness of these representations for flowchart images.
Further, the approach for QA using these representations gives good retrieval
performance using text-based embedding models, including a telecom-domain
adapted one. Our approach also alleviates the need for a VLM in inference,
which is an important cost benefit for deployed QA systems.

</details>


### [44] [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
*Bastien Le Guellec,Kokou Adambounou,Lisa C Adams,Thibault Agripnidis,Sung Soo Ahn,Radhia Ait Chalal,Tugba Akinci D Antonoli,Philippe Amouyel,Henrik Andersson,Raphael Bentegeac,Claudio Benzoni,Antonino Andrea Blandino,Felix Busch,Elif Can,Riccardo Cau,Armando Ugo Cavallo,Christelle Chavihot,Erwin Chiquete,Renato Cuocolo,Eugen Divjak,Gordana Ivanac,Barbara Dziadkowiec Macek,Armel Elogne,Salvatore Claudio Fanni,Carlos Ferrarotti,Claudia Fossataro,Federica Fossataro,Katarzyna Fulek,Michal Fulek,Pawel Gac,Martyna Gachowska,Ignacio Garcia Juarez,Marco Gatti,Natalia Gorelik,Alexia Maria Goulianou,Aghiles Hamroun,Nicolas Herinirina,Krzysztof Kraik,Dominik Krupka,Quentin Holay,Felipe Kitamura,Michail E Klontzas,Anna Kompanowska,Rafal Kompanowski,Alexandre Lefevre,Tristan Lemke,Maximilian Lindholz,Lukas Muller,Piotr Macek,Marcus Makowski,Luigi Mannacio,Aymen Meddeb,Antonio Natale,Beatrice Nguema Edzang,Adriana Ojeda,Yae Won Park,Federica Piccione,Andrea Ponsiglione,Malgorzata Poreba,Rafal Poreba,Philipp Prucker,Jean Pierre Pruvo,Rosa Alba Pugliesi,Feno Hasina Rabemanorintsoa,Vasileios Rafailidis,Katarzyna Resler,Jan Rotkegel,Luca Saba,Ezann Siebert,Arnaldo Stanzione,Ali Fuat Tekin,Liz Toapanta Yanchapaxi,Matthaios Triantafyllou,Ekaterini Tsaoulia,Evangelia Vassalou,Federica Vernuccio,Johan Wasselius,Weilang Wang,Szymon Urban,Adrian Wlodarczak,Szymon Wlodarczak,Andrzej Wysocki,Lina Xu,Tomasz Zatonski,Shuhang Zhang,Sebastian Ziegelmayer,Gregory Kuchcinski,Keno K Bressem*

Main category: cs.CL

TL;DR: 本文构建并公开了历史上最大、多语种、带详细标注的模拟影像学报告数据集PARROT，支持全球医学影像NLP研究，首次评估了人类分辨AI与人类影像报告的能力。


<details>
  <summary>Details</summary>
Motivation: 当前在医学影像领域，自然语言处理（NLP）应用的开发和验证受限于缺乏大型、多语言、开放获取且不受隐私限制的数据集。为推动NLP在影像报告分析中的进步，需要一个多中心、多语种、标签丰富的数据集。

Method: 2024年5月至9月，邀请放射科医师根据常规报告习惯编写虚构的影像学报告，每人至少提交20份，附带解剖部位、成像方式、临床背景等元数据，并对非英语报告提供英文翻译。所有报告分配ICD-10编码。还进行了人类vs. AI报告区分研究（154名参与者），判断报告是人类还是AI生成。

Result: 数据集共收录2658份来自21个国家、13种语言、76位作者的影像报告，覆盖多种成像方式和解剖部位。参与者在人类与AI报告区分实验中的准确率为53.9%，其中放射科医师表现更好（56.9%），均仅略高于随机水平。

Conclusion: PARROT为当前最大的开放式多语种影像报告数据集，为不同语种、地域和临床场景下NLP算法的开发验证提供了基础且无隐私限制的数据资源。

Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal
Annotated Radiology Reports for Open Testing), a large, multicentric,
open-access dataset of fictional radiology reports spanning multiple languages
for testing natural language processing applications in radiology. Materials
and Methods: From May to September 2024, radiologists were invited to
contribute fictional radiology reports following their standard reporting
practices. Contributors provided at least 20 reports with associated metadata
including anatomical region, imaging modality, clinical context, and for
non-English reports, English translations. All reports were assigned ICD-10
codes. A human vs. AI report differentiation study was conducted with 154
participants (radiologists, healthcare professionals, and non-healthcare
professionals) assessing whether reports were human-authored or AI-generated.
Results: The dataset comprises 2,658 radiology reports from 76 authors across
21 countries and 13 languages. Reports cover multiple imaging modalities (CT:
36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical
regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)
being most prevalent. In the differentiation study, participants achieved 53.9%
accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated
reports, with radiologists performing significantly better (56.9%, 95% CI:
53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the
largest open multilingual radiology report dataset, enabling development and
validation of natural language processing applications across linguistic,
geographic, and clinical boundaries without privacy constraints.

</details>


### [45] [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
*Rui Jiao,Yue Zhang,Jinku Li*

Main category: cs.CL

TL;DR: RELIANCE提出一套检测和优化大模型推理过程事实正确性的全新方法，显著提升高风险场景下推理的安全性和可靠性，并为未来相关研究和模型优化带来新思路。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在推理过程中，中间步骤经常出现事实性错误，尽管最终答案可能正确。这在医疗、法律和科学研究等高风险领域尤其危险，因为错误但自信的推理会误导用户，造成严重后果。

Method: 提出RELIANCE框架，包括三个核心部分：（1）基于反事实增强数据训练的事实检查分类器，用于检测推理链中的微妙事实错误；（2）采用群体相对策略优化（GRPO）的强化学习方法，利用多维奖励平衡事实性、连贯性及结构正确性；（3）机制可解释性模块，剖析事实性提升如何体现在模型推理过程中的激活变化。

Result: 在10个主流大模型上的评估发现，最先进模型如Claude-3.7和GPT-01的推理事实准确率分别只有81.93%和82.57%；RELIANCE框架能显著提升事实健壮性，最高提升可达49.90%，且在Math-500、AIME-2024及GPQA等基准测试中维持甚至提升整体表现。深入分析还揭示了事实性改进如何重塑模型推理路径，为未来基于激活导向优化的训练方法提供基础。

Conclusion: RELIANCE框架有效解决了大语言模型推理过程中的事实错误问题，大幅提升事实健壮性，并揭示激活层面上的改进机制，为未来相关模型训练方法提供了新方向。

Abstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy
for Confidence Enhancement), a novel framework addressing a critical
vulnerability in Large Language Models (LLMs): the prevalence of factual
inaccuracies within intermediate reasoning steps despite correct final answers.
This phenomenon poses substantial risks in high-stakes domains including
healthcare, legal analysis, and scientific research, where erroneous yet
confidently presented reasoning can mislead users into dangerous decisions. Our
framework integrates three core components: (1) a specialized fact-checking
classifier trained on counterfactually augmented data to detect subtle factual
inconsistencies within reasoning chains; (2) a Group Relative Policy
Optimization (GRPO) reinforcement learning approach that balances factuality,
coherence, and structural correctness through multi-dimensional rewards; and
(3) a mechanistic interpretability module examining how factuality improvements
manifest in model activations during reasoning processes. Extensive evaluation
across ten state-of-the-art models reveals concerning patterns: even leading
models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of
only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual
robustness (up to 49.90% improvement) while maintaining or improving
performance on challenging benchmarks including Math-500, AIME-2024, and GPQA.
Furthermore, our activation-level analysis provides actionable insights into
how factual enhancements reshape reasoning trajectories within model
architectures, establishing foundations for future training methodologies that
explicitly target factual robustness through activation-guided optimization.

</details>


### [46] [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)
*Paul Minchella,Loïc Verlingue,Stéphane Chrétien,Rémi Vaucher,Guillaume Metzler*

Main category: cs.CL

TL;DR: SigBERT通过对带时间戳的医学报告提取句向量并运用rough path几何特征，结合LASSO-Cox模型提升了肿瘤患者生存率预测，为利用EHR文本序列开展风险评估提供了高效新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生存分析方法在处理电子健康记录（EHR）中复杂的文本数据，尤其是其序列形式时表现不佳。随着医疗文本数据的不断增长，有效利用这些数据提升机器学习模型预测能力成为一个亟需解决的问题。

Method: 提出了SigBERT，一个创新的时间序列生存分析框架。具体方法是：首先对带时间戳的临床报告，通过词向量提取并平均形成句向量；其次应用rough path理论中的signature提取，获得每个患者的时序几何特征；最后将这些特征整合进LASSO惩罚的Cox模型中，评估个体化风险分数。

Result: 在真实世界的肿瘤学数据Léon Bérard Center corpus测试集上，SigBERT获得了C-index 0.75（标准差0.014）的优秀表现。

Conclusion: SigBERT框架能够有效整合和分析顺序医疗文本数据，提升风险预测能力，推动基于叙述文本的生存分析。

Abstract: Electronic medical reports (EHR) contain a vast amount of information that
can be leveraged for machine learning applications in healthcare. However,
existing survival analysis methods often struggle to effectively handle the
complexity of textual data, particularly in its sequential form. Here, we
propose SigBERT, an innovative temporal survival analysis framework designed to
efficiently process a large number of clinical reports per patient. SigBERT
processes timestamped medical reports by extracting and averaging word
embeddings into sentence embeddings. To capture temporal dynamics from the time
series of sentence embedding coordinates, we apply signature extraction from
rough path theory to derive geometric features for each patient, which
significantly enhance survival model performance by capturing complex temporal
dynamics. These features are then integrated into a LASSO-penalized Cox model
to estimate patient-specific risk scores. The model was trained and evaluated
on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a
C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT
integrates sequential medical data to enhance risk estimation, advancing
narrative-based survival analysis.

</details>


### [47] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

TL;DR: 本文提出用NLP结合适应性抽样来加速理赔数据库中代码算法的验证，大幅降低人力成本，对结论可靠性影响较小，可促进类似算法的常规评估。


<details>
  <summary>Details</summary>
Motivation: 大型理赔数据库分析中，基于代码的算法常用于识别健康结局等关键参数，但传统的手工核查参考标准极为耗时耗力，限制了这些算法的常规验证。

Method: 提出了一种高效验证流程，包括：1）利用自然语言处理（NLP）减少人工复核病历记录的时间；2）采用多波段自适应抽样，当算法表现达到预设精度标准时即可提前结束验证，避免无效劳动。

Result: 通过案例研究，NLP辅助下单份病历的审核时间减少了40%；多波段抽样及提前终止规则，可避免77%的病历被复核，同时对测量精确性的影响有限。

Conclusion: 该方法能够显著提升基于代码的算法常规验证效率，有助于数据库研究结果的可靠性评估。

Abstract: Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [48] [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)
*Naomi Omeonga wa Kayembe*

Main category: cs.CL

TL;DR: 本论文重新定义任意性，强调其作为一种中性且基础的结构机制，广泛影响人类语言、法律和社会系统运行，不仅有助于权威保护，也可应用于AI系统可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 当前主流批判传统往往将任意性等同于不公正，对其未有中性乃至积极结构性功能的全面认识。文章旨在重新界定任意性，强调其在系统高效运行和权威保护中的普遍作用。

Method: 文章从符号学理论出发，结合法律、社会系统和信息理论（香农熵模型），提出并验证了“动机->可证实性->可争议性”的链条，并通过“immotivization”等机制，分析任意性的实际运作与后果。

Result: 通过理论拓展和实例剖析，作者证明了任意性作为结构性机制，不仅适用于语言，还能普遍适用于法律与社会互动，并对人工智能系统的可解释性分析提出了新路径。提出了任意性为A=H(L|M)的形式化表达。

Conclusion: 作者提出了任意性（arbitrariness）并非一种规范上的缺陷或支配的表现，而是一种结构和组织人类系统与互动的基本功能机制，具有积极的中性作用。任意性通过结构性不透明性保护了权威并解释了系统内的诸多现象。

Abstract: This article redefines arbitrariness not as a normative flaw or a symptom of
domination, but as a foundational functional mechanism structuring human
systems and interactions. Diverging from critical traditions that conflate
arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a
property enabling systems - linguistic, legal, or social - to operate
effectively while withholding their internal rationale. Building on Ferdinand
de Saussure's concept of l'arbitraire du signe, the analysis extends this
principle beyond language to demonstrate its cross-domain applicability,
particularly in law and social dynamics. The paper introduces the "Motivation
-> Constatability -> Contestability" chain, arguing that motivation functions
as a crucial interface rendering an act's logic vulnerable to intersubjective
contestation. When this chain is broken through mechanisms like
"immotivization" or "Conflict Lateralization" (exemplified by "the blur of the
wolf drowned in the fish"), acts produce binding effects without exposing their
rationale, thus precluding justiciability. This structural opacity, while
appearing illogical, is a deliberate design protecting authority from
accountability. Drawing on Shannon's entropy model, the paper formalizes
arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern
theory of arbitrariness as a neutral operator central to control as well as
care, an overlooked dimension of interpersonal relations. While primarily
developed through human social systems, this framework also illuminates a new
pathway for analyzing explainability in advanced artificial intelligence
systems.

</details>


### [49] [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
*Chengqian Ma,Wei Tao,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出一个中英双语的语音对话基准数据集及LLM辅助评测方法，有助于更好衡量和改进语音对话模型在真实场景下的理解和仿真能力。


<details>
  <summary>Details</summary>
Motivation: 语音对话模型（SDMs）日益流行，但其在实际对话理解和模拟能力上的研究尚不充分，尤其是对比已有大量基准测试的文本大模型。语音交互由于语义和语音的多义性、上下文依赖性等因素更为复杂，亟需系统性的评测手段。

Method: 作者构建了包含1079个中英文实例的基准数据集，并提出一种基于大语言模型（LLM）的评测方法，与人工评价高度一致。

Result: 该数据集和评测方法有助于全面评估和分析语音对话模型在应对实际交流中歧义、多伦上下文等复杂情境时的表现。

Conclusion: 本文提出的数据集和评测方法能够有效促进对语音对话模型（SDMs）在实际人机对话理解和仿真能力方面的深入研究。

Abstract: Spoken Dialogue Models (SDMs) have recently attracted significant attention
for their ability to generate voice responses directly to users' spoken
queries. Despite their increasing popularity, there exists a gap in research
focused on comprehensively understanding their practical effectiveness in
comprehending and emulating human conversations. This is especially true
compared to text-based Large Language Models (LLMs), which benefit from
extensive benchmarking. Human voice interactions are inherently more complex
than text due to characteristics unique to spoken dialogue. Ambiguity poses one
challenge, stemming from semantic factors like polysemy, as well as
phonological aspects such as heterograph, heteronyms, and stress patterns.
Additionally, context-dependency, like omission, coreference, and multi-turn
interaction, adds further complexity to human conversational dynamics. To
illuminate the current state of SDM development and to address these
challenges, we present a benchmark dataset in this paper, which comprises 1,079
instances in English and Chinese. Accompanied by an LLM-based evaluation method
that closely aligns with human judgment, this dataset facilitates a
comprehensive exploration of the performance of SDMs in tackling these
practical challenges.

</details>


### [50] [Math Natural Language Inference: this should be easy!](https://arxiv.org/abs/2507.23063)
*Valeria de Paiva,Qiyue Gao,Hai Hu,Pavel Kovalev,Yikang Liu,Lawrence S. Moss,Zhiheng Qian*

Main category: cs.CL

TL;DR: 本文提出并系统分析了“数学文本自然语言推理”任务，发现多数LLM表决可接近人工标注质量，但在理解和推理数学内容时仍有明显不足，并提供了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）是否能在处理数学文本的自然语言推理（NLI）任务上取得良好表现。此前这类任务集中探讨通用文本，数学文本由于逻辑性和专用语较强，面临更多挑战。

Method: 构建一个以真实数学文献为前提的NLI数据集，由有相关领域经验的专家提供假设句及标签，并比较人工与LLM生成假设的不同数据集。同时考察多模型间一致性及其在该任务上的整体表现。

Result: 结果显示：1）通过多LLM的多数投票，在部分场景下能达到接近人工标注的数据质量；2）LLM仍难以完全理解和推理复杂数学语言，甚至在基础推理上存在失败；3）与旧一代模型相比，现有LLM不再容易发生基于假设句的伪推理。

Conclusion: LLM在处理数学NLI任务上取得了一定进步，但当前模型还无法完全胜任这类复杂推理任务。作者还公布了数据集支持后续研究。

Abstract: We ask whether contemporary LLMs are able to perform natural language
inference (NLI) tasks on mathematical texts. We call this the Math NLI problem.
We construct a corpus of Math NLI pairs whose premises are from extant
mathematical text and whose hypotheses and gold labels were provided by people
with experience in both research-level mathematics and also in the NLI field.
We also investigate the quality of corpora using the same premises but whose
hypotheses are provided by LLMs themselves. We not only investigate the
performance but also the inter-group consistency of the diverse group of LLMs.
We have both positive and negative findings. Among our positive findings: in
some settings, using a majority vote of LLMs is approximately equivalent to
using human-labeled data in the Math NLI area. On the negative side: LLMs still
struggle with mathematical language. They occasionally fail at even basic
inferences. Current models are not as prone to hypothesis-only "inference" in
our data the way the previous generation had been. In addition to our findings,
we also provide our corpora as data to support future work on Math NLI.

</details>


### [51] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
*Diego Garat,Guillermo Moncecchi,Dina Wonsever*

Main category: cs.CL

TL;DR: 本文提出自动生成基于FrameNet数据库的任务特定提示词，仅依靠LLM的上下文学习能力，无需额外微调，实现了在框架语义解析任务中的高准确率。


<details>
  <summary>Details</summary>
Motivation: 框架语义解析（FSP）是自然语言处理中理解语句语义结构的重要任务，传统方法依赖模型微调，成本较高。作者希望利用大型语言模型（LLM）的上下文学习能力，减少对模型微调的依赖。

Method: 自动从FrameNet数据库提取框架定义和带注释实例，构建帧识别和语义角色标注的提示词，分别指导六种不同的LLM，通过上下文学习完成FSP任务。

Result: 在与暴力事件相关的框架子集上，使用自动生成的任务特定提示指导六种不同的LLM，Frame Identification (FI) 子任务F1分数为94.3%，Frame Semantic Role Labeling (FSRL) 子任务F1分数为77.4%，取得了有竞争力的结果。

Conclusion: 上下文学习为领域特定的框架语义解析任务提供了有效且实用的替代方案，无需模型微调即可得到高性能。

Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling
their arguments according to Frame Semantics. This paper investigates the use
of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP
without model fine-tuning. We propose a method that automatically generates
task-specific prompts for the Frame Identification (FI) and Frame Semantic Role
Labeling (FSRL) subtasks, relying solely on the FrameNet database. These
prompts, constructed from frame definitions and annotated examples, are used to
guide six different LLMs. Experiments are conducted on a subset of frames
related to violent events. The method achieves competitive results, with F1
scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers
a practical and effective alternative to traditional fine-tuning for
domain-specific FSP tasks.

</details>


### [52] [Context-aware Rotary Position Embedding](https://arxiv.org/abs/2507.23083)
*Ali Veisi,Delaram Fartoot,Hamidreza Amirzadeh*

Main category: cs.CL

TL;DR: 本文提出了一种新的上下文感知旋转位置编码（CARoPE），通过动态调整每个head的编码提升Transformer对序列和上下文的建模能力。CARoPE在多个实验中显著优于现有RoPE等方法，并实现更快更稳定的训练。


<details>
  <summary>Details</summary>
Motivation: RoPE（旋转位置编码）尽管高效并适用于相对位置编码，但由于其静态且与输入无关的正弦频率模式，限制了对上下文敏感关系的建模能力。本文提出的动机在于提升位置编码对于输入和上下文的适应性和表达能力。

Method: 提出了一种新的位置编码方法CARoPE（Context-Aware Rotary Positional Embedding，具备上下文感知的旋转位置编码），它基于token嵌入动态生成每个注意力头的频率模式，通过对token嵌入进行有界变换，计算输入相关的相位偏移，并将其整合进旋转机制中，同时保持RoPE的高效性和简单性。

Result: 在FineWeb-Edu-10B数据集和基于GPT-2的模型（下一个token预测任务上）评测，CARoPE在各种常用的位置编码方法下表现出更低的困惑度，尤其是在更长上下文情况下优势明显。此外，CARoPE训练速度更快且模型更稳定。

Conclusion: CARoPE是一种高效、可扩展且表达能力强的位置编码方式，比现有的方法在保持计算效率的同时带来更优的模型性能。

Abstract: Positional encoding is a vital component of Transformer architectures,
enabling models to incorporate sequence order into self-attention mechanisms.
Rotary Positional Embeddings (RoPE) have become a widely adopted solution due
to their compatibility with relative position encoding and computational
efficiency. However, RoPE relies on static, input-independent sinusoidal
frequency patterns, limiting its ability to model context-sensitive
relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional
Embedding), a novel generalization of RoPE that dynamically generates
head-specific frequency patterns conditioned on token embeddings. This design
introduces token- and context-sensitive positional representations while
preserving RoPE efficiency and architectural simplicity. CARoPE computes
input-dependent phase shifts using a bounded transformation of token embeddings
and integrates them into the rotary mechanism across attention heads. We
evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on
next-token prediction tasks. Experimental results show that CARoPE consistently
outperforms RoPE and other common positional encoding baselines, achieving
significantly lower perplexity, even at longer context lengths. Additionally,
CARoPE enables faster training throughput without sacrificing model stability.
These findings demonstrate that CARoPE offers a scalable, expressive, and
efficient upgrade to existing positional encoding strategies in Transformer
models.

</details>
