<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 38]
- [cs.DM](#cs.DM) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文系统梳理了领域知识在需求工程中的应用现状，总结了方法与挑战，为今后可持续集成领域知识提供了研究方向与理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管领域知识对需求工程（RE）的成功至关重要，但科学文献中尚缺乏有关如何有效利用和应用领域知识于RE的系统性总结。现有研究缺乏对领域知识在需求工程实践中融合方法、技术和工具的全面梳理。

Method: 本文采用系统性映射研究方法，结合了数据库检索和递归的前向、后向雪球式检索，全面搜集相关文献。

Result: 共纳入了75篇相关文献，分析得出领域知识用于需求工程时主要关注的需求类型、常见的质量属性及在领域知识构建、获取和维护过程中遇到的问题。为领域知识融入RE指明了未来研究方向，强调可扩展、自动化和可持续解决方案的开发。

Conclusion: 本研究为知识驱动的需求工程提供了概念性和方法论基础，帮助研究者和实践者识别现有成熟方法及待解决的问题，推动领域知识与需求工程的深度融合。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [2] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文系统综述了ML系统敏捷管理的文献，总结了应用进展、挑战和未来研究方向，指出实证研究不足是主要空白。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统开发动态性强、变化快，传统项目管理方法难以应对。敏捷方法具备灵活和增量特性，但尚不清楚如何针对ML系统有效应用，需要梳理当前研究现状。

Method: 本文采用系统性映射综述，结合数据库检索及前向、后向滚雪球法，选取和分析相关文献。

Result: 共筛选出2008-2024年间的27篇论文，归纳出8个敏捷管理框架和8类关键主题，如迭代灵活性、ML特有工件、最小可用模型。最大挑战为ML任务的努力估算。

Conclusion: 本研究系统梳理了ML系统敏捷管理的研究现状和存在挑战，明确了当前成果和空白领域，强调未来需加强实证验证。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [3] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 为应对数据爆炸和本体生成需求，作者提出了一种用Python和rdflib库自动将Neo4j数据转化为OWL本体的简便方法，并在药品不良事件数据集上验证，提升了知识图谱构建效率，促进药品安全监测。


<details>
  <summary>Details</summary>
Motivation: 数据和知识的快速增长使得本体生成的系统方法变得至关重要。随着数据量日增和内容频繁变化，对数据库用于信息存储和知识图谱构建的需求日益迫切。当前Neo4j数据库和Web Ontology Language (OWL) 的集成面临困难，尤其是需要DL语法知识，这对很多用户来说并不友好。因此，迫切需要一种更简便的方法来弥补这一差距。

Method: 提出一种基于Python及其rdflib库的用户友好方法，支持本体开发。通过将FDA不良事件报告系统（FAERS）数据库导入Neo4j，并开发自动生成本体类及其公理的Python脚本，实现与OWL的无缝对接。

Result: 本方法实现了从Neo4j数据库到OWL本体的自动转换，简化了本体生成流程，使非专业用户也能便捷生成知识图谱，提升了数据整合与利用效率。

Conclusion: 论文提出的新方法为快速增长的不良药物事件数据集提供了本体生成的实用解决方案，有助于优化药物安全监测和公共卫生决策。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [4] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本研究开发并评估了五个实际领域的RAG系统，由用户从六方面进行验证，总结了十二条关键落地经验，对RAG在现实中实施的优劣与挑战进行了系统分析。


<details>
  <summary>Details</summary>
Motivation: RAG系统在解决大语言模型事实准确性和上下文相关性方面展现出巨大潜力，但缺乏基于真实应用、结合用户参与，并系统性总结经验教训的实证研究。

Method: 开发了五个面向治理、网络安全、农业、工业研究和医学诊断等领域的RAG系统，集成多语言OCR、向量语义检索和领域定制LLM，通过本地服务器或云API部署。采用基于Web的问卷评价，邀请100名用户，从六个维度（易用性、相关性、透明度、响应速度、准确性、推荐意愿）进行评估。同时总结开发过程中的实际经验和挑战。

Result: 系统在不同应用场景下得到用户评估，并基于用户反馈和开发实践，归纳出十二条涉及技术、运营和伦理的关键经验教训，反映了RAG系统在实际落地中的可靠性和可用性挑战。

Conclusion: RAG系统在实际应用中具有广泛前景，但在技术实现、运维管理及伦理规范方面仍面临不少障碍，需重视这些实际问题以推动RAG更可靠、可用地落地。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [5] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 该论文提出了一种在人类建议指导下，用强化学习开发复杂模型转换的方法和框架，实验表明即使人类建议不确定，依然能有效提升RL表现，推动了人机协同工程方向的研究。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程中的复杂模型转换开发难度大、易出错且手动开发不可行；纯强化学习在复杂问题中性能有限。引入人类指导帮助提升RL在此类复杂任务中的表现。

Method: 提出一种技术框架，把用户自定义的模型转换映射为RL原语，并在RL程序中执行，通过引入（可能不确定的）人类建议引导RL，寻找最优的模型转换序列。通过实验评估分析不同人类建议的不确定性和及时性对方法的影响。

Result: 实验表明，人的指导显著提升了RL在复杂模型转换开发中的表现，使开发更高效。所提框架可权衡建议的确定性与及时性。

Conclusion: 该论文提出的方法可以通过将不确定的人类建议引入到强化学习流程中，显著提升强化学习在复杂模型转换开发中的性能和效率。实践结果证明，即便人类建议存在不确定性，依然能有效优化复杂模型转换的开发过程。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [6] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: 该文提出了IFMA-VD漏洞检测框架，通过建模函数间多边关联与超图卷积，有效提升了漏洞检测准确率，尤其在复杂交互场景下具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测方法主要关注单个函数，忽视了函数间复杂的多边关联，这会导致无法检测到隐藏在交互关系中的漏洞。

Method: 提出了一个多函数多边关联分析框架IFMA-VD。该方法首先将函数解析为代码属性图提取函数内部特征。接着，划分程序依赖图，构造代码行为超图，并将行为特征编码为超边。最后，利用超图网络捕获函数间多边关联，加强漏洞检测能力。

Result: 在三个广泛使用的漏洞数据集上，IFMA-VD在F-measure和Recall等指标上优于基线方法。实验也表明，多边关联特征能够增强代码特征表达，并在真实世界数据集上验证了方法的有效性。

Conclusion: 通过利用函数间多边关联及超图卷积，IFMA-VD实现了对漏洞检测性能的提升，并证明了该方法在实际场景下的有效性。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [7] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 本文提出了用于需求工程的合成数据生成与优化新方法，并在多个分类任务中验证其有效性。部分任务中，合成数据效果甚至优于人工数据，为缓解数据短缺问题提供了切实方案。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在需求工程（AI4RE）领域发展受限，主要原因之一是公开、标注的需求数据集短缺。合成数据潜力巨大，但针对需求数据合成的生成质量优化方法仍不成熟，亟需系统性探索。

Method: 提出了一种增强的合成需求数据生成方法——Synthline v1。该方法在初代基础上，融入了多样化生成策略和数据筛选技术，对提示工程、多样性采样、自动化提示优化（如PACE）及基于相似度的筛选在四类任务中的效果进行了系统评估。

Result: 多样性采样法在效用和多样性上显著优于单样本法，F1 提升6~44分；自动提示优化（PACE）对特定任务（如功能分类）有较大提升，但对其它任务有损失；基于相似度筛选提升多样性但损害分类性能；部分任务（如安全性和缺陷检测）上，合成数据生成效果已超越人工标注数据。

Conclusion: 系统化的合成数据生成方法能有效缓解AI4RE领域的数据稀缺问题，部分合成数据甚至优于人类标注数据。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [8] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 本文提出T^3框架，将大语言模型、CoT推理与树搜索结合，显著提升了自动程序修复的精度，并为自动化调试提供坚实基础。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复（APR）强调减少人工干预，提高修复效率。尽管大规模语言模型（LLM）和Chain-of-Thought（CoT）技术在推理能力上有突破，但在处理APR所需的复杂逻辑和多步推理时，应用依然有限。

Method: 系统评测了多种常见的CoT技术在APR任务中的表现，并提出了创新性的T^3框架，将LLM的推理能力与树搜索相结合，提高了候选修复方案的精度。

Result: T^3框架不仅明显提升了修复方案的精度，还能为样本选择与修复策略优化提供指导，增强了自动调试的效率与鲁棒性。

Conclusion: LLM和CoT技术结合树搜索方法，在APR领域具备巨大潜力，T^3框架为高效自动程序修复提供了有效的实现路径。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [9] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: 作者提出了KOALA工具，可在JetBrains IDE中灵活采集学生编程过程的细致数据，并用实际案例验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的学生编程任务数据收集工具存在局限性，例如无法控制收集代码的粒度、无法收集编程环境的特定事件，且配置复杂，这限制了研究人员和教育者获取有价值数据的能力。

Method: 提出了KOALA工具，这是一款可高度配置的插件，能够在JetBrains IDE中收集学生解题过程中的代码快照和IDE特性使用情况。该插件可安装于IDE，支持任务分发、IDE特性启用/禁用、以及运行调查问卷；可根据配置采集代码快照、IDE操作（如运行、调试）、热键使用、文件焦点切换等数据，并上传至配套服务器。数据可转换为标准的ProgSnap2格式。

Result: 在两个课程中，作者用KOALA工具收集了28名学生在IDE中解题的数据，并从中展示了若干有意义的分析洞见。

Conclusion: KOALA工具能够弥补现有工具的不足，便捷高效地采集更为细致和丰富的学生编程活动数据，为教育和研究提供有力数据支持。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [10] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: 本文通过文献综述、实际案例和开发者问卷，分析了微前端在工业界实现的利弊，认为其适用性依赖于企业背景和系统结构，具有一定优势但非唯一选择。


<details>
  <summary>Details</summary>
Motivation: 微前端架构旨在提高可扩展性、系统弹性和团队独立性，但也带来了更高的复杂度和基础设施要求。作者希望探讨在工业界什么情况下采用微前端是值得的，尤其是对于已经存在的微服务后台系统。

Method: 首先，调研了微前端的学术和灰色文献现状。其次，在已有微服务架构的手工艺品市场公司中实际实现了微前端。最后，通过向开发者发放半开放性问卷，对实施效果进行了评估。

Result: 尽管微前端架构在公司中的实施是成功的，但实际上其他架构（如单一前端）也能达到类似结果。驱动采用微前端的关键因素是企业逐步剥离单体应用和采用微服务，这使得利用既有基础设施和团队知识更为方便。

Conclusion: 微前端架构的采用要结合企业上下文和已有架构基础。在合适的环境下，它通过促进基础设施复用和团队协作，能够带来便利，但并非唯一或必需选择。采用与否需权衡复杂度收益，与现有系统的耦合程度密切相关。

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [11] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: 本文针对IoT数据与业务流程数据集成带来的挑战，提出并实现了一个基于共通需求的核心数据模型，经验证能有效提升数据共享和协作效率。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）技术的发展促使各行业将IoT设备与业务流程（BP）集成。IoT设备产生大量数据，有助于通过流程挖掘（PM）获得业务流程洞察。但由于IoT数据与传统流程数据特性差异较大，集成二者具有挑战性。目前已有多种数据模型，但各自聚焦不同侧面，导致领域内的数据交换和协作受到阻碍。

Method: 本文提出了一种综合现有模型核心特征的核心数据模型。该模型基于共通需求构建，并通过Python原型实现，针对多个用例进行评估。

Result: 实验表明，所提出的核心模型能较好满足数据集成与共享的共通需求，支持不同用例的数据交换和协作。

Conclusion: 提出的核心数据模型能够解决IoT数据与流程数据集成碎片化问题，促进流程挖掘领域的数据共享与协作。

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [Pebble Games and Algebraic Proof Systems](https://arxiv.org/abs/2506.21149)
*Lisa-Marie Jaser,Jacobo Toran*

Main category: cs.LO

TL;DR: 本文首次系统建立和推广了pebbling游戏与三类主流代数证明系统之间的复杂性并行关系，不仅得出度和空间的精确对应，还实现了对这些系统度与空间复杂性的分离，拓展了复杂性理论和证明理论的研究基础。


<details>
  <summary>Details</summary>
Motivation: 研究pebbling游戏与代数证明系统（如Nullstellensatz, Monomial Calculus, Polynomial Calculus）之间的关系，探索二者在复杂性理论中的对应和平行性。此前仅在部分特例有相关结论，尚未全面揭示不同游戏和证明系统之间的联系。

Method: 分析针对0pebbling公式Peb(G)的反驳，建立了有向无环图（DAG）G上一些pebbling策略与代数证明系统中相应反驳之间的数学映射和复杂度关系。对比和推广了之前文献对reversible pebbling和Nullstellensatz的分析方法。

Result: 对于任意带单一汇点的DAG G，如果存在度为s、大小为t的Monomial Calculus反驳，则存在空间为s、时间为t+s的black pebbling策略，反之亦成立。在空间复杂度上，三类pebbling游戏策略的空间需求与各自代数证明系统反驳的变量空间复杂度精确匹配。这些结果揭示了不同证明系统在度与大小、空间复杂度上的分离。

Conclusion: 该研究深化了pebbling游戏与代数证明系统之间的结构性联系，提供了复杂度测度新的分离结果，并丰富了对代数证明系统内在复杂性的理解，对理论计算机科学的复杂性理论有重要促进作用。

Abstract: Analyzing refutations of the well known 0pebbling formulas Peb$(G)$ we prove
some new strong connections between pebble games and algebraic proof system,
showing that there is a parallelism between the reversible, black and
black-white pebbling games on one side, and the three algebraic proof systems
Nullstellensatz, Monomial Calculus and Polynomial Calculus on the other side.
In particular we prove that for any DAG $G$ with a single sink, if there is a
Monomial Calculus refutation for Peb$(G)$ having simultaneously degree $s$ and
size $t$ then there is a black pebbling strategy on $G$ with space $s$ and time
$t+s$. Also if there is a black pebbling strategy for $G$ with space $s$ and
time $t$ it is possible to extract from it a MC refutation for Peb$(G)$ having
simultaneously degree $s$ and size $ts$. These results are analogous to those
proven in {deRezende et al.21} for the case of reversible pebbling and
Nullstellensatz. Using them we prove degree separations between NS, MC and PC,
as well as strong degree-size tradeoffs for MC.
  We also notice that for any directed acyclic graph $G$ the space needed in a
pebbling strategy on $G$, for the three versions of the game, reversible, black
and black-white, exactly matches the variable space complexity of a refutation
of the corresponding pebbling formula Peb$(G)$ in each of the algebraic proof
systems NS, MC and PC. Using known pebbling bounds on graphs, this connection
implies separations between the corresponding variable space measures.

</details>


### [13] [Deciding Robust Instances of an Escape Problem for Dynamical Systems in Euclidean Space](https://arxiv.org/abs/2506.21481)
*Eike Neumann*

Main category: cs.LO

TL;DR: 文中提出了一种在实数比特模型下判定连续动力系统逃逸问题的健全部分算法，理论上具有最大适用范围，并首次将其应用推广到Mandelbrot集合等标志性案例，完善了相关可计算性问题的解答。


<details>
  <summary>Details</summary>
Motivation: 研究点在连续映射下是否逃离闭子集的问题，属于动力系统与可计算性的交叉问题。之前对这类问题的可计算性和判决过程理解有限，尤其是对一般连续函数及相关著名集合（如Mandelbrot集合）。

Method: 提出了一种健全的部分判定方法，针对在比特模型中的实数计算情境，对该判定算法的完备性和密集性进行了理论证明。同时，将该方法应用于特殊函数族，例如仿射线性系统和二次复多项式，并讨论其与超稳性猜想的关系。

Result: 该方法的停机集合包含所有同类健全部分算法的停机集合，在所有对小扰动稳定的问题实例上总能终止，且停机集合在问题空间中稠密。对特定函数族也能得到完备的决策方法，并为Mandelbrot集合可计算性提供了另一种证明。

Conclusion: 作者提出的方法统一且具有最优性（在健全算法范畴内），理论上拓展了连续动力系统逃逸问题的可决定性边界，并将其应用拓展到众多重要数学对象。

Abstract: We study the problem of deciding whether a point escapes a closed subset of
$\mathbb{R}^d$ under the iteration of a continuous map $f \colon \mathbb{R}^d
\to \mathbb{R}^d$ in the bit-model of real computation. We give a sound partial
decision method for this problem which is complete in the sense that its
halting set contains the halting set of all sound partial decision methods for
the problem. Equivalently, our decision method terminates on all problem
instances whose answer is robust under all sufficiently small perturbations of
the function. We further show that the halting set of our algorithm is dense in
the set of all problem instances. While our algorithm applies to general
continuous functions, we demonstrate that it also yields complete decision
methods for much more rigid function families: affine linear systems and
quadratic complex polynomials. In the latter case, completeness is subject to
the density of hyperbolicity conjecture in complex dynamics. This in particular
yields an alternative proof of Hertling's (2004) conditional answer to a
question raised by Penrose (1989) regarding the computability of the Mandelbrot
set.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 该论文针对表格数据上的概率性问答，提出结合贝叶斯网络和大语言模型的新方法，并取得了比现有方法更好的实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答系统善于直接查找答案，但面对需要不确定性推理的概率性问题表现不佳。为提升在此类复杂问题下的表现，提出新的基准和方法。

Method: 方法包括：从表格数据中自动诱导贝叶斯网络、将自然语言问题转化为概率查询、最后结合大语言模型（LLM）生成最终答案，实现神经-符号混合推理。

Result: 实验证明，所提方法在基准任务上优于现有方法，混合神经-符号推理显著增强了概率问答能力。

Conclusion: 论文提出的方法通过将表格数据转化为贝叶斯网络，并结合大语言模型，实现了更强的在不确定性下的概率推理问答任务，对比传统方法取得了显著提升。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [15] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 现有多语言基准并不充分反映大模型实际跨语种任务表现，本文提出更具实际意义的多语言功能性评测集，并揭示了模型在不同语言下的性能差异与不足。


<details>
  <summary>Details</summary>
Motivation: 现有多语言大模型的评估主要依赖于静态数据基准，如Belebele、M-MMLU与M-GSM，但这类评估无法充分揭示模型在实际多语场景中的表现与鲁棒性。作者为弥补这一缺陷，旨在提出更具功能性和实用意义的多语言测试方法。

Method: 通过将现有的功能性基准测试模板从英文扩展翻译到法语、西班牙语、印地语、阿拉伯语与约鲁巴语，作者构建了两个多语言功能性基准：CL-GSM Symbolic与CL-IFEval，以评估模型在多语环境下的实际能力与健壮性。

Result: 结果显示，不同静态多语言基准与实际功能性表现的吻合度差异较大：M-GSM与CL-GSM Symbolic之间在英、法、西三语下模型性能分别下降24%、17%、18%；Belebele与CL-IFEval之间各语言表现下降15-24%，而M-MMLU与CL-IFEval下降仅0.5%-3%。不同语言下的模型鲁棒性显著不同，阿拉伯语和英语表现尤为稳定。

Conclusion: 静态多语言基准不能全面反映模型在实际多语场景下的功能性表现和鲁棒性。建议未来多用功能性多语言基准，以更真实地评估和提升多语言大模型的能力。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [16] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLM能生成表面新颖的研究点子，但在实际执行和研究成效上不如人类专家，反映出当前LLM生成创新点子的局限性。缺乏执行验证时，仅凭新颖性难以评估想法是否优质。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在生成新颖的科研想法方面表现出色，甚至有时超过人类专家，但“新颖性”不足以判断一个想法的实际价值。真正好的想法不仅要看起来新颖，还需要在实际研究中产生更好的结果。因此，作者旨在检验AI生成的想法在实际执行后的研究成效。

Method: 作者招募了43位专家研究人员，随机分配由人类专家或LLM生成的研究想法，并要求每位专家花超过100小时实施该想法，最终提交一篇4页的短论文。随后，所有执行后的项目由其他NLP领域专家进行盲审，通过比较执行前后项目的各项评审分数数据，分析两类想法的表现差异。

Result: LLM生成的点子在执行后的所有指标（新颖性、激动性、有效性、整体评分）上，分数下降显著高于专家原创点子（p < 0.05），原本点子阶段LLM表现较好的人机差距被缩小甚至逆转。在执行研究的整体评审分数中，许多评价项出现人类点子得分高于LLM点子的现象。

Conclusion: 目前的LLM在生成表面上新颖的科研点子有一定能力，但在转化为可执行且高效的研究点子时存在明显不足。对想法的新颖性评价若缺少执行验证，可能无法反映其真正价值。未来需要关注如何提升LLM生成点子的实际可行性及评价标准的科学性。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [17] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: MultiFinRAG是一种面向金融问答的跨模态检索增强生成系统，通过结构化提取、智能检索和动态回退策略，有效整合文本、表格、图片信息，显著优于传统LLM和ChatGPT-4o，提升金融复杂问答的准确率。


<details>
  <summary>Details</summary>
Motivation: 金融文件如10-K、10-Q报告和投资者陈述，内容繁杂且页面众多，包含文本、表格、图像等多种模态。现有LLM和RAG方法在处理跨模态推理任务时受到token长度、版式丢失和上下文碎片化等限制，难以有效回答复杂的金融问答。

Method: 提出MultiFinRAG框架，先将表格和图像分组，输入轻量级多模态LLM，生成结构化JSON和简洁文本摘要。将这些输出与文本一起，采用模态感知的相似度阈值进行索引检索。采用分层回退策略，按需动态从文本扩展到文本+表格+图像，实现跨模态推理的同时减小无关上下文。

Result: MultiFinRAG在普通硬件上运行时，处理涉及文本、表格、图像及其组合推理的复杂金融问答任务，比免费版ChatGPT-4o准确率高出19个百分点。

Conclusion: MultiFinRAG通过高效跨模态检索增强和分层推理方法，在实际硬件条件下大幅提升金融文件QA任务表现。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [18] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 本研究将社科领域常用暴力行为情景问卷（VBVQ）应用于六个主流大语言模型，对它们在处理暴力场景和人口学人设变换下的表现进行了评测。结果显示，模型表面回答与其潜在偏好往往不符，并且在涉及不同人群时易出现与现实研究相悖的偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）被用于检测和应对网络暴力内容，但其对于现实中道德模糊场景的推理能力尚未充分研究。作者希望通过更严谨的方法评估LLMs在真实社会冲突情境中的表现。

Method: 利用经社科验证的Violent Behavior Vignette Questionnaire（VBVQ）来测试LLMs对日常冲突的反应，并通过设定带有种族、年龄和地理身份变量的人设提示，以分析模型的潜在偏见。评估对象为6种不同背景下开发的LLMs，采用统一的零样本推理（zero-shot）设置。

Result: 发现（1）LLMs的表层文本生成与其内在偏好（对暴力回应的倾向）常常不一致；（2）LLMs在不同人口统计人设下的暴力倾向有明显变化，并且这些变化常常与社会科学、心理学和犯罪学的既有研究结论相矛盾。

Conclusion: LLMs在处理暴力相关场景时，存在深层偏见和推理能力不足，而且这些偏差在不同人设下表现不一，可能影响其在现实世界中应对和评估暴力内容的可靠性。该研究为LLMs的实际部署提出了警示。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [19] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 本研究系统梳理医疗领域事实核查的实际挑战，指出简单端到端核查模式难以适应医学复杂性，应关注交互式沟通。


<details>
  <summary>Details</summary>
Motivation: 随着科技进步，自动事实核查取得了显著进展，尤其在医疗领域，高风险决策和文献复杂度让事实核查系统需求增加。然而当前这些系统在医学领域长期未被广泛采用。

Method: 本研究首次系统性地观察了临床专家如何在实际环境中，针对来自社交媒体的真实医学主张，检索并综合医学证据进行核查。

Result: 研究揭示了医学领域端到端事实核查面临的关键挑战：难以将野外声称与临床试验等科学证据关联、主张本身含糊且动机多样、以及事实判断本质上的主观性。

Conclusion: 作者认为事实核查应被视为一个交互式沟通问题，而非简单的端到端过程，以适应医学事实核查的特殊挑战。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [20] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 本文通过持续预训练、参数高效微调等创新方法，提高了语言模型在数据利用、效率和适应性方面的表现，有助于迈向更强大的通用人工智能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型（LM）规模和复杂性的增加，如何高效且稳健地将其适配到具体任务仍具挑战性。现有的微调方法对未标注数据利用不足，并且在数据较少时易过拟合，计算成本高，限制了实际应用。

Method: 论文提出了一系列新方法，包括：1）针对无标签数据，提出了一种新的持续预训练技术；2）提出了参数高效的微调方法，大幅降低内存和计算消耗；3）改进了有监督微调方法，使模型在标注数据稀缺时仍能良好地执行指令；4）开发了新评测方法和基准，如多跳空间推理任务，更全面地评估模型能力。

Result: 通过在多种NLP任务上的广泛实验，提出的方法显著提升了语言模型的鲁棒性、效率及泛化能力，适应性更强。

Conclusion: 这些方法推动了更高效、稳健的语言模型发展，促进实现通用人工智能。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [21] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 本文提出了自动化多语种数据筛选、去重及重平衡流程，可对任意语言高质量地构建训练数据，显著提升了多语种大语言模型性能，并公开发布了覆盖1000+语言、体量达20TB的大型数据集FineWeb2。


<details>
  <summary>Details</summary>
Motivation: 多语种大模型的预训练数据集难以获得，现有数据过滤和去重流程难以适应大量不同语言。亟需一种能够自动适配任意语言的数据集筛选方案。

Method: 提出基于FineWeb的数据集自动筛选和去重新流程，并对九种语言在实际流程选择上进行了充分的消融实验。引入了基于可量化标准的新任务筛选流程，并提出了基于重复次数和质量的新型数据重平衡方法。

Result: 新提出的自动化多语种数据集筛选流程创建的非英文数据集，生成的大模型效果超过了以往的数据集。同时，数据重平衡方法进一步提升了模型表现。最终，该流程被扩展至1000多种语言，处理了约100份Common Crawl快照，生成了20TB/50亿文档的大型多语种数据集FineWeb2。

Conclusion: 提出的FineWeb2及其自动数据集筛选与重平衡流程，极大提升了多语种大语言模型预训练数据的质量和适应性，为多语种模型的发展提供了坚实的数据基础。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [22] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: KaLM-Embedding-V2是一种小体量高性能文本嵌入模型，通过全新结构与多阶段训练，在中英文评测大幅超越同尺寸模型，并逼近甚至优于数倍更大模型，极具实际推广价值。


<details>
  <summary>Details</summary>
Motivation: 现有的通用文本嵌入模型在性能和模型体量之间通常存在明显权衡，如何构建表现优越且轻量化的通用嵌入模型仍是研究难点。

Method: 提出KaLM-Embedding-V2嵌入模型，通过三阶段训练（大规模弱监督预训练、高质量检索与非检索数据微调、参数平均提升泛化）、全双向Transformer结构（无因果掩码，均值池化生成定长嵌入）、设计困难样本聚焦机制与在线难负样本混合方法，并大规模收集预训练和微调数据。

Result: 在中英文通用嵌入基准（MTEB）评测中，KaLM-Embedding-V2显著优于同类尺寸模型，表现能够媲美3倍~26倍更大嵌入模型，单模型参数量不足1B。

Conclusion: KaLM-Embedding-V2实现了小体量嵌入模型的新标杆，在保证紧凑体积的同时兼具优越表现与强泛化能力。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [23] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文提出用元训练方式，让微调模拟prompt泛化，无需真实标签。实验显示模型经一次微调，表现接近prompt，提升对新知识的学习能力，对长文本建模有新启示。


<details>
  <summary>Details</summary>
Motivation: 将新信息融入到语言模型中常用的方法是修改prompt或参数微调，但二者各有优缺点。该论文关注能否使微调方法像prompt一样实现快速泛化。

Method: 提出了一种元训练方法，通过让模型在微调时模拟prompt方式对新信息进行推断。具体做法是利用梯度型元学习工具，并以模型自身prompt预测作为训练目标，无需真实标签。

Result: 使用该方法，模型经过一次梯度更新后，在某些任务（如“reversal curse”）表现能部分或全部恢复到prompt条件下的性能，并能高效处理长文本中的问题。

Conclusion: 合适的初始化后，梯度下降比预期更具表达性。这为长上下文建模提供了新思路，并加深了对梯度学习泛化能力的理解。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [24] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本研究提出将16PF人格模型与特质强度控制机制应用于大语言模型，通过语义锚定等手段，实现了更细粒度和动态可控的人格建模，推动了AI的拟人化发展。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于Big Five人格框架，粒度较粗，且无法控制人格特质的表达强度，难以实现真正拟人化和可控的人格建模。

Method: 扩展Machine Personality Inventory (MPI)，从传统Big Five模型升级到16PF模型，引入Specific Attribute Control(SAC)结构框架以动态诱发和评估大语言模型的人格特质强度，并采用形容词语义锚定和行为问题，从五个维度对特质强度进行量化和引导。

Result: 实验显示，将人格特质强度建模为连续光谱，个性表达更连贯、可控，且目标特质强度的变化会影响相关特质，证明了大语言模型内部能够协同处理多维人格结构。

Conclusion: 通过对大语言模型引入16PF人格模型及特质强度的可控表达，实现了更加连贯和可控的人格表现。人格特质不是孤立切换，而是多维连续光谱，人格特质的强度变化会系统性地影响相关特质。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [25] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文探讨了如何通过持续集成与工程实践，保障和提升大规模文本嵌入基准（MTEB）的可复现性、可扩展性和社区协作能力，成为行业标准的评测平台。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入评测平台（如MTEB）虽已成为标准，但持续的可复现性和易扩展性仍面临工程挑战，有必要通过良好工程实践保障其标准地位。

Method: 主要通过建立健壮的持续集成流程、数据集完整性校验、自动化测试执行、评测结果泛化能力评估，以及社区贡献管理和任务/数据集扩展等工程措施。

Result: MTEB已经规模化扩展，覆盖更多任务与数据集，同时维护了高质量和相关性。工程措施有效提升了平台于学术和工业界的影响力与适用性。

Conclusion: 作者通过工程化的方式，提升了MTEB评测平台的可复现性和可扩展性，确保其在模型评测领域中的稳定与实用性。其经验对于其他机器学习评测框架的维护同样具有参考意义。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [26] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 本文提出了CA-Ben基准，系统评估了六个主流LLM在印度财会考试环境下的多领域能力，发现其在法律和定量推理领域表现尚有不足，建议未来采用混合推理和增强检索策略改善氛围。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）在自然语言处理领域取得了显著进展，但其在金融特定知识及实际应用方面的有效性仍不明确，尤其在印度庞大复杂的财会环境下缺乏有针对性的评测。

Method: 本文构建了CA-Ben，这是一个基于印度特许会计师考试（ICAI）各阶段考试题目整理的结构化问答基准，用以系统评估LLM在财会、法律及定量推理方面的能力。采用标准化评测流程，测试了六个主流LLM（GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet、Microsoft Phi 4）。

Result: 实验显示各模型在不同领域表现差异显著，其中Claude 3.5 Sonnet和GPT-4o在概念性和法律推理方面表现最佳，但在数值计算和法律解析上仍存在挑战。

Conclusion: 当前LLMs在特定金融领域尤其是定量分析和精准法律解释方面仍有局限，未来应探索混合推理及检索增强等新方法以提升相关能力。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [27] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 作者提出了一个半监督、可扩展且统一的查询分类框架（SSUF），通过知识增强、标签增强和结构增强模块，缓解了电商查询短小、上下文信息稀缺和标签依赖难题，实验结果表明显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电商查询分类任务中的子任务（如意图和类别预测）很重要，但现有方法受限于短查询、标签信息无法利用、依赖用户点击行为采样，容易陷入马太效应恶性循环，且没有统一高效的算法框架。

Method: 提出了半监督可扩展统一框架（SSUF），包含三个增强模块：知识增强（引入外部知识强化查询表示）、标签增强（利用标签语义和半监督信号降低对后验标签依赖）、结构增强（利用标签间复杂关系提升标签表示），各模块可插拔并针对不同子任务灵活配置输入特征。

Result: 大量离线和在线A/B实验表明，SSUF在各项指标上显著优于最先进模型。

Conclusion: SSUF框架解决了查询分类中先验信息稀缺和子任务分散等痛点，提升了分类效果和算法优化效率。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [28] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 本文发布了迄今最大、最复杂的多方对话立场检测数据集MT2-CSD，并提出结合大语言模型推理力的LLM-CRAN新方法，在新数据集上显著优于现有方法，推动了对话立场检测研究。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体分析中，自动立场检测对于舆情挖掘至关重要。但现有方法大多面向单一实例，难以处理真实社交媒体中的多方讨论场景，主要原因是缺乏能真实反映社交互动动态的数据集，制约了对话立场检测研究的进展。

Method: 提出了一个多目标、多轮对话立场检测数据集MT2-CSD（包含24,457条标注实例），为立场检测带来了更高的对话复杂性。同时，提出了LLM增强对话关系注意力网络（LLM-CRAN），融合大模型推理能力以提升对话理解，并在该数据集上进行了系统实验。

Result: LLM-CRAN在新提出的MT2-CSD数据集上，对话立场检测任务显著优于当前强基线模型。

Conclusion: 该工作通过构建大型真实多方对话立场检测数据集和提出融合大模型推理能力的新架构，有效提升了复杂对话中的立场检测效果，推动了领域发展。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [29] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 本文针对现有多模态句表征方法存在的对齐与语义分歧问题，提出了DALR方法，在各项标准任务上超越了现有最佳方法，有效提升了多模态句表示能力。


<details>
  <summary>Details</summary>
Motivation: 以往多模态句子表征方法大多仅在粗粒度水平对齐图像和文本，未能有效解决跨模态对齐偏差和模态内语义分歧等问题，影响句子表征质量。

Method: 提出DALR（Dual-level Alignment Learning for Multimodal Sentence Representation），包括一致性学习模块，通过软化负样本并利用辅助任务的语义相似度进行细粒度跨模态对齐，以及将排序蒸馏集成到全局模态内对齐学习中，提高表征质量。

Result: 在语义文本相似度（STS）和迁移（TR）任务上的全面实验证明，所提方法在所有评测任务中均优于最新方法。

Conclusion: DALR方法通过细粒度跨模态对齐和模态内全局关系建模，显著提升了多模态句子表征质量。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [30] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出ComRAG框架，将静态知识与动态历史问答整合，有效提升社区问答检索与生成效果，并显著减少存储和延迟成本，适用于工业级应用。


<details>
  <summary>Details</summary>
Motivation: 当前社区问答平台作为社区知识库很重要，但如何有效地实时利用历史交互和领域知识仍然是挑战，现有方法对于外部知识和历史问答数据利用不足，且缺乏适用于工业部署的记忆机制。

Method: 提出了ComRAG，一个面向实时工业级社区问答的检索增强生成框架。该方法通过基于质心的记忆机制，将静态知识和动态历史问答对进行集成，涵盖高效的检索、生成和存储机制。

Result: 在三个工业级社区问答数据集上，ComRAG方法在所有基线方法中表现最好。其向量相似性提升最高达25.9%，查询延迟降低8.7%~23.3%，且多轮迭代下数据块增长率从20.23%降至2.06%。

Conclusion: ComRAG有效融合了静态知识和动态历史问答，具备高效检索、生成与存储能力，切实提升了工业级问答系统性能，具有实际部署优势。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [31] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: Progtuning是一种新型的渐进式参数高效微调框架，可根据不同Transformer区块的贡献动态调整更新比例，实现资源节约和高性能，并具备良好的兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型规模不断增大，微调全部参数的成本越来越高。现有参数高效微调方法虽然减少了参数量，但通常对Transformer各层贡献的差异视而不见，导致资源利用极为低效。

Method: 提出了一种结合渐进学习(Progtuning)的微调方法。该方法根据各Transformer块对性能的实际贡献，逐步减少被更新的区块数量，实现更高效的参数更新和资源分配。

Result: 通过Progtuning，更新参数数量减少约25%，且与全量微调持平的性能。同时，该方法能与主流参数高效微调法结合，在多种应用场景下都表现优异。

Conclusion: Progtuning实现了参数更新数量的大幅减少，在保证性能的同时，大幅优化了硬件/计算资源分配，为参数高效微调迈出了重要一步。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [32] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Cosmos提出了一种基于扩散的高效文本生成方法，通过构建压缩潜空间，显著提升了推理速度，同时保持或超越主流方法的生成质量，在多项任务取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流文本生成采用自回归语言模型，但其顺序生成导致生成速度慢、难以保持全局连贯性。扩散模型支持并行生成和灵活控制，但在高维的token表示下应用受限。该文旨在突破现有方法瓶颈，实现更快且有质量保证的文本生成。

Method: 提出了一种全新的文本生成方法Cosmos，该方法在专为扩散建模设计且压缩、平滑的潜在空间中执行。该潜在空间通过一个自动编码器学习，自动编码器训练时兼顾token级重建和与预训练语言编码器的激活对齐，以实现语义基础的鲁棒性并支持基于扰动的增强。通过这种方式，Cosmos实现了有效的扩散建模和文本生成。

Result: 实验证明，Cosmos能在保持与token级扩散模型相当的生成质量情况下，将文本表示压缩8倍。随着潜在序列长度的增加，Cosmos性能甚至超过现有的扩散与自回归基线。其在故事生成、问题生成、摘要和去毒化等四项任务中，与多种生成范式对比表现优异。推理速度比自回归或扩散基线快2倍多。

Conclusion: Cosmos不仅在生成质量上优于或等同于现有方法，还实现了更高的效率，在多个文本生成任务中展现了广泛的适用性和优越性，为大规模、高效、可控的文本生成提供了新的解决方案。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [33] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 该研究提出将文本提示引入变换器式语音轮换预测模型，可实现对对话时机的直观、动态控制，并且提升了模型准确率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的轮换预测模型缺乏通过指令（如“更快”，“更冷静”等）动态调整对话策略的能力，难以适应不同用户和语境。

Method: 基于变换器（transformer）的语音活动预测模型（VAP），将文本提示嵌入引入通道内及跨通道的变换器中。利用大语言模型（LLM）生成合成的文本提示句来补充缺乏真实数据。

Result: 实验结果显示，新模型不仅提升了预测准确性，还可根据不同文本提示灵活调整轮换行为。

Conclusion: 本文提出的方法提升了对话轮换预测的准确性，并能根据文本提示有效调节轮换时机。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [34] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 通过采用基于句法相似性的few-shot提示，能提升LLM在术语抽取任务上的性能，且这一方法适用于多领域，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动术语抽取（ATE）对领域特定表达的识别对于机器翻译和信息检索等下游任务至关重要，但大语言模型（LLMs）在该任务中的潜力尚未被充分研究。

Method: 提出了一种基于检索的提示策略，在few-shot场景下按照句法相似性（而非语义相似性）选择示例，用以指导LLM进行术语抽取。该句法检索方法具有领域无关性，更有效地捕捉术语边界。

Result: 在三个专门的ATE基准数据集上实验，句法检索方法提升了F1分数。分析结果还显示，查询句与检索示例间的词汇重叠对效果有影响。

Conclusion: 结果表明，句法线索对提升LLM在术语抽取任务中的适应性和效果具有重要作用。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [35] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出Agent-RewardBench基准，能够多维度细致评估多模态智能体的奖励建模能力。实验结果表明先进模型表现有限，说明该领域有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型（MLLMs）支持的多模态智能体在实际任务（如网页导航、具身智能）中展现出巨大潜力，但由于缺乏有效的外部反馈，智能体自我纠错和泛化能力有限。尽管引入奖励模型作为外部反馈是一个可行手段，但在如何选择和评估奖励模型方面存在明显空白。因此，亟需为智能体构建专门的奖励基准。

Method: 提出了Agent-RewardBench，这是一套针对多模态智能体奖励建模能力的基准测试。该基准包含三个核心特性：（1）多维度和真实场景评估，涵盖感知、规划、安全等7类场景；（2）步骤级奖励评估，可以细致分析任务执行过程每一步的奖励与表现；（3）任务难度适中且数据高质量，基于10种不同模型的多样数据抽样，难度控制且人工核查以确保数据有效性。

Result: 实验显示，即使是当前最先进的多模态模型，在该基准上的表现也十分有限，反映出智能体奖励建模方面仍需专门化训练和研究。

Conclusion: Agent-RewardBench为多模态大模型的智能体奖励建模提供了一个标准化、细致且高质量的评测工具，推动这一领域进一步发展。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [36] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 研究发现：虽然LLM越来越强大，但在经典侦探小说文本风格下，传统统计检测方法依然能有效识别虚假文本。Gemini模型的欺骗性稍有提升，而GPT在升级后未表现出更强欺骗性，说明检测方法仍具备前景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）能够生成具有欺骗性的虚假文本，这在学术写作、产品评论和政治新闻等领域引发了对人工生成文本检测的需求。本文关注于新一代LLM不断变大和检测器不断进步之间是否会形成“军备竞赛”的问题。

Method: 作者通过统计分类器检测以经典侦探小说风格生成的虚假文本，比较不同LLM（如GPT和Gemini）在文本欺骗性和可检测性方面的表现。

Result: 实验表明，Gemini模型在0.5版本升级后，生成欺骗性文本的能力有所提升，而GPT模型则没有明显变化。统计分类器在有限资源下依然能有效检测虚假文本。

Conclusion: 即使大型模型生成的虚假文本越来越逼真，基于统计方法的检测仍有望保持可靠，除非未来模型架构发生本质变革。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [37] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: 本文提出Double-Checker框架，通过自我批评和多轮自我修正，大幅提升LLM推理能力，成绩显著优于原有模型，为更可靠LLM提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有的慢思考大型语言模型（LLM）虽然能进行类似反思的推理过程，但它们在生成有意义的自我批评和优化已有解决方案的能力上依然有限。解决如何让LLM更好地自我反思和纠错，是提升其推理能力和可靠性的关键。

Method: 提出了一种名为Double-Checker的框架，通过微调1,730个人工筛选的自我批评案例，使LLM在推理过程中，能明确地自我批评和迭代优化自身输出，直到模型自认为达到正确解答。

Result: Double-Checker被验证能显著提升长链推理（Long-CoT）LLMs在多个推理基准上的表现。尤其是在AIME这样的挑战性测试上，pass@1的得分从原本的4.4%提升到18.2%。

Conclusion: 通过结构化的自我批评和迭代优化，Double-Checker极大增强了LLM的推理能力，表明这种方法有望提升LLM的可信度和实际应用价值。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [38] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 提出用轻量级、微调过的编码器模型（如RoBERTa、NomicBERT），在大型语言模型回答前检测查询是否有文档依据，准确率媲美主流大模型，但推理更快、更省资源。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以通过外部上下文提升自然语言处理任务的表现，但当上下文缺少关键信息时，模型常会产生无根据的猜测或依赖内部知识，因此如何保证模型回答有据可依（groundedness）成为提高事实一致性与可信度的关键问题。

Method: 在大型语言模型生成答案前，先检测查询是否能被当前文档上下文支撑，使用如RoBERTa和NomicBERT之类的轻量级、专用的编码器模型，经过精心数据集微调以实现此检测。

Result: 经过微调的RoBERTa和NomicBERT在groundedness检测任务上的准确率与SOTA大模型（如Llama3 8B和GPT4o）相当，但推理延时大幅降低。

Conclusion: 通过在生成答案前使用高效的检测模型判别查询是否有据可依，可以降低推理成本、节省资源并提高效率，且无需牺牲准确度。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [39] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文表明仅用文本大模型，也能较好地从视觉对话中提取指称表达，但强调这依然有多模态不可替代的局限。


<details>
  <summary>Details</summary>
Motivation: 探讨在视觉语境下的对话中，仅利用文本信息（而非多模态信息）来提取指称表达的可行性及有效性。

Method: 采用文本自回归语言模型，对预训练大型语言模型（LLM）进行再调优，使其通过预测下一个词来标注对话中涉及视觉实体的提及范围。

Result: 实验结果表明，即便仅使用中等规模的LLM、较小训练数据集和高效参数微调，仅依赖文本信息也能有效提取指称表达。

Conclusion: 语言上下文本身对视觉实例的指称检测具有重要作用，但归根结底这仍然是一个多模态任务，单一模态方法存在固有局限性。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [40] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 本文提出了一种结合格雷马斯符号学方格的LLM文学分析新框架GLASS，首次建立了相关数据集和评价指标，并在专家对比和经典作品分析中取得优异表现，有效推动了AI赋能文学研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然擅长理解和生成文本，但在对具有深刻思想和复杂叙事的文学作品提供专业批评时存在困难。为解决这一限制，作者提出了结构化分析框架。

Method: 本文提出了GLASS（基于格雷马斯符号学方格的文学分析）框架，利用格雷马斯符号学方格，对叙事结构和深层含义进行快速、结构化剖析。同时，构建了首个基于GSS的文学批评数据集，并采用LLM-as-a-judge范式提出了定量评估指标。

Result: GLASS框架在多个作品和多种LLM上与专家批评结果进行对比，表现出很高的性能。框架应用于39部经典作品，产生了原创且高质量的分析，弥补了现有研究的不足。

Conclusion: 该研究为文学研究和教育领域提供了基于AI的工具，并对文学参与的认知机制提供了新的洞见。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [41] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG框架针对现实复杂与多意图用户查询对RAG系统的挑战，通过三大创新模块（查询理解与拆分、意图感知检索、重排序生成）显著提升系统对噪声及复杂数据的处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在真实环境中面对用户输入时经常受到噪声、歧义、多意图等问题困扰，而现有RAG方法多数是在较干净的数据上训练或评估，难以应对真实复杂情境。因此，提升RAG系统对复杂与杂乱输入的处理能力成为了迫切需求。

Method: 本文提出Omni-RAG框架，包含三个关键模块：（1）深度查询理解与分解，利用LLM和定制化提示对查询进行去噪并拆分多意图为有结构的子查询；（2）意图感知的知识检索，对每个子查询分别进行检索并聚合结果；（3）重排序与生成，使用重排序器优化文档选择，最终通过LLM链式思维提示生成回复。

Result: Omni-RAG提升了RAG系统在真实、开放环境下处理噪声、多意图等复杂用户查询的健壮性和有效性，能够更好地满足实际应用场景对智能问答系统的需求。

Conclusion: Omni-RAG通过LLM辅助预处理与多模块优化，显著增强了RAG系统面对真实世界复杂输入时的性能，朝着满足实际应用要求的目标迈进。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [42] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合领域知识与大语言模型的框架，实现了对欺骗性对话和语义漂移的高准确率检测，在多个数据集上获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 动态平台上的对话中，因语言模式不断变化与概念漂移，欺诈对话检测变得越来越困难。传统大语言模型在风险场景下存在上下文歧义和幻觉等挑战。为提升高风险NLP任务中的表现，需引入结构化领域知识和漂移检测机制。

Method: 提出了一个基于领域知识强化的大语言模型（DK-LLM）框架。该架构包含三个模块：1）用于检测虚假或欺骗性对话的DK-LLM模块；2）OCDD语义漂移检测单元；3）用于对语义漂移进行良性/恶意分类的第二个DK-LLM模块。通过在虚假评论数据集和包含多种欺诈与垃圾攻击的SEConvo对话数据集上进行验证。

Result: 系统在检测虚假对话及语义漂移分类上均取得高准确率。LLaMA模型在结构化提示引导下，分类准确率达98%。与零样本基线对比，融入领域知识和漂移意识显著提升了性能、可解释性和鲁棒性。

Conclusion: 将领域知识与大语言模型结合，能有效提升对欺诈和语义漂移的检测准确性与鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)\-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk\-sensitive scenarios. To
address these challenges, we present a Domain Knowledge (DK)\-Enhanced LLM
framework that integrates pretrained LLMs with structured, task\-specific
insights to perform fraud and concept drift detection. The proposed
architecture consists of three main components: (1) a DK\-LLM module to detect
fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine
whether a semantic shift has occurred; and (3) a second DK\-LLM module to
classify the drift as either benign or fraudulent. We first validate the value
of domain knowledge using a fake review dataset and then apply our full
framework to SEConvo, a multiturn dialogue dataset that includes various types
of fraud and spam attacks. Results show that our system detects fake
conversations with high accuracy and effectively classifies the nature of
drift. Guided by structured prompts, the LLaMA\-based implementation achieves
98\% classification accuracy. Comparative studies against zero\-shot baselines
demonstrate that incorporating domain knowledge and drift awareness
significantly improves performance, interpretability, and robustness in
high\-stakes NLP applications.

</details>


### [43] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 本论文提出并评测了Text2Cypher多语言测试集，发现大模型对英语支持最好，对土耳其语最弱，提示应加强多语言环境下的数据库查询生成研究。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言到数据库查询接口（如Text2SQL、Text2SPARQL、Text2Cypher）的研究大多集中于英语，其他语言的评测和支持非常有限。为提高数据库可访问性，研究多语言环境下模型的性能变得尤为重要。

Method: 本论文首次将Text2Cypher任务推向多语言场景，通过将英文问题翻译为西班牙语和土耳其语，并保留原始Cypher查询，构建并发布了多语言测试集。研究还采用标准化提示词和评测指标，比较了多种基础大模型在各语言下的表现，并分析了任务提示语翻译对性能的影响。

Result: 实验表明，模型在英语文本下表现最佳，其次是西班牙语，表现最差的是土耳其语。原因可能与各语言训练数据的可获得性及语言特点有关。同时，翻译任务提示语对评价指标几乎无影响。

Conclusion: 应推动数据库查询生成领域更具包容性的多语言评估和开发。未来可关注模式本地化和多语言微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [44] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 本文提出一种面向实时语音对话的偏好对齐框架，通过大规模AI反馈数据微调，显著提升了语音对话系统在多轮对话中的准确性、安全性及上下文契合度，对构建高质量自然对话系统具有重要参考意义。


<details>
  <summary>Details</summary>
Motivation: 当前的偏好学习方法主要针对文本语言模型，难以直接应用到具备丰富互动特征（如打断、插话）和无明确说话轮次划分的实时语音对话中。该研究旨在解决语音对话系统中偏好对齐方面的挑战。

Method: 创建包含超过15万组优选对照的数据集，对原始多轮语音对话以AI反馈方式进行注释，涵盖语言内容和时间语境的偏好。采用离线对齐方法对全双工自回归语音到语音模型进行微调。

Result: 实验证明，对通用对话的反馈能持续提升语音对话模型的准确性、安全性及语境对齐能力。对模型上线后进行全方位人工评估，验证其在多轮对话中的有效改进。

Conclusion: 模型通过大量带AI反馈的真实语音对话数据进行偏好对齐和微调后，能更自然地应对实时语音互动中的复杂动态，对自然对话系统建设具有重要意义。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [45] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 本文提出在Transformer中内建TopK稀疏激活，替代传统SAE方式，实现神经元级的稳定、可靠解释力和可控性，实验验证保持性能、提升分析便利性，对未来模型解释性研究有重要推动意义。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAE）虽然被广泛用于解释和分析Transformer语言模型的激活空间，但其有效性受限于训练方式及特定架构选择的影响，导致在功能稳定性和可对比性方面存在显著不足。尤其是SAE是在模型训练之后进行的，难以区分特征学习失败的原因是SAE本身能力不足还是原始语言模型未蕴含该信息。

Method: 提出在Transformer特定层引入TopK激活函数，使模型隐藏状态直接具备TopK SAE的稀疏特征，无需额外后训练，以获得与SAE同等的可解释性。通过这种架构改进，实现了语言模型与SAE的功能合一，加强了解释力。

Result: TopK LMs（加入TopK激活函数后的LM）在保持原有模型能力的同时，提升了可解释性和特征稳定性，实现了较好的模型规模、计算效率和可解释性的权衡。实验显示TopK LMs学习到的稀疏特征可用于神经元干预与神经元分析，跨检查点和层面的特征对比也更为一致、可靠。

Conclusion: TopK LMs为模型可解释性和可控性研究提供了稳定、可靠的新工具，消除了以往SAE带来的技术障碍，有助于更深入理解语言模型如何学习和表征概念。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [46] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 本文系统比较了不同强化学习方法在大语言模型微调中的表现，发现半在线和全在线RL目标在可验证与不可验证任务中普遍优于离线方法，且多任务训练能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习在大语言模型微调中的应用多集中于离线场景，对半在线与全在线等动态场景及可验证和不可验证任务的优化效果研究较少。该论文旨在系统分析不同学习模式和任务类型下强化学习微调方法的有效性。

Method: 论文分别在可验证的数学任务和不可验证的指令跟随任务上进行实验，并涉及离线、半在线与全在线三种训练模式。重点比较了Direct Preference Optimization (DPO) 和 Group Reward Policy Optimization (GRPO) 两种目标函数在这些模式下的表现，并对训练过程及超参数选择进行了详细剖析。此外，论文还探索了多任务（可验证和不可验证任务并行训练）对性能的影响。

Result: 实验发现：1）DPO与GRPO在不同在线化程度下表现和收敛性相似，且明显优于离线方法；2）多任务训练可以提升不同任务类型上的性能。

Conclusion: DPO和GRPO等在线目标函数能够显著优于传统离线方法，且两者在多种场景下表现接近；多任务（可验证与不可验证任务联合训练）对于提升大模型微调效果具有重要意义。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [47] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出通过模拟用户与LLM互动，并将用户未来反应作为优化信号，有效提升LLM在社交对话场景下的用户参与度。


<details>
  <summary>Details</summary>
Motivation: 在社会性对话系统中，提高用户参与度至关重要，但以往方法仅关注知识推理或对话行为规划，未能直接提升用户的实际参与度。本文试图通过新的方式更加直接有效地增强用户参与度。

Method: 提出了一种利用用户未来对话反应作为用户参与度奖励信号的方法。具体做法包括：开发用户模拟器，与目标LLMs进行交互，并通过交互式蒙特卡洛树搜索（i×MCTS）探索用户与LLM的互动，收集高低体验质量数据对后，采用直接偏好优化（DPO）对LLM进行优化。

Result: 在情感支持和劝说对话两个社交场景下实验，结果证明该方法有效提升了互动式LLMs的用户参与度。

Conclusion: 基于用户未来反应优化LLM方法，不仅能够更直接提升用户参与度，而且具备较好的通用性和实用价值。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [48] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 本文提出了第一个专为斯洛伐克语自然语言理解设计的评测基准skLEP，覆盖九大任务，系统评估了多类模型表现，并开放数据及评测平台，极大推动斯洛伐克语及跨语言NLU研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于评估斯洛伐克语自然语言理解（NLU）模型的全面基准，现有资源以英语为主，阻碍了斯洛伐克语相关研究的进步。

Method: 构建了一个包含九项不同任务的斯洛伐克语NLU评测基准（skLEP），涵盖token级、句对级、文档级任务。新建原始数据集并细致翻译已有英文NLU资源。对多种模型（斯洛伐克语特定、多语种及英文预训练模型）进行系统性的全面评估，并开放全部数据与评测工具。

Result: 建立了首个斯洛伐克语NLU全面评测基准skLEP，发布了用于微调和评估的工具包、数据集及在线排行榜。首次系统性评估多种模型在斯洛伐克语NLU上的表现。

Conclusion: skLEP为斯洛伐克语的自然语言理解研究提供了重要资源，将推动该语言的NLU模型发展与跨语言研究的可复现性。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [49] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 现有基准测试未必能准确评估大语言模型的真实理解力，许多模型仅表现为“表面理解”，内部概念表征存在深层次不一致。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的评估通常依赖于标准化测试数据集，但这些数据集是否真实反映了模型理解能力，缺乏严格的理论支撑。该论文旨在系统性地探讨通过基准测试推断模型能力的合理性，识别现有方法的潜在误区。

Method: 论文提出了一个正式框架，对LLM在基准测试中表现与真实能力之间的关系进行理论分析。此外，提出了两种定量分析“表面理解”（Potemkin understanding）现象的流程：一个是在三个领域中设计专用基准，另一个是更通用的下界分析方法。

Result: 实验表明，大语言模型跨模型、任务和领域普遍存在“表面理解”现象。这种现象不仅表现为理解错误，还涉及到模型在概念表征上的深层次内在不一致。

Conclusion: LLM在标准测试中的高分，并不一定代表其真正的理解能力，当前基准测试手段存在结构性局限。未来评估模型能力需要更复杂、更能揭示内部概念表征一致性的手段。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [50] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文通过11K条真实健康对话分析，揭示了用户与医疗聊天AI互动中的典型行为及潜在风险，强调提升大模型医疗支持能力的迫切性。


<details>
  <summary>Details</summary>
Motivation: 越来越多的人通过大语言模型与聊天机器人获得健康信息，而这些互动过程中的风险和本质尚未被充分研究。

Method: 构建并分析了HealthChat-11K数据集。该数据集包含11,000个真实对话、25,000条用户消息，利用由临床医生制定的用户互动分类体系，系统分析了21个医疗专科领域下用户与LLM的互动。

Result: 发现用户常有信息不完整、情感表达及引导型提问等行为，这些行为会影响模型表现并引发系统性风险，表明当前医疗聊天AI能力仍需提升。

Conclusion: 本文揭示了用户在健康咨询聊天中常见的互动模式和风险，如不完整的上下文、情感交流、以及引发迎合行为（sycophancy）的提问，凸显了对医疗类大语言模型聊天机器人的改进需求。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [51] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 本文提出数据效能（Data Efficacy）新概念，并基于此设计了DELT训练范式，有效提升了语言模型性能，推荐关注数据组织优化而不仅仅是数据子集选择。


<details>
  <summary>Details</summary>
Motivation: 近年来，语言模型（LM）的数据效率问题受到高度关注，即如何通过选择训练数据的最小或最优子集以提升模型性能。然而，针对如何通过优化训练数据的组织方式提升模型表现（即数据效能，Data Efficacy）的研究仍然较为匮乏。本文旨在填补该领域空白。

Method: 本文提出了一个通用范式DELT，用于关注训练数据效能，其中包含数据评分（Data Scoring）、数据选择（Data Selection）和数据排序（Data Ordering）三个组成部分。提出了新颖的数据评分方法——Learnability-Quality Scoring (LQS)，从梯度一致性角度同时考虑数据样本的可学习性与质量。此外，还设计了创新的数据排序方法Folding Ordering (FO)，旨在解决模型遗忘和数据分布偏差问题。

Result: 经过大量实验验证，不扩大数据规模和模型体积的前提下，DELT框架有效提升了语言模型性能。尤其地，LQS评分方法与Folding排序方法的结合带来了最显著的性能提升。同时，通过数据选择还可兼得数据效能与数据效率。

Conclusion: 数据效能（Data Efficacy）在语言模型训练中潜力巨大，通过合理的数据组织可在不增加计算开销的前提下提升模型性能，值得进一步探索。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [52] [Making Graphs Irregular through Irregularising Walks](https://arxiv.org/abs/2506.21254)
*Julien Bensmail,Romain Bourneuf,Paul Colinot,Samuel Humeau,Timothée Martinod*

Main category: cs.DM

TL;DR: 本文在1-2-3猜想已解决的基础上，引入『附加平行边需形成walk』的额外限制，系统分析如何最短地实现局部不规则多重图，提出结构、组合与算法层面的多项新结果。


<details>
  <summary>Details</summary>
Motivation: 1-2-3猜想及其解决后的相关问题大多不加限制，但在某些实际应用或理论推广情境下引入操作连续性限制（如边需形成walk），能够更细致地刻画该类问题复杂性和丰富性。文章旨在填补该方向的研究空白。

Method: 对经Karoński等人1-2-3猜想及Keusch的解答之后的基础问题施加新约束，引入『边组成walk』这一条件，利用结构分析、组合推理和算法设计等手段系统考察限制下的最短不规则化walk长度。

Result: 证明了在新增walk约束后，不论一般图还是特定子类，最短实现局部不规则度的walk存在上界，并分析了其结构性质。得到部分类型图的具体算法或界限结果。

Conclusion: 本文引入并研究了在1-2-3猜想框架下，当使图变为局部不规则多重图时，要求所添的平行边必须形成原图的一个walk（可以重复经过顶点和边）这一额外限制。文章对最短不规则化walk的长度在一般图及特定类型图下均给出了若干结构性、组合性和算法性的研究结果。

Abstract: The 1-2-3 Conjecture, introduced by Karo\'nski, {\L}uczak, and Thomason in
2004, was recently solved by Keusch. This implies that, for any connected graph
$G$ different from $K_2$, we can turn $G$ into a locally irregular multigraph
$M(G)$, i.e., in which no two adjacent vertices have the same degree, by
replacing some of its edges with at most three parallel edges. In this work, we
introduce and study a restriction of this problem under the additional
constraint that edges added to $G$ to reach $M(G)$ must form a walk (i.e., a
path with possibly repeated edges and vertices) of $G$. We investigate the
general consequences of having this additional constraint, and provide several
results of different natures (structural, combinatorial, algorithmic) on the
length of the shortest irregularising walks, for general graphs and more
restricted classes.

</details>


### [53] [Playing Snake on a Graph](https://arxiv.org/abs/2506.21281)
*Denise Graafsma,Bodo Manthey,Alexander Skopalik*

Main category: cs.DM

TL;DR: 把贪吃蛇游戏推广到一般无向图，判定蛇能否吃满全图（snake-winnable）很难（NP-hard），但对某些图类能完全解决，并揭示了与Hamilton性及girth的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 基于经典的贪吃蛇游戏，作者将其推广到一般无向图上，研究在不同图结构下贪吃蛇持续吃到所有苹果（节点）并不断增长的可能性及其判定复杂度。

Method: 形式化贪吃蛇在图上的移动规则，引入“snake-winnable”图的定义，并通过复杂度理论和结构性图论方法，分析和刻画哪些图满足snake-winnable属性。证明NP-hard性，给出特定图类（如奇数阶二分图、连通度为1的图）的完全判定，分析 girth（最小环长度）与snake-winnable的关系。

Result: 判定一个图是否snake-winnable为NP-hard（即使是格点图）。完全刻画了奇数阶二分图和连通度为1的图是否snake-winnable。Hamilton图必然snake-winnable，非Hamilton的snake-winnable图的girth不超过6，且此上界是严格的。

Conclusion: 判定snake-winnable的判定在一般情况下极为困难，但对于部分特殊结构的图可以完全刻画其snake-winnable属性。此外，揭示了图的Hamilton性和girth对snake-winnable性质的影响。

Abstract: Snake is a classic computer game, which has been around for decades. Based on
this game, we study the game of Snake on arbitrary undirected graphs. A snake
forms a simple path that has to move to an apple while avoiding colliding with
itself. When the snake reaches the apple, it grows longer, and a new apple
appears. A graph on which the snake has a strategy to keep eating apples until
it covers all the vertices of the graph is called snake-winnable. We prove that
determining whether a graph is snake-winnable is NP-hard, even when restricted
to grid graphs. We fully characterize snake-winnable graphs for odd-sized
bipartite graphs and graphs with vertex-connectivity 1. While Hamiltonian
graphs are always snake-winnable, we show that non-Hamiltonian snake-winnable
graphs have a girth of at most 6 and that this bound is tight.

</details>
