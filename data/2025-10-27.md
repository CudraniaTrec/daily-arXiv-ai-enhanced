<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.CL](#cs.CL) [Total: 40]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents](https://arxiv.org/abs/2510.21031)
*Qinghua Lu,Dehai Zhao,Yue Liu,Hao Zhang,Liming Zhu,Xiwei Xu,Angela Shi,Tristan Tan,Rick Kazman*

Main category: cs.SE

TL;DR: 本文提出并验证了一种新颖的智能体架构评估方法AgentArcEval，专门针对基础模型智能体的复杂架构设计和评估难题，实际应用显示有效。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法有效应对智能体架构特有的复合性、自主性、不确定性和持续演进等特点，需开发专门的架构评估方法。

Method: 提出AgentArcEval架构评估方法，并建立了针对智能体的通用情景目录。采用案例研究法，在实际税务助手Luna系统上演示方法应用。

Result: AgentArcEval方法和情景目录能帮助有效评估和指导基础模型驱动的智能体架构，其效果在真实系统Luna上得到验证。

Conclusion: 本文提出的AgentArcEval能够针对基础模型驱动的智能体架构进行有效评估，并通过真实案例验证其实用性。伴随的情景目录也为设计和评估智能体架构提供了指导。

Abstract: The emergence of foundation models (FMs) has enabled the development of
highly capable and autonomous agents, unlocking new application opportunities
across a wide range of domains. Evaluating the architecture of agents is
particularly important as the architectural decisions significantly impact the
quality attributes of agents given their unique characteristics, including
compound architecture, autonomous and non-deterministic behaviour, and
continuous evolution. However, these traditional methods fall short in
addressing the evaluation needs of agent architecture due to the unique
characteristics of these agents. Therefore, in this paper, we present
AgentArcEval, a novel agent architecture evaluation method designed specially
to address the complexities of FM-based agent architecture and its evaluation.
Moreover, we present a catalogue of agent-specific general scenarios, which
serves as a guide for generating concrete scenarios to design and evaluate the
agent architecture. We demonstrate the usefulness of AgentArcEval and the
catalogue through a case study on the architecture evaluation of a real-world
tax copilot, named Luna.

</details>


### [2] [BDiff: Block-aware and Accurate Text-based Code Differencing](https://arxiv.org/abs/2510.21094)
*Yao Lu,Wanwei Liu,Tanghaoran Zhang,Kang Yang,Yang Zhang,Wenyu Xu,Longfei Sun,Xinjun Mao,Shuzheng Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 针对现有代码差分工具无法有效识别多行块级操作的问题，本文提出了BDiff算法，大幅提升了差分准确率与效率，并验证了LLM在该场景下的不可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有文本差异分析技术难以有效识别涉及多行代码块的编辑操作，这降低了开发者理解代码变更的效率。开发者希望能够直接看到块级别的操作，而不是离散的逐行变更。

Method: 提出了一种名为BDiff的文本差异分析算法，能够自动识别两类块级和五类行级编辑操作。该算法首先生成全部可能的行映射和块映射候选，然后利用Kuhn-Munkres算法计算最优映射集，以最小化编辑脚本长度并契合开发者初衷。

Result: BDiff在编辑脚本长度、结果质量和运行时间上均优于五款现有主流工具（包括LLM），能更高效、高质量地展现代码差异，且运行时间具有竞争力。此外，实验发现LLM在代码diff任务中的效率和结果可靠性远不及传统方法。

Conclusion: BDiff有效提升了多行块级代码编辑操作的识别能力与差分结果的质量，显著优于现有主流工具，并已实现网络可视化工具。

Abstract: Code differencing is a fundamental technique in software engineering practice
and research. While researchers have proposed text-based differencing
techniques capable of identifying line changes over the past decade, existing
methods exhibit a notable limitation in identifying edit actions (EAs) that
operate on text blocks spanning multiple lines. Such EAs are common in
developers' practice, such as moving a code block for conditional branching or
duplicating a method definition block for overloading. Existing tools represent
such block-level operations as discrete sequences of line-level EAs, compelling
developers to manually correlate them and thereby substantially impeding the
efficiency of change comprehension. To address this issue, we propose BDiff, a
text-based differencing algorithm capable of identifying two types of
block-level EAs and five types of line-level EAs. Building on traditional
differencing algorithms, we first construct a candidate set containing all
possible line mappings and block mappings. Leveraging the Kuhn-Munkres
algorithm, we then compute the optimal mapping set that can minimize the size
of the edit script (ES) while closely aligning with the original developer's
intent. To validate the effectiveness of BDiff, we selected five
state-of-the-art tools, including large language models (LLMs), as baselines
and adopted a combined qualitative and quantitative approach to evaluate their
performance in terms of ES size, result quality, and running time. Experimental
results show that BDiff produces higher-quality differencing results than
baseline tools while maintaining competitive runtime performance. Our
experiments also show the unreliability of LLMs in code differencing tasks
regarding result quality and their infeasibility in terms of runtime
efficiency. We have implemented a web-based visual differencing tool.

</details>


### [3] [R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking](https://arxiv.org/abs/2510.21106)
*Zhen Yang,Hongyi Lin,Xiao Yu,Jacky Wai Keung,Shuo Liu,Pak Yuen Patrick Chan,Yicheng Sun,Fengji Zhang*

Main category: cs.SE

TL;DR: 论文提出R2ComSync，通过混合检索和多轮重排序提升LLM在代码-注释同步任务的表现，实验显示其优于现有方法，并且生成注释质量更佳。


<details>
  <summary>Details</summary>
Motivation: 现有CCS方法泛化能力不足或依赖大量任务相关数据，LLM方法又未能优于当前方法，主要因缺乏有效示例和重排序机制。因此需探索更有效的自动代码-注释同步方案。

Method: 提出了结合检索与重排序的ICL方法，包括混合检索和多轮重排序策略；在五个LLM和三个CCS数据集上进行对比实验。

Result: R2ComSync在多项基准评测中性能超越五种先进方法，实现了更高质量自动注释同步，各项定量和定性分析均证实其有效性。

Conclusion: R2ComSync在多项实验中优于现有先进方法，并生成质量更高的同步注释。

Abstract: Code-Comment Synchronization (CCS) aims to synchronize the comments with code
changes in an automated fashion, thereby significantly reducing the workload of
developers during software maintenance and evolution. While previous studies
have proposed various solutions that have shown success, they often exhibit
limitations, such as a lack of generalization ability or the need for extensive
task-specific learning resources. This motivates us to investigate the
potential of Large Language Models (LLMs) in this area. However, a pilot
analysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches
because (1) they lack instructive demonstrations for In-Context Learning (ICL)
and (2) many correct-prone candidates are not prioritized.To tackle the above
challenges, we propose R2ComSync, an ICL-based code-Comment Synchronization
approach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync
carries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally
considers the similarity in both code-comment semantics and change patterns
when retrieval, thereby creating ICL prompts with effective examples. (2)
Multi-turn re-ranking strategy. We derived three significant rules through
large-scale CCS sample analysis. Given the inference results of LLMs, it
progressively exploits three re-ranking rules to prioritize relatively
correct-prone candidates. We evaluate R2ComSync using five recent LLMs on three
CCS datasets covering both Java and Python programming languages, and make
comparisons with five SOTA approaches. Extensive experiments demonstrate the
superior performance of R2ComSync against other approaches. Moreover, both
quantitative and qualitative analyses provide compelling evidence that the
comments synchronized by our proposal exhibit significantly higher quality.}

</details>


### [4] [GreenMalloc: Allocator Optimisation for Industrial Workloads](https://arxiv.org/abs/2510.21405)
*Aidan Dakhama,W. B. Langdon,Hector D. Menendez,Karine Even-Mendoza*

Main category: cs.SE

TL;DR: 作者提出GreenMalloc框架，自动调优内存分配器参数，在多个负载下兼顾效率与降低内存消耗，实验证明效果明显。


<details>
  <summary>Details</summary>
Motivation: 针对现代计算系统中内存分配器参数手动配置困难且影响性能，作者提出自动化配置方法以提升内存利用率和性能。

Method: 提出GreenMalloc，多目标搜索框架。利用NSGA-II 算法，对内存分配器参数自动搜索调优，且使用rand_malloc作为轻量级代理性能标定工具，并将最佳配置迁移到gem5仿真器进行系统级实验。重点考察glibc malloc和Google TCMalloc两种分配器。

Result: 在多种负载环境下，平均堆内存使用量最多降低4.1%，且不损失运行时效率，极端情况下还能降低0.25%的总运行时间。

Conclusion: GreenMalloc实现自动、高效的内存分配器参数调优，能在保证效率的前提下有效降低内存占用。

Abstract: We present GreenMalloc, a multi objective search-based framework for
automatically configuring memory allocators. Our approach uses NSGA II and
rand_malloc as a lightweight proxy benchmarking tool. We efficiently explore
allocator parameters from execution traces and transfer the best configurations
to gem5, a large system simulator, in a case study on two allocators: the GNU
C/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,
our empirical results show up to 4.1 percantage reduction in average heap usage
without loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.

</details>


### [5] [Context Engineering for AI Agents in Open-Source Software](https://arxiv.org/abs/2510.21413)
*Seyedmoein Mohsenimofidi,Matthias Galster,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文首次系统性研究了开源社区中AI配置文件（如AGENTS.md）的实际采用情况，发现其结构和内容高度多样，并尚未形成标准，提示有必要进一步研究其结构及展现方式对生成内容质量的影响。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI代码助手向更自主且无需人工监管的智能体转变，如何为AI智能体提供足够的项目上下文成为关键挑战。理解现有上下文传递方式的采用和效果，有助于提升AI代码助手生成内容的质量。

Method: 本文通过对466个开源软件项目的AI配置文件进行实证分析，考察了这些文件的采用情况、包含的信息、呈现方式及其随时间的演化。

Result: 1. 当前并没有统一的AI配置文件结构。
2. 项目对上下文的描述方式多样，包括描述性、规范性、禁止性、解释性及条件性。
3. 对AGENTS.md的持续维护和内容扩展能反映实际项目对AI助手需求的变化。

Conclusion: AI配置文件（如AGENTS.md）的采用为理解现实中提示与上下文工程提供了独特的研究机会，但目前其结构并未标准化，项目间存在很大差异。

Abstract: GenAI-based coding assistants have disrupted software development. Their next
generation is agent-based, operating with more autonomy and potentially without
human oversight. One challenge is to provide AI agents with sufficient context
about the software projects they operate in. Like humans, AI agents require
contextual information to develop solutions that are in line with the target
architecture, interface specifications, coding guidelines, standard workflows,
and other project-specific policies. Popular AI agents for software development
(e.g., Claude Code) advocate for maintaining tool-specific version-controlled
Markdown files that cover aspects such as the project structure, building and
testing, or code style. The content of these files is automatically added to
each prompt. AGENTS.md has emerged as a potential standard that consolidates
tool-specific formats. However, little is known about whether and how
developers adopt this format. Therefore, in this paper, we present the results
of a preliminary study investigating the adoption of AI configuration files in
466 open-source software projects, what information developers provide in these
files, how they present that information, and how they evolve over time. Our
findings indicate that there is no established structure yet, and that there is
a lot of variation in terms of how context is provided (descriptive,
prescriptive, prohibitive, explanatory, conditional). We see great potential in
studying which modifications in structure or presentation can positively affect
the quality of the generated content. Finally, our analysis of commits that
have modified AGENTS.md files provides first insights into how projects
continuously extend and maintain these files. We conclude the paper by
outlining how the adoption of AI configuration files in provides a unique
opportunity to study real-world prompt and context engineering.

</details>


### [6] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: SLMs在需求分类任务中表现接近LLMs，且更灵活高效，是实际应用中的有力选择。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在需求工程（RE）中的自然语言处理任务表现优异，但受限于高计算成本、数据共享风险及对外部服务的依赖；相比之下，小型语言模型（SLMs）具有本地轻部署等优势。但目前SLMs在需求分类任务中与LLMs的准确性对比尚不明确。

Method: 对8个模型（3个LLMs、5个SLMs）在PROMISE、PROMISE Reclass和SecReq三个需求分类数据集上的表现进行对比实验分析。主要关注F1分数、召回率等指标，并分析模型大小和数据集特性对任务表现的影响。

Result: LLMs的平均F1分数仅比SLMs高2%，但差异无统计学显著性。SLMs几乎与LLMs表现持平，且在PROMISE Reclass数据集上召回率更优，且SLMs体积最多可比LLMs小300倍。数据集特性对任务表现影响大于模型大小。

Conclusion: SLMs在需求分类任务中能成为LLMs有效替代方案，且在隐私、成本和本地化部署方面更具优势。

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [7] [Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components](https://arxiv.org/abs/2510.21451)
*Yinglong Zou,Juan Zhai,Chunrong Fang,An Guo,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: 论文针对自动驾驶深度学习框架测试难题，提出组件化模型生成和差分测试方法Scalpel，有效提升了质量问题检测能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习（DL）已成为自动驾驶系统中的关键技术，但其部署面临实时性、资源和能耗等多重挑战。现有的DL框架虽能优化推理效率，但由于部署环境复杂，容易出现内存分配错误等质量问题。当前测试方法无法有效检测这些问题，因为其生成的测试模型不具备自动驾驶所需的多输入/输出、多模态数据处理、和多层特征提取等能力。

Method: 本文提出Scalpel，一种针对汽车深度学习框架的测试方法。Scalpel通过组件级生成测试输入模型，将模型按头、颈、骨干等组件组合，支持自动驾驶所需功能。它维护并更新模型组件库，通过选择、变异和组装组件生成模型。有效生成的模型回流扩充组件库，随后部署于自动驾驶系统内，通过差分测试检测框架质量问题。

Result: 通过Scalpel方法，能生成满足自动驾驶复杂需求的测试输入模型，发现现有DL框架在真实部署环境中难以捕捉的独特质量问题，例如内存分配异常等。组件库的持续扩充提升测试覆盖面和模型多样性，从而更有效地保障DL框架的部署质量。

Conclusion: Scalpel能够弥补现有测试方法在自动驾驶DL框架测试上的不足，有效发现实际部署中易被忽视的质量问题，为自动驾驶系统的深度学习模块部署提供更强的安全与可靠性保障。

Abstract: Deep learning (DL) plays a key role in autonomous driving systems. DL models
support perception modules, equipped with tasks such as object detection and
sensor fusion. These DL models enable vehicles to process multi-sensor inputs
to understand complex surroundings. Deploying DL models in autonomous driving
systems faces stringent challenges, including real-time processing, limited
computational resources, and strict power constraints. To address these
challenges, automotive DL frameworks (e.g., PaddleInference) have emerged to
optimize inference efficiency. However, these frameworks encounter unique
quality issues due to their more complex deployment environments, such as
crashes stemming from limited scheduled memory and incorrect memory allocation.
Unfortunately, existing DL framework testing methods fail to detect these
quality issues due to the failure in deploying generated test input models, as
these models lack three essential capabilities: (1) multi-input/output tensor
processing, (2) multi-modal data processing, and (3) multi-level data feature
extraction. These capabilities necessitate specialized model components, which
existing testing methods neglect during model generation. To bridge this gap,
we propose Scalpel, an automotive DL frameworks testing method that generates
test input models at the model component level. Scalpel generates models by
assembling model components (heads, necks, backbones) to support capabilities
required by autonomous driving systems. Specifically, Scalpel maintains and
updates a repository of model components, generating test inputs by selecting,
mutating, and assembling them. Successfully generated models are added back to
enrich the repository. Newly generated models are then deployed within the
autonomous driving system to test automotive DL frameworks via differential
testing.

</details>


### [8] [Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains](https://arxiv.org/abs/2510.21452)
*Thomas Welsh,Kristófer Finnsson,Brynjólfur Stefánsson,Helmut Neukirchen*

Main category: cs.SE

TL;DR: 本文提出利用社会-技术模型提升软件供应链安全威胁检测的方向，并分析了XZ Utils攻击实例，强调技术与社会层面数据融合的重要性，针对该愿景提出了未来研究的挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 当前软件供应链（SSC）日益成为攻击目标，但由于其动态、复杂以及既有技术也有社会层面的特点，全面漏洞分析难以实现。因此，需要更有针对性、适应性强的威胁检测方法。此外，现有研究偏重于技术层面的监控和控制，缺乏结合社会与技术动态视角的方法。论文希望通过更好地理解软件供应链中的社会技术交互，提升威胁检测的有效性。

Method: 文章提出利用社会-技术模型分析软件供应链，通过结合对技术和社会数据（如GitHub、邮件列表等）的监控，识别异常行为趋势，从而实现更具自适应和针对性的威胁检测。此外，文章分析了XZ Utils攻击案例，指出社会层面（如维护者信任）同样是重要的攻击链环节。

Result: 论文未提供具体实验结果，而是提出了一项研究愿景，强调同时分析开发者行为和代码变动能有效发现可疑动向，为深入漏洞评估提供依据。并提出了实现这一目标所需解决的主要挑战，如开发者及软件分析技术、去中心化的适应机制以及建立用于供应链安全研究的实验平台。

Conclusion: 采用社会-技术模型，结合对开发行为与供应链组件的监控，有望提高软件供应链威胁检测的有效性。未来研究应注重开发与分析相关工具、机制及测试平台。

Abstract: Software supply chains (SSCs) are complex systems composed of dynamic,
heterogeneous technical and social components which collectively achieve the
production and maintenance of software artefacts. Attacks on SSCs are
increasing, yet pervasive vulnerability analysis is challenging due to their
complexity. Therefore, threat detection must be targeted, to account for the
large and dynamic structure, and adaptive, to account for its change and
diversity. While current work focuses on technical approaches for monitoring
supply chain dependencies and establishing component controls, approaches which
inform threat detection through understanding the socio-technical dynamics are
lacking. We outline a position and research vision to develop and investigate
the use of socio-technical models to support adaptive threat detection of SSCs.
We motivate this approach through an analysis of the XZ Utils attack whereby
malicious actors undermined the maintainers' trust via the project's GitHub and
mailing lists. We highlight that monitoring technical and social data can
identify trends which indicate suspicious behaviour to then inform targeted and
intensive vulnerability assessment. We identify challenges and research
directions to achieve this vision considering techniques for developer and
software analysis, decentralised adaptation and the need for a test bed for
software supply chain security research.

</details>


### [9] [Risk Management for Mitigating Benchmark Failure Modes: BenchRisk](https://arxiv.org/abs/2510.21460)
*Sean McGregor,Victor Lu,Vassil Tashev,Armstrong Foundjem,Aishwarya Ramasethu,Sadegh AlMahdi Kazemi Zarkouei,Chris Knotz,Kongtao Chen,Alicia Parrish,Anka Reuel,Heather Frase*

Main category: cs.SE

TL;DR: 该论文提出了LLM基准测试失效风险评估框架BenchRisk，通过分析26个主流基准，发现这些基准在多项维度上都存在高风险，BenchRisk工具有助于提高基准测试的可靠性并促进风险识别与缓解。


<details>
  <summary>Details</summary>
Motivation: 目前LLM基准测试在实际应用决策中发挥重要参考作用，但由于多种失效模式的影响，基准测试的可靠性受到了挑战，亟需系统性方法识别并缓解这些风险以保证评估有效性。

Method: 本研究基于美国国家标准与技术研究院（NIST）的风险管理流程，迭代分析了26个流行的LLM基准，识别了57种潜在失效模式及196种相应的缓解措施，并开发了BenchRisk评分系统对基准风险进行量化评估。

Result: 开发出BenchRisk工作流和评分工具，作为开源平台不仅能比较不同基准的风险，还便于识别、分享风险与相应缓解策略。全部26个基准均在至少一个维度上表现出较高风险。

Conclusion: 所有26个评估的大语言模型（LLM）基准测试在五个维度上（全面性、可理解性、一致性、正确性、持久性）都存在显著风险，这突出显示了LLM基准测试领域中的重要研究方向。

Abstract: Large language model (LLM) benchmarks inform LLM use decisions (e.g., "is
this LLM safe to deploy for my use case and context?"). However, benchmarks may
be rendered unreliable by various failure modes that impact benchmark bias,
variance, coverage, or people's capacity to understand benchmark evidence.
Using the National Institute of Standards and Technology's risk management
process as a foundation, this research iteratively analyzed 26 popular
benchmarks, identifying 57 potential failure modes and 196 corresponding
mitigation strategies. The mitigations reduce failure likelihood and/or
severity, providing a frame for evaluating "benchmark risk," which is scored to
provide a metaevaluation benchmark: BenchRisk. Higher scores indicate that
benchmark users are less likely to reach an incorrect or unsupported conclusion
about an LLM. All 26 scored benchmarks present significant risk within one or
more of the five scored dimensions (comprehensiveness, intelligibility,
consistency, correctness, and longevity), which points to important open
research directions for the field of LLM benchmarking. The BenchRisk workflow
allows for comparison between benchmarks; as an open-source tool, it also
facilitates the identification and sharing of risks and their mitigations.

</details>


### [10] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 单一LLM方案性能受限，多模型集成以多样性方式选解，能大幅提升代码生成与修复任务表现，成本不高。


<details>
  <summary>Details</summary>
Motivation: 当前主流做法是一切任务靠单一的大模型，但这不仅消耗资源，也忽视了多模型优势互补的潜力。实践者缺乏可行路径走出单一模型系统。

Method: 对来自五个家族的十个LLM以及这十个模型的三种集成方式，基于三个涵盖代码生成及程序修复的软件工程基准进行性能测试。比较了模型间互补性和集成与最佳单一模型间的表现差距，并评估了多种从集成候选集中选优的启发式方法。

Result: 集成的理论性能上限可比最优单模型高83%。共识策略易集体找错解，采用多样性策略则可达理论95%性能，且即使只用两个模型组合也能高效提升软件工程相关LLM任务性能，具性价比优势。

Conclusion: 基于共识的解决方案选择策略容易陷入“流行陷阱”，反而多样性驱动的策略能够实现高达理论上95%的潜力，即使是小规模的两模型集成也能大幅提升性能。组合多个LLM比单一模型具有更优表现，且这种提升成本较低。

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


### [11] [Lights-Out: An Automated Ground Segment for unstaffed Satellite Operations](https://arxiv.org/abs/2510.21516)
*Marvin Böcker,Ralph Biggins,Michael Schmeing*

Main category: cs.SE

TL;DR: 本论文介绍了首个实现地面段无人值守、全自动化运行的卫星任务，能全天候为用户提供实验和遥测服务，大幅提升了效率与灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统地面段操作需要大量人工值守，存在运行成本高、响应时间慢、灵活性不足等问题。随着卫星任务越来越复杂，提升自动化和用户自助能力变得必要。

Method: 采用全过程自动化：包括卫星跟踪、遥测和指令下达，全程由预先规划并自动执行的调度安排实现；设置自动化监控，系统检测异常并按需自动响应；设计可自助、全天候开放的用户门户，支持用户远程调度和监视实验。

Result: Heinrich Hertz卫星于2023年7月发射，首次实现全自动地面段操作（尤其无人办公时间），用户可自主安排、实时监控和远程实验操作。任务调度与用户调度自动冲突解决，系统能在1分钟内响应配置变化。

Conclusion: 德国空间机构在Heinrich Hertz卫星任务中首次实现了地面段无人值守、全自动化的运行，系统能在无人值守时段确保任务正常、有效开展。

Abstract: We present our approach for a periodically unstaffed, fully automated ground
segment. The concept is in use for the first time on the German satellite
communications mission Heinrich Hertz on behalf of the German Space Agency at
DLR. Heinrich Hertz was launched in July 2023 and offers access to scientific
and technical experiments to its users. The mission utilizes major automation
concepts for the satellite platform operations, allowing fully automated
operations outside of office hours. The concept includes tracking, telemetry
and commanding (TTC) of the satellite. Pre-planned and automatically executed
schedules enable commanding without human interaction. The user mission
schedule is planned separately from the main mission schedule and is
automatically de-conflicted. The automatic monitoring concept monitors the
systems of the satellite and all assets in the ground segment and triggers
reactions in operator-configurable ways depending on the mission needs, for
example emergency notifications or automated execution of flight operation
procedures. Additionally, the concept also puts special emphasis on a
self-service user portal that provides flexible access 24/7, even when the
control center is not staffed. The portal allows external users of the payload
to schedule pre-defined experiments, monitor the live execution of the
experiment with browser-based displays and access ground station telemetry and
dedicated RF test equipment during the time of their scheduled experiment.
Tasks can be planned long in advance as well as with a short reaction time
(less than 1 minute), which allows, for example, the reconfiguration of the
payload during a running experiment.

</details>


### [12] [Privacy by Design: Aligning GDPR and Software Engineering Specifications with a Requirements Engineering Approach](https://arxiv.org/abs/2510.21591)
*Oleksandr Kosenkov,Ehsan Zabardast,Davide Fucci,Daniel Mendez,Michael Unterkalmsteiner*

Main category: cs.SE

TL;DR: 作者发现，合规软件开发需将GDPR法律知识以多层次方式嵌入规范，现有新方法有助于实现法律知识捕获和需求可追溯性，但未来仍需深化其实际操作性和落地。


<details>
  <summary>Details</summary>
Motivation: 在软件系统工程中，确保系统符合《通用数据保护条例》（GDPR）是隐私设计（PbD）关键组成部分，然而对于实践者在规范目标方面的理解有限，现有方法难以应对GDPR中复杂的问题与解决空间交叉。本文旨在探索联合需求和系统规范以满足PbD的需求，并提出相应方法。

Method: 综述二级和相关一级研究，结合与实践者的访谈，调查当前实践状况并理解底层规范目标（如可追溯性）。据此，开发并评估一种PbD要求和系统规范方法，并以规范目标为评判标准。

Result: 研究展示了GDPR问题与解决空间的关系对支持PbD至关重要。基于法律原文建模，提出的方法有效实现了法律知识捕获、规范透明性及可追溯性等目标，经实践验证对实际需求具有适用性。

Conclusion: 实现PbD需在工程生命周期各层级响应GDPR要求，规范中需融入法律知识以满足不同利益相关者需求并确保合规。当前方法适用性获证实，但未来需进一步推动方法的有效应用。

Abstract: Context: Consistent requirements and system specifications are essential for
the compliance of software systems towards the General Data Protection
Regulation (GDPR). Both artefacts need to be grounded in the original text and
conjointly assure the achievement of privacy by design (PbD). Objectives: There
is little understanding of the perspectives of practitioners on specification
objectives and goals to address PbD. Existing approaches do not account for the
complex intersection between problem and solution space expressed in GDPR. In
this study we explore the demand for conjoint requirements and system
specification for PbD and suggest an approach to address this demand. Methods:
We reviewed secondary and related primary studies and conducted interviews with
practitioners to (1) investigate the state-of-practice and (2) understand the
underlying specification objectives and goals (e.g., traceability). We
developed and evaluated an approach for requirements and systems specification
for PbD, and evaluated it against the specification objectives. Results: The
relationship between problem and solution space, as expressed in GDPR, is
instrumental in supporting PbD. We demonstrate how our approach, based on the
modeling GDPR content with original legal concepts, contributes to
specification objectives of capturing legal knowledge, supporting specification
transparency, and traceability. Conclusion: GDPR demands need to be addressed
throughout different levels of abstraction in the engineering lifecycle to
achieve PbD. Legal knowledge specified in the GDPR text should be captured in
specifications to address the demands of different stakeholders and ensure
compliance. While our results confirm the suitability of our approach to
address practical needs, we also revealed specific needs for the future
effective operationalization of the approach.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People](https://arxiv.org/abs/2510.20886)
*Gabriel Grand,Valerio Pepe,Jacob Andreas,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 提出基于贝叶斯实验设计的新策略，显著提升弱大模型信息探索和决策能力，低成本下优于人类与先进模型，适用范围广。


<details>
  <summary>Details</summary>
Motivation: 许多人工智能在高风险领域（如科学研究和诊断）需要基于数据做出假设和精准推测，但在资源有限的情况下，基于大语言模型的代理的“理性程度”值得深入分析。

Method: 论文提出一个新的对话式博弈任务“Collaborative Battleship”，其中信息有限的Captain需要在探索（提问）和执行行为（射击）中寻求平衡，信息充分的Spotter需在信息瓶颈下准确答复。通过与42名人类玩家对比评估LLM代理，并提出基于贝叶斯实验设计的蒙特卡罗推断策略以提升模型表现。

Result: 新方法使Spotter代理的准确率较LLM基线提升了最高14.7个百分点，Captain代理的信息增益提升至最高0.227 bits（占理论极限的94.2%）。综合改进使弱模型如Llama-4-Scout的胜率远超人类和先进模型，同时在Guess Who?等任务中准确率提升28.3-42.4个百分点。

Conclusion: 结合人类行为洞察后的新算法能显著增强语言模型代理的信息探索、决策理性和实际效能，对低成本构建高效理性AI代理具有广泛适用性。

Abstract: Many high-stakes applications of AI require forming data-driven hypotheses
and making targeted guesses; e.g., in scientific and diagnostic settings. Given
limited resources, to what extent do agents based on language models (LMs) act
rationally? We develop methods to benchmark and enhance agentic
information-seeking, drawing on insights from human behavior. First, we
introduce a strategic decision-oriented dialogue task called Collaborative
Battleship, in which a partially-informed Captain must balance exploration
(asking questions) and action (taking shots), while a fully-informed Spotter
must provide accurate answers under an information bottleneck. Compared to
human players (N=42), we find that LM agents struggle to ground answers in
context, generate informative questions, and select high-value actions. Next,
to address these gaps, we develop novel Monte Carlo inference strategies for
LMs based on principles from Bayesian Experimental Design (BED). For Spotter
agents, our approach boosts accuracy by up to 14.7% absolute over LM-only
baselines; for Captain agents, it raises expected information gain (EIG) by up
to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these
components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs,
such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and
frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We
replicate these findings on Guess Who? where our methods significantly boost
accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for
building rational information-seeking agents.

</details>


### [14] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: 无需微调，普通指令式语言模型通过CodeAdapt方法即可在多领域推理任务上超越专门推理模型，且更节省算力和数据，实现更通用、强大的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型虽能力强，但训练和推理成本高。探究无需微调，能否仅靠任务诱导让普通LM具备强推理能力，并提升效率。

Method: 结合CodeAct框架（将自然语言推理与代码执行多步交织）和少样本自举式上下文学习（仅需5个训练问题），分析了四对LM和RM的配对。

Result: 在8个任务中，三款LM平均优于对应RM（优势达22.9%），且token效率提升10-81%；四模型平均在六任务上表现优越（优势高达35.7%），推理轨迹展现丰富、多样的问题解决策略。

Conclusion: CodeAdapt方法能让普通指令式语言模型（LMs）在多领域任务中达到甚至超过专门训练的推理模型（RMs）的表现，同时更高效，验证了代码增强的推理能力和广泛适用性。

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [15] [FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction](https://arxiv.org/abs/2510.20926)
*Natasha Johnson,Amanda Bertsch,Maria-Emil Deal,Emma Strubell*

Main category: cs.CL

TL;DR: 提出并公开了专为长篇小说设计的FICSIM相似性数据集，证明主流嵌入模型难以有效捕捉文学深层语义，数据集可助力计算文学研究任务。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在处理更长更复杂文本方面的能力提升，学界对将其用于计算文学研究的兴趣增加。然而，现有评估方式存在长文本精细标注成本高，以及公开文学数据集数据污染的问题。此外，目前的相似性嵌入数据集侧重短文本和粗粒度相似性，难以满足文学领域对细致分析的需求。

Method: 作者组建并公开了FICSIM数据集，涵盖由作者生成元数据、经学者验证的12个相似性维度评分，收录了大量近期长篇小说，并强调数据采集过程中对作者权益和持续知情同意的重视。同时，作者利用该数据集对多种嵌入模型进行了评估。

Result: 实验表明，现有嵌入模型普遍倾向于关注表层特征而非更适合文学研究任务的语义类别，说明模型在文学计算领域的应用仍存在局限性。

Conclusion: FICSIM数据集为评估语言模型在长篇文学中的表现提供了新的资源，并揭示了现有模型在捕捉深层文学特征方面的不足；强调了数据采集的人文关怀。作者公开该数据集为推动相关研究提供基础。

Abstract: As language models become capable of processing increasingly long and complex
texts, there has been growing interest in their application within
computational literary studies. However, evaluating the usefulness of these
models for such tasks remains challenging due to the cost of fine-grained
annotation for long-form texts and the data contamination concerns inherent in
using public-domain literature. Current embedding similarity datasets are not
suitable for evaluating literary-domain tasks because of a focus on
coarse-grained similarity and primarily on very short text. We assemble and
release FICSIM, a dataset of long-form, recently written fiction, including
scores along 12 axes of similarity informed by author-produced metadata and
validated by digital humanities scholars. We evaluate a suite of embedding
models on this task, demonstrating a tendency across models to focus on
surface-level features over semantic categories that would be useful for
computational literary studies tasks. Throughout our data-collection process,
we prioritize author agency and rely on continual, informed author consent.

</details>


### [16] [Do LLMs Truly Understand When a Precedent Is Overruled?](https://arxiv.org/abs/2510.20941)
*Li Zhang,Jaromir Savelka,Kevin Ashley*

Main category: cs.CL

TL;DR: 作者建立了法律推理领域面向真实场景的长文档基准，并发现当今LLM在历史案情、推理深度和高复杂性任务表现不足，为后续模型改进提供了新挑战。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评测多数依赖简化的合成任务，无法真实反映实际法律文档理解的复杂性。法律推翻关系反映了普通法的核心实践，是法律职业中真实且高风险的重要推理任务，因此需要更贴近真实应用场景的基准。

Method: 作者构建了一个针对美国最高法院判例的真实236个案例对数据集，用以评估最先进LLM识别推翻关系（overruling relationships）的能力，并分析了模型在不同任务和情境下的表现。

Result: 评测表明，当前LLM存在三个主要缺陷：对历史案例表现不佳（时代敏感），依赖浅层推理缺少深层法律理解，以及在复杂情境下推理失败（产生时间上不可能的关系），显示出对复杂法律任务的适应性仍有限。

Conclusion: 该论文为长上下文窗口大型语言模型（LLM）在真实法律推理任务中的表现提供了新的基准衡量方法，并揭示了当前LLM在处理复杂法律文档时存在的三个关键局限。

Abstract: Large language models (LLMs) with extended context windows show promise for
complex legal reasoning tasks, yet their ability to understand long legal
documents remains insufficiently evaluated. Developing long-context benchmarks
that capture realistic, high-stakes tasks remains a significant challenge in
the field, as most existing evaluations rely on simplified synthetic tasks that
fail to represent the complexity of real-world document understanding.
Overruling relationships are foundational to common-law doctrine and commonly
found in judicial opinions. They provide a focused and important testbed for
long-document legal understanding that closely resembles what legal
professionals actually do. We present an assessment of state-of-the-art LLMs on
identifying overruling relationships from U.S. Supreme Court cases using a
dataset of 236 case pairs. Our evaluation reveals three critical limitations:
(1) era sensitivity -- the models show degraded performance on historical cases
compared to modern ones, revealing fundamental temporal bias in their training;
(2) shallow reasoning -- models rely on shallow logical heuristics rather than
deep legal comprehension; and (3) context-dependent reasoning failures --
models produce temporally impossible relationships in complex open-ended tasks
despite maintaining basic temporal awareness in simple contexts. Our work
contributes a benchmark that addresses the critical gap in realistic
long-context evaluation, providing an environment that mirrors the complexity
and stakes of actual legal reasoning tasks.

</details>


### [17] [Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting](https://arxiv.org/abs/2510.20957)
*Josh McGiff,Khanh-Tung Tran,William Mulcahy,Dáibhidh Ó Luinín,Jake Dalzell,Róisín Ní Bhroin,Adam Burke,Barry O'Sullivan,Hoang D. Nguyen,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 该论文构建了爱尔兰语语法专用的极小对评测集，系统对比了人类与主流大模型的表现。人类远优于所有模型，且开源/闭源模型差距明显，模型在部分语法点与人类有不同弱项。Irish-BLiMP为低资源语言语法理解研究提供了首个标准化工具。


<details>
  <summary>Details</summary>
Motivation: 目前对低资源语言（如爱尔兰语）的语言能力评估工具严重缺乏，限制了大型语言模型在此类语言的研究和应用。研究者希望填补这一空白，为爱尔兰语提供规范化、系统性的评测基准，推动相关技术发展。

Method: 研究团队参考多种语言学文献和语法资料，手工构建并审查了涵盖11类语言学特征的1020对极小对（minimal pairs），由流利的爱尔兰语者参与生成和复核。随后，他们用该数据集对现有大型语言模型（LLMs）和流利人类参与者进行语法知识评测和比较。

Result: 实验发现，所有语言学特征上人类表现均优于所有模型，平均准确率高出16.6%。开源与闭源模型之间也存在18.1%的性能差距，最强模型gpt-5准确率仅为73.5%，而人类为90.1%。此外，人类和模型在不同语法点上存在不同的困难，反映出模型所学的语法表示与实际人类认知存在差异。

Conclusion: Irish-BLiMP作为首个为爱尔兰语设计的系统性语法评测框架和数据集，不仅为低资源语言的语言理解提供了有价值的基准，还揭示了当前主流语言模型在该领域的明显短板，可促进相关技术发展和研究深化。

Abstract: We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), the
first dataset and framework designed for fine-grained evaluation of linguistic
competence in the Irish language, an endangered language. Drawing on a variety
of linguistic literature and grammar reference works, we manually constructed
and reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,
through a team of fluent Irish speakers. We evaluate both existing Large
Language Models (LLMs) and fluent human participants on their syntactic
knowledge of Irish. Our findings show that humans outperform all models across
all linguistic features, achieving 16.6% higher accuracy on average. Moreover,
a substantial performance gap of 18.1% persists between open- and closed-source
LLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracy
compared to 90.1% by human. Interestingly, human participants and models
struggle on different aspects of Irish grammar, thus highlighting a difference
in representation learned by the models. Overall, Irish-BLiMP provides the
first systematic framework for evaluating the grammatical competence of LLMs in
Irish and offers a valuable benchmark for advancing research on linguistic
understanding in low-resource languages.

</details>


### [18] [Can Confidence Estimates Decide When Chain-of-thought is Necessary for Llms?](https://arxiv.org/abs/2510.21007)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 该工作研究了用训练外置信度估算方法来决定何时启用Chain-of-thought推理，大致可以更智能地减少无效推理，但不同置信度方法的实用性还不稳定，实际落地仍要探索。


<details>
  <summary>Details</summary>
Motivation: CoT能够提升大模型推理表现，但代价高且并非所有任务都适用，因此需要自动判断何时采用CoT以提升效率和准确性。

Method: 对无需训练的置信度估算方法进行系统性评估，包括与随机基线和理想的oracle对比，通过大量实验证明其在CoT触发上的表现。

Result: 无需训练的置信度估算方法能在部分情况下减少不必要的CoT并提升性能，但不同方法的稳定性和实用性有限，存在应用难点。

Conclusion: 当前无需训练的置信度估算方法在减少冗余推理和提升性能方面有一定潜力，但其效果因数据集和模型的不同而不一致，实际部署仍面临挑战。

Abstract: Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.

</details>


### [19] [Input Matters: Evaluating Input Structure's Impact on LLM Summaries of Sports Play-by-Play](https://arxiv.org/abs/2510.21034)
*Barkavi Sundararajan,Somayajulu Sripada,Ehud Reiter*

Main category: cs.CL

TL;DR: 将NBA比赛数据输入LLM时，采用有结构的格式（如JSON、行结构）能大幅减少幻觉和事实错误；输入结构选择远比想象重要。


<details>
  <summary>Details</summary>
Motivation: 部署大语言模型于体育报道等高准确度领域时，若生成文本未真实反映数据，可能导致事实错误。研究旨在量化输入数据结构对LLM事实错误和幻觉的影响。

Method: 通过对LLM生成的NBA比赛纪要进行人工标注，统计不同输入结构下的事实错误数，并采用双因素重复测量ANOVA以及Tukey HSD事后检验分析误差率的差异。

Result: JSON格式输入相比无结构输入可使Llama模型错误率降低69%，Qwen模型降低65%；行结构输入亦能显著改善。输入结构决定了错误率80%以上的方差。所有输入结构差异均显著。

Conclusion: 输入结构对LLM生成摘要的事实错误和幻觉率有显著影响，规范结构尤其能够显著降低错误。

Abstract: A major concern when deploying LLMs in accuracy-critical domains such as
sports reporting is that the generated text may not faithfully reflect the
input data. We quantify how input structure affects hallucinations and other
factual errors in LLM-generated summaries of NBA play-by-play data, across
three formats: row-structured, JSON and unstructured. We manually annotated
3,312 factual errors across 180 game summaries produced by two models,
Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input
reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured
input, while row-structured input reduces errors by 54% for Llama and 51% for
Qwen. A two-way repeated measures ANOVA shows that input structure accounts for
over 80% of the variance in error rates, with Tukey HSD post hoc tests
confirming statistically significant differences between all input formats.

</details>


### [20] [Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection](https://arxiv.org/abs/2510.21049)
*Atoosa Chegini,Hamid Kazemi,Garrett Souza,Maria Safi,Yang Song,Samy Bengio,Sinead Williamson,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 推理能提升大语言模型的整体表现，但在要求极低误报率的分类任务中反而不如直接预测，基于token的评分值得优先考虑。集成推理与非推理模式能兼顾准确性与精度，需根据实际需求选择推理模式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）中的推理方法已广泛提升了各种基准测试的准确率，但推理在对精确度要求极高的任务（如低误报率场景）中的适用性尚未明确。作者针对这一问题展开系统性研究。

Method: 作者在严格低误报率（low False Positive Rate, FPR）要求下，系统性地分析了推理在分类任务中的表现，涵盖安全检测和幻觉检测两种任务，在微调和零样本设置下，利用标准LLM及大型推理模型（LRM）进行评估。对比了推理增强（Think On）与无推理（Think Off）模式，并考察了基于token的打分和自我置信度表述。

Result: 推理增强（Think On）方法在整体准确率上更优，但在低FPR的精确场景下表现较差，无推理（Think Off）则在此类场景下更为有效；仅当能接受较高FPR时，Think On才表现更佳。另一方面，基于token的评分在精确部署时显著优于模型自我表述置信度。两种模式的简单集成能有效兼顾各自优势。

Conclusion: 推理对于提升平均准确率有积极作用，但在严格精确需求的场景（如低FPR分类任务）下，推理并非最佳选择。推理是一把双刃剑，需要在具体应用需求下权衡其优劣。

Abstract: Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.

</details>


### [21] [Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization](https://arxiv.org/abs/2510.21059)
*Mahmud Wasif Nafee,Maiqi Jiang,Haipeng Chen,Yanfu Zhang*

Main category: cs.CL

TL;DR: 本文提出了动态示例检索的知识编辑框架 DR-IKE，通过强化学习和可学习阈值实现适应性强、兼容黑盒 LLM 的知识编辑，显著提升了编辑成功率和效率。源码已开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在事实回忆方面表现出色，但仍存在传播过时或错误知识的问题。现有的上下文知识编辑方法多依赖于静态、基于表层相似性的示例选取，导致编辑效果受到数量与质量权衡以及对不同任务难度缺乏自适应的问题。

Method: 提出了一种名为 DR-IKE（Dynamic Retriever for In-Context Knowledge Editing）的动态检索框架。该方法训练一个基于 BERT 的检索器，利用 REINFORCE 算法根据编辑奖励对示例进行排序，同时引入可学习的阈值，用于剪枝低价值示例。当编辑任务简单时缩短提示，任务难时增加示例数量。整个过程无需修改大模型权重，仅依赖前向推理流程，兼容黑盒 LLMs。

Result: 在 COUNTERFACT 基准测试中，DR-IKE 的编辑成功率最高提升了17.1%，延迟降低了41.6%，且在无关查询上的准确率得以保持，验证了方法的可扩展性和自适应能力。

Conclusion: DR-IKE 实现了对黑盒大语言模型的高效、可扩展与自适应的知识编辑，为实际应用中高质量知识修正提供了可行解决方案。

Abstract: Large language models (LLMs) excel at factual recall yet still propagate
stale or incorrect knowledge. In-context knowledge editing offers a
gradient-free remedy suitable for black-box APIs, but current editors rely on
static demonstration sets chosen by surface-level similarity, leading to two
persistent obstacles: (i) a quantity-quality trade-off, and (ii) lack of
adaptivity to task difficulty. We address these issues by dynamically selecting
supporting demonstrations according to their utility for the edit. We propose
Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight
framework that (1) trains a BERT retriever with REINFORCE to rank
demonstrations by editing reward, and (2) employs a learnable threshold to
prune low-value examples, shortening the prompt when the edit is easy and
expanding it when the task is hard. DR-IKE performs editing without modifying
model weights, relying solely on forward passes for compatibility with
black-box LLMs. On the COUNTERFACT benchmark, it improves edit success by up to
17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries,
demonstrating scalable and adaptive knowledge editing. The code is available at
https://github.com/mwnafee/DR-IKE .

</details>


### [22] [Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering](https://arxiv.org/abs/2510.21068)
*William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono*

Main category: cs.CL

TL;DR: 本文提出面向印尼语的自适应RAG问答系统，用分类器调整策略并用机器翻译扩充数据。实验中复杂性分类器表现好，但多重检索存在不一致，影响了整体效果，为低资源语言问答未来改进提供启示。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）问答系统在英语上表现优异，但在印尼语等低资源语言中的研究有限，因此该研究致力于减少不同语言之间的差距。

Method: 提出一种自适应RAG系统，将问题复杂性分类器整合进RAG体系，以识别问题难易度并调整检索/回答策略，同时通过机器翻译进行数据增强，以弥补印尼语数据集的不足。

Result: 问题复杂性分类器表现可靠，但多重检索回答策略导致明显不一致，整体评测表现受负面影响。

Conclusion: 自适应RAG系统在低资源印尼语问答中展现潜力，但多重检索策略带来挑战，未来需进一步优化算法以提升整体性能。

Abstract: Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.

</details>


### [23] [CDrugRed: A Chinese Drug Recommendation Dataset for Discharge Medications in Metabolic Diseases](https://arxiv.org/abs/2510.21084)
*Juntao Li,Haobin Yuan,Ling Luo,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本研究发布了首个面向中文、涵盖代谢性疾病的公开药物推荐数据集CDrugRed，并对多种大模型进行了基准测试。结果显示任务复杂，有待改进。该数据集为后续智能药物推荐研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）为基础的智能药物推荐对提升临床决策效率和质量具有重要意义，但由于缺少公开、真实世界的EHR数据集，特别是非英文数据集，该领域的发展受限。

Method: 本研究发布了一个名为CDrugRed的中文药物推荐数据集，专注于代谢性疾病患者的出院用药推荐。该数据集包含3190名患者的5894条脱敏记录，涵盖病人基本信息、病史、诊疗过程及出院诊断等。并用该数据集针对多个当前主流的大型语言模型，进行了药物推荐任务的基准测试。

Result: 通过对多种主流大模型的实验，发现有监督微调可以提升模型表现，但目前最优模型的F1最高为0.5648，Jaccard得分为0.4477，表现仍有较大提升空间，凸显了临床药物推荐任务的复杂性。

Conclusion: CDrugRed为中文药物推荐提供了首个公开的真实数据资源，是发展更健壮和准确药物推荐系统的重要基石。该数据集难度较高，具有挑战性和实用价值，现已公开供研究社区使用。

Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is
critical for improving for improving the quality and efficiency of clinical
decision-making. By leveraging large-scale patient data, drug recommendation
systems can assist physicians in selecting the most appropriate medications
according to a patient's medical history, diagnoses, laboratory results, and
comorbidities. However, the advancement of such systems is significantly
hampered by the scarcity of publicly available, real-world EHR datasets,
particularly in languages other than English. In this work, we present
CDrugRed, a first publicly available Chinese drug recommendation dataset
focused on discharge medications for metabolic diseases. The dataset includes
5,894 de-identified records from 3,190 patients, containing comprehensive
information such as patient demographics, medical history, clinical course, and
discharge diagnoses. We assess the utility of CDrugRed by benchmarking several
state-of-the-art large language models (LLMs) on the discharge medication
recommendation task. Experimental results show that while supervised
fine-tuning improves model performance, there remains substantial room for
improvement, with the best model achieving the F1 score of 0.5648 and Jaccard
score of 0.4477. This result highlights the complexity of the clinical drug
recommendation task and establishes CDrugRed as a challenging and valuable
resource for developing more robust and accurate drug recommendation systems.
The dataset is publicly available to the research community under the data
usage agreements at https://github.com/DUTIR-BioNLP/CDrugRed.

</details>


### [24] [Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only](https://arxiv.org/abs/2510.21090)
*Qingru Zhang,Liang Qiu,Ilgee Hong,Zhenghao Xu,Tianyi Liu,Shiyang Li,Rongzhi Zhang,Zheng Li,Lihong Li,Bing Yin,Chao Zhang,Jianshu Chen,Haoming Jiang,Tuo Zhao*

Main category: cs.CL

TL;DR: 本文提出Self-Rewarding PPO，一种无需人工偏好标注、结合SFT与PPO的微调方法，有效提升了大模型在自然语言任务中的泛化能力和数据效率，特别适合数据量不足的情况。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）依赖于有监督微调（SFT）来与人工标注的示范数据对齐，但SFT存在过拟合与泛化能力差等问题，特别是在数据稀缺的情况下。为了解决这些局限性，需改进微调方法以提升模型泛化能力和数据利用效率。

Method: 提出了一种新的微调方法：Self-Rewarding PPO，将有监督微调（SFT）与近端策略优化（PPO）结合。其核心是利用SFT模型与预训练基座模型之间的对数策略比值构建奖励函数，将预训练模型视作基线，SFT模型为目标，实现无须额外人工偏好标注的on-policy微调。

Result: 在多个自然语言处理任务上的实验证明，Self-Rewarding PPO方法在泛化能力、数据效率及鲁棒性方面均优于传统SFT方法，尤其适用于高质量标注数据稀缺的场景。

Conclusion: Self-Rewarding PPO 能有效缓解SFT的过拟合与泛化能力不足问题，显著提高了LLMs与示范数据的对齐效果，为数据有限情况下的微调提供了更加可靠的方法。

Abstract: Supervised fine-tuning (SFT) has emerged as a crucial method for aligning
large language models (LLMs) with human-annotated demonstrations. However, SFT,
being an off-policy approach similar to behavior cloning, often struggles with
overfitting and poor out-of-domain generalization, especially in limited-data
scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel
fine-tuning method that leverages on-policy techniques to enhance
generalization performance. Our approach combines the strengths of SFT and
proximal policy optimization (PPO) to achieve more effective alignment from
demonstration data. At its core is a reward function designed as the log policy
ratio between the SFT model and the pretrained base model. This function serves
as an implicit reward signal, using the pretrained policy as a baseline and the
SFT policy as a target. By doing so, it enables on-policy fine-tuning without
relying on human preference annotations. The integration of this self-rewarding
mechanism with PPO addresses key limitations of SFT, improving generalization,
data efficiency, and robustness. Our empirical evaluation across a range of
natural language processing tasks demonstrates that Self-Rewarding PPO
consistently outperforms traditional SFT methods. The results highlight the
effectiveness of our approach in aligning LLMs using demonstration data,
particularly in scenarios where high-quality annotated data is scarce.

</details>


### [25] [The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection](https://arxiv.org/abs/2510.21118)
*Qiang Ding,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出了新的信实性标注框架与基准VeriGray，有效解决了外部知识界定模糊问题，实测显示领先LLM仍有幻觉和外依赖，新基准对模型具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有信实性评测基准因外部知识界定模糊导致标注不一致，亟需一种能明确区分外依赖和幻觉的新评测方式。

Method: 提出一种新的信实性注释框架，引入Out-Dependent中间类别，并据此建立了新的基准VeriGray，对现有自动摘要模型进行评测。

Result: 即便是最领先的LLM（如GPT-5）也有约6%的幻觉句子，约8%的生成句子需依赖外部知识，新基准测试对多种模型均具有高挑战性。

Conclusion: 他们提出的新注释框架和基准 VeriGray 能更精确区分外部知识对于模型生成的影响，并且现有SOTA模型仍存有一定百分比的幻觉和外依赖现象。

Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a
given source document is essential for real-world applications. While prior
research has explored LLM faithfulness, existing benchmarks suffer from
annotation ambiguity, primarily due to the ill-defined boundary of permissible
external knowledge in generated outputs. For instance, common sense is often
incorporated into responses and labeled as "faithful", yet the acceptable
extent of such knowledge remains unspecified, leading to inconsistent
annotations. To address this issue, we propose a novel faithfulness annotation
framework, which introduces an intermediate category, Out-Dependent, to
classify cases where external knowledge is required for verification. Using
this framework, we construct VeriGray (Verification with the Gray Zone) -- a
new unfaithfulness detection benchmark in summarization. Statistics reveal that
even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences)
in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on
average of models) of generated sentences fall into the Out-Dependent category,
underscoring the importance of resolving annotation ambiguity in unfaithfulness
detection benchmarks. Experiments demonstrate that our benchmark poses
significant challenges to multiple baseline methods, indicating considerable
room for future improvement.

</details>


### [26] [Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications](https://arxiv.org/abs/2510.21131)
*Guangxin Su,Hanchen Wang,Jianwei Wang,Wenjie Zhang,Ying Zhang,Jian Pei*

Main category: cs.CL

TL;DR: 本文综述了大语言模型与文本属性图的集成方法，从框架分类、技术手段、应用、数据集到未来挑战，为交叉领域研究提供了全面指引。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在语义理解和生成方面表现出色，但由于其黑箱属性，缺乏结构化、多跳推理能力。而文字属性图（TAG）含有清晰关系结构和文本上下文，虽然语义深度不足。把LLM与TAG结合能够互补，促进知识表达和推理能力，因此需要系统性梳理该方向方法和进展。

Method: 提出一项综述，从编排视角系统评述LLM与TAG的集成，并提出新颖分类法。分别分析LLM用于丰富TAG、TAG用于提升LLM推理两大方向，归纳顺序、并行、多模块等集成框架，同时讨论TAG特定预训练、提示学习与高效微调等技术。

Result: 归纳经验结果，整理现有数据集，指出了推荐系统、生物医疗分析、知识密集型问答等多样化应用领域。还总结了开发过程中遇到的挑战和未来的研究方向。

Conclusion: 该综述首次系统梳理了语言模型与文本属性图的集成进展，提供了方法、应用、挑战和未来方向参考，为后续语言与图学习交叉领域的研究提供了有价值的指导。

Abstract: Large Language Models (LLMs) have achieved remarkable success in natural
language processing through strong semantic understanding and generation.
However, their black-box nature limits structured and multi-hop reasoning. In
contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures
enriched with textual context, yet often lack semantic depth. Recent research
shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG
representation learning and improving the reasoning and interpretability of
LLMs. This survey provides the first systematic review of LLM--TAG integration
from an orchestration perspective. We introduce a novel taxonomy covering two
fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and
TAG for LLM, where structured graphs improve LLM reasoning. We categorize
orchestration strategies into sequential, parallel, and multi-module
frameworks, and discuss advances in TAG-specific pretraining, prompting, and
parameter-efficient fine-tuning. Beyond methodology, we summarize empirical
insights, curate available datasets, and highlight diverse applications across
recommendation systems, biomedical analysis, and knowledge-intensive question
answering. Finally, we outline open challenges and promising research
directions, aiming to guide future work at the intersection of language and
graph learning.

</details>


### [27] [Social Simulations with Large Language Model Risk Utopian Illusion](https://arxiv.org/abs/2510.21180)
*Ning Bian,Xianpei Han,Hongyu Lin,Baolei Wu,Jun Wang*

Main category: cs.CL

TL;DR: 本文系统分析了LLMs在社会情境下与真实人类行为的差异，指出LLMs过于理想化，需发展更接地气的社会类模型提升真实性。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）在模拟人类行为、互动和决策方面展现出巨大潜力，但它们与真实人类行为在社会情境中的差异尚未充分探讨，这可能导致科学研究误判和实际应用中出现意外后果。

Method: 作者提出了一种系统性框架，通过聊天室式对话模拟多智能体互动，并从五个语言维度分析LLMs的行为，以检验其在社会认知偏差上的表现。文章还对八种代表性LLMs（分属三大家族）进行了大量实验。

Result: 研究发现，LLMs并不能真实再现人类行为，而是表现出理想化、带有社会期望偏向的行为。例如，表现出社会角色偏差、首因效应和积极偏向，使得模拟出的社会过于“乌托邦化”，缺乏真实交互的复杂性和多样性。

Conclusion: LLMs在社会行为模拟上存在偏差，当前的模型过于理想化，应推动更加贴近人类社会多样性的模型发展，以更好地服务于社会科学和实际应用。

Abstract: Reliable simulation of human behavior is essential for explaining,
predicting, and intervening in our society. Recent advances in large language
models (LLMs) have shown promise in emulating human behaviors, interactions,
and decision-making, offering a powerful new lens for social science studies.
However, the extent to which LLMs diverge from authentic human behavior in
social contexts remains underexplored, posing risks of misinterpretation in
scientific studies and unintended consequences in real-world applications.
Here, we introduce a systematic framework for analyzing LLMs' behavior in
social simulation. Our approach simulates multi-agent interactions through
chatroom-style conversations and analyzes them across five linguistic
dimensions, providing a simple yet effective method to examine emergent social
cognitive biases. We conduct extensive experiments involving eight
representative LLMs across three families. Our findings reveal that LLMs do not
faithfully reproduce genuine human behavior but instead reflect overly
idealized versions of it, shaped by the social desirability bias. In
particular, LLMs show social role bias, primacy effect, and positivity bias,
resulting in "Utopian" societies that lack the complexity and variability of
real human interactions. These findings call for more socially grounded LLMs
that capture the diversity of human social behavior.

</details>


### [28] [Estonian Native Large Language Model Benchmark](https://arxiv.org/abs/2510.21193)
*Helena Grete Lillepalu,Tanel Alumäe*

Main category: cs.CL

TL;DR: 本文开发了多样化爱沙尼亚语LLM评测基准，比较了多类模型表现，证明了先进LLM可作为可靠评测工具，对相关研究有重要推动作用。


<details>
  <summary>Details</summary>
Motivation: 爱沙尼亚语的大语言模型（LLM）评测基准较少，且缺乏各类模型在爱沙尼亚语任务上的系统对比，有必要开发新的基准并全面评测相关模型表现。

Method: 设计并发布一个涵盖七个多样化子任务的数据集，这些子任务覆盖了通用知识、领域知识、语法和词汇理解、摘要、语境理解等多方面。数据均来自原生爱沙尼亚语源，未用机器翻译。对6个基础模型和26个指令微调模型进行测试，比较开源和商业模型，并结合人工评测和LLM作为评审的方法。

Result: 人工评测与基准得分在不同数据集上呈中到高相关性。以Claude 3.7 Sonnet为判官的结果与人工得分高度一致，表明高水平LLM可用于辅助爱沙尼亚语模型的评测。

Conclusion: 提出了首个涵盖广泛任务的爱沙尼亚语LLM评测基准，并对大量模型进行了系统评测，验证了LLM辅助评测方法的可靠性，为该语言技术研究提供了新的工具和参考。

Abstract: The availability of LLM benchmarks for the Estonian language is limited, and
a comprehensive evaluation comparing the performance of different LLMs on
Estonian tasks has yet to be conducted. We introduce a new benchmark for
evaluating LLMs in Estonian, based on seven diverse datasets. These datasets
assess general and domain-specific knowledge, understanding of Estonian grammar
and vocabulary, summarization abilities, contextual comprehension, and more.
The datasets are all generated from native Estonian sources without using
machine translation. We compare the performance of base models,
instruction-tuned open-source models, and commercial models. Our evaluation
includes 6 base models and 26 instruction-tuned models. To assess the results,
we employ both human evaluation and LLM-as-a-judge methods. Human evaluation
scores showed moderate to high correlation with benchmark evaluations,
depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated
strong alignment with human ratings, indicating that top-performing LLMs can
effectively support the evaluation of Estonian-language models.

</details>


### [29] [The "Right" Discourse on Migration: Analysing Migration-Related Tweets in Right and Far-Right Political Movements](https://arxiv.org/abs/2510.21220)
*Nishan Chatterjee,Veronika Bajt,Ana Zwitter Vitez,Senja Pollak*

Main category: cs.CL

TL;DR: 本研究通过多学科方法分析极右翼推特言论，识别仇恨与移民相关话语模式，揭示其传播与影响机制，为理解社交媒体上的极端主义提供了有价值视角。


<details>
  <summary>Details</summary>
Motivation: 近年来欧洲极右翼民粹主义崛起，其社交媒体话语影响政治结果，因此亟需深度分析社交平台上极端意识形态的传播机制及社会影响。

Method: 结合自然语言处理技术与社会学洞察，对英文和法文的极右翼推文 MIGR-TWIT 语料库进行分析，识别话语模式与说服技巧，并从语言、社会学、计算角度融合研究。

Result: 揭示了极右翼推特的话语结构、仇恨言论特征、移民相关话题及说服策略等，为理解右翼极端主义在社交媒体上的表现与影响提供了实证数据和新见解。

Conclusion: 通过跨学科方法分析社交媒体推特上的极右翼话语，有助于更好理解其对社会和政治的影响，特别是涉及移民与仇恨言论的传播。

Abstract: The rise of right-wing populism in Europe has brought to the forefront the
significance of analysing social media discourse to understand the
dissemination of extremist ideologies and their impact on political outcomes.
Twitter, as a platform for interaction and mobilisation, provides a unique
window into the everyday communication of far-right supporters. In this paper,
we propose a methodology that uses state-of-the-art natural language processing
techniques with sociological insights to analyse the MIGR-TWIT corpus of
far-right tweets in English and French. We aim to uncover patterns of discourse
surrounding migration, hate speech, and persuasion techniques employed by right
and far-right actors. By integrating linguistic, sociological, and
computational approaches, we seek to offer cross-disciplinary insights into
societal dynamics and contribute to a better understanding of contemporary
challenges posed by right-wing extremism on social media platforms.

</details>


### [30] [DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services](https://arxiv.org/abs/2510.21228)
*Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan*

Main category: cs.CL

TL;DR: 该论文提出并评估了一种结合临床分类和大语言模型的多智能体系统，高真实度地模拟紧急医疗调度场景，结果显示该系统能有效支持调度流程并具有培训与实时决策辅助潜力。


<details>
  <summary>Details</summary>
Motivation: 紧急医疗调度过程面临来电者焦虑、信息模糊和高认知负荷等挑战，亟需高效辅助工具。大语言模型和多智能体系统有潜力增强调度员能力，提升流程安全性和培训质量，因此需要开发并验证真实的模拟与决策辅助平台。

Method: 通过构建包含32种主诉、6种来电者身份的临床分类法，并制定六阶段呼叫协议，研发了基于AutoGen平台的MAS系统，包含来电者和调度员智能体，采用事实共享机制确保临床合理性。评估采用混合框架：四名医生对100例模拟案例进行人工审查（指导有效性、调度有效性），并结合自动化语言分析（情感、可读性、礼貌性）。

Result: 系统在医生评估中表现出色，调度有效率达94%，指导有效率为91%，均获高分认可。自动化指标显示交流情感以中性为主（73.7%），文本高度可读（Flesch 80.9），风格普遍礼貌（60.0%礼貌，0%不礼貌）。

Conclusion: 该研究提出并验证了一种基于临床分类法、由大语言模型驱动的多智能体系统（MAS）能够高真实度地模拟紧急医疗调度场景，有助于调度员培训、流程评估，并为实时决策支持奠定基础，为高级AI代理安全地融入紧急响应流程提供了方法路径。

Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process
challenged by caller distress, ambiguity, and cognitive load. Large Language
Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment
dispatchers. This study aimed to develop and evaluate a taxonomy-grounded,
LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods:
We constructed a clinical taxonomy (32 chief complaints, 6 caller identities
from MIMIC-III) and a six-phase call protocol. Using this framework, we
developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system
grounds interactions in a fact commons to ensure clinical plausibility and
mitigate misinformation. We used a hybrid evaluation framework: four physicians
assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch
Effectiveness," supplemented by automated linguistic analysis (sentiment,
readability, politeness). Results: Human evaluation, with substantial
inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high
performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 %
contacting the correct potential other agents) and Guidance Efficacy (advice
provided in 91 % of cases), both rated highly by physicians. Algorithmic
metrics corroborated these findings, indicating a predominantly neutral
affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high
readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 %
impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically
plausible dispatch scenarios with high fidelity. Findings support its use for
dispatcher training, protocol evaluation, and as a foundation for real-time
decision support. This work outlines a pathway for safely integrating advanced
AI agents into emergency response workflows.

</details>


### [31] [Correlation Dimension of Auto-Regressive Large Language Models](https://arxiv.org/abs/2510.21258)
*Xin Du,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 论文提出用分形几何中的相关维数来衡量LLMs生成文本的复杂性，不仅可反映局部和全局结构，还能有效检测生成异常，方法高效且适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）虽然在自然语言生成方面取得了很大进展，但仍会出现重复和不连贯等奇怪行为。目前的评估指标主要关注局部预测准确性，忽略了文本的长程结构复杂性。研究者希望找到能全面反映语言复杂性的评估方法。

Method: 提出使用相关维数（correlation dimension）这一分形几何的自相似性度量方法，来量化语言模型感知下文本的认识论复杂性。该方法可以在一个统一框架下捕捉语言的层级递归结构，连接文本的局部与全局属性。通过在不同模型和情形下的大量实验，评估该方法的表现。

Result: 实验结果显示，相关维数能够揭示预训练过程中的三个不同阶段，反映上下文相关的复杂性，反映模型幻觉（hallucination）倾向，并能可靠地检测生成文本的多种退化形式。此外，该方法计算高效、对模型量化（如4-bit精度）鲁棒、适用于各种自回归架构，并为理解LLMs的生成行为提供了新视角。

Conclusion: 相关维数为评估大型语言模型生成文本的复杂性提供了全面且有效的新工具，补足了现有评估方法的局限，对今后的研究和LLM优化具有重要意义。

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language generation, yet they continue to display puzzling behaviors -- such as
repetition and incoherence -- even when exhibiting low perplexity. This
highlights a key limitation of conventional evaluation metrics, which emphasize
local prediction accuracy while overlooking long-range structural complexity.
We introduce correlation dimension, a fractal-geometric measure of
self-similarity, to quantify the epistemological complexity of text as
perceived by a language model. This measure captures the hierarchical
recurrence structure of language, bridging local and global properties in a
unified framework. Through extensive experiments, we show that correlation
dimension (1) reveals three distinct phases during pretraining, (2) reflects
context-dependent complexity, (3) indicates a model's tendency toward
hallucination, and (4) reliably detects multiple forms of degeneration in
generated text. The method is computationally efficient, robust to model
quantization (down to 4-bit precision), broadly applicable across
autoregressive architectures (e.g., Transformer and Mamba), and provides fresh
insight into the generative dynamics of LLMs.

</details>


### [32] [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)
*Xinghao Wang,Pengyu Wang,Dong Zhang,Chenkun Tan,Shaojun Zhou,Zhaoxiang Liu,Shiguo Lian,Fangxu Liu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 该论文针对LLM长上下文高计算成本的问题，提出利用注意力置换优化的块稀疏注意力方法PBS-Attn，在准确率和速度上均优于现有方法，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型（LLM）上下文长度的扩展性带来显著益处，但由于自注意力机制随序列长度呈O(N^2)复杂度，计算成本高昂。注意力矩阵对长序列通常稀疏，存在优化空间。当前块稀疏注意力方法因注意力分布不均导致稀疏性不足，存在冗余计算。

Method: 提出了Permuted Block-Sparse Attention（PBS-Attn），通过注意力的置换属性提升块级稀疏性，优化LLM预填充阶段的效率。并开发了定制的permuted-FlashAttention内核实现此算法。

Result: 在多个真实长上下文数据集上的综合实验表明，PBS-Attn在模型精度上超过现有块稀疏注意力方法，并与完整注意力基线接近。同时，预填充阶段端到端速度提升可达2.75倍。

Conclusion: PBS-Attn方法不仅提升了大语言模型在长上下文下的处理效率，而且保持了模型准确率，兼具理论与实际应用价值。代码已开源。

Abstract: Scaling the context length of large language models (LLMs) offers significant
benefits but is computationally expensive. This expense stems primarily from
the self-attention mechanism, whose $O(N^2)$ complexity with respect to
sequence length presents a major bottleneck for both memory and latency.
Fortunately, the attention matrix is often sparse, particularly for long
sequences, suggesting an opportunity for optimization. Block-sparse attention
has emerged as a promising solution that partitions sequences into blocks and
skips computation for a subset of these blocks. However, the effectiveness of
this method is highly dependent on the underlying attention patterns, which can
lead to sub-optimal block-level sparsity. For instance, important key tokens
for queries within a single block may be scattered across numerous other
blocks, leading to computational redundancy. In this work, we propose Permuted
Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that
leverages the permutation properties of attention to increase block-level
sparsity and enhance the computational efficiency of LLM prefilling. We conduct
comprehensive experiments on challenging real-world long-context datasets,
demonstrating that PBS-Attn consistently outperforms existing block-sparse
attention methods in model accuracy and closely matches the full attention
baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn
achieves an end-to-end speedup of up to $2.75\times$ in long-context
prefilling, confirming its practical viability. Code available at
https://github.com/xinghaow99/pbs-attn

</details>


### [33] [PARL: Prompt-based Agents for Reinforcement Learning](https://arxiv.org/abs/2510.21306)
*Yarik Menchaca Resendiz,Roman Klinger*

Main category: cs.CL

TL;DR: 作者提出PARL方法，将LLM通过提示作为RL智能体应用于非语言推理任务。在简单环境下表现优秀但在复杂任务中受限，为LLM在RL领域的应用提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在自然语言表达的任务上表现出色，但其在强化学习（RL）环境中的应用和性能评估有限，尤其是在非语言类的推理任务方面。作者希望探索和扩展LLM在这些场景下的能力。

Method: 提出了一种无需微调、基于提示（prompt）的LLM强化学习智能体PARL，将动作、状态和奖励通过prompt编码，让模型能通过试错和环境交互学习。

Result: PARL在三项标准的非纯语言RL任务中与传统RL智能体进行了对比。结果表明，在简单环境中，PARL能够凭借预训练知识达到甚至超过传统RL方法，但在涉及较复杂数学运算或状态解码的任务上，性能存在限制。

Conclusion: 基于提示的LLM在无需微调的情况下，能作为RL智能体，在部分简单任务中表现优异，但在高复杂性任务上受到局限。该方法拓展了LLM应用于非语言推理场景的可能性。

Abstract: Large language models (LLMs) have demonstrated high performance on tasks
expressed in natural language, particularly in zero- or few-shot settings.
These are typically framed as supervised (e.g., classification) or unsupervised
(e.g., clustering) problems. However, limited work evaluates LLMs as agents in
reinforcement learning (RL) tasks (e.g., playing games), where learning occurs
through interaction with an environment and a reward system. While prior work
focused on representing tasks that rely on a language representation, we study
structured, non-linguistic reasoning - such as interpreting positions in a grid
world. We therefore introduce PARL (Prompt-based Agent for Reinforcement
Learning), a method that uses LLMs as RL agents through prompting, without any
fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling
the model to learn through trial-and-error interaction. We evaluate PARL on
three standard RL tasks that do not entirely rely on natural language. We show
that it can match or outperform traditional RL agents in simple environments by
leveraging pretrained knowledge. However, we identify performance limitations
in tasks that require complex mathematical operations or decoding states and
actions.

</details>


### [34] [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)
*Ji Won Park,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 论文提出了一种结合自然语言推断、语义多样性惩罚和采样效率提升的新采样方法，实现了在QA任务中更高效、更多样的答案生成及不确定性估计，适用于实际的模型风险管理场景。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）进行自由形式问答（QA）时，准确估计语义的不确定性（包括随机不确定性和知识不确定性）极具挑战性，且常需生成大量样本以获得稳定结果，成本高昂。

Method: 提出了一种多样性引导采样器，在解码过程中通过注入语义相似度惩罚，鼓励输出具有语义多样性的答案。该方法结合了自然语言推断（NLI）模型，对部分前缀或扩散状态进行微调，实现对自回归和掩码扩散范式的覆盖。同时，采用重要性重加权进行偏差校正，用控制变量降低不确定性估计的方差。

Result: 在四个QA基准任务上，该方法在相同样本数下覆盖了更多语义簇，且在不确定性估计方面表现与主流方法相当或更好。

Conclusion: 该方法无需获取基础模型梯度，模块化设计，可作为增强风险敏感模型部署中不确定性估计的即插即用工具。

Abstract: Accurately estimating semantic aleatoric and epistemic uncertainties in large
language models (LLMs) is particularly challenging in free-form question
answering (QA), where obtaining stable estimates often requires many expensive
generations. We introduce a diversity-steered sampler that discourages
semantically redundant outputs during decoding, covers both autoregressive and
masked diffusion paradigms, and yields substantial sample-efficiency gains. The
key idea is to inject a continuous semantic-similarity penalty into the model's
proposal distribution using a natural language inference (NLI) model lightly
finetuned on partial prefixes or intermediate diffusion states. We debias
downstream uncertainty estimates with importance reweighting and shrink their
variance with control variates. Across four QA benchmarks, our method matches
or surpasses baselines while covering more semantic clusters with the same
number of samples. Being modular and requiring no gradient access to the base
LLM, the framework promises to serve as a drop-in enhancement for uncertainty
estimation in risk-sensitive model deployments.

</details>


### [35] [Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words](https://arxiv.org/abs/2510.21326)
*Gianluca Sperduti,Alejandro Moreo*

Main category: cs.CL

TL;DR: 尽管乱序处理会合并一些英文单词，实际上下文能辅助消歧，模型（如BERT）性能下降很有限，具有很好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 最近一些NLP模型表现出对词内部字母扰动的鲁棒性，这与人类能读懂打乱字母顺序的英文单词（typoglycemia现象）有关。作者希望弄清楚，尽管许多不同的单词在typoglycemia下会变为相同表示，模型为什么还能表现良好。

Method: (1) 分析英国国家语料库，量化typoglycemia下的单词合并和歧义情况；(2) 评估BERT对合并词形的消歧能力；(3) 对BERT进行探针实验，比较其在正常文本与typoglycemia处理的Wikipedia文本上训练的表现。

Result: 实验结果显示，typoglycemia导致的模型性能下降比预期要小。也就是说，词内部乱序带来的影响有限。

Conclusion: 只有少数英文单词在typoglycemia下会发生合并，且这些词在语境中容易被区分，因此模型对乱序鲁棒性较高。

Abstract: Research in linguistics has shown that humans can read words with internally
scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP
models have recently been proposed that similarly demonstrate robustness to
such distortions by ignoring the internal order of characters by design. This
raises a fundamental question: how can models perform well when many distinct
words (e.g., form and from) collapse into identical representations under
typoglycemia? Our work, focusing exclusively on the English language, seeks to
shed light on the underlying aspects responsible for this robustness. We
hypothesize that the main reasons have to do with the fact that (i) relatively
few English words collapse under typoglycemia, and that (ii) collapsed words
tend to occur in contexts so distinct that disambiguation becomes trivial. In
our analysis, we (i) analyze the British National Corpus to quantify word
collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to
disambiguate collapsing forms, and (iii) conduct a probing experiment by
comparing variants of BERT trained from scratch on clean versus typoglycemic
Wikipedia text; our results reveal that the performance degradation caused by
scrambling is smaller than expected.

</details>


### [36] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: 本文提出TripTide基准，用多指标和多种评估方式，系统性分析了大模型应对实际旅行突发状况的修订能力，发现大模型能保持语义和顺序一致，但应对复杂扰动时仍有明显弱点，并首次为相关任务建立了标准评测体系。


<details>
  <summary>Details</summary>
Motivation: 目前针对个性化、约束敏感的旅行计划生成已有成果，但实际旅行中经常会出现各种突发状况，现有方法缺乏对这些实际中断情景的处理能力评估。该论文希望衡量大模型对真实世界干扰情境下计划修订的适应性。

Method: 论文提出TripTide，是第一个专注于评估大模型在现实扰动下修订旅行计划能力的基准。TripTide涵盖扰动严重程度和旅行者容忍度，搭建多维度评估体系，包括自动化指标如意图保持、响应能力、适应性，同时采用LLM判官和人工专家评估计划修订质量。

Result: 实验发现，大模型在顺序一致性和语义稳定性方面表现良好，但空间偏离在短途计划中较大，长途计划则更地理一致；同时，随着计划长度增加，应对扰动的能力下降，揭示了大模型在鲁棒性上的局限。

Conclusion: TripTide建立了第一个针对大模型旅游规划在真实世界不确定性下的适应性、个性化和韧性评估基准，为未来相关模型性能提升和评测方法标准化提供了重要参考。

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [37] [Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning](https://arxiv.org/abs/2510.21339)
*Qiang Liu,Wuganjing Song,Zhenzhou Lin,Feifan Chen,Qiaolong Cai,Chen Li,Yongduo Sui*

Main category: cs.CL

TL;DR: 在推理任务中，单轮训练比多轮训练更有效，多轮人类反馈训练可能反而削弱模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要通过单轮强化学习进行推理能力训练，而实际应用往往涉及到多轮与人的互动反馈，导致训练与部署条件可能不匹配。该工作旨在探究推理任务中是否有必要采用多轮人类反馈训练。

Method: 将传统的单轮训练方法与三种多轮训练策略进行对比，分析它们在推理任务中的泛化与表现。

Result: 单轮训练的模型对于单轮和多轮评估都能够很好地泛化，而采用多轮训练策略的模型在单轮推理性能上显著下降。

Conclusion: 对于信息完整的推理任务，稳健的单轮训练仍然更有效且更可靠，基本多轮反馈训练带来的好处有限，甚至可能降低推理能力。

Abstract: The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.

</details>


### [38] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 瑞典专属问答基准揭示：小型瑞典化模型对本地知识记忆效果可匹敌更大的多语种模型，但语言适配过程中会有信息遗忘，为跨语言模型诊断提供新工具。


<details>
  <summary>Details</summary>
Motivation: 现有瑞典基准数据多为翻译自以美国为中心的基准，不适合测试与瑞典相关或特有的知识。

Method: 人工编写针对瑞典人物和事件的问答基准，内容受瑞典流行广播和体育赛事启发，并包含英文翻译，可用于跨语言一致性测量。

Result: 小型、瑞典语覆盖较强的模型在回忆瑞典相关事实时表现与体积大三倍的多语种模型相当。瑞典语的持续预训练提升了事实知识，但会丢失部分原有信息。

Conclusion: 该数据集可作为诊断工具，用于研究多语种模型语言适配和知识保持过程。

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


### [39] [SindBERT, the Sailor: Charting the Seas of Turkish NLP](https://arxiv.org/abs/2510.21364)
*Raphael Scheible-Schmitt,Stefan Schweter*

Main category: cs.CL

TL;DR: SindBERT是首个大规模土耳其语RoBERTa编码器，基于312GB文本训练，表现优越但未因扩展模型规模而持续提升，强调语料质量及多样性比单纯扩大数据量更重要；模型及代码已经开源发布。


<details>
  <summary>Details</summary>
Motivation: 变形丰富语言在大规模预训练中往往被忽视，土耳其语缺乏专属的大型语言模型。该工作旨在填补土耳其语NLP领域RoBERTa编码器的空白。

Method: 从零开始，在312GB的土耳其语文本（mC4, OSCAR23, Wikipedia）上训练RoBERTa模型，并开放两个配置（base和large）。将SindBERT在词性标注、命名实体识别、攻击性语言检测、以及TurBLiMP语言可接受性基准测试等任务上进行评估。

Result: SindBERT在部分任务上优于现存土耳其语和多语言模型，尤其large版本在四项任务中有两项取得最佳表现；但整体上未显示明显的规模优势，暗示现有土耳其语基准已趋于饱和。此外，与小规模但高质量语料训练的模型（如BERTurk）对比，发现语料质量和多样性比数据量更关键。

Conclusion: SindBERT不仅作为开放资源推动土耳其语NLP发展，也为规模扩展与语料构成在形态丰富语言中作用提供了实证分析。该模型已以MIT协议公开，并支持fairseq和Huggingface格式。

Abstract: Transformer models have revolutionized NLP, yet many morphologically rich
languages remain underrepresented in large-scale pre-training efforts. With
SindBERT, we set out to chart the seas of Turkish NLP, providing the first
large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB
of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base
and large configurations, representing the first large-scale encoder-only
language model available for Turkish. We evaluate SindBERT on part-of-speech
tagging, named entity recognition, offensive language detection, and the
TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT
performs competitively with existing Turkish and multilingual models, with the
large variant achieving the best scores in two of four tasks but showing no
consistent scaling advantage overall. This flat scaling trend, also observed
for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
saturated. At the same time, comparisons with smaller but more curated models
such as BERTurk highlight that corpus quality and diversity can outweigh sheer
data volume. Taken together, SindBERT contributes both as an openly released
resource for Turkish NLP and as an empirical case study on the limits of
scaling and the central role of corpus composition in morphologically rich
languages. The SindBERT models are released under the MIT license and made
available in both fairseq and Huggingface formats.

</details>


### [40] [HalleluBERT: Let every token that has meaning bear its weight](https://arxiv.org/abs/2510.21372)
*Raphael Scheible-Schmitt*

Main category: cs.CL

TL;DR: HalleluBERT是用希伯来语大规模数据训练的RoBERTa编码器，超越了现有模型，成为希伯来语NLP的新标准。


<details>
  <summary>Details</summary>
Motivation: 现有的希伯来语Transformer模型（如HeBERT、AlephBERT和HeRo）受限于数据集规模、词表或训练深度，希伯来语领域缺乏大规模RoBERTa编码器。

Method: 提出并从零开始训练了HalleluBERT（base和large版本），在49.1GB经去重的希伯来语网页文本和Wikipedia上，采用希伯来语专用的字节级BPE词表。

Result: 在NER和情感分类任务上，HalleluBERT优于现有的单语和多语基线模型，在希伯来语任务上表现出最好的效果。

Conclusion: HalleluBERT成为希伯来语领域新的技术标杆，充分展示了单语预训练模型的优势。

Abstract: Transformer-based models have advanced NLP, yet Hebrew still lacks a
large-scale RoBERTa encoder which is extensively trained. Existing models such
as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or
training depth. We present HalleluBERT, a RoBERTa-based encoder family (base
and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and
Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER
and sentiment classification benchmarks, HalleluBERT outperforms both
monolingual and multilingual baselines. HalleluBERT sets a new state of the art
for Hebrew and highlights the benefits of fully converged monolingual
pretraining.

</details>


### [41] [Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings](https://arxiv.org/abs/2510.21424)
*Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray*

Main category: cs.CL

TL;DR: 本文针对VLMs在远程健康监测中用于人体活动识别的应用，提出了新数据集和评估方法，实验证明VLMs性能优异，有望推动其在智能医疗系统的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在医疗领域应用前景广阔，但其在远程健康监测中的人体活动识别（HAR）方面仍然研究较少，且现有深度学习模型存在局限性。VLMs 能提供更大灵活性，但其动态、非确定性输出的评估存在挑战。

Method: 提出了一个描述性字幕数据集，并设计了全面的评估方法，用于衡量VLMs在人体活动识别中的表现。之后将VLMs与当前主流深度学习模型进行了对比实验。

Result: 实验结果显示，VLMs在识别准确率方面可达到甚至超过传统深度学习方法。

Conclusion: 本研究为VLMs在人类活动识别领域建立了有力的基准，并推动了其在智能医疗系统中的集成应用。

Abstract: As generative AI continues to evolve, Vision Language Models (VLMs) have
emerged as promising tools in various healthcare applications. One area that
remains relatively underexplored is their use in human activity recognition
(HAR) for remote health monitoring. VLMs offer notable strengths, including
greater flexibility and the ability to overcome some of the constraints of
traditional deep learning models. However, a key challenge in applying VLMs to
HAR lies in the difficulty of evaluating their dynamic and often
non-deterministic outputs. To address this gap, we introduce a descriptive
caption data set and propose comprehensive evaluation methods to evaluate VLMs
in HAR. Through comparative experiments with state-of-the-art deep learning
models, our findings demonstrate that VLMs achieve comparable performance and,
in some cases, even surpass conventional approaches in terms of accuracy. This
work contributes a strong benchmark and opens new possibilities for the
integration of VLMs into intelligent healthcare systems.

</details>


### [42] [Redefining Retrieval Evaluation in the Era of LLMs](https://arxiv.org/abs/2510.21440)
*Giovanni Trappolini,Florin Cuconasu,Simone Filice,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 传统IR指标难以评估RAG系统的真实表现，因为LLM不按人类的顺序处理文档，且部分文档会对生成结果造成负面影响。本文提出结合正负贡献的新标注方式和UDCG指标，大幅提升了评估与实际结果的相关性。


<details>
  <summary>Details</summary>
Motivation: 传统的信息检索（IR）指标如nDCG、MAP和MRR假设用户会按顺序逐步浏览文档，并对于排名靠后的文档关注度递减。然而在检索增强生成(RAG)系统中，搜索结果由大型语言模型（LLM）整体处理而不是按顺序，此外传统指标未考虑相关但无用文档对生成质量的主动负面影响。因此，传统IR指标无法准确衡量RAG系统的性能。

Method: 提出了一种以效用为导向的标注方案，能够同时量化相关文档的正效用以及分散注意力文档的负效用。基于此方案，创新性提出了一种面向LLM的效用与干扰累积增益（UDCG）指标，对传统排名折扣进行调整，以更准确预测端到端答案准确性。

Result: 在五个数据集和六个LLM上进行实验，结果显示UDCG与答案准确率的相关性比传统指标提升最高可达36%。

Conclusion: UDCG能够更准确评估RAG系统表现，有效对齐评估方式和LLM实际处理方式，推动RAG组件评价的可靠性。

Abstract: Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components

</details>


### [43] [REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring](https://arxiv.org/abs/2510.21445)
*Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray*

Main category: cs.CL

TL;DR: 本研究提出了一种集成多模态大语言模型和物联网的远程健康监测系统，通过可穿戴设备与摄像头自动采集多模态数据，实现异常检测和自然语言人机交互，初步验证了其实用性和可扩展性，有望显著减轻医护压力和医疗成本。


<details>
  <summary>Details</summary>
Motivation: 尽管可穿戴设备和远程监测已在医疗领域广泛应用，但目前绝大多数研究仅聚焦于数据采集、可视化及针对特定疾病的异常检测，人机交互模块则明显不足。

Method: 提出了一个自适应的远程健康监测系统REMONI，集成多模态大语言模型（MLLMs）、物联网（IoT）与可穿戴设备。系统自动采集生命体征、加速度和视频数据，通过异常检测模块（包括跌倒检测），并内置自然语言处理组件，借助大模型识别活动、情绪，与医护人员自然交流。通过Prompt Engineering整合所有信息，医生可通过网页应用实时查询患者状态。

Result: 该系统已开发出原型并进行测试，结果显示其能够真实场景部署，具备可扩展性，有望减轻医护人员负担、降低医疗成本。

Conclusion: REMONI突破了远程健康监测领域缺乏高效人机交互的局限，实现了数据采集、异常检测与自然语言交互的无缝集成，对医疗场景具有实用性和推广前景。

Abstract: With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.

</details>


### [44] [MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization](https://arxiv.org/abs/2510.21473)
*Chenglong Wang,Yang Gan,Hang Zhou,Chi Hu,Yongyu Mu,Kai Song,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Jingbo Zhu,Zhengtao Yu,Tong Xiao*

Main category: cs.CL

TL;DR: 扩散语言模型在推理上不如大语言模型，原因在于未能捕捉标记相关性。论文提出MRO多重奖励优化方法，通过强化学习等手段显著提升推理性能和采样速度，为未来DLM应用提供重要进展。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）作为自回归大语言模型（LLMs）的替代方案，在推理性能上仍有差距，尤其是在去噪步骤较少时。论文分析发现问题主要源于去噪步骤中对标记的独立生成，未能捕捉标记间的相关性。

Method: 提出了Multi-Reward Optimization (MRO)方法，通过测试时缩放、拒绝采样和强化学习直接优化标记的相关性，并设计多种奖励函数。同时引入组步和重要性采样策略以减少奖励方差并提升采样效率。

Result: 大量实验证明，MRO方法不仅提升了扩散语言模型的推理性能，同时在保持高推理基准性能的前提下，大幅提升了采样速度。

Conclusion: 增强扩散语言模型中的标记相关性对于推理任务至关重要。MRO方法利用多重奖励优化策略显著改善了DLM的推理能力和采样效率，为DLMs成为LLMs有力替代方案提供了新途径。

Abstract: Recent advances in diffusion language models (DLMs) have presented a
promising alternative to traditional autoregressive large language models
(LLMs). However, DLMs still lag behind LLMs in reasoning performance,
especially as the number of denoising steps decreases. Our analysis reveals
that this shortcoming arises primarily from the independent generation of
masked tokens across denoising steps, which fails to capture the token
correlation. In this paper, we define two types of token correlation:
intra-sequence correlation and inter-sequence correlation, and demonstrate that
enhancing these correlations improves reasoning performance. To this end, we
propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to
consider the token correlation during the denoising process. More specifically,
our MRO approach leverages test-time scaling, reject sampling, and
reinforcement learning to directly optimize the token correlation with multiple
elaborate rewards. Additionally, we introduce group step and importance
sampling strategies to mitigate reward variance and enhance sampling
efficiency. Through extensive experiments, we demonstrate that MRO not only
improves reasoning performance but also achieves significant sampling speedups
while maintaining high performance on reasoning benchmarks.

</details>


### [45] [Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models](https://arxiv.org/abs/2510.21520)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: 本文提出多被试脑调优方法，使预训练语言模型能有效泛化到新被试，极大提高了脑数据对齐和下游语义任务表现，推动了神经科学与AI领域的交叉发展。


<details>
  <summary>Details</summary>
Motivation: 目前预训练语言模型在模拟大脑对自然语言刺激的反应上表现优异，但现有的方法在脑活动对齐估计与提升过程中依赖具体被试个体，且对每个被试的数据量十分敏感，限制了泛化能力和群体水平的分析。

Method: 本文提出了一种可扩展、具有泛化能力的多被试脑调优方法，通过联合微调预训练语音语言模型，使其能预测来自多个被试的fMRI反应。

Result: 所提出的脑调优模型在保证个体脑对齐效果的同时，具备了良好的群体泛化能力：1）将新被试所需fMRI数据减少5倍，2）整体脑对齐提升至多50%，3）对新数据集也具备强泛化能力。此外，多被试脑调优还能提升下游语义任务表现，说明它提升了语义表示的通用性。

Conclusion: 多被试脑调优方法促进了神经科学与人工智能的融合，不仅提升了模型的脑对齐和泛化性，也为研究大脑语言处理机制提供了新工具。

Abstract: Pretrained language models are remarkably effective in aligning with human
brain responses elicited by natural language stimuli, positioning them as
promising model organisms for studying language processing in the brain.
However, existing approaches for both estimating and improving this brain
alignment are participant-dependent and highly affected by the amount of data
available per participant, hindering both generalization to new participants
and population-level analyses. In this work, we address these limitations by
introducing a scalable, generalizable brain-tuning method, in which we
fine-tune pretrained speech language models to jointly predict fMRI responses
from multiple participants. We demonstrate that the resulting brain-tuned
models exhibit strong individual brain alignment while generalizing across
participants. Specifically, our method leads to 1) a 5-fold decrease in the
amount of fMRI data needed to predict brain data from new participants, 2) up
to a 50% increase in the overall brain alignment, and 3) strong generalization
to new unseen datasets. Furthermore, this multi-participant brain-tuning
additionally improves downstream performance on semantic tasks, suggesting that
training using brain data from multiple participants leads to more
generalizable semantic representations. Taken together, these findings
demonstrate a bidirectional benefit between neuroscience and AI, helping bridge
the gap between the two fields. We make our code and models publicly available
at https://github.com/bridge-ai-neuro/multi-brain-tuning.

</details>


### [46] [InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.21538)
*Likun Tan,Kuan-Wei Huang,Joy Shi,Kevin Wu*

Main category: cs.CL

TL;DR: 本文发现RAG幻觉源自后层FFN模块注入参数知识，提出利用机制信号（外部上下文分数与参数知识分数）进行幻觉检测的方法，并在多LLM上验证了其高效与可推广性，优于现有基线，为更准确的RAG幻觉检测提供了新方向。


<details>
  <summary>Details</summary>
Motivation: RAG模型虽然通过外部知识减少幻觉现象，但输出仍存在与检索内容不一致的问题。现有方法未能有效区分外部上下文与模型参数知识对幻觉的贡献，需要一种更精准的检测方法。

Method: 分析RAG幻觉产生机制，发现其源于后层FFN模块过度注入参数知识。提出基于外部上下文分数和参数知识分数的机制化检测方法，利用Qwen3-0.6b计算各层和注意力头的分数，并训练回归分类器预测幻觉。同时在主流LLM和检测基线下进行对比评估，并验证模型泛化能力。

Result: 所提出的分类器不仅在Qwen3-0.6b上有效，也能泛化到GPT-4.1-mini回应，表现优于当前基线方法。机制信号成为高效且可推广的RAG幻觉检测指标。

Conclusion: 通过机制化信号可实现高效且通用的RAG幻觉检测方法，为后续幻觉检测工具的开发提供了新思路，有望提升RAG系统输出的可信度。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to
mitigate hallucinations, yet models often generate outputs inconsistent with
retrieved content. Accurate hallucination detection requires disentangling the
contributions of external context and parametric knowledge, which prior methods
typically conflate. We investigate the mechanisms underlying RAG hallucinations
and find they arise when later-layer FFN modules disproportionately inject
parametric knowledge into the residual stream. To address this, we explore a
mechanistic detection approach based on external context scores and parametric
knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and
attention heads and train regression-based classifiers to predict
hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,
GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,
classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,
demonstrating the potential of proxy-model evaluation. Our results highlight
mechanistic signals as efficient, generalizable predictors for hallucination
detection in RAG systems.

</details>


### [47] [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/abs/2510.21553)
*Jared Claypoole,Yunye Gong,Noson S. Yanofsky,Ajay Divakaran*

Main category: cs.CL

TL;DR: 本文以范畴论建模文档结构，实现信息分解、测度、总结与自我提升方法，并提出多模态扩展及通过一致性自监督优化大模型。


<details>
  <summary>Details</summary>
Motivation: 目前对多模态文档缺乏统一的信息结构表示与测度方法，传统总结和扩展受限于信息冗余与模型能力，亟需数学化的统一框架提升模型表现与应用能力。

Method: 首先将文档数学建模为问答对的范畴结构，然后通过正交分解提取互不重叠的信息块。结合这些结构，提出信息测度、枚举、和新型文本总结方法，并应用于大模型实现和多模态扩展。采用基于RLVR的自监督一致性约束(如可组合性与封闭性)进一步优化预训练大模型。

Result: 实现了范畴理论下的文档结构抽取、新型信息测度和总结技术，并使大模型在一致性和信息处理能力上获得自监督提升。还提出了可扩展至多模态的范畴化框架。

Conclusion: 本文提出了一种基于范畴论的多模态文档结构抽取与分析方法，不仅能进行信息测度、内容总结与扩展，还能自监督地提升大模型。

Abstract: We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.

</details>


### [48] [Are the LLMs Capable of Maintaining at Least the Language Genus?](https://arxiv.org/abs/2510.21561)
*Sandra Mitrović,David Kletz,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: LLMs在多语言处理中会体现出家族语言相关性，但这种效果主要受训练数据量分布影响，说明数据平衡是提升多语言能力的关键。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多语言表现上存在显著差异，但家族语言结构对这种差异的影响尚未被充分研究。

Method: 基于MultiQ数据集，分析了LLMs在提示语言不保持时是否倾向于切换到有血缘关联的语言，并研究了模型在同一语系内外对知识一致性的保持能力。比较了不同LLMs家族的多语言策略。

Result: 发现家族层面的影响确实存在，但模型表现受训练资源分布极大影响。同时，不同LLMs家族展现了不同的多语言处理策略。

Conclusion: LLMs在一定程度上编码了语言家族的结构信息，但训练数据的不平衡仍是其多语言表现的主要影响因素。

Abstract: Large Language Models (LLMs) display notable variation in multilingual
behavior, yet the role of genealogical language structure in shaping this
variation remains underexplored. In this paper, we investigate whether LLMs
exhibit sensitivity to linguistic genera by extending prior analyses on the
MultiQ dataset. We first check if models prefer to switch to genealogically
related languages when prompt language fidelity is not maintained. Next, we
investigate whether knowledge consistency is better preserved within than
across genera. We show that genus-level effects are present but strongly
conditioned by training resource availability. We further observe distinct
multilingual strategies across LLMs families. Our findings suggest that LLMs
encode aspects of genus-level structure, but training data imbalances remain
the primary factor shaping their multilingual performance.

</details>


### [49] [From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene](https://arxiv.org/abs/2510.21575)
*Mojca Brglez,Špela Vintar*

Main category: cs.CL

TL;DR: 本论文提出了斯洛文尼亚语首个语用学理解基准，并用大语言模型进行了测试。结果说明模型在细腻语言理解上有进步但在非字面、文化特定表达推断上仍有不足，且开源与私有模型差距明显。作者呼吁未来相关基准需基于本土数据并以人类做对照验证。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在一些高难度基准测试中表现优异，但这些测试主要关注表层语言能力，缺乏对语言实际使用语境和文化语境的考察。语言能力不仅包括语法和语义，还需考虑语用学，即语境及文化规范下的意义理解。作者提出需要更加具有挑战性的评估方法，以检验模型对语言细微、语用层面能力的掌握。

Method: 作者提出了SloPragEval和SloPragMega，这是斯洛文尼亚语首个语用理解基准，包括405个选择题；描述了翻译挑战、人类基线的建立过程，并使用大型语言模型进行了初步测试。

Result: 结果显示目前的模型在理解细腻语言上已取得明显进步，但在推断非字面化表达（尤其是文化特定含义）方面仍有不足；私有模型与开源模型之间存在显著差距。

Conclusion: 针对细腻语言理解和特定文化知识的基准需谨慎设计，优选本土数据，并通过人类响应进行验证，以推动模型在语用层面理解能力的提升。

Abstract: Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

</details>


### [50] [Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist](https://arxiv.org/abs/2510.21584)
*Kellen Parker van Dam,Abishek Stephen*

Main category: cs.CL

TL;DR: 提出无监督异常检测方法，利用音系特征识别语言数据中的错误和外来词，在Kokborok和孟加拉语数据集上验证有效，尤其音节级特征表现突出，助力低资源语言数据质控。


<details>
  <summary>Details</summary>
Motivation: 语言文献中词汇数据常包含转写错误及未记录的外来词，这些问题会影响语言学分析。缺乏自动化手段让野外工作者有效发现数据异常。

Method: 提出无监督异常检测方法，利用基于字符和音节的音系特征，检测词表中音系不一致性。算法应用于多语种Kokborok及孟加拉语数据集，对潜在的转写错误和借词进行识别。

Result: 音节级特征在识别异常方面明显优于字符级基线方法。算法总体查全率较高，但查准率和查全率都受到异常微妙特性的影响。

Conclusion: 提出的方法可为低资源语言文献数据的质量提升提供系统辅助，帮助研究者高效标记需要复核的数据条目。

Abstract: Lexical data collection in language documentation often contains
transcription errors and undocumented borrowings that can mislead linguistic
analysis. We present unsupervised anomaly detection methods to identify
phonotactic inconsistencies in wordlists, applying them to a multilingual
dataset of Kokborok varieties with Bangla. Using character-level and
syllable-level phonotactic features, our algorithms identify potential
transcription errors and borrowings. While precision and recall remain modest
due to the subtle nature of these anomalies, syllable-aware features
significantly outperform character-level baselines. The high-recall approach
provides fieldworkers with a systematic method to flag entries requiring
verification, supporting data quality improvement in low-resourced language
documentation.

</details>


### [51] [RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models](https://arxiv.org/abs/2510.21604)
*Xueyuan Lin,Cehao Yang,Ye Ma,Ming Li,Rongjunchen Zhang,Yang Ni,Xiaojun Wu,Chengjin Xu,Jian Guo,Hui Xiong*

Main category: cs.CL

TL;DR: 提出了针对金融领域LLM推理弱点的RETuning方法，强化独立分析和多证据整合能力，实验验证了方法在A股数据集上的有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学和编程等任务上表现出色，但在金融任务，特别是关键的股票涨跌预测领域的应用和能力尚未充分挖掘。现有LLM在推理时存在依赖分析师观点、缺乏系统独立分析等缺陷。

Method: 提出了一种名为Reflective Evidence Tuning（RETuning）的方法，在生成推理链(CoT)的过程中，鼓励模型动态构建来自多元信息源的分析框架，并基于该框架组织、评分多方证据，最后反思形成预测。此外，构建了涵盖5123只A股、长上下文（32K tokens）、超20万样本的新大规模数据集。

Result: RETuning方法能有效激发LLM在金融领域的推理能力。即使在6个月后或对分布外股票推理时，该方法依然具有良好泛化与推理性能。

Conclusion: 通过RETuning，LLM在股票涨跌预测等金融任务上获得更独立、系统的推理能力，显著提升了预测可靠性与泛化能力。

Abstract: Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

</details>


### [52] [The Universal Landscape of Human Reasoning](https://arxiv.org/abs/2510.21623)
*Qiguang Chen,Jinhao Liu,Libo Qin,Yimeng Zhang,Yihao Liang,Shangxu Ren,Chengyu Luan,Dengyun Peng,Hanjing Li,Jiannan Guan,Zheng Yan,Jiaqi Wang,Mengkang Hu,Yantao Du,Zhi Chen,Xie Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 该文提出用大语言模型量化人类推理过程中的信息流动，首次用统一标准探究推理动态和个体差异，为理论和测量建立定量联系并深化推理机制理解。


<details>
  <summary>Details</summary>
Motivation: 人类推理过程中信息是如何动态积累和转化的，一直是心理学、哲学和人工智能领域的难题。现有模型大多只关注推理输出或个人建模，缺乏统一且可量化的关于推理动态的描述。

Method: 提出了一种名为信息流追踪（IF-Track）的方法，利用大语言模型（LLMs）作为概率编码器，对推理过程中每一步的信息熵和信息增益进行量化，并在多种推理任务中进行细致分析。

Result: IF-Track首次在单一度量空间下成功建模了人类普遍推理行为，能够捕捉推理的核心特征、识别系统性错误模式，并揭示个体差异。此外，IF-Track在先进心理学理论讨论中能够调和单/双过程理论，并揭示人工智能和人类认知的对齐现象，以及大语言模型对人类推理过程的重塑。

Conclusion: IF-Track为推理理论与实际测量之间建立了定量桥梁，提供了推理结构的机制性见解，推动人类与人工智能推理机制研究的融合与进步。

Abstract: Understanding how information is dynamically accumulated and transformed in
human reasoning has long challenged cognitive psychology, philosophy, and
artificial intelligence. Existing accounts, from classical logic to
probabilistic models, illuminate aspects of output or individual modelling, but
do not offer a unified, quantitative description of general human reasoning
dynamics. To solve this, we introduce Information Flow Tracking (IF-Track),
that uses large language models (LLMs) as probabilistic encoder to quantify
information entropy and gain at each reasoning step. Through fine-grained
analyses across diverse tasks, our method is the first successfully models the
universal landscape of human reasoning behaviors within a single metric space.
We show that IF-Track captures essential reasoning features, identifies
systematic error patterns, and characterizes individual differences. Applied to
discussion of advanced psychological theory, we first reconcile single- versus
dual-process theories in IF-Track and discover the alignment of artificial and
human cognition and how LLMs reshaping human reasoning process. This approach
establishes a quantitative bridge between theory and measurement, offering
mechanistic insights into the architecture of reasoning.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [53] [On Local Limits of Sparse Random Graphs: Color Convergence and the Refined Configuration Model](https://arxiv.org/abs/2510.21392)
*Alexander Pluska,Sagar Malhotra*

Main category: cs.DM

TL;DR: 提出基于Weisfeiler-Leman算法的颜色收敛新概念，对消息传递型图神经网络在随机图上的极限行为作出刻画，并构建推广后的配置模型RCM，统一描述多类局部树状随机图的极限性质。


<details>
  <summary>Details</summary>
Motivation: 本研究受到近年来局部收敛对稀疏随机图分析重要作用的启发，旨在拓展其应用范围，解决现有局部收敛方法在描述图神经网络极限行为和更多随机图模型时的局限性。

Method: 提出了一种基于Weisfeiler-Leman算法的新型局部收敛概念——颜色收敛(color convergence)，并据此提出了推广后的配置模型（Refined Configuration Model, RCM）来统一刻画多种随机图模型。通过理论分析，对局部树状随机图模型的极限行为进行全面刻画。

Result: 颜色收敛概念可以完整表征在极限下具有良好性质的随机图类别，特别是针对消息传递类图神经网络。提出的RCM模型在所有局部树状的随机图模型中，是实现局部收敛的通用模型。此外，该理论框架实现了对上述随机图模型局部极限树的完整刻画。

Conclusion: 文中提出的颜色收敛和Refined Configuration Model为分析局部树状随机图模型以及理解图神经网络极限行为提供了统一且强有力的新工具，对随机图与深度学习的交叉领域有重要理论和实际意义。

Abstract: Local convergence has emerged as a fundamental tool for analyzing sparse
random graph models. We introduce a new notion of local convergence, color
convergence, based on the Weisfeiler-Leman algorithm. Color convergence fully
characterizes the class of random graphs that are well-behaved in the limit for
message-passing graph neural networks. Building on this, we propose the Refined
Configuration Model (RCM), a random graph model that generalizes the
configuration model. The RCM is universal with respect to local convergence
among locally tree-like random graph models, including Erd\H{o}s-R\'enyi,
stochastic block and configuration models. Finally, this framework enables a
complete characterization of the random trees that arise as local limits of
such graphs.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [54] [Language Equivalence is Undecidable in VASS with Restricted Nondeterminism](https://arxiv.org/abs/2510.21514)
*Wojciech Czerwiński,Łukasz Orlikowski*

Main category: cs.FL

TL;DR: 本文证明了二维确定性与历史确定性VASS之间的语言等价性不可判定，并揭示了历史确定性VASS的相互模拟等价于语言等价，从而拓展了VASS相关理论的不可判定边界。


<details>
  <summary>Details</summary>
Motivation: 此前已知VASS语言等价性一般不可判定，作者动机在于探索该不可判定性在更受限的确定性模型和历史确定性模型中的边界及性质。

Method: 将不可判定性质从一般VASS扩展到确定性与历史确定性VASS组合，通过分析两者相互模拟条件，证明等价性与模拟之间的关系，从而推导不可判定性。

Result: 扩展了语言等价判定不可解的边界，证明了在双二维确定性-历史确定性VASS间语言等价仍不可判定，并分析了这类系统的模拟关系。

Conclusion: 两维VASS在coverability条件下的语言等价性问题在一个为确定性VASS、另一个为历史确定性VASS的情况下依然不可判定。对于两个历史确定性VASS，它们语言等价当且仅当可以互相模拟，这推动了更广泛的不可判定性结论。

Abstract: In this work, we extend undecidability of language equivalence for
two-dimensional Vector Addition System with States (VASS) accepting by
coverability condition. We show that the problem is undecidable even when one
of the two-dimensional VASSs is deterministic and the other is
history-deterministic. Moreover, we observe, that the languages of two
history-deterministic VASSs are equal if and only if each can simulate the
other. This observation allows us to extend the undecidability to any
equivalence relation between two-sided simulation and language equivalence.

</details>
