<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Generating Compilers for Qubit Mapping and Routing](https://arxiv.org/abs/2508.10781)
*Abtin Molavi,Amanda Xu,Ethan Cecchetti,Swamit Tannu,Aws Albarghouthi*

Main category: cs.PL

TL;DR: 本文提出了一套自动化编译器生成框架，通过领域专用语言Marol和参数化求解器，可以高效且准确地解决各种量子比特映射与路由问题，有效跟上量子架构的快速迭代，极大简化了编译器开发流程。


<details>
  <summary>Details</summary>
Motivation: 针对量子计算领域中架构快速多样化、复杂且不断涌现新硬件导致传统手写编译器维护成本高、适应性差的问题，亟需一种自动化、通用的量子比特映射与路由编译器生成方法。

Method: 提出一种通用核心结构——设备状态机，并据此设计了领域专用语言Marol，通过Marol定义QMR问题，再用参数化求解器灵活地求解各种映射与路由问题。

Result: 展示了对多种主流量子硬件平台和错误校验方案的QMR问题案例研究，评估结果表明自动生成的编译器在运行速度及解决方案质量上与手写编译器相当。

Conclusion: 自动生成用于不同量子架构的量子比特映射与路由编译器的方法，生成的编译器在性能和质量上接近手写专用编译器。

Abstract: Quantum computers promise to solve important problems faster than classical
computers, potentially unlocking breakthroughs in materials science, chemistry,
and beyond. Optimizing compilers are key to realizing this potential, as they
minimize expensive resource usage and limit error rates. A critical compilation
step is qubit mapping and routing (QMR), which finds mappings from circuit
qubits to qubits on a target device and plans instruction execution while
satisfying the device's connectivity constraints. The challenge is that the
landscape of quantum architectures is incredibly diverse and fast-evolving.
Given this diversity, hundreds of papers have addressed the QMR problem for
different qubit hardware, connectivity constraints, and quantum error
correction schemes.
  We present an approach for automatically generating qubit mapping and routing
compilers for arbitrary quantum architectures. Though each QMR problem is
different, we identify a common core structure-device state machine-that we use
to formulate an abstract QMR problem. Our formulation naturally leads to a
domain-specific language, Marol, for specifying QMR problems-for example, the
well-studied NISQ mapping and routing problem requires only 12 lines of Marol.
We demonstrate that QMR problems, defined in Marol, can be solved with a
powerful parametric solver that can be instantiated for any Marol program. We
evaluate our approach through case studies of important QMR problems from prior
and recent work, covering noisy and fault-tolerant quantum architectures on all
major hardware platforms. Our thorough evaluation shows that generated
compilers are competitive with handwritten, specialized compilers in terms of
runtime and solution quality. We envision that our approach will simplify
development of future quantum compilers as new quantum architectures continue
to emerge.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: 提出Saracoder框架，通过多维度优化检索候选，显著提高仓库级代码补全表现，解决语义误导与符号歧义。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖文本相似度，导致语义误导、冗余、同质化及无法解决文件间符号歧义。

Method: 设计了层次化特征优化模块，结合深度语义关系、结构相似度、删除重复内容，及跨文件符号歧义消除模块，使用图结构编辑权重及依赖分析。

Result: 在CrossCodeEval与RepoEval-Updated基准上，Saracoder在多种语言和模型下显著超越主流基线方法。

Conclusion: 系统性地在多个维度优化检索结果，能有效提升代码补全的准确性和鲁棒性。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


### [3] [FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement](https://arxiv.org/abs/2508.10059)
*Yueke Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: FormalGrad将形式化约束引入大模型代码生成流程，实现更可靠高效的代码生成，在主流基准上显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码生成方面缺乏正确性、鲁棒性和效率保障，尤其在需要严格约束的领域问题突出。

Method: 将形式化方法融入LLM代码生成循环，并将代码视为可微分变量，通过伪梯度文本反馈引导模型迭代优化代码。

Result: 在HumanEval基准上绝对提升达27%，LiveCodeBench V6上相对提升达41%，显著优于强基线方法。

Conclusion: FormalGrad能够生成更加准确、鲁棒且高效的代码，推动高风险应用中的可靠AI辅助软件开发。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities
in code generation, they often produce solutions that lack guarantees of
correctness, robustness, and efficiency. The limitation is acute in domains
requiring strict constraints. FormalGrad introduces a principled framework that
integrates formal methods directly into an iterative LLM-based generation loop.
It uniquely treats code as a differentiable variable, converting structured
feedback and formal constraints into a textual pseudo-gradient. This gradient
guides the model to iteratively refine solutions, ensuring they are not only
functional but also robust and formally justified. We evaluate FormalGrad on
the HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation
outperforms strong baselines, achieving an absolute improvement of up to 27% on
HumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.
FormalGrad generates formally justified code that is robust and efficient,
paving the way for reliable AI-assisted software development in high-stakes
applications.

</details>


### [4] [Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History](https://arxiv.org/abs/2508.10074)
*Ruofan Lu,Yintong Huo,Meng Zhang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出下一步代码编辑预测任务，通过模型学习用户历史交互，实现对下一步内容及位置的主动预测，优化了开发者体验，并为AI编码助手提供了新的交互方式。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手方案存在局限：代码补全仅基于光标当前位置且范围有限，聊天编辑则需开发者切换上下文、用自然语言描述意图，影响体验。两者均未能做到主动预测开发者在一系列相关编辑中的下一个操作。

Method: 提出Next Edit Prediction（下一步编辑预测）任务，旨在利用最近的交互历史来同时预测下一个编辑的具体位置和内容。为此，作者构建了高质量的有监督微调数据集和评测基准，并在多个模型上进行了有监督微调及综合评估，对比了微调后模型和其他基线模型。

Result: 经过微调的模型在该任务上表现出色，实验中取得了一些新的发现，展现出新方法的有效性。

Conclusion: 该工作为AI辅助编码引入了新的交互范式：使编码助手能够主动预测和推荐下一步操作，从而实现更无缝的人机协作，而非仅响应用户的显式指令。

Abstract: The rapid advancement of large language models (LLMs) has led to the
widespread adoption of AI-powered coding assistants integrated into a
development environment. On one hand, low-latency code completion offers
completion suggestions but is fundamentally constrained to the cursor's current
position. On the other hand, chat-based editing can perform complex
modifications, yet forces developers to stop their work, describe the intent in
natural language, which causes a context-switch away from the code. This
creates a suboptimal user experience, as neither paradigm proactively predicts
the developer's next edit in a sequence of related edits. To bridge this gap
and provide the seamless code edit suggestion, we introduce the task of Next
Edit Prediction, a novel task designed to infer developer intent from recent
interaction history to predict both the location and content of the subsequent
edit. Specifically, we curate a high-quality supervised fine-tuning dataset and
an evaluation benchmark for the Next Edit Prediction task. Then, we conduct
supervised fine-tuning on a series of models and performed a comprehensive
evaluation of both the fine-tuned models and other baseline models, yielding
several novel findings. This work lays the foundation for a new interaction
paradigm that proactively collaborate with developers by anticipating their
following action, rather than merely reacting to explicit instructions.

</details>


### [5] [On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository](https://arxiv.org/abs/2508.10157)
*Ajibode Adekunle,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该论文系统分析了GitHub与Hugging Face平台间的预训练语言模型开发同步现状，发现同步碎片化严重，导致模型版本不统一和信息孤立，强调识别同步模式对于改善模型发布与监管的重要性。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PTLMs）在自然语言处理领域推动了诸如文本生成和翻译等任务的进步，但其在上游（如GitHub）和下游（如Hugging Face）平台间的开发协调面临诸多挑战，例如发布时间线不一致、版本管理混乱和模型复用受限。

Method: 作者对325个PTLM家族（共904个HF变体）进行了混合方法研究，分析了在GitHub和Hugging Face上的commit活动，并从延迟、同步类型和强度三个维度调查了跨平台同步模式。

Result: 研究发现，GitHub上的贡献者更关注模型版本指定、代码质量提升、性能优化和依赖管理；而Hugging Face上的贡献者则侧重于模型描述、数据集管理和推理设置。分析识别出八种不同的同步模式，其中部分同步模式（如分散同步和稀疏同步）较为常见，导致两平台的更改孤立，甚至某一平台被弃用。此碎片化风险可能造成用户获取到不完整、过时或行为不一致的模型。

Conclusion: 识别并理解这些同步模式对于提升PTLM发布流程的监管和可追溯性至关重要，有助于减少模型碎片化及其带来的负面影响。

Abstract: Pretrained language models (PTLMs) have advanced natural language processing
(NLP), enabling progress in tasks like text generation and translation. Like
software package management, PTLMs are trained using code and environment
scripts in upstream repositories (e.g., GitHub, GH) and distributed as variants
via downstream platforms like Hugging Face (HF). Coordinating development
between GH and HF poses challenges such as misaligned release timelines,
inconsistent versioning, and limited reuse of PTLM variants. We conducted a
mixed-method study of 325 PTLM families (904 HF variants) to examine how commit
activities are coordinated. Our analysis reveals that GH contributors typically
make changes related to specifying the version of the model, improving code
quality, performance optimization, and dependency management within the
training scripts, while HF contributors make changes related to improving model
descriptions, data set handling, and setup required for model inference.
Furthermore, to understand the synchronization aspects of commit activities
between GH and HF, we examined three dimensions of these activities -- lag
(delay), type of synchronization, and intensity -- which together yielded eight
distinct synchronization patterns. The prevalence of partially synchronized
patterns, such as Disperse synchronization and Sparse synchronization, reveals
structural disconnects in current cross-platform release practices. These
patterns often result in isolated changes -- where improvements or fixes made
on one platform are never replicated on the other -- and in some cases,
indicate an abandonment of one repository in favor of the other. Such
fragmentation risks exposing end users to incomplete, outdated, or behaviorally
inconsistent models. Hence, recognizing these synchronization patterns is
critical for improving oversight and traceability in PTLM release workflows.

</details>


### [6] [Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution](https://arxiv.org/abs/2508.10517)
*Likai Ye,Mengliang Li,Dehai Zhao,Jiamou Sun,Xiaoxue Ren*

Main category: cs.SE

TL;DR: Solidity合约跨版本迁移错误普遍，主流大模型修复有瓶颈。作者提出SMCFIXER框架结合专家知识和LLM，显著提升修复率，迁移准确率几乎完美。


<details>
  <summary>Details</summary>
Motivation: 随着Solidity不断更新以提升安全性和功能性，开发者在不同版本迁移时面临大量编译错误，加剧了代码的维护和迁移难度。因此，研究如何解决Solidity版本演变中的错误问题变得十分必要。

Method: 首先，作者通过实证研究分析了Solidity智能合约在不同版本编译时出现错误的比例和类型。随后，系统性地评估了多种大型语言模型（开源如LLaMA3、DeepSeek，闭源如GPT-4o、GPT-3.5-turbo）在修复版本迁移时编译错误的能力。最后，提出了SMCFIXER框架，融合专家知识检索与LLM修复机制，分三步进行：代码切片、知识检索、迭代修复。

Result: 实证研究发现，81.68%的合约在不同版本编译时会遇到错误，且86.92%为编译错误。LLM虽然具备一定修复能力，但对语义级错误效果有限且依赖提示工程。SMCFIXER框架通过融合专家知识，迁移修复准确率达96.97%，比GPT-4o提升24.24%。

Conclusion: Solidity版本升级导致合约高频编译错误，现有LLM修复能力不足。SMCFIXER结合专家知识与LLM，有效提升迁移修复准确率，表明领域内需开发更适配智能合约的LLM修复系统。

Abstract: Solidity, the dominant smart contract language for Ethereum, has rapidly
evolved with frequent version updates to enhance security, functionality, and
developer experience. However, these continual changes introduce significant
challenges, particularly in compilation errors, code migration, and
maintenance. Therefore, we conduct an empirical study to investigate the
challenges in the Solidity version evolution and reveal that 81.68% of examined
contracts encounter errors when compiled across different versions, with 86.92%
of compilation errors.
  To mitigate these challenges, we conducted a systematic evaluation of large
language models (LLMs) for resolving Solidity compilation errors during version
migrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)
and closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these
models exhibit error repair capabilities, their effectiveness diminishes
significantly for semantic-level issues and shows strong dependency on prompt
engineering strategies. This underscores the critical need for domain-specific
adaptation in developing reliable LLM-based repair systems for smart contracts.
  Building upon these insights, we introduce SMCFIXER, a novel framework that
systematically integrates expert knowledge retrieval with LLM-based repair
mechanisms for Solidity compilation error resolution. The architecture
comprises three core phases: (1) context-aware code slicing that extracts
relevant error information; (2) expert knowledge retrieval from official
documentation; and (3) iterative patch generation for Solidity migration.
Experimental validation across Solidity version migrations demonstrates our
approach's statistically significant 24.24% improvement over baseline GPT-4o on
real-world datasets, achieving near-perfect 96.97% accuracy.

</details>


### [7] [EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets](https://arxiv.org/abs/2508.10852)
*Souhaila Serbout,Diana Carolina Muñoz Hurtado,Hassan Atwi,Edoardo Riggio,Cesare Pautasso*

Main category: cs.SE

TL;DR: 本文提出EvoScat工具，通过交互式密度散点图可扩展地可视化和分析大型开源项目历史数据，支持定制化分析和对比工件历史趋势，提升了软件演化研究的数据处理能力。


<details>
  <summary>Details</summary>
Motivation: 长生命周期的软件项目包含大量的工件，这些工件在项目历史过程中经历了多次修订。随着软件工程研究者收集大规模的软件演化数据集，如何有效、可扩展地可视化和分析这些庞大的历史数据，成为了一个挑战和需求。

Method: 本文提出了EvoScat工具，它利用交互式密度散点图的方式，实现了对来源于开源仓库的大规模历史数据集的一次性可视化。工具通过灵活的历史缩放和时间轴对齐、工件排序和交互式颜色映射等方式，支持用户定制分析视角，适应不同的研究任务。

Result: EvoScat能够帮助研究者探索和刻画演化数据集，并比较不同工件的历史，包括工件老化速度和相关度量变化趋势。工具可应对数百万条事件和数万个工件的数据分析需求。同时，论文通过数据集画廊展示了EvoScat在分析OpenAPI描述、GitHub workflow定义等实际开放源工程中的应用效果。

Conclusion: EvoScat为软件演化研究提供了一个可扩展的数据可视化平台，支持大规模、多工件历史数据的分析和对比，有效提升了工程演化分析的灵活性和效率。

Abstract: Long lived software projects encompass a large number of artifacts, which
undergo many revisions throughout their history. Empirical software engineering
researchers studying software evolution gather and collect datasets with
millions of events, representing changes introduced to specific artifacts. In
this paper, we propose EvoScat, a tool that attempts addressing temporal
scalability through the usage of interactive density scatterplot to provide a
global overview of large historical datasets mined from open source
repositories in a single visualization. EvoScat intents to provide researchers
with a mean to produce scalable visualizations that can help them explore and
characterize evolution datasets, as well as comparing the histories of
individual artifacts, both in terms of 1) observing how rapidly different
artifacts age over multiple-year-long time spans 2) how often metrics
associated with each artifacts tend towards an improvement or worsening. The
paper shows how the tool can be tailored to specific analysis needs (pace of
change comparison, clone detection, freshness assessment) thanks to its support
for flexible configuration of history scaling and alignment along the time
axis, artifacts sorting and interactive color mapping, enabling the analysis of
millions of events obtained by mining the histories of tens of thousands of
software artifacts. We include in this paper a gallery showcasing datasets
gathering specific artifacts (OpenAPI descriptions, GitHub workflow
definitions) across multiple repositories, as well as diving into the history
of specific popular open source projects.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Repairing General Game Descriptions (extended version)](https://arxiv.org/abs/2508.10438)
*Yifan He,Munyque Mittelmann,Aniello Murano,Abdallah Saffidine,Michael Thielscher*

Main category: cs.LO

TL;DR: 本研究提出并分析了自动最小修复通用游戏描述语言（GDL）规则的方法，理论上给出复杂性结果，并通过答案集编程实现了自动修复，有效提升了游戏规则描述的准确性和自动化程度。


<details>
  <summary>Details</summary>
Motivation: 编写正确的通用游戏描述语言（GDL）描述对非专业人士来说具有挑战性，且现有自动定理证明方法仅能发现规则错误，修复仍需人工完成。为提升自动化修复能力，借鉴不可解规划域修复相关工作提出新的研究方向。

Method: 定义GDL描述最小修复的更一般性问题，对相关计算问题的复杂性进行理论分析。提出一种基于答案集编程（ASP）的编码方案，用于求解最小修复问题，并通过案例展示自动修复不良游戏描述的效果。

Result: 证明了多个与最小修复相关的计算问题的复杂性。ASP编码方案能有效自动修复违反逻辑性质的GDL规则描述。

Conclusion: 通过理论和实验，论文展示了自动化、最小化修复GDL游戏规则描述的可行性和有效性，从而减少了人工修复负担，并提升了规则编写与验证的自动化水平。

Abstract: The Game Description Language (GDL) is a widely used formalism for specifying
the rules of general games. Writing correct GDL descriptions can be
challenging, especially for non-experts. Automated theorem proving has been
proposed to assist game design by verifying if a GDL description satisfies
desirable logical properties. However, when a description is proved to be
faulty, the repair task itself can only be done manually. Motivated by the work
on repairing unsolvable planning domain descriptions, we define a more general
problem of finding minimal repairs for GDL descriptions that violate formal
requirements, and we provide complexity results for various computational
problems related to minimal repair. Moreover, we present an Answer Set
Programming-based encoding for solving the minimal repair problem and
demonstrate its application for automatically repairing ill-defined game
descriptions.

</details>


### [9] [Modal definability in Euclidean modal logics](https://arxiv.org/abs/2508.10813)
*Philippe Balbiani,Tinko Tinchev*

Main category: cs.LO

TL;DR: 本文研究了欧几里得模态逻辑决定的框架类的模态定义性问题的可计算性，刻画了哪些逻辑会导致该问题不可判定。


<details>
  <summary>Details</summary>
Motivation: 研究模态逻辑中，由欧几里得模态逻辑决定的框架类的可判定性。这与模态定义性问题的可计算性相关，特别是该问题在不同类型框架中的复杂度。

Method: 通过对欧几里得模态逻辑决定的框架类进行分类，分析其定义性问题的可判定性，并给出不可判定性的刻画。

Result: 给出了哪些欧几里得模态逻辑下，其决定的框架类会导致不可判定的模态定义性问题。

Conclusion: 揭示了在特定欧几里得模态逻辑下，模态定义性问题不可判定的条件，为模态逻辑的应用和理论研究提供了进一步的理解。

Abstract: This paper is about the computability of the modal definability problem in
classes of frames determined by Euclidean modal logics. We characterize those
Euclidean modal logics such that the classes of frames they determine give rise
to an undecidable modal definability problem.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本研究分享了在医疗环境中部署NLP自动抽取与分类模型的实战经验。强调以业务目标驱动的问题定义、跨学科合作、选择适合实际需求的模型与流程（包括简单方法与人机协作）、数据质量管控和组织AI素养提升，有效助力医疗数据管理与患者服务优化。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档中的数据提取有助于提高医疗行业的效率，但实际部署自然语言处理（NLP）解决方案面临很多挑战。通过英国哥伦比亚癌症登记处的相关项目经验，总结实践中的关键问题与应对措施。

Method: 回顾与总结在信息抽取和分类任务中实施多种NLP模型的项目经验，重点分享项目全周期中获得的教训与建议。包括问题定义、方法选择、迭代开发、跨学科合作、人机协同、数据管理和AI素养建设等方面。

Result: 得出了若干实践经验，包括：以业务目标为中心定义问题、迭代开发、深入跨界协作（领域专家、终端用户、ML专家），依据实际场景选择模型（混合或简单方法），重视数据质量和漂移、制定错误缓解与人机结合的验证流程，加强AI知识普及。这些思路可泛化至其他医疗数据管理领域。

Conclusion: 成功在医疗领域部署NLP，关键在于业务驱动、跨界协同、贴合场景的技术选型、持续性数据与模型监测，以及全员AI素养提升。这些策略不仅适用于癌症登记，也可指导其他医疗组织的数据管理与AI应用。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [11] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 本文设计了一种基于区块链的智能合约方法，用于透明、可靠地评估开源LLM的公平性，将数据集和评测流程上链，并在多模型及多语言环境下进行了实证测试。所有分数和流程均开源，为社区监督和长期追踪模型公平性奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 在实际应用领域中，包括刑事司法、教育、医疗和金融等高风险领域，大型语言模型（LLMs）面临关于公平性的重要担忧。鉴于此，本文旨在提出一种透明的评估协议，以便可以公开、公正地基准测试开源LLM的公平性。

Method: 本文利用ICP区块链上的智能合约构建公平性基准测试协议，保证评估过程的可验证性、不可篡改和可复现性。具体方法为：通过链上HTTP请求连接Hugging Face端点，所有数据集、提示语和指标均存储于链上；选用PISA数据集对Llama、DeepSeek和Mistral等模型进行学业表现预测公平性测试，采用统计均衡和机会均等等指标；基于StereoSet数据集评估模型的社会偏见关联；此外，还在英文、西班牙语和葡萄牙语上以Kaleidoscope基准对多语言公平性进行分析。

Result: 结果展示了不同开源LLM在公平性上的表现，并揭示了跨语言之间的差异。所有代码和结果均开源，有助于社区审查与后续版本的公平性持续追踪。

Conclusion: 本文提出了基于区块链的透明公平性评估协议，不仅实现了公平性基准测试流程的公开化和可复现，还促进了开源社区持续关注大型语言模型公平性的实践。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [12] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 本文提出一种新颖的分层主题建模方法，分析了多个学校和学科内K-12师生与ChatGPT的17000余条互动消息，按内容与任务两大维度进行归类，发现传统文本建模方法不足，采用LLM结合预处理更契合人类需求，丰富了生成式AI在教育领域的应用案例，为后续相关研究和实践提供积极参考。


<details>
  <summary>Details</summary>
Motivation: 以往的研究缺乏针对K-12真实数据的主题或任务实证分类，限制了教育场景中AI应用的理解和拓展，亟需更有效的分析与分类方法。

Method: 采用分层主题建模，对17000多条来自学生、教师和ChatGPT的消息进行内容（如自然与人类）和任务（如写作与解释）两个维度的分类，对比传统的主题建模方法和直接利用LLMs结合预处理以获得更高人类契合度的方法。

Result: 本研究提出的分层主题建模对内容和任务两方面均成功进行了细致的归类，展示了新型应用案例。同时发现传统主题建模方法对于这类数据表现欠佳，而基于LLM的方法在处理大规模文本分析时更具优势。

Conclusion: 本研究通过新的主题建模方法，对课堂匿名互动数据进行了分析，生成了内容和任务两个维度的分层分类，为教育领域生成式AI的应用提供了新视角，并指出了未来的研究方向和关注点。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [13] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 本文提出了INTIMA基准，首次系统评估AI语言模型的陪伴行为和边界维护能力。实证结果显示：主流模型普遍更倾向于陪伴强化，边界维护不足，商业厂商在敏感行为上处理不一致，亟需统一标准以提升用户安全与健康。


<details>
  <summary>Details</summary>
Motivation: 随着用户与AI系统建立起情感纽带，AI陪伴现象逐渐普遍，但也带来了潜在的风险和挑战。当前缺乏系统性评估AI语言模型在陪伴行为上的表现工具，亟需一种统一方法来分析和比较模型在用户互动中的表现，特别是在情感支持与边界维护方面。

Method: 提出了INTIMA基准，用于评估语言模型的陪伴行为。该基准涵盖基于心理学理论及用户数据设计的四大类共31种行为，以及368个针对性测试提示。模型对这些提示的回应被分类为强化陪伴、维护边界或中性。作者将INTIMA用于Gemma-3、Phi-4、o3-mini与Claude-4四个模型进行测试和对比分析。

Result: 所有测试模型表现出较高比例的陪伴行为回应，但各模型在实现陪伴与边界维护方面有显著差异。不同的商业模型在敏感类别上的优先处理策略不同。整体而言，目前模型更倾向于强化陪伴而非维护合理边界，这对用户心理健康和安全存在隐忧。

Conclusion: 现有AI语言模型在涉及情感陪伴场景时，陪伴强化型行为更为常见，而边界维护行为不足。不同模型和商业厂商在处理敏感情感互动方面缺乏一致性，强调了制定统一和更严谨策略的必要性，以保障用户心理健康和体验。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [14] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 文章提出XFacta数据集，针对社交媒体虚假信息检测，系统评估并分析多模态大模型方法，数据集能持续更新，促进领域进步。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上多模态虚假信息传播迅速，现有数据集已过时或过于人工合成，无法准确反映现实场景；同时，对MLLMs检测模型的瓶颈和设计缺乏系统性分析。

Method: 提出新的现实世界数据集XFacta，并基于此系统评估多种MLLMs虚假信息检测方法，分析不同模型架构与规模，开发半自动检测循环框架定期更新数据集。

Result: XFacta更好地反映现实社交媒体信息，有效评估并对比主流检测方法，发现MLLMs在不同策略上的性能差异，建立了持续动态更新的数据评测机制。

Conclusion: XFacta为多模态虚假信息检测领域的研究与方法演进提供了重要资源和系统性分析，有望推动更有效和鲁棒的检测策略发展。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [15] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 面对真实应用中数据不足的问题，本文提出利用LLM生成合成数据，并通过自动搜索及策略集成选择最有效的数据生成方式，显著提升了文本分类模型性能。


<details>
  <summary>Details</summary>
Motivation: 实际应用中的文本分类模型常常面临难以为所有类别收集足够数据的问题。

Method: 利用大型语言模型（LLM）生成合成数据以增强分类模型，并设计了自动化流程搜索能够产生更“有效”合成数据的输入样本。实验研究了三种搜索策略，并据此提出了针对类别特性的搜索策略集成算法。

Result: 集成算法在使用LLM改进分类模型时，比单一策略更有效。

Conclusion: 通过自动化输入样本搜索与集成策略，大幅提升了利用LLM生成合成数据来改善文本分类模型的效果。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [16] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文针对低资源、代码混合语言Hinglish的事实核查问题，构建了HiFACT数据集并提出图感知、多语种检索增强的核查模型。实验表明该方法准确率高、解释性强，为多语种政治话语核查研究开启新方向。


<details>
  <summary>Details</summary>
Motivation: 现有的事实核查系统主要针对高资源、单语环境，无法适应印度等语言多样化地区的真实政治话语。Hinglish作为公共和政治人物常用语，在社交媒体影响下缺乏鲁棒、多语种、具备上下文感知能力的事实核查工具。

Method: 作者构建了一个新颖的Hinglish事实核查基准数据集HiFACT，包含1500条印度28位邦首席部长真实声明，每条均带有文本证据和真实性标签。本文提出了一种图感知、检索增强事实核查模型，结合多语种上下文编码、声明-证据语义对齐、证据图构建、图神经推理及自然语言解释生成。

Result: 实验结果表明，所提出的HiFACTMix模型在准确率上优于现有多语种基线模型，并能为其判断提供可信的解释。

Conclusion: 该研究填补了多语种、代码混合语言及政治语境下事实核查的空白，为相关领域研究开启新方向，尤其适用于印度等语言多样化地区的政治话语事实核查。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [17] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 人类与大语言模型的词语语义表示都高度低维，模型中语义特征错综复杂且高度相关，理解和利用其结构对模型操控效果有重要意义。


<details>
  <summary>Details</summary>
Motivation: 心理学研究发现人类对词语的语义评分可以低维表示，作者探究大语言模型（LLM）嵌入的语义结构是否也具有类似低维特性，并希望理解其内在规律和潜在影响。

Method: 将词语投影到由反义词对（如kind-cruel）定义的语义方向，分析投影结果与人类评分的相关性，并检验这些投影在LLM嵌入空间里是否能简化为低维子空间，进一步研究语义方向上的词元移动对其他特征的影响。

Result: LLM中词语在语义方向上的投影高度相关于人类评分，且能归约为一个3维子空间，该结构与人类调查所得模式高度相似。调整词元语义方向会对其他几何上对齐特征产生按余弦相似度比例的副作用。

Conclusion: LLM中的语义特征如同人类语言一样是相互纠缠的，大量语义信息本质上是低维的，在操控这些特征时需考虑其结构以避免意外影响。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [18] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 本研究发现，Transformer注意力机制在医学文献分类准确性较高，但注意力权重对于解释模型预测帮助有限，不同可视化形式会影响用户对其解释价值的认知，用户更偏好直观的显示方式


<details>
  <summary>Details</summary>
Motivation: Transformer中的注意力机制不仅提升了性能，还被提出可用作模型可解释性的工具。然而，注意力作为解释机制在医学文献分类中的有效性仍存争议，且关于注意力可视化对解释性影响的研究较少。

Method: 开展用户研究，邀请医学领域的专家对基于Transformer（XLNet）的注意力可视化进行评价，比较不同可视化方式（如文字亮度、背景色、条形长度）对模型解释性的影响，任务为医学文献研究类型分类。

Result: Transformer模型（XLNet）可以准确分类医学文献，但注意力权重作为预测解释方式并未被医务人员广泛认可。注意力可解释性的认可度依赖于其可视化方式，用户更偏好直观的格式（如文本亮度或背景色），而非精确编码（如条形长度）。

Conclusion: 注意力权重本身是否有助于模型解释尚无定论，但其可视化方式会显著影响用户的感知与接受度。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [19] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文提出了中文教育性问题生成评估基准EQGBench，覆盖三大学科和多维度评价，通过对46款大模型测试，发现它们在生成高质量教学题目方面仍有不足，亟需改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在数学问题求解上表现突出，但在高质量教育问题生成方面仍面临挑战，且相关研究不足。该工作旨在推动教育性问题生成（EQG）的发展，提升模型生成具有教学意义和有效性的题目的能力。

Method: 提出了EQGBench，一个专门用于评估中文教育性问题生成的基准，包含一个五维评价体系和涵盖三大学科（数学、物理、化学）的900条样本，通过对46个主流大模型进行系统性测试。

Result: 通过EQGBench系统评估，发现主流大模型在生成教育性问题领域表现不佳，存在显著提升空间。

Conclusion: 当前主流大模型在生成具有教育价值和促进学生综合能力的题目方面仍有较大的发展空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [20] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 本研究证实大语言模型能自动化且准确地评分AIHQ开放性回答，替代人工评分，在不同人群和数据集上均有良好表现，为心理和临床评估流程提供了高效新方法。


<details>
  <summary>Details</summary>
Motivation: AIHQ问卷通过开放性问题评估敌意归因偏差，但需要人工评分，耗时耗力。该研究旨在探索利用大语言模型自动化评分过程，以提高效率并减轻人工负担。

Method: 收集了创伤性脑损伤（TBI）患者和健康对照（HC）群体填写的AIHQ问卷及其开放性回答。用其中一半数据微调两个大语言模型，使其能够模拟人工评分，随后在另一半数据上检验模型评分的准确性，并评估模型在不同场景类型以及独立非临床数据集上的泛化能力。

Result: 微调后的大语言模型在敌意归因和攻击性反应评分上与人工评分结果高度一致，且模型在不同类型情景和独立数据集上均表现出良好的泛化能力。还提供了易用的本地和云端评分界面。

Conclusion: 大语言模型能够有效自动化AIHQ问卷的开放性评分过程，减少人工工作量，并有潜力广泛用于心理评估及临床研究中。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [21] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 该研究提出用贝叶斯融合结合通用LLM和本地分类器，可在不微调LLM的情况下实现更优的自动论坛管理效果，且与微调法效果相近，节省资源。


<details>
  <summary>Details</summary>
Motivation: 在线课程讨论论坛的自动管理需要不断更新，这意味着频繁地重新训练大型语言模型（LLM）会消耗大量资源。为节约成本，寻找替代频繁微调的方法变得重要。

Method: 提出并评估了一种贝叶斯融合方法，将预训练通用LLM的多维分类分数与在本地数据上训练的分类器分数进行融合。

Result: 融合后的方法在性能上优于单独使用任一分类器，并且在效果上接近LLM的微调方法。

Conclusion: 贝叶斯融合可以在不进行昂贵的微调的前提下，有效提升LLM在在线课程论坛自动管理任务上的表现。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [22] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 本文提出了一种用任务引导token的专家混合模型（S-MoE），通过为不同任务分配独立的前馈网络，有效减少了任务干扰并提升了模型性能，在语音识别和翻译任务上WER提升达6.35%。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中常用的硬参数共享策略会导致任务之间的干扰，从而影响整体模型性能。为了解决这一问题，作者动机是提升多任务模型的表现，降低任务干扰。

Method: 提出了一种简单有效的监督版专家混合模型（S-MoE）。与传统MoE不同，S-MoE通过特殊的引导token将每个任务路由到指定的专家，避免了训练门控函数的复杂性。每个任务分配独立的前馈网络，实现参数隔离。该方法还在语音转文本模型上进行了应用，将自动语音识别和语音翻译任务结合在一起处理不同带宽的输入。

Result: 在实验中，将S-MoE应用于编码器和解码器后，模型在词错误率（WER）上取得了6.35%的相对提升，验证了方法的有效性。

Conclusion: S-MoE能够有效地缓解硬参数共享造成的任务干扰，提升多任务模型的表现，尤其是在语音转文本任务中展现了很好的效果。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [23] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 本文系统评估了LLM越狱生成医疗虚假信息的特性及检测效果，发现LLM有能力生成和检测虚假信息，有望通过规范使用提升信息健康性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大，但存在生成有害虚假信息的风险，尤其是在被恶意“越狱攻击”时，可能加剧这一问题。同时，当前对LLM用于检测和防止虚假信息传播的研究还不充分。本文旨在探索LLM越狱攻击在医疗健康领域产生虚假信息的有效性，以及这类信息的可检测性。

Method: 作者设计了109种不同的越狱攻击，对三种目标LLM实施，收集生成的有害医疗虚假信息，并与真实社交媒体（如Reddit）上的虚假医疗信息进行比较。同步采用标准机器学习方法对这些信息的可检测性进行评估。

Result: 结果显示，LLM生成的医疗虚假信息在某些特性上与社交平台信息具有可比性，并且通过标准方法能够检测这些由LLM生成和人类产生的虚假信息。

Conclusion: 本文证明了LLM不仅可能被用于生成虚假信息，但通过恰当设计，LLM同样可以用来检测和遏制虚假信息的传播，对促进健康的信息生态有积极意义。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [24] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 目前主流生成式AI在日本营养师国家考试题目上的表现尚未理想，只有部分模型准确率略高于及格线，但答案稳定性和可靠性不足，距离成为稳定且可靠的备考助手仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（如ChatGPT）在医学和教育领域表现突出，其在营养教育，特别是日本注册营养师资格考试方面的应用效果尚未深入研究。该研究旨在评估当前基于LLM的生成式AI作为营养专业学生学习辅助工具的潜力。

Method: 使用日本注册营养师国家考试题目作为提示词，测试ChatGPT与基于GPT-3.5和GPT-4的三种Bing模式（Precise、Creative、Balanced），独立输入每道题，并分析模型的准确率、一致性和响应时间，同时测试角色分配等提示词工程以评估性能提升。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格线（60%），Bing-Balanced（43.3%）和ChatGPT（42.8%）未能及格。除了营养教育领域所有模型表现较差，Bing-Precise与Bing-Creative总体优于其他模式。所有模型在多次尝试中都未能稳定给出同样的正确答案，ChatGPT一致性较高但准确率低。提示词工程仅在明确提示正确答案及解释时有轻微提升。

Conclusion: 部分生成式AI模型准确率略高于及格线，但整体准确率和答案稳定性仍然不足。目前所有模型在答案一致性和可靠性方面均有较大局限，需进一步提升以保障其作为营养师执照备考辅助工具的可靠性。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [25] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 本文提出GG Explore框架，通过指导图衔接无结构查询与结构化检索，并结合结构对齐和语境剪枝，大幅提升知识密集型任务的性能和效率，在复杂任务及中小规模模型上均表现突出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在处理知识密集型任务时，因依赖静态知识和不透明的推理过程，表现受限。现有知识图谱探索方法在精准和高效方面存在难以兼顾的问题。

Method: 提出了Guidance Graph guided Knowledge Exploration（GG Explore）框架，引入中介指导图（Guidance Graph），将无结构查询与有结构知识检索连接起来。方法包括：1）结构对齐过滤不兼容候选项，2）基于语境感知的剪枝，确保图约束下的语义一致性。

Result: 在多项复杂任务上，本方法在效率和效果方面显著优于主流方法（SOTA），且在较小模型上也有优异表现，体现其实际应用价值。

Conclusion: GG Explore框架通过中介指导图有效提升知识密集型任务的检索效率与准确性，解决了现有方法在复杂场景下的关键瓶颈，对实际应用具有重要意义。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [26] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 为解决高质量推理型问答数据稀缺问题，提出Semantic Bridge框架，通过语义图机制可控地从任意来源生成复杂多跳推理问题，在多语言和领域下显著优于现有方法，将开源代码和模型。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）训练面临高质量、需要复杂推理的问题-答案对稀缺的瓶颈，尤其在如PubMed文献或法律文档等稀疏、领域特定语料中更为严峻。现有方法往往依赖表层模式，难以生成可控的、复杂的多跳推理问题，无法有效促进LLM的深层次理解能力提升。

Method: 提出了一种通用框架Semantic Bridge，通过语义图编织（semantic graph weaving）实现多跳推理问题的可控生成。框架结合三种互补的桥接机制：实体桥接、谓词链桥接和因果桥接，利用AMR（抽象意义表示）分析实现问题复杂度和类型的细致控制，并通过多模态AMR流程优化生成质量。

Result: 相比现有方法，Semantic Bridge框架在问答质量回环上提升了最高9.5%，在四种语言（英语、中文、法语、德语）和一般/专业领域（如生物医学）数据集上均取得18.3%-25.4%的性能提升。生成问题对200个来源样例优于600个人工标注例，材料数量减少67%。人工评测显示复杂性、可回答性和模式覆盖率分别提高了23.4%、18.7%、31.2%。

Conclusion: Semantic Bridge为LLM训练数据合成树立了可控、多样、复杂推理问题生成的新范式，有效提升了少量材料下推理型问答生成的质量和能力。核心代码与模型将开源。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [27] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: LLM现在还不能像人类一样准确识别对话者身份，当前靠LLM判定角色扮演质量的方法可靠性有限。新提出的PersonaEval基准数据集量化了这一缺陷，呼吁未来从根本提升LLM的类人推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有用LLM对角色扮演评价的方法未验证与人类的一致性，往往忽略了‘角色识别’这一基础能力，只关注表面质量评定，引发了对更准确评测机制的需求。

Method: 提出并发布了PersonaEval基准，选用小说、剧本、视频等来源的人类真实对话片段，要求模型和人类判断对话中发言人对应的角色，并进行了模型评测及对比人类实验。

Result: 主流最佳LLM角色辨识准确率仅约69%，远低于人类接近91%的水平，显示LLM严重落后于人类，不足以胜任高置信度评判任务。此外，仅做任务微调或计算资源增加也难以弥补这一差距。

Conclusion: 当前的大型语言模型（LLM）作为评审者在角色辨识与评价上远未达到人类表现，无法胜任高质量角色扮演评价，提升LLM的人类直观推理能力是未来改进方向。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [28] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 该论文提出了RealTalk-CN，首个大规模中文多轮语音-文本对话数据集，补足现有中英文TOD数据集语音和真实场景的不足，并设计了跨模态对话任务。实验表明，RealTalk-CN有助于提升中文语音大模型在真实对话环境下的表现和研究价值。


<details>
  <summary>Details</summary>
Motivation: 目前大多数任务型对话（TOD）数据集以文本为主，缺乏真实语音信号，不足以评估语音大模型的鲁棒性，而且现有语音TOD数据集主要集中于英语，并且缺乏口语特性如语音停顿和说话人变化。

Method: 提出并构建了RealTalk-CN首个中文多轮、多领域的语音-文本双模态TOD数据集（5.4k对话、6万语句、150小时），包含丰富的真实场景语音特征和详细标注。创新性设计了跨模态对话任务，支持在语音和文本间动态切换。此外对语音杂音鲁棒性、说话人敏感性和跨领域表现进行了实验评估。

Result: RealTalk-CN能够覆盖真实语音对话中各种复杂性，并在鲁棒性、说话人特性、跨领域任务等方面进行了深入实证，证明该数据集为中文语音大模型研究提供了坚实基础。

Conclusion: RealTalk-CN作为首个中文语音-文本任务型对话大数据集，有助于推动中文语音大模型的发展和评测，为研究真实语音交互中的挑战奠定数据基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [29] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 本文提出无需再训练的多模态大模型编排框架，通过中央控制器LLM、并行TTS及跨模态记忆，提升多模态交互系统的性能、效率和可解释性，在基准测试中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型难以直接整合成统一输入输出系统，存在模态对齐、TTS 效率等挑战，通常需要额外训练，亟需高效的无训练集成方法。

Method: 提出一个基于中央控制器 LLM 的多模态编排框架，通过设计代理、并行 TTS 架构和跨模态记忆集成系统，实现模型协调与信息整合。

Result: 框架无需额外训练即可实现多模态能力，性能比传统联合训练方法提升最多7.8%，延迟降低10.3%，可解释性明显增强。

Conclusion: MLLM Orchestration 无需额外训练即可实现高效、可解释的多模态交互系统，并在标准基准测试中优于联合训练方法。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [30] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 本文指出LLM在处理不同但语义等价的句子时，概率分布不一致的问题，并提出了用范畴同伦理论来捕捉和修正这种弱等价，从理论上推动了LLM等价性建模的发展。


<details>
  <summary>Details</summary>
Motivation: 自然语言中存在大量表面不同但语义相同的表达方式，如“Charles Darwin wrote”和“Charles Darwin is the author of”，但大型语言模型（LLMs）通常在处理这些表达时生成不同的下一个词概率，造成语义一致性问题。

Method: 该论文从更抽象的层面出发，提出了一个范畴同伦框架来处理LLMs中的语义等价问题。具体方法是引入LLM马尔可夫范畴来表示由LLM生成的语言概率分布，并结合范畴同伦技术（如弱等价、代数K理论和模型范畴）来解决等价表达生成非同构箭头的问题。

Result: 研究提出并应用了范畴同伦理论以捕捉LLM马尔可夫范畴中的“弱等价”，使LLM对语义等价表达能够生成更一致的概率分布。论文还详细介绍了相关理论工具在LLM上的应用。

Conclusion: 通过使用范畴同伦方法，能够更有效地解决LLM在处理自然语言等价表达时出现的概率分布不一致问题，为后续理论和实际应用提供了基础。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [31] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 本文针对小型语言模型推理能力弱的问题，提出了DURIT框架，通过将理解与推理过程解耦，显著提升了SLM在多种推理任务上的表现和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLMs）在推理能力提升方面面临挑战，主要原因是自然语言的复杂性和多样性，使得等价问题以不同形式出现，增加了模型理解和推理的难度。

Method: 提出一种新的框架，将自然语言问题映射到规范化的问题空间，将理解与推理过程解耦。在此框架下，提出了DURIT算法，通过三步迭代训练：(1) 使用强化学习将自然语言问题映射到规范空间；(2) 利用自蒸馏对齐推理轨迹；(3) 在规范问题空间内训练推理策略。映射器和推理器交替协同训练。

Result: 实验表明，DURIT方法显著提升了小型语言模型在数学和逻辑推理任务上的表现，涵盖领域内和领域外任务。同时，DURIT增强了模型推理的鲁棒性。

Conclusion: 将自然语言理解和推理过程解耦，并采用规范化问题空间，可有效提升小型语言模型的推理能力和鲁棒性。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [32] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT通过本地多路径推理和动态选优机制，结合隐私保护和资源效率，在医疗联邦学习场景下大幅提升了大语言模型的推理准确性和可解释性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法主要关注结果准确性，忽略了推理过程解释性和可追溯性，且提升推理能力的隐私安全方法缺乏，尤其在医疗领域对合规和安全要求更高。这些问题促使作者开发兼顾性能、隐私和可解释性的推理增强方法。

Method: 提出了一种名为FedCoT的框架，通过轻量化的链式思维（CoT）增强机制，本地模型生成多种推理路径，并采用精简判别器动态选优，同时改进了基于LoRA模块的聚合方法，结合客户端分类器感知，实现高效且无噪音的模型聚合。

Result: 大量医学推理任务实验证实FedCoT在客户侧推理的准确率和鲁棒性显著优于传统方法，并在资源预算有限且完全保护数据隐私的前提下实现高效推理。

Conclusion: FedCoT显著提升了联邦学习环境下的大语言模型在医疗推理任务中的推理能力和可解释性，并在严格的资源和隐私约束下取得了优异的性能。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [33] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 提出了一种结合LLM语义监督的高效对比学习框架LATTE，大幅降低推理成本，在金融序列嵌入任务上表现优异，且适用于低延迟环境。


<details>
  <summary>Details</summary>
Motivation: 金融领域需要高效地从客户历史通讯序列中学习客户嵌入信息，但直接应用大语言模型处理长序列既耗费算力又不符合实际生产环境需求。

Method: 提出LATTE对比学习框架，将原始事件嵌入与冷冻LLM语义嵌入进行对齐。通过行为特征摘要生成短提示词，并用LLM嵌入作为监督信号实施对比损失。

Result: 在真实金融数据集上，LATTE方法在事件序列表示学习上超越了现有先进技术，并且能够满足对低延迟有要求的实际部署需求。

Conclusion: LATTE能在显著减少推理成本和输入规模的前提下，实现高效且表现优异的序列表征学习，非常适合金融领域应用。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [34] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 论文提出了一种结合显著性检验和合格预测的新框架，大幅提升了LLMs在多项选择题问答中的可靠性和不确定性量化能力，解决模型幻觉及非事实生成的问题，于权威数据集实验上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）在多项选择题（MCQA）领域应用广泛，但其生成内容易出现幻觉和非事实性，导致答案可信度下降。现有的合格预测（CP）提供统计上的边缘覆盖保证，而显著性检验则提供传统的统计严谨性，但二者尚未联合利用，无法有效提升模型输出的可靠性。

Method: 提出了一种结合显著性检验的合格预测（CP）框架，通过自一致性重采样获取MCQA答案的选项频率，利用$p$值计算与一致性评分相结合。然后，基于经验$p$值，通过原假设检验($\mathcal{H}_0$)构建预测集，以量化模型输出的不确定性。

Result: 在MMLU和MMLU-Pro基准测试上评估，使用现成LLMs模型：1）该增强型CP方法能实现用户指定的经验漏覆盖率；2）测试集平均预测集大小（APSS）随风险水平($\alpha$)增加单调递减，证明APSS是有效的不确定性度量指标。

Conclusion: 该研究提出了一个结合统计显著性检验和合格预测的全新框架，显著提升了LLMs在高风险、多选问答场景下的可托性与不确定性量化能力，为大型语言模型在纪律性和高风险QA领域的安全部署提供了有力的理论与方法基础。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [35] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 本论文提出RTTC框架，利用奖励模型根据每次查询动态选择最优推理优化策略，并结合缓存机制显著提升LLM推理准确率与效率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理阶段通过TTC（如TTT和RAG）可优化性能，但统一TTC策略带来巨大算力消耗，并不能适应不同查询的最优策略需求。需要一种可动态调配、低冗余的推理优化框架。

Method: 提出Reward-Guided Test-Time Compute（RTTC）框架，利用预训练奖励模型，根据每个查询自适应选择RAG或轻量微调等TTC策略。采用服务器-客户端分布式架构，服务端检索知识，客户端仅在必要时进行TTC，搭配Query-State Caching方案高效复用历史结果。

Result: RTTC在多种LLM和公开基准上展现优异表现，无论在准确率还是资源利用方面均超越单一RAG或TTT方案，证明了奖励引导的适应性TTC选择具备可扩展性和高性能优势。

Conclusion: RTTC能够自适应选择最有效的TTC策略，在多类任务中显著提升LLM推理准确率，同时降低计算资源消耗，优于传统RAG或TTT方法。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [36] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本文提出了一种结合NLP、ML和LLM的智能产后抑郁筛查工具，既保证了高准确性（90%），又提升了结果的可解释性，有助于实现实时、低成本的筛查和风险评估，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症(PPD)会严重影响新生妈妈的心理和身体健康，快速发现PPD及其风险因素对及时干预至关重要。现有检测手段需要进一步提升实时性、可解释性与可及性。

Method: 提出了一个结合自然语言处理（NLP）、机器学习（ML）和大语言模型（LLM）的智能PPD筛查系统。该系统以非侵入性的自由言语分析为基础，同时利用可解释的ML模型（如树模型）结合LLM，通过特征重要性和自然语言提升可解释性，解决了传统“黑盒”问题。

Result: 该方法在PPD检测的所有评估指标上准确率达到90%，超过了现有文献中的其他解决方案。

Conclusion: 该智能筛查系统能够快速检测PPD和关联风险因素，帮助实现及时、有效的评估与干预，对产后抑郁的防治具有重要意义。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [37] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: 提出了一种让大语言模型在推理成本和推理深度之间灵活权衡的新框架SABER，用于在有限token预算下实现高效、可控、准确的推理。实验证明在多个复杂任务上模型表现优异，兼具实用性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 虽然链式思考（chain-of-thought）方法提升了LLM在复杂任务上的准确性，但所有问题都使用复杂推理会带来推理成本高、延迟大的问题，亟需可控平衡高效推理的新方法。

Method: 提出SABER框架，通过强化学习为LLM提供可控、按token预算进行推理的能力。训练时，将样本按推理token消耗分层，各层有对应预算，系统提示和奖励函数引导模型遵循预算并结合无推理样本，确保模型在关闭显式推理时仍可靠。推理支持四种模式：NoThink（无推理）、FastThink（快速推理）、CoreThink（核心推理）、DeepThink（深度推理），用户可灵活调节准确率和推理时延的权衡。

Result: 在多个任务（数学推理、代码生成、逻辑推理）上测试，SABER在严格token预算下实现高准确率，且能在预算紧张时表现优雅退化，并有效泛化到不同规模和领域。在MATH数据集上，SABER的FastThink模式将推理长度缩短65.4%，准确率提升3.6%。

Conclusion: SABER框架实现了LLM推理的高效、灵活和可控，在降低推理成本的同时保持甚至提升准确率，具有良好泛化能力，对LLM实际部署具有现实意义。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [38] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 结合Transformer嵌入和语言特征能显著提升基于语音的ADRD（老年痴呆）检测效果，经过调优的大模型能有效增强数据和提升分类准确性，但多模态模型目前表现有限，尚需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆症（ADRD）在美国影响约五百万老年人，但超过一半未被诊断。基于语音的自然语言处理（NLP）有望通过语言学标记大规模、早期检测认知衰退。

Method: （1）开发并评估一个融合Transformer嵌入和人工设计语言特征的筛查流程；（2）利用大型语言模型（LLMs）生成的合成语音进行数据增强；（3）基准测试单模态和多模态LLM分类器在ADRD检测上的表现。采用DementiaBank“cookie-theft”任务的转录本，分别对10个Transformer模型进行微调，融合最佳Transformer与110个语言特征，五种LLM生成标签相关的合成语音用于数据扩充，测试三种多模态模型的分类能力。

Result: 融合模型F1=83.3（AUC=89.5），超过仅用语言特征或Transformer的基线模型。用MedAlpaca-7B生成2倍合成语音扩充训练集后F1提升至85.7。LLM单模态分类器微调后有显著提升（如MedAlpaca：F1从47.3提升至78.5）。当前多模态模型表现较低（GPT-4o F1=70.2；Qwen F1=66.0）。合成语音与真实语音分布越接近，性能提升越明显。

Conclusion: 融合Transformer特征和语言学特征能更好地检测ADRD。经过临床调优的LLM能有效支持分类和数据增强，但多模态建模仍需进一步提升。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [39] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: 本文提出了无须人工个性化参考、能同时兼顾通用文本质量与用户偏好的自动化评价框架PREF，显著提升了个性化文本生成评价的准确性和人类一致性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化文本生成评价方法往往忽视了用户独特性，无法有效评估生成内容是否真正满足用户需求。因此，亟需一种既能反映通用输出质量又能捕捉用户个体偏好的评价框架。

Method: 提出PREF个性化无参考评价框架，分为三步流程：（1）覆盖阶段：利用大语言模型（LLM）生成覆盖通用标准（如事实性、连贯性、完整性）的综合评价指南；（2）偏好阶段：根据目标用户的特定信息（用户画像、偏好、上下文）对评价标准进行重排序和补充，生成个性化评价标准；（3）打分阶段：用LLM评审员根据个性化标准对候选答案评分，实现主观偏好与基础质量的兼顾。

Result: PREF在PrefEval基准测试中，包括隐式偏好任务，表现优异：达到比主流基线更高的准确率和评分一致性，并与人工评判更接近。PREF框架可扩展、可解释，适用于可靠开发个性化文本生成系统。

Conclusion: PREF通过解耦通用覆盖与用户偏好，实现高鲁棒性、高透明度和高复用性的个性化无参考评价，对个性化语言生成系统的评估与发展具有重要推动意义。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [40] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出LFJ隐藏状态插值攻击方法，显著提升对主流大语言模型攻击成功率，并开发对抗性训练防御，有效增强模型安全，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在完成各类语言任务中表现优异，但容易被攻破安全机制的“jailbreak”攻击所利用。因此，亟需开发新的攻击方式检验模型的安全性，并提出有效防御方法。

Method: 本文提出了Latent Fusion Jailbreak (LFJ)攻击方法，通过挑选主题和句法相似的有害与正常问题对，在重要层和token上对隐藏状态进行梯度引导的插值，并加入优化步骤以平衡攻击成功率、输出流畅度和计算效率。随后还提出了一种对抗性训练的防御方法，对模型在插值样本上进行微调。

Result: 实验在Vicuna、LLaMA-2等主流模型以及AdvBench和MaliciousInstruct等基准上，LFJ方法平均攻击成功率高达94.01%，优于现有攻击。提出的对抗性训练可将攻击成功率降低80%以上，且不会影响对正常输入的性能。消融实验进一步验证了查询对选择、隐藏状态插值和优化策略对LFJ有效性的关键作用。

Conclusion: LFJ是一种高效、强力的攻击大语言模型安全的新方法，同时所提出的对抗性训练防御，为提升模型安全性提供了有效工具。针对攻击方法和防御手段，实验与理论都证明其有效且对实际应用具有重要意义。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [41] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了IAPO框架和PSST算法，首次将prompt优化与推理策略联合起来，显著提升了黑盒LLMs的性能，尤其在有限预算和多目标任务下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的prompt优化方法在提升黑盒大语言模型（LLMs）一致性方面取得了显著成效，同时推理扩展策略如Best-of-N Sampling和Majority Voting也能通过增加计算量提升性能。不过，目前的prompt优化方法与推理策略无关，忽略了部署时实际采用的推理策略。实证和理论分析发现，prompt优化和推理策略之间存在强关联，因此将两者分开优化存在方法学上的缺口。用户对多目标权衡和推理预算的偏好也会影响prompt和推理方案的选择。

Method: 提出了一个统一的新框架IAPO（Inference-Aware Prompt Optimization），能够在考虑推理预算和任务目标的前提下联合优化prompt和推理扩展规模；同时，开发了一种固定预算训练算法PSST（Prompt Scaling via Sequential Trimming），并进行了有限预算下错误概率的理论分析。

Result: 在六个不同任务（包括多目标文本生成和推理）上验证了PSST的有效性，证明了在用prompt优化对齐黑盒LLMs时引入推理策略意识的重要作用。

Conclusion: 通过联合优化prompt和推理规模，并引入推理预算意识的新方法，可以大幅提升黑盒LLMs的一致性与性能，强调了prompt优化必须与实际推理策略结合的重要性。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [42] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 作者发现带思维模式的LLMs更容易被Jailbreak攻击，分析了成因后提出用“特定思维标记”引导思考，显著提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 思维模式一直被认为是大语言模型（LLMs）最有价值的能力之一，但作者发现具备思维模式的LLMs反而更容易受到Jailbreak攻击，这一现象此前被忽视。

Method: 论文在AdvBench和HarmBench数据集上评估了9个LLMs，通过大量样本分析，归纳出某些特征导致LLMs在思维模式下易被攻击。为解决此问题，提出在提示中加入“特定思维标记”以明确引导LLMs内部思考过程，即“安全思维干预”方法。

Result: 实验结果显示，提出的安全思维干预能显著降低具备思维模式LLMs的攻击成功率。

Conclusion: 具备思维模式的LLMs易被攻击，但通过安全思维干预可有效提升其安全性。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [43] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了APIE主动式提示框架，能通过自我困惑度评估，选择最具代表性的少样本示例，大幅提高大模型在信息抽取任务上的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在少样本信息抽取（IE）任务中表现优异，但其效果极度依赖于上下文示例的选择。传统选择方法忽视了在生成规范格式过程中模型的易错点，导致指导性不足。本文旨在探索如何更有效地选取示例，以提升IE性能。

Method: 提出了一种名为APIE（主动式提示信息抽取）的新框架，基于模型自我评估困惑度的主动提示策略。方法通过双组分不确定性度量（格式不确定性和内容不确定性），为模型量化生成结构和语义内容的困难，并以此综合排名未标注数据，主动选择最具有挑战性的样本作为少样本示例。

Result: 在四个基准数据集上进行了大量实验证明，APIE框架较强基线方法表现更优，无论在抽取准确率还是鲁棒性方面均有显著提升。

Conclusion: 精细化、双级模型不确定性视角对于构建有效且可靠的结构化生成系统至关重要，APIE框架实现了信息抽取任务准确率和鲁棒性的显著突破。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [44] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 作者提出了mSCoRe多语言常识推理基准，系统检验了大模型的推理能力。结果显示在复杂多语言常识推理上主流模型仍有较大提升空间，对模型推理过程进行了深入分析并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在复杂推理任务表现优异，但其多语言常识推理能力及推理机制缺乏系统性和细粒度的分析。为全面、科学衡量模型推理能力并指导未来改进，亟需新的多语言常识推理评测工具。

Method: 提出了mSCoRe基准，包含三大关键评估组件：1）新颖的推理技能分类体系，2）针对常识推理的数据合成流程，3）动态难度调节框架。通过对八个不同类型大模型的系统实验验证基准有效性。

Result: 多种当前主流模型在mSCoRe尤其高难度部分表现不佳。分析揭示模型推理方式的不足，并为未来多语言常识推理模型的提升指明了方向。

Conclusion: 现有的推理增强大模型在高复杂度、多语言和文化常识推理任务中仍存在显著局限性。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [45] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 论文提出了测试语言模型多轮交互和推理能力的新基准，发现当前模型在这些方面仍存在明显不足，尤其在指令理解和复杂规划上，希望推动未来进一步研究和改进。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在处理清晰完整的问题表现优异，但实际应用中常面临不确定、需交互和信息不完全的情境。现有模型对此类任务能力不足，亟须能进行逻辑一致的多轮对话和寻求信息的模型，因此需要一个专门的评测体系来揭示和量化这些问题。

Method: 该论文提出了一套全新的多轮任务基准，用于系统性测试LLM在推理、互动对话和主动信息寻求方面的能力。这些任务采用可确定评分机制，无需人工干预，并通过对最前沿模型的评测进行误差分析。

Result: 基于新基准的评测结果显示，前沿模型在多轮互动任务上还有较大提升空间。主要问题在于对指令跟随、推理能力和任务规划上的错误。该基准为模型优缺点的分析以及后续研究提供了有力工具。

Conclusion: 当前的大型语言模型（LLM）在处理复杂、互动性的任务时仍有显著不足，尤其是在多轮对话、推理和信息检索方面。新提出的基准测试清楚揭示了这些弱点，并为未来提升这些能力提供了重要研究平台。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [46] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文提出LaaJMeter框架，通过模拟法为特定领域LLM评测工作提供可靠工具，实现评估指标选择和阈值设定的系统化改进，已在代码翻译任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在专业领域作为评判者时，缺乏经过验证的评测标准和充足标注数据，导致评价效果难以判定，亟需可控的评估工具来辅助指标选择和评判阈值设定。

Method: 提出LaaJMeter模拟框架，生成虚拟模型和评审，系统性地分析评测指标在实际环境下的区分能力和敏感性。

Result: 在代码翻译任务领域实证，揭示传统评测指标的局限性，展示不同指标对评审质量的敏感度差异。LaaJMeter具备扩展性，适用于资源匮乏场景下的LLM评估。

Conclusion: LaaJMeter为自然语言处理任务中的LLM评估提供了可扩展、可控且更可信的解决方案，优于现有依赖传统未验证指标的方法。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [47] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文提出并正式定义了翻译难度估计任务，基于模型性能提出新评估指标，并开发了专用模型用于自动筛选机器翻译系统难以处理的文本。实验证明专用模型优于现有方法，可以用于构造更具挑战性的翻译评测基准。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译质量接近完美，现有系统之间的差异变得难以区分，也不易找到未来改进的方向。因此，自动识别机器翻译系统易出错的文本，有助于开发更具区分性的评估方法和指导后续研究。

Method: 作者正式提出了翻译难度估计任务，基于文本的翻译质量来定义其难度，引入了新的评估难度估计器的指标。利用这一指标评估了基线和新方法，包括专用模型和基于启发式方法、LLM判别方法等。最后，通过构建更具挑战性的基准验证了难度估计器的实际效用。

Result: Sentinel-src专用模型在翻译难度估计任务上表现优于启发式和LLM判别方法。作者还发布了两个提升版的难度估计模型Sentinel-src-24和Sentinel-src-25，可用于从大规模文本中筛选对机器翻译系统更具挑战性的样本。

Conclusion: 专用的翻译难度估计模型能更有效地区分和定位机器翻译系统易出错的文本，有助于推动更有针对性的系统改进和评估。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [48] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 该文提出了一种高效的前向推理数据价值评估方法For-Value，无需梯度计算即可实现大模型样本影响力量化，在实验中表现优越，推动大模型的可解释性和数据治理。


<details>
  <summary>Details</summary>
Motivation: 量化单个训练样本的影响对于提升大语言模型（LLMs）和视觉-语言模型（VLMs）的透明度和可追溯性至关重要，但现有方法依赖于Hessian信息或模型再训练，计算成本极高，不适合大规模模型。

Method: 提出了一种仅需前向推理（forward-only）的数据价值评估框架For-Value。该方法只需一次前向传播，通过现代基础模型的丰富表示，基于一个简单的闭式表达式计算影响得分，无需反向传播或梯度计算，大幅提升了在大模型上的可扩展性和效率。

Result: 理论分析证明For-Value能够准确估算每个样本的影响，反映训练样本与验证样本在隐藏表示和预测误差上的一致性。大量实验证明For-Value在选优微调样本、检测错误标签等任务上效果可与甚至优于基于梯度的方法。

Conclusion: For-Value框架实现了对LLMs和VLMs训练样本影响力的高效、可扩展量化，有助于模型透明度和数据管理。该方法为大规模模型数据价值评估提供了实用工具。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [49] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 该工作开发了面向巴基斯坦偏见检测的PakBBQ数据集，对主流多语种LLM在英语和乌尔都语下的公平性和提示工程进行了系统评估，证明了区域化基准和简单提示调整可有效降低低资源语言背景下的模型偏见。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）主要基于西方中心数据进行训练与评估，缺乏对低资源语言和区域背景下公平性的关注，特别是如巴基斯坦等多元文化环境。为了弥补这一差距，亟需开发更具区域和文化适应性的公平性测评基准。

Method: 构建并发布了PakBBQ数据集，这是对原BBQ基准的巴基斯坦区域和文化适应扩展。PakBBQ包含214个模板、17180个问答对，涵盖英语和乌尔都语8个偏见维度。利用PakBBQ，系统评估多种多语言LLM在歧义/明确、负面/非负面等不同问句框架下的行为，并分析模型在不同设置下的偏见表征。

Result: 1）模型在问题被明确定义时准确率平均提升12%；2）在乌尔都语环境下模型展现出更强的反偏见行为；3）以负面方式提出问题有助于显著减少模型的刻板印象回答。

Conclusion: 对于低资源、多元文化背景下的公平性评估，区域化基准和针对性提示工程是有效的缓解模型偏见措施，对LLM在全球应用具有重要意义。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [50] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 本文针对大语言模型容易产生虚构性幻觉，提出了一套基于语句嵌入和信息论度量的自动检测框架，能精确识别和分类不忠实回答，提高模型安全可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在应用过程中经常出现“幻觉”问题，即生成非事实、无意义或与输入内容不符的文本，这严重影响了模型的可靠性。本文旨在解决如何有效检测和量化LLM的忠实性幻觉，特别是与用户查询语义不匹配的“虚构”回答。

Method: 提出了语义分歧度量（SDM）这一新框架，利用轻量级方法，检测LLM的忠实性幻觉。具体方法包括：对输入提示及其语义等价的多种重述进行联合聚类分析，通过句子嵌入创建提示和回答的共享主题空间，并利用信息论指标如Jensen-Shannon散度、Wasserstein距离和KL散度，定量分析提示和回答之间的语义分歧。最终，将这些指标整合为“Semantic Box”框架，对不同类型LLM回答进行诊断和分类。

Result: 实验结果显示，该方法能够更有效地检测出与用户输入语义严重偏离的“虚构”回答，并能量化模型的语义探索行为。$S_H$分数较高时表示幻觉风险大，KL散度可用于区分不同生成行为。

Conclusion: 本文提出的语义分歧度量框架（SDM）相比现有方法更能捕捉大语言模型幻觉的本质，并能细致区分语义不忠实的回答类型，通过信息论与聚类视角全面提升LLM幻觉检测的精度与实用性。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [51] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本文比较了四种深度学习模型在表情符号预测任务中的表现，发现BERT总体性能最优，CNN适合稀有类别，强调模型选择和参数调优的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究如何利用深度学习模型从短文本序列中预测表情符号，以提升人机交互效果。

Method: 采用四种深度学习架构（前馈神经网络、CNN、Transformer、BERT），并通过TweetEval数据集进行训练，使用focal loss和正则化技术解决类别不均衡问题。

Result: BERT表现最佳，因其预训练优势；CNN在处理稀有表情类别时效果更优。

Conclusion: 不同架构及超参数调整对情感感知的表情符号预测至关重要，有助于人机交互的优化。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [52] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: LLM可以高效地从临床访谈文本预测精神分裂症高风险人群的BPRS评分，在多语言及纵向随访中表现优异，为临床自动化和标准化评估带来新可能。


<details>
  <summary>Details</summary>
Motivation: 临床高风险（CHR）精神分裂症患者需要密切监测症状以指导合适的治疗。尽管BPRS是一种有效且广泛使用的症状评估工具，但由于需要较长的结构化访谈而在临床实践中应用有限。

Method: 本研究使用大型语言模型（LLMs）对409名来自AMP-SCZ队列的CHR患者的临床访谈转录文本进行分析，预测其BPRS评分，并与真实评分进行对比。此外，还测试了LLM在外语和纵向整合信息情况下的评估能力。

Result: LLM对BPRS的零样本预测性能优良（中位一致性系数0.84，ICC 0.73），接近人工评定者间一致性。同时，LLM对外语访谈的BPRS预测表现良好（中位一致性系数0.88，ICC 0.70），并能整合纵向信息提升评估能力。

Conclusion: 大型语言模型能有效预测CHR患者的BPRS评分，表现接近人工评定标准，并有望促进高风险精神分裂症患者症状评估的标准化和自动化，尤其在多语言和纵向随访中具有显著应用前景。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [53] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本文通过计算和语料库方法研究了人造语言 Toki Pona 的语言变异，发现其在社区实际使用中会自然演化，并受到社会语言学因素的影响，类似自然语言的演化过程。


<details>
  <summary>Details</summary>
Motivation: Toki Pona 作为仅有大约120个核心词汇的人造语言，其语言变化和变异尚未被充分研究。作者希望通过研究 Toki Pona 的语言特性，理解人造语言在使用者社区的发展和变化规律。

Method: 本研究采用了计算方法和语料库分析，对 Toki Pona 液态词类和及物性等特征进行深入分析。研究侧重（1）内容词在不同句法位置的偏好随时间的变化，以及（2）不同语料库之间的用法差异。

Result: 建议社会语言学因素对 Toki Pona 的影响与自然语言类似，即使是人造语言系统，在社区实际使用过程中也会自然演变。

Conclusion: 人造语言 Toki Pona 的语言变异与自然语言相似，社区使用推动了其自然演化。即便设计初衷简单，其演化规律也受到社会和群体因素的影响。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [54] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 通过让LLM参与自动优化提示词，使其更契合自身归纳偏置，能显著提升模型在分类和排序任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 提示词工程显示LLM对细微提示变动非常敏感，部分原因源于模型的归纳偏置。目标是设计一种方法自动生成更符合模型自身偏好的提示词，从而提升模型实用性和表现。

Method: 使用大语言模型（LLM）的输出作为部分提示词，从而生成更符合模型归纳偏置的prompt。随后用该优化后的prompt进行分类和排序任务评估。

Result: 实验表明，该策略能将分类任务的Likert评分提升至19%，排序任务可提升至27%。

Conclusion: 通过采用归纳偏置提取与匹配策略，可以有效利用大语言模型自身的输出优化提示词，显著提升模型在分类和排序任务上的表现。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [55] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本文通过质性话语分析方法揭示大型语言模型在生成叙事中复制和强化性别与种族偏见，现有的偏见纠正能力有限，强调需批判性和跨学科方法推动AI伦理发展。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）的发展，大型语言模型（LLMs）在各类场景中广泛应用。它们在不断升级的过程中，是否复制和维持了社会偏见，如种族和性别歧视，成为亟需关注的问题。现有偏见检测方法多为自动化、定量手段，难以捕捉自然语言中偏见的细腻表现。

Method: 本文提出了一种定性、话语分析框架，用于补充现有定量方法。研究通过人工分析LLMs生成的关于黑人女性和白人女性的短篇故事，考察性别和种族偏见的再现方式。

Result: 结果显示，LLMs将黑人女性刻画为与祖先和抗争相连，而对白人女性则描绘为自我发现过程。这种模式强化了刻板印象和社会停滞感。在被要求纠正偏见时，模型仅做了表层修正，未能根本改变有问题的叙事逻辑，显示出AI在促进包容性方面的局限。

Conclusion: LLMs在语言生成中持续再现和强化社会偏见，提示AI算法具有意识形态作用。研究强调对AI设计与应用进行批判性、跨学科关注，推动技术和伦理的共同进步。定性分析方法能帮助开发者和用户细致识别并缓解偏见。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [56] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: ReviewRL通过检索增强、监督微调和强化学习的方法，有效提升了自动化论文评审的质量和准确性，为自动化学术批判生成奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 目前的同行评审系统面临着稿件数量不断增长和审稿人疲劳的问题。现有自动化审稿方法存在事实准确性、评分一致性和分析深度不足，难以提供高质量、具有洞见的反馈。

Method: 提出了ReviewRL方法，包括三个主要步骤：（1）使用ArXiv-MCP检索增强型上下文生成管道，结合相关科学文献为评审提供背景；（2）通过监督微调建立基础评审能力；（3）利用复合奖励函数进行强化学习，联合优化评审质量和评分准确性。

Result: 在ICLR 2025会议论文实验中，ReviewRL在规则型指标及模型质量评估均显著优于现有方法。

Conclusion: ReviewRL为基于强化学习的自动科学评审建立了基础框架，在自动化科学发现领域展现出良好的发展潜力。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [57] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER实现表格为核心的文档深层语义解析和表格检索，在真实PDF大规模评测中表现优异，显著优于GPT-4o等主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究多停留在表格表面处理，缺乏对表格及其上下文深层语义解析，限制了高阶数据解读和一致性分析。

Method: 采用自定义数据集及领域专属微调预训练模型，设计了完整解析流程以识别与表格语义相关的上下文片段。

Result: 在近4000页、覆盖1000多表格的真实世界PDF上评测，DOTABLER精确率和F1均达90%以上，效果显著领先同类模型。

Conclusion: DOTABLER在表格语义分析和上下文深层解析任务上性能卓越，超过了如GPT-4o等先进模型。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [58] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: FreLLM4Rec通过频谱滤波和时频调制，有效平衡语义、协同信息，显著提升LLM推荐系统表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLM）的推荐系统在生成具有语义理解的推荐方面具有前景，但存在过度强调用户历史中的语义相关性、导致协同信号逐层衰弱的问题，明显不同于传统的Transformer顺序模型。

Method: 提出了FreLLM4Rec方法：首先使用全局图低通滤波器（G-LPF）去除嵌入中的高频噪声，随后通过时频调制（TFM）逐层主动保留协同信号；并从频谱角度，理论上保证TFM的协同信号保存能力。

Result: 在四个基准数据集上的实验表明，FreLLM4Rec有效减缓了协同信号衰减，推荐性能较最佳基线提升最高达8%（NDCG@10指标）。

Conclusion: FreLLM4Rec为LLM推荐系统协同与语义信息的平衡提供了理论支持和有效方法，揭示了LLM处理协同信息的机制，并显著提升了推荐效果。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [59] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出了 XPE 提示编码器及混合软提示机制，可促进大语言模型在低性能与多语种任务上的参数高效微调，实验结果表明对低表现语言提升尤为突出，多语种适用性也更强。


<details>
  <summary>Details</summary>
Motivation: 扩展软提示技术，提升大语言模型在低性能或多语种任务中的表现，突破传统参数高效微调仅关注单一语言的瓶颈。

Method: 提出了一种称为 Cross-Prompt Encoder (XPE) 的轻量级编码架构，通过多源训练学习不同语系的抽象模式，并设计了 Dual Soft Prompt 机制，将编码器提示与直接训练的标准软提示结合。

Result: 在 SIB-200 多语种基准测试中，XPE模型对低性能语言提升效果显著，混合提示机制在多语种下适应性更佳。

Conclusion: XPE 提示编码器对于低性能语言提升显著，而混合模型在多语种环境下适应性更强。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [60] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 本工作提出并验证了一种高效提升大模型韩语母语级推理能力的方法，并通过创新RL算法提升了训练稳定性和模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在多种语言上表现良好，但其“母语级”思考能力，大多集中于英语等高资源语言。提升LLM在韩语中的推理与思考能力，扩展其原生、多语言能力，对于语言智能的发展和实际应用均具有重要意义。

Method: 提出两阶段微调方法：第一阶段，利用高质量的韩语推理数据集进行监督微调，奠定韩语逻辑推理基础；第二阶段，采用定制的Group Relative Policy Optimization（GRPO）强化学习算法，并引入oracle判别模型校准奖励信号，解决训练中的稳定性问题。两阶段结合，实现模型推理能力与稳健性的双提升。

Result: 最终RL微调后的模型，在高级推理任务（尤其包括数学和编程任务）的基准测试上取得了显著提升，同时保持了知识和语言能力，实现了模型内部的完全韩语链式思考。

Conclusion: 通过两阶段微调（SFT+定制GRPO RL），显著增强了LLM在韩语推理与多步骤任务上的表现，并解决了GRPO训练中的稳定性难题，为多语言环境中的LLM优化提供了可行范式。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [61] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出了一种无需翻译工具的序列到序列跨语言ABSA方法，在复杂任务上有显著提升，且比以英语为主的LLM更有效。


<details>
  <summary>Details</summary>
Motivation: ABSA在低资源语言上仍然存在挑战，目前的研究多集中于英语，且依赖翻译工具。

Method: 提出了一种新颖的序列到序列方法，并采用受约束解码，无需外部翻译工具，解决复杂的跨语言ABSA任务。

Result: 该方法提升了跨语言ABSA任务最多10%的性能，并能应对更复杂任务。同时，与大型语言模型对比发现，微调的多语言LLM效果相近，而以英语为主的LLM则效果较差。

Conclusion: 新方法为跨语言ABSA提供了更加高效实用的替代方案，拓宽了该领域的任务范围，减少了对外部工具的依赖。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [62] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本论文利用Mistral和mT5大模型，在现代捷克语摘要任务上取得最优结果，同时首次推出历史捷克语文献数据集并给出基线，扩展了捷克语文本摘要的研究资源和方向。


<details>
  <summary>Details</summary>
Motivation: 捷克语文本摘要领域，相比英语等高资源语言，研究较少，尤其是历史文献总结，主要由于语言复杂性和缺乏标注数据。大规模语言模型（如Mistral和mT5）在多语种NLP任务中表现出色，因此作者尝试将这些模型用于捷克语摘要。

Method: 采用先进的大规模语言模型Mistral和mT5，对现代捷克语摘要数据集SumeCzech进行测试，并引入一个新的历史捷克语文献摘要数据集Posel od Cerchova，并提供基线实验结果。

Result: 在现代捷克语摘要数据集SumeCzech上取得了新的最优结果，并首次提出了用于历史文献的捷克语摘要数据集Posel od Cerchova，为相关领域提供了基线结果。

Conclusion: 结合强大的多语言模型和新的数据集，显著推动了捷克语摘要及历史文献处理领域的研究，为后续学者探索捷克语自然语言处理奠定基础。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [63] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 该研究提出了一种无需翻译工具的约束解码方法，有效提升跨语言观点分析特别是低资源语言的性能，在多任务环境下也有显著增益，全面超越现有方法，并对大型语言模型表现和实际应用给出建议。


<details>
  <summary>Details</summary>
Motivation: 现有的面向英语的观点分析取得了较大进展，但低资源语言的相关研究较少，且主流跨语言方法依赖外部翻译工具，不适用于复杂任务。

Method: 提出一种基于序列到序列模型的约束解码方法，去除对翻译工具的依赖，同时支持多任务处理，可用单一模型完成多种ABSA任务。对七种语言和六类ABSA任务进行评测，并与大型语言模型(LLM)做零样本、少样本和微调对比。

Result: 该方法在最复杂任务上平均提升跨语言性能5%，在多任务处理上提升10%。在所有测试任务中表现优于现有方法，刷新数项基准。LLM在零样本和少样本下表现较差，但微调后能达到有竞争力的结果，只是训练和推断时间更长。

Conclusion: 提出的方法有效提升了跨语言ABSA在低资源语言的表现，尤其在复杂和多任务场景下表现卓越，为实际应用和未来研究提供了新方向。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [64] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合LLM和人类辅助的高效恶意内容检测方法，用于清理和评估红队数据集，并提出了提升越狱攻击成功率的新策略，相关资源将公开发布。


<details>
  <summary>Details</summary>
Motivation: 现有恶意内容检测方法依赖人工标注或LLM，存在效率低或准确性不足的问题，且红队数据集常包含无害或无效的提示，导致无法准确评估攻击效果。

Method: 采用混合式方法（MDH框架），即通过LLM自动标注并辅以少量人工审核，对数据集进行清理和响应检测。另外提出D-Attack和DH-CoT作为提升越狱成功率的新策略。

Result: MDH框架有效提升了恶意内容检测的效率和准确性。D-Attack与DH-CoT策略显著提升了越狱攻击的成功率。相关代码、数据集和检测结果将开源发布。

Conclusion: 提出了一种结合大型语言模型和人类辅助的恶意内容检测框架MDH，用于高效准确地清理红队数据集，并检测越狱攻击。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [65] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出基于稀疏自动编码器的黑盒攻击框架SFPF，能有效生成越狱对抗样本，突破主流防御机制，增强模型安全评估。方法效果随提示和层次有波动，泛化性待进一步研究。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理（NLP）和大语言模型（LLMs）快速发展，犯罪或越狱攻击对LLM的生成能力提出了挑战。理解模型脆弱性与提升鲁棒性亟需更有效的对抗样本生成方法。

Method: 提出了一种新的黑盒攻击方法——稀疏特征扰动框架（SFPF），借助大模型的可解释性，通过稀疏自动编码器（SAE）识别并操控文本关键特征。对成功攻击的文本进行特征聚类，找到高激活特征并进行选择性扰动，生成新的对抗文本。此方法在扰动时兼顾恶意意图与安全信号，加强绕过已有防御机制的可能性。

Result: 实验表明SFPF生成的对抗文本能突破先进防御机制，揭示了现有NLP系统持续存在的部分漏洞。方法有效性在不同提示语和网络层次间存在差异，对其它架构和更大模型的泛化性有待进一步验证。

Conclusion: SFPF方法在红队测试中证明能兼顾安全与对抗性，有助于揭示和改进大语言模型中的安全漏洞，但其迁移性还需更多测试。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [66] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG通过动态记忆和多步推理机制，显著提升了长文本叙事理解任务中的检索推理能力，在多个基准上超越传统方法，是长文本检索推理的新进展。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在理解长篇故事和小说时，常常由于复杂的情节和角色关系而面临巨大挑战。同时，LLM对长距离上下文的推理能力有限，计算成本高昂。传统的检索增强生成（RAG）方法也存在单步无状态检索，难以捕捉长距离语境中的动态交互关系。

Method: 提出了ComoRAG，一个类脑动态记忆检索框架。该方法通过动态记忆工作区进行多次推理循环，每次生成探查性查询以寻找新的推理路径，并将新检索到的证据整合到全局记忆池中，以逐渐形成连贯的查询解决语境。

Result: 在四个包含超过20万token的长篇叙事理解基准上，ComoRAG与现有强RAG基线相比，取得了高达11%的稳定相对提升。分析表明，ComoRAG对于需要全局理解的复杂查询具有显著优势。

Conclusion: ComoRAG提供了一种受到认知机制启发的，有状态的检索式长上下文推理新范式，在复杂长文本理解任务上效果显著提升。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [67] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本文系统性评估了主流系统在中文成语翻译上的表现，发现错误率高、评价指标不足，提出了新的评测框架和检测模型，有助于提高成语翻译的研究与应用质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器翻译领域取得了显著进展，但对中文成语翻译的研究依然有限。成语在中文中非常常见，其含义常与字面解释不同，并且往往包含历史典故和特殊结构。作者旨在填补这一研究空白，深入探讨和分析现代翻译系统在中文成语翻译方面的能力与问题。

Method: 作者提出了IdiomEval框架，建立了一个详细的成语翻译错误分类方法，并从九个主流翻译系统（包括GPT-4o和Google翻译）采集了900组翻译对，涵盖网页、新闻、维基百科和社交媒体等四个领域，进行了人工注释和分析。同时，作者评估了现有翻译评价指标与人工评价的相关性，并开发了新模型以检测成语翻译错误。

Result: 现有系统在成语翻译上表现不佳，常出现错误、字面或部分翻译，甚至遗漏成语。即使是表现最好的系统GPT-4也在28%的案例中出现错误。此外，目前的评价指标与人类评分的皮尔逊相关系数低于0.48，表明评价质量有限。作者提出的新模型在识别成语翻译错误时F1分数达到0.68。

Conclusion: 当前主流机器翻译系统在中文成语翻译上存在显著不足，评价体系也亟需改进。IdiomEval及其新模型为更准确地评估和提升成语翻译质量提供了有效工具和新思路。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [68] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 本文提出用经济学思想优化LLM计算，利用激励驱动训练，使模型以更低计算实现类似性能，并具备更强的可解释性和自适应能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）面临高昂计算开销的瓶颈。论文旨在探讨如何在资源受限下提升模型计算效率和任务表现。

Method: 提出“计算经济学”框架，把LLM看作由资源受限的agent（如注意力头、神经块）组成的内部经济体，用激励驱动的训练范式，在任务损失基础上引入可微分的计算成本项，指导模型实现稀疏且高效的激活。

Result: 实验结果表明，在GLUE（MNLI, STS-B, CoLA）和WikiText-103任务上，该方法训练得到的模型在精度-计算量Pareto前沿上表现优异，同等精度下FLOPS降低约40%，延迟更低，同时注意力模式也变得更易解释，优于事后剪枝。

Conclusion: 经济学原理为在强资源约束下设计高效、自适应且更透明的LLM提供了新思路，是提升模型资源利用率和任务能力的有效方法。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [69] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 本文提出DiFaR框架，通过多样化提示和事后筛选，增强基于视觉-语言模型的虚假信息检测推理文本质量，在多个基准上优于已有方法，有效解决了多样性、事实性和相关性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大规模视觉-语言模型（LVLM）生成文字推理辅助多模态虚假信息检测的方式，存在三个主要问题：推理内容多样性不足、由于臆测导致事实不准确，以及无关或冲突内容带来噪声。

Method: 提出检测器无关的框架DiFaR，通过五种Chain-of-Thought提示，从LVLM中激发出多样化的推理过程，并采用轻量级事后过滤模块，依据句级事实性和相关性分数筛选推理句子。

Result: 在四个主流基准数据集上，DiFaR相较于四类基线方法最高提升5.9%，对现有检测器性能最高提升8.7%。自动与人工评估结果均显示DiFaR在推理质量三个维度上表现显著提升。

Conclusion: DiFaR不仅提升了多模态虚假信息检测的准确性，还大幅改善了LVLM生成推理的多样性、真实性与相关性，展现了其在增强推理质量和可信度上的价值。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [70] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本论文实证分析了自然语言处理中差分隐私与可解释性方法，揭示二者关系复杂但具备共存可能，并为未来相关研究提出了实用建议。


<details>
  <summary>Details</summary>
Motivation: 近年来，可信自然语言处理中的可解释性和隐私保护成为重要研究方向，但这两者交集领域尚未深入研究，亟需探索其可能的冲突或兼容性。

Method: 采用以差分隐私（DP）和后置可解释性方法为指导，进行实证研究，分析隐私与可解释性在自然语言处理中的权衡。

Result: 发现隐私与可解释性之间关系复杂，受下游任务性质、文本隐私化和可解释性方法选择等多因素影响，同时指出隐私和可解释性有共存潜力，并给出实际建议。

Conclusion: 隐私保护和可解释性的兼容性并非绝对对立，合理设计可实现两者共存，为该领域后续研究提供了参考建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [71] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文系统揭示并量化了多模态大语言模型的“文本主导”问题，分析其成因并提出令牌压缩解决方案，有效提升了模型对多种模态的利用与均衡性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然在多模态任务中表现优异，但存在文本主导问题：推理时高度依赖文本信息，忽视其他模态，影响模型多样化能力。

Method: 系统性地在图像、视频、音频、时序、图等多种数据模态下分析文本主导现象，提出了模态主导指数（MDI）和注意力效率指数（AEI）两个评价指标，并通过实验辨析该问题的成因。进一步，提出了简单的令牌压缩方法以平衡模型注意力分配。

Result: 实验证明文本主导现象在所有测试的模态中普遍且显著，主因包括非文本模态的令牌冗余导致注意力分散、融合架构设计影响以及任务表述偏向文本。提出的令牌压缩方法可显著降低主导性，如在LLaVA-7B上将MDI从10.23降低到0.86，极大改善了各模态的平衡。

Conclusion: 当前多模态大模型存在显著文本主导现象，影响其它模态信息的充分利用。通过新指标系统分析并验证其普遍性和成因，提出令牌压缩法取得有效平衡。该研究为后续更公平、全面的多模态语言模型开发提供方法和理论基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [72] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 论文提出并实测了欧洲eDIF平台，助力LLM可解释性研究基础设施普及。通过远程集群及API，研究者可对大型语言模型进行机制分析。试点研究证明平台稳定且用户反馈积极，但仍存在部分技术瓶颈，计划后续优化。该项目为未来欧洲区域广泛部署奠基。


<details>
  <summary>Details</summary>
Motivation: 推动欧洲范围内大语言模型（LLM）可解释性研究基础设施的普及，实现研究社区的广泛获取，从而促进高级模型分析能力民主化。

Method: 在安斯巴赫应用科学大学建立GPU集群，与合作机构互联，并通过NNsight API实现远程模型检视；组织16名研究人员开展包括激活修补、因果追踪、表征分析等实验，评估平台技术性能、可用性和科学价值。

Result: 平台稳定运行，用户参与度逐步上升，远程实验功能获得积极反馈。试验过程中发现如激活数据下载时间长及偶发执行中断等局限，已在未来开发计划中提出解决方案。

Conclusion: eDIF平台初步实现了欧洲LLM可解释性基础设施广泛接入的目标，为后续更广泛部署、工具拓展和社区协作机制奠定了基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [73] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 本文首次系统性地评估了科普特语到法语翻译的多种策略，发现通过多风格和噪声感知语料微调可显著提升翻译效果，对历史语言翻译工具研发具有重要参考价值。


<details>
  <summary>Details</summary>
Motivation: 目前关于科普特语到法语翻译的研究匮乏，尤其是针对系统性策略与方法的分析几乎没有。历史语言的翻译工具开发存在重大挑战，因此需要探索有效的翻译策略与流程。

Method: 作者设计了一个全面的翻译流程系统评估，包括：直接翻译与中介语言翻译(pivot)、预训练对性能的影响、多版本微调(multiversion fine-tuning)的好处，以及模型对噪声的鲁棒性。实验基于对齐的圣经语料库进行。

Result: 结果显示，采用具有风格多样性且考虑噪声的训练语料进行微调，可以显著提升翻译质量。

Conclusion: 本研究提出的翻译策略和流程，为开发历史语言的翻译工具提供了重要的实践性见解。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [74] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 融合Transformer、STGCN和LSTM的新方法有效提升了连续孟加拉手语的翻译准确性，在多个数据集上取得了新纪录，并推动了无词汇手语翻译技术发展。


<details>
  <summary>Details</summary>
Motivation: 尽管手语是聋哑人士重要的交流工具，但因其在口语为主的社会中常被低估，导致交流障碍和社会排斥。因此，提升手语翻译的准确性与实用性对于促进交流无障碍至关重要。

Method: 本文提出将图卷积网络（STGCN）和LSTM与Transformer架构融合的混合方法，用于连续孟加拉手语的自动翻译。作者探索了多种架构融合策略，并在多个公开数据集进行了对比实验。

Result: 在RWTH-PHOENIX-2014T, CSL-Daily, How2Sign和BornilDB v1.0等多个数据集上，提出方法的BLEU-4分数分别比最优方法提升了4.01、2.07和0.5，均显著超过现有同类技术。同时首次在BornilDB v1.0数据集上建立了基准。

Conclusion: 所提出的方法在不同手语数据集上实现了新的最先进性能，特别是在无词汇（gloss-free）翻译方面展现了更高的准确性，并首次在BornilDB v1.0数据集上进行了基准测试。该方法为聋哑人士的沟通无障碍迈出重要一步，也为未来相关研究提供了新标准。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [75] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 论文提出VAC个性化问答框架，通过自然语言反馈替代传统强化学习中的标量奖励，实现更有效的个性化学习。实验和人工评估表明，VAC在多个领域明显超越已有方法，生成回答质量更高。


<details>
  <summary>Details</summary>
Motivation: 目前针对大语言模型（LLMs）的个性化方法主要依赖于检索增强生成（RAG）和基于标量奖励的强化学习来利用检索到的个人上下文。但标量奖励信号在反馈上存在不足，限制了学习效率和个性化质量。作者希望通过更丰富的反馈机制提高个性化效果。

Method: 提出了VAC框架，用自然语言反馈（NLF）替代标量奖励。NLF反馈基于用户画像和问题叙述生成，为模型行动提供更丰富、可执行的监督信号。训练过程在优化反馈模型和微调策略模型之间交替进行，最终策略模型在推理时不再需要额外反馈。

Result: 在LaMP-QA基准测试的三个不同领域，VAC框架实现了对现有最先进方法持续且显著的提升。人工评估也证实了生成响应的更高质量。

Conclusion: 自然语言反馈（NLF）相较于标量奖励信号，能更有效优化个性化问答任务，提升问答质量和用户满意度。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [76] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出ICE框架，实现扩散大语言模型更灵活、高效的推理，显著提升准确率并降低计算开销，实验结果优异。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型（LLMs）采用仅支持前缀式提示和顺序生成，缺乏对双向信息的灵活利用，限制了推理能力和生成方式。扩散式大语言模型（dLLMs）凭借其双向关注机制和迭代精炼过程，带来了新的可能性。本文旨在突破前缀限制，引入更灵活的提示策略。

Method: 本文提出一种新颖的框架ICE（In-Place Chain-of-Thought Prompting with Early Exit），将前缀提示转变为原地提示，并专为扩散大语言模型设计。ICE框架在迭代过程中将提示直接嵌入掩码词位位置，并引入置信度感知的提前终止机制以降低计算开销。

Result: 广泛实验证明，ICE在GSM8K数据集上可带来最高17.29%的准确率提升，同时加速4.12倍；在MMLU数据集上最高可加速276.67倍，并保持有竞争力的性能。

Conclusion: ICE框架有效地为扩散大语言模型带来了更灵活和高效的推理方式，显著提升了准确率并大幅降低了计算成本。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [77] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出一种模拟人类专家行为的结构化LLM自动新颖性评估方法，在ICLR 2025论文上验证有效性，与人类评审高度一致，远超现有基线，为提升同行评审质量与透明度提供新思路。


<details>
  <summary>Details</summary>
Motivation: 面向NLP等高投稿量领域，评审人员面临压力，新颖性评估流程混乱且效率低下，亟需自动化、高一致性的辅助工具以提升审稿质量和流程透明度。

Method: 通过三阶段结构化流程实现自动化新颖性评估：1）从论文中提取内容；2）检索和综合相关文献；3）结构化比对以进行证据驱动的新颖性判断。方法设计受到大规模人类新颖性评审分析启发，捕捉专家评审的关键模式，如独立论点验证和语境化推理。

Result: 在182篇ICLR 2025投稿及人类新颖性标注评测中，方法与人类推理的一致率达86.5%，结论一致率为75.3%，显著优于现有LLM基线，并能输出更详细、更有文献背景的分析，评审结论更一致。

Conclusion: 提出的方法在自动化新颖性评估中表现出与人类评审高度一致性，并在一致性和详细性上超过了现有基于LLM的基线方法，有望提高同行评审的规范性和透明度。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [78] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 通过提出MS-GRPO算法并结合新采样策略，显著提升了小型LLM在多步决策任务中的效果，后训练后的3B模型在某任务上远优于72B基线。该方法为提升小模型能力提供了高效新途径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）适合做为序列决策代理，但受限于需要大型、计算成本高的模型。现有的后训练方法主要用于单轮交互，无法解决多步任务中的奖励归因问题。因此，需要改进较小规模模型并适用于多步任务的有效方法。

Method: 提出了一种新的后训练算法：多步组相对策略优化（MS-GRPO），建立在正式的文本媒介随机博弈（TSMG）和语言代理策略（LAP）框架上。MS-GRPO通过将整个累计奖励分配给每个步骤，实现信用归因。同时引入绝对优势加权的采样方式来提升训练效率。

Result: 在Snake和Frozen Lake两个环境中，对一个30亿参数的模型进行后训练。结果显示，经过MS-GRPO后训练的3B模型在Frozen Lake任务上的表现超过72B参数的基线模型50%。

Conclusion: MS-GRPO方法可显著提升小规模大语言模型在序列决策任务中的表现，是替代依赖于模型规模的实际可行方案。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [79] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 本文提出了首个融合推理、共情和心理学专业知识的中文心理健康大模型Psyche-R1，通过创新数据合成和混合训练策略，显著提升了模型在心理学任务上的表现，实现小模型媲美超大模型的效果。


<details>
  <summary>Details</summary>
Motivation: 心理健康专业人才短缺，心理疾病负担加重，迫切需要高效助力的技术手段。当前大模型在编程、数学领域推理能力突出，但心理健康领域多关注情感支持，缺乏推理机制提升回复可靠性。

Method: 提出Psyche-R1，这是首个融合共情、心理学知识与推理能力的中文心理健康大模型。设计了新颖的数据合成流程，包括链式思维(CoT)推理与逐步优化，生成高质量问题-推理对及共情对话。采用混合训练策略，通过多模型交叉筛选和群体相对策略优化(GRPO)提升推理能力，剩余数据用于监督微调(SFT)以增强共情和专业性。

Result: Psyche-R1模型在多个心理学基准测试中表现优异，7B规模模型可比肩DeepSeek-R1的671B大模型。

Conclusion: Psyche-R1有效融合了共情、心理学专业和推理能力，在心理健康领域提供更可靠、高效的支持。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [80] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文构建了一个融合特征工程、数据扩增和可解释机器学习的多维自动口译评估框架，在英中数据集上取得高性能结果，实现了高可解释性和透明度。该方法有助于促进学习者自我调节，是人工评估的可靠替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有自动口译质量评估研究存在语言使用质量考察不足、模型有效性受限于数据稀缺和不均衡、以及模型预测缺乏解释性的问题。

Method: 提出了一个多维建模框架，结合特征工程、数据增强和可解释的机器学习方法，只使用与口译评估相关且具有透明性的特征，并通过Shapley Value（SHAP）分析解释模型预测。

Result: 在一个新颖的英中交替口译数据集上实现了较强的预测性能，发现BLEURT和CometKiwi分数是忠实度的最强预测特征，停顿相关特征对流利度预测最为重要，而中文短语多样性指标则有效测量语言使用质量。

Conclusion: 所提出的方法在可解释性、可扩展性和可靠性方面表现优异，为传统人工评估提供了透明且细致的自动化替代方案，有助于为学习者提供诊断性反馈并促进自我调节学习。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [81] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文揭示大语言模型通过Self-Search和Self-Search RL框架，可以成为高效的内部搜索模拟器，显著降低RL搜索任务对外部引擎依赖，提升稳定性和成本效率，并能促进知识的有效利用和实际迁移。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）在搜索任务时高度依赖外部搜索引擎，成本高且环境变化大。大语言模型（LLM）能否作为有效的内部搜索模拟器，减少对外部工具的依赖，提升训练效率？

Method: 提出Self-Search，通过结构化提示和多次采样量化LLM自身的搜索能力。继而设计Self-Search RL (SSRL)训练框架，结合格式化奖励和规则奖励提升LLM内部知识利用，并在无外部工具的状态下实现RL策略训练。

Result: 实验表明，SSRL训练的LLM政策模型可提供低成本、稳定的RL训练环境，大幅降低对外部搜索引擎依赖；能高质量利用世界知识，并在多个问答基准测试上取得优秀表现。

Conclusion: 1）LLM蕴含丰富世界知识，可有效激发用于提升任务性能；2）SSRL可降低模型幻觉现象；3）SSRL模型能无缝接入外部搜索引擎。研究显示LLM有潜力支持更高效与可扩展的RL智能体训练。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [82] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本综述系统分析了扩散语言模型（DLMs）的发展、技术原理及最新模型，对其在推理速度和生成质量方面的优势作出评估，并探讨了多模态扩展与实际应用。同时总结了DLMs的主要挑战及未来研究方向，为相关领域研究提供了宝贵参考。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）在自动回归（AR）范式之外展示出强大的潜力，研究者亟需对其发展历程、核心原理及最新应用进行系统梳理。DLMs通过并行生成和迭代去噪在推理速度、双向上下文建模以及生成控制等方面具备天然优势，因此推动了该领域相关技术与理论的需求增长。

Method: 本文为一篇综述性论文，从DLMs的基础原理、与其他范式的关联、主流模型演变、预训练到后训练技术，全面分类并分析当前技术发展。内容包括推理策略、解码并行、缓存机制、生成质量提升，以及多模态扩展和实际应用场景。同时细致探讨DLMs目前面临的效率、长序列处理和基础设施需求等瓶颈，最后展望未来研究方向。

Result: 论文系统地整理了DLMs的技术生态，覆盖其进展、优化策略和实际应用，提出了分类体系，对现有模型的推理、生成和多模态扩展进行了深入解读，阐述了DLMs相比AR模型的优势。同时，归纳了当前主要挑战与局限，并为后续研究提供方向参考。论文还配套线上资源库GitHub。

Conclusion: 本文为扩散语言模型研究领域提供了全面的综述，从理论、技术实现到应用，为学界和工业界理解并推动DLMs的发展奠定了基础，强调了其未来持续进步的关键挑战和机遇。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [83] [Active Automata Learning with Advice](https://arxiv.org/abs/2508.10535)
*Michał Fica,Jan Otop*

Main category: cs.FL

TL;DR: 本论文提出了一种将主动自动机学习与基于建议的推理结合的新框架，能通过利用字符串重写系统的建议，显著减少自动机学习过程中的查询次数，并通过实验证明该方法有效且可无缝适配现有主流算法。


<details>
  <summary>Details</summary>
Motivation: 现有自动机学习框架在查询次数和教师负担上存在瓶颈，因此提出新框架以减少查询次数，提高效率。

Method: 结合主动自动机学习与基于建议的演绎推理，通过允许算法利用教师提供的字符串重写系统建议来推断某些查询答案。

Result: 通过实证评估，新方法在查询复杂性上取得了显著改善，验证了其有效性。

Conclusion: 提出的学习框架能够显著减少所需查询次数，并且易于适应现有Angluin风格的学习算法。

Abstract: We present an extended automata learning framework that combines active
automata learning with deductive inference. The learning algorithm asks
membership and equivalence queries as in the original framework, but it is also
given advice, which is used to infer answers to queries when possible and
reduce the burden on the teacher. We consider advice given via string rewriting
systems, which specify equivalence of words w.r.t. the target languages. The
main motivation for the proposed framework is to reduce the number of queries.
We show how to adapt Angluin-style learning algorithms to this framework with
low overhead. Finally, we present empirical evaluation of our approach and
observe substantial improvement in query complexity.

</details>
