<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Herb.jl: A Unifying Program Synthesis Library](https://arxiv.org/abs/2510.09726)
*Tilman Hinnerichs,Reuben Gardos Reid,Jaap de Jong,Bart Swinkels,Pamela Wochner,Nicolae Filat,Tudor Magurescu,Issa Hanou,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: Herb.jl是一个模块化、易扩展的程序合成库，可以快速组合和复用已有方法，显著提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 程序合成是AI领域中的核心任务，但现有工具复用和组合不同方法十分繁琐且耗时。作者希望通过统一框架简化程序合成方法的复用和扩展。

Method: 提出了Herb.jl程序合成库，采用Julia语言开发，将合成算法模块化为可通讯和可扩展的组件，便于用户重新组装和应用合成方法。通过三个场景演示了工具的灵活性：包括简单问题实现、已有工具复刻和基准测试运行。

Result: Herb.jl实现了算法模块的高内聚性和易用性，通过少量代码即可应用或复用复杂的程序合成工具，有效提升开发效率。

Conclusion: Herb.jl统一并模块化了各种程序合成方法，极大简化了工具复用和扩展过程，对程序合成领域研究和应用提供了重要支持。

Abstract: Program synthesis -- the automatic generation of code given a specification
-- is one of the most fundamental tasks in artificial intelligence (AI) and
many programmers' dream. Numerous synthesizers have been developed to tackle
program synthesis, manifesting different ideas to approach the exponentially
growing program space. While numerous smart program synthesis tools exist,
reusing and remixing previously developed methods is tedious and
time-consuming. We propose Herb.jl, a unifying program synthesis library
written in the Julia programming language, to address these issues. Since
current methods rely on similar building blocks, we aim to modularize the
underlying synthesis algorithm into communicating and fully extendable
sub-compartments, allowing for straightforward reapplication of these modules.
To demonstrate the benefits of using Herb.jl, we show three common use cases:
1. how to implement a simple problem and grammar, and how to solve it, 2. how
to implement a previously developed synthesizer with just a few lines of code,
and 3. how to run a synthesizer against a benchmark.

</details>


### [2] [ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions](https://arxiv.org/abs/2510.09932)
*Devansh Jain,Akash Pardeshi,Marco Frigo,Krut Patel,Kaustubh Khulbe,Jai Arora,Charith Mendis*

Main category: cs.PL

TL;DR: ACT能够基于ISA描述，自动生成高性能张量加速器编译器后端，有效降低开发成本，提升编译效率，在多种平台上展示了卓越的性能和低开销。


<details>
  <summary>Details</summary>
Motivation: 当前许多新型张量加速器缺乏自动化编译器后端，且由于设计迭代快，手工开发编译器后端效率低下，影响软件开发和硬件采纳。

Method: 提出了ACT系统，通过对张量加速器ISA描述自动生成编译器后端，设计中采用参数化等价饱和指令选择和约束编程内存分配方法。

Result: 为三种产业界和学术界的加速器成功自动生成了编译器后端，并在代码性能和编译效率上均优于或等同于手工优化，实现了很好的通用性和自动化效果。

Conclusion: ACT能够自动、高效地为张量加速器生成编译器后端，生成的后端在性能上可媲美甚至优于手工优化库，并保持低编译开销。

Abstract: Tensor compilers play a key role in enabling high-performance implementations
of deep learning workloads. These compilers rely on existing CPU and GPU code
generation backends to generate device-specific code. Recently, many tensor
accelerators (neural processing units) have been proposed to further accelerate
these workloads. Compared to commodity hardware, however, most of the proposed
tensor accelerators do not have compiler backends with code generation support.
Moreover, the accelerator designs are subject to fast iteration cycles, making
it difficult to manually develop compiler backends similar to commodity
hardware platforms. Therefore, to increase adoption and enable faster software
development cycles for novel tensor accelerator designs, we need to make the
compiler backend construction process more agile.
  To address this gap, we introduce ACT, a compiler backend generator that
automatically generates compiler backends for tensor accelerators, given just
the instruction set architecture (ISA) descriptions. We first formally specify
the compiler backend generation problem that introduces a novel specification
for describing tensor accelerator ISAs. Next, we design ACT such that it
supports user-programmable memories and complex parameterized instructions that
are prevalent in tensor accelerators. ACT uses a novel parameterized equality
saturation-based instruction selection phase and a constraint programming-based
memory allocation phase. We prove that compiler backends generated by ACT are
sound and complete. Finally, we generate compiler backends for three
accelerator platforms from industry and academia, and show that they match or
outperform code written using hand-optimized kernel libraries while maintaining
low compilation overheads.

</details>


### [3] [End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation](https://arxiv.org/abs/2510.10015)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 为解决现代安全语言中安全与非安全模块混合时的端到端安全验证难题，本文提出了“开放安全”定义，实现了模块化、组合式的安全验证，并通过在Owlang与C混合程序上实证展示了方法效果。


<details>
  <summary>Details</summary>
Motivation: 现代安全编程语言（如Rust）需要在实现完整功能时混合使用安全与不安全模块，提高端到端安全验证的复杂度。现有方法难以模块化、可组合地验证和维护此类程序的安全性。

Method: 提出了基于开放标签转换系统（open LTS）的模块化、通用的“开放安全”（open safety）定义，可组合地对异构模块分别验证安全性，并在目标级进行结果合成。此外，提出了从“部分安全”到“完全安全”的形式化验证理论，并实现了适用于所有权语言Owlang的验证编译器。

Result: 该方法能够支持安全与非安全模块混合的程序，通过开放安全定义实现了模块级别的安全验证与集成。在Owlang与C混合实现的哈希映射表中成功验证了组合安全性，证明了方法的实用性与有效性。

Conclusion: 提出的开放安全框架实现了对于异构模块（安全与非安全代码）的模块化、组合式的安全验证，并为端到端程序安全性提供了理论和实践支持。框架能够结合验证与验证编译，适应现代安全语言和系统的需要。

Abstract: Program safety (i.e., absence of undefined behaviors) is critical for correct
operation of computer systems. It is usually verified at the source level
(e.g., by separation logics) and preserved to the target by verified compilers
(e.g., CompCert), thereby achieving end-to-end verification of safety. However,
modern safe programming languages like Rust pose new problems in achieving
end-to-end safety. Because not all functionalities can be implemented in the
safe language, mixing safe and unsafe modules is needed. Therefore, verified
compilation must preserve a modular notion of safety which can be composed at
the target level. Furthermore, certain classes of errors (e.g., memory errors)
are automatically excluded by verifying compilation (e.g., borrow checking) for
modules written in safe languages. As a result, verified compilation needs to
cooperate with verifying compilation to ensure end-to-end safety.
  To address the above problems, we propose a modular and generic definition of
safety called open safety based on program semantics described as open labeled
transition systems (LTS). Open safety is composable at the boundary of modules
and can be modularly preserved by verified compositional compilation. Those
properties enable separate verification of safety for heterogeneous modules and
composition of the safety results at the target level. Open safety can be
generalized to partial safety (i.e., only a certain class of errors can occur).
By this we formalized the correctness of verifying compilation as derivation of
total safety from partial safety. We demonstrate how our framework can combine
verified and verifying compilation by developing a verified compiler for an
ownership language (called Owlang) inspired by Rust. We evaluate our approach
on the compositional safety verification using a hash map implemented by Owlang
and C.

</details>


### [4] [LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization](https://arxiv.org/abs/2510.10209)
*Massinissa Merouani,Afif Boudaoud,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: LOOPerSet是一个超大规模多面体程序公开数据集，支持机器学习驱动的编译器优化研究，降低数据获取难度。


<details>
  <summary>Details</summary>
Motivation: 由于公开的大规模、多样化编译器性能数据集匮乏，学术界在利用机器学习提升编译器优化时受限明显，数据采集成本高，研究难以复现。为解决该痛点，作者提出开放数据集。

Method: 作者通过合成生成22万个多面体程序，并应用多种保持语义的转换序列（如融合、倾斜、分块、并行），获得每次转换后的真实执行时间（性能）作为标签数据，最终形成2800万条标记数据。

Result: 本文提出了LOOPerSet，一个包含2800万标签数据点和22万个独特合成多面体程序的公开数据集。每个数据点关联一个程序和一系列保持语义的转换操作（如融合、倾斜、分块和并行化）与其真实性能（执行时间）。该数据集为训练和评估学习型成本模型、基准测试新模型架构以及探索自动多面体调度领域提供了重要资源。数据集还以宽松许可发布，促进可复现研究并降低编译器优化门槛。

Conclusion: LOOPerSet通过大型、多样化且标记完善的数据集填补了编译优化领域数据缺口，有助于促进新模型开发、研究复现及自动调度创新。其公开与宽松许可极大降低了研究门槛。

Abstract: The advancement of machine learning for compiler optimization, particularly
within the polyhedral model, is constrained by the scarcity of large-scale,
public performance datasets. This data bottleneck forces researchers to
undertake costly data generation campaigns, slowing down innovation and
hindering reproducible research learned code optimization. To address this gap,
we introduce LOOPerSet, a new public dataset containing 28 million labeled data
points derived from 220,000 unique, synthetically generated polyhedral
programs. Each data point maps a program and a complex sequence of
semantics-preserving transformations (such as fusion, skewing, tiling, and
parallelism)to a ground truth performance measurement (execution time). The
scale and diversity of LOOPerSet make it a valuable resource for training and
evaluating learned cost models, benchmarking new model architectures, and
exploring the frontiers of automated polyhedral scheduling. The dataset is
released under a permissive license to foster reproducible research and lower
the barrier to entry for data-driven compiler optimization.

</details>


### [5] [Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis](https://arxiv.org/abs/2510.10216)
*Zhechong Huang,Zhao Zhang,Ruyi Ji,Tingxuan Xia,Qihao Zhu,Qinxiang Cao,Zeyu Sun,Yingfei Xiong*

Main category: cs.PL

TL;DR: TyFlow通过类型驱动的新代码表示，内化类型推理于代码生成过程，有效消除类型错误并提升代码功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在代码生成方面表现出色，但类型正确性问题仍难以解决。传统方法通过外部机制拒绝不可类型的代码，未能让模型自身掌握类型推理，限制了整体性能提升。

Method: 本文提出TyFlow系统，将类型推理内化于代码生成过程中。核心创新在于一种类型引导的程序综合系统，保持类型推导树与综合推导树的同构关系，并提出基于综合决策序列的新型代码表示，替代传统文本标记序列。这种方法将类型系统学习的复杂度转移到表示本身，使模型能将计算资源用于更高层次的程序语义。

Result: TyFlow在评估中不仅消除了类型错误，还显著提升了函数正确性。

Conclusion: 将类型系统与语言模型内部机制对齐至关重要，能显著改善代码生成中的类型正确性和功能正确性。

Abstract: Language models have shown remarkable proficiency in code generation;
nevertheless, ensuring type correctness remains a challenge. Although
traditional methods, such as constrained decoding, alleviate this problem by
externally rejecting untypable code, the model itself does not effectively
learn type reasoning internally, which ultimately limits its overall
performance. This paper introduces TyFlow, a novel system that internalizes
type reasoning within code generation to guide the model to learn the type
system. The core of our approach is a novel type-guided program synthesis
system that maintains an isomorphism between type derivation trees and
synthesis derivation trees, enabling a new code representation based on
synthesis decision sequences rather than traditional text-based token
sequences. By offloading the complexity of type system learning to the
representation itself, models can redirect their computational resources toward
higher-level program semantics. Our evaluation shows that TyFlow not only
eliminates type errors but also significantly improves functional correctness,
highlighting the importance of aligning LMs with type systems internally.

</details>


### [6] [Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)](https://arxiv.org/abs/2510.11007)
*Antonina Nepeivoda,Ilya Afanasyev*

Main category: cs.PL

TL;DR: 提出一种新字符串区间抽象域及约简策略，有效支持JavaScript字符串操作程序分析。


<details>
  <summary>Details</summary>
Motivation: 当前JavaScript等编程语言中，字符串操作非常普遍，但由于字符串可能无限多样，静态分析其属性和行为具有挑战性。为此，需要更精确且实用的方法来描述和分析字符串相关的程序行为。

Method: 提出了一个新的字符串区间抽象域（string-interval abstract domain），用一组单词方程（编码字符串的下界）和单词不等式（编码字符串的上界）来刻画字符串区间。基于这种区间的格结构，定义了抽象字符串对象，并采用长度-不递增映射将其建模为半格的约简乘积，并给出多种约简策略，保证其域结构形成格。此外，还设计了基本的抽象字符串操作，以减少约简带来的计算开销。

Result: 证明了在这些约简策略下，抽象字符串对象域构成一个格，并展示了该抽象域如何有效分析JavaScript字符串操作相关的程序属性。

Conclusion: 本文的方法通过新颖的字符串区间抽象，提高了对字符串操作程序的静态分析精度和效率，尤其适用于JavaScript相关场景。

Abstract: We introduce a string-interval abstract domain, where string intervals are
characterized by systems of word equations (encoding lower bounds on string
values) and word disequalities (encoding upper bounds). Building upon the
lattice structure of string intervals, we define an abstract string object as a
reduced product on a string property semilattice, determined by
length-non-increasing morphisms. We consider several reduction strategies for
abstract string objects and show that upon these strategies the string object
domain forms a lattice. We define basic abstract string operations on the
domain, aiming to minimize computational overheads on the reduction, and show
how the domain can be used to analyse properties of JavaScript string
manipulating programs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 本文系统梳理了LLM赋能的软件工程研究现状，提出完整评测—解决方案对照体系，揭示领域演进趋势与未来研究方向，为相关学者与从业者提供权威指引。


<details>
  <summary>Details</summary>
Motivation: 大模型（LLM）正在彻底改变软件工程，但目前领域内对评测方法与解决方案之间的关联缺乏系统性理解，阻碍了技术发展和评估。

Method: 对150多篇近期文献进行系统梳理，提出覆盖“解决方案”（基于提示、微调、智能体）和“基准评测”（代码生成、翻译、修复等任务）的综合分类法，并展示从任务到结果的统一流程。

Result: 系统总结了领域从简单提示工程到复杂智能体的演变，全面关联了50多个基准及对应解决范式，揭示了研究现状、存在问题，并提出了如多智能体协作、自进化代码生成、形式化验证集成等未来方向。

Conclusion: 本综述首次全景归纳了LLM在软件工程中的应用，弥补了评测方案与解决手段的联系空白，为后续研究和应用提供了基础性资源和前瞻建议。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [8] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: 本文提出InteractScience基准，系统评测LLM生成交互科学演示代码的能力，发现其在知识与前端交互整合方面仍有不足，并为领域发展和应用落地提供了有效检验工具。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测只关注知识问答或静态代码生成，缺乏对科学领域知识与前端交互性代码集成能力的真实测评，制约了科学演示类应用的开发与教育价值。

Method: 设计了结合程序化功能测试与视觉定性测试的混合评测框架，并创建涵盖五大科学领域的题集，配套单元测试、参考快照及检核表，对30个主流开源及闭源LLM进行系统化评估。

Result: 实验显示当前LLM在科学知识与交互前端代码整合能力方面仍有明显短板，InteractScience揭示了该方向的挑战和改进空间，为未来相关任务提供了真实评测和发展根基。

Conclusion: InteractScience为自动评测大语言模型在科学领域交互性代码生成能力提供了第一个系统化的基准，为相关能力的提升和研究奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [9] [Modular Counting over 3-Element and Conservative Domains](https://arxiv.org/abs/2510.09950)
*Andrei A. Bulatov,Amirhossein Kazeminia*

Main category: cs.LO

TL;DR: 本文系统研究了一般关系结构上的模计数CSP，提出了新的归约方法，并对3元域和保守域复杂性进行分析，为该领域提供了理论和工具支持。


<details>
  <summary>Details</summary>
Motivation: 约束满足问题（CSP）和其计数版本在建模组合问题方面非常重要，且得到广泛研究。传统的模计数（即对同构计数取模）研究集中于图同构，但很少系统性扩展到一般关系结构。因此，该研究旨在填补这一空白，对一般关系结构上的模计数CSP进行系统性研究。

Method: 该文提出了一种新的方法，将模计数问题归约到更小的定义域上。同时，深入分析了3元域以及保守域（能表达所有一元谓词的关系结构）上的问题复杂性。

Result: 主要成果包括新提出的模计数问题归约方法和对3元域及保守域上的问题复杂性分类。论文为通用关系结构上的模计数同构问题研究提供了工具和理论支持。

Conclusion: 该文推动了通用关系结构上模计数CSP复杂性理论的发展，提出了新的归约工具并对特殊情形作出复杂性刻画，为后续进一步研究奠定基础。

Abstract: In the Constraint Satisfaction Problem (CSP for short) the goal is to decide
the existence of a homomorphism from a given relational structure $G$ to a
given relational structure $H$. If the structure $H$ is fixed and $G$ is the
only input, the problem is denoted $CSP(H)$. In its counting version,
$\#CSP(H)$, the task is to find the number of such homomorphisms. The CSP and
#CSP have been used to model a wide variety of combinatorial problems and have
received a tremendous amount of attention from researchers from multiple
disciplines.
  In this paper we consider the modular version of the counting CSPs, that is,
problems of the form $\#_pCSP(H)$ of counting the number of homomorphisms to
$H$ modulo a fixed prime number $p$. Modular counting has been intensively
studied during the last decade, although mainly in the case of graph
homomorphisms. Here we continue the program of systematic research of modular
counting of homomorphisms to general relational structures. The main results of
the paper include a new way of reducing modular counting problems to smaller
domains and a study of the complexity of such problems over 3-element domains
and over conservative domains, that is, relational structures that allow to
express (in a certain exact way) every possible unary predicate.

</details>


### [10] [Proof Strategy Extraction from LLMs for Enhancing Symbolic Provers](https://arxiv.org/abs/2510.10131)
*Jian Fang,Yican Sun,Yingfei Xiong*

Main category: cs.LO

TL;DR: 本文提出Strat2Rocq方法，将LLM的证明策略转化为形式化引理，供符号证明器调用。实验表明该方法使自动化证明工具CoqHammer成功率提升13.41%，显示从LLM提取知识可显著增强符号工具能力。


<details>
  <summary>Details</summary>
Motivation: 软件验证中，交互式定理证明需要大量人工，但写正式证明很耗时，提高自动化很重要。目前自动化工具多依赖符号证明器，不过大语言模型（LLM）在定理证明方面也展现了强大能力。但直接使用LLM成本高，安全性差，因此纯符号方法依然重要。本文探索能否从LLM提取内部策略以增强符号证明器能力。

Method: 提出Strat2Rocq方法，将LLM产生的证明策略提取并形式化为Rocq中的引理，使符号证明器（如CoqHammer）可以调用。具体做法是让LLM生成定理的自然语言证明，再让其总结为带证明的形式化引理，并通过标准的agentic方法纠错。然后将这些新引理加入CoqHammer以提升其自动化证明能力。

Result: 将LLM提取的引理加入后，CoqHammer在开源Rocq项目上的证明成功率提升了13.41%。

Conclusion: 通过提取和形式化LLM的策略，引入符号证明器后显著提升了定理自动化证明能力，证明了二者互补与协同价值。

Abstract: One important approach to software verification is interactive theorem
proving. However, writing formal proofs often requires substantial human
effort, making proof automation highly important. Traditionally, proof
automation has relied on symbolic provers. Recently, large language models
(LLMs) have demonstrated strong capabilities in theorem proving, complementing
symbolic provers. Nonetheless, prompting LLMs can be expensive and may pose
security risks for confidential codebases. As a result, purely symbolic
approaches remain important even in the LLM era, as they are cost-effective,
secure, and complement the strengths of LLMs.
  Motivated by these considerations, we ask a new research question: can we
extract the internal strategies of LLMs to enhance the capabilities of symbolic
provers? As an initial attempt to answer this question, we propose Strat2Rocq,
which extracts proof strategies from LLMs and formalizes them as lemmas in
Rocq. These lemmas are accessible to symbolic provers such as CoqHammer. With
the addition of these LLM-extracted lemmas, CoqHammer is able to prove more
theorems. The knowledge extraction process involves analyzing the proof
trajectories of LLMs on a training set of proved theorems. For each theorem, we
prompt the LLM to generate a natural language proof, then ask it to summarize
this proof into formalized lemmas with proofs. We also employ a standard
agentic approach to mitigate errors during formalization. Our evaluation
demonstrates that, on open-source Rocq projects for software verification,
Strat2Rocq enhances the success rate of CoqHammer by 13.41%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 本文系统综述表格问答（TQA）领域，特别关注大语言模型相关进展，分类梳理任务设定、挑战与方法，分析优劣并指出尚待探究的前沿问题，为推动领域发展提供了框架和指导建议。


<details>
  <summary>Details</summary>
Motivation: 表格问答（TQA）是针对表格数据及相关文本回答自然语言问题的任务。由于涉及多种表格表现、问题复杂度、模态和领域，该领域发展迅速但缺乏系统性的整理和理解，尤其在强化学习等新兴方向下。文献亟需对现有研究进行梳理，帮助理清任务设定与研究趋势。

Method: 本文通过调研现有TQA相关文献，重点关注基于大语言模型（LLM）的方法。对已有基准和任务设置进行分类，对现有模型策略根据所应对的挑战归组，并分析其优劣。同时，关注尚未充分研究的新兴相关主题。

Result: 文章归纳了TQA领域的主要研究路线，分类整理了不同类型的任务设定和基准数据集，总结了当前方法解决问题的思路，并揭示了这些方法的局限性。此外，指出了前沿但尚未深入关注的研究话题和领域存在的开放问题。

Conclusion: 本综述统一了TQA领域的分散研究并揭示了核心挑战，梳理了LLM驱动的研究趋势，为相关社区提供了基础参考，有助于深入理解现状并指导未来发展。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [12] [The Tribonacci constant and finite automata](https://arxiv.org/abs/2510.10834)
*Jeffrey Shallit*

Main category: cs.FL

TL;DR: 本文证明了自动机无法接受和生成某些Tribonacci相关的表示和序列，指出了其理论局限性。


<details>
  <summary>Details</summary>
Motivation: 探索Tribonacci表示法与自动机之间的关系，以及自动机能否在Tribonacci数列相关计算中进行识别和生成，例如同时判定n和x=⌊nψ⌋的表示。

Method: 分析自动机对Tribonacci数列及其相关表示的接受能力，通过理论证明和反例说明自动机无法完成相关任务。

Result: 证明了不存在能够并行接受n和x=⌊nψ⌋的Tribonacci表示的自动机，也不存在能够生成斜率为ψ-1的Sturmian特征词的Tribonacci自动机。

Conclusion: Tribonacci数列相关的某些计算和生成任务超出了自动机的能力范围，自动机无法并行处理这些复杂关系。

Abstract: We show that there is no automaton accepting the Tribonacci representations
of $n$ and $x$ in parallel, where $\psi = 1.839\cdots$ is the Tribonacci
constant, and $x= \lfloor n \psi \rfloor$. Similarly, there is no Tribonacci
automaton generating the Sturmian characteristic word with slope $\psi-1$.

</details>
